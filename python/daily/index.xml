<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-12T01:32:02Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Nefcore/CRLFsuite</title>
    <updated>2022-06-12T01:32:02Z</updated>
    <id>tag:github.com,2022-06-12:/Nefcore/CRLFsuite</id>
    <link href="https://github.com/Nefcore/CRLFsuite" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast CRLF injection scanning tool&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Nefcore/CRLFsuite&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Nefcore/CRLFsuite&#34;&gt;&lt;img src=&#34;https://github.com/Nefcore/CRLFsuite/raw/main/static/CRLFsuite_logo2.0.png&#34; height=&#34;150&#34; width=&#34;150&#34;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;CRLFsuite - CRLF injection scanner&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Made%20with-Python-1f425f.svg?sanitize=true&#34; alt=&#34;made-with-python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Nefcore/CRLFsuite/releases/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/Nefcore/CRLFsuite&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/ansicolortags/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/ansicolortags.svg?sanitize=true&#34; alt=&#34;PyPI license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Nefcore/CRLFsuite/network/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/forks/Nefcore/CRLFsuite/&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Nefcore/badges/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/Nefcore/CRLFsuite&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CRLFsuite is a fast tool specially designed to scan &lt;code&gt;CRLF injection&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/Nefcore/CRLFsuite/raw/main/static/crlfsuitev2.0.svg?sanitize=true&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚¨áÔ∏è Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;$ git clone https://github.com/Nefcore/CRLFsuite.git&#xA;$ cd CRLFsuite&#xA;$ sudo python3 setup.py install&#xA;$ crlfsuite -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; Single URL scanning&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; Multiple URL scanning&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; WAF detection&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; XSS through CRLF injection&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; Stdin supported&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; GET &amp;amp; POST method supported&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; Concurrency&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; Powerful payloads (WAF evasion payloads are also included)&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚úî&lt;/span&gt; Fast and efficient scanning with negligible false-positive&lt;/p&gt; &#xA;&lt;h2&gt;Arguments&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Argument&lt;/th&gt; &#xA;   &lt;th&gt;Discription&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-u/--url&lt;/td&gt; &#xA;   &lt;td&gt;target URL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-i/--import-urls&lt;/td&gt; &#xA;   &lt;td&gt;Import targets from the file&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-s/--stdin&lt;/td&gt; &#xA;   &lt;td&gt;Scan URLs from stdin&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-o/--output&lt;/td&gt; &#xA;   &lt;td&gt;Path for output file&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-m/--method&lt;/td&gt; &#xA;   &lt;td&gt;Request method (GET/POST)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-d/--data&lt;/td&gt; &#xA;   &lt;td&gt;POST data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-uA/--user-agent&lt;/td&gt; &#xA;   &lt;td&gt;Specify User-Agent&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-To/--timeout&lt;/td&gt; &#xA;   &lt;td&gt;Connection timeout&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-c/--cookies&lt;/td&gt; &#xA;   &lt;td&gt;Specify cookies&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-v/--verify&lt;/td&gt; &#xA;   &lt;td&gt;Verify SSL cert.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-t/--threads&lt;/td&gt; &#xA;   &lt;td&gt;Number of concurrent threads&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-sB/--skip-banner&lt;/td&gt; &#xA;   &lt;td&gt;Skip banner and args info&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-sP/--show-payloads&lt;/td&gt; &#xA;   &lt;td&gt;Show all the available CRLF payloads&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Single URL scanning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ crlfsuite -u &#34;http://testphp.vulnweb.com&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multiple URLs scanning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ crlfsuite -i targets.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from stdin:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ subfinder -d google.com -silent | httpx -silent | crlfsuite -s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Specifying cookies üç™:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ crlfsuite -u &#34;http://testphp.vulnweb.com&#34; --cookies &#34;key=val; newkey=newval&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using POST method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ crlfsuite -i targets.txt -m POST -d &#34;key=val&amp;amp;newkey=newval&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;üëâ&lt;/span&gt; &lt;a href=&#34;https://github.com/Nefcore/CRLFsuite/raw/main/LICENSE&#34;&gt;MIT LICENSE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Bug report&lt;/h2&gt; &#xA;&lt;p&gt;If You&#39;re facing some errors or issues with this tool, you can open a issue here:&lt;/p&gt; &#xA;&lt;p&gt;üëâ &lt;a href=&#34;https://github.com/Nefcore/CRLFsuite/issues&#34;&gt;Open a issue&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mosaicml/composer</title>
    <updated>2022-06-12T01:32:02Z</updated>
    <id>tag:github.com,2022-06-12:/mosaicml/composer</id>
    <link href="https://github.com/mosaicml/composer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;library of algorithms to speed up neural network training&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/mosaicml/composer#gh-light-mode-only&#34; class=&#34;only-light&#34;&gt; &lt;img src=&#34;https://storage.googleapis.com/docs.mosaicml.com/images/header_light.svg?sanitize=true&#34; width=&#34;50%&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_BEGIN --&gt; &lt;a href=&#34;https://github.com/mosaicml/composer#gh-dark-mode-only&#34; class=&#34;only-dark&#34;&gt; &lt;img src=&#34;https://storage.googleapis.com/docs.mosaicml.com/images/header_dark.svg?sanitize=true&#34; width=&#34;50%&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_END --&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;p align=&#34;center&#34;&gt;A PyTorch Library for Efficient Neural Network Training&lt;/p&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;p align=&#34;center&#34;&gt;Train Faster, Reduce Cost, Get Better Models&lt;/p&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.mosaicml.com&#34;&gt;[Website]&lt;/a&gt; - &lt;a href=&#34;https://docs.mosaicml.com/en/stable/getting_started/installation.html&#34;&gt;[Getting Started]&lt;/a&gt; - &lt;a href=&#34;https://docs.mosaicml.com/&#34;&gt;[Docs]&lt;/a&gt; - &lt;a href=&#34;https://docs.mosaicml.com/en/stable/method_cards/methods_overview.html&#34;&gt;[Methods]&lt;/a&gt; - &lt;a href=&#34;https://www.mosaicml.com/team&#34;&gt;[We&#39;re Hiring!]&lt;/a&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/mosaicml/&#34;&gt; &lt;img alt=&#34;PyPi Version&#34; src=&#34;https://img.shields.io/pypi/pyversions/mosaicml&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/mosaicml/&#34;&gt; &lt;img alt=&#34;PyPi Package Version&#34; src=&#34;https://img.shields.io/pypi/v/mosaicml&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/mosaicml/&#34;&gt; &lt;img alt=&#34;PyPi Downloads&#34; src=&#34;https://img.shields.io/pypi/dm/mosaicml&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://docs.mosaicml.com/en/stable/&#34;&gt; &lt;img alt=&#34;Documentation&#34; src=&#34;https://readthedocs.org/projects/composer/badge/?version=stable&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg&#34;&gt; &lt;img alt=&#34;Chat @ Slack&#34; src=&#34;https://img.shields.io/badge/slack-chat-2eb67d.svg?logo=slack&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/mosaicml/composer/raw/dev/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/License-Apache%202.0-green.svg?logo=slack&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üëã Welcome&lt;/h1&gt; &#xA;&lt;p&gt;Composer is a library written in PyTorch that enables you to &lt;b&gt;train neural networks faster, at lower cost, and to higher accuracy&lt;/b&gt;. We&#39;ve implemented more than two dozen speed-up methods that can be applied to your training loop in just a few lines of code, or used with our built-in Trainer. We continually integrate the latest state-of-the-art in efficient neural network training.&lt;/p&gt; &#xA;&lt;p&gt;Composer features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;20+ methods for speeding up training networks for computer vision and language modeling. Don&#39;t waste hours trying to reproduce research papers when Composer has done the work for you.&lt;/li&gt; &#xA; &lt;li&gt;An easy-to-use trainer that has been written to be as performant as possible and &lt;a href=&#34;https://www.mosaicml.com/blog/5-best-practices-for-efficient-model-training&#34;&gt;integrates best practices&lt;/a&gt; for efficient training.&lt;/li&gt; &#xA; &lt;li&gt;Functional forms of all of our speedup methods that allow you to integrate them into your existing training loop.&lt;/li&gt; &#xA; &lt;li&gt;Strong, &lt;em&gt;reproducible&lt;/em&gt; baselines to get you started as quickly as possible.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benefits&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://storage.googleapis.com/docs.mosaicml.com/images/cost_graph_light.svg#gh-light-mode-only&#34; class=&#34;only-light&#34;&gt; &lt;img src=&#34;https://storage.googleapis.com/docs.mosaicml.com/images/cost_graph_light.svg?sanitize=true&#34; width=&#34;85%&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- link to the light mode image even on dark mode, so it will be readable in a new tab --&gt; &#xA; &lt;!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_BEGIN --&gt; &lt;a href=&#34;https://storage.googleapis.com/docs.mosaicml.com/images/cost_graph_light.svg#gh-dark-mode-only&#34; class=&#34;only-dark&#34;&gt; &lt;img src=&#34;https://storage.googleapis.com/docs.mosaicml.com/images/cost_graph_dark.svg?sanitize=true&#34; width=&#34;85%&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- SETUPTOOLS_LONG_DESCRIPTION_HIDE_END --&gt; &lt;/p&gt; &#xA;&lt;p&gt;With no additional tuning, you can apply our methods to:&lt;/p&gt; &#xA;&lt;!-- start numbers --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train ResNet-50 on ImageNet to the standard 76.6% top-one accuracy for $40 in 1.2 hours (&lt;em&gt;with vanilla PyTorch:&lt;/em&gt; $116 in 3.5 hours) on AWS.&lt;/li&gt; &#xA; &lt;li&gt;Train a GPT-2 125M to a standard perplexity of 24.11 for $145 in 4.5 hours (&lt;em&gt;with vanilla PyTorch&lt;/em&gt;: $255 in 7.8 hours) on AWS.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- end numbers --&gt; &#xA;&lt;h1&gt;üöÄ Quickstart&lt;/h1&gt; &#xA;&lt;h2&gt;üíæ Installation&lt;/h2&gt; &#xA;&lt;p&gt;Composer is available with Pip:&lt;/p&gt; &#xA;&lt;!--pytest-codeblocks:skip--&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install mosaicml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, install Composer with Conda:&lt;/p&gt; &#xA;&lt;!--pytest-codeblocks:skip--&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c mosaicml mosaicml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üöå Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can use Composer&#39;s speedup methods in two ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Through a standalone &lt;strong&gt;Functional API&lt;/strong&gt; (similar to &lt;code&gt;torch.nn.functional&lt;/code&gt;) that allows you to integrate them into your existing training code.&lt;/li&gt; &#xA; &lt;li&gt;Using Composer&#39;s built-in &lt;strong&gt;Trainer&lt;/strong&gt;, which is designed to be performant and automatically takes care of many of the low-level details of using speedup methods.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example: Functional API &lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/functional_api.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Integrate our speed-up methods into your training loop with just a few lines of code, and see the results. Here we easily apply &lt;a href=&#34;https://docs.mosaicml.com/en/stable/method_cards/blurpool.html&#34;&gt;BlurPool&lt;/a&gt; and SqueezeExcite:&lt;/p&gt; &#xA;&lt;!-- begin_example_1 ---&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import composer.functional as cf&#xA;from torchvision import models&#xA;&#xA;my_model = models.resnet18()&#xA;&#xA;# add blurpool and squeeze excite layers&#xA;my_model = cf.apply_blurpool(my_model)&#xA;my_model = cf.apply_squeeze_excite(my_model)&#xA;&#xA;# your own training code starts here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- end_example_1 ---&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/functional_api.ipynb&#34;&gt;Composer Functional API Colab notebook&lt;/a&gt; and &lt;a href=&#34;https://docs.mosaicml.com/en/latest/functional_api.html&#34;&gt;Functional API guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Example: Trainer &lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/getting_started.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;For the best experience and the most efficient possible training, we recommend using Composer&#39;s built-in trainer, which automatically takes care of the low-level details of using speedup methods and provides useful abstractions that facilitate rapid experimentation.&lt;/p&gt; &#xA;&lt;!-- begin_example_2 ---&gt; &#xA;&lt;!-- TODO: Address timeouts --&gt; &#xA;&lt;!--pytest-codeblocks:skip--&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import DataLoader&#xA;from torchvision import datasets, transforms&#xA;&#xA;from composer import Trainer&#xA;from composer.algorithms import BlurPool, ChannelsLast, CutMix, LabelSmoothing&#xA;from composer.models import MNIST_Classifier&#xA;&#xA;transform = transforms.Compose([transforms.ToTensor()])&#xA;train_dataset = datasets.MNIST(&#34;data&#34;, download=True, train=True, transform=transform)&#xA;eval_dataset = datasets.MNIST(&#34;data&#34;, download=True, train=False, transform=transform)&#xA;train_dataloader = DataLoader(train_dataset, batch_size=128)&#xA;eval_dataloader = DataLoader(eval_dataset, batch_size=128)&#xA;&#xA;trainer = Trainer(&#xA;    model=MNIST_Classifier(num_classes=10),&#xA;    train_dataloader=train_dataloader,&#xA;    eval_dataloader=eval_dataloader,&#xA;    max_duration=&#34;2ep&#34;,&#xA;    algorithms=[&#xA;        BlurPool(replace_convs=True, replace_maxpools=True, blur_first=True),&#xA;        ChannelsLast(),&#xA;        CutMix(num_classes=10),&#xA;        LabelSmoothing(smoothing=0.1),&#xA;    ]&#xA;)&#xA;trainer.fit()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- end_example_2 --&gt; &#xA;&lt;p&gt;Composer&#39;s built-in &lt;a href=&#34;https://docs.mosaicml.com/en/stable/trainer/using_the_trainer.html&#34;&gt;trainer&lt;/a&gt; makes it easy to &lt;strong&gt;add multiple speedup methods in a single line of code!&lt;/strong&gt; Trying out new methods or combinations of methods is as easy as changing a single list. As we continually implement more methods, they will be easy for you to add to your code.&lt;/p&gt; &#xA;&lt;p&gt;For concrete examples of methods in Composer, here are some (&lt;a href=&#34;https://docs.mosaicml.com/en/latest/trainer/algorithms.html&#34;&gt;&lt;em&gt;see here for all&lt;/em&gt;&lt;/a&gt;) speedup methods currently in Composer:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Attribution&lt;/th&gt; &#xA;   &lt;th&gt;tl;dr&lt;/th&gt; &#xA;   &lt;th&gt;Example Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;Speed Up*&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/alibi&#34;&gt;Alibi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.12409&#34;&gt;Press et al, 2021&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Replace attention with AliBi.&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/blurpool&#34;&gt;BlurPool&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.11486&#34;&gt;Zhang, 2019&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Applies an anti-aliasing filter before every downsampling operation.&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.2x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/channels_last&#34;&gt;ChannelsLast&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html&#34;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Uses channels last memory format (NHWC).&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.mosaicml.com/en/latest/method_cards/cutout.html&#34;&gt;CutOut&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.04552&#34;&gt;DeVries et al, 2017&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Randomly erases rectangular blocks from the image.&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.2x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/label_smoothing&#34;&gt;LabelSmoothing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.00567&#34;&gt;Szegedy et al, 2015&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Smooths the labels with a uniform prior&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/mixup&#34;&gt;MixUp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.09412&#34;&gt;Zhang et al, 2017&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Blends pairs of examples and labels.&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/randaugment&#34;&gt;RandAugment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Cubuk_Randaugment_Practical_Automated_Data_Augmentation_With_a_Reduced_Search_Space_CVPRW_2020_paper.html&#34;&gt;Cubuk et al, 2020&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Applies a series of random augmentations to each image.&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.3x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/sam&#34;&gt;SAM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.01412&#34;&gt;Foret et al, 2021&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An optimization strategy that seeks flatter minima.&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.4x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/seq_length_warmup&#34;&gt;SeqLengthWarmup&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06084&#34;&gt;Li et al, 2021&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Progressively increase sequence length.&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;1.2x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.mosaicml.com/en/latest/method_cards/stochastic_depth.html&#34;&gt;Stochastic Depth&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.09382&#34;&gt;Huang et al, 2016&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Replaces a specified layer with a stochastic version that randomly drops the layer or samples during training&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.1x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;right&#34;&gt;* = time-to-train to the same quality as the baseline.&lt;/p&gt; &#xA;&lt;h2&gt;üõ† Building Speedup Recipes&lt;/h2&gt; &#xA;&lt;p&gt;Given two methods that speed up training by 1.5x each, do they combine to provide a 2.25x (1.5x * 1.5x) speedup? Not necessarily. They may optimize the &lt;a href=&#34;https://en.wikipedia.org/wiki/Amdahl&#39;s_law&#34;&gt;same part of the training process&lt;/a&gt; and lead to diminishing returns, or they may even interact in ways that prove detrimental. Determining which methods to compose together isn&#39;t as simple as assembling a set of methods that perform best individually.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We have come up with compositions of methods that work especially well together&lt;/strong&gt; through rigorous exploration of the design space of recipes and research on the science behind composition. The &lt;a href=&#34;https://app.mosaicml.com/&#34;&gt;MosaicML Explorer&lt;/a&gt; contains all of the data we have collected so far on composition, and it highlights the compositions of methods that are &lt;em&gt;pareto-optimal&lt;/em&gt; - that provide the &lt;strong&gt;best possible tradeoffs between training time or cost and the quality of the trained model&lt;/strong&gt;. Whether you want to reach the same quality faster or get better quality within your current budget, Explorer can help you decide which speedup methods to use. We update this data regularly as we add new methods and develop better recipes.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://storage.googleapis.com/docs.mosaicml.com/images/methods/explorer.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;As an example, here are two performant recipes, one for ResNet-101 on ImageNet, and the other for GPT-2 on OpenWebText, on 8xA100s:&lt;/p&gt; &#xA;&lt;h3&gt;ResNet-101&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Functional&lt;/th&gt; &#xA;   &lt;th&gt;tl;dr&lt;/th&gt; &#xA;   &lt;th&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;Speed Up&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/blurpool&#34;&gt;Blur Pool&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;cf.apply_blurpool&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.11486&#34;&gt;Applies an anti-aliasing filter before every downsampling operation.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.2x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/channels_last&#34;&gt;Channels Last&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;cf.apply_&lt;/code&gt;&lt;br&gt;&lt;code&gt;channels_last&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html&#34;&gt;Uses channels last memory format (NHWC).&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/label_smoothing&#34;&gt;Label Smoothing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;cf.smooth_labels&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.00567&#34;&gt;Smooths the labels with a uniform prior.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/mixup&#34;&gt;MixUp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;CF.mixup_batch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.09412&#34;&gt;Blends pairs of examples and labels.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/progressive_resizing&#34;&gt;Progressive Resizing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;cf.resize_batch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fastai/fastbook/raw/780b76bef3127ce5b64f8230fce60e915a7e0735/07_sizing_and_tta.ipynb&#34;&gt;Increases the input image size during training.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.3x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/sam&#34;&gt;SAM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.01412&#34;&gt;SAM optimizer measures sharpness of optimization space.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-101&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Composition&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Cheapest: $49 @ 78.1% Acc&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ResNet-101&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.5x&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;GPT-2&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Functional&lt;/th&gt; &#xA;   &lt;th&gt;tl;dr&lt;/th&gt; &#xA;   &lt;th&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;Speed Up&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/alibi&#34;&gt;Alibi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;cf.apply_alibi&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.12409&#34;&gt;Replace attention with AliBi.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;1.6x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mosaicml/composer/tree/dev/composer/algorithms/seq_length_warmup&#34;&gt;Seq Length Warmup&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;cf.set_batch_&lt;/code&gt;&lt;br&gt;&lt;code&gt;sequence_length&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.06084&#34;&gt;Progressively increase sequence length.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;1.5x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Composition&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Cheapest: $145 @ 24.11 PPL&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.7x&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;‚öôÔ∏è What benchmarks does Composer support?&lt;/h1&gt; &#xA;&lt;p&gt;Composer uses a &lt;em&gt;benchmark&lt;/em&gt; as a term to denote a particular model trained on a particular dataset in a standardized, reproducible way. A benchmark is a specific model trained for a task, where a task = dataset + loss function + metric.&lt;/p&gt; &#xA;&lt;p&gt;We support computer vision and natural language processing use cases, such as (but not limited to) the following. New benchmarks will be added regularly, as will compatibility with existing libraries.&lt;/p&gt; &#xA;&lt;div class=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Dataset&lt;/th&gt; &#xA;    &lt;th&gt;Loss&lt;/th&gt; &#xA;    &lt;th&gt;Task&lt;/th&gt; &#xA;    &lt;th&gt;Evaluation Metrics&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;5&#34; align=&#34;center&#34;&gt;&lt;b&gt;Computer Vision&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ResNet Family&lt;/td&gt; &#xA;    &lt;td&gt;CIFAR-10&lt;/td&gt; &#xA;    &lt;td&gt;Cross Entropy&lt;/td&gt; &#xA;    &lt;td&gt;Image Classification&lt;/td&gt; &#xA;    &lt;td&gt;Classification Accuracy&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;ResNet Family&lt;/td&gt; &#xA;    &lt;td&gt;ImageNet&lt;/td&gt; &#xA;    &lt;td&gt;Cross Entropy&lt;/td&gt; &#xA;    &lt;td&gt;Image Classification&lt;/td&gt; &#xA;    &lt;td&gt;Classification Accuracy&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientNet Family&lt;/td&gt; &#xA;    &lt;td&gt;ImageNet&lt;/td&gt; &#xA;    &lt;td&gt;Cross Entropy&lt;/td&gt; &#xA;    &lt;td&gt;Image Classification&lt;/td&gt; &#xA;    &lt;td&gt;Classification Accuracy&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;UNet&lt;/td&gt; &#xA;    &lt;td&gt;BraTS&lt;/td&gt; &#xA;    &lt;td&gt;Dice Loss&lt;/td&gt; &#xA;    &lt;td&gt;Image Segmentation&lt;/td&gt; &#xA;    &lt;td&gt;Dice Coefficient&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DeepLab v3&lt;/td&gt; &#xA;    &lt;td&gt;ADE20K&lt;/td&gt; &#xA;    &lt;td&gt;Cross Entropy&lt;/td&gt; &#xA;    &lt;td&gt;Image Segmentation&lt;/td&gt; &#xA;    &lt;td&gt;mIoU&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34; colspan=&#34;5&#34;&gt;&lt;b&gt;Natural Language Processing&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;BERT Family&lt;/td&gt; &#xA;    &lt;td&gt;{Wikipedia &amp;amp; BooksCorpus, C4}&lt;/td&gt; &#xA;    &lt;td&gt;Cross Entropy&lt;/td&gt; &#xA;    &lt;td&gt;Masked Language Modeling&lt;/td&gt; &#xA;    &lt;td&gt;GLUE &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;GPT Family&lt;/td&gt; &#xA;    &lt;td&gt;{OpenWebText, C4}&lt;/td&gt; &#xA;    &lt;td&gt;Cross Entropy&lt;/td&gt; &#xA;    &lt;td&gt;Language Modeling&lt;br&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Perplexity&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;ü§î Why should I use Composer?&lt;/h1&gt; &#xA;&lt;p&gt;The compute required to train a state-of-the-art machine learning model is &lt;a href=&#34;https://arxiv.org/abs/2202.05924&#34;&gt;doubling every 6 months&lt;/a&gt;, putting these capabilities further and further out of reach for the broader community with each passing day. Composer addresses this challenge by focusing on training efficiency: it contains cutting-edge speedup methods that modify the training algorithm to reduce the time and cost necessary to train deep learning models. &lt;strong&gt;When you use Composer, you can rest assured that you are training efficiently.&lt;/strong&gt; We have combed the literature, done the science, and built industrial-grade implementations to ensure this is the case.&lt;/p&gt; &#xA;&lt;p&gt;Even after these speedup methods are implemented, assembling them together into recipes is nontrivial. We designed Composer with the &lt;strong&gt;right abstractions to composing (and creating new) speedup methods.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Specifically, Composer&#39;s efficiency methods use &lt;strong&gt;two-way callbacks&lt;/strong&gt; from (&lt;a href=&#34;https://arxiv.org/abs/2002.04688&#34;&gt;Howard et al, 2020&lt;/a&gt;) to modify the &lt;strong&gt;entire training state&lt;/strong&gt; at particular events in the training loop to effect speed-ups. We handle collisions between methods, the proper order of execution for algorithms, and more.&lt;/p&gt; &#xA;&lt;p&gt;Through this, our methods can modify:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;data inputs for batches (data augmentations, sequence length warmup, skipping examples, etc.)&lt;/li&gt; &#xA; &lt;li&gt;neural network architecture (pruning, model surgery, etc.)&lt;/li&gt; &#xA; &lt;li&gt;loss function (label smoothing, MixUp, CutMix, etc.)&lt;/li&gt; &#xA; &lt;li&gt;optimizer (Sharpness Aware Minimization)&lt;/li&gt; &#xA; &lt;li&gt;training dynamics (layer freezing, selective backprop, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Easily &lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/custom_speedup_methods.ipynb&#34;&gt;add your own methods&lt;/a&gt; or callbacks to instrument any part of the training loop.&lt;/p&gt; &#xA;&lt;h1&gt;üßê Why shouldn‚Äôt I use Composer?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Composer is mostly optimized for computer vision (CV) and natural language processing (NLP) use cases, including &lt;a href=&#34;https://docs.mosaicml.com/en/stable/composer_model.html&#34;&gt;custom models&lt;/a&gt; and custom datasets. We strongly encourage exploration on integrating our algorithms into new domains, such as reinforcement learning. Feel free to &lt;a href=&#34;https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg&#34;&gt;join our Slack&lt;/a&gt; and discuss!&lt;/li&gt; &#xA; &lt;li&gt;Composer currently supports NVIDIA GPUs. We are adding support for additional hardware platforms, and you should expect more soon.&lt;/li&gt; &#xA; &lt;li&gt;Composer is an active and ongoing project. Since Composer is still in alpha, our API may not be stable. We recommend pegging your work to a Composer version, and we will respond quickly to issues posted to this repository.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìö Learn More&lt;/h1&gt; &#xA;&lt;p&gt;Here&#39;s some resources actively maintained by the Composer community to help you get started:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;b&gt;Resource&lt;/b&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;b&gt;Details&lt;/b&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/getting_started.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Getting started with our Trainer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An interactive Colab Notebook aimed at teaching users about our Trainer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/functional_api.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Getting started with our Functional API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An interactive Colab Notebook aimed at teaching users about our Functional API&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/custom_speedup_methods.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Building Speedup Methods&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An interactive Colab Notebook aimed at teaching users about building speedup methods on top of Composer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/nlp_models.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Training BERTs with Composer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An interactive Colab Notebook aimed at helping users learn how to train BERT models with Composer!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mosaicml.com/jobs&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;We&#39;re Hiring!&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Join us! ü§©&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you have any questions, please feel free to reach out to us on &lt;a href=&#34;https://twitter.com/mosaicml&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;mailto:community@mosaicml.com&#34;&gt;email&lt;/a&gt;, or our &lt;a href=&#34;https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg&#34;&gt;Community Slack&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;üí´ Contributors&lt;/h1&gt; &#xA;&lt;p&gt;Composer is part of the broader Machine Learning community, and we welcome any contributions, pull requests, or issues!&lt;/p&gt; &#xA;&lt;p&gt;To start contributing, see our &lt;a href=&#34;https://github.com/mosaicml/composer/raw/dev/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h1&gt;‚úçÔ∏è Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{mosaicml2022composer,&#xA;    author = {The Mosaic ML Team},&#xA;    title = {composer},&#xA;    year = {2021},&#xA;    howpublished = {\url{https://github.com/mosaicml/composer/}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/detr</title>
    <updated>2022-06-12T01:32:02Z</updated>
    <id>tag:github.com,2022-06-12:/facebookresearch/detr</id>
    <link href="https://github.com/facebookresearch/detr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;End-to-End Object Detection with Transformers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;DE‚´∂TR&lt;/strong&gt;: End-to-End Object Detection with Transformers&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.fb.com/support-ukraine&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34; alt=&#34;Support Ukraine&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch training code and pretrained models for &lt;strong&gt;DETR&lt;/strong&gt; (&lt;strong&gt;DE&lt;/strong&gt;tection &lt;strong&gt;TR&lt;/strong&gt;ansformer). We replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining &lt;strong&gt;42 AP&lt;/strong&gt; on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/detr/main/.github/DETR.png&#34; alt=&#34;DETR&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What it is&lt;/strong&gt;. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. Due to this parallel nature, DETR is very fast and efficient.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;About the code&lt;/strong&gt;. We believe that object detection should not be more difficult than classification, and should not require complex libraries for training and inference. DETR is very simple to implement and experiment with, and we provide a &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb&#34;&gt;standalone Colab Notebook&lt;/a&gt; showing how to do inference with DETR in only a few lines of PyTorch code. Training code follows this idea - it is not a library, but simply a &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detr/main/main.py&#34;&gt;main.py&lt;/a&gt; importing model and criterion definitions with standard training loops.&lt;/p&gt; &#xA;&lt;p&gt;Additionnally, we provide a Detectron2 wrapper in the d2/ folder. See the readme there for more information.&lt;/p&gt; &#xA;&lt;p&gt;For details see &lt;a href=&#34;https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers&#34;&gt;End-to-End Object Detection with Transformers&lt;/a&gt; by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.&lt;/p&gt; &#xA;&lt;h1&gt;Model Zoo&lt;/h1&gt; &#xA;&lt;p&gt;We provide baseline DETR and DETR-DC5 models, and plan to include more in future. AP is computed on COCO 2017 val5k, and inference time is over the first 100 val5k COCO images, with torchscript transformer.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;schedule&lt;/th&gt; &#xA;   &lt;th&gt;inf_time&lt;/th&gt; &#xA;   &lt;th&gt;box AP&lt;/th&gt; &#xA;   &lt;th&gt;url&lt;/th&gt; &#xA;   &lt;th&gt;size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;0&lt;/th&gt; &#xA;   &lt;td&gt;DETR&lt;/td&gt; &#xA;   &lt;td&gt;R50&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;0.036&lt;/td&gt; &#xA;   &lt;td&gt;42.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth&#34;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/logs/detr-r50_log.txt&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;159Mb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1&lt;/th&gt; &#xA;   &lt;td&gt;DETR-DC5&lt;/td&gt; &#xA;   &lt;td&gt;R50&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;0.083&lt;/td&gt; &#xA;   &lt;td&gt;43.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-f0fb7ef5.pth&#34;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/logs/detr-r50-dc5_log.txt&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;159Mb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;2&lt;/th&gt; &#xA;   &lt;td&gt;DETR&lt;/td&gt; &#xA;   &lt;td&gt;R101&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;0.050&lt;/td&gt; &#xA;   &lt;td&gt;43.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth&#34;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/logs/detr-r101_log.txt&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;232Mb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;3&lt;/th&gt; &#xA;   &lt;td&gt;DETR-DC5&lt;/td&gt; &#xA;   &lt;td&gt;R101&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;0.097&lt;/td&gt; &#xA;   &lt;td&gt;44.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth&#34;&gt;model&lt;/a&gt;&amp;nbsp;|&amp;nbsp;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/logs/detr-r101-dc5_log.txt&#34;&gt;logs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;232Mb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;COCO val5k evaluation results can be found in this &lt;a href=&#34;https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918&#34;&gt;gist&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The models are also available via torch hub, to load DETR R50 with pretrained weights simply do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = torch.hub.load(&#39;facebookresearch/detr:main&#39;, &#39;detr_resnet50&#39;, pretrained=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;COCO panoptic val5k models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;box AP&lt;/th&gt; &#xA;   &lt;th&gt;segm AP&lt;/th&gt; &#xA;   &lt;th&gt;PQ&lt;/th&gt; &#xA;   &lt;th&gt;url&lt;/th&gt; &#xA;   &lt;th&gt;size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;0&lt;/th&gt; &#xA;   &lt;td&gt;DETR&lt;/td&gt; &#xA;   &lt;td&gt;R50&lt;/td&gt; &#xA;   &lt;td&gt;38.8&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;   &lt;td&gt;43.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/detr-r50-panoptic-00ce5173.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;165Mb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1&lt;/th&gt; &#xA;   &lt;td&gt;DETR-DC5&lt;/td&gt; &#xA;   &lt;td&gt;R50&lt;/td&gt; &#xA;   &lt;td&gt;40.2&lt;/td&gt; &#xA;   &lt;td&gt;31.9&lt;/td&gt; &#xA;   &lt;td&gt;44.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-panoptic-da08f1b1.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;165Mb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;2&lt;/th&gt; &#xA;   &lt;td&gt;DETR&lt;/td&gt; &#xA;   &lt;td&gt;R101&lt;/td&gt; &#xA;   &lt;td&gt;40.1&lt;/td&gt; &#xA;   &lt;td&gt;33&lt;/td&gt; &#xA;   &lt;td&gt;45.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/detr/detr-r101-panoptic-40021d53.pth&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;237Mb&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Checkout our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb&#34;&gt;panoptic colab&lt;/a&gt; to see how to use and visualize DETR&#39;s panoptic segmentation prediction.&lt;/p&gt; &#xA;&lt;h1&gt;Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;We provide a few notebooks in colab to help you get a grasp on DETR:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_attention.ipynb&#34;&gt;DETR&#39;s hands on Colab Notebook&lt;/a&gt;: Shows how to load a model from hub, generate predictions, then visualize the attention of the model (similar to the figures of the paper)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb&#34;&gt;Standalone Colab Notebook&lt;/a&gt;: In this notebook, we demonstrate how to implement a simplified version of DETR from the grounds up in 50 lines of Python, then visualize the predictions. It is a good starting point if you want to gain better understanding the architecture and poke around before diving in the codebase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb&#34;&gt;Panoptic Colab Notebook&lt;/a&gt;: Demonstrates how to use DETR for panoptic segmentation and plot the predictions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage - Object detection&lt;/h1&gt; &#xA;&lt;p&gt;There are no extra compiled components in DETR and package dependencies are minimal, so the code is very simple to use. We provide instructions how to install dependencies via conda. First, clone the repository locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/facebookresearch/detr.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install PyTorch 1.5+ and torchvision 0.6+:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c pytorch pytorch torchvision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install pycocotools (for evaluation on COCO) and scipy (for training):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install cython scipy&#xA;pip install -U &#39;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it, should be good to train and evaluate detection models.&lt;/p&gt; &#xA;&lt;p&gt;(optional) to work with panoptic install panopticapi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/cocodataset/panopticapi.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;p&gt;Download and extract COCO 2017 train and val images with annotations from &lt;a href=&#34;http://cocodataset.org/#download&#34;&gt;http://cocodataset.org&lt;/a&gt;. We expect the directory structure to be the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;path/to/coco/&#xA;  annotations/  # annotation json files&#xA;  train2017/    # train images&#xA;  val2017/      # val images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train baseline DETR on a single node with 8 gpus for 300 epochs run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A single epoch takes 28 minutes, so 300 epoch training takes around 6 days on a single machine with 8 V100 cards. To ease reproduction of our results we provide &lt;a href=&#34;https://gist.github.com/szagoruyko/b4c3b2c3627294fc369b899987385a3f&#34;&gt;results and training logs&lt;/a&gt; for 150 epoch schedule (3 days on a single machine), achieving 39.5/60.3 AP/AP50.&lt;/p&gt; &#xA;&lt;p&gt;We train DETR with AdamW setting learning rate in the transformer to 1e-4 and 1e-5 in the backbone. Horizontal flips, scales and crops are used for augmentation. Images are rescaled to have min size 800 and max size 1333. The transformer is trained with dropout of 0.1, and the whole model is trained with grad clip of 0.1.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To evaluate DETR R50 on COCO val5k with a single GPU run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --batch_size 2 --no_aux_loss --eval --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --coco_path /path/to/coco&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide results for all DETR detection models in this &lt;a href=&#34;https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918&#34;&gt;gist&lt;/a&gt;. Note that numbers vary depending on batch size (number of images) per GPU. Non-DC5 models were trained with batch size 2, and DC5 with 1, so DC5 models show a significant drop in AP if evaluated with more than 1 image per GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Multinode training&lt;/h2&gt; &#xA;&lt;p&gt;Distributed training is available via Slurm and &lt;a href=&#34;https://github.com/facebookincubator/submitit&#34;&gt;submitit&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install submitit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train baseline DETR-6-6 model on 4 nodes for 300 epochs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_with_submitit.py --timeout 3000 --coco_path /path/to/coco&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Usage - Segmentation&lt;/h1&gt; &#xA;&lt;p&gt;We show that it is relatively straightforward to extend DETR to predict segmentation masks. We mainly demonstrate strong panoptic segmentation results.&lt;/p&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;p&gt;For panoptic segmentation, you need the panoptic annotations additionally to the coco dataset (see above for the coco dataset). You need to download and extract the &lt;a href=&#34;http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip&#34;&gt;annotations&lt;/a&gt;. We expect the directory structure to be the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;path/to/coco_panoptic/&#xA;  annotations/  # annotation json files&#xA;  panoptic_train2017/    # train panoptic annotations&#xA;  panoptic_val2017/      # val panoptic annotations&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We recommend training segmentation in two stages: first train DETR to detect all the boxes, and then train the segmentation head. For panoptic segmentation, DETR must learn to detect boxes for both stuff and things classes. You can train it on a single node with 8 gpus for 300 epochs with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic --dataset_file coco_panoptic --output_dir /output/path/box_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance segmentation, you can simply train a normal box model (or used a pre-trained one we provide).&lt;/p&gt; &#xA;&lt;p&gt;Once you have a box model checkpoint, you need to freeze it, and train the segmentation head in isolation. For panoptic segmentation you can train on a single node with 8 gpus for 25 epochs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --masks --epochs 25 --lr_drop 15 --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic  --dataset_file coco_panoptic --frozen_weights /output/path/box_model/checkpoint.pth --output_dir /output/path/segm_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance segmentation only, simply remove the &lt;code&gt;dataset_file&lt;/code&gt; and &lt;code&gt;coco_panoptic_path&lt;/code&gt; arguments from the above command line.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;DETR is released under the Apache 2.0 license. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detr/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We actively welcome your pull requests! Please see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detr/main/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detr/main/.github/CODE_OF_CONDUCT.md&#34;&gt;CODE_OF_CONDUCT.md&lt;/a&gt; for more info.&lt;/p&gt;</summary>
  </entry>
</feed>