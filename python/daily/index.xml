<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-22T01:42:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>horizon3ai/CVE-2022-47966</title>
    <updated>2023-01-22T01:42:45Z</updated>
    <id>tag:github.com,2023-01-22:/horizon3ai/CVE-2022-47966</id>
    <link href="https://github.com/horizon3ai/CVE-2022-47966" rel="alternate"></link>
    <summary type="html">&lt;p&gt;POC for CVE-2022-47966 affecting multiple ManageEngine products&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2022-47966&lt;/h1&gt; &#xA;&lt;p&gt;POC for CVE-2022-47966 affecting the following ManageEngine products:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Access Manager Plus&lt;/li&gt; &#xA; &lt;li&gt;Active Directory 360&lt;/li&gt; &#xA; &lt;li&gt;ADAudit Plus&lt;/li&gt; &#xA; &lt;li&gt;ADManager Plus&lt;/li&gt; &#xA; &lt;li&gt;ADSelfService Plus&lt;/li&gt; &#xA; &lt;li&gt;Analytics Plus&lt;/li&gt; &#xA; &lt;li&gt;Application Control Plus&lt;/li&gt; &#xA; &lt;li&gt;Asset Explorer&lt;/li&gt; &#xA; &lt;li&gt;Browser Security Plus&lt;/li&gt; &#xA; &lt;li&gt;Device Control Plus&lt;/li&gt; &#xA; &lt;li&gt;Endpoint Central&lt;/li&gt; &#xA; &lt;li&gt;Endpoint Central MSP&lt;/li&gt; &#xA; &lt;li&gt;Endpoint DLP&lt;/li&gt; &#xA; &lt;li&gt;Key Manager Plus&lt;/li&gt; &#xA; &lt;li&gt;OS Deployer&lt;/li&gt; &#xA; &lt;li&gt;PAM 360&lt;/li&gt; &#xA; &lt;li&gt;Password Manager Pro&lt;/li&gt; &#xA; &lt;li&gt;Patch Manager Plus&lt;/li&gt; &#xA; &lt;li&gt;Remote Access Plus&lt;/li&gt; &#xA; &lt;li&gt;Remote Monitoring and Management (RMM)&lt;/li&gt; &#xA; &lt;li&gt;ServiceDesk Plus&lt;/li&gt; &#xA; &lt;li&gt;ServiceDesk Plus MSP&lt;/li&gt; &#xA; &lt;li&gt;SupportCenter Plus&lt;/li&gt; &#xA; &lt;li&gt;Vulnerability Manager Plus&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This specific POC only works on products utilizing Apache Santuario (xmlsec) &amp;lt;= 1.4.1 such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ServiceDesk Plus&lt;/li&gt; &#xA; &lt;li&gt;Endpoint Central&lt;/li&gt; &#xA; &lt;li&gt;ADManager Plus&lt;/li&gt; &#xA; &lt;li&gt;ADSelfService Plus&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other products may perform additional checks on the SAML response. Modifying this POC to work on products that perform additional checks involves:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scanning the logs of the vulnerable product for stack traces or additional logs message indicating an invalid SAML response.&lt;/li&gt; &#xA; &lt;li&gt;Reverse engineering the vulnerable product and searching for the code that implements the checks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Technical Analysis&lt;/h2&gt; &#xA;&lt;p&gt;A technical root cause analysis of the vulnerability can be found on our blog: &lt;a href=&#34;https://www.horizon3.ai/manageengine-cve-2022-47966-technical-deep-dive&#34;&gt;https://www.horizon3.ai/manageengine-cve-2022-47966-technical-deep-dive&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Original Researcher&#39;s Writeup&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/_l0gg&#34;&gt;Khoadha of Viettel Security&lt;/a&gt; documents his original research of this vulnerability and how it can be exploited across many versions of xmlsec: &lt;a href=&#34;https://blog.viettelcybersecurity.com/saml-show-stopper/&#34;&gt;https://blog.viettelcybersecurity.com/saml-show-stopper/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Indicators of Compromise&lt;/h2&gt; &#xA;&lt;p&gt;For analyzing ManageEngine logs for indicators of compromise check out our IOC blog: &lt;a href=&#34;https://www.horizon3.ai/manageengine-cve-2022-47966-iocs/&#34;&gt;https://www.horizon3.ai/manageengine-cve-2022-47966-iocs/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Summary&lt;/h2&gt; &#xA;&lt;p&gt;This POC abuses the pre-authentication remote code execution vulnerability to run a command with Java&#39;s Runtime.exec method.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;For Active Directory related products, such as ADManager, an issuer argument is required:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;root@kali:~# python3 ./CVE-2022-47966.py --url https://10.0.40.90:8443/samlLogin/&amp;lt;guid&amp;gt; --issuer https://sts.windows.net/&amp;lt;guid&amp;gt;/ --command notepad.exe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other products, a URL is all that is required:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;root@kali:~# python3 ./CVE-2022-47966.py --url https://10.0.40.64:8080/SamlResponseServlet --command notepad.exe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Mitigations&lt;/h2&gt; &#xA;&lt;p&gt;Update to the latest version of the affected product.&lt;/p&gt; &#xA;&lt;h2&gt;Follow the Horizon3.ai Attack Team on Twitter for the latest security research:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/Horizon3Attack&#34;&gt;Horizon3 Attack Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/JamesHorseman2&#34;&gt;James Horseman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/hacks_zach&#34;&gt;Zach Hanley&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This software has been created purely for the purposes of academic research and for the development of effective defensive techniques, and is not intended to be used to attack systems except where explicitly authorized. Project maintainers are not responsible or liable for misuse of the software. Use responsibly.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rougier/scientific-visualization-book</title>
    <updated>2023-01-22T01:42:45Z</updated>
    <id>tag:github.com,2023-01-22:/rougier/scientific-visualization-book</id>
    <link href="https://github.com/rougier/scientific-visualization-book" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open access book on scientific visualization using python and matplotlib&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Scientific Visualization: Python + Matplotlib&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nicolas P. Rougier, Bordeaux, November 2021.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/book.png&#34; width=&#34;25%&#34; alt=&#34;Front cover&#34; align=&#34;left&#34;&gt; &#xA;&lt;p&gt;The Python scientific visualisation landscape is huge. It is composed of a myriad of tools, ranging from the most versatile and widely used down to the more specialised and confidential. Some of these tools are community based while others are developed by companies. Some are made specifically for the web, others are for the desktop only, some deal with 3D and large data, while others target flawless 2D rendering. In this landscape, Matplotlib has a very special place. It is a versatile and powerful library that allows you to design very high quality figures, suitable for scientific publishing. It also offers a simple and intuitive interface as well as an object oriented architecture that allows you to tweak anything within a figure. Finally, it can be used as a regular graphic library in order to design non‐scientific figures. This book is organized into four parts. The first part considers the fundamental principles of the Matplotlib library. This includes reviewing the different parts that constitute a figure, the different coordinate systems, the available scales and projections, and we’ll also introduce a few concepts related to typography and colors. The second part is dedicated to the actual design of a figure. After introducing some simple rules for generating better figures, we’ll then go on to explain the Matplotlib defaults and styling system before diving on into figure layout organization. We’ll then explore the different types of plot available and see how a figure can be ornamented with different elements. The third part is dedicated to more advanced concepts, namely 3D figures, optimization &amp;amp; animation. The fourth and final part is a collection of showcases.&lt;/p&gt; &#xA;&lt;h3&gt;Read the book&lt;/h3&gt; &#xA;&lt;p&gt;You can read the book &lt;strong&gt;&lt;a href=&#34;https://hal.inria.fr/hal-03427242/document&#34;&gt;PDF&lt;/a&gt;&lt;/strong&gt; (95Mo, preferred site) that is open access and hosted on &lt;a href=&#34;https://hal.archives-ouvertes.fr/&#34;&gt;HAL&lt;/a&gt; which is a French open archive for academics. Up to date version is also available on GitHub &lt;a href=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/pdf/book.pdf&#34;&gt;here&lt;/a&gt;. Sources for the book (including code examples) are available at &lt;a href=&#34;https://github.com/rougier/scientific-visualization-book&#34;&gt;github.com/rougier/scientific-visualization-book&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Buy the book&lt;/h3&gt; &#xA;&lt;p&gt;If you want to buy the book, you can order a &lt;strong&gt;printed edition&lt;/strong&gt; at &lt;a href=&#34;https://www.amazon.com/dp/2957990105&#34;&gt;amazon.com&lt;/a&gt; for 49$. If you want to support or sponsor my future work on Python (and &lt;a href=&#34;https://github.com/rougier/nano-emacs&#34;&gt;Emacs&lt;/a&gt;), you can use &lt;a href=&#34;https://www.paypal.com/paypalme/NicolasPRougier/10&#34;&gt;paypal&lt;/a&gt;, &lt;a href=&#34;https://github.com/sponsors/rougier&#34;&gt;github&lt;/a&gt; or &lt;a href=&#34;https://en.liberapay.com/rougier/&#34;&gt;liberapay&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.paypal.com/paypalme/NicolasPRougier/5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-TIP_5$-yellow.svg?style=flat-square&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt; &lt;a href=&#34;https://www.paypal.com/paypalme/NicolasPRougier/10&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-TIP_10$-orange.svg?style=flat-square&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt; &lt;a href=&#34;https://www.paypal.com/paypalme/NicolasPRougier/25&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-TIP_25$-red.svg?style=flat-square&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/rougier/sponsorships?sponsor=rougier&amp;amp;tier_id=6981&amp;amp;preview=false&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-5$/Mo-yellow.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/rougier/sponsorships?sponsor=rougier&amp;amp;tier_id=11147&amp;amp;preview=false&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-10$/Mo-orange.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/rougier/sponsorships?sponsor=rougier&amp;amp;tier_id=108712&amp;amp;preview=false&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-25$/Mo-red.svg?style=flat-square&amp;amp;logo=github&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt; &lt;a href=&#34;https://en.liberapay.com/rougier/donate&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-PATRON/Week-green.svg?style=flat-square&amp;amp;logo=liberapay&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t want to spend money, you can simply &lt;a href=&#34;https://stars.github.com/nominate/&#34;&gt;nominate me&lt;/a&gt; for the GitHub stars program if you find my work useful for the community.&lt;/p&gt; &#xA;&lt;h3&gt;Build the book&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://labdmitriy.github.io/blog/building-scientific-visualization-book/&#34;&gt;Article&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/scripts/build_book/ubuntu.sh&#34;&gt;Script&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;See also&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.labri.fr/perso/nrougier/python-opengl/&#34;&gt;Python &amp;amp; OpenGL for Scientific Visualization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.labri.fr/perso/nrougier/from-python-to-numpy/&#34;&gt;From Python to Numpy&lt;/a&gt; (Scientific Python Volume I)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rougier/numpy-100&#34;&gt;100 Numpy exercices&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/matplotlib/cheatsheets&#34;&gt;Matplotlib cheat sheets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Book gallery&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/contour-dropshadow.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/domain-coloring.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/metropolis.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/zorder-plots.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/scales.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/histogram-pca.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/hatched-bars.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/platonic-solids.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/projection-3d-gaussian.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/polygon-clipping.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/multisample.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/typography-matters.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/scatter-3d.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/waterfall-3d.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/bunnies.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/polar-projection.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/recursive-voronoi.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/text-polar.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/spiral-pi.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/escher.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/radial-maze.png&#34; width=&#34;31%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rougier/scientific-visualization-book/master/images/text-shadow.png&#34; width=&#34;95%&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>timothybrooks/instruct-pix2pix</title>
    <updated>2023-01-22T01:42:45Z</updated>
    <id>tag:github.com,2023-01-22:/timothybrooks/instruct-pix2pix</id>
    <link href="https://github.com/timothybrooks/instruct-pix2pix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2211.09800&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;http://instruct-pix2pix.eecs.berkeley.edu/&#34;&gt;Data&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;PyTorch implementation of InstructPix2Pix, an instruction-based image editing model, based on the original &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;CompVis/stable_diffusion&lt;/a&gt; repo. &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix/&#34;&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.timothybrooks.com/&#34;&gt;Tim Brooks&lt;/a&gt;*, &lt;a href=&#34;https://holynski.org/&#34;&gt;Aleksander Holynski&lt;/a&gt;*, &lt;a href=&#34;https://people.eecs.berkeley.edu/~efros/&#34;&gt;Alexei A. Efros&lt;/a&gt; &lt;br&gt; UC Berkeley &lt;br&gt; *denotes equal contribution&lt;/p&gt; &#xA;&lt;img src=&#34;https://instruct-pix2pix.timothybrooks.com/teaser.jpg&#34;&gt; &#xA;&lt;h2&gt;TL;DR: quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Set up a conda environment, and download a pretrained model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate ip2p&#xA;bash scripts/download_checkpoints.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit a single image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python edit_cli.py --input imgs/example.jpg --output imgs/output.jpg --edit &#34;turn him into a cyborg&#34;&#xA;&#xA;# Optionally, you can specify parameters to tune your result:&#xA;# python edit_cli.py --steps 100 --resolution 512 --seed 1371 --cfg-text 7.5 --cfg-image 1.2 --input imgs/example.jpg --output imgs/output.jpg --edit &#34;turn him into a cyborg&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or launch your own interactive editing Gradio app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python edit_app.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/timothybrooks/instruct-pix2pix/raw/main/imgs/edit_app.jpg?raw=true&#34; alt=&#34;Edit app&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(For advice on how to get the best results by tuning parameters, see the &lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix#tips&#34;&gt;Tips&lt;/a&gt; section).&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Install all dependencies with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the pretrained models by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/download_checkpoints.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generated Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Our image editing model is trained on a generated dataset consisting of 454,445 examples. Each example contains (1) an input image, (2) an editing instruction, and (3) an output edited image. We provide two versions of the dataset, one in which each pair of edited images is generated 100 times, and the best examples are chosen based on CLIP metrics (Section 3.1.2 in the paper) (&lt;code&gt;clip-filtered-dataset&lt;/code&gt;), and one in which examples are randomly chosen (&lt;code&gt;random-sample-dataset&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;For the released version of this dataset, we&#39;ve additionally filtered prompts and images for NSFW content. After NSFW filtering, the GPT-3 generated dataset contains 451,990 examples. The final image-pair datasets contain:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;# of image editing examples&lt;/th&gt; &#xA;   &lt;th&gt;Dataset size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;random-sample-dataset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;451990&lt;/td&gt; &#xA;   &lt;td&gt;727GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;clip-filtered-dataset&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;313010&lt;/td&gt; &#xA;   &lt;td&gt;436GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To download one of these datasets, along with the entire NSFW-filtered text data, run the following command with the appropriate dataset name:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/download_data.sh clip-filtered-dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training InstructPix2Pix&lt;/h2&gt; &#xA;&lt;p&gt;InstructPix2Pix is trained by fine-tuning from an initial StableDiffusion checkpoint. The first step is to download a Stable Diffusion checkpoint. For our trained models, we used the v1.5 checkpoint as the starting point. To download the same ones we used, you can run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/download_pretrained_sd.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to use a different checkpoint, point to it in the config file &lt;code&gt;configs/train.yaml&lt;/code&gt;, on line 8, after &lt;code&gt;ckpt_path:&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Next, we need to change the config to point to our downloaded (or generated) dataset. If you&#39;re using the &lt;code&gt;clip-filtered-dataset&lt;/code&gt; from above, you can skip this. Otherwise, you may need to edit lines 85 and 94 of the config (&lt;code&gt;data.params.train.params.path&lt;/code&gt;, &lt;code&gt;data.params.validation.params.path&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Finally, start a training job with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --name default --base configs/train.yaml --train --gpus 0,1,2,3,4,5,6,7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Creating your own dataset&lt;/h2&gt; &#xA;&lt;p&gt;Our generated dataset of paired images and editing instructions is made in two phases: First, we use GPT-3 to generate text triplets: (a) a caption describing an image, (b) an edit instruction, (c) a caption describing the image after the edit. Then, we turn pairs of captions (before/after the edit) into pairs of images using Stable Diffusion and Prompt-to-Prompt.&lt;/p&gt; &#xA;&lt;h3&gt;(1) Generate a dataset of captions and instructions&lt;/h3&gt; &#xA;&lt;p&gt;We provide our generated dataset of captions and edit instructions &lt;a href=&#34;https://instruct-pix2pix.eecs.berkeley.edu/gpt-generated-prompts.jsonl&#34;&gt;here&lt;/a&gt;. If you plan to use our captions+instructions, skip to step (2). Otherwise, if you would like to create your own text dataset, please follow steps (1.1-1.3) below. Note that generating very large datasets using GPT-3 can be expensive.&lt;/p&gt; &#xA;&lt;h4&gt;(1.1) Manually write a dataset of instructions and captions&lt;/h4&gt; &#xA;&lt;p&gt;The first step of the process is fine-tuning GPT-3. To do this, we made a dataset of 700 examples broadly covering of edits that we might want our model to be able to perform. Our examples are available &lt;a href=&#34;https://instruct-pix2pix.eecs.berkeley.edu/human-written-prompts.jsonl&#34;&gt;here&lt;/a&gt;. These should be diverse and cover a wide range of possible captions and types of edits. Ideally, they should avoid duplication or significant overlap of captions and instructions. It is also important to be mindful of limitations of Stable Diffusion and Prompt-to-Prompt in writing these examples, such as inability to perform large spatial transformations (e.g., moving the camera, zooming in, swapping object locations).&lt;/p&gt; &#xA;&lt;p&gt;Input prompts should closely match the distribution of input prompts used to generate the larger dataset. We sampled the 700 input prompts from the &lt;em&gt;LAION Improved Aesthetics 6.5+&lt;/em&gt; dataset and also use this dataset for generating examples. We found this dataset is quite noisy (many of the captions are overly long and contain irrelevant text). For this reason, we also considered MSCOCO and LAION-COCO datasets, but ultimately chose &lt;em&gt;LAION Improved Aesthetics 6.5+&lt;/em&gt; due to its diversity of content, proper nouns, and artistic mediums. If you choose to use another dataset or combination of datasets as input to GPT-3 when generating examples, we recommend you sample the input prompts from the same distribution when manually writing training examples.&lt;/p&gt; &#xA;&lt;h4&gt;(1.2) Finetune GPT-3&lt;/h4&gt; &#xA;&lt;p&gt;The next step is to finetune a large language model on the manually written instructions/outputs to generate edit instructions and edited caption from a new input caption. For this, we finetune GPT-3&#39;s Davinci model via the OpenAI API, although other language models could be used.&lt;/p&gt; &#xA;&lt;p&gt;To prepare training data for GPT-3, one must first create an OpenAI developer account to access the needed APIs, and &lt;a href=&#34;https://beta.openai.com/docs/api-reference/introduction&#34;&gt;set up the API keys on your local device&lt;/a&gt;. Also, run the &lt;code&gt;prompts/prepare_for_gpt.py&lt;/code&gt; script, which forms the prompts into the correct format by concatenating instructions and captions and adding delimiters and stop sequences.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python dataset_creation/prepare_for_gpt.py --input-path data/human-written-prompts.jsonl --output-path data/human-written-prompts-for-gpt.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, finetune GPT-3 via the OpenAI CLI. We provide an example below, although please refer to OpenAI&#39;s official documentation for this, as best practices may change. We trained the Davinci model for a single epoch. You can experiment with smaller less expensive GPT-3 variants or with open source language models, although this may negatively affect performance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openai api fine_tunes.create -t data/human-written-prompts-for-gpt.jsonl -m davinci --n_epochs 1 --suffix &#34;instruct-pix2pix&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can test out the finetuned GPT-3 model by launching the provided Gradio app:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python prompt_app.py --openai-api-key OPENAI_KEY --openai-model OPENAI_MODEL_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/timothybrooks/instruct-pix2pix/raw/main/imgs/prompt_app.jpg?raw=true&#34; alt=&#34;Prompt app&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;(1.3) Generate a large dataset of captions and instructions&lt;/h4&gt; &#xA;&lt;p&gt;We now use the finetuned GPT-3 model to generate a large dataset. Our dataset cost thousands of dollars to create. See &lt;code&gt;prompts/gen_instructions_and_captions.py&lt;/code&gt; for the script which generates these examples. We recommend first generating a small number of examples (by setting a low value of &lt;code&gt;--num-samples&lt;/code&gt;) and gradually increasing the scale to ensure the results are working as desired before increasing scale.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python dataset_creation/generate_txt_dataset.py --openai-api-key OPENAI_KEY --openai-model OPENAI_MODEL_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are generating at a very large scale (e.g., 100K+), it will be noteably faster to generate the dataset with multiple processes running in parallel. This can be accomplished by setting &lt;code&gt;--partitions=N&lt;/code&gt; to a higher number and running multiple processes, setting each &lt;code&gt;--partition&lt;/code&gt; to the corresponding value.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python dataset_creation/generate_txt_dataset.py --openai-api-key OPENAI_KEY --openai-model OPENAI_MODEL_NAME --partitions=10 --partition=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;(2) Turn paired captions into paired images&lt;/h3&gt; &#xA;&lt;p&gt;The next step is to turn pairs of text captions into pairs of images. For this, we need to copy some pre-trained Stable Diffusion checkpoints to &lt;code&gt;stable_diffusion/models/ldm/stable-diffusion-v1/&lt;/code&gt;. You may have already done this if you followed the instructions above for training with our provided data, but if not, you can do this by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/download_pretrained_sd.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For our model, we used &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt&#34;&gt;checkpoint v1.5&lt;/a&gt;, and the &lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt&#34;&gt;new autoencoder&lt;/a&gt;, but other models may work as well. If you choose to use other models, make sure to change point to the corresponding checkpoints by passing in the &lt;code&gt;--ckpt&lt;/code&gt; and &lt;code&gt;--vae-ckpt&lt;/code&gt; arguments. Once all checkpoints have been downloaded, we can generate the dataset with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python dataset_creation/generate_img_dataset.py --out_dir data/instruct-pix2pix-dataset-000 --prompts_file path/to/generated_prompts.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command operates on a single GPU (typically a V100 or A100). To parallelize over many GPUs/machines, set &lt;code&gt;--n-partitions&lt;/code&gt; to the total number of parallel jobs and &lt;code&gt;--partition&lt;/code&gt; to the index of each job.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python dataset_creation/generate_img_dataset.py --out_dir data/instruct-pix2pix-dataset-000 --prompts_file path/to/generated_prompts.jsonl --n-partitions 100 --partition 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default parameters match that of our dataset, although in practice you can use a smaller number of steps (e.g., &lt;code&gt;--steps=25&lt;/code&gt;) to generate high quality data faster. By default, we generate 100 samples per prompt and use CLIP filtering to keep a max of 4 per prompt. You can experiment with fewer samples by setting &lt;code&gt;--n-samples&lt;/code&gt;. The command below turns off CLIP filtering entirely and is therefore faster:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python dataset_creation/generate_img_dataset.py --out_dir data/instruct-pix2pix-dataset-000 --prompts_file path/to/generated_prompts.jsonl --n-samples 4 --clip-threshold 0 --clip-dir-threshold 0 --clip-img-threshold 0 --n-partitions 100 --partition 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After generating all of the dataset examples, run the following command below to create a list of the examples. This is needed for the dataset onject to efficiently be able to sample examples without needing to iterate over the entire dataset directory at the start of each training run.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python dataset_creation/prepare_dataset.py data/instruct-pix2pix-dataset-000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To generate plots like the ones in Figures 8 and 10 in the paper, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python metrics/compute_metrics.py --ckpt /path/to/your/model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re not getting the quality result you want, there may be a few reasons:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Is the image not changing enough?&lt;/strong&gt; Your Image CFG weight may be too high. This value dictates how similar the output should be to the input. It&#39;s possible your edit requires larger changes from the original image, and your Image CFG weight isn&#39;t allowing that. Alternatively, your Text CFG weight may be too low. This value dictates how much to listen to the text instruction. The default Image CFG of 1.5 and Text CFG of 7.5 are a good starting point, but aren&#39;t necessarily optimal for each edit. Try: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Decreasing the Image CFG weight, or&lt;/li&gt; &#xA;   &lt;li&gt;Incerasing the Text CFG weight, or&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Conversely, &lt;strong&gt;is the image changing too much&lt;/strong&gt;, such that the details in the original image aren&#39;t preserved? Try: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Increasing the Image CFG weight, or&lt;/li&gt; &#xA;   &lt;li&gt;Decreasing the Text CFG weight&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Try generating results with different random seeds by setting &#34;Randomize Seed&#34; and running generation multiple times. You can also try setting &#34;Randomize CFG&#34; to sample new Text CFG and Image CFG values each time.&lt;/li&gt; &#xA; &lt;li&gt;Rephrasing the instruction sometimes improves results (e.g., &#34;turn him into a dog&#34; vs. &#34;make him a dog&#34; vs. &#34;as a dog&#34;).&lt;/li&gt; &#xA; &lt;li&gt;Increasing the number of steps sometimes improves results.&lt;/li&gt; &#xA; &lt;li&gt;Do faces look weird? The Stable Diffusion autoencoder has a hard time with faces that are small in the image. Try cropping the image so the face takes up a larger portion of the frame.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our codebase is based on the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion codebase&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{brooks2022instructpix2pix,&#xA;  title={InstructPix2Pix: Learning to Follow Image Editing Instructions},&#xA;  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},&#xA;  journal={arXiv preprint arXiv:2211.09800},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>