<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-24T01:40:16Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cumulo-autumn/StreamDiffusion</title>
    <updated>2023-12-24T01:40:16Z</updated>
    <id>tag:github.com,2023-12-24:/cumulo-autumn/StreamDiffusion</id>
    <link href="https://github.com/cumulo-autumn/StreamDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StreamDiffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/README-ja.md&#34;&gt;日本語&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_07.gif&#34; width=&#34;90%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_09.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/akio-kodaira-1a7b98252/&#34;&gt;Akio Kodaira&lt;/a&gt;, &lt;a href=&#34;https://www.chenfengx.com/&#34;&gt;Chenfeng Xu&lt;/a&gt;, Toshiki Hazama, &lt;a href=&#34;https://twitter.com/__ramu0e__&#34;&gt;Takanori Yoshimoto&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kohei--ohno/&#34;&gt;Kohei Ohno&lt;/a&gt;, &lt;a href=&#34;https://me.ddpn.world/&#34;&gt;Shogo Mitsuhori&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/toni_nimono&#34;&gt;Soichi Sugano&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/hanyingcl&#34;&gt;Hanying Cho&lt;/a&gt;, &lt;a href=&#34;https://zhijianliu.com/&#34;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=ID9QePIAAAAJ&#34;&gt;Kurt Keutzer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;StreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation techniques.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.12491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2307.04725-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2312.12491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-papers-yellow&#34; alt=&#34;Hugging Face Papers&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We sincerely thank &lt;a href=&#34;https://twitter.com/AttaQjp&#34;&gt;Taku Fujimoto&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/radamar&#34;&gt;Radamés Ajna&lt;/a&gt; and Hugging Face team for their invaluable feedback, courteous support, and insightful discussions.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stream Batch&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Streamlined data processing through efficient batch operations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Residual Classifier-Free Guidance&lt;/strong&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/#residual-cfg-rcfg&#34;&gt;Learn More&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Improved guidance mechanism that minimizes computational redundancy.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stochastic Similarity Filter&lt;/strong&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/#stochastic-similarity-filter&#34;&gt;Learn More&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Improves GPU utilization efficiency through advanced filtering techniques.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;IO Queues&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Efficiently manages input and output operations for smoother execution.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-Computation for KV-Caches&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optimizes caching strategies for accelerated processing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Acceleration Tools&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Utilizes various tools for model optimization and performance boost.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When images are produced using our proposed StreamDiffusion pipeline in an environment with &lt;strong&gt;GPU: RTX 4090&lt;/strong&gt;, &lt;strong&gt;CPU: Core i9-13900K&lt;/strong&gt;, and &lt;strong&gt;OS: Ubuntu 22.04.3 LTS&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Denoising Step&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;fps on Txt2Img&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;fps on Img2Img&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SD-turbo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;106.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.897&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LCM-LoRA &lt;br&gt;+&lt;br&gt; KohakuV2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.023&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.133&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to explore each feature by following the provided links to learn more about StreamDiffusion&#39;s capabilities. If you find it helpful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{kodaira2023streamdiffusion,&#xA;      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},&#xA;      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},&#xA;      year={2023},&#xA;      eprint={2312.12491},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Step0: clone this repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/cumulo-autumn/StreamDiffusion.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step1: Make Environment&lt;/h3&gt; &#xA;&lt;p&gt;You can install StreamDiffusion via pip, conda, or Docker(explanation below).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n streamdiffusion python=3.10&#xA;conda activate streamdiffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OR&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;python -m venv .venv&#xA;# Windows&#xA;.\.venv\Scripts\activate&#xA;# Linux&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step2: Install PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;Select the appropriate version for your system.&lt;/p&gt; &#xA;&lt;p&gt;CUDA 11.8&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA 12.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;details: &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step3: Install StreamDiffusion&lt;/h3&gt; &#xA;&lt;h4&gt;For User&lt;/h4&gt; &#xA;&lt;p&gt;Install StreamDiffusion&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#for Latest Version (recommended)&#xA;pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]&#xA;&#xA;&#xA;#or&#xA;&#xA;&#xA;#for Stable Version&#xA;pip install streamdiffusion[tensorrt]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install TensorRT extension and pywin32 (※※pywin32 is required only for Windows.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m streamdiffusion.tools.install-tensorrt&#xA;# If you use Windows, you need to install pywin32 &#xA;pip install pywin32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For Developer&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python setup.py develop easy_install streamdiffusion[tensorrt]&#xA;python -m streamdiffusion.tools.install-tensorrt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Installation (TensorRT Ready)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/cumulo-autumn/StreamDiffusion.git&#xA;cd StreamDiffusion&#xA;docker build -t stream-diffusion:latest -f Dockerfile .&#xA;docker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You can try StreamDiffusion in &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_02.gif&#34; alt=&#34;画像3&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_03.gif&#34; alt=&#34;画像4&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_04.gif&#34; alt=&#34;画像5&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_05.gif&#34; alt=&#34;画像6&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Real-Time Txt2Img Demo&lt;/h2&gt; &#xA;&lt;p&gt;There is an interactive txt2img demo in &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/demo/realtime-txt2img&#34;&gt;&lt;code&gt;demo/realtime-txt2img&lt;/code&gt;&lt;/a&gt; directory!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_01.gif&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Usage Example&lt;/h2&gt; &#xA;&lt;p&gt;We provide a simple example of how to use StreamDiffusion. For more detailed examples, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Image-to-Image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import AutoencoderTiny, StableDiffusionPipeline&#xA;from diffusers.utils import load_image&#xA;&#xA;from streamdiffusion import StreamDiffusion&#xA;from streamdiffusion.image_utils import postprocess_image&#xA;&#xA;# You can load any models using diffuser&#39;s StableDiffusionPipeline&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#34;KBlueLeaf/kohaku-v2.1&#34;).to(&#xA;    device=torch.device(&#34;cuda&#34;),&#xA;    dtype=torch.float16,&#xA;)&#xA;&#xA;# Wrap the pipeline in StreamDiffusion&#xA;stream = StreamDiffusion(&#xA;    pipe,&#xA;    t_index_list=[32, 45],&#xA;    torch_dtype=torch.float16,&#xA;)&#xA;&#xA;# If the loaded model is not LCM, merge LCM&#xA;stream.load_lcm_lora()&#xA;stream.fuse_lora()&#xA;# Use Tiny VAE for further acceleration&#xA;stream.vae = AutoencoderTiny.from_pretrained(&#34;madebyollin/taesd&#34;).to(device=pipe.device, dtype=pipe.dtype)&#xA;# Enable acceleration&#xA;pipe.enable_xformers_memory_efficient_attention()&#xA;&#xA;&#xA;prompt = &#34;1girl with dog hair, thick frame glasses&#34;&#xA;# Prepare the stream&#xA;stream.prepare(prompt)&#xA;&#xA;# Prepare image&#xA;init_image = load_image(&#34;assets/img2img_example.png&#34;).resize((512, 512))&#xA;&#xA;# Warmup &amp;gt;= len(t_index_list) x frame_buffer_size&#xA;for _ in range(2):&#xA;    stream(init_image)&#xA;&#xA;# Run the stream infinitely&#xA;while True:&#xA;    x_output = stream(init_image)&#xA;    postprocess_image(x_output, output_type=&#34;pil&#34;)[0].show()&#xA;    input_response = input(&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;)&#xA;    if input_response == &#34;stop&#34;:&#xA;        break&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text-to-Image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import AutoencoderTiny, StableDiffusionPipeline&#xA;&#xA;from streamdiffusion import StreamDiffusion&#xA;from streamdiffusion.image_utils import postprocess_image&#xA;&#xA;# You can load any models using diffuser&#39;s StableDiffusionPipeline&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#34;KBlueLeaf/kohaku-v2.1&#34;).to(&#xA;    device=torch.device(&#34;cuda&#34;),&#xA;    dtype=torch.float16,&#xA;)&#xA;&#xA;# Wrap the pipeline in StreamDiffusion&#xA;# Requires more long steps (len(t_index_list)) in text2image&#xA;# You recommend to use cfg_type=&#34;none&#34; when text2image&#xA;stream = StreamDiffusion(&#xA;    pipe,&#xA;    t_index_list=[0, 16, 32, 45],&#xA;    torch_dtype=torch.float16,&#xA;    cfg_type=&#34;none&#34;,&#xA;)&#xA;&#xA;# If the loaded model is not LCM, merge LCM&#xA;stream.load_lcm_lora()&#xA;stream.fuse_lora()&#xA;# Use Tiny VAE for further acceleration&#xA;stream.vae = AutoencoderTiny.from_pretrained(&#34;madebyollin/taesd&#34;).to(device=pipe.device, dtype=pipe.dtype)&#xA;# Enable acceleration&#xA;pipe.enable_xformers_memory_efficient_attention()&#xA;&#xA;&#xA;prompt = &#34;1girl with dog hair, thick frame glasses&#34;&#xA;# Prepare the stream&#xA;stream.prepare(prompt)&#xA;&#xA;# Warmup &amp;gt;= len(t_index_list) x frame_buffer_size&#xA;for _ in range(4):&#xA;    stream()&#xA;&#xA;# Run the stream infinitely&#xA;while True:&#xA;    x_output = stream.txt2img()&#xA;    postprocess_image(x_output, output_type=&#34;pil&#34;)[0].show()&#xA;    input_response = input(&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;)&#xA;    if input_response == &#34;stop&#34;:&#xA;        break&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can make it faster by using SD-Turbo.&lt;/p&gt; &#xA;&lt;h3&gt;Faster generation&lt;/h3&gt; &#xA;&lt;p&gt;Replace the following code in the above example.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe.enable_xformers_memory_efficient_attention()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt&#xA;&#xA;stream = accelerate_with_tensorrt(&#xA;    stream, &#34;engines&#34;, max_batch_size=2,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It requires TensorRT extension and time to build the engine, but it will be faster than the above example.&lt;/p&gt; &#xA;&lt;h2&gt;Optionals&lt;/h2&gt; &#xA;&lt;h3&gt;Stochastic Similarity Filter&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_06.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stochastic Similarity Filter reduces processing during video input by minimizing conversion operations when there is little change from the previous frame, thereby alleviating GPU processing load, as shown by the red frame in the above GIF. The usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stream = StreamDiffusion(&#xA;    pipe,&#xA;    [32, 45],&#xA;    torch_dtype=torch.float16,&#xA;)&#xA;stream.enable_similar_image_filter(&#xA;    similar_image_filter_threshold,&#xA;    similar_image_filter_max_skip_frame,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are the following parameters that can be set as arguments in the function:&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;similar_image_filter_threshold&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The threshold for similarity between the previous frame and the current frame before the processing is paused.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;code&gt;similar_image_filter_max_skip_frame&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The maximum interval during the pause before resuming the conversion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Residual CFG (RCFG)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/cfg_conparision.png&#34; alt=&#34;rcfg&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;RCFG is a method for approximately realizing CFG with competitive computational complexity compared to cases where CFG is not used. It can be specified through the cfg_type argument in the StreamDiffusion. There are two types of RCFG: one with no specified items for negative prompts RCFG Self-Negative and one where negative prompts can be specified RCFG Onetime-Negative. In terms of computational complexity, denoting the complexity without CFG as N and the complexity with a regular CFG as 2N, RCFG Self-Negative can be computed in N steps, while RCFG Onetime-Negative can be computed in N+1 steps.&lt;/p&gt; &#xA;&lt;p&gt;The usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# w/0 CFG&#xA;cfg_type = &#34;none&#34;&#xA;# CFG&#xA;cfg_type = &#34;full&#34;&#xA;# RCFG Self-Negative&#xA;cfg_type = &#34;self&#34;&#xA;# RCFG Onetime-Negative&#xA;cfg_type = &#34;initialize&#34;&#xA;stream = StreamDiffusion(&#xA;    pipe,&#xA;    [32, 45],&#xA;    torch_dtype=torch.float16,&#xA;    cfg_type=cfg_type,&#xA;)&#xA;stream.prepare(&#xA;    prompt=&#34;1girl, purple hair&#34;,&#xA;    guidance_scale=guidance_scale,&#xA;    delta=delta,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The delta has a moderating effect on the effectiveness of RCFG.&lt;/p&gt; &#xA;&lt;h2&gt;Development Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/cumulo_autumn&#34;&gt;Aki&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/AttaQjp&#34;&gt;Ararat&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/Chenfeng_X&#34;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ddPn08&#34;&gt;ddPn08&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ArtengMimi&#34;&gt;kizamimi&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/__ramu0e__&#34;&gt;ramune&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/hanyingcl&#34;&gt;teftef&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/toni_nimono&#34;&gt;Tonimono&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/IMG_5955&#34;&gt;Verb&lt;/a&gt;,&lt;/p&gt; &#xA;&lt;p&gt;(*alphabetical order) &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The video and image demos in this GitHub repository were generated using &lt;a href=&#34;https://huggingface.co/latent-consistency/lcm-lora-sdv1-5&#34;&gt;LCM-LoRA&lt;/a&gt; + &lt;a href=&#34;https://civitai.com/models/136268/kohaku-v2&#34;&gt;KohakuV2&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2311.17042&#34;&gt;SD-Turbo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://latent-consistency-models.github.io/&#34;&gt;LCM-LoRA authors&lt;/a&gt; for providing the LCM-LoRA and Kohaku BlueLeaf (&lt;a href=&#34;https://twitter.com/KBlueleaf&#34;&gt;@KBlueleaf&lt;/a&gt;) for providing the KohakuV2 model and ,to &lt;a href=&#34;https://ja.stability.ai/&#34;&gt;Stability AI&lt;/a&gt; for &lt;a href=&#34;https://arxiv.org/abs/2311.17042&#34;&gt;SD-Turbo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;KohakuV2 Models can be downloaded from &lt;a href=&#34;https://civitai.com/models/136268/kohaku-v2&#34;&gt;Civitai&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/KBlueLeaf/kohaku-v2.1&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SD-Turbo is also available on &lt;a href=&#34;https://huggingface.co/stabilityai/sd-turbo&#34;&gt;Hugging Face Space&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>szymanowiczs/splatter-image</title>
    <updated>2023-12-24T01:40:16Z</updated>
    <id>tag:github.com,2023-12-24:/szymanowiczs/splatter-image</id>
    <link href="https://github.com/szymanowiczs/splatter-image" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of `Splatter Image: Ultra-Fast Single-View 3D Reconstruction&#39;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;splatter-image&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation of `Splatter Image: Ultra-Fast Single-View 3D Reconstruction&#39;&lt;/p&gt; &#xA;&lt;h1&gt;Using this repository&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a conda environment and install requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name splatter-image&#xA;conda activate splatter-image&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Gaussian Splatting renderer, i.e. the library for rendering a Gaussian Point cloud to an image. To do so, pull the &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting/tree/main&#34;&gt;Gaussian Splatting repository&lt;/a&gt; and, with your conda environment activated, run &lt;code&gt;pip install submodules/diff-gaussian-rasterization&lt;/code&gt;. You will need to meet the &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting/raw/main/README.md#hardware-requirements&#34;&gt;hardware and software requirements&lt;/a&gt;. We did all our experimentation on an NVIDIA A6000 GPU and speed measurements on an NVIDIA V100 GPU.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want to train on CO3D data you will need to install Pytorch3D. See instructions &lt;a href=&#34;https://github.com/facebookresearch/pytorch3d/raw/main/INSTALL.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For training / evaluating on ShapeNet-SRN follow instructions from &lt;a href=&#34;https://github.com/sxyu/pixel-nerf#getting-the-data&#34;&gt;PixelNeRF&lt;/a&gt; and change &lt;code&gt;SHAPENET_DATASET_ROOT&lt;/code&gt; in &lt;code&gt;scene/srn.py&lt;/code&gt; to your download directory. No additional prepreocessing is needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For training / evaluating on CO3D download the hydrant and teddybear classes from the &lt;a href=&#34;https://ai.meta.com/datasets/co3d-downloads/&#34;&gt;CO3D release&lt;/a&gt;. Next, set &lt;code&gt;CO3D_RAW_ROOT&lt;/code&gt; to your download directory in &lt;code&gt;data_preprocessing/preoprocess_co3d.py&lt;/code&gt;. Set &lt;code&gt;CO3D_OUT_ROOT&lt;/code&gt; to where you want to store preprocessed data. Run &lt;code&gt;python data_preprocessing/preprocess_co3d.py&lt;/code&gt; and set &lt;code&gt;CO3D_DATASET_ROOT:=CO3D_OUT_ROOT&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;p&gt;Pretrained models will be released in early 2024!&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Single-view models can be trained with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_network.py +dataset=[cars,chairs,hydrants,teddybears]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train a 2-view model run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_network.py +dataset=cars cam_embd=pose_pos data.input_images=2 opt.imgs_per_obj=5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Once a model is trained evaluation can be run with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python eval.py [model directory path]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To save renders modify variable &lt;code&gt;save_vis&lt;/code&gt; and &lt;code&gt;out_folder&lt;/code&gt; in eval.py.&lt;/p&gt; &#xA;&lt;h2&gt;Code structure&lt;/h2&gt; &#xA;&lt;p&gt;Training loop is implemented in &lt;code&gt;train_network.py&lt;/code&gt; and evaluation code is in &lt;code&gt;eval.py&lt;/code&gt;. Datasets are implemented in &lt;code&gt;scene/srn.py&lt;/code&gt; and &lt;code&gt;scene/co3d.py&lt;/code&gt;. Model is implemented in &lt;code&gt;scene/gaussian_predictor.py&lt;/code&gt;. The call to renderer can be found in &lt;code&gt;gaussian_renderer/__init__.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Camera conventions&lt;/h2&gt; &#xA;&lt;p&gt;Gaussian rasterizer assumes row-major order of rigid body transform matrices, i.e. that position vectors are row vectors. It also requires cameras in the COLMAP / OpenCV convention, i.e., that x points right, y down, and z away from the camera (forward).&lt;/p&gt; &#xA;&lt;h1&gt;BibTeX&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{szymanowicz23splatter,&#xA;      title={Splatter Image: Ultra-Fast Single-View 3D Reconstruction},&#xA;      author={Stanislaw Szymanowicz and Christian Rupprecht and Andrea Vedaldi},&#xA;      year={2023},&#xA;      booktitle={arXiv},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;S. Szymanowicz is supported by an EPSRC Doctoral Training Partnerships Scholarship (DTP) EP/R513295/1 and the Oxford-Ashton Scholarship. A. Vedaldi is supported by ERC-CoG UNION 101001212. We thank Eldar Insafutdinov for his help with installation requirements.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>joaomdmoura/crewAI</title>
    <updated>2023-12-24T01:40:16Z</updated>
    <id>tag:github.com,2023-12-24:/joaomdmoura/crewAI</id>
    <link href="https://github.com/joaomdmoura/crewAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;crewAI&lt;/h1&gt; &#xA;&lt;p&gt;🤖 Cutting-edge framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#why-crewai&#34;&gt;Why CrewAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#local-open-source-models&#34;&gt;Local Open Source Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#how-crewai-compares&#34;&gt;CrewAI x AutoGen x ChatDev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why CrewAI?&lt;/h2&gt; &#xA;&lt;p&gt;The power of AI collaboration has too much to offer. CrewAI is designed to enable AI agents to assume roles, share goals, and operate in a cohesive unit - much like a well-oiled crew. Whether you&#39;re building a smart assistant platform, an automated customer service ensemble, or a multi-agent research team, CrewAI provides the backbone for sophisticated multi-agent interactions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🤖 &lt;a href=&#34;https://chat.openai.com/g/g-qqTuUWsBY-crewai-assistant&#34;&gt;Talk with the Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📄 &lt;a href=&#34;https://github.com/joaomdmoura/CrewAI/wiki&#34;&gt;Documention Wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Installation&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install crewai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Setting Up Your Crew&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from crewai import Agent, Task, Crew, Process&#xA;&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;Your Key&#34;&#xA;&#xA;# Define your agents with roles and goals&#xA;researcher = Agent(&#xA;  role=&#39;Researcher&#39;,&#xA;  goal=&#39;Discover new insights&#39;,&#xA;  backstory=&#34;You&#39;re a world class researcher working on a major data science company&#34;,&#xA;  verbose=True,&#xA;  allow_delegation=False&#xA;  # llm=OpenAI(temperature=0.7, model_name=&#34;gpt-4&#34;). It uses langchain.chat_models, default is GPT4 &#xA;)&#xA;writer = Agent(&#xA;  role=&#39;Writer&#39;,&#xA;  goal=&#39;Create engaging content&#39;,&#xA;  backstory=&#34;You&#39;re a famous technical writer, specialized on writing data related content&#34;,&#xA;  verbose=True,&#xA;  allow_delegation=False&#xA;)&#xA;&#xA;# Create tasks for your agents&#xA;task1 = Task(description=&#39;Investigate the latest AI trends&#39;, agent=researcher)&#xA;task2 = Task(description=&#39;Write a blog post on AI advancements&#39;, agent=writer)&#xA;&#xA;# Instantiate your crew with a sequential process&#xA;crew = Crew(&#xA;  agents=[researcher, writer],&#xA;  tasks=[task1, task2],&#xA;  verbose=True, # Crew verbose more will let you know what tasks are being worked on&#xA;  process=Process.sequential # Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.&#xA;)&#xA;&#xA;# Get your crew to work!&#xA;result = crew.kickoff()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently the only supported process is &lt;code&gt;Process.sequential&lt;/code&gt;, where one task is executed after the other and the outcome of one is passed as extra content into this next.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Role-Based Agent Design&lt;/strong&gt;: Customize agents with specific roles, goals, and tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Autonomous Inter-Agent Delegation&lt;/strong&gt;: Agents can autonomously delegate tasks and inquire amongst themselves, enhancing problem-solving efficiency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Task Management&lt;/strong&gt;: Define tasks with customizable tools and assign them to agents dynamically.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Processes Driven&lt;/strong&gt;: Currently only supports &lt;code&gt;sequential&lt;/code&gt; task execution but more complex processes like consensual and hierarchical being worked on.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/joaomdmoura/crewAI/main/crewAI-mindmap.png&#34; alt=&#34;CrewAI Mind Map&#34; title=&#34;CrewAI Mind Map&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;You can test different real life examples of AI crews &lt;a href=&#34;https://github.com/joaomdmoura/crewAI-examples?tab=readme-ov-file&#34;&gt;in the examples repo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Local Open Source Models&lt;/h2&gt; &#xA;&lt;p&gt;crewAI supports integration with local models, thorugh tools such as &lt;a href=&#34;https://ollama.ai/&#34;&gt;Ollama&lt;/a&gt;, for enhanced flexibility and customization. This allows you to utilize your own models, which can be particularly useful for specialized tasks or data privacy concerns.&lt;/p&gt; &#xA;&lt;h3&gt;Setting Up Ollama&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install Ollama&lt;/strong&gt;: Ensure that Ollama is properly installed in your environment. Follow the installation guide provided by Ollama for detailed instructions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configure Ollama&lt;/strong&gt;: Set up Ollama to work with your local model. You will probably need to &lt;a href=&#34;https://github.com/jmorganca/ollama/raw/main/docs/modelfile.md&#34;&gt;tweak the model using a Modelfile&lt;/a&gt;. I&#39;d recommend adding &lt;code&gt;Observation&lt;/code&gt; as a stop word and playing with &lt;code&gt;top_p&lt;/code&gt; and &lt;code&gt;temperature&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Integrating Ollama with CrewAI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instantiate Ollama Model: Create an instance of the Ollama model. You can specify the model and the base URL during instantiation. For example:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain.llms import Ollama&#xA;ollama_openhermes = Ollama(model=&#34;agent&#34;)&#xA;# Pass Ollama Model to Agents: When creating your agents within the CrewAI framework, you can pass the Ollama model as an argument to the Agent constructor. For instance:&#xA;&#xA;local_expert = Agent(&#xA;  role=&#39;Local Expert at this city&#39;,&#xA;  goal=&#39;Provide the BEST insights about the selected city&#39;,&#xA;  backstory=&#34;&#34;&#34;A knowledgeable local guide with extensive information&#xA;  about the city, it&#39;s attractions and customs&#34;&#34;&#34;,&#xA;  tools=[&#xA;    SearchTools.search_internet,&#xA;    BrowserTools.scrape_and_summarize_website,&#xA;  ],&#xA;  llm=ollama_openhermes, # Ollama model passed here&#xA;  verbose=True&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels in creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents&#39; interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;CrewAI&#39;s Advantage&lt;/strong&gt;: CrewAI is built with production in mind. It offers the flexibility of Autogen&#39;s conversational agents and the structured process approach of ChatDev, but without the rigidity. CrewAI&#39;s processes are designed to be dynamic and adaptable, fitting seamlessly into both development and production workflows.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;CrewAI is open-source and we welcome contributions. If you&#39;re looking to contribute, please:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; &#xA; &lt;li&gt;Add your feature or improvement.&lt;/li&gt; &#xA; &lt;li&gt;Send a pull request.&lt;/li&gt; &#xA; &lt;li&gt;We appreciate your input!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installing Dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry lock&#xA;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Virtual Env&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Tests&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Packaging&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing Locally&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dist/*.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;CrewAI is released under the MIT License&lt;/p&gt;</summary>
  </entry>
</feed>