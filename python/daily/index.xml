<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-29T01:34:03Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>princeton-nlp/SWE-agent</title>
    <updated>2024-05-29T01:34:03Z</updated>
    <id>tag:github.com,2024-05-29:/princeton-nlp/SWE-agent</id>
    <link href="https://github.com/princeton-nlp/SWE-agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4, or your LM of choice. It solves 12.29% of bugs in the SWE-bench evaluation set and takes just 1.5 minutes to run.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.swe-agent.com/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/assets/swe-agent-banner.png&#34; alt=&#34;swe-agent.com&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://swe-agent.com&#34;&gt;&lt;strong&gt;Website &amp;amp; Demo&lt;/strong&gt;&lt;/a&gt;&amp;nbsp; | &amp;nbsp; &lt;a href=&#34;https://princeton-nlp.github.io/SWE-agent/&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt;&amp;nbsp; | &amp;nbsp; &lt;a href=&#34;https://discord.gg/AVEFbBn2rH&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;&amp;nbsp; | &amp;nbsp; &lt;a href=&#34;https://arxiv.org/abs/2405.15793&#34;&gt;&lt;strong&gt;Preprint&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üëã Overview &lt;a name=&#34;overview&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can fix bugs and issues in real GitHub repositories.&lt;/p&gt; &#xA;&lt;p&gt;On &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench&#34;&gt;SWE-bench&lt;/a&gt;, SWE-agent resolves &lt;strong&gt;12.47%&lt;/strong&gt; of issues, achieving the state-of-the-art performance on the full test set.&lt;/p&gt; &#xA;&lt;p&gt;We accomplish our results by designing simple LM-centric commands and feedback formats to make it easier for the LM to browse the repository, view, edit and execute code files. We call this an ü§ñ &lt;strong&gt;Agent-Computer Interface (ACI)&lt;/strong&gt;. Read more about it in our &lt;a href=&#34;https://arxiv.org/abs/2405.15793&#34;&gt;paper&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;SWE-agent is built and maintained by researchers from Princeton University.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/assets/results+preview.png&#34; style=&#34;width: 80%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If you found this work helpful, please consider using the following citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{yang2024sweagent,&#xA;      title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},&#xA;      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press},&#xA;      year={2024},&#xA;      eprint={2405.15793},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;‚ú® Use SWE-agent as a dev tool&lt;/h3&gt; &#xA;&lt;p&gt;We provide a command line tool and a graphical web interface:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/princeton-nlp/SWE-agent/assets/13602468/fa201621-ec31-4644-b658-c1d0feb92253&#34; alt=&#34;My Movie 3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Get started!&lt;/h2&gt; &#xA;&lt;p&gt;All information is provided in our &lt;a href=&#34;https://princeton-nlp.github.io/SWE-agent/&#34;&gt;documentation&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://princeton-nlp.github.io/SWE-agent/installation/&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://princeton-nlp.github.io/SWE-agent/usage/cl_tutorial/&#34;&gt;Command line usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://princeton-nlp.github.io/SWE-agent/usage/web_ui/&#34;&gt;Using the web UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://princeton-nlp.github.io/SWE-agent/usage/benchmarking/&#34;&gt;Benchmarking on SWE-bench&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;and many more topics.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://princeton-nlp.github.io/SWE-agent/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/assets/doc-scrot.png&#34; style=&#34;width: 500px&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üí´ Contributions &lt;a name=&#34;contributions&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you&#39;d like to ask questions, learn about upcoming features, and participate in future development, join our &lt;a href=&#34;https://discord.gg/AVEFbBn2rH&#34;&gt;Discord community&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;d like to contribute to the codebase, we welcome &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/issues&#34;&gt;issues&lt;/a&gt; and &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/pulls&#34;&gt;pull requests&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;d like to see a post or tutorial about some topic, please let us know via an &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/issues&#34;&gt;issue&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;https://john-b-yang.github.io/&#34;&gt;John Yang&lt;/a&gt; and &lt;a href=&#34;http://www.carlosejimenez.com/&#34;&gt;Carlos E. Jimenez&lt;/a&gt; (Email: {jy1682, carlosej}@princeton.edu).&lt;/p&gt; &#xA;&lt;h2&gt;ü™™ License &lt;a name=&#34;license&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;MIT. Check &lt;code&gt;LICENSE&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/pytest.yaml&#34;&gt;&lt;img src=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/pytest.yaml/badge.svg?sanitize=true&#34; alt=&#34;Pytest&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/test_build_containers.yaml&#34;&gt;&lt;img src=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/test_build_containers.yaml/badge.svg?sanitize=true&#34; alt=&#34;Test build containers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/release-dockerhub.yaml&#34;&gt;&lt;img src=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/release-dockerhub.yaml/badge.svg?sanitize=true&#34; alt=&#34;Release to dockerhub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/build-docs.yaml&#34;&gt;&lt;img src=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/build-docs.yaml/badge.svg?sanitize=true&#34; alt=&#34;build-docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/princeton-nlp/SWE-agent&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/princeton-nlp/SWE-agent/graph/badge.svg?token=18XAVDK365&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://results.pre-commit.ci/latest/github/princeton-nlp/SWE-agent/main&#34;&gt;&lt;img src=&#34;https://results.pre-commit.ci/badge/github/princeton-nlp/SWE-agent/main.svg?sanitize=true&#34; alt=&#34;pre-commit.ci status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/check-links.yaml&#34;&gt;&lt;img src=&#34;https://github.com/princeton-nlp/SWE-agent/actions/workflows/check-links.yaml/badge.svg?sanitize=true&#34; alt=&#34;Markdown links&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>lks-ai/anynode</title>
    <updated>2024-05-29T01:34:03Z</updated>
    <id>tag:github.com,2024-05-29:/lks-ai/anynode</id>
    <link href="https://github.com/lks-ai/anynode" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Node for ComfyUI that does what you ask it to do&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AnyNode v0.1 (üçÑ beta)&lt;/h1&gt; &#xA;&lt;p&gt;A ComfyUI Node that uses the power of LLMs to do anything with your input to make any type of output.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/43043c8f-24f6-4693-bc9e-43666cda78b3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/a0596d98-911e-4a93-b0f7-6f6a8782d49d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/f52K5pkbZy8&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/f52K5pkbZy8/maxresdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=f52K5pkbZy8&amp;amp;list=PL-EiB44NKrkcxJnR9MwD4hOSZOTlHn6Tr&#34;&gt;üì∫ More Tutorials on AnyNode at YouTube&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Update: Day 3 - Local LLMs Generating Nodes!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/70cb508e-b2af-470a-b777-1ddebe1cd59c&#34; alt=&#34;Screenshot from 2024-05-27 13-32-58&#34;&gt; We now have an &lt;code&gt;AnyNode üçÑ (Gemini)&lt;/code&gt; Node and our big star: The &lt;code&gt;AnyNode üçÑ (Local LLM)&lt;/code&gt; Node. This was the most requested feature since Day 1. The classic &lt;code&gt;AnyNode üçÑ&lt;/code&gt; will still use OpenAI directly.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can set each LocalLLM node to use a different local or hosted service as long as it&#39;s OpenAI compatible&lt;/li&gt; &#xA; &lt;li&gt;This means you can use &lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/&#34;&gt;vLLM&lt;/a&gt; and any other LocalLLM server from wherever you want&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install: Update (We just got on ComfyUI Manager!)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository into &lt;code&gt;comfy/custom_nodes&lt;/code&gt; &lt;em&gt;or&lt;/em&gt; Just search for &lt;code&gt;AnyNode&lt;/code&gt; on ComfyUI Manager&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re using openAI API, follow the OpenAI instructions&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re using Gemini, follow the Gemini Instructions&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re using LocalLLMs API, make sure your LLM server (ollama, etc.) is running&lt;/li&gt; &#xA; &lt;li&gt;Restart Comfy&lt;/li&gt; &#xA; &lt;li&gt;In ComfyUI double-click and search for &lt;code&gt;AnyNode&lt;/code&gt; or you can find it in Nodes &amp;gt; utils&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;OpenAI Instructions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you have the &lt;code&gt;openai&lt;/code&gt; module installed through pip: &lt;code&gt;pip install openai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; variable to your Environment Variables. &lt;a href=&#34;https://platform.openai.com/docs/quickstart&#34;&gt;How to get your OpenAI API key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;AnyNode üçÑ&lt;/code&gt; Is the node that directly uses OpenAI with the latest ChatGPT (whichever that may be at the time)&lt;/p&gt; &#xA;&lt;h3&gt;Gemini Instructions&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You don&#39;t need any extra module, so don&#39;t worry about that&lt;/li&gt; &#xA; &lt;li&gt;Add your &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; variable to your Environment Variables. &lt;a href=&#34;https://aistudio.google.com/app/apikey&#34;&gt;How to get your Google API key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;AnyNode üçÑ (Gemini)&lt;/code&gt; is still being tested so it probably contains bugs. I will update this today.&lt;/p&gt; &#xA;&lt;h3&gt;A Note about Security for the Local LLM variant&lt;/h3&gt; &#xA;&lt;p&gt;The way that AnyNode works, is that it executes code which happens externally from python that is coming back from the &lt;code&gt;server&lt;/code&gt; on a ChatCompletions endpoint. To put that into perspective, wherever you point it, you are giving some sort of control in python to that place. &lt;strong&gt;BE CAREFUL&lt;/strong&gt; that if you are not pointing it to &lt;code&gt;localhost&lt;/code&gt; that you absolutely trust the address that you put into &lt;code&gt;server&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How it Works&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Put in what you want the node to do with the input and output.&lt;/li&gt; &#xA; &lt;li&gt;Connect it up to anything on both sides&lt;/li&gt; &#xA; &lt;li&gt;Hit &lt;code&gt;Queue Prompt&lt;/code&gt; in ComfyUI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;AnyNode codes a python function based on your request and whatever input you connect to it to generate the output you requested which you can then connect to compatible nodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/1245aa94-fa4d-4490-a3f4-5e8b9918ca28&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Because of the ability to link &lt;em&gt;ANY&lt;/em&gt; node, you have to make sure it nails the output.&lt;/p&gt; &#xA;&lt;h2&gt;Caveats&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I have no idea how far you can take this nor it&#39;s limits&lt;/li&gt; &#xA; &lt;li&gt;LLMs can&#39;t read your mind. To make complex stuff in one node you&#39;d have to know a bit about programming&lt;/li&gt; &#xA; &lt;li&gt;The smaller the LLM you use to code your nodes, the less coding skills it might have&lt;/li&gt; &#xA; &lt;li&gt;Right now you can only see code the LLM generates in the console&lt;/li&gt; &#xA; &lt;li&gt;Can&#39;t make a sandwich&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Strengths&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use OpenAI &lt;code&gt;AnyNode üçÑ&lt;/code&gt;, Local LLMs &lt;code&gt;AnyNode üçÑ (Local LLM)&lt;/code&gt;, Gemini &lt;code&gt;AnyNode üçÑ (Gemini)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can use as many of these as you want in your workflow creating possibly complex node groups&lt;/li&gt; &#xA; &lt;li&gt;Really great at single purpose nodes&lt;/li&gt; &#xA; &lt;li&gt;Uses OpenAI API for simple access to the latest and greatest in generation models&lt;/li&gt; &#xA; &lt;li&gt;Technically you could point this at vLLM or Ollama for you LocalLLM fans&lt;/li&gt; &#xA; &lt;li&gt;Can use most of the popular python libraries and most standard like (numpy, torch, collections, re)&lt;/li&gt; &#xA; &lt;li&gt;Ability to make more complex nodes that use inputs like MODEL, VAE and CLIP with input type awareness&lt;/li&gt; &#xA; &lt;li&gt;Error Mitigation: Auto-correct errors it made in code (just press &lt;code&gt;Queue Prompt&lt;/code&gt; again)&lt;/li&gt; &#xA; &lt;li&gt;Incremental Code editing (the last generated function serves as example for next generation)&lt;/li&gt; &#xA; &lt;li&gt;Copying cool nodes you prompt is as easy as copying the workflow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Coming Soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Export to Node&lt;/strong&gt;: Compile a new comfy node from your AnyNode (Requires restart to use your new node)&lt;/li&gt; &#xA; &lt;li&gt;Saving the generated functions in your workflows&lt;/li&gt; &#xA; &lt;li&gt;Multiple Inputs and outputs&lt;/li&gt; &#xA; &lt;li&gt;RAG based function storage and semantic search across comfy modules (not a pipe dream)&lt;/li&gt; &#xA; &lt;li&gt;Persistent data storage in the AnyNode (functions store extra data for iterative processing or persistent memory)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Coding Errors you Might Encounter&lt;/h2&gt; &#xA;&lt;p&gt;As with any LLMs or text generating language model, when it comes to coding, it can sometimes make mistakes that it can&#39;t fix by itself even if you show it the error of it&#39;s ways. A lot of these can be mitigated by modifying your prompt. If you encounter some of the known ones, we have some prompt engineering solutions here for you.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;invalid syntax (, line 1)&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;ve used ChatGPT, you know the output can be rather chatty. That being said, sometimes when AnyNode runs, you will see this ambiguous error. So without you having to check the console I&#39;ll just tell you... it&#39;s the chattyness. What happens is the LLM tries to talk to you. This can be mitigated by re-inforcing your prompt with &lt;code&gt;Quit yapping. Only write the function.&lt;/code&gt; at the very end. We&#39;ve reinforced it in the latest updates, but yes, there is also a deeper fix to do, which just ignores the chattyness. Only, I&#39;d rather it never be chatty, because that tends to make it do other things which don&#39;t make sense. We&#39;re looking for strict rule following here, and not random creativity.&lt;/p&gt; &#xA;&lt;h2&gt;If you&#39;re still here&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s enjoy some stuff I made while up all night!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/02801f5c-9f67-40f1-83a7-a93e6103d362&#34; alt=&#34;image&#34;&gt; This one, well... the prompts explain it all, but TLDR; It takes an image as input and outputs only the red channel of that image.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/4cfe5b0b-d515-4f9d-9d86-eff1a08595ed&#34; alt=&#34;Screenshot from 2024-05-26 01-30-40&#34;&gt; Here I use three AnyNodes: One to load a file, one to summarize the text in that file, and the other to just do some parsing of that text. No coding needed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/4bc5c6c0-ca56-4f4c-88d5-5339b6d5ada1&#34; alt=&#34;image&#34;&gt; I took that Ant example a bit further and added in the normal nodes to do img2img with my color transforms from AnyNode&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/0e02ae11-7e46-4d50-8645-fe7a5d3c46c9&#34; alt=&#34;Screenshot from 2024-05-26 20-45-57&#34;&gt; Here I ask for an instagram-like sepia tone filter for my AnyNode ... I titled the node Image Filter just so I can remember what it&#39;s supposed to be doing in the workflow&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/b8879685-6a78-4314-a8e4-5d88d046621d&#34; alt=&#34;image&#34;&gt; Let&#39;s try a much more complex description of an HSV transform, but still in plain english. And we get a node that will randomly filter HSV every time it&#39;s run! &lt;a href=&#34;https://raw.githubusercontent.com/lks-ai/anynode/main/workflows/anynode_hsl-tweak.json&#34;&gt;Here&#39;s that workflow&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/c00531c9-c93a-471a-bca0-bb62abea4943&#34; alt=&#34;Screenshot from 2024-05-26 21-05-25&#34;&gt; Then I ask for a more legacy instagram filter (normally it would pop the saturation and warm the light up, which it did!)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/dda13811-7e0e-4d9e-ab7c-fd2ff3d594ba&#34; alt=&#34;image&#34;&gt; How about a psychedelic filter?&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lks-ai/anynode/assets/163685473/29db4cd9-db77-4931-a340-10755e0211fa&#34; alt=&#34;image&#34;&gt; Here I ask it to make a &#34;sota edge detector&#34; for the output image, and it makes me a pretty cool Sobel filter. And I pretend that I&#39;m on the moon. &lt;a href=&#34;https://raw.githubusercontent.com/lks-ai/anynode/main/workflows/sobel-charcoal.json&#34;&gt;Here&#39;s that workflow&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InternLM/lmdeploy</title>
    <updated>2024-05-29T01:34:03Z</updated>
    <id>tag:github.com,2024-05-29:/InternLM/lmdeploy</id>
    <link href="https://github.com/InternLM/lmdeploy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LMDeploy is a toolkit for compressing, deploying, and serving LLMs.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/_static/image/lmdeploy-logo.svg?sanitize=true&#34; width=&#34;450&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/lmdeploy&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lmdeploy&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/dm/lmdeploy&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;a href=&#34;https://github.com/InternLM/lmdeploy/tree/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/InternLM/lmdeploy.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/lmdeploy/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed-raw/InternLM/lmdeploy&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/lmdeploy/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/InternLM/lmdeploy&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://lmdeploy.readthedocs.io/en/latest/&#34;&gt;üìòDocumentation&lt;/a&gt; | &lt;a href=&#34;https://lmdeploy.readthedocs.io/en/latest/get_started.html&#34;&gt;üõ†Ô∏èQuick Start&lt;/a&gt; | &lt;a href=&#34;https://github.com/InternLM/lmdeploy/issues/new/choose&#34;&gt;ü§îReporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;üëã join us on &lt;a href=&#34;https://cdn.vansin.top/internlm/lmdeploy.jpg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-grey?style=social&amp;amp;logo=wechat&amp;amp;label=WeChat&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intern_lm&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-grey?style=social&amp;amp;logo=twitter&amp;amp;label=Twitter&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/xa29JuW87d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-grey?style=social&amp;amp;logo=discord&amp;amp;label=Discord&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Latest News üéâ&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;b&gt;2024&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[2024/05] Balance vision model when deploying VLMs with multiple GPUs&lt;/li&gt; &#xA;  &lt;li&gt;[2024/05] Support 4-bits weight-only quantization and inference on VMLs, such as InternVL v1.5, LLaVa, InternLMXComposer2&lt;/li&gt; &#xA;  &lt;li&gt;[2024/04] Support Llama3 and more VLMs, such as InternVL v1.1, v1.2, MiniGemini, InternLMXComposer2.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/04] TurboMind adds online int8/int4 KV cache quantization and inference for all supported devices. Refer &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/quantization/kv_quant.md&#34;&gt;here&lt;/a&gt; for detailed guide&lt;/li&gt; &#xA;  &lt;li&gt;[2024/04] TurboMind latest upgrade boosts GQA, rocketing the &lt;a href=&#34;https://huggingface.co/internlm/internlm2-20b&#34;&gt;internlm2-20b&lt;/a&gt; model inference to 16+ RPS, about 1.8x faster than vLLM.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/04] Support Qwen1.5-MOE and dbrx.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/03] Support DeepSeek-VL offline inference pipeline and serving.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/03] Support VLM offline inference pipeline and serving.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/02] Support Qwen 1.5, Gemma, Mistral, Mixtral, Deepseek-MOE and so on.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/01] &lt;a href=&#34;https://github.com/InternLM/OpenAOE&#34;&gt;OpenAOE&lt;/a&gt; seamless integration with &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/serving/api_server.md&#34;&gt;LMDeploy Serving Service&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/01] Support for multi-model, multi-machine, multi-card inference services. For usage instructions, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/serving/proxy_server.md&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;[2024/01] Support &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/pytorch.md&#34;&gt;PyTorch inference engine&lt;/a&gt;, developed entirely in Python, helping to lower the barriers for developers and enable rapid experimentation with new features and technologies.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;&lt;b&gt;2023&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[2023/12] Turbomind supports multimodal input. &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/examples/vl/README.md&#34;&gt;Gradio Demo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;[2023/11] Turbomind supports loading hf model directly. Click &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/load_hf.md&#34;&gt;here&lt;/a&gt; for details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/11] TurboMind major upgrades, including: Paged Attention, faster attention kernels without sequence length limitation, 2x faster KV8 kernels, Split-K decoding (Flash Decoding), and W4A16 inference for sm_75&lt;/li&gt; &#xA;  &lt;li&gt;[2023/09] TurboMind supports Qwen-14B&lt;/li&gt; &#xA;  &lt;li&gt;[2023/09] TurboMind supports InternLM-20B&lt;/li&gt; &#xA;  &lt;li&gt;[2023/09] TurboMind supports all features of Code Llama: code completion, infilling, chat / instruct, and python specialist. Click &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/supported_models/codellama.md&#34;&gt;here&lt;/a&gt; for deployment guide&lt;/li&gt; &#xA;  &lt;li&gt;[2023/09] TurboMind supports Baichuan2-7B&lt;/li&gt; &#xA;  &lt;li&gt;[2023/08] TurboMind supports flash-attention2.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/08] TurboMind supports Qwen-7B, dynamic NTK-RoPE scaling and dynamic logN scaling&lt;/li&gt; &#xA;  &lt;li&gt;[2023/08] TurboMind supports Windows (tp=1)&lt;/li&gt; &#xA;  &lt;li&gt;[2023/08] TurboMind supports 4-bit inference, 2.4x faster than FP16, the fastest open-source implementation. Check &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/quantization/w4a16.md&#34;&gt;this&lt;/a&gt; guide for detailed info&lt;/li&gt; &#xA;  &lt;li&gt;[2023/08] LMDeploy has launched on the &lt;a href=&#34;https://huggingface.co/lmdeploy&#34;&gt;HuggingFace Hub&lt;/a&gt;, providing ready-to-use 4-bit models.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/08] LMDeploy supports 4-bit quantization using the &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt; algorithm.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/07] TurboMind supports Llama-2 70B with GQA.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/07] TurboMind supports Llama-2 7B/13B.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/07] TurboMind supports tensor-parallel inference of InternLM.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;LMDeploy is a toolkit for compressing, deploying, and serving LLM, developed by the &lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt; teams. It has the following core features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: LMDeploy delivers up to 1.8x higher request throughput than vLLM, by introducing key features like persistent batch(a.k.a. continuous batching), blocked KV cache, dynamic split&amp;amp;fuse, tensor parallelism, high-performance CUDA kernels and so on.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Effective Quantization&lt;/strong&gt;: LMDeploy supports weight-only and k/v quantization, and the 4-bit inference performance is 2.4x higher than FP16. The quantization quality has been confirmed via OpenCompass evaluation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Effortless Distribution Server&lt;/strong&gt;: Leveraging the request distribution service, LMDeploy facilitates an easy and efficient deployment of multi-model services across multiple machines and cards.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interactive Inference Mode&lt;/strong&gt;: By caching the k/v of attention during multi-round dialogue processes, the engine remembers dialogue history, thus avoiding repetitive processing of historical sessions.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Performance&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/InternLM/lmdeploy/assets/4560679/8e455cf1-a792-4fa8-91a2-75df96a2a5ba&#34; alt=&#34;v0 1 0-benchmark&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For detailed inference benchmarks in more devices and more settings, please refer to the following link:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/benchmark/a100_fp16.md&#34;&gt;A100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;V100&lt;/li&gt; &#xA; &lt;li&gt;4090&lt;/li&gt; &#xA; &lt;li&gt;3090&lt;/li&gt; &#xA; &lt;li&gt;2080&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Supported Models&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;middle&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;LLMs&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;VLMs&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td align=&#34;left&#34; valign=&#34;top&#34;&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Llama (7B - 65B)&lt;/li&gt; &#xA;     &lt;li&gt;Llama2 (7B - 70B)&lt;/li&gt; &#xA;     &lt;li&gt;Llama3 (8B, 70B)&lt;/li&gt; &#xA;     &lt;li&gt;InternLM (7B - 20B)&lt;/li&gt; &#xA;     &lt;li&gt;InternLM2 (7B - 20B)&lt;/li&gt; &#xA;     &lt;li&gt;QWen (1.8B - 72B)&lt;/li&gt; &#xA;     &lt;li&gt;QWen1.5 (0.5B - 110B)&lt;/li&gt; &#xA;     &lt;li&gt;QWen1.5 - MoE (0.5B - 72B)&lt;/li&gt; &#xA;     &lt;li&gt;Baichuan (7B)&lt;/li&gt; &#xA;     &lt;li&gt;Baichuan2 (7B-13B)&lt;/li&gt; &#xA;     &lt;li&gt;Code Llama (7B - 34B)&lt;/li&gt; &#xA;     &lt;li&gt;ChatGLM2 (6B)&lt;/li&gt; &#xA;     &lt;li&gt;Falcon (7B - 180B)&lt;/li&gt; &#xA;     &lt;li&gt;YI (6B-34B)&lt;/li&gt; &#xA;     &lt;li&gt;Mistral (7B)&lt;/li&gt; &#xA;     &lt;li&gt;DeepSeek-MoE (16B)&lt;/li&gt; &#xA;     &lt;li&gt;Mixtral (8x7B, 8x22B)&lt;/li&gt; &#xA;     &lt;li&gt;Gemma (2B - 7B)&lt;/li&gt; &#xA;     &lt;li&gt;Dbrx (132B)&lt;/li&gt; &#xA;     &lt;li&gt;Phi-3-mini (3.8B)&lt;/li&gt; &#xA;     &lt;li&gt;StarCoder2 (3B - 15B)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;LLaVA(1.5,1.6) (7B-34B)&lt;/li&gt; &#xA;     &lt;li&gt;InternLM-XComposer2 (7B, 4khd-7B)&lt;/li&gt; &#xA;     &lt;li&gt;QWen-VL (7B)&lt;/li&gt; &#xA;     &lt;li&gt;DeepSeek-VL (7B)&lt;/li&gt; &#xA;     &lt;li&gt;InternVL-Chat (v1.1-v1.5)&lt;/li&gt; &#xA;     &lt;li&gt;MiniGeminiLlama (7B)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;LMDeploy has developed two inference engines - &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/turbomind.md&#34;&gt;TurboMind&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/pytorch.md&#34;&gt;PyTorch&lt;/a&gt;, each with a different focus. The former strives for ultimate optimization of inference performance, while the latter, developed purely in Python, aims to decrease the barriers for developers.&lt;/p&gt; &#xA;&lt;p&gt;They differ in the types of supported models and the inference data type. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/supported_models/supported_models.md&#34;&gt;this table&lt;/a&gt; for each engine&#39;s capability and choose the proper one that best fits your actual needs.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start &lt;a href=&#34;https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install lmdeploy with pip ( python 3.8+) or &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/build.md&#34;&gt;from source&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install lmdeploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since v0.3.0, The default prebuilt package is compiled on &lt;strong&gt;CUDA 12&lt;/strong&gt;. However, if CUDA 11+ is required, you can install lmdeploy by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export LMDEPLOY_VERSION=0.3.0&#xA;export PYTHON_VERSION=38&#xA;pip install https://github.com/InternLM/lmdeploy/releases/download/v${LMDEPLOY_VERSION}/lmdeploy-${LMDEPLOY_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux2014_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Offline Batch Inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import lmdeploy&#xA;pipe = lmdeploy.pipeline(&#34;internlm/internlm2-chat-7b&#34;)&#xA;response = pipe([&#34;Hi, pls intro yourself&#34;, &#34;Shanghai is&#34;])&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] By default, LMDeploy downloads model from HuggingFace. If you would like to use models from ModelScope, please install ModelScope by &lt;code&gt;pip install modelscope&lt;/code&gt; and set the environment variable:&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;export LMDEPLOY_USE_MODELSCOPE=True&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more information about inference pipeline, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/pipeline.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;Please overview &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/get_started.md&#34;&gt;getting_started&lt;/a&gt; section for the basic usage of LMDeploy.&lt;/p&gt; &#xA;&lt;p&gt;For detailed user guides and advanced guides, please refer to our &lt;a href=&#34;https://lmdeploy.readthedocs.io/en/latest/&#34;&gt;tutorials&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;User Guide &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/pipeline.md&#34;&gt;LLM Inference pipeline&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1Dh-YlSwg78ZO3AlleO441NF_QP2shs95#scrollTo=YALmXnwCG1pQ&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/vl_pipeline.md&#34;&gt;VLM Inference pipeline&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1nKLfnPeDA3p-FMNw2NhI-KOpk7-nlNjF?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/serving/api_server.md&#34;&gt;LLM Serving&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/serving/api_server_vl.md&#34;&gt;VLM Serving&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/quantization&#34;&gt;Quantization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Advance Guide &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/turbomind.md&#34;&gt;Inference Engine - TurboMind&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/inference/pytorch.md&#34;&gt;Inference Engine - PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/advance/chat_template.md&#34;&gt;Customize chat templates&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/advance/pytorch_new_model.md&#34;&gt;Add a new model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;gemm tuning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/advance/long_context.md&#34;&gt;Long context inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/docs/en/serving/proxy_server.md&#34;&gt;Multi-model inference service&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Third-party projects&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deploying LLMs offline on the NVIDIA Jetson platform by LMDeploy: &lt;a href=&#34;https://github.com/BestAnHongjun/LMDeploy-Jetson&#34;&gt;LMDeploy-Jetson&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We appreciate all contributions to LMDeploy. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mit-han-lab/llm-awq&#34;&gt;llm-awq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed-MII&#34;&gt;DeepSpeed-MII&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2023lmdeploy,&#xA;    title={LMDeploy: A Toolkit for Compressing, Deploying, and Serving LLM},&#xA;    author={LMDeploy Contributors},&#xA;    howpublished = {\url{https://github.com/InternLM/lmdeploy}},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/lmdeploy/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>