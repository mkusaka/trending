<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-19T01:36:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zjc062/mind-vis</title>
    <updated>2022-11-19T01:36:56Z</updated>
    <id>tag:github.com,2022-11-19:/zjc062/mind-vis</id>
    <link href="https://github.com/zjc062/mind-vis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code base for MinD-Vis&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Seeing Beyond the Brain: Masked Modeling Conditioned Diffusion Model for Human Vision Decoding&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zjc062/mind-vis/main/assets/first_fig.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;MinD-Vis&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MinD-Vis&lt;/strong&gt; is a framework for decoding human visual stimuli from brain recording. This document introduces the precesedures required for replicating the results in &lt;em&gt;Seeing Beyond the Brain: Masked Modeling Conditioned Diffusion Model for Human Vision Decoding&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, due to the scarcity of data annotations and the complexity of underlying brain information, it is challenging to decode images with faithful details and meaningful semantics. In this work, we present &lt;strong&gt;MinD-Vis&lt;/strong&gt;: Sparse &lt;strong&gt;M&lt;/strong&gt;asked Bra&lt;strong&gt;in&lt;/strong&gt; Modeling with Double-Conditioned Latent &lt;strong&gt;D&lt;/strong&gt;iffusion Model for Human &lt;strong&gt;Vis&lt;/strong&gt;ion Decoding. Specifically, by boosting the information capacity of feature representations learned from a large-scale resting-state fMRI dataset, we show that our MinD-Vis can reconstruct highly plausible images with semantically matching details from brain recordings with very few paired annotations. We benchmarked our model qualitatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality (FID) by &lt;strong&gt;66%&lt;/strong&gt; and &lt;strong&gt;41%&lt;/strong&gt; respectively.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zjc062/mind-vis/main/assets/flowchart_r.png&#34; alt=&#34;flowchar-img&#34;&gt; Our framework consists of two main stages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stage A: Sparse-Coded Masked Brain Modeling (&lt;em&gt;SC-MBM&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Stage B: Double-Conditioned Latent Diffusion Model (&lt;em&gt;DC-LDM&lt;/em&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;strong&gt;data&lt;/strong&gt; folder and &lt;strong&gt;pretrains&lt;/strong&gt; folder are not included in this repository. Please download them from &lt;a href=&#34;https://figshare.com/s/94cd778e6afafb00946e&#34;&gt;FigShare&lt;/a&gt; and put them in the root directory of this repository as shown below.&lt;/p&gt; &#xA;&lt;p&gt;File path | Description&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;/data&#xA;‚î£ üìÇ HCP&#xA;‚îÉ   ‚î£ üìÇ npz&#xA;‚îÉ   ‚îÉ   ‚î£ üìÇ dummy_sub_01&#xA;‚îÉ   ‚îÉ   ‚îÉ   ‚îó HCP_visual_voxel.npz&#xA;‚îÉ   ‚îÉ   ‚î£ üìÇ dummy_sub_02&#xA;‚îÉ   ‚îÉ   ‚îÉ   ‚îó ...&#xA;&#xA;‚î£ üìÇ Kamitani&#xA;‚îÉ   ‚î£ üìÇ npz&#xA;‚îÉ   ‚îÉ   ‚îó üìú sbj_1.npz&#xA;‚îÉ   ‚îÉ   ‚îó üìú sbj_2.npz&#xA;‚îÉ   ‚îÉ   ‚îó üìú sbj_3.npz&#xA;‚îÉ   ‚îÉ   ‚îó üìú sbj_4.npz&#xA;‚îÉ   ‚îÉ   ‚îó üìú sbj_5.npz&#xA;‚îÉ   ‚îÉ   ‚îó üìú images_256.npz&#xA;‚îÉ   ‚îÉ   ‚îó üìú imagenet_class_index.json&#xA;‚îÉ   ‚îÉ   ‚îó üìú imagenet_training_label.csv&#xA;‚îÉ   ‚îÉ   ‚îó üìú imagenet_testing_label.csv&#xA;&#xA;‚î£ üìÇ BOLD5000&#xA;‚îÉ   ‚î£ üìÇ BOLD5000_GLMsingle_ROI_betas&#xA;‚îÉ   ‚îÉ   ‚î£ üìÇ py&#xA;‚îÉ   ‚îÉ   ‚îÉ   ‚îó CSI1_GLMbetas-TYPED-FITHRF-GLMDENOISE-RR_allses_LHEarlyVis.npy&#xA;‚îÉ   ‚îÉ   ‚îÉ   ‚îó ...&#xA;‚îÉ   ‚îÉ   ‚îÉ   ‚îó CSIx_GLMbetas-TYPED-FITHRF-GLMDENOISE-RR_allses_xx.npy&#xA;‚îÉ   ‚î£ üìÇ BOLD5000_Stimuli&#xA;‚îÉ   ‚îÉ   ‚î£ üìÇ Image_Labels&#xA;‚îÉ   ‚îÉ   ‚î£ üìÇ Scene_Stimuli&#xA;‚îÉ   ‚îÉ   ‚î£ üìÇ Stimuli_Presentation_Lists&#xA;&#xA;&#xA;/pretrains&#xA;‚î£ üìÇ ldm&#xA;‚îÉ   ‚î£ üìÇ label2img  ÔºàImageNet pre-trained label-conditioned LDM)&#xA;‚îÉ   ‚îÉ   ‚îó üìú config.yaml&#xA;‚îÉ   ‚îÉ   ‚îó üìú model.ckpt&#xA;&#xA;‚î£ üìÇ GOD  &#xA;‚îÉ   ‚îó üìú fmri_encoder.pth  (SC-MBM pre-trained fMRI encoder)&#xA;‚îÉ   ‚îó üìú finetuned.pth     (finetuned fMRI encoder + finetuned LDM)&#xA;&#xA;‚î£ üìÇ BOLD5000&#xA;‚îÉ   ‚îó üìú fmri_encoder.pth  (SC-MBM pre-trained fMRI encoder)&#xA;‚îÉ   ‚îó üìú finetuned.pth     (finetuned fMRI encoder + finetuned LDM)&#xA;&#xA;&#xA;/code&#xA;‚î£ üìÇ sc_mbm&#xA;‚îÉ   ‚îó üìú mae_for_fmri.py&#xA;‚îÉ   ‚îó üìú trainer.py&#xA;‚îÉ   ‚îó üìú utils.py&#xA;&#xA;‚î£ üìÇ dc_ldm&#xA;‚îÉ   ‚îó üìú ldm_for_fmri.py&#xA;‚îÉ   ‚îó üìú utils.py&#xA;‚îÉ   ‚î£ üìÇ models&#xA;‚îÉ   ‚îÉ   ‚îó (adopted from LDM)&#xA;‚îÉ   ‚î£ üìÇ modules&#xA;‚îÉ   ‚îÉ   ‚îó (adopted from LDM)&#xA;&#xA;‚îó  üìú stageA1_mbm_pretrain.py   (main script for pre-training for SC-MBM)&#xA;‚îó  üìú stageA2_mbm_finetune.py   (main script for tuning SC-MBM on fMRI only from test sets)&#xA;‚îó  üìú stageB_ldm_finetune.py    (main script for fine-tuning DC-LDM)&#xA;‚îó  üìú gen_eval.py               (main script for generating decoded images)&#xA;&#xA;‚îó  üìú dataset.py                (functions for loading datasets)&#xA;‚îó  üìú eval_metrics.py           (functions for evaluation metrics)&#xA;‚îó  üìú config.py                 (configurations for the main scripts)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Environment setup&lt;/h2&gt; &#xA;&lt;p&gt;Create and activate conda environment named &lt;code&gt;mind-vis&lt;/code&gt; from our &lt;code&gt;env.yaml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda env create -f env.yaml&#xA;conda activate mind-vis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download data and checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;Due to size limi and license issue, the full fMRI pre-training dataset (required to replicate &lt;strong&gt;Stage A&lt;/strong&gt;) needs to be downloaded from the &lt;a href=&#34;https://db.humanconnectome.org/data/projects/HCP_1200&#34;&gt;Human Connectome Projects (HCP)&lt;/a&gt; offical website. The pre-processing scripts are also included in this repo.&lt;/p&gt; &#xA;&lt;p&gt;We also provide checkpoints and finetuning data at &lt;a href=&#34;https://figshare.com/s/94cd778e6afafb00946e&#34;&gt;FigShare&lt;/a&gt; to run the finetuing and decoding directly. Due to the size limit, we only release the checkpoints for Subject 3 and CSI4 in the GOD and BOLD5000 respectively. Checkpoints for other subjects are also available upon request. After downloading, extract the &lt;code&gt;data/&lt;/code&gt; and &lt;code&gt;pretrains/&lt;/code&gt; to the project directory.&lt;/p&gt; &#xA;&lt;h2&gt;SC-MBM Pre-training on fMRI (Stage A)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zjc062/mind-vis/main/assets/mbm_mask.png&#34; alt=&#34;mbm-fig&#34;&gt; The fMRI pre-training is performed with masked brain modeling in the fMRI dataset containing around 136,000 fMRI samples from 1205 subjects (HCP + GOD). To perform the pre-training from scratch with defaults parameters, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python code/stageA1_mbm_pretrain.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Hyper-parameters can be changed with command line arguments,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python code/stageA1_mbm_pretrain.py --mask_ratio 0.65 --num_epoch 800 --batch_size 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or the parameters can also be changed in &lt;code&gt;code/config.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Multiple-GPU (DDP) training is supported, run with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m torch.distributed.launch --nproc_per_node=NUM_GPUS code/stageA1_mbm_pretrain.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The pre-training results will be saved locally at &lt;code&gt;results/fmri_pretrain&lt;/code&gt; and remotely at &lt;code&gt;wandb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After pre-training on the large-scale fMRI dataset, we need to finetune the autoencoder with fMRI data from the testing set. Run the following,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python code/stageA2_mbm_finetune.py --dataset GOD --pretrain_mbm_path results/fmri_pretrain/RUN_FOLDER_NAME/checkpoints/checkpoint.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;--dataset&lt;/code&gt; can be either &lt;code&gt;GOD&lt;/code&gt; or &lt;code&gt;BOLD5000&lt;/code&gt;. And &lt;code&gt;RUN_FOLDER_NAME&lt;/code&gt; is the folder name generated for the pre-training. For example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python code/stageA2_mbm_finetune.py --dataset GOD --pretrain_mbm_path results/fmri_pretrain/01-08-2022-11:37:22/checkpoints/checkpoint.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The fMRI finetuning results will be saved locally at &lt;code&gt;results/fmri_finetune&lt;/code&gt; and remotely at &lt;code&gt;wandb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetune the Double-Conditional LDM with Pre-trained fMRI Encoder (Stage B)&lt;/h2&gt; &#xA;&lt;p&gt;In this stage, the cross-attention heads and pre-trained fMRI encoder will be jointly optimized with fMRI-image pairs. Decoded images will be generated in this stage. This stage can be run without downloading HCP. Only finetuning datasets and pre-trained fMRI encoder shared in our FigShare link are required. Run this stage with our provided pre-trained fMRI encoder and default parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python code/stageB_ldm_finetune.py --dataset GOD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;--dataset&lt;/code&gt; can be either &lt;code&gt;GOD&lt;/code&gt; or &lt;code&gt;BOLD5000&lt;/code&gt;. The results and generated samples will be saved locally at &lt;code&gt;results/generation&lt;/code&gt; and remotely at &lt;code&gt;wandb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run with custom-pre-trained fMRI encoder and parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python code/stageB_ldm_finetune.py --dataset GOD --pretrain_mbm_path results/fmri_fintune/RUN_FOLDER_NAME/checkpoints/checkpoint.pth --num_epoch 500 --batch_size 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run fMRI Decoding and Generate Images with Trained Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;Only finetuning datasets and trained checkpoints in our FigShare link are required. Notice that images generated by the provided checkpoins gives the same evaluation reuslts as in the paper, but may not produce the exact same images as in the paper due to sampling variance. Run this stage with our provided checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python code/gen_eval.py --dataset GOD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;--dataset&lt;/code&gt; can be either &lt;code&gt;GOD&lt;/code&gt; or &lt;code&gt;BOLD5000&lt;/code&gt;. The results and generated samples will be saved locally at &lt;code&gt;results/eval&lt;/code&gt; and remotely at &lt;code&gt;wandb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zjc062/mind-vis/main/assets/bold5000.png&#34; alt=&#34;bold5000&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank &lt;a href=&#34;https://github.com/KamitaniLab&#34;&gt;Kamitani Lab&lt;/a&gt;, &lt;a href=&#34;https://github.com/WeizmannVision&#34;&gt; Weizmann Vision Lab&lt;/a&gt; and &lt;a href=&#34;https://bold5000-dataset.github.io/website/&#34;&gt;BOLD5000 team&lt;/a&gt; for making their raw and pre-processed data public. Our Masked Brain Modeling implementation is based on the &lt;a href=&#34;https://github.com/facebookresearch/mae&#34;&gt;Masked Autoencoders&lt;/a&gt; by Facebook Research. Our Conditional Latent Diffusion Model implementation is based on the &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion Model&lt;/a&gt; implementation from CompVis. We thank these authors for making their codes and checkpoints publicly available!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>raminmh/CfC</title>
    <updated>2022-11-19T01:36:56Z</updated>
    <id>tag:github.com,2022-11-19:/raminmh/CfC</id>
    <link href="https://github.com/raminmh/CfC" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Closed-form Continuous-time Neural Networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Closed-form Continuous-time Models&lt;/h1&gt; &#xA;&lt;p&gt;Closed-form Continuous-time Neural Networks (CfCs) are powerful sequential neural information processing units.&lt;/p&gt; &#xA;&lt;p&gt;Paper Open Access: &lt;a href=&#34;https://www.nature.com/articles/s42256-022-00556-7&#34;&gt;https://www.nature.com/articles/s42256-022-00556-7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Arxiv: &lt;a href=&#34;https://arxiv.org/abs/2106.13898&#34;&gt;https://arxiv.org/abs/2106.13898&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python3.6 or newer&lt;/li&gt; &#xA; &lt;li&gt;Tensorflow 2.4 or newer&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.8 or newer&lt;/li&gt; &#xA; &lt;li&gt;pytorch-lightning 1.3.0 or newer&lt;/li&gt; &#xA; &lt;li&gt;scikit-learn 0.24.2 or newer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Module description&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tf_cfc.py&lt;/code&gt; Implementation of the CfC (various versions) in Tensorflow 2.x&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;torch_cfc.py&lt;/code&gt; Implementation of the CfC (various versions) in PyTorch&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train_physio.py&lt;/code&gt; Trains the CfC models on the Physionet 2012 dataset in PyTorch (code adapted from Rubanova et al. 2019)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train_xor.py&lt;/code&gt; Trains the CfC models on the XOR dataset in Tensorflow (code adapted from Lechner &amp;amp; Hasani, 2020)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train_imdb.py&lt;/code&gt; Trains the CfC models on the IMDB dataset in Tensorflow (code adapted from Keras examples website)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train_walker.py&lt;/code&gt; Trains the CfC models on the Walker2d dataset in Tensorflow (code adapted from Lechner &amp;amp; Hasani, 2020)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;irregular_sampled_datasets.py&lt;/code&gt; Datasets (same splits) from Lechner &amp;amp; Hasani (2020)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;duv_physionet.py&lt;/code&gt; and &lt;code&gt;duv_utils.py&lt;/code&gt; Physionet dataset (same split) from Rubanova et al. (2019)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;All training scripts except the following three flags&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;no_gate&lt;/code&gt; Runs the CfC without the (1-sigmoid) part&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;minimal&lt;/code&gt; Runs the CfC direct solution&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;use_ltc&lt;/code&gt; Runs an LTC with a semi-implicit ODE solver instead of a CfC&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;use_mixed&lt;/code&gt; Mixes the CfC&#39;s RNN-state with a LSTM to avoid vanishing gradients&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If none of these flags are provided, the full CfC model is used&lt;/p&gt; &#xA;&lt;p&gt;For instance&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 train_physio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;train the full CfC model on the Physionet dataset.&lt;/p&gt; &#xA;&lt;p&gt;Similarly&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;train_walker.py --minimal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;runs the direct CfC solution on the walker2d dataset.&lt;/p&gt; &#xA;&lt;p&gt;For downloading the Walker2d dataset of Lechner &amp;amp; Hasani 2020, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source download_dataset.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-@article{hasani_closed-form_2022,&#34;&gt;&#x9;title = {Closed-form continuous-time neural networks},&#xA;&#x9;journal = {Nature Machine Intelligence},&#xA;&#x9;author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Liebenwein, Lucas and Ray, Aaron and Tschaikowski, Max and Teschl, Gerald and Rus, Daniela},&#xA;  issn = {2522-5839},&#xA;&#x9;month = nov,&#xA;&#x9;year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ludwig-ai/ludwig</title>
    <updated>2022-11-19T01:36:56Z</updated>
    <id>tag:github.com,2022-11-19:/ludwig-ai/ludwig</id>
    <link href="https://github.com/ludwig-ai/ludwig" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data-centric declarative deep learning framework&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/ludwig-ai/ludwig-docs/raw/master/docs/images/ludwig_hero.png&#34; alt=&#34;Ludwig logo&#34; title=&#34;Ludwig logo&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/ludwig&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/ludwig.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/github/commit-activity/m/ludwig-ai/ludwig&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/ludwig-ai/ludwig&#34; alt=&#34;Commit Activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/4210&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/4210/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/ludwig-ai/shared_invite/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-chat-green.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/ludwigai&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/ludwigai/ludwig.svg?sanitize=true&#34; alt=&#34;DockerHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/ludwig&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/ludwig&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ludwig-ai/ludwig/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/ludwig_ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/ludwig_ai.svg?style=social&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Full Documentation: &lt;a href=&#34;https://ludwig.ai&#34;&gt;ludwig.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;What is Ludwig?&lt;/h1&gt; &#xA;&lt;p&gt;Ludwig is a &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/what_is_ludwig/#why-declarative-machine-learning-systems&#34;&gt;declarative machine learning framework&lt;/a&gt; that makes it easy to define machine learning pipelines using a simple and flexible data-driven configuration system. Ludwig is suitable for a wide variety of AI tasks, and is hosted by the &lt;a href=&#34;https://lfaidata.foundation/&#34;&gt;Linux Foundation AI &amp;amp; Data&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The configuration declares the input and output features, with their respective data types. Users can also specify additional parameters to preprocess, encode, and decode features, load from pre-trained models, compose the internal model architecture, set training parameters, or run hyperparameter optimization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ludwig-ai/ludwig-docs/master/docs/images/ludwig_legos_unanimated.gif&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ludwig will build an end-to-end machine learning pipeline automatically, using whatever is explicitly specified in the configuration, while falling back to smart defaults for any parameters that are not.&lt;/p&gt; &#xA;&lt;h1&gt;Declarative Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;Ludwig‚Äôs declarative approach to machine learning empowers you to have full control of the components of the machine learning pipeline that you care about, while leaving it up to Ludwig to make reasonable decisions for the rest.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ludwig-ai/ludwig/master/images/why_declarative.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Analysts, scientists, engineers, and researchers use Ludwig to explore state-of-the-art model architectures, run hyperparameter search, scale up to larger than available memory datasets and multi-node clusters, and finally serve the best model in production.&lt;/p&gt; &#xA;&lt;p&gt;Finally, the use of abstract interfaces throughout the codebase makes it easy for users to extend Ludwig by adding new models, metrics, losses, and preprocessing functions that can be registered to make them immediately useable in the same unified configuration system.&lt;/p&gt; &#xA;&lt;h1&gt;Main Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/how_ludwig_works&#34;&gt;Data-Driven configuration system&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A config YAML file that describes the schema of your data (input features, output features, and their types) is all you need to start training deep learning models. Ludwig uses declared features to compose a deep learning model accordingly.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;input_features:&#xA;    - name: data_column_1&#xA;      type: number&#xA;    - name: data_column_2&#xA;      type: category&#xA;    - name: data_column_3&#xA;      type: text&#xA;    - name: data_column_4&#xA;      type: image&#xA;    ...&#xA;&#xA;output_features:&#xA;    - name: data_column_5&#xA;      type: number&#xA;    - name: data_column_6&#xA;      type: category&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/command_line_interface&#34;&gt;Training, prediction, and evaluation from the command line&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Simple commands can be used to train models and predict new data.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig train --config config.yaml --dataset data.csv&#xA;ludwig predict --model_path results/experiment_run/model --dataset test.csv&#xA;ludwig eval --model_path results/experiment_run/model --dataset test.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/api/LudwigModel&#34;&gt;Programmatic API&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ludwig also provides a simple programmatic API for all of the functionality described above and more.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ludwig.api import LudwigModel&#xA;&#xA;# train a model&#xA;config = {&#xA;    &#34;input_features&#34;: [...],&#xA;    &#34;output_features&#34;: [...],&#xA;}&#xA;model = LudwigModel(config)&#xA;data = pd.read_csv(&#34;data.csv&#34;)&#xA;train_stats, _, model_dir = model.train(data)&#xA;&#xA;# or load a model&#xA;model = LudwigModel.load(model_dir)&#xA;&#xA;# obtain predictions&#xA;predictions = model.predict(data)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/distributed_training&#34;&gt;Distributed training&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Train models in a distributed setting using &lt;a href=&#34;https://github.com/horovod/horovod&#34;&gt;Horovod&lt;/a&gt;, which allows training on a single machine with multiple GPUs or multiple machines with multiple GPUs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/serving&#34;&gt;Serving&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Serve models using FastAPI.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig serve --model_path ./results/experiment_run/model&#xA;curl http://0.0.0.0:8000/predict -X POST -F &#34;movie_title=Friends With Money&#34; -F &#34;content_rating=R&#34; -F &#34;genres=Art House &amp;amp; International, Comedy, Drama&#34; -F &#34;runtime=88.0&#34; -F &#34;top_critic=TRUE&#34; -F &#34;review_content=The cast is terrific, the movie isn&#39;t.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/hyperopt&#34;&gt;Hyperparameter optimization&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Run hyperparameter optimization locally or using &lt;a href=&#34;https://docs.ray.io/en/latest/tune/index.html&#34;&gt;Ray Tune&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig hyperopt --config config.yaml --dataset data.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/automl&#34;&gt;AutoML&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ludwig AutoML takes a dataset, the target column, and a time budget, and returns a trained Ludwig model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/integrations&#34;&gt;Third-Party integrations&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ludwig provides an extendable interface to integrate with third-party systems for tracking experiments. Third-party integrations exist for Comet ML, Weights &amp;amp; Biases, WhyLabs, and MLFlow.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/developer_guide&#34;&gt;Extensibility&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ludwig is built from the ground up with extensibility in mind. It is easy to add new data types by implementing clear, well-documented abstract classes that define functions to preprocess, encode, and decode data.&lt;/p&gt; &lt;p&gt;Furthermore, new &lt;code&gt;torch nn.Module&lt;/code&gt; models can be easily added by them to a registry. This encourages reuse and sharing new models with the community. Refer to the &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/developer_guide&#34;&gt;Developer Guide&lt;/a&gt; for further details.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;For a full tutorial, check out the official &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/getting_started/&#34;&gt;getting started guide&lt;/a&gt;, or take a look at end-to-end &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples&#34;&gt;Examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Step 1: Install&lt;/h2&gt; &#xA;&lt;p&gt;Install from PyPi. Be aware that Ludwig requires Python 3.7+.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install ludwig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Step 2: Define a configuration&lt;/h2&gt; &#xA;&lt;p&gt;Create a config that describes the schema of your data.&lt;/p&gt; &#xA;&lt;p&gt;Assume we have a text classification task, with data containing a sentence and class column like the following.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;sentence&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;class&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Former president Barack Obama ...&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;politics&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Juventus hired Cristiano Ronaldo ...&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;sport&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LeBron James joins the Lakers ...&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;sport&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;...&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;A configuration will look like this.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;input_features:&#xA;- name: sentence&#xA;  type: text&#xA;&#xA;output_features:&#xA;- name: class&#xA;  type: category&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Starting from a simple config like the one above, any and all aspects of the model architecture, training loop, hyperparameter search, and backend infrastructure can be modified as additional fields in the declarative configuration to customize the pipeline to meet your requirements.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;input_features:&#xA;- name: sentence&#xA;  type: text&#xA;  encoder: transformer&#xA;  layers: 6&#xA;  embedding_size: 512&#xA;&#xA;output_features:&#xA;- name: class&#xA;  type: category&#xA;  loss: cross_entropy&#xA;&#xA;trainer:&#xA;  epochs: 50&#xA;  batch_size: 64&#xA;  optimizer:&#xA;    type: adamw&#xA;    beat1: 0.9&#xA;  learning_rate: 0.001&#xA;&#xA;backend:&#xA;  type: ray&#xA;  cache_format: parquet&#xA;  processor:&#xA;    type: dask&#xA;  trainer:&#xA;    use_gpu: true&#xA;    num_workers: 4&#xA;    resources_per_worker:&#xA;      CPU: 4&#xA;      GPU: 1&#xA;&#xA;hyperopt:&#xA;  metric: f1&#xA;  sampler: random&#xA;  parameters:&#xA;    title.num_layers:&#xA;      lower: 1&#xA;      upper: 5&#xA;    trainer.learning_rate:&#xA;      values: [0.01, 0.003, 0.001]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For details on what can be configured, check out &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/configuration/&#34;&gt;Ludwig Configuration&lt;/a&gt; docs.&lt;/p&gt; &#xA;&lt;h2&gt;Step 3: Train a model&lt;/h2&gt; &#xA;&lt;p&gt;Simple commands can be used to train models and predict new data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig train --config config.yaml --dataset data.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Step 4: Predict and evaluate&lt;/h2&gt; &#xA;&lt;p&gt;The training process will produce a model that can be used for evaluating on and obtaining predictions for new data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig predict --model path/to/trained/model --dataset heldout.csv&#xA;ludwig evaluate --model path/to/trained/model --dataset heldout.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Step 5: Visualize&lt;/h2&gt; &#xA;&lt;p&gt;Ludwig provides a suite of visualization tools allows you to analyze models&#39; training and test performance and to compare them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig visualize --visualization compare_performance --test_statistics path/to/test_statistics_model_1.json path/to/test_statistics_model_2.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the full set of visualization see the &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/visualizations&#34;&gt;Visualization Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Step 6: Happy modeling&lt;/h2&gt; &#xA;&lt;p&gt;Try applying Ludwig to your data. &lt;a href=&#34;https://join.slack.com/t/ludwig-ai/shared_invite/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ&#34;&gt;Reach out&lt;/a&gt; if you have any questions.&lt;/p&gt; &#xA;&lt;h1&gt;Advantages&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Minimal machine learning boilerplate&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ludwig takes care of the engineering complexity of machine learning out of the box, enabling research scientists to focus on building models at the highest level of abstraction. Data preprocessing, hyperparameter optimization, device management, and distributed training for &lt;code&gt;torch.nn.Module&lt;/code&gt; models come completely free.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easily build your benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Creating a state-of-the-art baseline and comparing it with a new model is a simple config change.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easily apply new architectures to multiple problems and datasets&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Apply new models across the extensive set of tasks and datasets that Ludwig supports. Ludwig includes a &lt;a href=&#34;https://arxiv.org/abs/2111.04260&#34;&gt;full benchmarking toolkit&lt;/a&gt; accessible to any user, for running experiments with multiple models across multiple datasets with just a simple configuration.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Highly configurable data preprocessing, modeling, and metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Any and all aspects of the model architecture, training loop, hyperparameter search, and backend infrastructure can be modified as additional fields in the declarative configuration to customize the pipeline to meet your requirements. For details on what can be configured, check out &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/configuration/&#34;&gt;Ludwig Configuration&lt;/a&gt; docs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-modal, multi-task learning out-of-the-box&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mix and match tabular data, text, images, and even audio into complex model configurations without writing code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rich model exporting and tracking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Automatically track all trials and metrics with tools like Tensorboard, Comet ML, Weights &amp;amp; Biases, MLFlow, and Aim Stack.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatically scale training to multi-GPU, multi-node clusters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Go from training on your local machine to the cloud without code changes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Low-code interface for state-of-the-art models, including pre-trained Huggingface Transformers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ludwig also natively integrates with pre-trained models, such as the ones available in &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;Huggingface Transformers&lt;/a&gt;. Users can choose from a vast collection of state-of-the-art pre-trained PyTorch models to use without needing to write any code at all. For example, training a BERT-based sentiment analysis model with Ludwig is as simple as:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig train --dataset sst5 --config_str ‚Äú{input_features: [{name: sentence, type: text, encoder: bert}], output_features: [{name: label, type: category}]}‚Äù&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Low-code interface for AutoML&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/automl/&#34;&gt;Ludwig AutoML&lt;/a&gt; allows users to obtain trained models by providing just a dataset, the target column, and a time budget.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;auto_train_results = ludwig.automl.auto_train(dataset=my_dataset_df, target=target_column_name, time_limit_s=7200)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy productionisation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ludwig makes it easy to serve deep learning models, including on GPUs. Launch a REST API for your trained Ludwig model.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig serve --model_path=/path/to/model&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ludwig supports exporting models to efficient Torschscript bundles.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ludwig export_torchscript -‚Äìmodel_path=/path/to/model&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tutorials&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/text_classification&#34;&gt;Text Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/adult_census_income&#34;&gt;Tabular Data Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/mnist&#34;&gt;Image Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/multimodal_classification&#34;&gt;Multimodal Classification&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Example Use Cases&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/ner_tagging&#34;&gt;Named Entity Recognition Tagging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/nlu&#34;&gt;Natural Language Understanding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/machine_translation&#34;&gt;Machine Translation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/seq2seq&#34;&gt;Chit-Chat Dialogue Modeling through seq2seq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/sentiment_analysis&#34;&gt;Sentiment Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/oneshot&#34;&gt;One-shot Learning with Siamese Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/visual_qa&#34;&gt;Visual Question Answering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/speech_recognition&#34;&gt;Spoken Digit Speech Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/speaker_verification&#34;&gt;Speaker Verification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/titanic&#34;&gt;Binary Classification (Titanic)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/forecasting&#34;&gt;Timeseries forecasting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/weather&#34;&gt;Timeseries forecasting (Weather)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/movie_ratings&#34;&gt;Movie rating prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/multi_label&#34;&gt;Multi-label classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/multi_task&#34;&gt;Multi-Task Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/fuel_efficiency&#34;&gt;Simple Regression: Fuel Efficiency Prediction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples/fraud&#34;&gt;Fraud Detection&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;More Information&lt;/h1&gt; &#xA;&lt;p&gt;Read our publications on &lt;a href=&#34;https://arxiv.org/pdf/1909.07930.pdf&#34;&gt;Ludwig&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2107.08148.pdf&#34;&gt;declarative ML&lt;/a&gt;, and &lt;a href=&#34;https://openreview.net/pdf?id=hwjnu6qW7E4&#34;&gt;Ludwig‚Äôs SoTA benchmarks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Learn more about &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/user_guide/how_ludwig_works/&#34;&gt;how Ludwig works&lt;/a&gt;, &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/getting_started/&#34;&gt;how to get started&lt;/a&gt;, and work through more &lt;a href=&#34;https://ludwig-ai.github.io/ludwig-docs/latest/examples&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in contributing, have questions, comments, or thoughts to share, or if you just want to be in the know, please consider &lt;a href=&#34;https://join.slack.com/t/ludwig-ai/shared_invite/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ&#34;&gt;joining the Ludwig Slack&lt;/a&gt; and follow us on &lt;a href=&#34;https://twitter.com/ludwig_ai&#34;&gt;Twitter&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Getting Involved&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/ludwig-ai/shared_invite/zt-mrxo87w6-DlX5~73T2B4v_g6jj0pJcQ&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/ludwig_ai&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/ludwig-ai&#34;&gt;Medium&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ludwig-ai/ludwig/issues&#34;&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>