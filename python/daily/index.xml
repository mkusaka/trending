<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-15T01:35:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>De3vil/KLogger</title>
    <updated>2022-12-15T01:35:50Z</updated>
    <id>tag:github.com,2022-12-15:/De3vil/KLogger</id>
    <link href="https://github.com/De3vil/KLogger" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Remote persistent üîëLogger for Windows and Linux&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;br&gt; KüîëLogger v3.0.0 &lt;br&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Author-mido--de3vil-orange&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Open%20Source-Yes-cyan?style=flat-square&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Written%20In-Python-blue?style=flat-square&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Description&lt;/h3&gt; &#xA;&lt;p&gt;Remote persistent üîëLogger for Windows and Linux&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logs keys pressed on keyboard&lt;/li&gt; &#xA; &lt;li&gt;upload reports in &lt;a href=&#34;https://anonfiles.com/&#34;&gt;anonfile&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Works with Linux and Windows.&lt;/li&gt; &#xA; &lt;li&gt;Does not require root or admin privlages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;All previous problems have been resolved&lt;/h3&gt; &#xA;&lt;h3&gt;It is still undetectable&lt;/h3&gt; &#xA;&lt;h4&gt;Requirements&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python &amp;gt;= 3.8 ++ Download &lt;a href=&#34;https://www.python.org/ftp/python/3.8.10/python-3.8.10-amd64.exe&#34;&gt;Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;os : Windows&lt;/li&gt; &#xA; &lt;li&gt;os : Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation to Windows:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/De3vil/KLogger.git&#xA;cd KLogger&#xA;pip install -r win_requirements.txt&#xA;python logger.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation to Linux&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/De3vil/KLogger.git&#xA;cd KLogger&#xA;chmod +x linux_setup.sh&#xA;bash linux_setup.sh&#xA;python logger.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;warning:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;* Don&#39;t Upload in VirusTotal.com Bcz This tool will not work with Time.&#xA;* Virustotal Share Signatures With AV Comapnies.&#xA;* Again Don&#39;t be an Idiot!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;AV detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/De3vil/KLogger/main/src/AV.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/De3vil/KLogger/main/src/1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt; Abdulrahman Mohammed &lt;/h4&gt; &#xA;&lt;a href=&#34;https://t.me/De3vil_3&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/De3vil__3-blue?style=for-the-badge&amp;amp;logo=Telegram&amp;amp;logoColor=00AEFF&amp;amp;labelColor=black&amp;amp;color=black&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://www.facebook.com/De3vil.3&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/De3vil__3-blue?style=for-the-badge&amp;amp;logo=Facebook&amp;amp;logoColor=00AEFF&amp;amp;labelColor=black&amp;amp;color=black&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;If this tool has been useful for you, feel free to thank me by buying me a coffee :) &lt;a href=&#34;https://www.buymeacoffee.com/De3vil&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;Coffee&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.paypal.com/paypalme/De3vil01&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/$-support-ff69b4.svg?style=flat&#34; alt=&#34;B De3vil&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/glide-text2im</title>
    <updated>2022-12-15T01:35:50Z</updated>
    <id>tag:github.com,2022-12-15:/openai/glide-text2im</id>
    <link href="https://github.com/openai/glide-text2im" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GLIDE: a diffusion-based text-conditional image synthesis model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GLIDE&lt;/h1&gt; &#xA;&lt;p&gt;This is the official codebase for running the small, filtered-data GLIDE model from &lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For details on the pre-trained models in this repository, see the &lt;a href=&#34;https://raw.githubusercontent.com/openai/glide-text2im/main/model-card.md&#34;&gt;Model Card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;To install this package, clone this repository and then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For detailed usage examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/openai/glide-text2im/main/notebooks&#34;&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/openai/glide-text2im/main/notebooks/text2im.ipynb&#34;&gt;text2im&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/text2im.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; notebook shows how to use GLIDE (filtered) with classifier-free guidance to produce images conditioned on text prompts.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/openai/glide-text2im/main/notebooks/inpaint.ipynb&#34;&gt;inpaint&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/inpaint.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; notebook shows how to use GLIDE (filtered) to fill in a masked region of an image, conditioned on a text prompt.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/openai/glide-text2im/main/notebooks/clip_guided.ipynb&#34;&gt;clip_guided&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/clip_guided.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; notebook shows how to use GLIDE (filtered) + a filtered noise-aware CLIP model to produce images conditioned on text prompts.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>allenai/RL4LMs</title>
    <updated>2022-12-15T01:35:50Z</updated>
    <id>tag:github.com,2022-12-15:/allenai/RL4LMs</id>
    <link href="https://github.com/allenai/RL4LMs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modular RL library to fine-tune language models to human preferences&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/allenai/RL4LMs/main/RL4LMs_logo.png&#34; width=&#34;512px&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;span&gt;ü§ñ&lt;/span&gt; RL4LMs &lt;span&gt;üöÄ&lt;/span&gt; &lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; A modular RL library to fine-tune language models to human preferences &lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;We provide easily customizable building blocks for training language models including implementations of &lt;strong&gt;on-policy algorithms&lt;/strong&gt;, &lt;strong&gt;reward functions&lt;/strong&gt;, &lt;strong&gt;metrics&lt;/strong&gt;, &lt;strong&gt;datasets&lt;/strong&gt; and &lt;strong&gt;LM based actor-critic policies&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Paper Link: &lt;a href=&#34;https://arxiv.org/abs/2210.01241&#34;&gt;https://arxiv.org/abs/2210.01241&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Website Link: &lt;a href=&#34;https://rl4lms.apps.allenai.org/&#34;&gt;https://rl4lms.apps.allenai.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thoroughly &lt;strong&gt;tested&lt;/strong&gt; and &lt;strong&gt;benchmarked&lt;/strong&gt; with over &lt;strong&gt;2000 experiments&lt;/strong&gt; &lt;span&gt;üî•&lt;/span&gt; (GRUE benchmark &lt;span&gt;üèÜ&lt;/span&gt;) on a comprehensive set of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;6 different Natural Language Processing (NLP) Tasks: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Summarization&lt;/li&gt; &#xA;   &lt;li&gt;Generative Commonsense Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;IMDB Sentiment-based Text Continuation&lt;/li&gt; &#xA;   &lt;li&gt;Table-to-text generation&lt;/li&gt; &#xA;   &lt;li&gt;Abstractive Question Answering&lt;/li&gt; &#xA;   &lt;li&gt;Machine Translation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Different types of NLG metrics (20+) which can be used as reward functions: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lexical Metrics (eg: ROUGE, BLEU, SacreBLEU, METEOR)&lt;/li&gt; &#xA;   &lt;li&gt;Semantic Metrics (eg: BERTSCORE, BLEURT)&lt;/li&gt; &#xA;   &lt;li&gt;Task specific metrics (eg: PARENT, CIDER, SPICE)&lt;/li&gt; &#xA;   &lt;li&gt;Scores from pre-trained classifiers (eg: Sentiment scores)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;On-policy algorithms of PPO, A2C, TRPO and novel &lt;strong&gt;NLPO (Natural Language Policy Optimization)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Actor-Critic Policies supporting causal LMs (eg. GPT-2/3) and seq2seq LMs (eg. T5, BART)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of these building blocks can be customizable allowing users to train transformer-based LMs to optimize any arbitrary reward function on any dataset of their choice.&lt;/p&gt; &#xA;&lt;h2&gt;Recent updates (v0.2.0) on 23-Nov-22&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added daily dialog task&lt;/li&gt; &#xA; &lt;li&gt;Fixed compatibility issues with some Seq2seq models such as BART, blendorbot etc&lt;/li&gt; &#xA; &lt;li&gt;Implemented data parallel support&lt;/li&gt; &#xA; &lt;li&gt;Refactored policy classes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;h2&gt;Local Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/allenai/RL4LMs.git&#xA;cd RL4LMs&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;We provide also a Dockerfile for development using docker containers containing all the dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build . -t rl4lms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Additional dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Optionally, coreNLP libraries are required for certain metric computations (eg. SPICE) which can be downloaded through &lt;code&gt;cd rl4lms/envs/text_generation/caption_metrics/spice &amp;amp;&amp;amp; bash get_stanford_models.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Quick Start - Train PPO/NLPO using pre-defined YAML configs&lt;/h1&gt; &#xA;&lt;p&gt;We provide a simple training API that can be invoked via train &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/main/scripts/training/train_text_generation.py&#34;&gt;script&lt;/a&gt; that allows to train PPO, NLPO or a supervised model by using a config file (YAML).&lt;/p&gt; &#xA;&lt;p&gt;For example, to train T5-base on CNN/DM summarization on PPO using Rouge-1 as reward function, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/training/train_text_generation.py --config_path scripts/training/task_configs/summarization/t5_ppo.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Config files for all tasks can be found &lt;a href=&#34;https://github.com/allenai/RL4LMs/tree/main/scripts/training/task_configs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;YAML file schema - Configuring building blocks&lt;/h2&gt; &#xA;&lt;p&gt;Config file contains details about hyper-parameter settings for building blocks which are described below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset/Task&lt;/strong&gt;: Dataset containing samples with input prompts and reference sentences. Available datasets are found in the class &lt;code&gt;DataPoolRegistry&lt;/code&gt; in &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/main/rl4lms/envs/text_generation/registry.py&#34;&gt;registry&lt;/a&gt;. (See how to create your own dataset &lt;a href=&#34;https://raw.githubusercontent.com/allenai/RL4LMs/main/#adding-dataset&#34;&gt;here&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;datapool:&#xA;  id: cnn_daily_mail&#xA;  args:&#xA;    prompt_prefix: &#34;Summarize: &#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tokenizer&lt;/strong&gt; - A pre-trained tokenizer that is used to (de)tokenize input and output sequences with settings for padding and truncation&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;tokenizer:&#xA;  model_name: t5-base&#xA;  padding_side: left&#xA;  truncation_side: left&#xA;  pad_token_as_eos_token: False&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reward Function&lt;/strong&gt;: Reward function which computes token-level scores at each time step of MDP. Available reward functions can be found in the class &lt;code&gt;RewardFunctionRegistry&lt;/code&gt;. (See how to create your own reward function &lt;a href=&#34;https://raw.githubusercontent.com/allenai/RL4LMs/main/#adding-reward-function&#34;&gt;here&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;reward_fn:&#xA;  id: rouge&#xA;  args:&#xA;    rouge_type: &#34;rouge1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Configures a gym-style text generation &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/main/rl4lms/envs/text_generation/env.py&#34;&gt;environment&lt;/a&gt; which simulates MDP episodes. Rollouts are generated using train samples from dataset consisting of input and reference texts. Further, we wrap our env with &lt;code&gt;SubProcVecEnv&lt;/code&gt; from stable-baselines that processes &lt;code&gt;n_envs&lt;/code&gt; episodes in parallel using multi-processing to compute step-wise rewards.&lt;br&gt; Further configuration settings include:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;max_episode_length&lt;/code&gt; : max length of the episode&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;max_prompt_length&lt;/code&gt; - maximum length of the input text to consider&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;terminate_on_eos&lt;/code&gt; - whether to terminate the episode as soon as EOS action is performed&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt_truncation_side&lt;/code&gt; - truncation side for the prompt text&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;context_start_token&lt;/code&gt; - id for context token (corresponds to initial token given to decoder in encoder-decoder models)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;env:&#xA;  n_envs: 10&#xA;  args:&#xA;    max_prompt_length: 512&#xA;    max_episode_length: 100&#xA;    terminate_on_eos: True&#xA;    prompt_truncation_side: &#34;right&#34;&#xA;    context_start_token: 0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;On-policy alg&lt;/strong&gt;: We provide implementations of 4 on-policy algorithms: PPO, NLPO, A2C and TRPO adapted from &lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34;&gt;stable-baselines3&lt;/a&gt; tailored to work with NLP tasks which can be used out-of-the-box with either a causal policy or a seq2seq LM policy. (See how to create your own &lt;a href=&#34;https://raw.githubusercontent.com/allenai/RL4LMs/main/#adding-custom-on-policy-algorithms&#34;&gt;on-policy algorithm&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/allenai/RL4LMs/main/#adding-custom-policies&#34;&gt;policy&lt;/a&gt;)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;We also provide a supervised &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/2863116cd5860e4a4106a76486e70bfac25df2ba/rl4lms/envs/text_generation/training_utils.py#L225&#34;&gt;trainer&lt;/a&gt; for benchmarking purposes. Supervised Warm start models are already uploaded to Huggingface Hub and specified in the respective config files.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Hyper-parameters for the algorithm can be specified at &lt;code&gt;alg/args&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Further, all RL algorithms use adaptive KL controller to keep the LM close to original LM by setting initial KL co-efficient (&lt;code&gt;alg/kl_div/coeff&lt;/code&gt;) and target KL (&lt;code&gt;alg/kl_div/target_kl&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;We support two types of LM policy: &lt;strong&gt;causal LM policy&lt;/strong&gt; (for decoder only models) and &lt;strong&gt;seq2seq LM policy&lt;/strong&gt; (for encoder-decoder models). Further for NLPO, we also provide maskable variants of these. Policy implementations can be found &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/main/rl4lms/envs/text_generation/policy.py&#34;&gt;here&lt;/a&gt; in and it can be attached to algorithms by specifying &lt;code&gt;alg/policy/id&lt;/code&gt; and &lt;code&gt;alg/policy/args&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;alg:&#xA;  id: ppo&#xA;  args: &#xA;    n_steps: 512&#xA;    batch_size: 64&#xA;    verbose: 1&#xA;    learning_rate: 0.000002&#xA;    n_epochs: 5&#xA;    ent_coef: 0.0&#xA;  kl_div:&#xA;    coeff: 0.001&#xA;    target_kl: 0.2&#xA;  policy:&#xA;    id: seq2seq_lm_actor_critic_policy&#xA;    args:&#xA;      model_name: t5-base&#xA;      apply_model_parallel: True&#xA;      prompt_truncation_side: &#34;right&#34;&#xA;      generation_kwargs:&#xA;        do_sample: True&#xA;        top_k: 50&#xA;        min_length: 50&#xA;        max_new_tokens: 100          &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Trainer Config&lt;/strong&gt;: We provide an &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/2863116cd5860e4a4106a76486e70bfac25df2ba/rl4lms/envs/text_generation/training_utils.py#L126&#34;&gt;On-policy trainer&lt;/a&gt; - a feature-complete wrapper that instantiates building blocks from their corresponding configs and provides an outer training loop consisting of &lt;em&gt;train&lt;/em&gt; and &lt;em&gt;eval&lt;/em&gt; iterations &lt;code&gt;train_evaluation/n_iters&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Each iteration corresponds to performing updates with &lt;code&gt;alg/args/n_steps&lt;/code&gt; x &lt;code&gt;env/n_envs&lt;/code&gt; of the chosen algorithm.&lt;/li&gt; &#xA;   &lt;li&gt;For every &lt;code&gt;eval_every&lt;/code&gt; iters, LM is evaluated on validation split using metrics listed in &lt;code&gt;train_evaluation/metrics&lt;/code&gt; with generation kwargs provided in &lt;code&gt;train_evaluation/generation_kwargs&lt;/code&gt; (this overrides rollout &lt;code&gt;alg/policy/generation_kwargs&lt;/code&gt; for inference purposes only)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# train and evaluation&#xA;train_evaluation:&#xA;  eval_batch_size: 100&#xA;  n_iters: 100&#xA;  eval_every: 10&#xA;  save_every: 1&#xA;  metrics:&#xA;    - id: meteor&#xA;      args: {}&#xA;    - id: rouge&#xA;    - id: bleu&#xA;      args: {}&#xA;    - id: bert_score&#xA;      args:&#xA;        language: en&#xA;    - id: diversity&#xA;      args: {}&#xA;  generation_kwargs: &#xA;    do_sample: True&#xA;    top_k: 0&#xA;    temperature: 0.7&#xA;    min_length: 50&#xA;    max_new_tokens: 100&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Custom Building Blocks &lt;span&gt;üîß&lt;/span&gt;&lt;/h1&gt; &#xA;&lt;p&gt;RL4LMs provide complete customizability - with respect to adding new tasks/datasets, reward functions, evaluation metric, on-policy algorithms and actor-critic policies.&lt;/p&gt; &#xA;&lt;h2&gt;Adding dataset&lt;/h2&gt; &#xA;&lt;p&gt;Users can create their own datasets by sub-classing &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/data_pools/text_generation_pool.py#L15&#34;&gt;TextGenPool&lt;/a&gt; just by overriding &lt;code&gt;prepare(cls, split: str, **args) -&amp;gt; &#39;TextGenPool&#39;:&lt;/code&gt; method to return an instance of TextGenPool. An example is shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rl4lms.data_pools.text_generation_pool import Sample, TextGenPool&#xA;&#xA;class MyDataPool(TextGenPool):&#xA;   @classmethod&#xA;   def prepare(cls, split: str):&#xA;       .. &#xA;       samples = []&#xA;       for ix, item in enumerate(..):&#xA;           sample = Sample(id=f&#34;{split}_{ix}&#34;,&#xA;                           prompt_or_input_text=item[&#34;document&#34;],&#xA;                           references=[item[&#34;target&#34;]]&#xA;                           )&#xA;           samples.append(sample)&#xA;       pool_instance = cls(samples)&#xA;       return pool_instance&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Adding reward function&lt;/h2&gt; &#xA;&lt;p&gt;Custom reward funtions can be implemented easily by sub-classing &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/reward.py#L12&#34;&gt;RewardFunction&lt;/a&gt; (a callable) which takes observation ($s$), next observation ($s&#39;$), action ($a$), done (indicating whether episode is finished) and meta info (containing other information about textual input). Here, &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/observation.py#L11&#34;&gt;Observation&lt;/a&gt; is a data class object consisting of generated text (at a particular step), prompt text, context text (at that step), reference text which can be used to compute token-level or sentence level rewards.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rl4lms.envs.text_generation.observation import Observation&#xA;from rl4lms.envs.text_generation.reward import RewardFunction&#xA;&#xA;&#xA;class MyRewardFunction(RewardFunction):&#xA;   def __init__(self, *args) -&amp;gt; None:&#xA;       super().__init__()&#xA;&#xA;   def __call__(self, prev_observation: Observation,&#xA;                action: int,&#xA;                current_observation: Observation,&#xA;                done: bool,&#xA;                meta_info: Dict[str, Any] = None) -&amp;gt; float:&#xA;       if done:&#xA;           reward = ..&#xA;           return reward&#xA;       return 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;üí°&lt;/span&gt; In addition to traditional NLG metrics, for quick prototyping, we provide two synthetic reward functions which trains LMs to &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/test_reward.py#L8&#34;&gt;generate numbers&lt;/a&gt; in increasing order and &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/test_reward.py#L54&#34;&gt;generate dates&lt;/a&gt;. These can be used to quickly test different algorithms and policies. Corresponding configs can be found here (&lt;a href=&#34;https://github.com/allenai/RL4LMs/tree/main/scripts/training/task_configs/synthetic_generate_increasing_numbers&#34;&gt;numbers&lt;/a&gt;, &lt;a href=&#34;https://github.com/allenai/RL4LMs/tree/main/scripts/training/task_configs/synthetic_generate_dates&#34;&gt;dates&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Adding custom metrics&lt;/h2&gt; &#xA;&lt;p&gt;Users can create their own evaluation metric which then will be used to periodically evaluate the model on validation split of dataset. This can be done by sub-classing &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/metric.py#L20&#34;&gt;BaseMetric&lt;/a&gt; which takes prompt texts, generated texts, reference texts, meta_infos, current LM model, split name as inputs and returns a dict with metric name as key and value consisting of tuple of sentence-level scores and corpus level scores. An example is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from rl4lms.envs.text_generation.metric import BaseMetric&#xA;&#xA;class MyMetric(BaseMetric):&#xA;   def __init__(self) -&amp;gt; None:&#xA;       super().__init__()&#xA;&#xA;   def compute(self,&#xA;               prompt_texts: List[str],&#xA;               generated_texts: List[str],&#xA;               reference_texts: List[List[str]],&#xA;               meta_infos: List[Dict[str, Any]] = None,&#xA;               model: PreTrainedModel = None,&#xA;               split_name: str = None):&#xA;       metric_dict = {&#xA;           &#34;custom_metrics/my_metric&#34;: ([0.4, 0.7, 0.9], 0.7)&#xA;       }&#xA;       return metric_dict&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Adding custom on-policy algorithms&lt;/h2&gt; &#xA;&lt;p&gt;In addition to supported on-policy algorithms (PPO, NLPO, A2C,TRPO), users can implement their own on-policy algorithms with ease by sub-classing stable-baselines3&#39;s &lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/raw/a697401e032dd4fecbbd4162755ddd707df980d3/stable_baselines3/common/on_policy_algorithm.py#L20&#34;&gt;OnPolicyAlgorithm&lt;/a&gt;. Since we provide &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/af5a1326578789856ca8550cb5496c9ccc1afdc5/rl4lms/envs/text_generation/alg_wrappers.py#L67&#34;&gt;wrappers&lt;/a&gt; for on-policy algorithms that handles rollouts using LM policies, environment, computing rewards etc, users just need to implement &lt;code&gt;train()&lt;/code&gt; method with custom loss functions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm&#xA;&#xA;class MyOnPolicyAlgorithm(OnPolicyAlgorithm):&#xA;    def __init__(**args):&#xA;        super().__init__(**args)&#xA;&#xA;    def train(self) -&amp;gt; None:&#xA;        # train for n_epochs epochs&#xA;        for epoch in range(self.n_epochs):&#xA;            # Do a complete pass on the rollout buffer&#xA;            for rollout_data in self.rollout_buffer.get(self.batch_size):&#xA;              # compute loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Adding custom policies&lt;/h2&gt; &#xA;&lt;p&gt;We provide LM based actor-critic policy &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/main/rl4lms/envs/text_generation/policy.py&#34;&gt;implementations&lt;/a&gt; that wraps causal LM and seq2seq LMs. These can be also extended (for eg: use a different critic architecture) by overriding appropriate methods (eg. &lt;code&gt;evaluate_actions()&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Registry&lt;/h2&gt; &#xA;&lt;p&gt;Finally, just register your custom components by adding them to corresponding &lt;a href=&#34;https://github.com/allenai/RL4LMs/raw/main/rl4lms/envs/text_generation/registry.py&#34;&gt;registry&lt;/a&gt;, after which they can be used directly from configs similar to pre-defined components &lt;span&gt;üëã&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Crowdsourcing templates&lt;/h2&gt; &#xA;&lt;p&gt;We have provided the crowdsourcing templates we used on mechanical turk, along with example inputs in &lt;code&gt;scripts/crowdworking_templates&lt;/code&gt;. You might find these a helpful starting point either for evaluating your own model&#39;s generations, or for gathering training data for a learned reward function.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Logging and Experiment Results&lt;/h1&gt; &#xA;&lt;p&gt;Additionally, we support WANDB logging and warm-starting of training by storing checkpoints and other training artifacts in a user-specified path. This is especially useful for running preemptible jobs on large, scheduled clusters.&lt;/p&gt; &#xA;&lt;p&gt;Artifacts include (1) jsonl file containing rollout infos at specified intervals (2) jsonl file containing training infos at specified intervals (3) jsonl file containing validation metrics at specified intervals (4) jsonl file containing test metrics before and after training (5) json file with validation predictions at specified intervals (6) json file with test predictions before and after training (7) trained LM model (8) config json used to run the experiment&lt;/p&gt; &#xA;&lt;p&gt;Complete usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;WANDB_API_KEY=&amp;lt;YOUR-WANDB-API-KEY-HERE&amp;gt;  python scripts/training/train_text_generation.py \&#xA;--config_path &amp;lt;PATH-TO-CONFIG-FILE&amp;gt; \&#xA;--experiment_name &amp;lt;EXPERIMENT-NAME&amp;gt; \&#xA;--base_path_to_store_results &amp;lt;PATH-TO-STORE-RESULTS&amp;gt; \&#xA;--log_to_wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Ramamurthy2022IsRL,&#xA;  title={Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization},&#xA;  author={Rajkumar Ramamurthy and Prithviraj Ammanabrolu and Kiant{\&#39;e} Brantley and Jack Hessel and Rafet Sifa and Christian Bauckhage and Hannaneh Hajishirzi and Yejin Choi},&#xA;  journal={arXiv preprint arXiv:2210.01241},&#xA;  url={https://arxiv.org/abs/2210.01241},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Questions/Discussion/Ideas?&lt;/h1&gt; &#xA;&lt;p&gt;For discussion, questions, ideas exchange, join our slack channel &lt;a href=&#34;https://join.slack.com/t/slack-1sa3880/shared_invite/zt-1idqlnbnm-NIiZeMIOpYReXfX9uIT_PA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&amp;amp;logo=slack&amp;amp;logoColor=white&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>