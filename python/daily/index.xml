<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-13T01:42:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>linyiLYi/voice-assistant</title>
    <updated>2023-12-13T01:42:12Z</updated>
    <id>tag:github.com,2023-12-13:/linyiLYi/voice-assistant</id>
    <link href="https://github.com/linyiLYi/voice-assistant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;一个简单的 Python 脚本，可以通过语音与本地大语言模型进行对话。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;语音助手&lt;/h1&gt; &#xA;&lt;p&gt;一个简单的 Python 脚本，可以通过语音与本地大语言模型进行对话。&lt;/p&gt; &#xA;&lt;h3&gt;macOS 安装指南&lt;/h3&gt; &#xA;&lt;p&gt;以下为 macOS 的安装过程，Windows 与 Linux 可以使用 speech_recognition 与 pyttsx3 来替代下文中的 macOS 的 hear 与 say 指令。&lt;/p&gt; &#xA;&lt;h4&gt;创建环境&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n VoiceAI python=3.11&#xA;conda activate VoiceAI&#xA;pip install langchain&#xA;CMAKE_ARGS=&#34;-DLLAMA_METAL=on&#34; FORCE_CMAKE=1 pip install llama-cpp-python&#xA;&#xA;# 安装音频处理工具&#xA;brew install portaudio&#xA;pip install pyaudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;hear 语音识别模块&lt;/h4&gt; &#xA;&lt;p&gt;使用 &lt;a href=&#34;https://github.com/sveinbjornt/hear&#34;&gt;hear&lt;/a&gt; 指令可以直接调用 macOS 的语音识别模块。注意要开启电脑设置里的键盘听写选项：设置 -&amp;gt; 键盘 -&amp;gt; 听写（开启开关）。&lt;/p&gt; &#xA;&lt;h4&gt;模型文件&lt;/h4&gt; &#xA;&lt;p&gt;模型文件存放于 &lt;code&gt;models/&lt;/code&gt; 文件夹下，在脚本中通过变量 &lt;code&gt;MODEL_PATH&lt;/code&gt; 指定。 推荐下载 TheBloke 的 gguf 格式模型：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Yi-34B-Chat-GGUF/blob/main/yi-34b-chat.Q8_0.gguf&#34;&gt;Yi-34B-Chat-GGUF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/XeIaso/yi-chat-6B-GGUF/tree/main&#34;&gt;Yi-6B-Chat-GGUF，适用小显存平台，尚未测试&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>VikParuchuri/marker</title>
    <updated>2023-12-13T01:42:12Z</updated>
    <id>tag:github.com,2023-12-13:/VikParuchuri/marker</id>
    <link href="https://github.com/VikParuchuri/marker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert PDF to markdown quickly with high accuracy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Marker&lt;/h1&gt; &#xA;&lt;p&gt;Marker converts PDF, EPUB, and MOBI to markdown. It&#39;s 10x faster than nougat, more accurate on most documents, and has low hallucination risk.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for a range of PDF documents (optimized for books and scientific papers)&lt;/li&gt; &#xA; &lt;li&gt;Removes headers/footers/other artifacts&lt;/li&gt; &#xA; &lt;li&gt;Converts most equations to latex&lt;/li&gt; &#xA; &lt;li&gt;Formats code blocks and tables&lt;/li&gt; &#xA; &lt;li&gt;Support for multiple languages (although most testing is done in English). See &lt;code&gt;settings.py&lt;/code&gt; for a language list.&lt;/li&gt; &#xA; &lt;li&gt;Works on GPU, CPU, or MPS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Marker is a pipeline of deep learning models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extract text, OCR if necessary (heuristics, tesseract)&lt;/li&gt; &#xA; &lt;li&gt;Detect page layout (&lt;a href=&#34;https://huggingface.co/vikp/layout_segmenter&#34;&gt;layout segmenter&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/vikp/column_detector&#34;&gt;column detector&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Clean and format each block (heuristics, &lt;a href=&#34;https://huggingface.co/facebook/nougat-base&#34;&gt;nougat&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Combine blocks and postprocess complete text (heuristics, &lt;a href=&#34;https://huggingface.co/vikp/pdf_postprocessor_t5&#34;&gt;pdf_postprocessor&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Relying on autoregressive forward passes to generate text is slow and prone to hallucination/repetition. From the nougat paper: &lt;code&gt;We observed [repetition] in 1.5% of pages in the test set, but the frequency increases for out-of-domain documents.&lt;/code&gt; In my anecdotal testing, repetitions happen on 5%+ of out-of-domain (non-arXiv) pages.&lt;/p&gt; &#xA;&lt;p&gt;Nougat is an amazing model, but I wanted a faster and more general purpose solution. Marker is 10x faster and has low hallucination risk because it only passes equation blocks through an LLM forward pass.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;PDF&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Marker&lt;/th&gt; &#xA;   &lt;th&gt;Nougat&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://greenteapress.com/thinkpython/thinkpython.pdf&#34;&gt;Think Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Textbook&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/thinkpython.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/thinkpython.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://greenteapress.com/thinkos/thinkos.pdf&#34;&gt;Think OS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Textbook&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/thinkos.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/thinkos.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2101.03961.pdf&#34;&gt;Switch Transformers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;arXiv paper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/switch_transformers.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/switch_transformers.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1804.07821.pdf&#34;&gt;Multi-column CNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;arXiv paper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/multicolcnn.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/multicolcnn.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/overall.png&#34; alt=&#34;Benchmark overall&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The above results are with marker and nougat setup so they each take ~3GB of VRAM on an A6000.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/marker/master/#benchmarks&#34;&gt;below&lt;/a&gt; for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;p&gt;PDF is a tricky format, so marker will not always work perfectly. Here are some known limitations that are on the roadmap to address:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Marker will convert fewer equations to latex than nougat. This is because it has to first detect equations, then convert them without hallucation.&lt;/li&gt; &#xA; &lt;li&gt;Whitespace and indentations are not always respected.&lt;/li&gt; &#xA; &lt;li&gt;Not all lines/spans will be joined properly.&lt;/li&gt; &#xA; &lt;li&gt;Only languages similar to English (Spanish, French, German, Russian, etc) are supported. Languages with different character sets (Chinese, Japanese, Korean, etc) are not.&lt;/li&gt; &#xA; &lt;li&gt;This works best on digital PDFs that won&#39;t require a lot of OCR. It&#39;s optimized for speed, and limited OCR is used to fix errors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;This has been tested on Mac and Linux (Ubuntu and Debian). You&#39;ll need python 3.9+ and &lt;a href=&#34;https://python-poetry.org/docs/#installing-with-the-official-installer&#34;&gt;poetry&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First, clone the repo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/VikParuchuri/marker.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd marker&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Linux&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install system requirements &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optional: Install tesseract 5 by following &lt;a href=&#34;https://notesalexp.org/tesseract-ocr/html/&#34;&gt;these instructions&lt;/a&gt; or running &lt;code&gt;scripts/install/tesseract_5_install.sh&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Install ghostscript &amp;gt; 9.55 by following &lt;a href=&#34;https://ghostscript.readthedocs.io/en/latest/Install.html&#34;&gt;these instructions&lt;/a&gt; or running &lt;code&gt;scripts/install/ghostscript_install.sh&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Install other requirements with &lt;code&gt;cat scripts/install/apt-requirements.txt | xargs sudo apt-get install -y&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set the tesseract data folder path &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Find the tesseract data folder &lt;code&gt;tessdata&lt;/code&gt; with &lt;code&gt;find / -name tessdata&lt;/code&gt;. Make sure to use the one corresponding to the latest tesseract version if you have multiple.&lt;/li&gt; &#xA;   &lt;li&gt;Create a &lt;code&gt;local.env&lt;/code&gt; file in the root &lt;code&gt;marker&lt;/code&gt; folder with &lt;code&gt;TESSDATA_PREFIX=/path/to/tessdata&lt;/code&gt; inside it&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install python requirements &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;poetry install&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;poetry shell&lt;/code&gt; to activate your poetry venv&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Update pytorch since poetry doesn&#39;t play nicely with it &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPU only: run &lt;code&gt;pip install torch&lt;/code&gt; to install other torch dependencies.&lt;/li&gt; &#xA;   &lt;li&gt;CPU only: Uninstall torch with &lt;code&gt;poetry remove torch&lt;/code&gt;, then follow the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;CPU install&lt;/a&gt; instructions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Mac&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install system requirements from &lt;code&gt;scripts/install/brew-requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Set the tesseract data folder path &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Find the tesseract data folder &lt;code&gt;tessdata&lt;/code&gt; with &lt;code&gt;brew list tesseract&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Create a &lt;code&gt;local.env&lt;/code&gt; file in the root &lt;code&gt;marker&lt;/code&gt; folder with &lt;code&gt;TESSDATA_PREFIX=/path/to/tessdata&lt;/code&gt; inside it&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install python requirements &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;poetry install&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;poetry shell&lt;/code&gt; to activate your poetry venv&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;First, some configuration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set your torch device in the &lt;code&gt;local.env&lt;/code&gt; file. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt; or &lt;code&gt;TORCH_DEVICE=mps&lt;/code&gt;. &lt;code&gt;cpu&lt;/code&gt; is the default. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If using GPU, set &lt;code&gt;INFERENCE_RAM&lt;/code&gt; to your GPU VRAM (per GPU). For example, if you have 16 GB of VRAM, set &lt;code&gt;INFERENCE_RAM=16&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Depending on your document types, marker&#39;s average memory usage per task can vary slightly. You can configure &lt;code&gt;VRAM_PER_TASK&lt;/code&gt; to adjust this if you notice tasks failing with GPU out of memory errors.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Inspect the other settings in &lt;code&gt;marker/settings.py&lt;/code&gt;. You can override any settings in the &lt;code&gt;local.env&lt;/code&gt; file, or by setting environment variables. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;By default, the final editor model is off. Turn it on with &lt;code&gt;ENABLE_EDITOR_MODEL&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;By default, marker will use ocrmypdf for OCR, which is slower than base tesseract, but higher quality. You can change this with the &lt;code&gt;OCR_ENGINE&lt;/code&gt; setting.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Convert a single file&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;convert_single.py&lt;/code&gt;, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python convert_single.py /path/to/file.pdf /path/to/output.md --parallel_factor 2 --max_pages 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--parallel_factor&lt;/code&gt; is how much to increase batch size and parallel OCR workers by. Higher numbers will take more VRAM and CPU, but process faster. Set to 1 by default.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max_pages&lt;/code&gt; is the maximum number of pages to process. Omit this to convert the entire document.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure the &lt;code&gt;DEFAULT_LANG&lt;/code&gt; setting is set appropriately for your document.&lt;/p&gt; &#xA;&lt;h2&gt;Convert multiple files&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;convert.py&lt;/code&gt;, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python convert.py /path/to/input/folder /path/to/output/folder --workers 10 --max 10 --metadata_file /path/to/metadata.json --min_length 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--workers&lt;/code&gt; is the number of pdfs to convert at once. This is set to 1 by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Parallelism will not increase beyond &lt;code&gt;INFERENCE_RAM / VRAM_PER_TASK&lt;/code&gt; if you&#39;re using GPU.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; is the maximum number of pdfs to convert. Omit this to convert all pdfs in the folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--metadata_file&lt;/code&gt; is an optional path to a json file with metadata about the pdfs. If you provide it, it will be used to set the language for each pdf. If not, &lt;code&gt;DEFAULT_LANG&lt;/code&gt; will be used. The format is:&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--min_length&lt;/code&gt; is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing. If you&#39;re processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;pdf1.pdf&#34;: {&#34;language&#34;: &#34;English&#34;},&#xA;  &#34;pdf2.pdf&#34;: {&#34;language&#34;: &#34;Spanish&#34;},&#xA;  ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Convert multiple files on multiple GPUs&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;chunk_convert.sh&lt;/code&gt;, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MIN_LENGTH=10000 METADATA_FILE=../pdf_meta.json NUM_DEVICES=4 NUM_WORKERS=15 bash chunk_convert.sh ../pdf_in ../md_out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;METADATA_FILE&lt;/code&gt; is an optional path to a json file with metadata about the pdfs. See above for the format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NUM_DEVICES&lt;/code&gt; is the number of GPUs to use. Should be &lt;code&gt;2&lt;/code&gt; or greater.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NUM_WORKERS&lt;/code&gt; is the number of parallel processes to run on each GPU. Per-GPU parallelism will not increase beyond &lt;code&gt;INFERENCE_RAM / VRAM_PER_TASK&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MIN_LENGTH&lt;/code&gt; is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing. If you&#39;re processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;p&gt;Benchmarking PDF extraction quality is hard. I&#39;ve created a test set by finding books and scientific papers that have a pdf version and a latex source. I convert the latex to text, and compare the reference to the output of text extraction methods.&lt;/p&gt; &#xA;&lt;p&gt;Benchmarks show that marker is 10x faster than nougat, and more accurate outside arXiv (nougat was trained on arXiv data). We show naive text extraction (pulling text out of the pdf with no processing) for comparison.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Average Score&lt;/th&gt; &#xA;   &lt;th&gt;Time per page&lt;/th&gt; &#xA;   &lt;th&gt;Time per document&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;naive&lt;/td&gt; &#xA;   &lt;td&gt;0.350727&lt;/td&gt; &#xA;   &lt;td&gt;0.00152378&lt;/td&gt; &#xA;   &lt;td&gt;0.326524&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;marker&lt;/td&gt; &#xA;   &lt;td&gt;0.641062&lt;/td&gt; &#xA;   &lt;td&gt;0.360622&lt;/td&gt; &#xA;   &lt;td&gt;77.2762&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nougat&lt;/td&gt; &#xA;   &lt;td&gt;0.629211&lt;/td&gt; &#xA;   &lt;td&gt;3.77259&lt;/td&gt; &#xA;   &lt;td&gt;808.413&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;First 3 are non-arXiv books, last 3 are arXiv papers.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;switch_trans.pdf&lt;/th&gt; &#xA;   &lt;th&gt;crowd.pdf&lt;/th&gt; &#xA;   &lt;th&gt;multicolcnn.pdf&lt;/th&gt; &#xA;   &lt;th&gt;thinkos.pdf&lt;/th&gt; &#xA;   &lt;th&gt;thinkdsp.pdf&lt;/th&gt; &#xA;   &lt;th&gt;thinkpython.pdf&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;naive&lt;/td&gt; &#xA;   &lt;td&gt;0.244114&lt;/td&gt; &#xA;   &lt;td&gt;0.140669&lt;/td&gt; &#xA;   &lt;td&gt;0.0868221&lt;/td&gt; &#xA;   &lt;td&gt;0.366856&lt;/td&gt; &#xA;   &lt;td&gt;0.412521&lt;/td&gt; &#xA;   &lt;td&gt;0.468281&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;marker&lt;/td&gt; &#xA;   &lt;td&gt;0.482091&lt;/td&gt; &#xA;   &lt;td&gt;0.466882&lt;/td&gt; &#xA;   &lt;td&gt;0.537062&lt;/td&gt; &#xA;   &lt;td&gt;0.754347&lt;/td&gt; &#xA;   &lt;td&gt;0.78825&lt;/td&gt; &#xA;   &lt;td&gt;0.779536&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nougat&lt;/td&gt; &#xA;   &lt;td&gt;0.696458&lt;/td&gt; &#xA;   &lt;td&gt;0.552337&lt;/td&gt; &#xA;   &lt;td&gt;0.735099&lt;/td&gt; &#xA;   &lt;td&gt;0.655002&lt;/td&gt; &#xA;   &lt;td&gt;0.645704&lt;/td&gt; &#xA;   &lt;td&gt;0.650282&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Peak GPU memory usage during the benchmark is &lt;code&gt;3.3GB&lt;/code&gt; for nougat, and &lt;code&gt;3.1GB&lt;/code&gt; for marker. Benchmarks were run on an A6000.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Marker takes about 2GB of VRAM on average per task, so you can convert 24 documents in parallel on an A6000.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/per_doc.png&#34; alt=&#34;Benchmark results&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can benchmark the performance of marker on your machine. First, download the benchmark data &lt;a href=&#34;https://drive.google.com/file/d/1WiN4K2-jQfwyQMe4wSSurbpz3hxo2fG9/view?usp=drive_link&#34;&gt;here&lt;/a&gt; and unzip.&lt;/p&gt; &#xA;&lt;p&gt;Then run &lt;code&gt;benchmark.py&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark.py data/pdfs data/references report.json --nougat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will benchmark marker against other text extraction methods. It sets up batch sizes for nougat and marker to use a similar amount of GPU RAM for each.&lt;/p&gt; &#xA;&lt;p&gt;Omit &lt;code&gt;--nougat&lt;/code&gt; to exclude nougat from the benchmark. I don&#39;t recommend running nougat on CPU, since it is very slow.&lt;/p&gt; &#xA;&lt;h1&gt;Commercial usage&lt;/h1&gt; &#xA;&lt;p&gt;Due to the licensing of the underlying models like layoutlmv3 and nougat, this is only suitable for noncommercial usage.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m building a version that can be used commercially, by stripping out the dependencies below. If you would like to get early access, email me at &lt;a href=&#34;mailto:marker@vikas.sh&#34;&gt;marker@vikas.sh&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here are the non-commercial/restrictive dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LayoutLMv3: CC BY-NC-SA 4.0 . &lt;a href=&#34;https://huggingface.co/microsoft/layoutlmv3-base&#34;&gt;Source&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nougat: CC-BY-NC . &lt;a href=&#34;https://github.com/facebookresearch/nougat&#34;&gt;Source&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyMuPDF - GPL . &lt;a href=&#34;https://pymupdf.readthedocs.io/en/latest/about.html#license-and-copyright&#34;&gt;Source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other dependencies/datasets are openly licensed (doclaynet, byt5), or used in a way that is compatible with commercial usage (ghostscript).&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;This work would not have been possible without amazing open source models and datasets, including (but not limited to):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nougat from Meta&lt;/li&gt; &#xA; &lt;li&gt;Layoutlmv3 from Microsoft&lt;/li&gt; &#xA; &lt;li&gt;DocLayNet from IBM&lt;/li&gt; &#xA; &lt;li&gt;ByT5 from Google&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you to the authors of these models and datasets for making them available to the community!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SqueezeAILab/LLMCompiler</title>
    <updated>2023-12-13T01:42:12Z</updated>
    <id>tag:github.com,2023-12-13:/SqueezeAILab/LLMCompiler</id>
    <link href="https://github.com/SqueezeAILab/LLMCompiler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLMCompiler: An LLM Compiler for Parallel Function Calling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLMCompiler: An LLM Compiler for Parallel Function Calling [&lt;a href=&#34;https://arxiv.org/abs/2312.04511&#34;&gt;Paper&lt;/a&gt;]&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SqueezeAILab/LLMCompiler/main/figs/thumbnail.png&#34; alt=&#34;Thumbnail&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLMCompiler&lt;/strong&gt; is a framework that enables an &lt;em&gt;efficient and effective orchestration of parallel function calling&lt;/em&gt; with LLMs, including both open-source and close-source models, by automatically identifying which tasks can be performed in parallel and which ones are interdependent.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The reasoning capabilities of LLMs enable them to execute multiple function calls, using user-provided functions to overcome their inherent limitations (e.g. knowledge cutoffs, poor arithmetic skills, or lack of access to private data). While multi-function calling allows them to tackle more complex problems, current methods often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. LLMCompiler addresses this by decomposing problems into multiple tasks that can be executed in parallel, thereby efficiently orchestrating multi-function calling. With LLMCompiler, the user specifies the tools along with optional in-context examples, and &lt;strong&gt;LLMCompiler automatically computes an optimized orchestration for the function calls&lt;/strong&gt;. LLMCompiler can be used with open-source models such as LLaMA, as well as OpenAI’s GPT models. Across a range of tasks that exhibit different patterns of parallel function calling, LLMCompiler consistently demonstrated &lt;strong&gt;latency speedup, cost saving, and accuracy improvement&lt;/strong&gt;. For more details, please check out our &lt;a href=&#34;https://arxiv.org/abs/2312.04511&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a conda environment and install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name llmcompiler python=3.10 -y&#xA;conda activate llmcompiler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone and install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/SqueezeAILab/LLMCompiler&#xA;cd LLMCompiler&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Basic Runs&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce the evaluation results in the paper, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_llm_compiler.py --api_key {openai-api-key} --benchmark {benchmark-name} --store {store-path} [--logging] [--stream]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--api_key&lt;/code&gt;: OpenAI API Key&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--benchmark&lt;/code&gt;: Benchmark name. Use &lt;code&gt;hotpotqa&lt;/code&gt;, &lt;code&gt;movie&lt;/code&gt;, and &lt;code&gt;parallelqa&lt;/code&gt; to evaluate LLMCompiler on the HotpotQA, Movie Recommendation, and ParallelQA benchmarks, respectively.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--store&lt;/code&gt;: Path to save the result. Question, true label, prediction, and latency per example will be stored in a JSON format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--logging&lt;/code&gt;: (Optional) Enables logging.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--stream&lt;/code&gt;: (Optional, Recommended) Enables streaming. It improves latency by streaming out tasks from the Planner to the Task Fetching Unit and Executor immediately after their generation, rather than blocking the Executor until all the tasks are generated from the Planner.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After the run is over, you can get the summary of the results by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python evaluate_results.py --file {store-path}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Adding Your Custom Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;To use LLMCompiler on your custom benchmarks or use cases, you only need to provide the functions and their descriptions, as well as example prompts. Please refer to &lt;code&gt;configs/hotpotqa&lt;/code&gt;, &lt;code&gt;configs/movie&lt;/code&gt;, and &lt;code&gt;configs/parallelqa&lt;/code&gt; as examples.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt_prompts.py&lt;/code&gt;: Defines in-context example prompts&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tools.py&lt;/code&gt;: Defines functions (i.e. tools) to use, and their descriptions (i.e. instructions and arguments)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;We are planning to update the following features soon:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for open-source models (e.g. LLaMA-2)&lt;/li&gt; &#xA; &lt;li&gt;Baseline methods we used in the paper&lt;/li&gt; &#xA; &lt;li&gt;Tree-of-Thoughts evaluation we used in the paper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;LLMCompiler has been developed as part of the following paper. We appreciate it if you would please cite the following paper if you found the library useful for your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{kim2023llmcompiler,&#xA;  title={An LLM Compiler for Parallel Function Calling},&#xA;  author={Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael and Keutzer, Kurt and Gholami, Amir},&#xA;  journal={arXiv},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>