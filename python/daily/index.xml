<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-16T01:42:43Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>noprobelm/tempy</title>
    <updated>2023-01-16T01:42:43Z</updated>
    <id>tag:github.com,2023-01-16:/noprobelm/tempy</id>
    <link href="https://github.com/noprobelm/tempy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple, visually pleasing weather report in your terminal.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tempy&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/noprobelm/tempy/main/tempy.png&#34; alt=&#34;tempy&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tempy&lt;/code&gt; will render a simple, visually pleasing weather report for current and near-future conditions to your terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;tempy&lt;/code&gt; does not require an API key, but you can register one for yourself if you wish. See the section on &lt;a href=&#34;https://raw.githubusercontent.com/noprobelm/tempy/main/#api-key-registry&#34;&gt;registering your own API key&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/noprobelm/tempy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Use &lt;code&gt;tempy new york city&lt;/code&gt; to get the current weather for NYC&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Units are imperial by default. Specify the unit system you desire with the &lt;code&gt;-u&lt;/code&gt; or &lt;code&gt;--units&lt;/code&gt; flags: &lt;code&gt;tempy new york city -u metric&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/noprobelm/tempy/main/demo.png&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;api-key-registry&#34;&gt;&lt;/a&gt; OPTIONAL: Register your own API key&lt;/h2&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;tempy&lt;/code&gt; makes requests to the &lt;a href=&#34;https://www.weatherapi.com/&#34;&gt;https://www.weatherapi.com&lt;/a&gt; API endpoint via my proxy server at &lt;a href=&#34;http://www.noprobelm.dev&#34;&gt;http://www.noprobelm.dev&lt;/a&gt;. This is to prevent you from needing to register your own API key.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d rather not make requests through my server, you can always register your own key for free in minutes at &lt;a href=&#34;https://www.weatherapi.com/&#34;&gt;www.weatherapi.com&lt;/a&gt;. Just store your key in &lt;code&gt;$HOME/.config/tempyrc&lt;/code&gt; and &lt;code&gt;tempy&lt;/code&gt; will take care of the rest.&lt;/p&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Credits due to &lt;a href=&#34;https://github.com/willmcgugan&#34;&gt;Will McGugan&lt;/a&gt; and the team at &lt;a href=&#34;https://www.textualize.io/&#34;&gt;textualize&lt;/a&gt; for creating such a wonderfully robust tool for rendering rich text to the terminal using Python.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Copyright © 2023 Jeff Barfield.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>innnky/so-vits-svc</title>
    <updated>2023-01-16T01:42:43Z</updated>
    <id>tag:github.com,2023-01-16:/innnky/so-vits-svc</id>
    <link href="https://github.com/innnky/so-vits-svc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;基于vits与softvc的歌声音色转换模型&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SoftVC VITS Singing Voice Conversion&lt;/h1&gt; &#xA;&lt;h2&gt;English docs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/Eng_docs.md&#34;&gt;英语资料&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;据不完全统计，多说话人似乎会导致&lt;strong&gt;音色泄漏加重&lt;/strong&gt;，不建议训练超过5人的模型，目前的建议是如果想炼出来更像目标音色，&lt;strong&gt;尽可能炼单说话人的&lt;/strong&gt;&lt;br&gt; 断音问题已解决，音质提升了不少&lt;br&gt; 2.0版本已经移至 sovits_2.0分支&lt;br&gt; 3.0版本使用FreeVC的代码结构，与旧版本不通用&lt;br&gt; 与&lt;a href=&#34;https://github.com/prophesier/diff-svc&#34;&gt;DiffSVC&lt;/a&gt; 相比，在训练数据质量非常高时diffsvc有着更好的表现，对于质量差一些的数据集，本仓库可能会有更好的表现，此外，本仓库推理速度上比diffsvc快很多&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;模型简介&lt;/h2&gt; &#xA;&lt;p&gt;歌声音色转换模型，通过SoftVC内容编码器提取源音频语音特征，与F0同时输入VITS替换原本的文本输入达到歌声转换的效果。同时，更换声码器为 &lt;a href=&#34;https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan&#34;&gt;NSF HiFiGAN&lt;/a&gt; 解决断音问题&lt;/p&gt; &#xA;&lt;h2&gt;注意&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;当前分支是32khz版本的分支，32khz模型推理更快，显存占用大幅减小，数据集所占硬盘空间也大幅降低，推荐训练该版本模型&lt;/li&gt; &#xA; &lt;li&gt;如果要训练48khz的模型请切换到&lt;a href=&#34;https://github.com/innnky/so-vits-svc/tree/main&#34;&gt;main分支&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;预先下载的模型文件&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;soft vc hubert：&lt;a href=&#34;https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#34;&gt;hubert-soft-0d54a1f4.pt&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;放在hubert目录下&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;预训练底模文件 &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#34;&gt;G_0.pth&lt;/a&gt; 与 &lt;a href=&#34;https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#34;&gt;D_0.pth&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;放在logs/32k 目录下&lt;/li&gt; &#xA;   &lt;li&gt;预训练底模为必选项，因为据测试从零开始训练有概率不收敛，同时底模也能加快训练速度&lt;/li&gt; &#xA;   &lt;li&gt;预训练底模训练数据集包含云灏 即霜 辉宇·星AI 派蒙 绫地宁宁，覆盖男女生常见音域，可以认为是相对通用的底模&lt;/li&gt; &#xA;   &lt;li&gt;底模删除了optimizer speaker_embedding 等无关权重, 只可以用于初始化训练，无法用于推理&lt;/li&gt; &#xA;   &lt;li&gt;该底模和48khz底模通用&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 一键下载&#xA;# hubert&#xA;wget -P hubert/ https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt&#xA;# G与D预训练模型&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/G_0.pth&#xA;wget -P logs/32k/ https://huggingface.co/innnky/sovits_pretrained/resolve/main/D_0.pth&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;colab一键数据集制作、训练脚本&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1_-gh9i-wCPNlRZw6pYF-9UufetcVrGBX?usp=sharing&#34;&gt;一键colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;数据集准备&lt;/h2&gt; &#xA;&lt;p&gt;仅需要以以下文件结构将数据集放入dataset_raw目录即可&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset_raw&#xA;├───speaker0&#xA;│   ├───xxx1-xxx1.wav&#xA;│   ├───...&#xA;│   └───Lxx-0xx8.wav&#xA;└───speaker1&#xA;    ├───xx2-0xxx2.wav&#xA;    ├───...&#xA;    └───xxx7-xxx007.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;数据预处理&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;重采样至 32khz&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python resample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;自动划分训练集 验证集 测试集 以及自动生成配置文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_flist_config.py&#xA;# 注意&#xA;# 自动生成的配置文件中，说话人数量n_speakers会自动按照数据集中的人数而定&#xA;# 为了给之后添加说话人留下一定空间，n_speakers自动设置为 当前数据集人数乘2&#xA;# 如果想多留一些空位可以在此步骤后 自行修改生成的config.json中n_speakers数量&#xA;# 一旦模型开始训练后此项不可再更改&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;生成hubert与f0&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python preprocess_hubert_f0.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;执行完以上步骤后 dataset 目录便是预处理完成的数据，可以删除dataset_raw文件夹了&lt;/p&gt; &#xA;&lt;h2&gt;训练&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py -c configs/config.json -m 32k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;推理&lt;/h2&gt; &#xA;&lt;p&gt;使用&lt;a href=&#34;https://raw.githubusercontent.com/innnky/so-vits-svc/32k/inference_main.py&#34;&gt;inference_main.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;更改model_path为你自己训练的最新模型记录点&lt;/li&gt; &#xA; &lt;li&gt;将待转换的音频放在raw文件夹下&lt;/li&gt; &#xA; &lt;li&gt;clean_names 写待转换的音频名称&lt;/li&gt; &#xA; &lt;li&gt;trans 填写变调半音数量&lt;/li&gt; &#xA; &lt;li&gt;spk_list 填写合成的说话人名称&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>enhuiz/vall-e</title>
    <updated>2023-01-16T01:42:43Z</updated>
    <id>tag:github.com,2023-01-16:/enhuiz/vall-e</id>
    <link href="https://github.com/enhuiz/vall-e" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unofficial PyTorch implementation of the audio LM VALL-E, WIP&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/enhuiz/vall-e/main/vall-e.png&#34; width=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;VALL-E&lt;/h1&gt; &#xA;&lt;p&gt;An unofficial PyTorch implementation of &lt;a href=&#34;https://valle-demo.github.io/&#34;&gt;VALL-E&lt;/a&gt;, based on the &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec&lt;/a&gt; tokenizer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/enhuiz&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&#34; alt=&#34;&amp;quot;Buy Me A Coffee&amp;quot;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Since the trainer is based on &lt;a href=&#34;https://github.com/microsoft/DeepSpeed.git&#34;&gt;DeepSpeed&lt;/a&gt;, you will need to have a GPU that DeepSpeed has developed and tested against, as well as a CUDA or ROCm compiler pre-installed to install this package.&lt;/p&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/enhuiz/vall-e&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Clone&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/enhuiz/vall-e.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the code is only tested under &lt;code&gt;Python 3.10.7&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Put your data into a folder, e.g. &lt;code&gt;data/your_data&lt;/code&gt;. Audio files should be named with the suffix &lt;code&gt;.wav&lt;/code&gt; and text files with &lt;code&gt;.normalized.txt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Quantize the data:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m vall_e.emb.qnt data/your_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Generate phonemes based on the text:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m vall_e.emb.g2p data/your_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Customize your configuration by creating &lt;code&gt;config/your_data/ar.yml&lt;/code&gt; and &lt;code&gt;config/your_data/nar.yml&lt;/code&gt;. Refer to the example configs in &lt;code&gt;config/test&lt;/code&gt; and &lt;code&gt;vall_e/config.py&lt;/code&gt; for details. You may choose different model presets, check &lt;code&gt;vall_e/vall_e/__init__.py&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the AR or NAR model using the following scripts:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m vall_e.train yaml=config/your_data/ar_or_nar.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AR model for the first quantizer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Audio decoding from tokens&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; NAR model for the rest quantizers&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Trainers for both models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement AdaLN for NAR model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Sample-wise quantization level sampling for NAR training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Pre-trained checkpoint and demos on LibriTTS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notice&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec&lt;/a&gt; is licensed under CC-BY-NC 4.0. If you use the code to generate audio quantization or perform decoding, it is important to adhere to the terms of their license.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2023neural,&#xA;  title={Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers},&#xA;  author={Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others},&#xA;  journal={arXiv preprint arXiv:2301.02111},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{defossez2022highfi,&#xA;  title={High Fidelity Neural Audio Compression},&#xA;  author={Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},&#xA;  journal={arXiv preprint arXiv:2210.13438},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>