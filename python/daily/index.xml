<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-27T01:43:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>stephansturges/WALDO</title>
    <updated>2023-08-27T01:43:14Z</updated>
    <id>tag:github.com,2023-08-27:/stephansturges/WALDO</id>
    <link href="https://github.com/stephansturges/WALDO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Whereabouts Ascertainment for Low-lying Detectable Objects. The SOTA in FOSS AI for drones!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;W.A.L.D.O. Whereabouts Ascertainment for Low-lying Detectable Objects !&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Welcome to the WALDO v2.5 FINAL release! ü•≥ü•≥ü•≥ü•≥&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/stephansturges/WALDO/assets/20320678/3a5ad37c-db34-4d71-8a88-325672427b7a&#34; alt=&#34;fe361d16-c588-47ad-bff4-1c5185c0cd9f&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Thanks to all participants in the beta! I had over 3000 sign-ups for the beta release and iterated really fast... I hope you&#39;ll like the result!&lt;/p&gt; &#xA;&lt;p&gt;I am assuming you have some experience with deployment of AI systems, but if you have any trouble using this release you can contact me at stephan.sturges at gmail&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;WHAT IS WALDO?&lt;/p&gt; &#xA;&lt;p&gt;WALDO is a detection AI model, based on a large YOLO-v7 backbone and my own synthetic data pipeline. The basic model shared here, which is the only one published as FOSS at the moment, is capable of detecting these classes of items in overhead images ranging in altitude from about 30 feet to satellite imagery with a resolution of 50cm per pixel or better.&lt;/p&gt; &#xA;&lt;p&gt;Well trained classes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&#39;car&#39; --&amp;gt; all kinds of civilan cars, including pickup trucks&lt;/li&gt; &#xA; &lt;li&gt;&#39;van&#39; --&amp;gt; all kinds of civilian vans, gets confused with &#34;car&#34; a lot. You might want to fuse them! üöó&lt;/li&gt; &#xA; &lt;li&gt;&#39;truck&#39; --&amp;gt; all kinds of box-trucks, flatbeds or articulated trucks, NOT small pickup trucks üöö&lt;/li&gt; &#xA; &lt;li&gt;&#39;building&#39; --&amp;gt; buildings of all kinds üè£&lt;/li&gt; &#xA; &lt;li&gt;&#39;human&#39; --&amp;gt; people! üßç&lt;/li&gt; &#xA; &lt;li&gt;&#39;gastank&#39;--&amp;gt; cylindrical tanks such as butane tanks and gas expansion tanks, or grain silos ü´ô&lt;/li&gt; &#xA; &lt;li&gt;&#39;digger&#39; --&amp;gt; all kinds of construction vehicles, including tractors and construction gear üöú&lt;/li&gt; &#xA; &lt;li&gt;&#39;container&#39; --&amp;gt; shipping containers, including on the back of an articulated truck&lt;/li&gt; &#xA; &lt;li&gt;&#39;bus&#39; --&amp;gt; a bus üöå&lt;/li&gt; &#xA; &lt;li&gt;&#39;u_pole&#39; --&amp;gt; utility poles, power poles, anything thin and sticking up that you should avoid with a plane üéè&lt;/li&gt; &#xA; &lt;li&gt;&#39;boat&#39; --&amp;gt; boats üö¢&lt;/li&gt; &#xA; &lt;li&gt;&#39;bike&#39; --&amp;gt; bikes, mopeds, motorbikes, all things with 2 wheels üö≤&lt;/li&gt; &#xA; &lt;li&gt;&#39;smoke&#39; --&amp;gt; smoke and fire üî•üî•üî•&lt;/li&gt; &#xA; &lt;li&gt;&#39;solarpanels&#39; --&amp;gt; solar panels&lt;/li&gt; &#xA; &lt;li&gt;&#39;arm/mil&#39; --&amp;gt; this class detects certain types of armored vehicles (very unreliable for now, don&#39;t use it yet)&lt;/li&gt; &#xA; &lt;li&gt;&#39;plane&#39; --&amp;gt; planes (very unreliable for now, probably not worth using yet)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;WHERE IS WALDO?&lt;/p&gt; &#xA;&lt;p&gt;Due to the size of the model files and the constraints of github LFS the files are no longer stored directly on Github, please download the latest package using the link below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bit.ly/3P7UdZ6&#34;&gt;https://bit.ly/3P7UdZ6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;FOR AI NERDS !&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s a big set of YOLOv7 model, trained on my own datasets of synthetic and &#34;augmented&#34; / semi-synthetic data. I&#39;m not going to release the dataset for the time being.&lt;/p&gt; &#xA;&lt;p&gt;The ONNX models are exported for onnx-runtime with a batch-size of 1 and a max input size corresponding to the the network dimensions. They are also set up to export only the top 200 highest-confidence objects in most cases.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m planning to set up a way for people to get the .pt files and the ONNX models with unlimited outputs for people who support further development of the project on Ko-Fi (&lt;a href=&#34;https://ko-fi.com/stephansturges&#34;&gt;https://ko-fi.com/stephansturges&lt;/a&gt;), the goal being to offset some of the cost of training these networks (over 60K USD spent on AWS to date! üòÖ)&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;HOW CAN I START WITH WALDO?&lt;/p&gt; &#xA;&lt;p&gt;Setup the environment with python3:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(optional) create a virtual python env for the project&lt;/li&gt; &#xA; &lt;li&gt;install dependencies using the requirements file: pip install -r requirements.txt&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You may need to install a couple of other bits and pieces depending on your python3 env... If you find anything really blocking send me an email and I&#39;ll update this readme.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;RUN THE MODELS USING THE BOILERPLATE CODE IN /playground:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To run on video: put one or multiple .mp4 files in the ./input_vids subfolder, and copy one or more .onnx model files to the /playground folder, ALL of the videos in the ./input_vids folder will be processed with EACH of the .onnx files that you put in the ./playground folder (useful for comparison of models!)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;...and then run:&lt;/p&gt; &#xA;&lt;p&gt;python3 run_local_network_on_videos_onnxruntime.py&lt;/p&gt; &#xA;&lt;p&gt;This will run the detection network in default settings and save an annotated video to the ./output_vids/ subfolder.&lt;/p&gt; &#xA;&lt;p&gt;You can also use the following command-line arguments:&lt;/p&gt; &#xA;&lt;p&gt;python3 run_local_network_on_videos_onnxruntime.py --frame_limit 3000 --frame_skip 8&lt;/p&gt; &#xA;&lt;p&gt;&#34;frame limit&#34; defines where to stop processing the video, if you only want to test it on the first 1000 frame then use --frame_limit 1000 for example&lt;/p&gt; &#xA;&lt;p&gt;&#34;frame skip&#34; allows you to skip frames to keep processing quicker for testing, so if your video is 30 fps and you only one 1 frame per second to be AI-annotated then you can use --frame_skip 30 for instance&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;To run on a single image of any size:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Put some images in ./images_in/ and run:&lt;/p&gt; &#xA;&lt;p&gt;python3 run_local_network_on_images_onnxruntime.py --model &#34;/path_to_your_preferred_onnx_model.onnx&#34;&lt;/p&gt; &#xA;&lt;p&gt;&#34;model&#34; is a REQUIRED arguemnt which accepts a path, pointing to the ONNX model you want to use to process the files.&lt;/p&gt; &#xA;&lt;p&gt;This will run detection on all images in the input folder and save the annotated output images in the output folder, along with the txt files of the detections in YOLO format.&lt;/p&gt; &#xA;&lt;p&gt;If the image is LARGER than 960x960px format it will be tiled into squares of 960px with a litte overlap for analysis and then merged back together, so you can process huge satellite images for example without needing to split them first.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run the network on a single image that should be processed at native resolution you can use the OPTIONAL &#34;--resize&#34; flag like this:&lt;/p&gt; &#xA;&lt;p&gt;python3 run_local_network_on_images_onnxruntime.py --model &#34;/path_to_your_preferred_onnx_model.onnx&#34; --resize&lt;/p&gt; &#xA;&lt;p&gt;The output can be found in ./images_out/, you&#39;ll get images with pretty overlays and .txt files with the actual detections&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;WHAT IS INCLUDED?&lt;/p&gt; &#xA;&lt;p&gt;In the FOSS package there are a bunch of networks in ONNX format prepared for ONNXruntime, as well as a few examples of networks in other export formats. Only the &#34;V7-base/square/416px&#34; network is included in all formats as part of this release, meaning you get a selection of ONNX exported models including some quantized and prepared for Nvidia TensorRT, and you also have the raw .pt files for the training run so that you can export your own. I also added the base .pt files for the 512px V7 model. These files also exist for each other network (or can be exported), but I&#39;m thinking about how to make those available for people who support the future development of WALDO in order to support the cost of AI model training (which is over 50K $ already up to this point). Reach out to me via email if you want a model / export that isn&#39;t in here!&lt;/p&gt; &#xA;&lt;p&gt;/!\ Some tips for use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In real-world use cases you may want to merge classes 1 &amp;amp; 2 since there this still a lot of confusion between those classes&lt;/li&gt; &#xA; &lt;li&gt;The models are exported with non-maximum-suppression, so if you are using the AI system in cases where objects are occluded by one another you will only get the &#34;most valid&#34; object in most cases.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some of the network that is in this repo is very large, and is meant to be run on an inference server, and some are made for embedding on tiny edge devices... take a look around and find one that works for you!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;GOING DEEPER&lt;/p&gt; &#xA;&lt;p&gt;Of course if you know your way around deploying AI models there is a lot more you do with this release, inclusing:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;There are certain models already released in CoreML format for iOS, give those a try&lt;/li&gt; &#xA; &lt;li&gt;There are some models that are exported for TensorRT, including some cool quantization!&lt;/li&gt; &#xA; &lt;li&gt;For a couple of models the .pt files are included in this release, play with making your own exports or running thos directly using YOLOv7 from &lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;https://github.com/WongKinYiu/yolov7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Get yourself a cool, cheap, little AI camera from Luxonis and run one of the OpenVino blobs that are currently exported for the V7-base/416px network and the V7-tiny/512px network. These are super cool and do excellent AI detections directly on 15g hardware that costs &amp;lt;200$... crazy stuff.&lt;/li&gt; &#xA; &lt;li&gt;Build your own commercial application!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Enjoy!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;PREVIOUS VERSIONS&lt;/p&gt; &#xA;&lt;p&gt;You can find the repo with WALDO v1.0 here: &lt;a href=&#34;https://github.com/stephansturges/WALDO&#34;&gt;https://github.com/stephansturges/WALDO&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;CAN YOU HELP ME WITH X?&lt;/p&gt; &#xA;&lt;p&gt;Sure, email me at &lt;a href=&#34;mailto:stephan.sturges@gmail.com&#34;&gt;stephan.sturges@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;DETECTION OF X ISN&#39;T WORKING AS EXPECTED:&lt;/p&gt; &#xA;&lt;p&gt;I&#39;d love to see example images, videos, sample data, etc at: &lt;a href=&#34;mailto:stephan.sturges@gmail.com&#34;&gt;stephan.sturges@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;HOW DOES AIRCORTEX MAKE MONEY?&lt;/p&gt; &#xA;&lt;p&gt;Aircortex&#39; mission statement is to make the SOTA in ground-risk AI and sensing, and to make the basic models free and easy to use for both hobbyists and professionals in the UAV / AAM industry, to acclerate safe access to the skies in the 21st century.&lt;/p&gt; &#xA;&lt;p&gt;Aircortex is an &#34;open-core&#34; AI company: the basic model is completely free and open-source for anyone to use including in commercial products.&lt;/p&gt; &#xA;&lt;p&gt;I make money by charging for:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;help with training additional detection classes,&lt;/li&gt; &#xA; &lt;li&gt;retraining for your specific hardware,&lt;/li&gt; &#xA; &lt;li&gt;building the software stack to support specific deployment cases,&lt;/li&gt; &#xA; &lt;li&gt;helping companies set up the right hardware architecture for AI integration,&lt;/li&gt; &#xA; &lt;li&gt;custom hardware setups for specific environments&lt;/li&gt; &#xA; &lt;li&gt;more &#34;feature-complete&#34; versions of my FOSS products such as integrating 3D perception etc...&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Contact me at &lt;a href=&#34;mailto:stephan.sturges@gmail.com&#34;&gt;stephan.sturges@gmail.com&lt;/a&gt; to find out more.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;SUPPORT WALDO!&lt;/p&gt; &#xA;&lt;p&gt;Training this base model took about 3 months of work and ~20K$ in cloud compute. If you find value in it, please support development of the next version on: &lt;a href=&#34;https://ko-fi.com/stephansturges&#34;&gt;https://ko-fi.com/stephansturges&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also sign-up there to be a sponsor of WALDO for 500$ / month and get early access to future models.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;/_&lt;em&gt;&lt;strong&gt;//_&lt;/strong&gt;&lt;/em&gt;//_&lt;em&gt;&lt;strong&gt;//_&lt;/strong&gt;&lt;/em&gt;//_&lt;em&gt;&lt;strong&gt;//_&lt;/strong&gt;&lt;/em&gt;//_&lt;em&gt;&lt;strong&gt;//_&lt;/strong&gt;&lt;/em&gt;//_&lt;em&gt;&lt;strong&gt;//_&lt;/strong&gt; /&lt;/em&gt;&lt;em&gt;&lt;strong&gt;//&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;//&lt;/em&gt;&lt;em&gt;&lt;strong&gt;//&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;//&lt;/em&gt;&lt;em&gt;&lt;strong&gt;//&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;//&lt;/em&gt;&lt;em&gt;&lt;strong&gt;//&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;//&lt;/em&gt;&lt;em&gt;&lt;strong&gt;//&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;h2&gt;Unless otherwise specified all code in this release is published with the licence conditions below.&lt;/h2&gt; &#xA;&lt;p&gt;MIT License&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2023 Stephan Sturges / Aircortex.com&lt;/p&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/codellama</title>
    <updated>2023-08-27T01:43:14Z</updated>
    <id>tag:github.com,2023-08-27:/facebookresearch/codellama</id>
    <link href="https://github.com/facebookresearch/codellama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for CodeLlama models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introducing Code Llama&lt;/h1&gt; &#xA;&lt;p&gt;Code Llama is a family of large language models for code based on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2&lt;/a&gt; providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model training, architecture and parameters, evaluations, responsible AI and safety refer to our &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;research paper&lt;/a&gt;. Output generated by code generation features of the Llama Materials, including Code Llama, may be subject to third party licenses, including, without limitation, open source licenses.&lt;/p&gt; &#xA;&lt;p&gt;We are unlocking the power of large language models and our latest version of Code Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. This release includes model weights and starting code for pretrained and fine-tuned Llama language models ‚Äî ranging from 7B to 34B parameters.&lt;/p&gt; &#xA;&lt;p&gt;This repository is intended as a minimal example to load &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama&lt;/a&gt; models and run inference.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;In order to download the model weights and tokenizers, please visit the &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;Meta AI website&lt;/a&gt; and accept our License.&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, &lt;strong&gt;do not use the &#39;Copy link address&#39; option&lt;/strong&gt; when you right click the URL. If the copied URL text starts with: &lt;a href=&#34;https://download.llamameta.net&#34;&gt;https://download.llamameta.net&lt;/a&gt;, you copied it correctly. If the copied URL text starts with: &lt;a href=&#34;https://l.facebook.com&#34;&gt;https://l.facebook.com&lt;/a&gt;, you copied it the wrong way.&lt;/p&gt; &#xA;&lt;p&gt;Pre-requisites: make sure you have &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;md5sum&lt;/code&gt; installed. Then to run the script: &lt;code&gt;bash download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;, you can always re-request a link.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;In a conda env with PyTorch / CUDA available, clone the repo and run in the top-level directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Different models require different model-parallel (MP) values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; values. So set those according to your hardware and use-case.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Code Models&lt;/h3&gt; &#xA;&lt;p&gt;The Code Llama and Code Llama - Python models are not fine-tuned to follow instructions. They should be prompted so that the expected answer is the natural continuation of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_completion.py&lt;/code&gt; for some examples. To illustrate, see command below to run it with the &lt;code&gt;CodeLlama-7b&lt;/code&gt; model (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_completion.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 128 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained code models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt;, &lt;code&gt;CodeLlama-13b&lt;/code&gt;, &lt;code&gt;CodeLlama-34b&lt;/code&gt; and the Code Llama - Python models &lt;code&gt;CodeLlama-7b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Python&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Code Infilling&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama and Code Llama - Instruct 7B and 13B models are capable of filling in code given the surrounding context.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_infilling.py&lt;/code&gt; for some examples. The &lt;code&gt;CodeLlama-7b&lt;/code&gt; model can be run for infilling with the command below (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_infilling.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 192 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained infilling models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt; and &lt;code&gt;CodeLlama-13b&lt;/code&gt; and the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuned Instruction Models&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for them, a specific formatting defined in &lt;a href=&#34;https://github.com/facebookresearch/codellama/raw/main/llama/generation.py#L212&#34;&gt;&lt;code&gt;chat_completion&lt;/code&gt;&lt;/a&gt; needs to be followed, including the &lt;code&gt;INST&lt;/code&gt; and &lt;code&gt;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/code&gt; tags, &lt;code&gt;BOS&lt;/code&gt; and &lt;code&gt;EOS&lt;/code&gt; tokens, and the whitespaces and linebreaks in between (we recommend calling &lt;code&gt;strip()&lt;/code&gt; on inputs to avoid double-spaces).&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/raw/main/inference/inference.py&#34;&gt;an example&lt;/a&gt; of how to add a safety checker to the inputs and outputs of your inference code.&lt;/p&gt; &#xA;&lt;p&gt;Examples using &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_instructions.py \&#xA;    --ckpt_dir CodeLlama-7b-Instruct/ \&#xA;    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fine-tuned instruction-following models are: the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Code Llama is a new technology that carries potential risks with use. Testing conducted to date has not ‚Äî and could not ‚Äî cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research papers as well.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software ‚Äúbug,‚Äù or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;http://github.com/facebookresearch/codellama&#34;&gt;github.com/facebookresearch/codellama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/codellama/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt; for the model card of Code Llama.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, as well as our accompanying &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/USE_POLICY.md&#34;&gt;Acceptable Use Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama Research Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/blog/code-llama-large-language-model-coding/&#34;&gt;Code Llama Blog Post&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>zauberzeug/nicegui</title>
    <updated>2023-08-27T01:43:14Z</updated>
    <id>tag:github.com,2023-08-27:/zauberzeug/nicegui</id>
    <link href="https://github.com/zauberzeug/nicegui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create web-based user interfaces with Python. The nice way.&lt;/p&gt;&lt;hr&gt;&lt;a href=&#34;http://nicegui.io/#about&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zauberzeug/nicegui/main/sceenshots/ui-elements-narrow.png&#34; width=&#34;200&#34; align=&#34;right&#34; alt=&#34;Try online!&#34;&gt; &lt;/a&gt; &#xA;&lt;h1&gt;NiceGUI&lt;/h1&gt; &#xA;&lt;p&gt;NiceGUI is an easy-to-use, Python-based UI framework, which shows up in your web browser. You can create buttons, dialogs, Markdown, 3D scenes, plots and much more.&lt;/p&gt; &#xA;&lt;p&gt;It is great for micro web apps, dashboards, robotics projects, smart home solutions and similar use cases. You can also use it in development, for example when tweaking/configuring a machine learning algorithm or tuning motor controllers.&lt;/p&gt; &#xA;&lt;p&gt;NiceGUI is available as &lt;a href=&#34;https://pypi.org/project/nicegui/&#34;&gt;PyPI package&lt;/a&gt;, &lt;a href=&#34;https://hub.docker.com/r/zauberzeug/nicegui&#34;&gt;Docker image&lt;/a&gt; and on &lt;a href=&#34;https://anaconda.org/conda-forge/nicegui&#34;&gt;conda-forge&lt;/a&gt; as well as &lt;a href=&#34;https://github.com/zauberzeug/nicegui&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/nicegui/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/nicegui?color=dark-green&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/nicegui/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/nicegui?color=dark-green&#34; alt=&#34;PyPI downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/nicegui&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/v/conda-forge/nicegui?color=green&amp;amp;label=conda-forge&#34; alt=&#34;Conda version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/nicegui&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/dn/conda-forge/nicegui?color=green&amp;amp;label=downloads&#34; alt=&#34;Conda downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/zauberzeug/nicegui&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/zauberzeug/nicegui&#34; alt=&#34;Docker pulls&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/zauberzeug/nicegui/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/zauberzeug/nicegui?color=orange&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zauberzeug/nicegui/graphs/commit-activity&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/zauberzeug/nicegui&#34; alt=&#34;GitHub commit activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zauberzeug/nicegui/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/zauberzeug/nicegui?color=blue&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zauberzeug/nicegui/network&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/zauberzeug/nicegui&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zauberzeug/nicegui/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zauberzeug/nicegui&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;browser-based graphical user interface&lt;/li&gt; &#xA; &lt;li&gt;implicit reload on code change&lt;/li&gt; &#xA; &lt;li&gt;acts as webserver (accessed by the browser) or in native mode (eg. desktop window)&lt;/li&gt; &#xA; &lt;li&gt;standard GUI elements like label, button, checkbox, switch, slider, input, file upload, ...&lt;/li&gt; &#xA; &lt;li&gt;simple grouping with rows, columns, cards and dialogs&lt;/li&gt; &#xA; &lt;li&gt;general-purpose HTML and Markdown elements&lt;/li&gt; &#xA; &lt;li&gt;powerful high-level elements to &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;plot graphs and charts,&lt;/li&gt; &#xA;   &lt;li&gt;render 3D scenes,&lt;/li&gt; &#xA;   &lt;li&gt;get steering events via virtual joysticks&lt;/li&gt; &#xA;   &lt;li&gt;annotate and overlay images&lt;/li&gt; &#xA;   &lt;li&gt;interact with tables&lt;/li&gt; &#xA;   &lt;li&gt;navigate foldable tree structures&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;built-in timer to refresh data in intervals (even every 10 ms)&lt;/li&gt; &#xA; &lt;li&gt;straight-forward data binding and refreshable functions to write even less code&lt;/li&gt; &#xA; &lt;li&gt;notifications, dialogs and menus to provide state of the art user interaction&lt;/li&gt; &#xA; &lt;li&gt;shared and individual web pages&lt;/li&gt; &#xA; &lt;li&gt;easy-to-use per-user and general persistence&lt;/li&gt; &#xA; &lt;li&gt;ability to add custom routes and data responses&lt;/li&gt; &#xA; &lt;li&gt;capture keyboard input for global shortcuts etc.&lt;/li&gt; &#xA; &lt;li&gt;customize look by defining primary, secondary and accent colors&lt;/li&gt; &#xA; &lt;li&gt;live-cycle events and session data&lt;/li&gt; &#xA; &lt;li&gt;runs in Jupyter Notebooks and allows Python&#39;s interactive mode&lt;/li&gt; &#xA; &lt;li&gt;auto-complete support for Tailwind CSS&lt;/li&gt; &#xA; &lt;li&gt;SVG, Base64 and emoji favicon support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m pip install nicegui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Write your nice GUI in a file &lt;code&gt;main.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nicegui import ui&#xA;&#xA;ui.label(&#39;Hello NiceGUI!&#39;)&#xA;ui.button(&#39;BUTTON&#39;, on_click=lambda: ui.notify(&#39;button was pressed&#39;))&#xA;&#xA;ui.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The GUI is now available through &lt;a href=&#34;http://localhost:8080/&#34;&gt;http://localhost:8080/&lt;/a&gt; in your browser. Note: NiceGUI will automatically reload the page when you modify the code.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation and Examples&lt;/h2&gt; &#xA;&lt;p&gt;The documentation is hosted at &lt;a href=&#34;https://nicegui.io/documentation&#34;&gt;https://nicegui.io/documentation&lt;/a&gt; and provides plenty of live demos. The whole content of &lt;a href=&#34;https://nicegui.io&#34;&gt;https://nicegui.io&lt;/a&gt; is &lt;a href=&#34;https://github.com/zauberzeug/nicegui/raw/main/main.py&#34;&gt;implemented with NiceGUI itself&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may also have a look at our &lt;a href=&#34;https://github.com/zauberzeug/nicegui/tree/main/examples&#34;&gt;in-depth examples&lt;/a&gt; of what you can do with NiceGUI. In our wiki we have a list of great &lt;a href=&#34;https://github.com/zauberzeug/nicegui/wiki#community-projects&#34;&gt;NiceGUI projects from the community&lt;/a&gt;, a section with &lt;a href=&#34;https://github.com/zauberzeug/nicegui/wiki#tutorials&#34;&gt;Tutorials&lt;/a&gt;, a growing list of &lt;a href=&#34;https://github.com/zauberzeug/nicegui/wiki/FAQs&#34;&gt;FAQs&lt;/a&gt; and &lt;a href=&#34;https://github.com/zauberzeug/nicegui/wiki#chatgpt&#34;&gt;some strategies for using ChatGPT / LLMs to get help about NiceGUI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why?&lt;/h2&gt; &#xA;&lt;p&gt;We at &lt;a href=&#34;https://zauberzeug.com&#34;&gt;Zauberzeug&lt;/a&gt; like &lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt; but find it does &lt;a href=&#34;https://github.com/zauberzeug/nicegui/issues/1#issuecomment-847413651&#34;&gt;too much magic&lt;/a&gt; when it comes to state handling. In search for an alternative nice library to write simple graphical user interfaces in Python we discovered &lt;a href=&#34;https://justpy.io/&#34;&gt;JustPy&lt;/a&gt;. Although we liked the approach, it is too &#34;low-level HTML&#34; for our daily usage. But it inspired us to use &lt;a href=&#34;https://vuejs.org/&#34;&gt;Vue&lt;/a&gt; and &lt;a href=&#34;https://quasar.dev/&#34;&gt;Quasar&lt;/a&gt; for the frontend.&lt;/p&gt; &#xA;&lt;p&gt;We have built on top of &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt;, which itself is based on the ASGI framework &lt;a href=&#34;https://www.starlette.io/&#34;&gt;Starlette&lt;/a&gt; and the ASGI webserver &lt;a href=&#34;https://www.uvicorn.org/&#34;&gt;Uvicorn&lt;/a&gt; because of their great performance and ease of use.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for your interest in contributing to NiceGUI! We are thrilled to have you on board and appreciate your efforts to make this project even better.&lt;/p&gt; &#xA;&lt;p&gt;As a growing open-source project, we understand that it takes a community effort to achieve our goals. That&#39;s why we welcome all kinds of contributions, no matter how small or big they are. Whether it&#39;s adding new features, fixing bugs, improving documentation, or suggesting new ideas, we believe that every contribution counts and adds value to our project.&lt;/p&gt; &#xA;&lt;p&gt;We have provided a detailed guide on how to contribute to NiceGUI in our &lt;a href=&#34;https://github.com/zauberzeug/nicegui/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file. We encourage you to read it carefully before making any contributions to ensure that your work aligns with the project&#39;s goals and standards.&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions or need help with anything, please don&#39;t hesitate to reach out to us. We are always here to support and guide you through the contribution process.&lt;/p&gt; &#xA;&lt;h3&gt;Included Web Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/zauberzeug/nicegui/raw/main/DEPENDENCIES.md&#34;&gt;DEPENDENCIES.md&lt;/a&gt; for a list of web frameworks NiceGUI depends on.&lt;/p&gt;</summary>
  </entry>
</feed>