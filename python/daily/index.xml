<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-28T01:36:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>diffgram/diffgram</title>
    <updated>2022-07-28T01:36:45Z</updated>
    <id>tag:github.com,2022-07-28:/diffgram/diffgram</id>
    <link href="https://github.com/diffgram/diffgram" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Training Data (Data Labeling, Annotation, Workflow) for all Data Types (Image, Video, 3D, Text, Geo, Audio, more) at scale.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/DiffgramLogoVECTOR.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://diffgram.com/&#34;&gt;Diffgram.com&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://join.slack.com/t/diffgram-workspace/shared_invite/zt-twn6529v-hhSPzpQrAxvoZB95PhfAFg&#34;&gt;Join Slack Community&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/slack.PNG&#34; alt=&#34;&#34;&gt; ‚Ä¢ &lt;a href=&#34;https://diffgram.com/main/enterprise&#34;&gt;Enterprise&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://twitter.com/diffgram&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Support &amp;amp; Community&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/diffgram/diffgram/issues&#34;&gt;Open an issue&lt;/a&gt; (Technical, bugs, etc)&lt;/li&gt; &#xA; &lt;li&gt;üòç &lt;a href=&#34;https://join.slack.com/t/diffgram-workspace/shared_invite/zt-twn6529v-hhSPzpQrAxvoZB95PhfAFg&#34;&gt;Join us on slack!&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Open Source Training Data Platform&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/diffgram_high_level.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Modern Training Data platform for machine learning delivered as a single application.&lt;/p&gt; &#xA;&lt;p&gt;Open Source Data Labeling, Workflow, Automation, Exploring, Streaming, and so much more!&lt;/p&gt; &#xA;&lt;p&gt;Watch a high level &lt;a href=&#34;https://www.youtube.com/watch?v=dws6J3bDbcU&#34;&gt;video explanation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;New 1st Class AI/ML Integrations&lt;/h1&gt; &#xA;&lt;p&gt;Integrate and share your machine learning. Event driven backed by RabbitMQ. Learn more &lt;a href=&#34;https://diffgram.readme.io/docs/workflows&#34;&gt;Workflow&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hugging Face&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/hugging-face-zero-shot&#34;&gt;Hugging Face Zero Shot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deepchecks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/deepchecks-image-properties-outliers&#34;&gt;Deepchecks Image Properties Outliers &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build your Own&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/developing-you-custom-actions&#34;&gt;Custom Actions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Annotate Anything - Images, Video, 3D, Text, Geo, Audio And more&lt;/h1&gt; &#xA;&lt;h2&gt;Images&lt;/h2&gt; &#xA;&lt;p&gt;Box, Polygons, Lines, &lt;a href=&#34;https://diffgram.readme.io/docs/keypoints-annotation-type&#34;&gt;Keypoints&lt;/a&gt;, Classification Tags, Quadratic Curves, Cuboids, Segmentation, and More&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/autoborder.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Video&lt;/h2&gt; &#xA;&lt;p&gt;Long, High Frame Rate, High Resolution Videos. &lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/video_general.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;3D&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/3d-lidar-annotation-guide&#34;&gt;3D Labeling Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/3d_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Text&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/text-annotation-guide&#34;&gt;Text Labeling docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Named Entity Recognition, Part of Speech Tagging, Coreference Resolution, Dependency Parsing&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/diffgram_text_interface.png&#34; alt=&#34;Diffgram Text Interface&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Audio&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/audio-annotation-guide&#34;&gt;Audio Labeling docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Annotate Audio Regions available now,&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/diffgram_audio_interface.png&#34; alt=&#34;Diffgram Audio Interface&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Audios Transcription (coming soon)&lt;/p&gt; &#xA;&lt;h2&gt;Geospatial &amp;amp; Tiled Imagery&lt;/h2&gt; &#xA;&lt;p&gt;Support for COG (Cloud Optimized GeoTIFF), streaming, multi-layer, standard and cloud-optimized.&lt;/p&gt; &#xA;&lt;p&gt;Alpha Release: &lt;a href=&#34;https://diffgram.readme.io/docs/geospatial-annotation&#34;&gt;Geospatial labeling docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/diffgram_geospatial_interface.png&#34; alt=&#34;Diffgram Geospatial Interface&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;h3&gt;More&lt;/h3&gt; &#xA;&lt;p&gt;Build your own UI or &lt;a href=&#34;https://diffgram.com/main/contact&#34;&gt;contact us&lt;/a&gt;. Our intent is to build and cover all major media types in 2022, including timeseries, DICOM, and more.&lt;/p&gt; &#xA;&lt;h1&gt;Manage all of your training data&lt;/h1&gt; &#xA;&lt;p&gt;Manage multiple Schemas, Users, Datasets, Process, and so much more.&lt;/p&gt; &#xA;&lt;h2&gt;Customize Everything&lt;/h2&gt; &#xA;&lt;p&gt;With Diffgram you can get the exact branded experience you want through the what-you-see-is-what-you-get editor. Whitelabel UI Layout &amp;amp; Branding, Automations, Schema, Geometry, Processes, Pipelines, Queries, and More. Diffgram is the most customizable training data platform. &lt;a href=&#34;https://diffgram.readme.io/docs/customization&#34;&gt;Training Data Customization&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cybersecurity&lt;/h2&gt; &#xA;&lt;p&gt;How secure is your training data? &lt;a href=&#34;https://diffgram.readme.io/docs/cybersecurity-101&#34;&gt;Learn more about Cybersecurity for Training Data&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://diffgram.readme.io/docs/security-policies&#34;&gt;Security Policies&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Migration&lt;/h1&gt; &#xA;&lt;h2&gt;Labelbox to Diffgram&lt;/h2&gt; &#xA;&lt;p&gt;Are you getting great value from Labelbox? &lt;a href=&#34;https://anthony-sarkis.medium.com/considering-labelbox-consider-diffgram-too-fe7a7b8ee8d7&#34;&gt;Labelbox vs Diffgram&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/1-click-migration-from-labelbox&#34;&gt;One Click Migration from Labelbox&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Labelstudio to Diffgram&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.com/main/best-open-source-data-labeling-2022&#34;&gt;Learn about upgrading to Diffgram&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;SuperAnnotate to Diffgram&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.com/main/contact&#34;&gt;Contact us&lt;/a&gt; to request prioritization of the automatic migration.&lt;/p&gt; &#xA;&lt;h1&gt;What is Training Data?&lt;/h1&gt; &#xA;&lt;p&gt;Training Data is the art of supervising machines through data. This includes the activities of annotation, which produces structured data; ready to be consumed by a machine learning model. Annotation is required because raw media is considered to be unstructured and not usable without it. That‚Äôs why training data is required for many modern machine learning use cases including computer vision, natural language processing and speech recognition.&lt;/p&gt; &#xA;&lt;h1&gt;What is Diffgram?&lt;/h1&gt; &#xA;&lt;p&gt;Diffgram is multiple training data tools in one single application. &lt;a href=&#34;https://diffgram.readme.io/docs/what-is-diffgram&#34;&gt;What is Diffgram&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Diffgram is Open Source and optionally Client Installed. &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/#Quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/#Who-is-Diffgram-for&#34;&gt;Who is Diffgram for?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/#Why-Diffgram&#34;&gt;Why Diffgram?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/#What-are-Diffgrams-competitive-advantages&#34;&gt;What are Diffgram&#39;s competitive advantages?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/#Features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/#Standard-Features&#34;&gt;Standard Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/scale&#34;&gt;Built for Scale&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Who is Diffgram for?&lt;/h3&gt; &#xA;&lt;p&gt;Data Engineers, Machine Learning Leaders, AI Experts, Software Engineers, Data Scientists, Data Annotators and Subject Matter Experts.&lt;/p&gt; &#xA;&lt;h3&gt;New to Training Data?&lt;/h3&gt; &#xA;&lt;p&gt;Learn more about the general concepts with the &lt;a href=&#34;https://www.oreilly.com/library/view/training-data-for/9781492094517/&#34;&gt;Training Data Book&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Why Diffgram?&lt;/h3&gt; &#xA;&lt;p&gt;Diffgram brings the functions of a complex toolchain directly into one application. Providing multiple tools with one single integrated application.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/diffgram_solution.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Enterprise Questions? Please &lt;a href=&#34;https://diffgram.com/contact&#34;&gt;contact us&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Security issues: Do not create a public issue. Email &lt;a href=&#34;mailto:security@diffgram.com&#34;&gt;security@diffgram.com&lt;/a&gt; with the details. &lt;a href=&#34;https://diffgram.readme.io/docs&#34;&gt;Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Online Playground&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.com/user/data_platform/new&#34;&gt;Try Diffgram Online&lt;/a&gt; (Hosted Service, No Setup.)&lt;/p&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/install&#34;&gt;Install Diffgram&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/updating-an-existing-installation&#34;&gt;Updating Existing Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/quickstart-installation-of-diffgram-open-core&#34;&gt;Development Install Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/open-installation-production&#34;&gt;Production Install Docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Read also our &lt;a href=&#34;https://diffgram.readme.io/docs/open-core-docker-install-cheatsheet&#34;&gt;Docker compose commands cheat-sheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Bugs and Issues&lt;/h2&gt; &#xA;&lt;p&gt;If you see any missing features, bugs etc please report them ASAP to &lt;a href=&#34;https://github.com/diffgram/diffgram/issues&#34;&gt;diffgram/issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://diffgram.readme.io/docs/developer-contribution-guide&#34;&gt;Contribution Guide&lt;/a&gt; for more. &lt;a href=&#34;https://diffgram.readme.io/docs/help-im-new-what-is-diffgram-exactly&#34;&gt;More on Understanding Diffgram High Level&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Cloud&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/github_assets/cloud_logos.png&#34; alt=&#34;Cloud logos&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Full support for Amazon AWS, Google Cloud, Microsoft Azure, and MinIO.&lt;/p&gt; &#xA;&lt;p&gt;Run Diffgram on and access data from any of the clouds.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/minio&#34;&gt;MinIO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/diffgram/tutorial-install-diffgram-in-google-compute-engine-134aae7d8a9b&#34;&gt;Google GCP Install Guide Compute Engine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/diffgram/tutorial-installing-diffgram-on-azure-aks-b9447685e271&#34;&gt;Azure AKS Kubernetes Install Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://get.diffgram.com/kubernetes-install-guide-aws-amazon-elastic-kubernetes-service-k8s-helm-install-vpc-on-premise/&#34;&gt;AWS Full Kubernetes Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/diffgram/diffgram-helm&#34;&gt;Helm Chart for Kubernetes Clusters&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What is Diffgram a drop in replacement for?&lt;/h3&gt; &#xA;&lt;p&gt;Diffgram is a drop in replacement for the following systems: Labelbox, CVAT, SuperAnnotate, Label Studio (Heartex), V7 Labs (Darwin), BasicAI, SuperbAI, Kili-Technology, Cord, HastyAI, Dataloop, Keymakr, Scale Nucleus.&lt;/p&gt; &#xA;&lt;p&gt;Please see the roadmap and talk with us if you see a missing feature.&lt;/p&gt; &#xA;&lt;h3&gt;How much does this cost? What&#39;s your business model?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/diffgram-versions-open-source-premium-enterprise&#34;&gt;Compare Diffgram Versions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Premium Support&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/diffgram-versions-open-source-premium-enterprise#premium-licenses-with-open-source&#34;&gt;Learn more.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Enterprise&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.com/main/enterprise&#34;&gt;Enterprise Edition&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;p&gt;This is an ACTIVE project. We are very open to feedback and encourage you to create &lt;a href=&#34;https://github.com/diffgram/diffgram/issues&#34;&gt;Issues&lt;/a&gt; and help us grow!&lt;/p&gt; &#xA;&lt;h2&gt;User Friendly&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NEW Streamlined Annotation UI suitable both from &#34;First Time&#34; Subject Matter Experts, and powerful options for Professional Full Time Annotators&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Standard Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Many User Labeling - Designed for many users from Day 1.&lt;/li&gt; &#xA; &lt;li&gt;Scale to Mega Projects with sophisticated organizational concepts.&lt;/li&gt; &#xA; &lt;li&gt;Fully configurable - customize labels, attributes, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Ingest&lt;/h2&gt; &#xA;&lt;p&gt;Ingest prediction data without writing extra scripts.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/diffgram/now-anyone-can-turn-spreadsheets-into-editable-pre-labels-more-saving-hundreds-of-hours-of-bbc756ec7b49&#34;&gt;NEW Import Wizard&lt;/a&gt; saves you hours having to map your data (pre-labels, QA, debug etc.).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=w7yiW5wpnMg&amp;amp;t=59s&#34;&gt;All-Cloud Integrated File Browser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Scalable pipeline for massive ingestion - we have tested to 600+ hardware nodes&lt;/li&gt; &#xA; &lt;li&gt;Integrated pipeline hooks - newly added data auto creates tasks and more&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Store&lt;/h2&gt; &#xA;&lt;p&gt;Collaboration across teams between machine learning, product, ops, managers, and more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Store virtually &lt;a href=&#34;https://diffgram.readme.io/docs/scale&#34;&gt;any scale&lt;/a&gt; of dataset and instantly access slices of the data to avoid having to download/unzip/load.&lt;/li&gt; &#xA; &lt;li&gt;Fast access to datasets from multiple machines. Have multiple Data Scientists working on the same data.&lt;/li&gt; &#xA; &lt;li&gt;Integrates with your tools and 3rd party workforces. &lt;a href=&#34;https://raw.githubusercontent.com/diffgram/diffgram/master/#integrations&#34;&gt;Integrations&lt;/a&gt; It&#39;s a database for your training data, both metadata and access of raw BLOB data (over top of your storage choice).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;QA &amp;amp; Human Tasks&lt;/h2&gt; &#xA;&lt;p&gt;Manage Annotation Workflow, Tasks, Quality Assurance and more. Task features can be used as modules within &lt;a href=&#34;https://diffgram.readme.io/docs/workflows&#34;&gt;Workflow&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;QA Features including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;QA Slideshow: Reduce Costly Errors&lt;/li&gt; &#xA; &lt;li&gt;Reduce Context Switching Costs with Discussions &amp;amp; Issue Tracking&lt;/li&gt; &#xA; &lt;li&gt;Get New Team Members Certified with Training and Exams&lt;/li&gt; &#xA; &lt;li&gt;Hold People Accountable with Per User Reporting&lt;/li&gt; &#xA; &lt;li&gt;Reduce Human Errors with Human Centered Tasks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more -&amp;gt; &lt;a href=&#34;https://diffgram.readme.io/docs/measure-data-quality-annotations-more&#34;&gt;Quality Assurance Features&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatic Per Task Review Routing, with configurable review chance&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.com/streaming&#34;&gt;Human Task Pipelines&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Webhooks with &lt;a href=&#34;https://diffgram.readme.io/docs/setting-up-webhooks&#34;&gt;Actions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Easily annotate a single dataset, or scale to hundreds of projects with thousands of subdivided &lt;a href=&#34;https://diffgram.readme.io/docs/tasks-introduction&#34;&gt;task sets&lt;/a&gt;. Includes easy search and filtering.&lt;/li&gt; &#xA; &lt;li&gt;Fully integrated customizable Annotation &lt;a href=&#34;https://diffgram.readme.io/docs/reporting-introduction&#34;&gt;Reporting&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Continually upgrade your data, including easily adding more depth to existing partially annotated sets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Annotation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/annotation&#34;&gt;Annotation Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Schema (Ontology): Diffgram supports all popular &lt;a href=&#34;https://diffgram.readme.io/docs/attributes-1&#34;&gt;attributes&lt;/a&gt; and spatial types including &lt;em&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/custom-spatial-templates-user-defined-shapes&#34;&gt;Custom Spatial types&lt;/a&gt;&lt;/em&gt;. (Best Data Annotation for AI/ML)&lt;/p&gt; &#xA;&lt;h2&gt;Annotation Automation&lt;/h2&gt; &#xA;&lt;p&gt;Run models instantly with &lt;a href=&#34;https://diffgram.readme.io/docs/userscripts-overview&#34;&gt;Javascript&lt;/a&gt; or make API calls to any language of your choice.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/userscript-examples&#34;&gt;Automation Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/interactions&#34;&gt;Build your own interactions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Play with model parameters, and see the results in real time (Coming Soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;General purpose automation language, solve &lt;a href=&#34;https://diffgram.readme.io/docs/userscripts-overview&#34;&gt;any annotation automation&lt;/a&gt; challenge. Less annotation and automation costs.&lt;/p&gt; &#xA;&lt;h2&gt;Stream to Training&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NEW - &lt;a href=&#34;https://medium.com/diffgram/stream-training-data-to-your-models-with-diffgram-f0f25f6688c5&#34;&gt;Stream Training Data is Now Available&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1mW9AXnx1Ywuz_f090x0NLJDxlniKygSk?authuser=1#scrollTo=dzoSS_W0EqgW&#34;&gt;Colab Notebook Example&lt;/a&gt; || &lt;a href=&#34;https://diffgram.readme.io/docs/datasets#data-streaming-concepts&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/sdk-dataset-to_pytorch&#34;&gt;Pytorch&lt;/a&gt; || &lt;a href=&#34;https://diffgram.readme.io/docs/sdk-dataset-to_tensorflow&#34;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Easier and faster for data science. Less compute cost. More privacy controls. Load streaming data from Diffgram directly into pytorch and tensorflow with one line (alpha release live!)&lt;/p&gt; &#xA;&lt;h2&gt;Explore&lt;/h2&gt; &#xA;&lt;p&gt;Skip downloading and unzipping massive datasets. Explore data instantly through the browser.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/diffgram/debug-the-humans-querying-your-training-datasets-with-diffgram-595447194ad&#34;&gt;NEW Data Explorer:&lt;/a&gt; Visualize in seconds multiple datasets (Including Video!) and compare models easily without extra computation. &lt;a href=&#34;https://diffgram.com/studio/annotate/coco-dataset&#34;&gt;Try it now&lt;/a&gt; (click Dataset Explorer)&lt;/li&gt; &#xA; &lt;li&gt;Automatic &lt;a href=&#34;https://diffgram.com/versioning&#34;&gt;Dataset Versioning&lt;/a&gt; and user definable datasets.&lt;/li&gt; &#xA; &lt;li&gt;Collaborate share and comment on specific instances with a &lt;a href=&#34;https://diffgram.readme.io/docs/permalink-deep-linking&#34;&gt;Diffgram Permalink&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Debug&lt;/h2&gt; &#xA;&lt;p&gt;Use your models to debug the human. Visually see errors.&lt;/p&gt; &#xA;&lt;p&gt;Diffgram is an amazing way to access, view, compare, and collaborate on datasets to create the highest quality models. Because these features are fully integrated with the Annotation Tooling, it&#39;s absolutely seamless to go from spotting an issue, to creating a labeling campaign, updating schema, etc to correct it.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uncover bad data and edge cases&lt;/li&gt; &#xA; &lt;li&gt;Curate data and send for labeling with one click&lt;/li&gt; &#xA; &lt;li&gt;Automatic error highlighting (Coming Soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Secure and Private&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Runs on your local system or cloud. Less lag, more secure, more control.&lt;/li&gt; &#xA; &lt;li&gt;Enforce PII &amp;amp; RBAC automatically across life-cycle of training data from ingest to dataset to model predictions and back again&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tested and Stable Core&lt;/h2&gt; &#xA;&lt;p&gt;Fully integrated automatic test suite, with comprehensive &lt;a href=&#34;https://medium.com/diffgram/implementing-a-ci-system-with-e2e-testing-using-cypress-percy-and-circleci-246b50be466c&#34;&gt;End to End&lt;/a&gt; tests and many unit tests.&lt;/p&gt; &#xA;&lt;h2&gt;Flexible &amp;amp; Scaleable&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Flexible deploy and many integrations - run Diffgram anywhere in the way you want.&lt;/li&gt; &#xA; &lt;li&gt;Scale every aspect - from volume of data, to number of supervisors, to ML speed up approaches.&lt;/li&gt; &#xA; &lt;li&gt;Fully featured - &#39;batteries included&#39;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Docs&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://diffgram.readme.io/docs&#34;&gt;Docs&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/getting-started-plan&#34;&gt;Getting Started Plan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC4ZVmvMA6oa3Lwaq6Si17pg/videos&#34;&gt;Videos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/cookbook&#34;&gt;Cookbook (Advanced)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Vision&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/vision&#34;&gt;Vision&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Speed Ups &amp;amp; AI&lt;/h1&gt; &#xA;&lt;p&gt;Latest AI + More&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/userscript-examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/userscripts-overview&#34;&gt;Userscripts Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Workflow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Ecosystem&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffgram.readme.io/docs/ecosystem&#34;&gt;Ecosystem&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We welcome contributions! Please see our &lt;a href=&#34;https://diffgram.readme.io/docs/contributing-guide&#34;&gt;contributing documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Architecture &amp;amp; Design Docs&lt;/h1&gt; &#xA;&lt;p&gt;We plan to release more internal architecture docs over time. Please see the &lt;a href=&#34;https://diffgram.readme.io/docs&#34;&gt;general docs&lt;/a&gt; in the mean time.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/multiface</title>
    <updated>2022-07-28T01:36:45Z</updated>
    <id>tag:github.com,2022-07-28:/facebookresearch/multiface</id>
    <link href="https://github.com/facebookresearch/multiface" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hosts the Multiface dataset, which is a multi-view dataset of multiple identities performing a sequence of facial expressions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Multiface Dataset&lt;/h1&gt; &#xA;&lt;p&gt;Our dataset consists of high quality recordings of the faces of 13 identities, each captured in a multi-view capture stage performing various facial expressions. An average of 12,200 (v1 scripts) to 23,000 (v2 scripts) frames per subject with capture rate at 30 fps. Each frame includes roughly 40 (v1) to 160 (v2) different camera views under uniform illumination, yielding a total dataset size of 65TB. We provide the raw captured images from each camera view at a resolution of 2048 √ó 1334 pixels, tracked meshes including headposes, unwrapped textures at 1024 √ó 1024 pixels, metadata including intrinsic and extrinsic camera calibrations, and audio. This repository hosts the code of downloading the dataset and building a Codec Avatar using a &lt;a href=&#34;https://arxiv.org/pdf/1808.00362.pdf&#34;&gt;deep appearance model&lt;/a&gt;. To learn more about how the dataset is captured and how different model architectures can influence performance, you may refer to our &lt;a href=&#34;https://arxiv.org/abs/2207.11243&#34;&gt;Technical Report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/gif.gif?raw=true&#34; width=&#34;500&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/#works-using-this-dataset&#34;&gt;Works Using this Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/#contributors&#34;&gt;Contributors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comprehensive capture of a wide range of &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/documentation/EXPRESSIONS.md&#34;&gt;facial expressions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/multiface/raw/main/images/mesh.png?raw=true&#34;&gt;High-quality tracked mesh&lt;/a&gt; for each frame&lt;/li&gt; &#xA; &lt;li&gt;High-resolution (2k), &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/documentation/CAMERA_VIEW.md&#34;&gt;multi-view captured images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/documentation/DATASET_ASSET.md&#34;&gt;6 assets&lt;/a&gt; are provided: raw images, unwrapped textures, tracked meshes, headpose, audio and metadata.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h3&gt;Quick Data Exploration&lt;/h3&gt; &#xA;&lt;p&gt;To download our data, first clone this repository and install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/facebookresearch/multiface&#xA;cd multiface&#xA;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since the full dataset takes terabytes of storage, one may wish to download partially. If you want to view the example assets, you may download the mini-dataset (&amp;lt; 1 GB)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 download_dataset.py --dest &#34;/path/to/mini_dataset/&#34; --download_config &#34;./mini_download_config.json&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;download_config&lt;/code&gt; argument points to the configuration file specifying assets to be downloaded, options include:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variable&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;entity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;list of string&lt;/td&gt; &#xA;   &lt;td&gt;All the entity will be downloaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;image&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;boolean&lt;/td&gt; &#xA;   &lt;td&gt;Raw images of enities selected will be downloaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mesh&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;boolean&lt;/td&gt; &#xA;   &lt;td&gt;Tracked mesh of enities selected will be downloaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;texture&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;boolean&lt;/td&gt; &#xA;   &lt;td&gt;Unwrapped texture of enities selected will be downloaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;metadata&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;boolean&lt;/td&gt; &#xA;   &lt;td&gt;Metadata of enities selected will be downloaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;audio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;boolean&lt;/td&gt; &#xA;   &lt;td&gt;Audio of enities selected will be downloaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;expression&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;list of string&lt;/td&gt; &#xA;   &lt;td&gt;All the facial expression (contains both v1 and v2 scripts) will be downloaded&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The configuration to download all assets can be found at &lt;code&gt;download_config.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Full Installation&lt;/h3&gt; &#xA;&lt;p&gt;To run training and render the 3D faces, please refer to our full &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/documentation/INSTALLATION.md&#34;&gt;installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;To learn more on selecting model architecture, camera split and expression split for training and testing set, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/documentation/QUICK_START.md&#34;&gt;quick start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Works Using this Dataset&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1808.00362.pdf&#34;&gt;Deep Appearance Models For Face Rendering&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.09955.pdf&#34;&gt;Learning Compositional Radiance Fields of Dynamic Human Heads&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.04638.pdf&#34;&gt;Pixel Codec Avatars&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/dvae.png?raw=true&#34; data-canonical-src=&#34;https://github.com/facebookresearch/multiface/blob/main/images/dvae.png?raw=true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/rd.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/pixel.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.08223.pdf&#34;&gt;MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3272127.3275101&#34;&gt;Deep Incremental Learning for Efficient High-Fidelity Face Tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Bagautdinov_Modeling_Facial_Geometry_CVPR_2018_paper.pdf&#34;&gt;Modeling Facial Geometry using Compositional VAEs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/talk.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/face.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/fvae.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Nam_Strand-Accurate_Multi-View_Hair_Capture_CVPR_2019_paper.pdf&#34;&gt;Strand-accurate Multi-view Hair Capture&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.01954.pdf&#34;&gt;Mixture of Volumetric Primitives for Efficient Neural Rendering&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookresearch/mvp&#34;&gt;[Code Available]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://cseweb.ucsd.edu/~ravir/hairinverse.pdf&#34;&gt;Human Hair Inverse Rendering using Multi-View Photometric data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/hair1.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/mvp_long.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/facebookresearch/multiface/raw/main/images/hair2.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to all the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/multiface/main/documentation/CONTRIBUTOR.md&#34;&gt;people&lt;/a&gt; who has helped generate and maintain this dataset!&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use any data from this dataset or any code released in this repository, please cite the technical report (&lt;a href=&#34;https://arxiv.org/abs/2207.11243&#34;&gt;https://arxiv.org/abs/2207.11243&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{wuu2022multiface,&#xA;  title={Multiface: A Dataset for Neural Face Rendering},&#xA;  author = {Wuu, Cheng-hsin and Zheng, Ningyuan and Ardisson, Scott and Bali, Rohan and Belko, Danielle and Brockmeyer, Eric and Evans, Lucas and Godisart, Timothy and Ha, Hyowon and Hypes, Alexander and Koska, Taylor and Krenn, Steven and Lombardi, Stephen and Luo, Xiaomin and McPhail, Kevyn and Millerschoen, Laura and Perdoch, Michal and Pitts, Mark and Richard, Alexander and Saragih, Jason and Saragih, Junko and Shiratori, Takaaki and Simon, Tomas and Stewart, Matt and Trimble, Autumn and Weng, Xinshuo and Whitewolf, David and Wu, Chenglei and Yu, Shoou-I and Sheikh, Yaser},&#xA;  booktitle={arXiv},&#xA;  year={2022},&#xA;  doi = {10.48550/ARXIV.2207.11243},&#xA;  url = {https://arxiv.org/abs/2207.11243}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Multiface is CC-BY-NC 4.0 licensed, as found in the &lt;a href=&#34;https://github.com/facebookresearch/multiface/raw/main/LICENSE&#34;&gt;LICENSE file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://opensource.facebook.com/legal/terms&#34;&gt;Terms of Use&lt;/a&gt;] [&lt;a href=&#34;https://opensource.facebook.com/legal/privacy&#34;&gt;Privacy Policy&lt;/a&gt;]&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sphinx-doc/sphinx</title>
    <updated>2022-07-28T01:36:45Z</updated>
    <id>tag:github.com,2022-07-28:/sphinx-doc/sphinx</id>
    <link href="https://github.com/sphinx-doc/sphinx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Main repository for the Sphinx documentation builder&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;======== Sphinx&lt;/h1&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/pypi/v/sphinx.svg&#34;&gt;https://img.shields.io/pypi/v/sphinx.svg&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/Sphinx/&#34;&gt;https://pypi.org/project/Sphinx/&lt;/a&gt; :alt: Package on PyPI&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/sphinx-doc/sphinx/actions/workflows/main.yml/badge.svg&#34;&gt;https://github.com/sphinx-doc/sphinx/actions/workflows/main.yml/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/sphinx-doc/sphinx/actions/workflows/main.yml&#34;&gt;https://github.com/sphinx-doc/sphinx/actions/workflows/main.yml&lt;/a&gt; :alt: Build Status&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://readthedocs.org/projects/sphinx/badge/?version=master&#34;&gt;https://readthedocs.org/projects/sphinx/badge/?version=master&lt;/a&gt; :target: &lt;a href=&#34;https://www.sphinx-doc.org/&#34;&gt;https://www.sphinx-doc.org/&lt;/a&gt; :alt: Documentation Status&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/badge/License-BSD%202--Clause-blue.svg&#34;&gt;https://img.shields.io/badge/License-BSD%202--Clause-blue.svg&lt;/a&gt; :target: &lt;a href=&#34;https://opensource.org/licenses/BSD-2-Clause&#34;&gt;https://opensource.org/licenses/BSD-2-Clause&lt;/a&gt; :alt: BSD 2 Clause&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sphinx makes it easy to create intelligent and beautiful documentation.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sphinx uses reStructuredText as its markup language, and many of its strengths come from the power and straightforwardness of reStructuredText and its parsing and translating suite, the Docutils.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Output formats&lt;/strong&gt;: HTML, PDF, plain text, EPUB, TeX, manual pages, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensive cross-references&lt;/strong&gt;: semantic markup and automatic links for functions, classes, glossary terms and similar pieces of information&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hierarchical structure&lt;/strong&gt;: easy definition of a document tree, with automatic links to siblings, parents and children&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic indices&lt;/strong&gt;: general index as well as a module index&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code highlighting&lt;/strong&gt;: automatic highlighting using the Pygments highlighter&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Templating&lt;/strong&gt;: Flexible HTML output using the Jinja 2 templating engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extension ecosystem&lt;/strong&gt;: Many extensions are available, for example for automatic function documentation or working with Jupyter notebooks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language Support&lt;/strong&gt;: Python, C, C++, JavaScript, mathematics, and many other languages through extensions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more information, refer to the &lt;code&gt;the documentation&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;The following command installs Sphinx from the &lt;code&gt;Python Package Index&lt;/code&gt;_. You will need a working installation of Python and pip.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: sh&lt;/p&gt; &#xA;&lt;p&gt;pip install -U sphinx&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We appreciate all contributions! Refer to &lt;code&gt;the contributors guide&lt;/code&gt;_ for information.&lt;/p&gt; &#xA;&lt;h1&gt;Release signatures&lt;/h1&gt; &#xA;&lt;p&gt;Releases are signed with following keys:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;498D6B9E &amp;lt;https://pgp.mit.edu/pks/lookup?op=vindex&amp;amp;search=0x102C2C17498D6B9E&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;5EBA0E07 &amp;lt;https://pgp.mit.edu/pks/lookup?op=vindex&amp;amp;search=0x1425F8CE5EBA0E07&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;61F0FB52 &amp;lt;https://pgp.mit.edu/pks/lookup?op=vindex&amp;amp;search=0x52C8F72A61F0FB52&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;.. _the documentation: &lt;a href=&#34;https://www.sphinx-doc.org/&#34;&gt;https://www.sphinx-doc.org/&lt;/a&gt; .. _the contributors guide: &lt;a href=&#34;https://www.sphinx-doc.org/en/master/internals/contributing.html&#34;&gt;https://www.sphinx-doc.org/en/master/internals/contributing.html&lt;/a&gt; .. _Python Package Index: &lt;a href=&#34;https://pypi.org/project/Sphinx/&#34;&gt;https://pypi.org/project/Sphinx/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>