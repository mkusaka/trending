<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-02T01:39:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jianchang512/ChatTTS-ui</title>
    <updated>2024-06-02T01:39:56Z</updated>
    <id>tag:github.com,2024-06-02:/jianchang512/ChatTTS-ui</id>
    <link href="https://github.com/jianchang512/ChatTTS-ui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ä¸€ä¸ªç®€å•çš„æœ¬åœ°ç½‘é¡µç•Œé¢ï¼Œç›´æ¥ä½¿ç”¨ChatTTSå°†æ–‡å­—åˆæˆä¸ºè¯­éŸ³ï¼ŒåŒæ—¶æ”¯æŒå¯¹å¤–æä¾›APIæ¥å£ã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatTTS webUI &amp;amp; API&lt;/h1&gt; &#xA;&lt;p&gt;ä¸€ä¸ªç®€å•çš„æœ¬åœ°ç½‘é¡µç•Œé¢ï¼Œç›´æ¥åœ¨ç½‘é¡µä½¿ç”¨ &lt;a href=&#34;https://github.com/2noise/chattts&#34;&gt;ChatTTS&lt;/a&gt; å°†æ–‡å­—åˆæˆä¸ºè¯­éŸ³ï¼ŒåŒæ—¶æ”¯æŒå¯¹å¤–æä¾›APIæ¥å£ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/ChatTTS-ui/releases&#34;&gt;Releasesä¸­å¯ä¸‹è½½Windowsæ•´åˆåŒ…&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ç•Œé¢é¢„è§ˆ&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/6ed7c993-3882-4c34-9abd-f0635b133012&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;è¯•å¬åˆæˆè¯­éŸ³æ•ˆæœ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/03cf1c0f-0245-44b5-8007-370d9db2bda8&#34;&gt;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/03cf1c0f-0245-44b5-8007-370d9db2bda8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Windowsé¢„æ‰“åŒ…ç‰ˆ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ä» &lt;a href=&#34;https://github.com/jianchang512/chatTTS-ui/releases&#34;&gt;Releases&lt;/a&gt;ä¸­ä¸‹è½½å‹ç¼©åŒ…ï¼Œè§£å‹ååŒå‡» app.exe å³å¯ä½¿ç”¨&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Linux ä¸‹æºç éƒ¨ç½²&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;é…ç½®å¥½ python3.9+ç¯å¢ƒ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºç©ºç›®å½• &lt;code&gt;/data/chattts&lt;/code&gt; æ‰§è¡Œå‘½ä»¤ &lt;code&gt;cd /data/chattts &amp;amp;&amp;amp; git clone https://github.com/jianchang512/chatTTS-ui .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ &lt;code&gt;python3 -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®‰è£…ä¾èµ– &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¦‚æœä¸éœ€è¦CUDAåŠ é€Ÿï¼Œæ‰§è¡Œ &lt;code&gt;pip3 install torch torchaudio&lt;/code&gt;&lt;/p&gt; &lt;p&gt;å¦‚æœéœ€è¦CUDAåŠ é€Ÿï¼Œæ‰§è¡Œ&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118&#xA;&#x9;&#x9;&#xA;pip install nvidia-cublas-cu11 nvidia-cudnn-cu11&#xA;&#x9;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;å¦éœ€å®‰è£… CUDA11.8+ ToolKitï¼Œè¯·è‡ªè¡Œæœç´¢å®‰è£…æ–¹æ³• æˆ–å‚è€ƒ &lt;a href=&#34;https://juejin.cn/post/7318704408727519270&#34;&gt;https://juejin.cn/post/7318704408727519270&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ‰§è¡Œ &lt;code&gt;python3 app.py&lt;/code&gt; å¯åŠ¨ï¼Œå°†è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨çª—å£ï¼Œé»˜è®¤åœ°å€ &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;MacOS ä¸‹æºç éƒ¨ç½²&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;é…ç½®å¥½ python3.9+ç¯å¢ƒ,å®‰è£…git ï¼Œæ‰§è¡Œå‘½ä»¤ &lt;code&gt;brew install git python@3.10&lt;/code&gt; ç»§ç»­æ‰§è¡Œ&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export PATH=&#34;/usr/local/opt/python@3.10/bin:$PATH&#34;&#xA;&#xA;source ~/.bash_profile &#xA;&#xA;source ~/.zshrc&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºç©ºç›®å½• &lt;code&gt;/data/chattts&lt;/code&gt; æ‰§è¡Œå‘½ä»¤ &lt;code&gt;cd /data/chattts &amp;amp;&amp;amp; git clone https://github.com/jianchang512/chatTTS-ui .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ &lt;code&gt;python3 -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ &lt;code&gt;source ./venv/bin/activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®‰è£…ä¾èµ– &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®‰è£…torch &lt;code&gt;pip3 install torch torchaudio&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ‰§è¡Œ &lt;code&gt;python3 app.py&lt;/code&gt; å¯åŠ¨ï¼Œå°†è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨çª—å£ï¼Œé»˜è®¤åœ°å€ &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Macosä¸‹å¯èƒ½ä¼šåˆ°ä¸€äº›é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ &lt;a href=&#34;https://raw.githubusercontent.com/jianchang512/ChatTTS-ui/main/faq.md&#34;&gt;å¸¸è§é—®é¢˜ä¸æŠ¥é”™è§£å†³æ–¹æ³•&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Windowsæºç éƒ¨ç½²&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;ä¸‹è½½python3.9+ï¼Œå®‰è£…æ—¶æ³¨æ„é€‰ä¸­&lt;code&gt;Add Python to environment variables&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ä¸‹è½½å¹¶å®‰è£…gitï¼Œ&lt;a href=&#34;https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe&#34;&gt;https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºç©ºæ–‡ä»¶å¤¹ &lt;code&gt;D:/chattts&lt;/code&gt; å¹¶è¿›å…¥ï¼Œåœ°å€æ è¾“å…¥ &lt;code&gt;cmd&lt;/code&gt;å›è½¦ï¼Œåœ¨å¼¹å‡ºçš„cmdçª—å£ä¸­æ‰§è¡Œå‘½ä»¤ &lt;code&gt;git clone https://github.com/jianchang512/chatTTS-ui .&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œæ‰§è¡Œå‘½ä»¤ &lt;code&gt;python -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼Œæ‰§è¡Œ &lt;code&gt;.\venv\scripts\activate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®‰è£…ä¾èµ–,æ‰§è¡Œ &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¦‚æœä¸éœ€è¦CUDAåŠ é€Ÿï¼Œæ‰§è¡Œ &lt;code&gt;pip install torch torchaudio&lt;/code&gt;&lt;/p&gt; &lt;p&gt;å¦‚æœéœ€è¦CUDAåŠ é€Ÿï¼Œæ‰§è¡Œ&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118&lt;/code&gt;&lt;/p&gt; &lt;p&gt;å¦éœ€å®‰è£… CUDA11.8+ ToolKitï¼Œè¯·è‡ªè¡Œæœç´¢å®‰è£…æ–¹æ³•æˆ–å‚è€ƒ &lt;a href=&#34;https://juejin.cn/post/7318704408727519270&#34;&gt;https://juejin.cn/post/7318704408727519270&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ‰§è¡Œ &lt;code&gt;python app.py&lt;/code&gt; å¯åŠ¨ï¼Œå°†è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨çª—å£ï¼Œé»˜è®¤åœ°å€ &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;æºç éƒ¨ç½²æ³¨æ„&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;æºç éƒ¨ç½²å¯åŠ¨åï¼Œä¼šå…ˆä» modelscopeä¸‹è½½æ¨¡å‹ï¼Œä½†modelscopeç¼ºå°‘spk_stat.ptï¼Œä¼šæŠ¥é”™ï¼Œè¯·ç‚¹å‡»é“¾æ¥ &lt;a href=&#34;https://huggingface.co/2Noise/ChatTTS/blob/main/asset/spk_stat.pt&#34;&gt;https://huggingface.co/2Noise/ChatTTS/blob/main/asset/spk_stat.pt&lt;/a&gt; ä¸‹è½½ spk_stat.ptï¼Œå°†è¯¥æ–‡ä»¶å¤åˆ¶åˆ° &lt;code&gt;é¡¹ç›®ç›®å½•/models/pzc163/chatTTS/asset/ æ–‡ä»¶å¤¹å†…&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ³¨æ„ modelscope ä»…å…è®¸ä¸­å›½å¤§é™†ipä¸‹è½½æ¨¡å‹ï¼Œå¦‚æœé‡åˆ° proxy ç±»é”™è¯¯ï¼Œè¯·å…³é—­ä»£ç†ã€‚å¦‚æœä½ å¸Œæœ›ä» huggingface.co ä¸‹è½½æ¨¡å‹ï¼Œè¯·æ‰“å¼€ &lt;code&gt;app.py&lt;/code&gt; æŸ¥çœ‹å¤§çº¦ç¬¬50è¡Œ-60è¡Œçš„æ³¨é‡Šã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¦‚æœéœ€è¦GPUåŠ é€Ÿï¼Œå¿…é¡»æ˜¯è‹±ä¼Ÿè¾¾æ˜¾å¡ï¼Œå¹¶ä¸”å®‰è£… cudaç‰ˆæœ¬çš„torchã€‚&lt;code&gt;pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# é»˜è®¤ä» modelscope ä¸‹è½½æ¨¡å‹,å¦‚æœæƒ³ä»huggingfaceä¸‹è½½æ¨¡å‹ï¼Œè¯·å°†ä»¥ä¸‹3è¡Œæ³¨é‡Šæ‰&#xA;CHATTTS_DIR = snapshot_download(&#39;pzc163/chatTTS&#39;,cache_dir=MODEL_DIR)&#xA;chat = ChatTTS.Chat()&#xA;chat.load_models(source=&#34;local&#34;,local_path=CHATTTS_DIR)&#xA;&#xA;# å¦‚æœå¸Œæœ›ä» huggingface.coä¸‹è½½æ¨¡å‹ï¼Œå°†ä»¥ä¸‹æ³¨é‡Šåˆ æ‰ã€‚å°†ä¸Šæ–¹3è¡Œå†…å®¹æ³¨é‡Šæ‰&#xA;#os.environ[&#39;HF_HUB_CACHE&#39;]=MODEL_DIR&#xA;#os.environ[&#39;HF_ASSETS_CACHE&#39;]=MODEL_DIR&#xA;#chat = ChatTTS.Chat()&#xA;#chat.load_models()&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jianchang512/ChatTTS-ui/main/faq.md&#34;&gt;å¸¸è§é—®é¢˜ä¸æŠ¥é”™è§£å†³æ–¹æ³•&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;ä¿®æ”¹httpåœ°å€&lt;/h2&gt; &#xA;&lt;p&gt;é»˜è®¤åœ°å€æ˜¯ &lt;code&gt;http://127.0.0.1:9966&lt;/code&gt;,å¦‚æœæƒ³ä¿®æ”¹ï¼Œå¯æ‰“å¼€ç›®å½•ä¸‹çš„ &lt;code&gt;.env&lt;/code&gt;æ–‡ä»¶ï¼Œå°† &lt;code&gt;WEB_ADDRESS=127.0.0.1:9966&lt;/code&gt;æ”¹ä¸ºåˆé€‚çš„ipå’Œç«¯å£ï¼Œæ¯”å¦‚ä¿®æ”¹ä¸º&lt;code&gt;WEB_ADDRESS=192.168.0.10:9966&lt;/code&gt;ä»¥ä¾¿å±€åŸŸç½‘å¯è®¿é—®&lt;/p&gt; &#xA;&lt;h2&gt;ä½¿ç”¨APIè¯·æ±‚ v0.5+&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;è¯·æ±‚æ–¹æ³•:&lt;/strong&gt; POST&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;è¯·æ±‚åœ°å€:&lt;/strong&gt; &lt;a href=&#34;http://127.0.0.1:9966/tts&#34;&gt;http://127.0.0.1:9966/tts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;è¯·æ±‚å‚æ•°:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;text: str| å¿…é¡»ï¼Œ è¦åˆæˆè¯­éŸ³çš„æ–‡å­—&lt;/p&gt; &#xA;&lt;p&gt;voice: int| å¯é€‰ï¼Œé»˜è®¤ 2222, å†³å®šéŸ³è‰²çš„æ•°å­—ï¼Œ 2222 | 7869 | 6653 | 4099 | 5099ï¼Œå¯é€‰å…¶ä¸€ï¼Œæˆ–è€…ä»»æ„ä¼ å…¥å°†éšæœºä½¿ç”¨éŸ³è‰²&lt;/p&gt; &#xA;&lt;p&gt;prompt: str| å¯é€‰ï¼Œé»˜è®¤ ç©ºï¼Œ è®¾å®š ç¬‘å£°ã€åœé¡¿ï¼Œä¾‹å¦‚ [oral_2][laugh_0][break_6]&lt;/p&gt; &#xA;&lt;p&gt;temperature: float| å¯é€‰ï¼Œ é»˜è®¤ 0.3&lt;/p&gt; &#xA;&lt;p&gt;top_p: float| å¯é€‰ï¼Œ é»˜è®¤ 0.7&lt;/p&gt; &#xA;&lt;p&gt;top_k: int| å¯é€‰ï¼Œ é»˜è®¤ 20&lt;/p&gt; &#xA;&lt;p&gt;skip_refine: int| å¯é€‰ï¼Œ é»˜è®¤0ï¼Œ 1=è·³è¿‡ refine textï¼Œ0=ä¸è·³è¿‡&lt;/p&gt; &#xA;&lt;p&gt;custom_voice: int| å¯é€‰ï¼Œ é»˜è®¤0ï¼Œè‡ªå®šä¹‰è·å–éŸ³è‰²å€¼æ—¶çš„ç§å­å€¼ï¼Œéœ€è¦å¤§äº0çš„æ•´æ•°ï¼Œå¦‚æœè®¾ç½®äº†åˆ™ä»¥æ­¤ä¸ºå‡†ï¼Œå°†å¿½ç•¥ &lt;code&gt;voice&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;è¿”å›:jsonæ•°æ®&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æˆåŠŸè¿”å›: {code:0,msg:ok,audio_files:[dict1,dict2]}&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;å…¶ä¸­ audio_files æ˜¯å­—å…¸æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ dictä¸º {filename:wavæ–‡ä»¶ç»å¯¹è·¯å¾„ï¼Œurl:å¯ä¸‹è½½çš„wavç½‘å€}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¤±è´¥è¿”å›:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{code:1,msg:é”™è¯¯åŸå› }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;# APIè°ƒç”¨ä»£ç &#xA;&#xA;import requests&#xA;&#xA;res = requests.post(&#39;http://127.0.0.1:9966/tts&#39;, data={&#xA;  &#34;text&#34;: &#34;è‹¥ä¸æ‡‚æ— éœ€å¡«å†™&#34;,&#xA;  &#34;prompt&#34;: &#34;&#34;,&#xA;  &#34;voice&#34;: &#34;3333&#34;,&#xA;  &#34;temperature&#34;: 0.3,&#xA;  &#34;top_p&#34;: 0.7,&#xA;  &#34;top_k&#34;: 20,&#xA;  &#34;skip_refine&#34;: 0,&#xA;  &#34;custom_voice&#34;: 0&#xA;})&#xA;print(res.json())&#xA;&#xA;#ok&#xA;{code:0, msg:&#39;ok&#39;, audio_files:[{filename: E:/python/chattts/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav, url: http://127.0.0.1:9966/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav}]}&#xA;&#xA;#error&#xA;{code:1, msg:&#34;error&#34;}&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;åœ¨pyVideoTransè½¯ä»¶ä¸­ä½¿ç”¨&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å‡çº§ pyVideoTrans åˆ° 1.82+ &lt;a href=&#34;https://github.com/jianchang512/pyvideotrans&#34;&gt;https://github.com/jianchang512/pyvideotrans&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ç‚¹å‡»èœå•-è®¾ç½®-ChatTTSï¼Œå¡«å†™è¯·æ±‚åœ°å€ï¼Œé»˜è®¤åº”è¯¥å¡«å†™ &lt;a href=&#34;http://127.0.0.1:9966&#34;&gt;http://127.0.0.1:9966&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;æµ‹è¯•æ— é—®é¢˜åï¼Œåœ¨ä¸»ç•Œé¢ä¸­é€‰æ‹©&lt;code&gt;ChatTTS&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jianchang512/ChatTTS-ui/assets/3378335/7118325f-2b9a-46ce-a584-1d5c6dc8e2da&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tencent-ailab/V-Express</title>
    <updated>2024-06-02T01:39:56Z</updated>
    <id>tag:github.com,2024-06-02:/tencent-ailab/V-Express</id>
    <link href="https://github.com/tencent-ailab/V-Express" rel="alternate"></link>
    <summary type="html">&lt;p&gt;V-Express aims to generate a talking head video under the control of a reference image, an audio, and a sequence of V-Kps images.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;&lt;em&gt;V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation&lt;/em&gt;&lt;/strong&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tenvence.github.io/p/v-express/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tenvence.github.io/p/v-express/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technique-Report-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/tk93/V-Express&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![GitHub](https://img.shields.io/github/stars/tencent-ailab/IP-Adapter?style=social)](https://github.com/tencent-ailab/IP-Adapter/) --&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent. A common approach involves leveraging generative models to enhance adapters for controlled generation. However, control signals can vary in strength, including text, audio, image reference, pose, depth map, etc. Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions. In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as pose and original image. However, direct training with weak signals often leads to difficulties in convergence. To address this, we propose V-Express, a simple method that balances different control signals through a series of progressive drop operations. Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account pose, input image, and audio.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/V-Express/main/assets/global_framework.png&#34; alt=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/05/29] ğŸ”¥ We have added video post-processing that can effectively mitigate the flicker problem.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05/23] ğŸ”¥ We release the code and models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# install requirements&#xA;pip install diffusers==0.24.0&#xA;pip install imageio-ffmpeg==0.4.9&#xA;pip install insightface==0.7.3&#xA;pip install omegaconf==2.2.3&#xA;pip install onnxruntime==1.16.3&#xA;pip install safetensors==0.4.2&#xA;pip install torch==2.0.1&#xA;pip install torchaudio==2.0.2&#xA;pip install torchvision==0.15.2&#xA;pip install transformers==4.30.2&#xA;pip install einops==0.4.1&#xA;pip install tqdm==4.66.1&#xA;pip install xformers==0.0.22&#xA;pip install av==11.0.0&#xA;&#xA;# download the codes&#xA;git clone https://github.com/tencent-ailab/V-Express&#xA;&#xA;# download the models&#xA;cd V-Express&#xA;git lfs install&#xA;git clone https://huggingface.co/tk93/V-Express&#xA;mv V-Express/model_ckpts model_ckpts&#xA;&#xA;# then you can use the scripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Models&lt;/h2&gt; &#xA;&lt;p&gt;you can download models from &lt;a href=&#34;https://huggingface.co/tk93/V-Express&#34;&gt;here&lt;/a&gt;. We have included all the required models in the model card. You can also download the models separately from the original repository.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;stabilityai/sd-vae-ft-mse&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;. Only the model configuration file for unet is needed here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/wav2vec2-base-960h&#34;&gt;facebook/wav2vec2-base-960h&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip&#34;&gt;insightface/buffalo_l&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;span style=&#34;color:red&#34;&gt;Important Reminder&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Important! Important!! Important!!!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the talking-face generation task, when the target video is not the same person as the reference character, the retarget of the face will be a &lt;span style=&#34;color:red&#34;&gt;very important&lt;/span&gt; part. And choosing a target video that is more similar to the pose of the reference face will be able to get better results. In addition, our model now performs better on English, and other languages have not yet been tested in detail.&lt;/p&gt; &#xA;&lt;h3&gt;Run the demo (step1, &lt;em&gt;optional&lt;/em&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;If you have a target talking video, you can follow the script below to extract the audio and face V-kps sequences from the video. You can also skip this step and run the script in Step 2 directly to try the example we provided.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/extract_kps_sequence_and_audio.py \&#xA;    --video_path &#34;./test_samples/short_case/AOC/gt.mp4&#34; \&#xA;    --kps_sequence_save_path &#34;./test_samples/short_case/AOC/kps.pth&#34; \&#xA;    --audio_save_path &#34;./test_samples/short_case/AOC/aud.mp3&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend cropping a clear square face image as in the example below and making sure the resolution is no lower than 512x512. The green to red boxes in the image below are the recommended cropping ranges.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/V-Express/main/assets/crop_example.jpeg&#34; alt=&#34;drawing&#34; style=&#34;width:500px;&#34;&gt; &#xA;&lt;h3&gt;Run the demo (step2, &lt;em&gt;core&lt;/em&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scenario 1 (A&#39;s picture and A&#39;s talking video.) (Best Practice)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have a picture of A and a talking video of A in another scene. Then you should run the following script. Our model is able to generate speaking videos that are consistent with the given video. &lt;em&gt;You can see more examples on our &lt;a href=&#34;https://tenvence.github.io/p/v-express/&#34;&gt;project page&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/AOC/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/AOC/aud.mp3&#34; \&#xA;    --kps_path &#34;./test_samples/short_case/AOC/kps.pth&#34; \&#xA;    --output_path &#34;./output/short_case/talk_AOC_no_retarget.mp4&#34; \&#xA;    --retarget_strategy &#34;no_retarget&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/17dd4103-eaf7-4045-8bc0-e90093deaee8&#34; style=&#34;width: 80%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;p&gt;&lt;strong&gt;Scenario 2 (A&#39;s picture and any talking audio.)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you only have a picture and any talking audio. With the following script, our model can generate vivid mouth movements for fixed faces.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/tys/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/tys/aud.mp3&#34; \&#xA;    --output_path &#34;./output/short_case/talk_tys_fix_face.mp4&#34; \&#xA;    --retarget_strategy &#34;fix_face&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/fe782c16-f341-424d-83ce-89531af2a292&#34; style=&#34;width: 40%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;p&gt;&lt;strong&gt;Scenario 3 (A&#39;s picture and B&#39;s talking video.)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With the script below, our model generates vivid mouth movements accompanied by slight facial motion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/tys/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/tys/aud.mp3&#34; \&#xA;    --kps_path &#34;./test_samples/short_case/tys/kps.pth&#34; \&#xA;    --output_path &#34;./output/short_case/talk_tys_offset_retarget.mp4&#34; \&#xA;    --retarget_strategy &#34;offset_retarget&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/4951d06c-579d-499e-994d-14fa7e524713&#34; style=&#34;width: 40%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With the following script, our model generates a video with the same movements as the target video, and the character&#39;s lip-synching matches the target audio.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We have only implemented the very naive retarget strategy so far, which allows us to achieve driving the reference face with different character videos under limited conditions. To get better results, we strongly recommend you to choose a target video that is closer to the reference face. We are also trying to implement a more robust face retargeting strategy, which hopefully can further solve the problem of inconsistency between the reference face and the target face. We also welcome experienced people who can help.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/tys/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/tys/aud.mp3&#34; \&#xA;    --kps_path &#34;./test_samples/short_case/tys/kps.pth&#34; \&#xA;    --output_path &#34;./output/short_case/talk_tys_naive_retarget.mp4&#34; \&#xA;    --retarget_strategy &#34;naive_retarget&#34; \&#xA;    --num_inference_steps 25&#xA;&lt;/code&gt;&lt;/pre&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/d555ed02-56eb-44e5-94e5-772edcd3338b&#34; style=&#34;width: 40%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;h3&gt;More parameters&lt;/h3&gt; &#xA;&lt;p&gt;For different types of input condition, such as reference image and target audio, we provide parameters for adjusting the role played by that condition information in the model prediction. We refer to these two parameters as &lt;code&gt;reference_attention_weight&lt;/code&gt; and &lt;code&gt;audio_attention_weight&lt;/code&gt;. Different parameters can be applied to achieve different effects using the following script. &lt;code&gt;Through our experiments, we suggest that reference_attention_weight takes the value 0.9-1.0 and audio_attention_weight takes the value 1.0-3.0.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference.py \&#xA;    --reference_image_path &#34;./test_samples/short_case/10/ref.jpg&#34; \&#xA;    --audio_path &#34;./test_samples/short_case/10/aud.mp3&#34; \&#xA;    --output_path &#34;./output/short_case/talk_10_fix_face_with_weight.mp4&#34; \&#xA;    --retarget_strategy &#34;fix_face&#34; \    # this strategy do not need kps info&#xA;    --reference_attention_weight 0.95 \&#xA;    --audio_attention_weight 3.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We show the different effects produced by different parameters in the following video. You can adjust the parameters accordingly to your needs.&lt;/p&gt;   &#xA;&lt;video muted autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; src=&#34;https://github.com/tencent-ailab/V-Express/assets/19601425/2e977b8c-c69b-4815-8565-d4d7c3c349a9&#34; style=&#34;width: 100%; height: auto;&#34;&gt;&lt;/video&gt;   &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the contributors to the &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;magic-animate&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt;, &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/discussions/1236&#34;&gt;sd-webui-controlnet&lt;/a&gt;, and &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt; repositories, for their open research and exploration.&lt;/p&gt; &#xA;&lt;p&gt;The code of V-Express is released for both academic and commercial usage. However, both manual-downloading and auto-downloading models from V-Express are for non-commercial research purposes. Our released checkpoints are also for research purposes only. Users are granted the freedom to create videos using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find V-Express useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2024V-Express,&#xA;  title={V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation},&#xA;  author={Wang, Cong and Tian, Kuan and Zhang, Jun and Guan, Yonghang and Luo, Feng and Shen, Fei and Jiang, Zhiwei and Gu, Qing and Han, Xiao and Yang, Wei},&#xA;  booktitle={arXiv preprint arxiv: comming soon},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ToonCrafter/ToonCrafter</title>
    <updated>2024-06-02T01:39:56Z</updated>
    <id>tag:github.com,2024-06-02:/ToonCrafter/ToonCrafter</id>
    <link href="https://github.com/ToonCrafter/ToonCrafter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;a research paper for generative cartoon interpolation&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;&lt;em&gt;&lt;strong&gt;&lt;em&gt;&lt;strong&gt;ToonCrafter: Generative Cartoon Interpolation&lt;/strong&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt; &#xA;&lt;!-- ![](./assets/logo_long.png#gh-light-mode-only){: width=&#34;50%&#34;} --&gt; &#xA;&lt;!-- ![](./assets/logo_long_dark.png#gh-dark-mode-only=100x20) --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ğŸ”† Introduction&lt;/h2&gt; &#xA;&lt;p&gt;âš ï¸ Please check our &lt;a href=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/#disc&#34;&gt;disclaimer&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ¤— ToonCrafter can interpolate two cartoon images by leveraging the pre-trained image-to-video diffusion priors. Please check our project page and paper for more information. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1.1 Showcases (512x320)&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input starting frame&lt;/td&gt; &#xA;   &lt;td&gt;Input ending frame&lt;/td&gt; &#xA;   &lt;td&gt;Generated video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72109_125.mp4_00-00.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72109_125.mp4_00-01.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/00.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_2_062266_s2_frame1.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_2_062266_s2_frame3.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/03.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_1_070321_s3_frame1.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/Japan_v2_1_070321_s3_frame3.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/02.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/74302_1349_frame1.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/74302_1349_frame3.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/01.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;1.2 Sparse sketch guidance&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input starting frame&lt;/td&gt; &#xA;   &lt;td&gt;Input ending frame&lt;/td&gt; &#xA;   &lt;td&gt;Input sketch guidance&lt;/td&gt; &#xA;   &lt;td&gt;Generated video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72105_388.mp4_00-00.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72105_388.mp4_00-01.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/06.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/07.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72110_255.mp4_00-00.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/72110_255.mp4_00-01.png&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/12.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/13.gif&#34; width=&#34;200&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;2. Applications&lt;/h3&gt; &#xA;&lt;h4&gt;2.1 Cartoon Sketch Interpolation (see project page for more details)&lt;/h4&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input starting frame&lt;/td&gt; &#xA;   &lt;td&gt;Input ending frame&lt;/td&gt; &#xA;   &lt;td&gt;Generated video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_10.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0016_10.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/10.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_11.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0016_11.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/11.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.2 Reference-based Sketch Colorization&lt;/h4&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td&gt;Input sketch&lt;/td&gt; &#xA;   &lt;td&gt;Input reference&lt;/td&gt; &#xA;   &lt;td&gt;Colorization results&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/04.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_05.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/05.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/08.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/frame0001_09.png&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ToonCrafter/ToonCrafter/main/assets/09.gif&#34; width=&#34;250&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;ğŸ“ Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add sketch control and colorization function.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.05.29]&lt;/strong&gt;: ğŸ”¥ğŸ”¥ Release code and model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.05.28]&lt;/strong&gt;: Launch the project page and update the arXiv preprint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;ğŸ§° Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;GPU Mem. &amp;amp; Inference Time (A100, ddim 50steps)&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ToonCrafter_512&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;320x512&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TBD (&lt;code&gt;perframe_ae=True&lt;/code&gt;)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/Doubiiu/ToonCrafter/blob/main/model.ckpt&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Currently, our ToonCrafter can support generating videos of up to 16 frames with a resolution of 512x320. The inference time can be reduced by using fewer DDIM steps.&lt;/p&gt; &#xA;&lt;h2&gt;âš™ï¸ Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Install Environment via Anaconda (Recommended)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n tooncrafter python=3.8.5&#xA;conda activate tooncrafter&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ’« Inference&lt;/h2&gt; &#xA;&lt;h3&gt;1. Command line&lt;/h3&gt; &#xA;&lt;p&gt;Download pretrained ToonCrafter_512 and put the &lt;code&gt;model.ckpt&lt;/code&gt; in &lt;code&gt;checkpoints/tooncrafter_512_interp_v1/model.ckpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  sh scripts/run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Local Gradio demo&lt;/h3&gt; &#xA;&lt;p&gt;Download the pretrained model and put it in the corresponding directory according to the previous guidelines.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  python gradio_app.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- ## ğŸ¤ Community Support --&gt; &#xA;&lt;p&gt;&lt;a name=&#34;disc&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“¢ Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Calm down. Our framework opens up the era of generative cartoon interpolation, but due to the variaity of generative video prior, the success rate is not guaranteed.&lt;/p&gt; &#xA;&lt;p&gt;âš ï¸This is an open-source research exploration, instead of commercial products. It can&#39;t meet all your expectations.&lt;/p&gt; &#xA;&lt;p&gt;This project strives to impact the domain of AI-driven video generation positively. Users are granted the freedom to create videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
</feed>