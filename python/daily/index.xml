<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-08T01:36:48Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chengazhen/cursor-auto-free</title>
    <updated>2025-04-08T01:36:48Z</updated>
    <id>tag:github.com,2025-04-08:/chengazhen/cursor-auto-free</id>
    <link href="https://github.com/chengazhen/cursor-auto-free" rel="alternate"></link>
    <summary type="html">&lt;p&gt;auto sign cursor&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cursor Pro 自动化工具使用说明&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chengazhen/cursor-auto-free/main/README.EN.md&#34;&gt;English doc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;在线文档&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cursor-auto-free-doc.vercel.app&#34;&gt;cursor-auto-free-doc.vercel.app&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;公众号 回复 1 获取 qq 群&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chengazhen/cursor-auto-free/main/screen/qrcode_for_gh_c985615b5f2b_258.jpg&#34; alt=&#34;公众号&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;英文名字集&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/toniprada/usa-names-dataset&#34;&gt;https://github.com/toniprada/usa-names-dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;许可证声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目采用 &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt; 许可证。 这意味着您可以：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;分享 — 在任何媒介以任何形式复制、发行本作品 但必须遵守以下条件：&lt;/li&gt; &#xA; &lt;li&gt;非商业性使用 — 您不得将本作品用于商业目的&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;声明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本项目仅供学习交流使用，请勿用于商业用途。&lt;/li&gt; &#xA; &lt;li&gt;本项目不承担任何法律责任，使用本项目造成的任何后果，由使用者自行承担。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;骗子&lt;/h2&gt; &#xA;&lt;p&gt;海豚&lt;/p&gt; &#xA;&lt;h2&gt;感谢 linuxDo 这个开源社区(一个真正的技术社区)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://linux.do/&#34;&gt;https://linux.do/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;特别鸣谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目的开发过程中得到了众多开源项目和社区成员的支持与帮助，在此特别感谢：&lt;/p&gt; &#xA;&lt;h3&gt;开源项目&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yuaotian/go-cursor-help&#34;&gt;go-cursor-help&lt;/a&gt; - 一个优秀的 Cursor 机器码重置工具，本项目的机器码重置功能使用该项目实现。该项目目前已获得 9.1k Stars，是最受欢迎的 Cursor 辅助工具之一。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;请我喝杯茶 | buy me a cup of tea&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chengazhen/cursor-auto-free/main/screen/image.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chengazhen/cursor-auto-free/main/screen/28613e3f3f23a935b66a7ba31ff4e3f.jpg&#34; width=&#34;300&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chengazhen/cursor-auto-free/main/screen/mm_facetoface_collect_qrcode_1738583247120.png&#34; width=&#34;300&#34;&gt;</summary>
  </entry>
  <entry>
    <title>meta-llama/PurpleLlama</title>
    <updated>2025-04-08T01:36:48Z</updated>
    <id>tag:github.com,2025-04-08:/meta-llama/PurpleLlama</id>
    <link href="https://github.com/meta-llama/PurpleLlama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Set of tools to assess and improve LLM security.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/PurpleLlama/raw/main/logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🤗 &lt;a href=&#34;https://huggingface.co/meta-Llama&#34;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai&#34;&gt; Blog&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/llama/purple-llama&#34;&gt;Website&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/&#34;&gt;CyberSec Eval Paper&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&#34;&gt;Llama Guard Paper&lt;/a&gt;&amp;nbsp; &lt;br&gt; &lt;/p&gt;&#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Purple Llama&lt;/h1&gt; &#xA;&lt;p&gt;Purple Llama is an umbrella project that over time will bring together tools and evals to help the community build responsibly with open generative AI models. The initial release will include tools and evals for Cyber Security and Input/Output safeguards but we plan to contribute more in the near future.&lt;/p&gt; &#xA;&lt;h2&gt;Why purple?&lt;/h2&gt; &#xA;&lt;p&gt;Borrowing a &lt;a href=&#34;https://www.youtube.com/watch?v=ab_Fdp6FVDI&#34;&gt;concept&lt;/a&gt; from the cybersecurity world, we believe that to truly mitigate the challenges which generative AI presents, we need to take both attack (red team) and defensive (blue team) postures. Purple teaming, composed of both red and blue team responsibilities, is a collaborative approach to evaluating and mitigating potential risks and the same ethos applies to generative AI and hence our investment in Purple Llama will be comprehensive.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Components within the Purple Llama project will be licensed permissively enabling both research and commercial usage. We believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development. More concretely evals and benchmarks are licensed under the MIT license while any models use the corresponding Llama Community license. See the table below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Component Type&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Components&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Evals/Benchmarks&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cyber Security Eval (others to come)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Safeguard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama Guard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/PurpleLlama/raw/main/LICENSE&#34;&gt;Llama 2 Community License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Safeguard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama Guard 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/meta-llama/llama3/raw/main/LICENSE&#34;&gt;Llama 3 Community License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Safeguard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama Guard 3-8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/PurpleLlama/main/LICENSE&#34;&gt;Llama 3.2 Community License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Safeguard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama Guard 3-1B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/PurpleLlama/main/LICENSE&#34;&gt;Llama 3.2 Community License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Safeguard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama Guard 3-11B-vision&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/PurpleLlama/main/LICENSE&#34;&gt;Llama 3.2 Community License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Safeguard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Prompt Guard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/PurpleLlama/main/LICENSE&#34;&gt;Llama 3.2 Community License&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Safeguard&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Code Shield&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;System-Level Safeguards&lt;/h2&gt; &#xA;&lt;p&gt;As we outlined in Llama 3’s &lt;a href=&#34;https://ai.meta.com/llama/responsible-use-guide/&#34;&gt;Responsible Use Guide&lt;/a&gt;, we recommend that all inputs and outputs to the LLM be checked and filtered in accordance with content guidelines appropriate to the application.&lt;/p&gt; &#xA;&lt;h3&gt;Llama Guard&lt;/h3&gt; &#xA;&lt;p&gt;Llama Guard 3 consists of a series of high-performance input and output moderation models designed to support developers to detect various common types of violating content.&lt;/p&gt; &#xA;&lt;p&gt;They were built by fine-tuning Meta-Llama 3.1 and 3.2 models and optimized to support the detection of the MLCommons standard hazards taxonomy, catering to a range of developer use cases. They support the release of Llama 3.2 capabilities, including 7 new languages, a 128k context window, and image reasoning. Llama Guard 3 models were also optimized to detect helpful cyberattack responses and prevent malicious code output by LLMs to be executed in hosting environments for Llama systems using code interpreters.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt Guard&lt;/h3&gt; &#xA;&lt;p&gt;Prompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.&lt;/p&gt; &#xA;&lt;p&gt;Categories of prompt attacks include prompt injection and jailbreaking:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prompt Injections are inputs that exploit the inclusion of untrusted data from third parties into the context window of a model to get it to execute unintended instructions.&lt;/li&gt; &#xA; &lt;li&gt;Jailbreaks are malicious instructions designed to override the safety and security features built into a model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Code Shield&lt;/h3&gt; &#xA;&lt;p&gt;Code Shield adds support for inference-time filtering of insecure code produced by LLMs. Code Shield offers mitigation of insecure code suggestions risk, code interpreter abuse prevention, and secure command execution. &lt;a href=&#34;https://github.com/meta-llama/PurpleLlama/raw/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb&#34;&gt;CodeShield Example Notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Evals &amp;amp; Benchmarks&lt;/h2&gt; &#xA;&lt;h3&gt;Cybersecurity&lt;/h3&gt; &#xA;&lt;h4&gt;CyberSec Eval v1&lt;/h4&gt; &#xA;&lt;p&gt;CyberSec Eval v1 was what we believe was the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&amp;amp;CK) and built in collaboration with our security subject matter experts. We aim to provide tools that will help address some risks outlined in the &lt;a href=&#34;https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/&#34;&gt;White House commitments on developing responsible AI&lt;/a&gt;, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Metrics for quantifying LLM cybersecurity risks.&lt;/li&gt; &#xA; &lt;li&gt;Tools to evaluate the frequency of insecure code suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our &lt;a href=&#34;https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/&#34;&gt;Cybersec Eval paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h4&gt;CyberSec Eval 2&lt;/h4&gt; &#xA;&lt;p&gt;CyberSec Eval 2 expands on its predecessor by measuring an LLM’s propensity to abuse a code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection. You can read the paper &lt;a href=&#34;https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also check out the 🤗 leaderboard &lt;a href=&#34;https://huggingface.co/spaces/facebook/CyberSecEval&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;CyberSec Eval 3&lt;/h4&gt; &#xA;&lt;p&gt;The newly released CyberSec Eval 3 features three additional test suites: visual prompt injection tests, spear phishing capability tests, and autonomous offensive cyber operations tests.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;As part of the &lt;a href=&#34;https://github.com/meta-llama/llama-agentic-system&#34;&gt;Llama reference system&lt;/a&gt;, we’re integrating a safety layer to facilitate adoption and deployment of these safeguards. Resources to get started with the safeguards are available in the &lt;a href=&#34;https://github.com/meta-llama/llama-recipes&#34;&gt;Llama-recipe GitHub repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;For a running list of frequently asked questions, for not only Purple Llama components but also generally for Llama models, see the FAQ &lt;a href=&#34;https://ai.meta.com/llama/faq/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Join the Purple Llama community&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/PurpleLlama/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vllm-project/llm-compressor</title>
    <updated>2025-04-08T01:36:48Z</updated>
    <id>tag:github.com,2025-04-08:/vllm-project/llm-compressor</id>
    <link href="https://github.com/vllm-project/llm-compressor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Transformers-compatible library for applying various compression algorithms to LLMs for optimized deployment with vLLM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img width=&#34;40&#34; alt=&#34;tool icon&#34; src=&#34;https://github.com/user-attachments/assets/f9b86465-aefa-4625-a09b-54e158efcf96&#34;&gt; LLM Compressor&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;llmcompressor&lt;/code&gt; is an easy-to-use library for optimizing models for deployment with &lt;code&gt;vllm&lt;/code&gt;, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comprehensive set of quantization algorithms for weight-only and activation quantization&lt;/li&gt; &#xA; &lt;li&gt;Seamless integration with Hugging Face models and repositories&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;safetensors&lt;/code&gt;-based file format compatible with &lt;code&gt;vllm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Large model support via &lt;code&gt;accelerate&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;✨ Read the announcement blog &lt;a href=&#34;https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/&#34;&gt;here&lt;/a&gt;! ✨&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;LLM Compressor Flow&#34; src=&#34;https://github.com/user-attachments/assets/adf07594-6487-48ae-af62-d9555046d51b&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Supported Formats&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Activation Quantization: W8A8 (int8 and fp8)&lt;/li&gt; &#xA; &lt;li&gt;Mixed Precision: W4A16, W8A16&lt;/li&gt; &#xA; &lt;li&gt;2:4 Semi-structured and Unstructured Sparsity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported Algorithms&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple PTQ&lt;/li&gt; &#xA; &lt;li&gt;GPTQ&lt;/li&gt; &#xA; &lt;li&gt;SmoothQuant&lt;/li&gt; &#xA; &lt;li&gt;SparseGPT&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;When to Use Which Optimization&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/docs/schemes.md&#34;&gt;docs/schemes.md&lt;/a&gt; for detailed information about available optimization schemes and their use cases.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install llmcompressor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;End-to-End Examples&lt;/h3&gt; &#xA;&lt;p&gt;Applying quantization with &lt;code&gt;llmcompressor&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/examples/quantization_w8a8_int8/README.md&#34;&gt;Activation quantization to &lt;code&gt;int8&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/examples/quantization_w8a8_fp8/README.md&#34;&gt;Activation quantization to &lt;code&gt;fp8&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/examples/quantization_w4a16/README.md&#34;&gt;Weight only quantization to &lt;code&gt;int4&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/examples/quantizing_moe/README.md&#34;&gt;Quantizing MoE LLMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/examples/multimodal_vision/README.md&#34;&gt;Quantizing Vision-Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/examples/multimodal_audio/README.md&#34;&gt;Quantizing Audio-Language Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;User Guides&lt;/h3&gt; &#xA;&lt;p&gt;Deep dives into advanced usage of &lt;code&gt;llmcompressor&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/examples/big_models_with_accelerate/README.md&#34;&gt;Quantizing with large models with the help of &lt;code&gt;accelerate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Tour&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s quantize &lt;code&gt;TinyLlama&lt;/code&gt; with 8 bit weights and activations using the &lt;code&gt;GPTQ&lt;/code&gt; and &lt;code&gt;SmoothQuant&lt;/code&gt; algorithms.&lt;/p&gt; &#xA;&lt;p&gt;Note that the model can be swapped for a local or remote HF-compatible checkpoint and the &lt;code&gt;recipe&lt;/code&gt; may be changed to target different quantization algorithms or formats.&lt;/p&gt; &#xA;&lt;h3&gt;Apply Quantization&lt;/h3&gt; &#xA;&lt;p&gt;Quantization is applied by selecting an algorithm and calling the &lt;code&gt;oneshot&lt;/code&gt; API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llmcompressor.modifiers.smoothquant import SmoothQuantModifier&#xA;from llmcompressor.modifiers.quantization import GPTQModifier&#xA;from llmcompressor import oneshot&#xA;&#xA;# Select quantization algorithm. In this case, we:&#xA;#   * apply SmoothQuant to make the activations easier to quantize&#xA;#   * quantize the weights to int8 with GPTQ (static per channel)&#xA;#   * quantize the activations to int8 (dynamic per token)&#xA;recipe = [&#xA;    SmoothQuantModifier(smoothing_strength=0.8),&#xA;    GPTQModifier(scheme=&#34;W8A8&#34;, targets=&#34;Linear&#34;, ignore=[&#34;lm_head&#34;]),&#xA;]&#xA;&#xA;# Apply quantization using the built in open_platypus dataset.&#xA;#   * See examples for demos showing how to pass a custom calibration set&#xA;oneshot(&#xA;    model=&#34;TinyLlama/TinyLlama-1.1B-Chat-v1.0&#34;,&#xA;    dataset=&#34;open_platypus&#34;,&#xA;    recipe=recipe,&#xA;    output_dir=&#34;TinyLlama-1.1B-Chat-v1.0-INT8&#34;,&#xA;    max_seq_length=2048,&#xA;    num_calibration_samples=512,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference with vLLM&lt;/h3&gt; &#xA;&lt;p&gt;The checkpoints created by &lt;code&gt;llmcompressor&lt;/code&gt; can be loaded and run in &lt;code&gt;vllm&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install vllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from vllm import LLM&#xA;model = LLM(&#34;TinyLlama-1.1B-Chat-v1.0-INT8&#34;)&#xA;output = model.generate(&#34;My name is&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Questions / Contribution&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have any questions or requests open an &lt;a href=&#34;https://github.com/vllm-project/llm-compressor/issues&#34;&gt;issue&lt;/a&gt; and we will add an example or documentation.&lt;/li&gt; &#xA; &lt;li&gt;We appreciate contributions to the code, examples, integrations, and documentation as well as bug reports and feature requests! &lt;a href=&#34;https://raw.githubusercontent.com/vllm-project/llm-compressor/main/CONTRIBUTING.md&#34;&gt;Learn how here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>