<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-09T01:44:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>InternLM/InternLM</title>
    <updated>2023-07-09T01:44:11Z</updated>
    <id>tag:github.com,2023-07-09:/InternLM/InternLM</id>
    <link href="https://github.com/InternLM/InternLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;InternLM has open-sourced a 7 billion parameter base model, a chat model tailored for practical scenarios and the training system.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;InternLM&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/imgs/logo.svg?sanitize=true&#34; width=&#34;200&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;InternLM&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://internlm.intern-ai.org.cn/&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA;  &lt;div&gt;&#xA;   &amp;nbsp;&#xA;  &lt;/div&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/LICENSE&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/imgs/license.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/internLM/OpenCompass/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/imgs/compass_support.svg?sanitize=true&#34; alt=&#34;evaluation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/en/usage.md&#34;&gt;üìòUsage&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/en/install.md&#34;&gt;üõ†Ô∏èInstallation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/en/train_performance.md&#34;&gt;üìäTrain Performance&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/#model-zoo&#34;&gt;üëÄModel&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/CHANGE_LOG.md&#34;&gt;üÜïUpdate News&lt;/a&gt; | &lt;a href=&#34;https://github.com/InternLM/InternLM/issues/new&#34;&gt;ü§îReporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/README-zh-Hans.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It leverages trillions of high-quality tokens for training to establish a powerful knowledge base.&lt;/li&gt; &#xA; &lt;li&gt;It supports an 8k context window length, enabling longer input sequences and stronger reasoning capabilities.&lt;/li&gt; &#xA; &lt;li&gt;It provides a versatile toolset for users to flexibly build their own workflows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, a lightweight training framework is offered to support model pre-training without the need for extensive dependencies. With a single codebase, it supports pre-training on large-scale clusters with thousands of GPUs, and fine-tuning on a single GPU while achieving remarkable performance optimizations. InternLM achieves nearly 90% acceleration efficiency during training on 1024 GPUs.&lt;/p&gt; &#xA;&lt;h2&gt;InternLM-7B&lt;/h2&gt; &#xA;&lt;h3&gt;Performance Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool &lt;a href=&#34;https://github.com/internLM/OpenCompass/&#34;&gt;OpenCompass&lt;/a&gt;. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the &lt;a href=&#34;https://opencompass.org.cn/rank&#34;&gt;OpenCompass leaderboard&lt;/a&gt; for more evaluation results.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datasets\Models&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;InternLM-Chat-7B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;InternLM-7B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;LLaMA-7B&lt;/th&gt; &#xA;   &lt;th&gt;Baichuan-7B&lt;/th&gt; &#xA;   &lt;th&gt;ChatGLM2-6B&lt;/th&gt; &#xA;   &lt;th&gt;Alpaca-7B&lt;/th&gt; &#xA;   &lt;th&gt;Vicuna-7B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C-Eval(Val)&lt;/td&gt; &#xA;   &lt;td&gt;53.2&lt;/td&gt; &#xA;   &lt;td&gt;53.4&lt;/td&gt; &#xA;   &lt;td&gt;24.2&lt;/td&gt; &#xA;   &lt;td&gt;42.7&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;28.9&lt;/td&gt; &#xA;   &lt;td&gt;31.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td&gt;50.8&lt;/td&gt; &#xA;   &lt;td&gt;51.0&lt;/td&gt; &#xA;   &lt;td&gt;35.2*&lt;/td&gt; &#xA;   &lt;td&gt;41.5&lt;/td&gt; &#xA;   &lt;td&gt;46.0&lt;/td&gt; &#xA;   &lt;td&gt;39.7&lt;/td&gt; &#xA;   &lt;td&gt;47.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AGIEval&lt;/td&gt; &#xA;   &lt;td&gt;42.5&lt;/td&gt; &#xA;   &lt;td&gt;37.6&lt;/td&gt; &#xA;   &lt;td&gt;20.8&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;   &lt;td&gt;39.0&lt;/td&gt; &#xA;   &lt;td&gt;24.1&lt;/td&gt; &#xA;   &lt;td&gt;26.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CommonSenseQA&lt;/td&gt; &#xA;   &lt;td&gt;75.2&lt;/td&gt; &#xA;   &lt;td&gt;59.5&lt;/td&gt; &#xA;   &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;td&gt;58.8&lt;/td&gt; &#xA;   &lt;td&gt;60.0&lt;/td&gt; &#xA;   &lt;td&gt;68.7&lt;/td&gt; &#xA;   &lt;td&gt;66.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BUSTM&lt;/td&gt; &#xA;   &lt;td&gt;74.3&lt;/td&gt; &#xA;   &lt;td&gt;50.6&lt;/td&gt; &#xA;   &lt;td&gt;48.5&lt;/td&gt; &#xA;   &lt;td&gt;51.3&lt;/td&gt; &#xA;   &lt;td&gt;55.0&lt;/td&gt; &#xA;   &lt;td&gt;48.8&lt;/td&gt; &#xA;   &lt;td&gt;62.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CLUEWSC&lt;/td&gt; &#xA;   &lt;td&gt;78.6&lt;/td&gt; &#xA;   &lt;td&gt;59.1&lt;/td&gt; &#xA;   &lt;td&gt;50.3&lt;/td&gt; &#xA;   &lt;td&gt;52.8&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;   &lt;td&gt;50.3&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MATH&lt;/td&gt; &#xA;   &lt;td&gt;6.4&lt;/td&gt; &#xA;   &lt;td&gt;7.1&lt;/td&gt; &#xA;   &lt;td&gt;2.8&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;6.6&lt;/td&gt; &#xA;   &lt;td&gt;2.2&lt;/td&gt; &#xA;   &lt;td&gt;2.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GSM8K&lt;/td&gt; &#xA;   &lt;td&gt;34.5&lt;/td&gt; &#xA;   &lt;td&gt;31.2&lt;/td&gt; &#xA;   &lt;td&gt;10.1&lt;/td&gt; &#xA;   &lt;td&gt;9.7&lt;/td&gt; &#xA;   &lt;td&gt;29.2&lt;/td&gt; &#xA;   &lt;td&gt;6.0&lt;/td&gt; &#xA;   &lt;td&gt;15.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HumanEval&lt;/td&gt; &#xA;   &lt;td&gt;14.0&lt;/td&gt; &#xA;   &lt;td&gt;10.4&lt;/td&gt; &#xA;   &lt;td&gt;14.0&lt;/td&gt; &#xA;   &lt;td&gt;9.2&lt;/td&gt; &#xA;   &lt;td&gt;9.2&lt;/td&gt; &#xA;   &lt;td&gt;9.2&lt;/td&gt; &#xA;   &lt;td&gt;11.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RACE(High)&lt;/td&gt; &#xA;   &lt;td&gt;76.3&lt;/td&gt; &#xA;   &lt;td&gt;57.4&lt;/td&gt; &#xA;   &lt;td&gt;46.9*&lt;/td&gt; &#xA;   &lt;td&gt;28.1&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;40.7&lt;/td&gt; &#xA;   &lt;td&gt;54.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The evaluation results were obtained from &lt;a href=&#34;https://github.com/internLM/OpenCompass/&#34;&gt;OpenCompass 20230706&lt;/a&gt; (some data marked with *, which means come from the original papers), and evaluation configuration can be found in the configuration files provided by &lt;a href=&#34;https://github.com/internLM/OpenCompass/&#34;&gt;OpenCompass&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The evaluation data may have numerical differences due to the version iteration of &lt;a href=&#34;https://github.com/internLM/OpenCompass/&#34;&gt;OpenCompass&lt;/a&gt;, so please refer to the latest evaluation results of &lt;a href=&#34;https://github.com/internLM/OpenCompass/&#34;&gt;OpenCompass&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;p&gt;InternLM 7B and InternLM 7B Chat, trained using InternLM, have been open-sourced. We provide two formats of model weights for use. In addition to loading the models using the Transformers format, you can also load the weights directly using InternLM for further pre-training or human preference alignment training.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;InternLM Format Weight Download Link&lt;/th&gt; &#xA;   &lt;th&gt;Transformers Format Weight Download Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;InternLM 7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-7b&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/header/openxlab_models.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/internlm/internlm-7b&#34;&gt;ü§óinternlm/intern-7b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;InternLM Chat 7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-chat-7b&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/header/openxlab_models.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/internlm/internlm-chat-7b&#34;&gt;ü§óinternlm/intern-chat-7b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;InternLM Chat 7B 8k&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openxlab.org.cn/models/detail/OpenLMLab/InternLM-chat-7b-8k&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/header/openxlab_models.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/internlm/internlm-chat-7b-8k&#34;&gt;ü§óinternlm/intern-chat-7b-8k&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt; Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.&lt;/p&gt; &#xA;&lt;h3&gt;Import from Transformers&lt;/h3&gt; &#xA;&lt;p&gt;To load the InternLM 7B Chat model using Transformers, use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(&#34;internlm/internlm-chat-7b&#34;, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model = AutoModelForCausalLM.from_pretrained(&#34;internlm/internlm-chat-7b&#34;, trust_remote_code=True).cuda()&#xA;&amp;gt;&amp;gt;&amp;gt; model = model.eval()&#xA;&amp;gt;&amp;gt;&amp;gt; response, history = model.chat(tokenizer, &#34;hello&#34;, history=[])&#xA;&amp;gt;&amp;gt;&amp;gt; print(response)&#xA;Hello! How can I help you today?&#xA;&amp;gt;&amp;gt;&amp;gt; response, history = model.chat(tokenizer, &#34;please provide three suggestions about time management&#34;, history=history)&#xA;&amp;gt;&amp;gt;&amp;gt; print(response)&#xA;Sure, here are three tips for effective time management:&#xA;&#xA;1. Prioritize tasks based on importance and urgency: Make a list of all your tasks and categorize them into &#34;important and urgent,&#34; &#34;important but not urgent,&#34; and &#34;not important but urgent.&#34; Focus on completing the tasks in the first category before moving on to the others.&#xA;2. Use a calendar or planner: Write down deadlines and appointments in a calendar or planner so you don&#39;t forget them. This will also help you schedule your time more effectively and avoid overbooking yourself.&#xA;3. Minimize distractions: Try to eliminate any potential distractions when working on important tasks. Turn off notifications on your phone, close unnecessary tabs on your computer, and find a quiet place to work if possible.&#xA;&#xA;Remember, good time management skills take practice and patience. Start with small steps and gradually incorporate these habits into your daily routine.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dialogue&lt;/h3&gt; &#xA;&lt;p&gt;You can interact with the InternLM Chat 7B model through a frontend interface by running the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install streamlit==1.24.0&#xA;pip install transformers==4.30.2&#xA;streamlit run web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The effect is as follows&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/InternLM/InternLM/assets/9102141/11b60ee0-47e4-42c0-8278-3051b2f17fe4&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/InternLM/LMDeploy&#34;&gt;LMDeploy&lt;/a&gt; to complete the one-click deployment of InternLM.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, install LMDeploy:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;  python3 -m pip install lmdeploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Use the following command for quick deployment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;  python3 -m lmdeploy.serve.turbomind.deploy InternLM-7B /path/to/internlm-7b/model hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;After exporting the model, you can start a server and have a conversation with the deployed model using the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;  python3 -m lmdeploy.serve.client {server_ip_addresss}:33337&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/InternLM/LMDeploy&#34;&gt;LMDeploy&lt;/a&gt; provides a complete workflow for deploying InternLM. Please refer to the &lt;a href=&#34;https://github.com/InternLM/LMDeploy&#34;&gt;deployment tutorial&lt;/a&gt; for more details on deploying InternLM.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning &amp;amp; Training&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-training and Fine-tuning Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/en/usage.md&#34;&gt;Usage Tutorial&lt;/a&gt; to start InternLM installation, data processing, pre-training and fine-tuning.&lt;/p&gt; &#xA;&lt;h3&gt;Convert to Transformers Format&lt;/h3&gt; &#xA;&lt;p&gt;The model trained by InternLM can be easily converted to HuggingFace Transformers format, which is convenient for seamless docking with various open source projects in the community. With the help of &lt;code&gt;tools/convert2hf.py&lt;/code&gt;, the weights saved during training can be converted into transformers format with one command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python convert2hf.py --src_folder origin_ckpt/ --tgt_folder hf_ckpt/ --tokenizer tokenizes/tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After conversion, it can be loaded as transformers by the following code&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer, AutoModel&#xA;&amp;gt;&amp;gt;&amp;gt; model = AutoModel.from_pretrained(&#34;hf_ckpt/&#34;, trust_remote_code=True).cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training System&lt;/h2&gt; &#xA;&lt;h3&gt;System Architecture&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/en/structure.md&#34;&gt;System Architecture document&lt;/a&gt; for further details.&lt;/p&gt; &#xA;&lt;h3&gt;Training Performance&lt;/h3&gt; &#xA;&lt;p&gt;InternLM deeply integrates Flash-Attention, Apex and other high-performance model operators to improve training efficiency. By building the Hybrid Zero technique, it achieves efficient overlap of computation and communication, significantly reducing cross-node communication traffic during training. InternLM supports expanding the 7B model from 8 GPUs to 1024 GPUs, with an acceleration efficiency of up to 90% at the thousand-GPU scale, a training throughput of over 180 TFLOPS, and an average of over 3600 tokens per GPU per second. The following table shows InternLM&#39;s scalability test data at different configurations:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;GPU Number&lt;/th&gt; &#xA;   &lt;th&gt;8&lt;/th&gt; &#xA;   &lt;th&gt;16&lt;/th&gt; &#xA;   &lt;th&gt;32&lt;/th&gt; &#xA;   &lt;th&gt;64&lt;/th&gt; &#xA;   &lt;th&gt;128&lt;/th&gt; &#xA;   &lt;th&gt;256&lt;/th&gt; &#xA;   &lt;th&gt;512&lt;/th&gt; &#xA;   &lt;th&gt;1024&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TGS&lt;/td&gt; &#xA;   &lt;td&gt;4078&lt;/td&gt; &#xA;   &lt;td&gt;3939&lt;/td&gt; &#xA;   &lt;td&gt;3919&lt;/td&gt; &#xA;   &lt;td&gt;3944&lt;/td&gt; &#xA;   &lt;td&gt;3928&lt;/td&gt; &#xA;   &lt;td&gt;3920&lt;/td&gt; &#xA;   &lt;td&gt;3835&lt;/td&gt; &#xA;   &lt;td&gt;3625&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TFLOPS&lt;/td&gt; &#xA;   &lt;td&gt;193&lt;/td&gt; &#xA;   &lt;td&gt;191&lt;/td&gt; &#xA;   &lt;td&gt;188&lt;/td&gt; &#xA;   &lt;td&gt;188&lt;/td&gt; &#xA;   &lt;td&gt;187&lt;/td&gt; &#xA;   &lt;td&gt;185&lt;/td&gt; &#xA;   &lt;td&gt;186&lt;/td&gt; &#xA;   &lt;td&gt;184&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;TGS represents the average number of tokens processed per GPU per second. For more performance test data, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM/main/doc/en/train_performance.md&#34;&gt;Training Performance document&lt;/a&gt; for further details.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all the contributors for their efforts to improve and enhance InternLM. Community users are highly encouraged to participate in the project. Please refer to the contribution guidelines for instructions on how to contribute to the project.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;InternLM codebase is an open-source project contributed by Shanghai AI Laboratory and researchers from different universities and companies. We would like to thank all the contributors for their support in adding new features to the project and the users for providing valuable feedback. We hope that this toolkit and benchmark can provide the community with flexible and efficient code tools for fine-tuning InternLM and developing their own models, thus continuously contributing to the open-source community. Special thanks to the two open-source projects, &lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;flash-attention&lt;/a&gt; and &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;ColossalAI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow &lt;strong&gt;free&lt;/strong&gt; commercial usage. To apply for a commercial license, please fill in the &lt;a href=&#34;https://wj.qq.com/s2/12727483/5dba/&#34;&gt;application form (English)&lt;/a&gt;/&lt;a href=&#34;https://wj.qq.com/s2/12725412/f7c1/&#34;&gt;Áî≥ËØ∑Ë°®Ôºà‰∏≠ÊñáÔºâ&lt;/a&gt;. For other questions or collaborations, please contact &lt;a href=&#34;mailto:internlm@pjlab.org.cn&#34;&gt;internlm@pjlab.org.cn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{2023internlm,&#xA;    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},&#xA;    author={InternLM Team},&#xA;    howpublished = {\url{https://github.com/InternLM/InternLM}},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>AssassinUKG/googleSearcher</title>
    <updated>2023-07-09T01:44:11Z</updated>
    <id>tag:github.com,2023-07-09:/AssassinUKG/googleSearcher</id>
    <link href="https://github.com/AssassinUKG/googleSearcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A custom Google search (to bypass some limitations on google and VPNs)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;googleSearcher&lt;/h1&gt; &#xA;&lt;p&gt;A custom Google search (to bypass some limitations on Google with regards to timeouts, vpns etc)&lt;/p&gt; &#xA;&lt;p&gt;The tool has been coded to only return 100 (max) URLs from google search. Google only allows 100 results. You can get 100 from pages 1-10 or pages 11-20 etc, but only ever 100 max results at a time.&lt;br&gt; A Limitation imposed by Google.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;WARNING you will quickly hit the Google limit and will need a paid account to get more. Script has been edited to allow number choice 1-100 results&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Great for OSINT and Google Dorks!&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/AssassinUKG/googleSearcher.git&#xA;cd googleSearcher&#xA;pip install -r requirements.txt&#xA;&#xA;cd gSearcher&#xA;python3 gsearcher.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 gsearcher.py -s &#34;filetype:pdf site:tesla.com&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 gsearcher.py -s &#34;cats&#34; -p 4 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 gsearcher.py -s &#34;dogs&#34; -n 30 -p 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Options&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: gSearch.py [-h] [-s SEARCHTERM] [-n NUMRESULTS] [-p PAGE]&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  -s SEARCHTERM, --searchTerm SEARCHTERM&#xA;                        Enter a search term!&#xA;  -n NUMRESULTS, --numResults NUMRESULTS&#xA;                        Enter the number of results to fetch (max 100)&#xA;  -p PAGE, --page PAGE  Enter the page number&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;filetype dork (pdf) &lt;img src=&#34;https://github.com/AssassinUKG/googleSearcher/assets/5285547/e9f3bce8-3481-4a6f-82e6-3401ae72b463&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AssassinUKG/googleSearcher/assets/5285547/d39c662f-4768-41a4-8fba-d31eaf410d80&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;** Update: 04/07/2023: Added option for number of results to help free users ** &lt;img src=&#34;https://github.com/AssassinUKG/googleSearcher/assets/5285547/76534204-3b00-41f8-8a82-4f864b6d57f8&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Setup (Google stuffs)&lt;/h2&gt; &#xA;&lt;p&gt;You will need a...&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GOOGLE API Key&lt;/li&gt; &#xA; &lt;li&gt;GOOGLE CUSTOM SEARCH ENGINE ID&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Google API Key&lt;/h3&gt; &#xA;&lt;p&gt;Link: &lt;a href=&#34;https://console.cloud.google.com/&#34;&gt;https://console.cloud.google.com/&lt;/a&gt; To create your application&#39;s API key:&lt;/p&gt; &#xA;&lt;p&gt;Create a new key:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the Google API Console.&lt;/li&gt; &#xA; &lt;li&gt;Create or select a project.&lt;/li&gt; &#xA; &lt;li&gt;On the Credentials page, get an existing API key or create a new one (Create credentials &amp;gt; API key). You can restrict the key before using it in production by clicking Restrict key.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Protect your API key by changing the restrictions to only search API or whatever you need.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Google CSE (Custom Search Engine)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Goto &lt;a href=&#34;https://programmablesearchengine.google.com/controlpanel/all&#34;&gt;https://programmablesearchengine.google.com/controlpanel/all&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add a new search engine and hit save after (see image for settings) &lt;img src=&#34;https://github.com/AssassinUKG/googleSearcher/assets/5285547/86a3dda2-b104-4741-bad3-dc6659084e9a&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;Get your search engine ID&lt;br&gt; &lt;img src=&#34;https://github.com/AssassinUKG/googleSearcher/assets/5285547/cb664dc2-eb03-417d-8dd3-d3f721f7d9e0&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now in the gsearcher.py file replace the variables with your key and cse id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;my_api_key = &#34;Your GOOGLE API KEY&#34;&#xA;my_cse_id = &#34;YOU Custom Search Engine ID&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you&#39;re ready to get results!&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h3&gt;Update: 04/07/2023:&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Added option for number of results to help free users&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>198808xc/Pangu-Weather</title>
    <updated>2023-07-09T01:44:11Z</updated>
    <id>tag:github.com,2023-07-09:/198808xc/Pangu-Weather</id>
    <link href="https://github.com/198808xc/Pangu-Weather" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An official implementation of Pangu-Weather&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Pangu-Weather&lt;/h2&gt; &#xA;&lt;p&gt;This is the official repository for the Pangu-Weather paper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.02556&#34;&gt;Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast&lt;/a&gt;, arXiv preprint: 2211.02556, 2022.&lt;/p&gt; &#xA;&lt;p&gt;by Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu and Qi Tian&lt;/p&gt; &#xA;&lt;p&gt;Resources including pseudocode, pre-trained models, and inference code are released.&lt;/p&gt; &#xA;&lt;p&gt;The slides used in a series of recent talks are attached here. &lt;a href=&#34;https://pan.baidu.com/s/14ZGywcr4XAK5dk75-8PUqA?pwd=9sco&#34;&gt;Baidu Netdisk&lt;/a&gt;, extraction code: 9sco&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The downloaded files shall be organized as the following hierarchy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;‚îú‚îÄ‚îÄ root&#xA;‚îÇ   ‚îú‚îÄ‚îÄ input_data&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ input_surface.npy&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ input_upper.npy&#xA;‚îÇ   ‚îú‚îÄ‚îÄ output_data&#xA;‚îÇ   ‚îú‚îÄ‚îÄ pangu_weather_1.onnx&#xA;‚îÇ   ‚îú‚îÄ‚îÄ pangu_weather_3.onnx&#xA;‚îÇ   ‚îú‚îÄ‚îÄ pangu_weather_6.onnx&#xA;‚îÇ   ‚îú‚îÄ‚îÄ pangu_weather_24.onnx&#xA;‚îÇ   ‚îú‚îÄ‚îÄ inference_cpu.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ inference_gpu.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ inference_iterative.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use a CPU environment, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements_cpu.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use a GPU environment, please first confirm that the cuda version is 11.6 and the cudnn version is the 8.2.4 for Linux and 8.5.0.96 for Windows (please see &lt;a href=&#34;https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html&#34;&gt;this page&lt;/a&gt; for details). Then, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements_gpu.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Global weather forecasting (inference) using the trained models&lt;/h2&gt; &#xA;&lt;h4&gt;Downloading trained models&lt;/h4&gt; &#xA;&lt;p&gt;Please download the four pre-trained models (~1.1GB each) from Google drive or Baidu netdisk:&lt;/p&gt; &#xA;&lt;p&gt;The 1-hour model (pangu_weather_1.onnx): &lt;a href=&#34;https://drive.google.com/file/d/1fg5jkiN_5dHzKb-5H9Aw4MOmfILmeY-S/view?usp=share_link&#34;&gt;Google drive&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1M7SAigVsCSH8hpw6DE8TDQ?pwd=ie0h&#34;&gt;Baidu netdisk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The 3-hour model (pangu_weather_3.onnx): &lt;a href=&#34;https://drive.google.com/file/d/1EdoLlAXqE9iZLt9Ej9i-JW9LTJ9Jtewt/view?usp=share_link&#34;&gt;Google drive&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/197fZsoiCqZYzKwM7tyRrfg?pwd=gmcl&#34;&gt;Baidu netdisk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The 6-hour model (pangu_weather_6.onnx): &lt;a href=&#34;https://drive.google.com/file/d/1a4XTktkZa5GCtjQxDJb_fNaqTAUiEJu4/view?usp=share_link&#34;&gt;Google drive&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1q7IB7tNjqIwoGC7KVMPn4w?pwd=vxq3&#34;&gt;Baidu netdisk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The 24-hour model (pangu_weather_24.onnx): &lt;a href=&#34;https://drive.google.com/file/d/1lweQlxcn9fG0zKNW8ne1Khr9ehRTI6HP/view?usp=share_link&#34;&gt;Google drive&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/179q2gkz2BrsOR6g3yfTVQg?pwd=eajy&#34;&gt;Baidu netdisk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;These models are stored using the ONNX format, and thus can be used via different languages such as Python, C++, C#, Java, etc.&lt;/p&gt; &#xA;&lt;h4&gt;Input data preparation using Python&lt;/h4&gt; &#xA;&lt;p&gt;Please prepare the input data using &lt;a href=&#34;https://numpy.org/&#34;&gt;numpy&lt;/a&gt;. There are two files that shall be put under the &lt;code&gt;input_data&lt;/code&gt; folder, namely, &lt;code&gt;input_surface.npy&lt;/code&gt; and &lt;code&gt;input_upper.npy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;input_surface.npy&lt;/code&gt; stores the input surface variables. It is a numpy array shaped (4,721,1440) where the first dimension represents the 4 surface variables (MSLP, U10, V10, T2M &lt;strong&gt;in the exact order&lt;/strong&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;input_upper.npy&lt;/code&gt; stores the upper-air variables. It is a numpy array shaped (5,13,721,1440) where the first dimension represents the 5 surface variables (Z, Q, T, U and V &lt;strong&gt;in the exact order&lt;/strong&gt;), and the second dimension represents the 13 pressure levels (1000hPa, 925hPa, 850hPa, 700hPa, 600hPa, 500hPa, 400hPa, 300hPa, 250hPa, 200hPa, 150hPa, 100hPa and 50hPa &lt;strong&gt;in the exact order&lt;/strong&gt;).&lt;/p&gt; &#xA;&lt;p&gt;In both cases, the dimensions of 721 and 1440 represent the size along the latitude and longitude, where the numerical range is [90,-90] degree and [0,359.75] degree, respectively, and the spacing is 0.25 degrees. For each 721x1440 slice, the data format is exactly the same as the &lt;code&gt;.nc&lt;/code&gt; file download from the ERA5 official website.&lt;/p&gt; &#xA;&lt;p&gt;Note that the numpy arrays should be in single precision (&lt;code&gt;.astype(np.float32)&lt;/code&gt;), not in double precision.&lt;/p&gt; &#xA;&lt;p&gt;We support ERA5 initial fields and ECMWF initial fields (e.g., the initial fields of the HRES forecast), where the latter often leads to a slight accuracy drop (mainly for T2M because the two fields are quite different in temperature). A &lt;code&gt;.nc&lt;/code&gt; file of ERA5 can be transformed into a &lt;code&gt;.npy&lt;/code&gt; file using the netCDF4 package, and a &lt;code&gt;.grib&lt;/code&gt; file of the ECMWF initial fields can be transformed into a &lt;code&gt;.npy&lt;/code&gt; file using the pygrib package. Note that Z represents geopotential, not geopotential height, so a factor of 9.80665 should be multiplied if the raw data contains the geopotential height.&lt;/p&gt; &#xA;&lt;p&gt;We temporarily do not support other kinds of initial fields due to the possibly dramatic differences in the fields when Z&amp;lt;0.&lt;/p&gt; &#xA;&lt;p&gt;We provide an example of transferred input files, &lt;code&gt;input_surface.npy&lt;/code&gt; and &lt;code&gt;input_upper.npy&lt;/code&gt;, which correspond to the ERA5 initial fields of at 12:00UTC, 2018/09/27. Please download them from Google drive or Baidu netdisk:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;input_surface.npy&lt;/code&gt;: &lt;a href=&#34;https://drive.google.com/file/d/1pj8QEVNpC1FyJfUabDpV4oU3NpSe0BkD/view?usp=share_link&#34;&gt;Google drive&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1i4o5i8guAqmOus6PWncAlA?pwd=4z9s&#34;&gt;Baidu netdisk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;input_upper.npy&lt;/code&gt;: &lt;a href=&#34;https://drive.google.com/file/d/1--7xEBJt79E3oixizr8oFmK_haDE77SS/view?usp=share_link&#34;&gt;Google drive&lt;/a&gt;/&lt;a href=&#34;https://pan.baidu.com/s/1mS8X5MqEdbVfF2u2Us62FQ?pwd=sgx6&#34;&gt;Baidu netdisk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Inference&lt;/h4&gt; &#xA;&lt;p&gt;After the above steps are finished, please check &lt;code&gt;inference_cpu.py&lt;/code&gt; for an example of making a 24-hour weather forecast on CPU with the 24-hour model, and &lt;code&gt;inference_gpu.py&lt;/code&gt; for the GPU version.&lt;/p&gt; &#xA;&lt;p&gt;For example, running the following command, one can get the 24-hour forecast in the &lt;code&gt;output_data&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference_cpu.py # python inference_gpu.py for gpu environment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, &lt;code&gt;inference_iterative.py&lt;/code&gt; shows an example to generate per-6-hour forecast within a week.&lt;/p&gt; &#xA;&lt;h2&gt;Pseudocode and how to use&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pseudocode.py&lt;/code&gt; contains the pseudocode that elaborates our main algorithm. It is written in Python and can be implemented using any deep learning library, e.g. PyTorch and TensorFlow.&lt;/p&gt; &#xA;&lt;p&gt;Note that one needs to download about 60TB of ERA5 data and prepare for computational resource of 3000 GPU-days (in V100) to train each model.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Pangu-Weather is released by Huawei Cloud.&lt;/p&gt; &#xA;&lt;p&gt;The trained parameters of Pangu-Weather are made available under the terms of the BY-NC-SA 4.0 license. You can find details &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The commercial use of these models is forbidden.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, please note that all models were trained using the ERA5 dataset provided by ECMWF. Please do follow &lt;a href=&#34;https://apps.ecmwf.int/datasets/licences/copernicus/&#34;&gt;their policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;If you use the resource in your research, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{bi2022pangu,&#xA;  title={Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast},&#xA;  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},&#xA;  journal={arXiv preprint arXiv:2211.02556},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>