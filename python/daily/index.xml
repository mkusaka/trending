<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-12T01:35:09Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>airweave-ai/airweave</title>
    <updated>2025-05-12T01:35:09Z</updated>
    <id>tag:github.com,2025-05-12:/airweave-ai/airweave</id>
    <link href="https://github.com/airweave-ai/airweave" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Airweave lets agents search any app or database&lt;/p&gt;&lt;hr&gt;&lt;img width=&#34;1673&#34; alt=&#34;airweave-lettermark&#34; style=&#34;padding-bottom: 12px;&#34; src=&#34;https://github.com/user-attachments/assets/e79a9af7-2e93-4888-9cf4-0f700f19fe05&#34;&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml&#34;&gt;&lt;img src=&#34;https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg?sanitize=true&#34; alt=&#34;Ruff&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml&#34;&gt;&lt;img src=&#34;https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg?sanitize=true&#34; alt=&#34;ESLint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/airweave-ai/airweave/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/airweave-ai/airweave/actions/workflows/tests.yml/badge.svg?branch=main&#34; alt=&#34;Backend Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/airweave-ai/airweave&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/airweave-ai/airweave/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/484HY9Ehxt&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1323415085011701870?label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Airweave is a tool that lets agents semantically search any application or database. It&#39;s MCP compatible and seamlessly connects any app, database, or API, to transform their contents into agent-ready knowledge.&lt;/p&gt; &#xA;&lt;h3&gt;🎥 Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/abdf85cb-a8f5-4b6c-b5a3-d4b5177e6bda&#34;&gt;https://github.com/user-attachments/assets/abdf85cb-a8f5-4b6c-b5a3-d4b5177e6bda&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Airweave simplifies the process of making information retrievable for your agent.&lt;/p&gt; &#xA;&lt;p&gt;Whether you have structured or unstructured data, Airweave helps you break it into processable entities, store the data and make it retrievable through &lt;a href=&#34;https://docs.airweave.ai&#34;&gt;REST and MCP endpoints&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#steps&#34;&gt;Steps&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#frontend&#34;&gt;Frontend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#api-endpoints-fastapi&#34;&gt;API Endpoints (FastAPI)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#integrations---adding-more-every-day&#34;&gt;Integrations - adding more every day!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#key-features&#34;&gt;Key Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#technology-stack&#34;&gt;Technology Stack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/#contact--community&#34;&gt;Contact &amp;amp; Community&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Below is a simple guide to get Airweave up and running locally. For more detailed instructions, refer to the &lt;a href=&#34;https://docs.airweave.ai&#34;&gt;docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Steps&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/airweave-ai/airweave.git&#xA;cd airweave&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build and Run&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x start.sh&#xA;./start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;That&#39;s it!&lt;/p&gt; &#xA;&lt;p&gt;You now have Airweave running locally. You can log in to the dashboard, add a connection, configure your sync schedule and find information on your apps.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use Airweave, you can either use the frontend or the API.&lt;/p&gt; &#xA;&lt;h3&gt;Frontend&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Access the React UI at &lt;code&gt;http://localhost:8080&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to Sources to add new integrations.&lt;/li&gt; &#xA; &lt;li&gt;Set up or view your sync schedules under Schedules.&lt;/li&gt; &#xA; &lt;li&gt;Monitor sync jobs in Jobs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;API Endpoints (FastAPI)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Swagger Documentation:&lt;/strong&gt; &lt;code&gt;http://localhost:8001/docs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Get All Sources:&lt;/strong&gt; &lt;code&gt;GET /sources&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connect a Source:&lt;/strong&gt; &lt;code&gt;POST /connections/{short_name}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Find information:&lt;/strong&gt;: &lt;code&gt;POST /search&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;You can configure your own vector database in the app UI or via the API.&lt;/h2&gt; &#xA;&lt;h2&gt;Integrations - adding more every day!&lt;/h2&gt; &#xA;&lt;!-- START_APP_GRID --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;div style=&#34;display: inline-block; text-align: center; padding: 4px;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/asana.svg?sanitize=true&#34; alt=&#34;Asana&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/calendly.svg?sanitize=true&#34; alt=&#34;Calendly&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/chat-gpt.svg?sanitize=true&#34; alt=&#34;Chat-gpt&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/clickup.svg?sanitize=true&#34; alt=&#34;Clickup&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/confluence.svg?sanitize=true&#34; alt=&#34;Confluence&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/dropbox.svg?sanitize=true&#34; alt=&#34;Dropbox&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/facebook.svg?sanitize=true&#34; alt=&#34;Facebook&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/github.svg?sanitize=true&#34; alt=&#34;Github&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/gmail.svg?sanitize=true&#34; alt=&#34;Gmail&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/google_calendar.svg?sanitize=true&#34; alt=&#34;Google Calendar&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/google_drive.svg?sanitize=true&#34; alt=&#34;Google Drive&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/hubspot.svg?sanitize=true&#34; alt=&#34;Hubspot&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/intercom.svg?sanitize=true&#34; alt=&#34;Intercom&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/jira.svg?sanitize=true&#34; alt=&#34;Jira&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/linear.svg?sanitize=true&#34; alt=&#34;Linear&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/linkedin.svg?sanitize=true&#34; alt=&#34;Linkedin&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/mailchimp.svg?sanitize=true&#34; alt=&#34;Mailchimp&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/monday.svg?sanitize=true&#34; alt=&#34;Monday&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/mysql.svg?sanitize=true&#34; alt=&#34;Mysql&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/notion.svg?sanitize=true&#34; alt=&#34;Notion&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/onedrive.svg?sanitize=true&#34; alt=&#34;Onedrive&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/oracle.svg?sanitize=true&#34; alt=&#34;Oracle&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/outlook_calendar.svg?sanitize=true&#34; alt=&#34;Outlook Calendar&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/outlook_mail.svg?sanitize=true&#34; alt=&#34;Outlook Mail&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/perplexity.svg?sanitize=true&#34; alt=&#34;Perplexity&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/postgresql.svg?sanitize=true&#34; alt=&#34;Postgresql&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/salesforce.svg?sanitize=true&#34; alt=&#34;Salesforce&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/slack.svg?sanitize=true&#34; alt=&#34;Slack&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/sql_server.svg?sanitize=true&#34; alt=&#34;Sql Server&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/sqlite.svg?sanitize=true&#34; alt=&#34;Sqlite&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/stripe.svg?sanitize=true&#34; alt=&#34;Stripe&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/todoist.svg?sanitize=true&#34; alt=&#34;Todoist&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt; &#xA; &lt;span style=&#34;width: 40px; display: inline-block; margin: 4px;&#34;&gt;&lt;/span&gt;&#xA; &lt;span style=&#34;width: 40px; display: inline-block; margin: 4px;&#34;&gt;&lt;/span&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/trello.svg?sanitize=true&#34; alt=&#34;Trello&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/whatsapp.svg?sanitize=true&#34; alt=&#34;Whatsapp&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/frontend/src/components/icons/apps/zendesk.svg?sanitize=true&#34; alt=&#34;Zendesk&#34; width=&#34;40&#34; height=&#34;40&#34; style=&#34;margin: 4px; padding: 2px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;!-- END_APP_GRID --&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Over 25 integrations and counting&lt;/strong&gt;: Airweave is your one-stop shop for building agents that need to find information in a single queryable layer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;: Minimal configuration needed to find information in diverse sources: APIs, databases, apps and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt;: Easily add new source and embedder integrations with our&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;White-Labeled Multi-Tenant Support&lt;/strong&gt;: Ideal for SaaS builders, Airweave provides a streamlined OAuth2-based platform for syncing data across multiple tenants while maintaining privacy and security.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Entity Generators&lt;/strong&gt;: Each source (like a database, API, or file system) defines a &lt;code&gt;async def generate_entities()&lt;/code&gt; that yields data in a consistent format. You can also define your own.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automated Sync&lt;/strong&gt;: Schedule data synchronization or run on-demand sync jobs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versioning &amp;amp; Hashing&lt;/strong&gt;: Airweave detects changes in your data via hashing, updating only the modified entities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Async-First&lt;/strong&gt;: Built to handle large-scale data synchronization asynchronously (upcoming: managed Redis workers for production scale).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Deploy locally via Docker Compose for development (upcoming: deploy with Kubernetes for production scale)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Technology Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: &lt;a href=&#34;https://reactjs.org/&#34;&gt;React&lt;/a&gt; (JavaScript/TypeScript)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; (Python)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Infrastructure&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Local / Dev: &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Production: (upcoming) &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; for relational data&lt;/li&gt; &#xA;   &lt;li&gt;Vector database: &lt;a href=&#34;https://qdrant.tech/&#34;&gt;Qdrant&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous Tasks&lt;/strong&gt;: &lt;a href=&#34;https://arq-docs.helpmanual.io/&#34;&gt;ARQ&lt;/a&gt; Redis for background workers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome all contributions! Whether you&#39;re fixing a bug, improving documentation, or adding a new feature:&lt;/p&gt; &#xA;&lt;p&gt;Please follow the existing code style and conventions. See &lt;a href=&#34;https://github.com/airweave-ai/airweave/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Additional Integrations&lt;/strong&gt;: Expand entity generators for popular SaaS APIs and databases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Redis &amp;amp; Worker Queues&lt;/strong&gt;: Improved background job processing and caching for large or frequent syncs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Webhooks&lt;/strong&gt;: Trigger syncs on external events (e.g., new data in a database).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kubernetes Support&lt;/strong&gt;: Offer easy Helm charts for production-scale deployments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Commercial Offerings&lt;/strong&gt;: Enterprise features, extended metrics, and priority support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Airweave is released under an open-core model. The community edition is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/airweave-ai/airweave/main/LICENSE&#34;&gt;MIT&lt;/a&gt;. Additional modules (for enterprise or advanced features) may be licensed separately.&lt;/p&gt; &#xA;&lt;h2&gt;Contact &amp;amp; Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: Join our Discord channel &lt;a href=&#34;https://discord.com/invite/484HY9Ehxt&#34;&gt;here&lt;/a&gt; to get help or discuss features.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: Report bugs or request new features in GitHub Issues.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Twitter&lt;/strong&gt;: Follow &lt;a href=&#34;https://x.com/airweave_ai&#34;&gt;@airweave_ai&lt;/a&gt; for updates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;That&#39;s it! We&#39;re looking forward to seeing what you build. If you have any questions, please don&#39;t hesitate to open an issue or reach out on Discord.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>remsky/Kokoro-FastAPI</title>
    <updated>2025-05-12T01:35:09Z</updated>
    <id>tag:github.com,2025-05-12:/remsky/Kokoro-FastAPI</id>
    <link href="https://github.com/remsky/Kokoro-FastAPI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dockerized FastAPI wrapper for Kokoro-82M text-to-speech model w/CPU ONNX and NVIDIA GPU PyTorch support, handling, and auto-stitching&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/githubbanner.png&#34; alt=&#34;Kokoro TTS Banner&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;sub&gt;&lt;sub&gt;&lt;em&gt;&lt;code&gt;FastKoko&lt;/code&gt;&lt;/em&gt; &lt;/sub&gt;&lt;/sub&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/tests-69-darkgreen&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/coverage-54%25-tan&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Remsky/Kokoro-TTS-Zero&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Try%20on-Spaces-blue&#34; alt=&#34;Try on Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hexgrad/kokoro&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/kokoro-0.9.2-BB5420&#34; alt=&#34;Kokoro&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hexgrad/misaki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/misaki-0.9.3-B8860B&#34; alt=&#34;Misaki&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/hexgrad/Kokoro-82M/commit/9901c2b79161b6e898b7ea857ae5298f47b8b0d6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/last--tested--model--commit-1.0::9901c2b-blue&#34; alt=&#34;Tested at Model Commit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dockerized FastAPI wrapper for &lt;a href=&#34;https://huggingface.co/hexgrad/Kokoro-82M&#34;&gt;Kokoro-82M&lt;/a&gt; text-to-speech model&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-language support (English, Japanese, Korean, Chinese, &lt;em&gt;Vietnamese soon&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;OpenAI-compatible Speech endpoint, NVIDIA GPU accelerated or CPU inference with PyTorch&lt;/li&gt; &#xA; &lt;li&gt;ONNX support coming soon, see v0.1.5 and earlier for legacy ONNX support in the interim&lt;/li&gt; &#xA; &lt;li&gt;Debug endpoints for monitoring system stats, integrated web UI on localhost:8880/web&lt;/li&gt; &#xA; &lt;li&gt;Phoneme-based audio generation, phoneme generation&lt;/li&gt; &#xA; &lt;li&gt;Per-word timestamped caption generation&lt;/li&gt; &#xA; &lt;li&gt;Voice mixing with weighted combinations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Integration Guides&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/remsky/Kokoro-FastAPI/wiki/Setup-Kubernetes&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Helm%20Chart-black?style=flat&amp;amp;logo=helm&amp;amp;logoColor=white&#34; alt=&#34;Helm Chart&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/remsky/Kokoro-FastAPI/wiki/Integrations-DigitalOcean&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/DigitalOcean-black?style=flat&amp;amp;logo=digitalocean&amp;amp;logoColor=white&#34; alt=&#34;DigitalOcean&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/remsky/Kokoro-FastAPI/wiki/Integrations-SillyTavern&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SillyTavern-black?style=flat&amp;amp;color=red&#34; alt=&#34;SillyTavern&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/remsky/Kokoro-FastAPI/wiki/Integrations-OpenWebUi&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenWebUI-black?style=flat&amp;amp;color=white&#34; alt=&#34;OpenWebUI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Quickest Start (docker run)&lt;/summary&gt; &#xA; &lt;p&gt;Pre built images are available to run, with arm/multi-arch support, and baked in models Refer to the core/config.py file for a full list of variables which can be managed via the environment&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# the `latest` tag can be used, though it may have some unexpected bonus features which impact stability.&#xA; Named versions should be pinned for your regular usage.&#xA; Feedback/testing is always welcome&#xA;&#xA;docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest # CPU, or:&#xA;docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest  #NVIDIA GPU&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Quick Start (docker compose) &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install prerequisites, and start the service using Docker Compose (Full setup including UI): &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Install &lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Clone the repository: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/remsky/Kokoro-FastAPI.git&#xA;cd Kokoro-FastAPI&#xA;&#xA;cd docker/gpu  # For GPU support&#xA;# or cd docker/cpu  # For CPU support&#xA;docker compose up --build&#xA;&#xA;# *Note for Apple Silicon (M1/M2) users:&#xA;# The current GPU build relies on CUDA, which is not supported on Apple Silicon.  &#xA;# If you are on an M1/M2/M3 Mac, please use the `docker/cpu` setup.  &#xA;# MPS (Apple&#39;s GPU acceleration) support is planned but not yet available.&#xA;&#xA;# Models will auto-download, but if needed you can manually download:&#xA;python docker/scripts/download_model.py --output api/src/models/v1_0&#xA;&#xA;# Or run directly via UV:&#xA;./start-gpu.sh  # For GPU support&#xA;./start-cpu.sh  # For CPU support&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Direct Run (via uv) &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install prerequisites (): &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;astral-uv&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng&#34;&gt;espeak-ng&lt;/a&gt; in your system if you want it available as a fallback for unknown words/sounds. The upstream libraries may attempt to handle this, but results have varied.&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/remsky/Kokoro-FastAPI.git&#xA;cd Kokoro-FastAPI&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the &lt;a href=&#34;https://github.com/remsky/Kokoro-FastAPI/raw/master/docker/scripts/download_model.py&#34;&gt;model download script&lt;/a&gt; if you haven&#39;t already&lt;/p&gt; &lt;p&gt;Start directly via UV (with hot-reload)&lt;/p&gt; &lt;p&gt;Linux and macOS&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./start-cpu.sh OR&#xA;./start-gpu.sh &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Windows&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;.\start-cpu.ps1 OR&#xA;.\start-gpu.ps1 &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; Up and Running? &lt;/summary&gt; &#xA; &lt;p&gt;Run locally as an OpenAI-Compatible Speech Endpoint&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    base_url=&#34;http://localhost:8880/v1&#34;, api_key=&#34;not-needed&#34;&#xA;)&#xA;&#xA;with client.audio.speech.with_streaming_response.create(&#xA;    model=&#34;kokoro&#34;,&#xA;    voice=&#34;af_sky+af_bella&#34;, #single or multiple voicepack combo&#xA;    input=&#34;Hello world!&#34;&#xA;  ) as response:&#xA;      response.stream_to_file(&#34;output.mp3&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;The API will be available at &lt;a href=&#34;http://localhost:8880&#34;&gt;http://localhost:8880&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;API Documentation: &lt;a href=&#34;http://localhost:8880/docs&#34;&gt;http://localhost:8880/docs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Web Interface: &lt;a href=&#34;http://localhost:8880/web&#34;&gt;http://localhost:8880/web&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;div align=&#34;center&#34; style=&#34;display: flex; justify-content: center; gap: 10px;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/docs-screenshot.png&#34; width=&#34;42%&#34; alt=&#34;API Documentation&#34; style=&#34;border: 2px solid #333; padding: 10px;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/webui-screenshot.png&#34; width=&#34;42%&#34; alt=&#34;Web UI Screenshot&#34; style=&#34;border: 2px solid #333; padding: 10px;&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OpenAI-Compatible Speech Endpoint&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Using OpenAI&#39;s Python library&#xA;from openai import OpenAI&#xA;client = OpenAI(base_url=&#34;http://localhost:8880/v1&#34;, api_key=&#34;not-needed&#34;)&#xA;response = client.audio.speech.create(&#xA;    model=&#34;kokoro&#34;,  &#xA;    voice=&#34;af_bella+af_sky&#34;, # see /api/src/core/openai_mappings.json to customize&#xA;    input=&#34;Hello world!&#34;,&#xA;    response_format=&#34;mp3&#34;&#xA;)&#xA;&#xA;response.stream_to_file(&#34;output.mp3&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Or Via Requests:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;&#xA;&#xA;response = requests.get(&#34;http://localhost:8880/v1/audio/voices&#34;)&#xA;voices = response.json()[&#34;voices&#34;]&#xA;&#xA;# Generate audio&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/v1/audio/speech&#34;,&#xA;    json={&#xA;        &#34;model&#34;: &#34;kokoro&#34;,  &#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;af_bella&#34;,&#xA;        &#34;response_format&#34;: &#34;mp3&#34;,  # Supported: mp3, wav, opus, flac&#xA;        &#34;speed&#34;: 1.0&#xA;    }&#xA;)&#xA;&#xA;# Save audio&#xA;with open(&#34;output.mp3&#34;, &#34;wb&#34;) as f:&#xA;    f.write(response.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Quick tests (run from another terminal):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/assorted_checks/test_openai/test_openai_tts.py # Test OpenAI Compatibility&#xA;python examples/assorted_checks/test_voices/test_all_voices.py # Test all available voices&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Voice Combination&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Weighted voice combinations using ratios (e.g., &#34;af_bella(2)+af_heart(1)&#34; for 67%/33% mix)&lt;/li&gt; &#xA;  &lt;li&gt;Ratios are automatically normalized to sum to 100%&lt;/li&gt; &#xA;  &lt;li&gt;Available through any endpoint by adding weights in parentheses&lt;/li&gt; &#xA;  &lt;li&gt;Saves generated voicepacks for future use&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Combine voices and generate audio:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;response = requests.get(&#34;http://localhost:8880/v1/audio/voices&#34;)&#xA;voices = response.json()[&#34;voices&#34;]&#xA;&#xA;# Example 1: Simple voice combination (50%/50% mix)&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/v1/audio/speech&#34;,&#xA;    json={&#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;af_bella+af_sky&#34;,  # Equal weights&#xA;        &#34;response_format&#34;: &#34;mp3&#34;&#xA;    }&#xA;)&#xA;&#xA;# Example 2: Weighted voice combination (67%/33% mix)&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/v1/audio/speech&#34;,&#xA;    json={&#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;af_bella(2)+af_sky(1)&#34;,  # 2:1 ratio = 67%/33%&#xA;        &#34;response_format&#34;: &#34;mp3&#34;&#xA;    }&#xA;)&#xA;&#xA;# Example 3: Download combined voice as .pt file&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/v1/audio/voices/combine&#34;,&#xA;    json=&#34;af_bella(2)+af_sky(1)&#34;  # 2:1 ratio = 67%/33%&#xA;)&#xA;&#xA;# Save the .pt file&#xA;with open(&#34;combined_voice.pt&#34;, &#34;wb&#34;) as f:&#xA;    f.write(response.content)&#xA;&#xA;# Use the downloaded voice file&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/v1/audio/speech&#34;,&#xA;    json={&#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;combined_voice&#34;,  # Use the saved voice file&#xA;        &#34;response_format&#34;: &#34;mp3&#34;&#xA;    }&#xA;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/voice_analysis.png&#34; width=&#34;80%&#34; alt=&#34;Voice Analysis Comparison&#34; style=&#34;border: 2px solid #333; padding: 10px;&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Multiple Output Audio Formats&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;mp3&lt;/li&gt; &#xA;  &lt;li&gt;wav&lt;/li&gt; &#xA;  &lt;li&gt;opus&lt;/li&gt; &#xA;  &lt;li&gt;flac&lt;/li&gt; &#xA;  &lt;li&gt;m4a&lt;/li&gt; &#xA;  &lt;li&gt;pcm&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/format_comparison.png&#34; width=&#34;80%&#34; alt=&#34;Audio Format Comparison&#34; style=&#34;border: 2px solid #333; padding: 10px;&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Streaming Support&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# OpenAI-compatible streaming&#xA;from openai import OpenAI&#xA;client = OpenAI(&#xA;    base_url=&#34;http://localhost:8880/v1&#34;, api_key=&#34;not-needed&#34;)&#xA;&#xA;# Stream to file&#xA;with client.audio.speech.with_streaming_response.create(&#xA;    model=&#34;kokoro&#34;,&#xA;    voice=&#34;af_bella&#34;,&#xA;    input=&#34;Hello world!&#34;&#xA;) as response:&#xA;    response.stream_to_file(&#34;output.mp3&#34;)&#xA;&#xA;# Stream to speakers (requires PyAudio)&#xA;import pyaudio&#xA;player = pyaudio.PyAudio().open(&#xA;    format=pyaudio.paInt16, &#xA;    channels=1, &#xA;    rate=24000, &#xA;    output=True&#xA;)&#xA;&#xA;with client.audio.speech.with_streaming_response.create(&#xA;    model=&#34;kokoro&#34;,&#xA;    voice=&#34;af_bella&#34;,&#xA;    response_format=&#34;pcm&#34;,&#xA;    input=&#34;Hello world!&#34;&#xA;) as response:&#xA;    for chunk in response.iter_bytes(chunk_size=1024):&#xA;        player.write(chunk)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Or via requests:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/v1/audio/speech&#34;,&#xA;    json={&#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;af_bella&#34;,&#xA;        &#34;response_format&#34;: &#34;pcm&#34;&#xA;    },&#xA;    stream=True&#xA;)&#xA;&#xA;for chunk in response.iter_content(chunk_size=1024):&#xA;    if chunk:&#xA;        # Process streaming chunks&#xA;        pass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/gpu_first_token_timeline_openai.png&#34; width=&#34;45%&#34; alt=&#34;GPU First Token Timeline&#34; style=&#34;border: 2px solid #333; padding: 10px; margin-right: 1%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/cpu_first_token_timeline_stream_openai.png&#34; width=&#34;45%&#34; alt=&#34;CPU First Token Timeline&#34; style=&#34;border: 2px solid #333; padding: 10px;&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;Key Streaming Metrics:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;First token latency @ chunksize &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;~300ms (GPU) @ 400&lt;/li&gt; &#xA;    &lt;li&gt;~3500ms (CPU) @ 200 (older i7)&lt;/li&gt; &#xA;    &lt;li&gt;~&amp;lt;1s (CPU) @ 200 (M3 Pro)&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Adjustable chunking settings for real-time playback&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;em&gt;Note: Artifacts in intonation can increase with smaller chunks&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Processing Details&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Performance Benchmarks&lt;/summary&gt; &#xA; &lt;p&gt;Benchmarking was performed on generation via the local API using text lengths up to feature-length books (~1.5 hours output), measuring processing time and realtime factor. Tests were run on:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Windows 11 Home w/ WSL2&lt;/li&gt; &#xA;  &lt;li&gt;NVIDIA 4060Ti 16gb GPU @ CUDA 12.1&lt;/li&gt; &#xA;  &lt;li&gt;11th Gen i7-11700 @ 2.5GHz&lt;/li&gt; &#xA;  &lt;li&gt;64gb RAM&lt;/li&gt; &#xA;  &lt;li&gt;WAV native output&lt;/li&gt; &#xA;  &lt;li&gt;H.G. Wells - The Time Machine (full text)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/gpu_processing_time.png&#34; width=&#34;45%&#34; alt=&#34;Processing Time&#34; style=&#34;border: 2px solid #333; padding: 10px; margin-right: 1%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/remsky/Kokoro-FastAPI/master/assets/gpu_realtime_factor.png&#34; width=&#34;45%&#34; alt=&#34;Realtime Factor&#34; style=&#34;border: 2px solid #333; padding: 10px;&#34;&gt; &lt;/p&gt; &#xA; &lt;p&gt;Key Performance Metrics:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Realtime Speed: Ranges between 35x-100x (generation time to output audio length)&lt;/li&gt; &#xA;  &lt;li&gt;Average Processing Rate: 137.67 tokens/second (cl100k_base)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;GPU Vs. CPU&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# GPU: Requires NVIDIA GPU with CUDA 12.8 support (~35x-100x realtime speed)&#xA;cd docker/gpu&#xA;docker compose up --build&#xA;&#xA;# CPU: PyTorch CPU inference&#xA;cd docker/cpu&#xA;docker compose up --build&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;em&gt;Note: Overall speed may have reduced somewhat with the structural changes to accommodate streaming. Looking into it&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Natural Boundary Detection&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Automatically splits and stitches at sentence boundaries&lt;/li&gt; &#xA;  &lt;li&gt;Helps to reduce artifacts and allow long form processing as the base model is only currently configured for approximately 30s output&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The model is capable of processing up to a 510 phonemized token chunk at a time, however, this can often lead to &#39;rushed&#39; speech or other artifacts. An additional layer of chunking is applied in the server, that creates flexible chunks with a &lt;code&gt;TARGET_MIN_TOKENS&lt;/code&gt; , &lt;code&gt;TARGET_MAX_TOKENS&lt;/code&gt;, and &lt;code&gt;ABSOLUTE_MAX_TOKENS&lt;/code&gt; which are configurable via environment variables, and set to 175, 250, 450 by default&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Timestamped Captions &amp;amp; Phonemes&lt;/summary&gt; &#xA; &lt;p&gt;Generate audio with word-level timestamps without streaming:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;import base64&#xA;import json&#xA;&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/dev/captioned_speech&#34;,&#xA;    json={&#xA;        &#34;model&#34;: &#34;kokoro&#34;,&#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;af_bella&#34;,&#xA;        &#34;speed&#34;: 1.0,&#xA;        &#34;response_format&#34;: &#34;mp3&#34;,&#xA;        &#34;stream&#34;: False,&#xA;    },&#xA;    stream=False&#xA;)&#xA;&#xA;with open(&#34;output.mp3&#34;,&#34;wb&#34;) as f:&#xA;&#xA;    audio_json=json.loads(response.content)&#xA;    &#xA;    # Decode base 64 stream to bytes&#xA;    chunk_audio=base64.b64decode(audio_json[&#34;audio&#34;].encode(&#34;utf-8&#34;))&#xA;    &#xA;    # Process streaming chunks&#xA;    f.write(chunk_audio)&#xA;    &#xA;    # Print word level timestamps&#xA;    print(audio_json[&#34;timestamps&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Generate audio with word-level timestamps with streaming:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;import base64&#xA;import json&#xA;&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/dev/captioned_speech&#34;,&#xA;    json={&#xA;        &#34;model&#34;: &#34;kokoro&#34;,&#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;af_bella&#34;,&#xA;        &#34;speed&#34;: 1.0,&#xA;        &#34;response_format&#34;: &#34;mp3&#34;,&#xA;        &#34;stream&#34;: True,&#xA;    },&#xA;    stream=True&#xA;)&#xA;&#xA;f=open(&#34;output.mp3&#34;,&#34;wb&#34;)&#xA;for chunk in response.iter_lines(decode_unicode=True):&#xA;    if chunk:&#xA;        chunk_json=json.loads(chunk)&#xA;        &#xA;        # Decode base 64 stream to bytes&#xA;        chunk_audio=base64.b64decode(chunk_json[&#34;audio&#34;].encode(&#34;utf-8&#34;))&#xA;        &#xA;        # Process streaming chunks&#xA;        f.write(chunk_audio)&#xA;        &#xA;        # Print word level timestamps&#xA;        print(chunk_json[&#34;timestamps&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Phoneme &amp;amp; Token Routes&lt;/summary&gt; &#xA; &lt;p&gt;Convert text to phonemes and/or generate audio directly from phonemes:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;&#xA;def get_phonemes(text: str, language: str = &#34;a&#34;):&#xA;    &#34;&#34;&#34;Get phonemes and tokens for input text&#34;&#34;&#34;&#xA;    response = requests.post(&#xA;        &#34;http://localhost:8880/dev/phonemize&#34;,&#xA;        json={&#34;text&#34;: text, &#34;language&#34;: language}  # &#34;a&#34; for American English&#xA;    )&#xA;    response.raise_for_status()&#xA;    result = response.json()&#xA;    return result[&#34;phonemes&#34;], result[&#34;tokens&#34;]&#xA;&#xA;def generate_audio_from_phonemes(phonemes: str, voice: str = &#34;af_bella&#34;):&#xA;    &#34;&#34;&#34;Generate audio from phonemes&#34;&#34;&#34;&#xA;    response = requests.post(&#xA;        &#34;http://localhost:8880/dev/generate_from_phonemes&#34;,&#xA;        json={&#34;phonemes&#34;: phonemes, &#34;voice&#34;: voice},&#xA;        headers={&#34;Accept&#34;: &#34;audio/wav&#34;}&#xA;    )&#xA;    if response.status_code != 200:&#xA;        print(f&#34;Error: {response.text}&#34;)&#xA;        return None&#xA;    return response.content&#xA;&#xA;# Example usage&#xA;text = &#34;Hello world!&#34;&#xA;try:&#xA;    # Convert text to phonemes&#xA;    phonemes, tokens = get_phonemes(text)&#xA;    print(f&#34;Phonemes: {phonemes}&#34;)  # e.g. ðɪs ɪz ˈoʊnli ɐ tˈɛst&#xA;    print(f&#34;Tokens: {tokens}&#34;)      # Token IDs including start/end tokens&#xA;&#xA;    # Generate and save audio&#xA;    if audio_bytes := generate_audio_from_phonemes(phonemes):&#xA;        with open(&#34;speech.wav&#34;, &#34;wb&#34;) as f:&#xA;            f.write(audio_bytes)&#xA;        print(f&#34;Generated {len(audio_bytes)} bytes of audio&#34;)&#xA;except Exception as e:&#xA;    print(f&#34;Error: {e}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;See &lt;code&gt;examples/phoneme_examples/generate_phonemes.py&lt;/code&gt; for a sample script.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Debug Endpoints&lt;/summary&gt; &#xA; &lt;p&gt;Monitor system state and resource usage with these endpoints:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;/debug/threads&lt;/code&gt; - Get thread information and stack traces&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;/debug/storage&lt;/code&gt; - Monitor temp file and output directory usage&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;/debug/system&lt;/code&gt; - Get system information (CPU, memory, GPU)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;/debug/session_pools&lt;/code&gt; - View ONNX session and CUDA stream status&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Useful for debugging resource exhaustion or performance issues.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Known Issues &amp;amp; Troubleshooting&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Missing words &amp;amp; Missing some timestamps&lt;/summary&gt; &#xA; &lt;p&gt;The api will automaticly do text normalization on input text which may incorrectly remove or change some phrases. This can be disabled by adding &lt;code&gt;&#34;normalization_options&#34;:{&#34;normalize&#34;: false}&lt;/code&gt; to your request json:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;&#xA;response = requests.post(&#xA;    &#34;http://localhost:8880/v1/audio/speech&#34;,&#xA;    json={&#xA;        &#34;input&#34;: &#34;Hello world!&#34;,&#xA;        &#34;voice&#34;: &#34;af_heart&#34;,&#xA;        &#34;response_format&#34;: &#34;pcm&#34;,&#xA;        &#34;normalization_options&#34;:&#xA;        {&#xA;            &#34;normalize&#34;: False&#xA;        }&#xA;    },&#xA;    stream=True&#xA;)&#xA;&#xA;for chunk in response.iter_content(chunk_size=1024):&#xA;    if chunk:&#xA;        # Process streaming chunks&#xA;        pass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Versioning &amp;amp; Development&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Branching Strategy:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;code&gt;release&lt;/code&gt; branch:&lt;/strong&gt; Contains the latest stable build, recommended for production use. Docker images tagged with specific versions (e.g., &lt;code&gt;v0.3.0&lt;/code&gt;) are built from this branch.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;&lt;code&gt;master&lt;/code&gt; branch:&lt;/strong&gt; Used for active development. It may contain experimental features, ongoing changes, or fixes not yet in a stable release. Use this branch if you want the absolute latest code, but be aware it might be less stable. The &lt;code&gt;latest&lt;/code&gt; Docker tag often points to builds from this branch.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Note: This is a &lt;em&gt;development&lt;/em&gt; focused project at its core.&lt;/p&gt; &#xA; &lt;p&gt;If you run into trouble, you may have to roll back a version on the release tags if something comes up, or build up from source and/or troubleshoot + submit a PR.&lt;/p&gt; &#xA; &lt;p&gt;Free and open source is a community effort, and there&#39;s only really so many hours in a day. If you&#39;d like to support the work, feel free to open a PR, buy me a coffee, or report any bugs/features/etc you find during use.&lt;/p&gt; &#xA; &lt;a href=&#34;https://www.buymeacoffee.com/remsky&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://cdn.buymeacoffee.com/buttons/v2/default-violet.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: 30px !important;width: 110px !important;&#34;&gt; &lt;/a&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Linux GPU Permissions&lt;/summary&gt; &#xA; &lt;p&gt;Some Linux users may encounter GPU permission issues when running as non-root. Can&#39;t guarantee anything, but here are some common solutions, consider your security requirements carefully&lt;/p&gt; &#xA; &lt;h3&gt;Option 1: Container Groups (Likely the best option)&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;services:&#xA;  kokoro-tts:&#xA;    # ... existing config ...&#xA;    group_add:&#xA;      - &#34;video&#34;&#xA;      - &#34;render&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Option 2: Host System Groups&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;services:&#xA;  kokoro-tts:&#xA;    # ... existing config ...&#xA;    user: &#34;${UID}:${GID}&#34;&#xA;    group_add:&#xA;      - &#34;video&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note: May require adding host user to groups: &lt;code&gt;sudo usermod -aG docker,video $USER&lt;/code&gt; and system restart.&lt;/p&gt; &#xA; &lt;h3&gt;Option 3: Device Permissions (Use with caution)&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;services:&#xA;  kokoro-tts:&#xA;    # ... existing config ...&#xA;    devices:&#xA;      - /dev/nvidia0:/dev/nvidia0&#xA;      - /dev/nvidiactl:/dev/nvidiactl&#xA;      - /dev/nvidia-uvm:/dev/nvidia-uvm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;⚠️ Warning: Reduces system security. Use only in development environments.&lt;/p&gt; &#xA; &lt;p&gt;Prerequisites: NVIDIA GPU, drivers, and container toolkit must be properly configured.&lt;/p&gt; &#xA; &lt;p&gt;Visit &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;NVIDIA Container Toolkit installation&lt;/a&gt; for more detailed information&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Model and License&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Model&lt;/summary&gt; &#xA; &lt;p&gt;This API uses the &lt;a href=&#34;https://huggingface.co/hexgrad/Kokoro-82M&#34;&gt;Kokoro-82M&lt;/a&gt; model from HuggingFace.&lt;/p&gt; &#xA; &lt;p&gt;Visit the model page for more details about training, architecture, and capabilities. I have no affiliation with any of their work, and produced this wrapper for ease of use and personal projects.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;License&lt;/summary&gt; This project is licensed under the Apache License 2.0 - see below for details: &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;The Kokoro model weights are licensed under Apache 2.0 (see &lt;a href=&#34;https://huggingface.co/hexgrad/Kokoro-82M&#34;&gt;model page&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;The FastAPI wrapper code in this repository is licensed under Apache 2.0 to match&lt;/li&gt; &#xA;  &lt;li&gt;The inference code adapted from StyleTTS2 is MIT licensed&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The full Apache 2.0 license text can be found at: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
</feed>