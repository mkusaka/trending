<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-06T01:32:16Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIAGameWorks/kaolin-wisp</title>
    <updated>2022-08-06T01:32:16Z</updated>
    <id>tag:github.com,2022-08-06:/NVIDIAGameWorks/kaolin-wisp</id>
    <link href="https://github.com/NVIDIAGameWorks/kaolin-wisp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA Kaolin Wisp is a PyTorch library powered by NVIDIA Kaolin Core to work with neural fields (including NeRFs, NGLOD, instant-ngp and VQAD).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kaolin Wisp: A PyTorch Library and Engine for Neural Fields Research&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/media/demo.jpg&#34; alt=&#34;drawing&#34; width=&#34;1000&#34;&gt; &#xA;&lt;p&gt;NVIDIA Kaolin Wisp is a PyTorch library powered by &lt;a href=&#34;https://github.com/NVIDIAGameWorks/kaolin&#34;&gt;NVIDIA Kaolin Core&lt;/a&gt; to work with neural fields (including NeRFs, &lt;a href=&#34;https://nv-tlabs.github.io/nglod&#34;&gt;NGLOD&lt;/a&gt;, &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/&#34;&gt;instant-ngp&lt;/a&gt; and &lt;a href=&#34;https://nv-tlabs.github.io/vqad&#34;&gt;VQAD&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;NVIDIA Kaolin Wisp aims to provide a set of common utility functions for performing research on neural fields. This includes datasets, image I/O, mesh processing, and ray utility functions. Wisp also comes with building blocks like differentiable renderers and differentiable data structures (like octrees, hash grids, triplanar features) which are useful to build complex neural fields. It also includes debugging visualization tools, interactive rendering and training, logging, and trainer classes.&lt;/p&gt; &#xA;&lt;p&gt;For an overview on neural fields, we recommend you checkout the EG STAR report: &lt;a href=&#34;https://arxiv.org/abs/2111.11426&#34;&gt;Neural Fields for Visual Computing and Beyond&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License and Citation&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is licensed under the NVIDIA Source Code License. Commercial licenses are also available, free of charge. Please apply using this link (use &#34;Other&#34; and specify Kaolin Wisp): &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;https://www.nvidia.com/en-us/research/inquiries/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find the NVIDIA Kaolin Wisp library useful for your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{KaolinWispLibrary,&#xA;      author = {Towaki Takikawa and Or Perel and Clement Fuji Tsang and Charles Loop and Joey Litalien and Jonathan Tremblay and Maria Shugrina and Sanja Fidler},&#xA;      title = {Kaolin Wisp: A PyTorch Library and Engine for Neural Fields Research},&#xA;      year = {2022},&#xA;      howpublished={\url{https://github.com/NVIDIAGameWorks/kaolin-wisp}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/media/blocks.jpg&#34; alt=&#34;drawing&#34; width=&#34;750&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Differentiable feature grids &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Octree grids (from NGLOD)&lt;/li&gt; &#xA;   &lt;li&gt;Hash grids (from Instant-NGP)&lt;/li&gt; &#xA;   &lt;li&gt;Triplanar texture grids (from ConvOccNet, EG3D)&lt;/li&gt; &#xA;   &lt;li&gt;Codebook grids (from VQAD)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Acceleration structures for fast raytracing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Octree acceleration structures based on Kaolin Core SPC&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Tracers to trace rays against neural fields &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PackedSDFTracer for SDFs&lt;/li&gt; &#xA;   &lt;li&gt;PackedRFTracer for radiance fields (NeRFs)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Various datasets for common neural fields &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Standard Instant-NGP compatible datasets&lt;/li&gt; &#xA;   &lt;li&gt;RTMV dataset&lt;/li&gt; &#xA;   &lt;li&gt;SDF sampled from meshes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;An interactive renderer where you can train and visualize neural fields&lt;/li&gt; &#xA; &lt;li&gt;A set of core framework features (&lt;code&gt;wisp.core&lt;/code&gt;) for convenience&lt;/li&gt; &#xA; &lt;li&gt;A set of utility functions (&lt;code&gt;wisp.ops&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Have a feature request? Leave a GitHub issue!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Create an anaconda environment&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to get started is to create a virtual Python 3.8 Anaconda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get update&#xA;sudo apt-get install libopenexr-dev &#xA;conda create -n wisp python=3.8&#xA;conda activate wisp&#xA;pip install --upgrade pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Install PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;You should first install PyTorch by following the &lt;a href=&#34;https://pytorch.org/&#34;&gt;official instructions&lt;/a&gt;. The code has been tested with &lt;code&gt;1.9.1&lt;/code&gt; to &lt;code&gt;1.12.0&lt;/code&gt; on Ubuntu 20.04.&lt;/p&gt; &#xA;&lt;h3&gt;3. Install Kaolin&lt;/h3&gt; &#xA;&lt;p&gt;You should also install Kaolin, following the &lt;a href=&#34;https://kaolin.readthedocs.io/en/latest/notes/installation.html&#34;&gt;instructions here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;4. Install the rest of the dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install the rest of the dependencies from &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/requirements.txt&#34;&gt;requirements&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. Installing the interactive renderer (optional)&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to use the interactive renderer and training visualizer, you will need additional dependencies. Note that you need to have OpenGL available on your system.&lt;/p&gt; &#xA;&lt;p&gt;To install (&lt;strong&gt;make sure you have the CUDA_HOME environment variable set!&lt;/strong&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/inducer/pycuda&#xA;cd pycuda&#xA;python configure.py --cuda-root=$CUDA_HOME --cuda-enable-gl&#xA;python setup.py develop&#xA;cd ..&#xA;pip install -r requirements_app.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;6. Installing Wisp&lt;/h3&gt; &#xA;&lt;p&gt;To install wisp, simply execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in the main wisp directory. You should now be able to run some examples!&lt;/p&gt; &#xA;&lt;h2&gt;Training &amp;amp; Rendering with Wisp&lt;/h2&gt; &#xA;&lt;h3&gt;Training NGLOD-NeRF from multiview RGB-D data&lt;/h3&gt; &#xA;&lt;p&gt;You will first need to download some sample data to run NGLOD-NeRF. Go to this &lt;a href=&#34;https://drive.google.com/file/d/18hY0DpX2bK-q9iY_cog5Q0ZI7YEjephE/view?usp=sharing&#34;&gt;Google Drive link&lt;/a&gt; to download a cool Lego V8 engine from the &lt;a href=&#34;http://www.cs.umd.edu/~mmeshry/projects/rtmv/&#34;&gt;RTMV dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have downloaded and extracted the data somewhere, you can train a NeRF using &lt;a href=&#34;https://nv-tlabs.github.io/nglod/&#34;&gt;NGLOD&lt;/a&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --dataset-num-workers 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate logs inside &lt;code&gt;_results/logs/runs/test-nglod-nerf&lt;/code&gt; in which you can find the trained checkpoint, and &lt;code&gt;EXR&lt;/code&gt; images of validation outputs. We highly recommend that you install &lt;a href=&#34;https://github.com/Tom94/tev&#34;&gt;tev&lt;/a&gt; as the default application to open EXRs. Note that the &lt;code&gt;--dataset-num-workers&lt;/code&gt; argument is used here to control the multiprocessing used to load ground truth images. To disable the multiprocessing, you can pass in &lt;code&gt;--dataset-num-workers -1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To view the logs with TensorBoard:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensorboard --logdir _results/logs/runs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Want to run the code with different options? Our configuration system makes this very easy. If you want to run with a different number of levels of details:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --num-lods 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Take a look at &lt;code&gt;wisp/config_parser.py&lt;/code&gt; for the list of different options you can pass in, and &lt;code&gt;configs/nglod_nerf.yaml&lt;/code&gt; for the options that are already passed in.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive training&lt;/h3&gt; &#xA;&lt;p&gt;To run the training task interactively using the renderer engine, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;WISP_HEADLESS=0 python3 app/main_interactive.py --config configs/nglod_nerf_interactive.yaml --dataset-path /path/to/V8 --dataset-num-workers 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Every config file that we ship has a &lt;code&gt;*_interactive.yaml&lt;/code&gt; counterpart that can be used for better settings (in terms of user experience) for the interactive training app. The later examples we show can all be run interactively with &lt;code&gt;WISP_HEADLESS=1 python3 app/main_interactive.py&lt;/code&gt; and the corresponding configs.&lt;/p&gt; &#xA;&lt;h3&gt;Using &lt;code&gt;wisp&lt;/code&gt; in headless mode&lt;/h3&gt; &#xA;&lt;p&gt;To disable interactive mode, and run wisp &lt;em&gt;without&lt;/em&gt; loading the graphics API, set the env variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;WISP_HEADLESS=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Toggling this flag is useful for debugging on machines without a display. This is also needed if you opt to avoid installing the interactive renderer requirements.&lt;/p&gt; &#xA;&lt;h3&gt;Training NGLOD-SDF from meshes&lt;/h3&gt; &#xA;&lt;p&gt;We also support training neural SDFs from meshes. You will first need to download a mesh. Go to this &lt;a href=&#34;https://github.com/alecjacobson/common-3d-test-models/raw/master/data/spot.obj&#34;&gt;link&lt;/a&gt; to download an OBJ file of the Spot cow.&lt;/p&gt; &#xA;&lt;p&gt;Then, run the SDF training with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/nglod_sdf.yaml --dataset-path /path/to/spot.obj&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently the SDF sampler we have shipped with our code can be quite slow for larger meshes. We plan to release a more optimized version of the SDF sampler soon.&lt;/p&gt; &#xA;&lt;h3&gt;Training NGP for forward facing scenes&lt;/h3&gt; &#xA;&lt;p&gt;Lastly, we also show an example of training a forward-facing scene: the &lt;code&gt;fox&lt;/code&gt; scene from &lt;code&gt;instant-ngp&lt;/code&gt;. To train a version of the &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/&#34;&gt;Instant-NGP&lt;/a&gt;, first download the &lt;code&gt;fox&lt;/code&gt; dataset from the &lt;code&gt;instant-ngp&lt;/code&gt; repository somewhere. Then, run the training with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/ngp_nerf.yaml --multiview-dataset-format standard --mip 0 --dataset-path /path/to/fox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our code supports any &#34;standard&#34; NGP-format datasets that has been converted with the scripts from the &lt;code&gt;instant-ngp&lt;/code&gt; library. We pass in the &lt;code&gt;--multiview-dataset-format&lt;/code&gt; argument to specify the dataset type, which in this case is different from the RTMV dataset type used for the other examples.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--mip&lt;/code&gt; argument controls the amount of downscaling that happens on the images when they get loaded. This is useful for datasets with very high resolution images to prevent overload on system memory, but is usually not necessary for the fox dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note that our architecture, training, and implementation details still have slight differences from the published Instant-NGP.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration System&lt;/h3&gt; &#xA;&lt;p&gt;Wisp accepts configuration from both the command line interface (CLI) and a &lt;code&gt;yaml&lt;/code&gt; config file (examples in &lt;code&gt;configs&lt;/code&gt;). Whatever config file you pass in through the &lt;code&gt;--config&lt;/code&gt; option will be checked against the options in &lt;code&gt;wisp/options.py&lt;/code&gt; and serve as the &lt;em&gt;default arguments&lt;/em&gt;. This means any CLI argument you additionally pass in will overwrite the options you pass in through the &lt;code&gt;--config&lt;/code&gt;. The order of arguments does not matter.&lt;/p&gt; &#xA;&lt;p&gt;Wisp also supports hierarchical configs, by using the &lt;code&gt;parent&lt;/code&gt; argument in the config to set a parent config file path in relative path from the config location or with an absolute path. Note however that only a single level of hierarchy is allowed to keep the indirection manageable.&lt;/p&gt; &#xA;&lt;p&gt;If you get any errors from loading in config files, you likely made a typo in your field names. Check against &lt;code&gt;wisp/options.py&lt;/code&gt; as your source of truth. (Or pass in &lt;code&gt;-h&lt;/code&gt; for help).&lt;/p&gt; &#xA;&lt;h2&gt;What is &#34;wisp&#34;?&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/media/wisp.jpg&#34; alt=&#34;drawing&#34; height=&#34;300&#34;&gt; &#xA;&lt;p&gt;Our library is named after the atmospheric ghost light, will-o&#39;-the-wisp, which are volumetric ghosts that are harder to model with common standard geometry representations like meshes. We provide a &lt;a href=&#34;https://drive.google.com/file/d/1jKIkqm4XhdeEQwXTqbKlZw-9dO7kJfsZ/view&#34;&gt;multiview dataset&lt;/a&gt; of the wisp as a reference dataset for a volumetric object. We also provide the &lt;a href=&#34;https://drive.google.com/drive/folders/1Via1TOsnG-3mUkkGteEoRJdEYJEx3wgf?usp=sharing&#34;&gt;blender file and rendering scripts&lt;/a&gt; if you want to generate specific data with this scene, please refer to the &lt;a href=&#34;https://drive.google.com/file/d/1IrWKjxxrJOlD3C5lDYvejaSXiPtm_XI_/view?usp=sharing&#34;&gt;readme.md&lt;/a&gt; for greater details on how to generate the data.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;We thank James Lucas, Jonathan Tremblay, Valts Blukis, Anita Hu, and Nishkrit Desai for giving us early feedback and testing out the code at various stages throughout development. We thank Rogelio Olguin and Jonathan Tremblay for the Wisp reference data.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ifzhang/FairMOT</title>
    <updated>2022-08-06T01:32:16Z</updated>
    <id>tag:github.com,2022-08-06:/ifzhang/FairMOT</id>
    <link href="https://github.com/ifzhang/FairMOT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[IJCV-2021] FairMOT: On the Fairness of Detection and Re-Identification in Multi-Object Tracking&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FairMOT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-2dmot15-1?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-2dmot15-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot16?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-mot16&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot17?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-mot17&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-mot20-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A simple baseline for one-shot multi-object tracking: &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/pipeline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2004.01888&#34;&gt;&lt;strong&gt;FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu,&lt;br&gt; &lt;em&gt;IJCV2021 (&lt;a href=&#34;http://arxiv.org/abs/2004.01888&#34;&gt;arXiv 2004.01888&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problems. It remarkably outperforms the state-of-the-arts on the MOT challenge datasets at 30 FPS. We hope this baseline could inspire and help evaluate new ideas in this field.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2021.08.03) Our paper is accepted by IJCV!&lt;/li&gt; &#xA; &lt;li&gt;(2021.06.01) A &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.1/configs/mot&#34;&gt;nice re-implementation&lt;/a&gt; by Baidu &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection&#34;&gt;PaddleDetection&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;(2021.05.24) A light version of FairMOT using yolov5s backbone is released!&lt;/li&gt; &#xA; &lt;li&gt;(2020.09.10) A new version of FairMOT is released! (73.7 MOTA on MOT17)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Main updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We pretrain FairMOT on the CrowdHuman dataset using a weakly-supervised learning approach.&lt;/li&gt; &#xA; &lt;li&gt;To detect bounding boxes outside the image, we use left, top, right and bottom (4 channel) to replace the WH head (2 channel).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tracking performance&lt;/h2&gt; &#xA;&lt;h3&gt;Results on MOT challenge test set&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDS&lt;/th&gt; &#xA;   &lt;th&gt;MT&lt;/th&gt; &#xA;   &lt;th&gt;ML&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2DMOT15&lt;/td&gt; &#xA;   &lt;td&gt;60.6&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;591&lt;/td&gt; &#xA;   &lt;td&gt;47.6%&lt;/td&gt; &#xA;   &lt;td&gt;11.0%&lt;/td&gt; &#xA;   &lt;td&gt;30.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT16&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;72.8&lt;/td&gt; &#xA;   &lt;td&gt;1074&lt;/td&gt; &#xA;   &lt;td&gt;44.7%&lt;/td&gt; &#xA;   &lt;td&gt;15.9%&lt;/td&gt; &#xA;   &lt;td&gt;25.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT17&lt;/td&gt; &#xA;   &lt;td&gt;73.7&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;3303&lt;/td&gt; &#xA;   &lt;td&gt;43.2%&lt;/td&gt; &#xA;   &lt;td&gt;17.3%&lt;/td&gt; &#xA;   &lt;td&gt;25.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT20&lt;/td&gt; &#xA;   &lt;td&gt;61.8&lt;/td&gt; &#xA;   &lt;td&gt;67.3&lt;/td&gt; &#xA;   &lt;td&gt;5243&lt;/td&gt; &#xA;   &lt;td&gt;68.8%&lt;/td&gt; &#xA;   &lt;td&gt;7.6%&lt;/td&gt; &#xA;   &lt;td&gt;13.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All of the results are obtained on the &lt;a href=&#34;https://motchallenge.net&#34;&gt;MOT challenge&lt;/a&gt; evaluation server under the “private detector” protocol. We rank first among all the trackers on 2DMOT15, MOT16, MOT17 and MOT20. The tracking speed of the entire system can reach up to &lt;strong&gt;30 FPS&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Video demos on MOT challenge test set&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT15.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT16.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT17.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT20.gif&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo, and we&#39;ll call the directory that you cloned as ${FAIRMOT_ROOT}&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies. We use python 3.8 and pytorch &amp;gt;= 1.7.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n FairMOT&#xA;conda activate FairMOT&#xA;conda install pytorch==1.7.0 torchvision==0.8.0 cudatoolkit=10.2 -c pytorch&#xA;cd ${FAIRMOT_ROOT}&#xA;pip install cython&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use &lt;a href=&#34;https://github.com/ifzhang/DCNv2/tree/pytorch_1.7&#34;&gt;DCNv2_pytorch_1.7&lt;/a&gt; in our backbone network (pytorch_1.7 branch). Previous versions can be found in &lt;a href=&#34;https://github.com/CharlesShang/DCNv2&#34;&gt;DCNv2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone -b pytorch_1.7 https://github.com/ifzhang/DCNv2.git&#xA;cd DCNv2&#xA;./make.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In order to run the code for demos, you also need to install &lt;a href=&#34;https://www.ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CrowdHuman&lt;/strong&gt; The CrowdHuman dataset can be downloaded from their &lt;a href=&#34;https://www.crowdhuman.org&#34;&gt;official webpage&lt;/a&gt;. After downloading, you should prepare the data in the following structure:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;crowdhuman&#xA;   |——————images&#xA;   |        └——————train&#xA;   |        └——————val&#xA;   └——————labels_with_ids&#xA;   |         └——————train(empty)&#xA;   |         └——————val(empty)&#xA;   └------annotation_train.odgt&#xA;   └------annotation_val.odgt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to pretrain on CrowdHuman (we train Re-ID on CrowdHuman), you can change the paths in src/gen_labels_crowd_id.py and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python gen_labels_crowd_id.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to add CrowdHuman to the MIX dataset (we do not train Re-ID on CrowdHuman), you can change the paths in src/gen_labels_crowd_det.py and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python gen_labels_crowd_det.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MIX&lt;/strong&gt; We use the same training data as &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT&#34;&gt;JDE&lt;/a&gt; in this part and we call it &#34;MIX&#34;. Please refer to their &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT/raw/master/DATASET_ZOO.md&#34;&gt;DATA ZOO&lt;/a&gt; to download and prepare all the training data including Caltech Pedestrian, CityPersons, CUHK-SYSU, PRW, ETHZ, MOT17 and MOT16.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2DMOT15 and MOT20&lt;/strong&gt; &lt;a href=&#34;https://motchallenge.net/data/2D_MOT_2015/&#34;&gt;2DMOT15&lt;/a&gt; and &lt;a href=&#34;https://motchallenge.net/data/MOT20/&#34;&gt;MOT20&lt;/a&gt; can be downloaded from the official webpage of MOT challenge. After downloading, you should prepare the data in the following structure:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;MOT15&#xA;   |——————images&#xA;   |        └——————train&#xA;   |        └——————test&#xA;   └——————labels_with_ids&#xA;            └——————train(empty)&#xA;MOT20&#xA;   |——————images&#xA;   |        └——————train&#xA;   |        └——————test&#xA;   └——————labels_with_ids&#xA;            └——————train(empty)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can change the seq_root and label_root in src/gen_labels_15.py and src/gen_labels_20.py and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python gen_labels_15.py&#xA;python gen_labels_20.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to generate the labels of 2DMOT15 and MOT20. The seqinfo.ini files of 2DMOT15 can be downloaded here &lt;a href=&#34;https://drive.google.com/open?id=1kJYySZy7wyETH4fKMzgJrYUrTfxKlN1w&#34;&gt;[Google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1zb5tBW7-YTzWOXpd9IzS0g&#34;&gt;[Baidu],code:8o0w&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained models and baseline model&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pretrained models&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;DLA-34 COCO pretrained model: &lt;a href=&#34;https://drive.google.com/file/d/1pl_-ael8wERdUREEnaIfqOV_VF2bEVRT/view&#34;&gt;DLA-34 official&lt;/a&gt;. HRNetV2 ImageNet pretrained model: &lt;a href=&#34;https://1drv.ms/u/s!Aus8VCZ_C_33cMkPimlmClRvmpw&#34;&gt;HRNetV2-W18 official&lt;/a&gt;, &lt;a href=&#34;https://1drv.ms/u/s!Aus8VCZ_C_33dYBMemi9xOUFR0w&#34;&gt;HRNetV2-W32 official&lt;/a&gt;. After downloading, you should put the pretrained models in the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${FAIRMOT_ROOT}&#xA;   └——————models&#xA;           └——————ctdet_coco_dla_2x.pth&#xA;           └——————hrnetv2_w32_imagenet_pretrained.pth&#xA;           └——————hrnetv2_w18_imagenet_pretrained.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Baseline model&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our baseline FairMOT model (DLA-34 backbone) is pretrained on the CrowdHuman for 60 epochs with the self-supervised learning approach and then trained on the MIX dataset for 30 epochs. The models can be downloaded here: crowdhuman_dla34.pth &lt;a href=&#34;https://drive.google.com/file/d/1SFOhg_vos_xSYHLMTDGFVZBYjo8cr2fG/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1JZMCVDyQnQCa5veO73YaMw&#34;&gt;[Baidu, code:ggzx ]&lt;/a&gt; &lt;a href=&#34;https://microsoftapc-my.sharepoint.com/:u:/g/personal/v-yifzha_microsoft_com/EUsj0hkTNuhKkj9bo9kE7ZsBpmHvqDz6DylPQPhm94Y08w?e=3OF4XN&#34;&gt;[Onedrive]&lt;/a&gt;. fairmot_dla34.pth &lt;a href=&#34;https://drive.google.com/file/d/1iqRQjsG9BawIl8SlFomMg5iwkb6nqSpi/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1H1Zp8wrTKDk20_DSPAeEkg&#34;&gt;[Baidu, code:uouv]&lt;/a&gt; &lt;a href=&#34;https://microsoftapc-my.sharepoint.com/:u:/g/personal/v-yifzha_microsoft_com/EWHN_RQA08BDoEce_qFW-ogBNUsb0jnxG3pNS3DJ7I8NmQ?e=p0Pul1&#34;&gt;[Onedrive]&lt;/a&gt;. (This is the model we get 73.7 MOTA on the MOT17 test set. ) After downloading, you should put the baseline model in the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${FAIRMOT_ROOT}&#xA;   └——————models&#xA;           └——————fairmot_dla34.pth&#xA;           └——————...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the training data&lt;/li&gt; &#xA; &lt;li&gt;Change the dataset root directory &#39;root&#39; in src/lib/cfg/data.json and &#39;data_dir&#39; in src/lib/opts.py&lt;/li&gt; &#xA; &lt;li&gt;Pretrain on CrowdHuman and train on MIX:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/crowdhuman_dla34.sh&#xA;sh experiments/mix_ft_ch_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Only train on MIX:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mix_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Only train on MOT17:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mot17_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finetune on 2DMOT15 using the baseline model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mot15_ft_mix_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train on MOT20: The data annotation of MOT20 is a little different from MOT17, the coordinates of the bounding boxes are all inside the image, so we need to uncomment line 313 to 316 in the dataset file src/lib/datasets/dataset/jde.py:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;#np.clip(xy[:, 0], 0, width, out=xy[:, 0])&#xA;#np.clip(xy[:, 2], 0, width, out=xy[:, 2])&#xA;#np.clip(xy[:, 1], 0, height, out=xy[:, 1])&#xA;#np.clip(xy[:, 3], 0, height, out=xy[:, 3])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, we can train on the mix dataset and finetune on MOT20:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/crowdhuman_dla34.sh&#xA;sh experiments/mix_ft_ch_dla34.sh&#xA;sh experiments/mot20_ft_mix_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The MOT20 model &#39;mot20_fairmot.pth&#39; can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1HVzDTrYSSZiVqExqG9rou3zZXX1-GGQn/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1bpMtu972ZszsBx4TzIT_CA&#34;&gt;[Baidu, code:jmce]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For ablation study, we use MIX and half of MOT17 as training data, you can use different backbones such as ResNet, ResNet-FPN, HRNet and DLA::&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mix_mot17_half_dla34.sh&#xA;sh experiments/mix_mot17_half_hrnet18.sh&#xA;sh experiments/mix_mot17_half_res34.sh&#xA;sh experiments/mix_mot17_half_res34fpn.sh&#xA;sh experiments/mix_mot17_half_res50.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The ablation study model &#39;mix_mot17_half_dla34.pth&#39; can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1dJDGSa6-FMq33XY-cOd_nYxuilv30YDM/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://microsoftapc-my.sharepoint.com/:u:/g/personal/v-yifzha_microsoft_com/ESh1SlUvZudKgUX4A8E3yksBhfRHIf2AsKaaPJ-v_5lVAw?e=NB6UHR&#34;&gt;[Onedrive]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1RQD8ik1labWuwd8jJ-0ukQ&#34;&gt;[Baidu, code:iifa]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance on the test set of MOT17 when using different training data:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training Data&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT17&lt;/td&gt; &#xA;   &lt;td&gt;69.8&lt;/td&gt; &#xA;   &lt;td&gt;69.9&lt;/td&gt; &#xA;   &lt;td&gt;3996&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MIX&lt;/td&gt; &#xA;   &lt;td&gt;72.9&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;3345&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CrowdHuman + MIX&lt;/td&gt; &#xA;   &lt;td&gt;73.7&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;3303&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use CrowdHuman, MIX and MOT17 to train the light version of FairMOT using yolov5s as backbone:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/all_yolov5s.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The pretrained model of yolov5s on the COCO dataset can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1Ur3_pa9r3KRY-5qM2cdFhFJ5exghRJvh/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1JHjN_l1nkMnRHRF5TcHYXg&#34;&gt;[Baidu, code:wh9h]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The model of the light version &#39;fairmot_yolov5s&#39; can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1MEvsRPyoAqYSCdKaS5Ofrl7ZfKbBZ1Jb/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1dyBEeiGpRfZhqae0c264rg&#34;&gt;[Baidu, code:2y3a]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tracking&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The default settings run tracking on the validation dataset from 2DMOT15. Using the baseline model, you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --load_model ../models/fairmot_dla34.pth --conf_thres 0.6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to see the tracking results (76.5 MOTA and 79.3 IDF1 using the baseline model). You can also set save_images=True in src/track.py to save the visualization results of each frame.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For ablation study, we evaluate on the other half of the training set of MOT17, you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track_half.py mot --load_model ../exp/mot/mix_mot17_half_dla34.pth --conf_thres 0.4 --val_mot17 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use our pretrained model &#39;mix_mot17_half_dla34.pth&#39;, you can get 69.1 MOTA and 72.8 IDF1.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To get the txt results of the test set of MOT16 or MOT17, you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --test_mot17 True --load_model ../models/fairmot_dla34.pth --conf_thres 0.4&#xA;python track.py mot --test_mot16 True --load_model ../models/fairmot_dla34.pth --conf_thres 0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run tracking using the light version of FairMOT (68.5 MOTA on the test of MOT17), you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --test_mot17 True --load_model ../models/fairmot_yolov5s.pth --conf_thres 0.4 --arch yolo --reid_dim 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and send the txt files to the &lt;a href=&#34;https://motchallenge.net&#34;&gt;MOT challenge&lt;/a&gt; evaluation server to get the results. (You can get the SOTA results 73+ MOTA on MOT17 test set using the baseline model &#39;fairmot_dla34.pth&#39;.)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To get the SOTA results of 2DMOT15 and MOT20, run the tracking code:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --test_mot15 True --load_model your_mot15_model.pth --conf_thres 0.3&#xA;python track.py mot --test_mot20 True --load_model your_mot20_model.pth --conf_thres 0.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results of the test set all need to be evaluated on the MOT challenge server. You can see the tracking results on the training set by setting --val_motxx True and run the tracking code. We set &#39;conf_thres&#39; 0.4 for MOT16 and MOT17. We set &#39;conf_thres&#39; 0.3 for 2DMOT15 and MOT20.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python demo.py mot --load_model ../models/fairmot_dla34.pth --conf_thres 0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can change --input-video and --output-root to get the demos of your own videos. --conf_thres can be set from 0.3 to 0.7 depending on your own videos.&lt;/p&gt; &#xA;&lt;h2&gt;Train on custom dataset&lt;/h2&gt; &#xA;&lt;p&gt;You can train FairMOT on custom dataset by following several steps bellow:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate one txt label file for one image. Each line of the txt label file represents one object. The format of the line is: &#34;class id x_center/img_width y_center/img_height w/img_width h/img_height&#34;. You can modify src/gen_labels_16.py to generate label files for your custom dataset.&lt;/li&gt; &#xA; &lt;li&gt;Generate files containing image paths. The example files are in src/data/. Some similar code can be found in src/gen_labels_crowd.py&lt;/li&gt; &#xA; &lt;li&gt;Create a json file for your custom dataset in src/lib/cfg/. You need to specify the &#34;root&#34; and &#34;train&#34; keys in the json file. You can find some examples in src/lib/cfg/.&lt;/li&gt; &#xA; &lt;li&gt;Add --data_cfg &#39;../src/lib/cfg/your_dataset.json&#39; when training.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;A large part of the code is borrowed from &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT&#34;&gt;Zhongdao/Towards-Realtime-MOT&lt;/a&gt; and &lt;a href=&#34;https://github.com/xingyizhou/CenterNet&#34;&gt;xingyizhou/CenterNet&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2021fairmot,&#xA;  title={Fairmot: On the fairness of detection and re-identification in multiple object tracking},&#xA;  author={Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},&#xA;  journal={International Journal of Computer Vision},&#xA;  volume={129},&#xA;  pages={3069--3087},&#xA;  year={2021},&#xA;  publisher={Springer}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tr0uble-mAker/POC-bomber</title>
    <updated>2022-08-06T01:32:16Z</updated>
    <id>tag:github.com,2022-08-06:/tr0uble-mAker/POC-bomber</id>
    <link href="https://github.com/tr0uble-mAker/POC-bomber" rel="alternate"></link>
    <summary type="html">&lt;p&gt;利用大量高威胁poc/exp快速获取目标权限，用于渗透和红队快速打点&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🔥 POC-bomber&lt;/h1&gt; &#xA;&lt;p&gt;🦄 &lt;strong&gt;POC bomber 是一款漏洞检测/利用工具，旨在利用大量高危害漏洞的POC/EXP快速获取目标服务器权限&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;本项目收集互联网各种危害性大的 RCE · 任意文件上传 · 反序列化 · sql注入 等高危害且能够获取到服务器核心权限的漏洞POC/EXP，并集成在 POC bomber 武器库中，利用大量高危害POC对单个或多个目标进行模糊测试，以此在大量资产中快速获取发现脆弱性目标，获取目标服务器权限。适用场景包括但不仅限于以下:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;红蓝对抗或hvv中帮助红队在大量资产中快速找到突破口进入内网&lt;/li&gt; &#xA; &lt;li&gt;内网安全测试，横向移动&lt;/li&gt; &#xA; &lt;li&gt;利用新型0day对企业资产进行批量评估&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;📝 简介&lt;/h2&gt; &#xA;&lt;p&gt;POC bomber 的poc支持weblogic，tomcat，apache，jboss，nginx，struct2，thinkphp2x3x5x，spring，redis，jenkins，php语言漏洞，shiro，泛微OA，致远OA，通达OA等易受攻击组件的漏洞检测，支持调用dnslog平台检测无回显的rce(包括log4j2的检测)，支持单个目标检测和批量检测，程序采用高并发线程池，支持自定义导入poc/exp，并能够生成漏洞报告&lt;br&gt; POC bomber默认使用验证模式进行poc的验证，如返回结果中attack的值为True时，可以加参数(--attack)进入攻击模式直接调用exp进行攻击(需要指定poc文件名)，达到一键getshell&lt;/p&gt; &#xA;&lt;h2&gt;💻 Screenshots&lt;/h2&gt; &#xA;&lt;h4&gt;🏆 验证模式&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python3 pocbomber.py -u http://xxx.xxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/71172892/168099161-f46a54f7-562b-4ba5-a751-1d65492b17d9.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/147481630-f8b94566-572f-4d89-a874-dc01f5041377.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/148684886-98b0f1ff-76f5-48d3-8d2d-932635392a33.gif&#34; alt=&#34;verify模试演示&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;⚡️ 攻击模式&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python3 pocbomber.py -u http://xxx.xxx --poc=&#34;thinkphp2_rce.py&#34; --attack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/71172892/147629887-def9d18e-f6aa-466a-ab2c-2538752b82aa.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/148206720-86f77246-301c-481f-a16c-b36047f72d7c.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/148684097-67b59320-6758-458d-ac6b-ae219c327924.gif&#34; alt=&#34;attack模式演示&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔧 安装&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/tr0uble-mAker/POC-bomber.git            &#xA;  cd POC-bomber&#xA;  pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🚀 用法&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;    查看用法:     python3 pocbomber.py&#xA;    &#xA;    模式:&#xA;            获取poc/exp信息:   python3 pocbomber.py --show&#xA;            单目标检测:        python3 pocbomber.py -u http://xxx.xxx.xx&#xA;            批量检测:          python3 pocbomber.py -f url.txt -o report.txt &#xA;            指定poc检测:       python3 pocbomber.py -f url.txt --poc=&#34;thinkphp2_rce.py&#34;&#xA;            exp攻击模式:       python3 pocbomber.py -u 目标url --poc=&#34;指定poc文件&#34; --attack&#xA;    参数:&#xA;            -u  --url      目标url&#xA;            -f  --file     指定目标url文件   &#xA;            -o  --output   指定生成报告的文件(默认不生成报告)&#xA;            -p  --poc      指定单个或多个poc进行检测, 直接传入poc文件名, 多个poc用(,)分开&#xA;            -t  --thread   指定线程池最大并发数量(默认30)&#xA;            --show         展示poc/exp详细信息&#xA;            --attack       使用poc文件中的exp进行攻击&#xA;            --dnslog       使用dnslog平台检测无回显漏洞(默认不启用dnslog,可在配置文件中启用)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔆 配置文件&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;  /inc/config.py   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;⚠️ 常见问题&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;程序不安装requirements.txt就可以直接运行，只依赖requests第三方库，其他库安装不上不影响程序运行，但有些poc会不能检测&lt;/li&gt; &#xA; &lt;li&gt;log4j2命令执行漏洞的检测：需要添加 --dnslog 参数&lt;/li&gt; &#xA; &lt;li&gt;无回显漏洞检测默认使用 dnslog.cn 平台且默认关闭, 要开启需前往配置文件将 dnslog_flag 开关置为True&lt;/li&gt; &#xA; &lt;li&gt;需要指定一个poc才能调用--attack攻击模式&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;📁 目录结构:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;  +--------- poc_bomber.py (启动 POC-bomber)&#xA;  | &#xA;  +--------- inc(存放支撑 POC-bomber 框架运行的核心文件)&#xA;  |&#xA;  \--------- pocs(POC存放列表)----------- framework(存放框架漏洞POC)&#xA;                                  |&#xA;                                  |------ middleware(存放中间件漏洞POC)&#xA;                                  |&#xA;                                  |------ ports(存放常见端口漏洞,主机服务漏洞POC)&#xA;                                  |&#xA;                                   \----- webs(存放常见web页面漏洞POC)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📖 POC编写规则&lt;/h2&gt; &#xA;&lt;p&gt;POC bomber支持自定义编写poc&lt;br&gt; poc统一要求python3编写，具有verify和attack(非必须)两个函数分别进行验证和攻击,&lt;/p&gt; &#xA;&lt;h4&gt;👻 漏洞验证函数(verify)编写应该满足以下条件:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;函数名为 verify ， 参数接收目标url的参数&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;函数的返回结果以字典的形式返回并且具有name和vulnerable两个键值，name说明漏洞名称，vulnerable通过True和False的状态表明漏洞是否存在&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果存在漏洞要将返回字典中vulnerable的值置为True, 并添加目标url, 漏洞利用相关网页等信息&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;用try方法尝试验证，使用request等发送数据包时要设置超时时间, 避免poc会卡死&lt;/p&gt; &lt;pre&gt;&lt;code&gt; def verify(url):                        &#xA;     relsult = {                                            &#xA;         &#39;name&#39;: &#39;Thinkphp5 5.0.22/5.1.29 Remote Code Execution Vulnerability&#39;,                          &#xA;         &#39;vulnerable&#39;: False，&#xA;         &#39;attack&#39;： False，        # 如果有exp支持attack模式将attack的值置为True&#xA;     }              &#xA;     try:                    &#xA;         ......        &#xA;         (用任意方法检测漏洞)             &#xA;         ......&#xA;         if 存在漏洞:&#xA;             relsult[&#39;vulnerable&#39;] = True     # 将relsult的vulnerable的值置为True&#xA;             relsult[&#39;url&#39;] = url             # 返回验证的url&#xA;             relust[&#39;xxxxx&#39;] = &#39;xxxxx&#39;        # 可以添加该漏洞相关来源等信息   &#xA;             ......           &#xA;             return relsult     # 将vulnerable值为True的relsult返回                   &#xA;         else:  # 不存在漏洞           &#xA;             return relsult    # 若不存在漏洞将vulnerable值为False的relsult返回&#xA;&#xA;     execpt:&#xA;         return relsult&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;如果有exp可以编写 attack 函数作为exp攻击函数，&lt;/p&gt; &#xA;&lt;h4&gt;🎃 漏洞攻击函数(attack)编写应该满足以下条件：&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;函数名为 attack ， 参数接收目标url的参数&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;并在try中编写exp代码进行攻击, 可以与用户交互输入&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;编写完成后将该漏洞的verify函数返回字典中attack值置为True&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;攻击成功后返回True，其他原因失败的话返回False即可&lt;/p&gt; &lt;pre&gt;&lt;code&gt; def attack(url):    &#xA;   try:            &#xA;       ........................................            &#xA;         攻击代码(执行命令或反弹shell上传木马等)             &#xA;       ........................................&#xA;       return True&#xA;   except:               &#xA;       return False    &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;编写完成后的poc直接放入 /pocs 目录下任意位置即可被递归调用!&lt;/p&gt; &#xA;&lt;p&gt;项目持续更新中，欢迎各位师傅贡献poc共筑网络安全！&lt;br&gt; 有问题欢迎issues留言: &lt;a href=&#34;https://github.com/tr0uble-mAker/POC-bomber/issues&#34;&gt;https://github.com/tr0uble-mAker/POC-bomber/issues&lt;/a&gt;&lt;br&gt; 联系: &lt;a href=&#34;mailto:929305053@qq.com&#34;&gt;929305053@qq.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>