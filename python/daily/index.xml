<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-07T01:31:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google-research/multinerf</title>
    <updated>2022-08-07T01:31:33Z</updated>
    <id>tag:github.com,2022-08-07:/google-research/multinerf</id>
    <link href="https://github.com/google-research/multinerf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Code Release for Mip-NeRF 360, Ref-NeRF, and RawNeRF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MultiNeRF: A Code Release for Mip-NeRF 360, Ref-NeRF, and RawNeRF&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;This is not an officially supported Google product.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the code release for three CVPR 2022 papers: &lt;a href=&#34;https://jonbarron.info/mipnerf360/&#34;&gt;Mip-NeRF 360&lt;/a&gt;, &lt;a href=&#34;https://dorverbin.github.io/refnerf/&#34;&gt;Ref-NeRF&lt;/a&gt;, and &lt;a href=&#34;https://bmild.github.io/rawnerf/&#34;&gt;RawNeRF&lt;/a&gt;. This codebase was written by integrating our internal implementions of Ref-NeRF and RawNeRF into our mip-NeRF 360 implementation. As such, this codebase should exactly reproduce the results shown in mip-NeRF 360, but may differ slightly when reproducing Ref-NeRF or RawNeRF results.&lt;/p&gt; &#xA;&lt;p&gt;This implementation is written in &lt;a href=&#34;https://github.com/google/jax&#34;&gt;JAX&lt;/a&gt;, and is a fork of &lt;a href=&#34;https://github.com/google/mipnerf&#34;&gt;mip-NeRF&lt;/a&gt;. This is research code, and should be treated accordingly.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo.&#xA;git clone https://github.com/google-research/multinerf.git&#xA;cd multinerf&#xA;&#xA;# Make a conda environment.&#xA;conda create --name multinerf python=3.9&#xA;conda activate multinerf&#xA;&#xA;# Prepare pip.&#xA;conda install pip&#xA;pip install --upgrade pip&#xA;&#xA;# Install requirements.&#xA;pip install -r requirements.txt&#xA;&#xA;# Manually install rmbrualla&#39;s `pycolmap` (don&#39;t use pip&#39;s! It&#39;s different).&#xA;git clone https://github.com/rmbrualla/pycolmap.git ./internal/pycolmap&#xA;&#xA;# Confirm that all the unit tests pass.&#xA;./scripts/run_all_unit_tests.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll probably also need to update your JAX installation to support GPUs or TPUs.&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;Example scripts for training, evaluating, and rendering can be found in &lt;code&gt;scripts/&lt;/code&gt;. You&#39;ll need to change the paths to point to wherever the datasets are located. &lt;a href=&#34;https://github.com/google/gin-config&#34;&gt;Gin&lt;/a&gt; configuration files for our model and some ablations can be found in &lt;code&gt;configs/&lt;/code&gt;. After evaluating on the test set of each scene in one of the datasets, you can use &lt;code&gt;scripts/generate_tables.ipynb&lt;/code&gt; to produce error metrics across all scenes in the same format as was used in tables in the paper.&lt;/p&gt; &#xA;&lt;h3&gt;OOM errors&lt;/h3&gt; &#xA;&lt;p&gt;You may need to reduce the batch size (&lt;code&gt;Config.batch_size&lt;/code&gt;) to avoid out of memory errors. If you do this, but want to preserve quality, be sure to increase the number of training iterations and decrease the learning rate by whatever scale factor you decrease batch size by.&lt;/p&gt; &#xA;&lt;h2&gt;Using your own data&lt;/h2&gt; &#xA;&lt;p&gt;In order to run MultiNeRF on your own captured images of a scene, you must first run &lt;a href=&#34;https://colmap.github.io/install.html&#34;&gt;COLMAP&lt;/a&gt; to calculate camera poses. You can do this using our provided script &lt;code&gt;scripts/local_colmap_and_resize.sh&lt;/code&gt;. Just make a directory &lt;code&gt;my_dataset_dir/&lt;/code&gt; and copy your input images into a folder &lt;code&gt;my_dataset_dir/images/&lt;/code&gt;, then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/local_colmap_and_resize.sh my_dataset_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run COLMAP and create 2x, 4x, and 8x downsampled versions of your images. These lower resolution images can be used in NeRF by setting, e.g., the &lt;code&gt;Config.factor = 4&lt;/code&gt; gin flag.&lt;/p&gt; &#xA;&lt;p&gt;If you have a very large capture of more than around 500 images, we recommend switching from the exhaustive matcher to the vocabulary tree matcher in COLMAP (see the script for an example).&lt;/p&gt; &#xA;&lt;p&gt;Our script is simply a thin wrapper for COLMAP--if you have run COLMAP yourself, all you need to do to load your scene in NeRF is ensure it has the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;my_dataset_dir/images/    &amp;lt;--- all input images&#xA;my_dataset_dir/sparse/0/  &amp;lt;--- COLMAP sparse reconstruction files (cameras, images, points)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Writing a custom dataloader&lt;/h3&gt; &#xA;&lt;p&gt;If you already have poses for your own data, you may prefer to write your own custom dataloader.&lt;/p&gt; &#xA;&lt;p&gt;MultiNeRF includes a variety of dataloaders, all of which inherit from the base &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/datasets.py#L152&#34;&gt;Dataset class&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The job of this class is to load all image and pose information from disk, then create batches of ray and color data for training or rendering a NeRF model.&lt;/p&gt; &#xA;&lt;p&gt;Any inherited subclass is responsible for loading images and camera poses from disk by implementing the &lt;code&gt;_load_renderings&lt;/code&gt; method (which is marked as abstract by the decorator &lt;code&gt;@abc.abstractmethod&lt;/code&gt;). This data is then used to generate train and test batches of ray + color data for feeding through the NeRF model. The ray parameters are calculated in &lt;code&gt;_make_ray_batch&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Existing data loaders&lt;/h4&gt; &#xA;&lt;p&gt;To work from an example, you can see how this function is overloaded for the different dataloaders we have already implemented:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/datasets.py#L470&#34;&gt;Blender&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/datasets.py#L793&#34;&gt;DTU dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/datasets.py#L680&#34;&gt;Tanks and Temples&lt;/a&gt;, as processed by the NeRF++ paper&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/datasets.py#L728&#34;&gt;Tanks and Temples&lt;/a&gt;, as processed by the Free View Synthesis paper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The main data loader we rely on is &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/datasets.py#L526&#34;&gt;LLFF&lt;/a&gt; (named for historical reasons), which is the loader for a dataset that has been posed by COLMAP.&lt;/p&gt; &#xA;&lt;h4&gt;Making your own loader by implementing &lt;code&gt;_load_renderings&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;To make a new dataset, make a class inheriting from &lt;code&gt;Dataset&lt;/code&gt; and overload the &lt;code&gt;_load_renderings&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;class MyNewDataset(Dataset):&#xA;  def _load_renderings(self, config):&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this function, you &lt;strong&gt;must&lt;/strong&gt; set the following public attributes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;images&lt;/li&gt; &#xA; &lt;li&gt;camtoworlds&lt;/li&gt; &#xA; &lt;li&gt;pixtocams&lt;/li&gt; &#xA; &lt;li&gt;height, width&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Many of our dataset loaders also set other useful attributes, but these are the critical ones for generating rays. You can see how they are used (along with a batch of pixel coordinates) to create rays in &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/camera_utils.py#L520&#34;&gt;&lt;code&gt;camera_utils.pixels_to_rays&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Images&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;images&lt;/code&gt; = [N, height, width, 3] numpy array of RGB images. Currently we require all images to have the same resolution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Extrinsic camera poses&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;camtoworlds&lt;/code&gt; = [N, 3, 4] numpy array of extrinsic pose matrices. &lt;code&gt;camtoworlds[i]&lt;/code&gt; should be in &lt;strong&gt;camera-to-world&lt;/strong&gt; format, such that we can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pose = camtoworlds[i]&#xA;x_world = pose[:3, :3] @ x_camera + pose[:3, 3:4]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to convert a 3D camera space point &lt;code&gt;x_camera&lt;/code&gt; into a world space point &lt;code&gt;x_world&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These matrices must be stored in the &lt;strong&gt;OpenGL&lt;/strong&gt; coordinate system convention for camera rotation: x-axis to the right, y-axis upward, and z-axis backward along the camera&#39;s focal axis.&lt;/p&gt; &#xA;&lt;p&gt;The most common conventions are&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[right, up, backwards]&lt;/code&gt;: OpenGL, NeRF, most graphics code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[right, down, forwards]&lt;/code&gt;: OpenCV, COLMAP, most computer vision code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Fortunately switching from OpenCV/COLMAP to NeRF is &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/datasets.py#L108&#34;&gt;simple&lt;/a&gt;: you just need to right-multiply the OpenCV pose matrices by &lt;code&gt;np.diag([1, -1, -1, 1])&lt;/code&gt;, which will flip the sign of the y-axis (from down to up) and z-axis (from forwards to backwards):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;camtoworlds_opengl = camtoworlds_opencv @ np.diag([1, -1, -1, 1])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also want to &lt;strong&gt;scale&lt;/strong&gt; your camera pose translations such that they all lie within the &lt;code&gt;[-1, 1]^3&lt;/code&gt; cube for best performance with the default mipnerf360 config files.&lt;/p&gt; &#xA;&lt;p&gt;We provide a useful helper function &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/camera_utils.py#L191&#34;&gt;&lt;code&gt;camera_utils.transform_poses_pca&lt;/code&gt;&lt;/a&gt; that computes a translation/rotation/scaling transform for the input poses that aligns the world space x-y plane with the ground (based on PCA) and scales the scene so that all input pose positions lie within &lt;code&gt;[-1, 1]^3&lt;/code&gt;. (This function is applied by default when loading mip-NeRF 360 scenes with the LLFF data loader.) For a scene where this transformation has been applied, &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/camera_utils.py#L230&#34;&gt;&lt;code&gt;camera_utils.generate_ellipse_path&lt;/code&gt;&lt;/a&gt; can be used to generate a nice elliptical camera path for rendering videos.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intrinsic camera poses&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pixtocams&lt;/code&gt;= [N, 3, 4] numpy array of inverse intrinsic matrices, OR [3, 4] numpy array of a single shared inverse intrinsic matrix. These should be in &lt;strong&gt;OpenCV&lt;/strong&gt; format, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;camtopix = np.array([&#xA;  [focal,     0,  width/2],&#xA;  [    0, focal, height/2],&#xA;  [    0,     0,        1],&#xA;])&#xA;pixtocam = np.linalg.inv(camtopix)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Given a focal length and image size (and assuming a centered principal point, this matrix can be created using &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/camera_utils.py#L411&#34;&gt;&lt;code&gt;camera_utils.get_pixtocam&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, it can be created by using &lt;a href=&#34;https://github.com/google-research/multinerf/raw/main/internal/camera_utils.py#L398&#34;&gt;&lt;code&gt;camera_utils.intrinsic_matrix&lt;/code&gt;&lt;/a&gt; and inverting the resulting matrix.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;height&lt;/code&gt; = int, height of images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;width&lt;/code&gt; = int, width of images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Distortion parameters (optional)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;distortion_params&lt;/code&gt; = dict, camera lens distortion model parameters. This dictionary must map from strings -&amp;gt; floats, and the allowed keys are &lt;code&gt;[&#39;k1&#39;, &#39;k2&#39;, &#39;k3&#39;, &#39;k4&#39;, &#39;p1&#39;, &#39;p2&#39;]&lt;/code&gt; (up to four radial coefficients and up to two tangential coefficients). By default, this is set to the empty dictionary &lt;code&gt;{}&lt;/code&gt;, in which case undistortion is not run.&lt;/p&gt; &#xA;&lt;h3&gt;Details of the inner workings of Dataset&lt;/h3&gt; &#xA;&lt;p&gt;The public interface mimics the behavior of a standard machine learning pipeline dataset provider that can provide infinite batches of data to the training/testing pipelines without exposing any details of how the batches are loaded/created or how this is parallelized. Therefore, the initializer runs all setup, including data loading from disk using &lt;code&gt;_load_renderings&lt;/code&gt;, and begins the thread using its parent start() method. After the initializer returns, the caller can request batches of data straight away.&lt;/p&gt; &#xA;&lt;p&gt;The internal &lt;code&gt;self._queue&lt;/code&gt; is initialized as &lt;code&gt;queue.Queue(3)&lt;/code&gt;, so the infinite loop in &lt;code&gt;run()&lt;/code&gt; will block on the call &lt;code&gt;self._queue.put(self._next_fn())&lt;/code&gt; once there are 3 elements. The main thread training job runs in a loop that pops 1 element at a time off the front of the queue. The Dataset thread&#39;s &lt;code&gt;run()&lt;/code&gt; loop will populate the queue with 3 elements, then wait until a batch has been removed and push one more onto the end.&lt;/p&gt; &#xA;&lt;p&gt;This repeats indefinitely until the main thread&#39;s training loop completes (typically hundreds of thousands of iterations), then the main thread will exit and the Dataset thread will automatically be killed since it is a daemon.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this software package, please cite whichever constituent paper(s) you build upon, or feel free to cite this entire codebase as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{multinerf2022,&#xA;      title={MultiNeRF: A Code Release for Mip-NeRF 360, Ref-NeRF, and RawNeRF},&#xA;      author={Ben Mildenhall and Dor Verbin and Pratul P. Srinivasan and Peter Hedman and Ricardo Martin-Brualla and Jonathan T. Barron},&#xA;      year={2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>moyix/fauxpilot</title>
    <updated>2022-08-07T01:31:33Z</updated>
    <id>tag:github.com,2022-08-07:/moyix/fauxpilot</id>
    <link href="https://github.com/moyix/fauxpilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FauxPilot - an open-source GitHub Copilot server&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FauxPilot&lt;/h1&gt; &#xA;&lt;p&gt;This is an attempt to build a locally hosted version of &lt;a href=&#34;https://copilot.github.com/&#34;&gt;GitHub Copilot&lt;/a&gt;. It uses the &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;SalesForce CodeGen&lt;/a&gt; models inside of NVIDIA&#39;s &lt;a href=&#34;https://developer.nvidia.com/nvidia-triton-inference-server&#34;&gt;Triton Inference Server&lt;/a&gt; with the &lt;a href=&#34;https://github.com/triton-inference-server/fastertransformer_backend/&#34;&gt;FasterTransformer backend&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;docker-compose&lt;/code&gt; &amp;gt;= 1.28&lt;/li&gt; &#xA; &lt;li&gt;An NVIDIA GPU with enough VRAM to run the model you want.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/nvidia-docker&#34;&gt;&lt;code&gt;nvidia-docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;curl&lt;/code&gt; and &lt;code&gt;zstd&lt;/code&gt; for downloading and unpacking the models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that the VRAM requirements listed by &lt;code&gt;setup.sh&lt;/code&gt; are &lt;em&gt;total&lt;/em&gt; -- if you have multiple GPUs, you can split the model across them. So, if you have two NVIDIA RTX 3080 GPUs, you &lt;em&gt;should&lt;/em&gt; be able to run the 6B model by putting half on each GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Support and Warranty&lt;/h2&gt; &#xA;&lt;p&gt;lmao&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Run the setup script to choose a model to use. This will download the model from Huggingface and then convert it for use with FasterTransformer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./setup.sh &#xA;Models available:&#xA;[1] codegen-350M-mono (2GB total VRAM required; Python-only)&#xA;[2] codegen-350M-multi (2GB total VRAM required; multi-language)&#xA;[3] codegen-2B-mono (7GB total VRAM required; Python-only)&#xA;[4] codegen-2B-multi (7GB total VRAM required; multi-language)&#xA;[5] codegen-6B-mono (13GB total VRAM required; Python-only)&#xA;[6] codegen-6B-multi (13GB total VRAM required; multi-language)&#xA;[7] codegen-16B-mono (32GB total VRAM required; Python-only)&#xA;[8] codegen-16B-multi (32GB total VRAM required; multi-language)&#xA;Enter your choice [6]: 2&#xA;Enter number of GPUs [1]: 1&#xA;Where do you want to save the model [/home/moyix/git/fauxpilot/models]? /fastdata/mymodels&#xA;Downloading and converting the model, this will take a while...&#xA;Converting model codegen-350M-multi with 1 GPUs&#xA;Loading CodeGen model&#xA;Downloading config.json: 100%|██████████| 996/996 [00:00&amp;lt;00:00, 1.25MB/s]&#xA;Downloading pytorch_model.bin: 100%|██████████| 760M/760M [00:11&amp;lt;00:00, 68.3MB/s] &#xA;Creating empty GPTJ model&#xA;Converting...&#xA;Conversion complete.&#xA;Saving model to codegen-350M-multi-hf...&#xA;&#xA;=============== Argument ===============&#xA;saved_dir: /models/codegen-350M-multi-1gpu/fastertransformer/1&#xA;in_file: codegen-350M-multi-hf&#xA;trained_gpu_num: 1&#xA;infer_gpu_num: 1&#xA;processes: 4&#xA;weight_data_type: fp32&#xA;========================================&#xA;transformer.wte.weight&#xA;transformer.h.0.ln_1.weight&#xA;[... more conversion output trimmed ...]&#xA;transformer.ln_f.weight&#xA;transformer.ln_f.bias&#xA;lm_head.weight&#xA;lm_head.bias&#xA;Done! Now run ./launch.sh to start the FauxPilot server.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can just run &lt;code&gt;./launch.sh&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./launch.sh &#xA;[+] Running 2/0&#xA; ⠿ Container fauxpilot-triton-1         Created                                                                                                                                                                                                                                                                                             0.0s&#xA; ⠿ Container fauxpilot-copilot_proxy-1  Created                                                                                                                                                                                                                                                                                             0.0s&#xA;Attaching to fauxpilot-copilot_proxy-1, fauxpilot-triton-1&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | =============================&#xA;fauxpilot-triton-1         | == Triton Inference Server ==&#xA;fauxpilot-triton-1         | =============================&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | NVIDIA Release 22.06 (build 39726160)&#xA;fauxpilot-triton-1         | Triton Server Version 2.23.0&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | Copyright (c) 2018-2022, NVIDIA CORPORATION &amp;amp; AFFILIATES.  All rights reserved.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | Various files include modifications (c) NVIDIA CORPORATION &amp;amp; AFFILIATES.  All rights reserved.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | This container image and its contents are governed by the NVIDIA Deep Learning Container License.&#xA;fauxpilot-triton-1         | By pulling and using the container, you accept the terms and conditions of this license:&#xA;fauxpilot-triton-1         | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license&#xA;fauxpilot-copilot_proxy-1  | WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.&#xA;fauxpilot-copilot_proxy-1  |  * Debug mode: off&#xA;fauxpilot-copilot_proxy-1  |  * Running on all addresses (0.0.0.0)&#xA;fauxpilot-copilot_proxy-1  |    WARNING: This is a development server. Do not use it in a production deployment.&#xA;fauxpilot-copilot_proxy-1  |  * Running on http://127.0.0.1:5000&#xA;fauxpilot-copilot_proxy-1  |  * Running on http://172.18.0.3:5000 (Press CTRL+C to quit)&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | ERROR: This container was built for NVIDIA Driver Release 515.48 or later, but&#xA;fauxpilot-triton-1         |        version  was detected and compatibility mode is UNAVAILABLE.&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         |        [[]]&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:02.690042 93 pinned_memory_manager.cc:240] Pinned memory pool is created at &#39;0x7f6104000000&#39; with size 268435456&#xA;fauxpilot-triton-1         | I0803 01:51:02.690461 93 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864&#xA;fauxpilot-triton-1         | I0803 01:51:02.692434 93 model_repository_manager.cc:1191] loading: fastertransformer:1&#xA;fauxpilot-triton-1         | I0803 01:51:02.936798 93 libfastertransformer.cc:1226] TRITONBACKEND_Initialize: fastertransformer&#xA;fauxpilot-triton-1         | I0803 01:51:02.936818 93 libfastertransformer.cc:1236] Triton TRITONBACKEND API version: 1.10&#xA;fauxpilot-triton-1         | I0803 01:51:02.936821 93 libfastertransformer.cc:1242] &#39;fastertransformer&#39; TRITONBACKEND API version: 1.10&#xA;fauxpilot-triton-1         | I0803 01:51:02.936850 93 libfastertransformer.cc:1274] TRITONBACKEND_ModelInitialize: fastertransformer (version 1)&#xA;fauxpilot-triton-1         | W0803 01:51:02.937855 93 libfastertransformer.cc:149] model configuration:&#xA;fauxpilot-triton-1         | {&#xA;[... lots more output trimmed ...]&#xA;fauxpilot-triton-1         | I0803 01:51:04.711929 93 libfastertransformer.cc:321] After Loading Model:&#xA;fauxpilot-triton-1         | I0803 01:51:04.712427 93 libfastertransformer.cc:537] Model instance is created on GPU NVIDIA RTX A6000&#xA;fauxpilot-triton-1         | I0803 01:51:04.712694 93 model_repository_manager.cc:1345] successfully loaded &#39;fastertransformer&#39; version 1&#xA;fauxpilot-triton-1         | I0803 01:51:04.712841 93 server.cc:556] &#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | | Repository Agent | Path |&#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | +------------------+------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.712916 93 server.cc:583] &#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | Backend           | Path                                                                        | Config                                                                                                                                                         |&#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | fastertransformer | /opt/tritonserver/backends/fastertransformer/libtriton_fastertransformer.so | {&#34;cmdline&#34;:{&#34;auto-complete-config&#34;:&#34;false&#34;,&#34;min-compute-capability&#34;:&#34;6.000000&#34;,&#34;backend-directory&#34;:&#34;/opt/tritonserver/backends&#34;,&#34;default-max-batch-size&#34;:&#34;4&#34;}} |&#xA;fauxpilot-triton-1         | +-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.712959 93 server.cc:626] &#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | | Model             | Version | Status |&#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | | fastertransformer | 1       | READY  |&#xA;fauxpilot-triton-1         | +-------------------+---------+--------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.738989 93 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA RTX A6000&#xA;fauxpilot-triton-1         | I0803 01:51:04.739373 93 tritonserver.cc:2159] &#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | Option                           | Value                                                                                                                                                                                        |&#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | | server_id                        | triton                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | server_version                   | 2.23.0                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |&#xA;fauxpilot-triton-1         | | model_repository_path[0]         | /model                                                                                                                                                                                       |&#xA;fauxpilot-triton-1         | | model_control_mode               | MODE_NONE                                                                                                                                                                                    |&#xA;fauxpilot-triton-1         | | strict_model_config              | 1                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | rate_limit                       | OFF                                                                                                                                                                                          |&#xA;fauxpilot-triton-1         | | pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |&#xA;fauxpilot-triton-1         | | cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |&#xA;fauxpilot-triton-1         | | response_cache_byte_size         | 0                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | min_supported_compute_capability | 6.0                                                                                                                                                                                          |&#xA;fauxpilot-triton-1         | | strict_readiness                 | 1                                                                                                                                                                                            |&#xA;fauxpilot-triton-1         | | exit_timeout                     | 30                                                                                                                                                                                           |&#xA;fauxpilot-triton-1         | +----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+&#xA;fauxpilot-triton-1         | &#xA;fauxpilot-triton-1         | I0803 01:51:04.740423 93 grpc_server.cc:4587] Started GRPCInferenceService at 0.0.0.0:8001&#xA;fauxpilot-triton-1         | I0803 01:51:04.740608 93 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000&#xA;fauxpilot-triton-1         | I0803 01:51:04.781561 93 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;Once everything is up and running, you should have a server listening for requests on &lt;code&gt;http://localhost:5000&lt;/code&gt;. You can now talk to it using the standard &lt;a href=&#34;https://beta.openai.com/docs/api-reference/&#34;&gt;OpenAI API&lt;/a&gt; (although the full API isn&#39;t implemented yet). For example, from Python, using the &lt;a href=&#34;https://github.com/openai/openai-python&#34;&gt;OpenAI Python bindings&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;$ ipython&#xA;Python 3.8.10 (default, Mar 15 2022, 12:22:08) &#xA;Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information&#xA;IPython 8.2.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.&#xA;&#xA;In [1]: import openai&#xA;&#xA;In [2]: openai.api_key = &#39;dummy&#39;&#xA;&#xA;In [3]: openai.api_base = &#39;http://127.0.0.1:5000/v1&#39;&#xA;&#xA;In [4]: result = openai.Completion.create(engine=&#39;codegen&#39;, prompt=&#39;def hello&#39;, max_tokens=16, temperature=0.1, stop=[&#34;\n\n&#34;])&#xA;&#xA;In [5]: result&#xA;Out[5]: &#xA;&amp;lt;OpenAIObject text_completion id=cmpl-6hqu8Rcaq25078IHNJNVooU4xLY6w at 0x7f602c3d2f40&amp;gt; JSON: {&#xA;  &#34;choices&#34;: [&#xA;    {&#xA;      &#34;finish_reason&#34;: &#34;stop&#34;,&#xA;      &#34;index&#34;: 0,&#xA;      &#34;logprobs&#34;: null,&#xA;      &#34;text&#34;: &#34;() {\n    return \&#34;Hello, World!\&#34;;\n}&#34;&#xA;    }&#xA;  ],&#xA;  &#34;created&#34;: 1659492191,&#xA;  &#34;id&#34;: &#34;cmpl-6hqu8Rcaq25078IHNJNVooU4xLY6w&#34;,&#xA;  &#34;model&#34;: &#34;codegen&#34;,&#xA;  &#34;object&#34;: &#34;text_completion&#34;,&#xA;  &#34;usage&#34;: {&#xA;    &#34;completion_tokens&#34;: 15,&#xA;    &#34;prompt_tokens&#34;: 2,&#xA;    &#34;total_tokens&#34;: 17&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Copilot Plugin&lt;/h2&gt; &#xA;&lt;p&gt;Perhaps more excitingly, you can configure the official &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=GitHub.copilot&#34;&gt;VSCode Copilot plugin&lt;/a&gt; to use your local server. Just edit your &lt;code&gt;settings.json&lt;/code&gt; to add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;    &#34;github.copilot.advanced&#34;: {&#xA;        &#34;debug.overrideEngine&#34;: &#34;codegen&#34;,&#xA;        &#34;debug.testOverrideProxyUrl&#34;: &#34;http://localhost:5000&#34;,&#xA;        &#34;debug.overrideProxyUrl&#34;: &#34;http://localhost:5000&#34;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you should be able to use Copilot with your own locally hosted suggestions! Of course, probably a lot of stuff is subtly broken. In particular, the probabilities returned by the server are partly fake. Fixing this would require changing FasterTransformer so that it can return log-probabilities for the top k tokens rather that just the chosen token.&lt;/p&gt; &#xA;&lt;p&gt;Have fun!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/paranoid_crypto</title>
    <updated>2022-08-07T01:31:33Z</updated>
    <id>tag:github.com,2022-08-07:/google/paranoid_crypto</id>
    <link href="https://github.com/google/paranoid_crypto" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Project Paranoid&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paranoid&lt;/strong&gt; project checks for well known weaknesses on cryptographic artifacts such as public keys, digital signatures and general pseudorandom numbers. This library contains implementations and optimizations of existing work found in the literature. The existing work showed that the generation of these artifacts was flawed in some cases. The following are some examples of publications the library is based on.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://eprint.iacr.org/2012/064&#34;&gt;Arjen K. Lenstra, James P. Hughes, Maxime Augier, Joppe W. Bos, Thorsten Kleinjung, and Christophe Wachter. (2012). &lt;strong&gt;Ron was wrong, Whit is right&lt;/strong&gt;&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity12/technical-sessions/presentation/heninger&#34;&gt;Nadia Heninger, Zakir Durumeric, Eric Wustrow, and J. Alex Halderman. (2012). &lt;strong&gt;Mining Your Ps and Qs: Detection of Widespread Weak Keys in Network Devices&lt;/strong&gt;&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://eprint.iacr.org/2013/599&#34;&gt;Daniel J. Bernstein, Yun-An Chang, Chen-Mou Cheng, Li-Ping Chou, Nadia Heninger, Tanja Lange, and Nicko van Someren. (2013). &lt;strong&gt;Factoring RSA keys from certified smart cards: Coppersmith in the wild&lt;/strong&gt;&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://eprint.iacr.org/2019/023&#34;&gt;Joachim Breitner and Nadia Heninger. (2019). &lt;strong&gt;Biased Nonce Sense: Lattice Attacks against Weak ECDSA Signatures in Cryptocurrencies&lt;/strong&gt;&lt;/a&gt;;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Goal&lt;/h2&gt; &#xA;&lt;p&gt;The goal is to increase the confidence in cryptography use cases inside and outside Google.&lt;/p&gt; &#xA;&lt;p&gt;When dealing with asymmetric encryption, crypto artifacts usually are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generated by one of our own tools (e.g., at Google we use for example &lt;a href=&#34;https://github.com/google/boringssl&#34;&gt;boringssl&lt;/a&gt; or &lt;a href=&#34;https://github.com/google/tink&#34;&gt;tink&lt;/a&gt;); or,&lt;/li&gt; &#xA; &lt;li&gt;Generated by third party tools that we have access to (so these tools can be, for example, checked for vulnerabilities using &lt;a href=&#34;https://github.com/google/wycheproof&#34;&gt;wycheproof&lt;/a&gt;); or,&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generated by third party tools and/or hardware or software black boxes that we do not have access to.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;With Paranoid, any cryptographic artifact can be tested, but its primary motivation is to detect the usage of weak third party hardware or software black boxes. Hence, Paranoid can be used even if we are not able to inspect the source code (situation 3. listed above).&lt;/p&gt; &#xA;&lt;p&gt;The project aims to detect known vulnerabilities as well as unknown ones. E.g., it tries to identify vulnerabilities caused by programming errors or the use of weak proprietary random number generators. Detecting new vulnerabilities is of course much more difficult than detecting known ones. Such detections may require large sets of artifacts or find weak ones only with a low probability.&lt;/p&gt; &#xA;&lt;p&gt;Therefore, we are very interested to receive feedback and learn about the cryptographic library that generated weak cryptographic artifacts. The project is constantly work in progress. After learning about weak implementations the plan is to analyze and add detections targeting them.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Documentation for the current implemented checks is located at &lt;a href=&#34;https://raw.githubusercontent.com/google/paranoid_crypto/main/docs&#34;&gt;docs&lt;/a&gt;. The documentation will be populated with more content over time.&lt;/p&gt; &#xA;&lt;p&gt;To learn how to use the checks, you can look at the &lt;a href=&#34;https://raw.githubusercontent.com/google/paranoid_crypto/main/examples&#34;&gt;examples&lt;/a&gt; folder or the unit tests (*test.py files). The examples demonstrate testing different crypto artifacts.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ git clone https://github.com/google/paranoid_crypto.git &amp;amp;&amp;amp; cd paranoid_crypto&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The commands below have been tested on Debian latest stable version (bullseye). Make sure you will be using &lt;code&gt;python3.9&lt;/code&gt; or newer.&lt;/p&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ sudo apt update &amp;amp;&amp;amp; sudo apt install python3 python3-pip python3-pybind11 python3-fpylll libgmp-dev protobuf-compiler&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install paranoid_crypto python package:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ python3 -m pip install .&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To check whether the installation was successful, you can run the unit tests. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd paranoid_crypto &amp;amp;&amp;amp; python3 -m unittest discover -b -p &#34;*test.py&#34;&#xA;.................................................................................................................................................................................................................................................................................................................&#xA;----------------------------------------------------------------------&#xA;Ran 305 tests in 314.660s&#xA;&#xA;OK&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, if you want to run it in a container, you can use our provided &lt;a href=&#34;https://raw.githubusercontent.com/google/paranoid_crypto/main/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; as shown below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Make sure you have &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;docker&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;After cloning the repository, build the docker image:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;$ docker build -t paranoid-img .&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Create and start the container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker run --name paranoid-container -it paranoid-img&#xA;paranoid-user@6191368b26b8:~$&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To check whether the installation was successful, you can run the unit tests. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;paranoid-user@6191368b26b8:~$ cd paranoid_crypto &amp;amp;&amp;amp; python3 -m unittest discover -b -p &#34;*test.py&#34;&#xA;.................................................................................................................................................................................................................................................................................................................&#xA;----------------------------------------------------------------------&#xA;Ran 305 tests in 307.555s&#xA;&#xA;OK&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ABOUT&lt;/h2&gt; &#xA;&lt;p&gt;This library is developed and maintained by members of Google Security Team, but this is not an officially supported Google product. If you want to contribute, please read &lt;a href=&#34;https://raw.githubusercontent.com/google/paranoid_crypto/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; and send us pull requests. You can also report bugs or file feature requests.&lt;/p&gt; &#xA;&lt;p&gt;If you use Paranoid in your research, you can cite it using the following BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{Barbosa_Bleichenbacher_Paranoid_Crypto_2022,&#xA;  author = {Barbosa, Pedro and Bleichenbacher, Daniel},&#xA;  license = {Apache-2.0},&#xA;  month = {8},&#xA;  title = {{Paranoid Crypto}},&#xA;  url = {https://github.com/google/paranoid_crypto},&#xA;  year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>