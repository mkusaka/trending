<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-28T01:38:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>daveebbelaar/ai-cookbook</title>
    <updated>2025-07-28T01:38:42Z</updated>
    <id>tag:github.com,2025-07-28:/daveebbelaar/ai-cookbook</id>
    <link href="https://github.com/daveebbelaar/ai-cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples and tutorials to help developers build AI systems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;This Cookbook contains examples and tutorials to help developers build AI systems, offering copy/paste code snippets that you can easily integrate into your own projects.&lt;/p&gt; &#xA;&lt;h2&gt;About Me&lt;/h2&gt; &#xA;&lt;p&gt;Hi! I&#39;m Dave, AI Engineer and founder of Datalumina®. On my &lt;a href=&#34;https://www.youtube.com/@daveebbelaar?sub_confirmation=1&#34;&gt;YouTube channel&lt;/a&gt;, I share practical tutorials that teach developers how to build AI systems that actually work in the real world. Beyond these tutorials, I also help people start successful freelancing careers. Check out the links below to learn more!&lt;/p&gt; &#xA;&lt;h3&gt;Explore More Resources&lt;/h3&gt; &#xA;&lt;p&gt;Whether you&#39;re a learner, a freelancer, or a business looking for AI expertise, we&#39;ve got something for you:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning Python for AI and Data Science?&lt;/strong&gt;&lt;br&gt; Join our &lt;strong&gt;free community, Data Alchemy&lt;/strong&gt;, where you&#39;ll find resources, tutorials, and support&lt;br&gt; ▶︎ &lt;a href=&#34;https://www.skool.com/data-alchemy&#34;&gt;Learn Python for AI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ready to start or scale your freelancing career?&lt;/strong&gt;&lt;br&gt; Learn how to land clients and grow your business&lt;br&gt; ▶︎ &lt;a href=&#34;https://www.datalumina.com/data-freelancer&#34;&gt;Find freelance projects&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Need expert help on your next project?&lt;/strong&gt;&lt;br&gt; Work with me and my team to solve your data and AI challenges&lt;br&gt; ▶︎ &lt;a href=&#34;https://www.datalumina.com/solutions&#34;&gt;Work with me&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Already building AI applications?&lt;/strong&gt;&lt;br&gt; Explore the &lt;strong&gt;GenAI Launchpad&lt;/strong&gt;, our production framework for AI systems&lt;br&gt; ▶︎ &lt;a href=&#34;https://launchpad.datalumina.com/&#34;&gt;Explore the GenAI Launchpad&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>ml-explore/mlx-lm</title>
    <updated>2025-07-28T01:38:42Z</updated>
    <id>tag:github.com,2025-07-28:/ml-explore/mlx-lm</id>
    <link href="https://github.com/ml-explore/mlx-lm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run LLMs with MLX&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;MLX LM&lt;/h2&gt; &#xA;&lt;p&gt;MLX LM is a Python package for generating text and fine-tuning large language models on Apple silicon with MLX.&lt;/p&gt; &#xA;&lt;p&gt;Some key features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration with the Hugging Face Hub to easily use thousands of LLMs with a single command.&lt;/li&gt; &#xA; &lt;li&gt;Support for quantizing and uploading models to the Hugging Face Hub.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/LORA.md&#34;&gt;Low-rank and full model fine-tuning&lt;/a&gt; with support for quantized models.&lt;/li&gt; &#xA; &lt;li&gt;Distributed inference and fine-tuning with &lt;code&gt;mx.distributed&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The easiest way to get started is to install the &lt;code&gt;mlx-lm&lt;/code&gt; package:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install mlx-lm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda install -c conda-forge mlx-lm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;To generate text with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mlx_lm.generate --prompt &#34;How tall is Mt Everest?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To chat with an LLM use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mlx_lm.chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will give you a chat REPL that you can use to interact with the LLM. The chat context is preserved during the lifetime of the REPL.&lt;/p&gt; &#xA;&lt;p&gt;Commands in &lt;code&gt;mlx-lm&lt;/code&gt; typically take command line options which let you specify the model, sampling parameters, and more. Use &lt;code&gt;-h&lt;/code&gt; to see a list of available options for a command, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mlx_lm.generate -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;mlx-lm&lt;/code&gt; as a module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_lm import load, generate&#xA;&#xA;model, tokenizer = load(&#34;mlx-community/Mistral-7B-Instruct-v0.3-4bit&#34;)&#xA;&#xA;prompt = &#34;Write a story about Einstein&#34;&#xA;&#xA;messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}]&#xA;prompt = tokenizer.apply_chat_template(&#xA;    messages, add_generation_prompt=True&#xA;)&#xA;&#xA;text = generate(model, tokenizer, prompt=prompt, verbose=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(generate)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py&#34;&gt;generation example&lt;/a&gt; to see how to use the API in more detail.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;mlx-lm&lt;/code&gt; package also comes with functionality to quantize and optionally upload models to the Hugging Face Hub.&lt;/p&gt; &#xA;&lt;p&gt;You can convert models using the Python API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_lm import convert&#xA;&#xA;repo = &#34;mistralai/Mistral-7B-Instruct-v0.3&#34;&#xA;upload_repo = &#34;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&#34;&#xA;&#xA;convert(repo, quantize=True, upload_repo=upload_repo)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate a 4-bit quantized Mistral 7B and upload it to the repo &lt;code&gt;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&lt;/code&gt;. It will also save the converted model in the path &lt;code&gt;mlx_model&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(convert)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Streaming&lt;/h4&gt; &#xA;&lt;p&gt;For streaming generation, use the &lt;code&gt;stream_generate&lt;/code&gt; function. This yields a generation response object.&lt;/p&gt; &#xA;&lt;p&gt;For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mlx_lm import load, stream_generate&#xA;&#xA;repo = &#34;mlx-community/Mistral-7B-Instruct-v0.3-4bit&#34;&#xA;model, tokenizer = load(repo)&#xA;&#xA;prompt = &#34;Write a story about Einstein&#34;&#xA;&#xA;messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}]&#xA;prompt = tokenizer.apply_chat_template(&#xA;    messages, add_generation_prompt=True&#xA;)&#xA;&#xA;for response in stream_generate(model, tokenizer, prompt, max_tokens=512):&#xA;    print(response.text, end=&#34;&#34;, flush=True)&#xA;print()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Sampling&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;stream_generate&lt;/code&gt; functions accept &lt;code&gt;sampler&lt;/code&gt; and &lt;code&gt;logits_processors&lt;/code&gt; keyword arguments. A sampler is any callable which accepts a possibly batched logits array and returns an array of sampled tokens. The &lt;code&gt;logits_processors&lt;/code&gt; must be a list of callables which take the token history and current logits as input and return the processed logits. The logits processors are applied in order.&lt;/p&gt; &#xA;&lt;p&gt;Some standard sampling functions and logits processors are provided in &lt;code&gt;mlx_lm.sample_utils&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Command Line&lt;/h3&gt; &#xA;&lt;p&gt;You can also use &lt;code&gt;mlx-lm&lt;/code&gt; from the command line with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt &#34;hello&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download a Mistral 7B model from the Hugging Face Hub and generate text using the given prompt.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of options run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mlx_lm.generate --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To quantize a model from the command line run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more options run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mlx_lm.convert --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can upload new models to Hugging Face by specifying &lt;code&gt;--upload-repo&lt;/code&gt; to &lt;code&gt;convert&lt;/code&gt;. For example, to upload a quantized Mistral-7B model to the &lt;a href=&#34;https://huggingface.co/mlx-community&#34;&gt;MLX Hugging Face community&lt;/a&gt; you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mlx_lm.convert \&#xA;    --hf-path mistralai/Mistral-7B-Instruct-v0.3 \&#xA;    -q \&#xA;    --upload-repo mlx-community/my-4bit-mistral&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Models can also be converted and quantized directly in the &lt;a href=&#34;https://huggingface.co/spaces/mlx-community/mlx-my-repo&#34;&gt;mlx-my-repo&lt;/a&gt; Hugging Face Space.&lt;/p&gt; &#xA;&lt;h3&gt;Long Prompts and Generations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; has some tools to scale efficiently to long prompts and generations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A rotating fixed-size key-value cache.&lt;/li&gt; &#xA; &lt;li&gt;Prompt caching&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use the rotating key-value cache pass the argument &lt;code&gt;--max-kv-size n&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; can be any integer. Smaller values like &lt;code&gt;512&lt;/code&gt; will use very little RAM but result in worse quality. Larger values like &lt;code&gt;4096&lt;/code&gt; or higher will use more RAM but have better quality.&lt;/p&gt; &#xA;&lt;p&gt;Caching prompts can substantially speedup reusing the same long context with different queries. To cache a prompt use &lt;code&gt;mlx_lm.cache_prompt&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat prompt.txt | mlx_lm.cache_prompt \&#xA;  --model mistralai/Mistral-7B-Instruct-v0.3 \&#xA;  --prompt - \&#xA;  --prompt-cache-file mistral_prompt.safetensors&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use the cached prompt with &lt;code&gt;mlx_lm.generate&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mlx_lm.generate \&#xA;    --prompt-cache-file mistral_prompt.safetensors \&#xA;    --prompt &#34;\nSummarize the above text.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The cached prompt is treated as a prefix to the supplied prompt. Also notice when using a cached prompt, the model to use is read from the cache and need not be supplied explicitly.&lt;/p&gt; &#xA;&lt;p&gt;Prompt caching can also be used in the Python API in order to avoid recomputing the prompt. This is useful in multi-turn dialogues or across requests that use the same context. See the &lt;a href=&#34;https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/examples/chat.py&#34;&gt;example&lt;/a&gt; for more usage details.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; supports thousands of Hugging Face format LLMs. If the model you want to run is not supported, file an &lt;a href=&#34;https://github.com/ml-explore/mlx-lm/issues/new&#34;&gt;issue&lt;/a&gt; or better yet, submit a pull request.&lt;/p&gt; &#xA;&lt;p&gt;Here are a few examples of Hugging Face models that work with this example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct&#34;&gt;deepseek-ai/deepseek-coder-6.7b-instruct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/01-ai/Yi-6B-Chat&#34;&gt;01-ai/Yi-6B-Chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/microsoft/phi-2&#34;&gt;microsoft/phi-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;mistralai/Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B&#34;&gt;Qwen/Qwen-7B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/pfnet/plamo-13b&#34;&gt;pfnet/plamo-13b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/pfnet/plamo-13b-instruct&#34;&gt;pfnet/plamo-13b-instruct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b&#34;&gt;stabilityai/stablelm-2-zephyr-1_6b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/internlm/internlm2-7b&#34;&gt;internlm/internlm2-7b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-mamba-7b-instruct&#34;&gt;tiiuae/falcon-mamba-7b-instruct&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most &lt;a href=&#34;https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mistral&amp;amp;sort=trending&#34;&gt;Mistral&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=llama&amp;amp;sort=trending&#34;&gt;Llama&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=phi&amp;amp;sort=trending&#34;&gt;Phi-2&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mixtral&amp;amp;sort=trending&#34;&gt;Mixtral&lt;/a&gt; style models should work out of the box.&lt;/p&gt; &#xA;&lt;p&gt;For some models (such as &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;plamo&lt;/code&gt;) the tokenizer requires you to enable the &lt;code&gt;trust_remote_code&lt;/code&gt; option. You can do this by passing &lt;code&gt;--trust-remote-code&lt;/code&gt; in the command line. If you don&#39;t specify the flag explicitly, you will be prompted to trust remote code in the terminal when running the model.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;Qwen&lt;/code&gt; models you must also specify the &lt;code&gt;eos_token&lt;/code&gt;. You can do this by passing &lt;code&gt;--eos-token &#34;&amp;lt;|endoftext|&amp;gt;&#34;&lt;/code&gt; in the command line.&lt;/p&gt; &#xA;&lt;p&gt;These options can also be set in the Python API. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model, tokenizer = load(&#xA;    &#34;qwen/Qwen-7B&#34;,&#xA;    tokenizer_config={&#34;eos_token&#34;: &#34;&amp;lt;|endoftext|&amp;gt;&#34;, &#34;trust_remote_code&#34;: True},&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Large Models&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] This requires macOS 15.0 or higher to work.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Models which are large relative to the total RAM available on the machine can be slow. &lt;code&gt;mlx-lm&lt;/code&gt; will attempt to make them faster by wiring the memory occupied by the model and cache. This requires macOS 15 or higher to work.&lt;/p&gt; &#xA;&lt;p&gt;If you see the following warning message:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[WARNING] Generating with a model that requires ...&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;then the model will likely be slow on the given machine. If the model fits in RAM then it can often be sped up by increasing the system wired memory limit. To increase the limit, set the following &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo sysctl iogpu.wired_limit_mb=N&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The value &lt;code&gt;N&lt;/code&gt; should be larger than the size of the model in megabytes but smaller than the memory size of the machine.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OWASP/Nest</title>
    <updated>2025-07-28T01:38:42Z</updated>
    <id>tag:github.com,2025-07-28:/OWASP/Nest</id>
    <link href="https://github.com/OWASP/Nest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Your gateway to OWASP. Discover, engage, and help shape the future!&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source srcset=&#34;https://nest.owasp.org/img/owasp_icon_white_sm.png&#34; media=&#34;(prefers-color-scheme: dark)&#34;&gt; &#xA;  &lt;img src=&#34;https://nest.owasp.org/img/owasp_icon_black_sm.png&#34; alt=&#34;OWASP Logo&#34; width=&#34;200&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;h1&gt;&lt;a href=&#34;https://nest.owasp.org/&#34;&gt;OWASP Nest&lt;/a&gt;&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://owasp.org/www-project-nest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OWASP-Incubator-blue?style=for-the-badge&#34; alt=&#34;OWASP&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/OWASP-Code-blue?style=for-the-badge&#34; alt=&#34;OWASP&#34;&gt; &lt;a href=&#34;https://owasp.slack.com/messages/project-nest&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OWASP-%23project--nest-blue?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Project-Nest&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/owasp/nest?color=41BE4A&amp;amp;label=License&amp;amp;style=for-the-badge&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/owasp/nest/main?style=for-the-badge&amp;amp;label=Last%20commit&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/owasp/nest?style=for-the-badge&amp;amp;label=Contributors&#34; alt=&#34;Contributors&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/owasp/nest/actions/workflows/run-ci-cd.yaml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/owasp/nest/run-ci-cd.yaml?branch=main&amp;amp;label=Build&amp;amp;style=for-the-badge&#34; alt=&#34;CI/CD&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/owasp/nest/actions/workflows/run-code-ql.yaml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/owasp/nest/run-code-ql.yaml?branch=main&amp;amp;label=CodeQL&amp;amp;style=for-the-badge&#34; alt=&#34;CodeQL&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sonarcloud.io/project/information?id=OWASP_Nest&#34;&gt;&lt;img src=&#34;https://img.shields.io/sonar/quality_gate/OWASP_Nest?server=https://sonarcloud.io&amp;amp;style=for-the-badge&amp;amp;label=Sonarqube&#34; alt=&#34;Sonarqube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/issues/owasp/nest?color=blue&amp;amp;style=for-the-badge&amp;amp;label=Issues&#34; alt=&#34;Issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/owasp/nest?color=blue&amp;amp;style=for-the-badge&amp;amp;label=Pull%20Requests&#34; alt=&#34;Pull Requests&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.bestpractices.dev/projects/10174&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenSSF-84%25-blue?style=for-the-badge&#34; alt=&#34;OpenSSF&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://snyk.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Snyk-Scanned-blue?style=for-the-badge&#34; alt=&#34;Snyk Security&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/forks/owasp/nest?style=for-the-badge&amp;amp;label=Forks&#34; alt=&#34;Forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/owasp/nest?style=for-the-badge&amp;amp;label=Stars&#34; alt=&#34;Stars&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/OWASP/Nest/commit/2a213c2efcfc2f8889c2f1d330da0d2e6f649fc1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/created-aug,%202024-blue?style=for-the-badge&#34; alt=&#34;CREATED&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;OWASP Nest&lt;/strong&gt; is a comprehensive platform designed to enhance collaboration and contribution within the OWASP community. The application serves as a central hub for exploring OWASP projects and ways to contribute to them, empowering contributors to find opportunities that align with their interests and expertise.&lt;/p&gt; &#xA;&lt;p&gt;Key features of the platform include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced Search Capabilities:&lt;/strong&gt; Enables efficient navigation and filtering of projects and issues based on keywords, tags, and contributor preferences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Slack Integration:&lt;/strong&gt; Supports seamless communication through a &lt;a href=&#34;https://owasp.slack.com/team/U07M1C4JASK&#34;&gt;Slack bot&lt;/a&gt; that facilitates direct and channel messaging for updates and discussions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OWASP Chapters Proximity Page:&lt;/strong&gt; Offers localized information about nearby OWASP chapters to foster community engagement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI-Generated Insights:&lt;/strong&gt; Provides summarized descriptions and actionable steps for tackling project issues.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;OWASP Nest promotes collaboration, making it easier for both new and experienced contributors to engage meaningfully with OWASP&#39;s mission to improve software security worldwide.&lt;/p&gt; &#xA;&lt;h2&gt;Leaders&lt;/h2&gt; &#xA;&lt;p&gt;OWASP Nest is led by a dedicated team committed to fostering collaboration and supporting contributors. The leadership team ensures the platform aligns with OWASP&#39;s mission, continually improving its features to serve the community better. Current Leaders:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/arkid15r/&#34;&gt;Arkadii Yakovets&lt;/a&gt; -- CCSP, CISSP, CSSLP&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kasya/&#34;&gt;Kate Golovanova&lt;/a&gt; -- CC&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mamicidal/&#34;&gt;Starr Brown&lt;/a&gt; -- CISSP&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All OWASP Nest leaders are certified ISC2 professionals and adhere to the OWASP Code of Conduct. For questions or discussions with the leadership team and other contributors, please use the &lt;a href=&#34;https://owasp.slack.com/archives/project-nest&#34;&gt;#project-nest&lt;/a&gt; channel on OWASP Slack.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;OWASP Nest thrives on community contributions. Whether you are a developer, designer, writer, or enthusiast, there are various ways to get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Code Contributions: Help improve the platform by fixing issues or adding new features.&lt;/li&gt; &#xA; &lt;li&gt;Code Review: Review and provide feedback on pull requests to ensure code quality and maintainability.&lt;/li&gt; &#xA; &lt;li&gt;Documentation: Enhance user guides or create tutorials to help others navigate the platform.&lt;/li&gt; &#xA; &lt;li&gt;Issue Reporting: Identify and report bugs or suggest improvements.&lt;/li&gt; &#xA; &lt;li&gt;Engagement: Share feedback, participate in discussions, or promote the project in your network.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To get started, visit the &lt;a href=&#34;https://github.com/OWASP/Nest&#34;&gt;OWASP Nest Repository&lt;/a&gt;, explore the &lt;a href=&#34;https://github.com/OWASP/Nest/raw/main/CONTRIBUTING.md&#34;&gt;Contributing Guidelines&lt;/a&gt;, and &lt;a href=&#34;https://github.com/OWASP/Nest/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;OWASP Nest was &lt;strong&gt;originally created by Arkadii Yakovets&lt;/strong&gt; (Ark) to address challenges in navigating OWASP projects. The project was &lt;strong&gt;built from scratch based on Ark&#39;s ideas and discussions with Starr Brown&lt;/strong&gt; (Starr), ensuring a well-structured system design aligned with OWASP&#39;s ecosystem. Ark, an experienced software development professional with over 10 years of expertise in Python, Django, Django REST Framework (DRF), and related backend technologies, led the development of the backend using widely adopted Python &lt;strong&gt;open-source frameworks and libraries&lt;/strong&gt;, including DRF, django-filter, OpenAI, Algolia Search, slack-bolt, PyGitHub, pre-commit, pytest, and more. The initial frontend, based on Vue.js, was introduced by &lt;strong&gt;Kateryna Golovanova&lt;/strong&gt; (Kate), who later became the project co-leader due to her invaluable frontend and project management skills. The &lt;strong&gt;code is licensed under the MIT License&lt;/strong&gt;, encouraging contributions while protecting the authors from legal claims. All OWASP Nest leaders are OWASP members and adhere to the OWASP Code of Conduct.&lt;/p&gt; &#xA;&lt;p&gt;Over time, OWASP Nest has expanded to address broader OWASP community needs, such as Google Summer of Code (GSoC) student guidance and contribution opportunities discovery. The platform, along with NestBot, has become a popular entry point for various OWASP aspects, including projects, chapters, users, and aggregated contribution opportunities -- with even more features planned. OWASP Nest&#39;s success is also the result of many valuable &lt;a href=&#34;https://github.com/OWASP/Nest/graphs/contributors&#34;&gt;contributions&lt;/a&gt; from the broader &lt;a href=&#34;https://owasp.slack.com/archives/project-nest&#34;&gt;OWASP Nest community&lt;/a&gt;, whose efforts have helped shape and improve the project in countless ways.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;No other OWASP projects&#39; code was used in OWASP Nest&#39;s development.&lt;/strong&gt; While explicit attribution (other than per MIT license) is not required, contributors and other OWASP project leaders are welcome to provide it at their discretion.&lt;/p&gt; &#xA;&lt;h3&gt;Community and Social Media&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bsky.app/profile/nest.owasp.org&#34;&gt;BlueSky account&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/groups/14656108/&#34;&gt;LinkedIn group&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://owasp.slack.com/archives/project-nest&#34;&gt;Slack channel&lt;/a&gt; (join &lt;a href=&#34;https://owasp.org/slack/invite&#34;&gt;here&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>