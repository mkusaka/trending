<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-14T01:46:20Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AntonOsika/gpt-engineer</title>
    <updated>2023-06-14T01:46:20Z</updated>
    <id>tag:github.com,2023-06-14:/AntonOsika/gpt-engineer</id>
    <link href="https://github.com/AntonOsika/gpt-engineer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Specify what you want it to build, the AI asks for clarification, and then builds it.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPT Engineer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Specify what you want it to build, the AI asks for clarification, and then builds it.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;GPT Engineer is made to be easy to adapt, extend, and make your agent learn how you want your code to look. It generates an entire codebase based on a prompt.&lt;/p&gt; &#xA;&lt;h2&gt;Project philosophy&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple to get value&lt;/li&gt; &#xA; &lt;li&gt;Flexible and easy to add new own &#34;AI steps&#34;. See &lt;code&gt;steps.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Incrementally build towards a user experience of: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;high level prompting&lt;/li&gt; &#xA;   &lt;li&gt;giving feedback to the AI that it will remember over time&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast handovers back and forth between AI and human&lt;/li&gt; &#xA; &lt;li&gt;Simplicity, all computation is &#34;resumable&#34; and persisted to the filesystem&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;export OPENAI_API_KEY=[your api key]&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Run&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new empty folder with a &lt;code&gt;main_prompt&lt;/code&gt; file (or copy the example folder &lt;code&gt;cp example -r my-new-project&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fill in the &lt;code&gt;main_prompt&lt;/code&gt; in your new folder&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;python main.py my-new-project&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check the generated files in my-new-project/workspace_clarified&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Limitations&lt;/h3&gt; &#xA;&lt;p&gt;Implementing additional chain of thought prompting, e.g. &lt;a href=&#34;https://github.com/noahshinn024/reflexion&#34;&gt;Reflexion&lt;/a&gt;, should be able to make it more reliable and not miss requested functionality in the main prompt.&lt;/p&gt; &#xA;&lt;p&gt;Contributors welcome! If you are unsure what to add, check out the ideas listed in the Projects part of the github repo.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;You can specify the &#34;identity&#34; of the AI agent by editing the files in the &lt;code&gt;identity&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Editing the identity, and evolving the main_prompt, is currently how you make the agent remember things between projects.&lt;/p&gt; &#xA;&lt;p&gt;Each step in steps.py will have its communication history with GPT4 stored in the logs folder, and can be rerun with scripts/rerun_edited_message_logs.py.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer/assets/4467025/6e362e45-4a94-4b0d-973d-393a31d92d9b&#34;&gt;https://github.com/AntonOsika/gpt-engineer/assets/4467025/6e362e45-4a94-4b0d-973d-393a31d92d9b&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yvann-hub/Robby-chatbot</title>
    <updated>2023-06-14T01:46:20Z</updated>
    <id>tag:github.com,2023-06-14:/yvann-hub/Robby-chatbot</id>
    <link href="https://github.com/yvann-hub/Robby-chatbot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI chatbot ðŸ¤– for chat with CSV, PDF, TXT files ðŸ“„ and YTB videos ðŸŽ¥ | using LangchainðŸ¦œ | OpenAI | Streamlit âš¡&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Robby-chatbot ðŸ¤–&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/yvann_hub&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/yvann_hub?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;An AI chatbot featuring conversational memory, designed to enable users to discuss their CSV, PDF, TXT data and YTB videos in a more intuitive manner. ðŸš€&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yvann-hub/Robby-chatbot/main/robby-pic.png&#34; alt=&#34;Robby&#34;&gt; Robby the Robot from &lt;a href=&#34;https://youtu.be/bflfQN_YsTM&#34;&gt;Forbidden Planet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;For better understanding, see my medium article ðŸ–– : &lt;a href=&#34;https://medium.com/@yvann-hub/build-a-chatbot-on-your-csv-data-with-langchain-and-openai-ed121f85f0cd&#34;&gt;Build a chat-bot over your CSV data&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;Quick Start ðŸš€&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://robby-chatbot.streamlit.app/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Robby-Chatbot&amp;amp;message=Visit%20Website&amp;amp;color=ffffff&amp;amp;labelColor=ADD8E6&amp;amp;style=for-the-badge&#34; alt=&#34;Robby-Chatbot&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally ðŸ’»&lt;/h2&gt; &#xA;&lt;p&gt;Follow these steps to set up and run the service locally :&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8 or higher&lt;/li&gt; &#xA; &lt;li&gt;Git&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository :&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/yvann-hub/Robby-chatbot.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Navigate to the project directory :&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;cd Robby-chatbot&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Create a virtual environment :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .venv&#xA;.\.venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the required dependencies in the virtual environment :&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Launch the chat service locally :&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;streamlit run src/Home.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;That&#39;s it! The service is now up and running locally. ðŸ¤—&lt;/h4&gt; &#xA;&lt;h2&gt;Contributing ðŸ™Œ&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute to this project, please open an issue, submit a pull request or contact me at &lt;a href=&#34;mailto:barbot.yvann@gmail.com&#34;&gt;barbot.yvann@gmail.com&lt;/a&gt; (:&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SHI-Labs/Matting-Anything</title>
    <updated>2023-06-14T01:46:20Z</updated>
    <id>tag:github.com,2023-06-14:/SHI-Labs/Matting-Anything</id>
    <link href="https://github.com/SHI-Labs/Matting-Anything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Matting Anything&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XY2Q0HATGOk&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/shi-labs/Matting-Anything&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-HuggingFace%20Space-cyan.svg?sanitize=true&#34; alt=&#34;HuggingFace Space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorch.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Framework-PyTorch-orange.svg?sanitize=true&#34; alt=&#34;Framework: PyTorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-red.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chrisjuniorli.github.io/&#34;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&#34;https://praeclarumjj3.github.io/&#34;&gt;Jitesh Jain&lt;/a&gt;, &lt;a href=&#34;https://www.humphreyshi.com/home&#34;&gt;Humphrey Shi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://chrisjuniorli.github.io/project/Matting-Anything/&#34;&gt;&lt;code&gt;Project page&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2306.05399&#34;&gt;&lt;code&gt;ArXiv&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/pdf/2306.05399.pdf&#34;&gt;&lt;code&gt;Pdf&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=XY2Q0HATGOk&#34;&gt;&lt;code&gt;Video&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/shi-labs/Matting-Anything&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/assets/teaser_arxiv_v2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/09&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/shi-labs/Matting-Anything&#34;&gt;&lt;strong&gt;HuggingFace Demo&lt;/strong&gt;&lt;/a&gt; is released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/08&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2306.05399&#34;&gt;&lt;strong&gt;Arxiv Preprint&lt;/strong&gt;&lt;/a&gt; is released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/06&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://chrisjuniorli.github.io/project/Matting-Anything&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=XY2Q0HATGOk&#34;&gt;&lt;strong&gt;Demo Video&lt;/strong&gt;&lt;/a&gt; are released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/#matting-anything&#34;&gt;Matting-Anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Matting Anything&lt;/h2&gt; &#xA;&lt;h3&gt;Abstract&lt;/h3&gt; &#xA;&lt;p&gt;In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance. MAM offers several significant advantages over previous specialized image matting networks: (i) MAM is capable of dealing with various types of image matting, including semantic, instance, and referring image matting with only a single model; (ii) MAM leverages the feature maps from the Segment Anything Model (SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha matte through iterative refinement, which has only 2.7 million trainable parameters. (iii) By incorporating SAM, MAM simplifies the user intervention required for the interactive use of image matting from the trimap to the box, point, or text prompt. We evaluate the performance of MAM on various image matting benchmarks, and the experimental results demonstrate that MAM achieves comparable performance to the state-of-the-art specialized image matting models under different metrics on each benchmark. Overall, MAM shows superior generalization ability and can effectively handle various image matting tasks with fewer parameters, making it a practical solution for unified image matting.&lt;/p&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/assets/archi_arxiv.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;&lt;/div&gt;&#xA;&lt;br&gt; The MAM architecture consists of a pre-trained SAM and an M2M module. Given an input image I, SAM generates the mask prediction for the target instance based on the box or point user prompt. The M2M module takes the concatenated inputs, including the image, mask, and feature maps, and produces multi-scale predictions Î±os8, Î±os4, and Î±os1. The iterative refinement process, detailed in Section 3, progressively improves the precision of the final meticulous alpha matte Î±, incorporating information from the multi-scale outputs. &#xA;&lt;h3&gt;Visualization&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/assets/teaser.gif&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/assets/vis.png&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt; &#xA;&lt;/div&gt;&#xA;&lt;br&gt; &#xA;&lt;p&gt;We provide visualizations of the alpha matte predictions from SAM and MAM. Notably, we emphasize the differences in the red boxes. The visualizations demonstrate that MAM achieves improved predictions in the transition areas even without the trimap guidance. Additionally, MAM effectively addresses some of the holes present in the mask predictions generated by SAM. These visual comparisons highlight the superior performance of MAM in refining and enhancing the quality of alpha matte predictions.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/INSTALL.md&#34;&gt;Installation Instructions&lt;/a&gt; for complete installation instructions of MAM.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/SHI-Labs/Matting-Anything/main/GETTING_STARTED.md&#34;&gt;Getting Started&lt;/a&gt; for dataset preparation, training and inference details of MAM.&lt;/p&gt; &#xA;&lt;h2&gt;Third-Party Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/camenduru/Matting-Anything-colab&#34;&gt;Matting-Anything-Colab&lt;/a&gt; (&lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;@camenduru&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/fffiloni/Video-Matting-Anything&#34;&gt;Matting-Anything-Video&lt;/a&gt; (&lt;a href=&#34;https://twitter.com/fffiloni&#34;&gt;@fffiloni&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{li2023matting,&#xA;      title={Matting Anything},&#xA;      author={Jiachen Li and Jitesh Jain and Humphrey Shi},&#xA;      journal={arXiv: 2306.05399}, &#xA;      year={2023}&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank the authors of &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-SAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/yucornetto/MGMatting&#34;&gt;MGMatting&lt;/a&gt;, and &lt;a href=&#34;https://github.com/nowsyn/InstMatt/tree/main&#34;&gt;InstMatt&lt;/a&gt; for releasing the codebases.&lt;/p&gt;</summary>
  </entry>
</feed>