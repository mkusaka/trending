<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-27T01:35:15Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ArcInstitute/state</title>
    <updated>2025-06-27T01:35:15Z</updated>
    <id>tag:github.com,2025-06-27:/ArcInstitute/state</id>
    <link href="https://github.com/ArcInstitute/state" rel="alternate"></link>
    <summary type="html">&lt;p&gt;State is a machine learning model that predicts cellular perturbation response across diverse contexts&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Predicting cellular responses to perturbation across diverse contexts with State&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Train State transition models or pretrain State embedding models. See the State &lt;a href=&#34;https://arcinstitute.org/manuscripts/State&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Associated repositories&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model evaluation framework: &lt;a href=&#34;https://github.com/ArcInstitute/cell-eval&#34;&gt;cell-eval&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dataloaders and preprocessing: &lt;a href=&#34;https://github.com/ArcInstitute/cell-load&#34;&gt;cell-load&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Installation from PyPI&lt;/h3&gt; &#xA;&lt;p&gt;This package is distributed via &lt;a href=&#34;https://docs.astral.sh/uv&#34;&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv tool install arc-state&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation from Source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:ArcInstitute/state.git&#xA;cd state&#xA;uv run state&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When making fundamental changes to State, install an editable version with the &lt;code&gt;-e&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:ArcInstitute/state.git&#xA;cd state&#xA;uv tool install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CLI Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can access the CLI help menu with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;state --help&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: state [-h] {emb,tx} ...&#xA;&#xA;positional arguments:&#xA;  {emb,tx}&#xA;&#xA;options:&#xA;  -h, --help  show this help message and exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;State Transition Model (ST)&lt;/h2&gt; &#xA;&lt;p&gt;To start an experiment, write a TOML file (see &lt;code&gt;examples/zeroshot.toml&lt;/code&gt; or &lt;code&gt;examples/fewshot.toml&lt;/code&gt; to start). The TOML file specifies the dataset paths (containing h5ad files) as well as the machine learning task.&lt;/p&gt; &#xA;&lt;p&gt;Training an ST example below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;state tx train \&#xA;data.kwargs.toml_config_path=&#34;examples/fewshot.toml&#34; \&#xA;data.kwargs.embed_key=X_hvg \&#xA;data.kwargs.num_workers=12 \&#xA;data.kwargs.batch_col=batch_var \&#xA;data.kwargs.pert_col=target_gene \&#xA;data.kwargs.cell_type_key=cell_type \&#xA;data.kwargs.control_pert=TARGET1 \&#xA;training.max_steps=40000 \&#xA;training.val_freq=100 \&#xA;training.ckpt_every_n_steps=100 \&#xA;training.batch_size=8 \&#xA;training.lr=1e-4 \&#xA;model.kwargs.cell_set_len=64 \&#xA;model.kwargs.hidden_dim=328 \&#xA;model=pertsets \&#xA;wandb.tags=&#34;[test]&#34; \&#xA;output_dir=&#34;$HOME/state&#34; \&#xA;name=&#34;test&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The cell lines and perturbations specified in the TOML should match the values appearing in the &lt;code&gt;data.kwargs.cell_type_key&lt;/code&gt; and &lt;code&gt;data.kwargs.pert_col&lt;/code&gt; used above. To evaluate STATE on the specified task, you can use the &lt;code&gt;tx predict&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;state tx predict --output_dir $HOME/state/test/ --checkpoint final.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will look in the &lt;code&gt;output_dir&lt;/code&gt; above, for a &lt;code&gt;checkpoints&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;If you instead want to use a trained checkpoint for inference (e.g. on data not specified) in the TOML file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;state tx infer --output $HOME/state/test/ --output_dir /path/to/model/ --checkpoint /path/to/model/final.ckpt --adata /path/to/anndata/processed.h5 --pert_col gene --embed_key X_hvg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, &lt;code&gt;/path/to/model/&lt;/code&gt; is the folder downloaded from &lt;a href=&#34;https://huggingface.co/arcinstitute&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;TOML Configuration Files&lt;/h2&gt; &#xA;&lt;p&gt;State experiments are configured using TOML files that define datasets, training splits, and evaluation scenarios. The configuration system supports both &lt;strong&gt;zeroshot&lt;/strong&gt; (unseen cell types) and &lt;strong&gt;fewshot&lt;/strong&gt; (limited perturbation examples) evaluation paradigms.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration Structure&lt;/h3&gt; &#xA;&lt;h4&gt;Required Sections&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;[datasets]&lt;/code&gt;&lt;/strong&gt; - Maps dataset names to their file system paths&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[datasets]&#xA;replogle = &#34;/path/to/replogle/dataset/&#34;&#xA;# YOU CAN ADD MORE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;[training]&lt;/code&gt;&lt;/strong&gt; - Specifies which datasets participate in training&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[training]&#xA;replogle = &#34;train&#34;  # Include all replogle data in training (unless overridden below)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Optional Evaluation Sections&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;[zeroshot]&lt;/code&gt;&lt;/strong&gt; - Reserves entire cell types for validation/testing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[zeroshot]&#xA;&#34;replogle.jurkat&#34; = &#34;test&#34;     # All jurkat cells go to test set&#xA;&#34;replogle.k562&#34; = &#34;val&#34;        # All k562 cells go to validation set&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;[fewshot]&lt;/code&gt;&lt;/strong&gt; - Specifies perturbation-level splits within cell types&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[fewshot]&#xA;[fewshot.&#34;replogle.rpe1&#34;]      # Configure splits for rpe1 cell type&#xA;val = [&#34;AARS&#34;, &#34;TUFM&#34;]         # These perturbations go to validation&#xA;test = [&#34;NUP107&#34;, &#34;RPUSD4&#34;]    # These perturbations go to test&#xA;# Note: All other perturbations in rpe1 automatically go to training&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Example 1: Pure Zeroshot Evaluation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Evaluate generalization to completely unseen cell types&#xA;[datasets]&#xA;replogle = &#34;/data/replogle/&#34;&#xA;&#xA;[training]&#xA;replogle = &#34;train&#34;&#xA;&#xA;[zeroshot]&#xA;&#34;replogle.jurkat&#34; = &#34;test&#34;     # Hold out entire jurkat cell line&#xA;&#34;replogle.rpe1&#34; = &#34;val&#34;        # Hold out entire rpe1 cell line&#xA;&#xA;[fewshot]&#xA;# Empty - no perturbation-level splits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example 2: Pure Fewshot Evaluation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Evaluate with limited examples of specific perturbations&#xA;[datasets]&#xA;replogle = &#34;/data/replogle/&#34;&#xA;&#xA;[training]&#xA;replogle = &#34;train&#34;&#xA;&#xA;[zeroshot]&#xA;# Empty - all cell types participate in training&#xA;&#xA;[fewshot]&#xA;[fewshot.&#34;replogle.k562&#34;]&#xA;val = [&#34;AARS&#34;]                 # Limited AARS examples for validation&#xA;test = [&#34;NUP107&#34;, &#34;RPUSD4&#34;]    # Limited examples of these genes for testing&#xA;&#xA;[fewshot.&#34;replogle.jurkat&#34;]&#xA;val = [&#34;TUFM&#34;]&#xA;test = [&#34;MYC&#34;, &#34;TP53&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example 3: Mixed Evaluation Strategy&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# Combine both zeroshot and fewshot evaluation&#xA;[datasets]&#xA;replogle = &#34;/data/replogle/&#34;&#xA;&#xA;[training]&#xA;replogle = &#34;train&#34;&#xA;&#xA;[zeroshot]&#xA;&#34;replogle.jurkat&#34; = &#34;test&#34;        # Zeroshot: unseen cell type&#xA;&#xA;[fewshot]&#xA;[fewshot.&#34;replogle.k562&#34;]      # Fewshot: limited perturbation examples&#xA;val = [&#34;STAT1&#34;]&#xA;test = [&#34;MYC&#34;, &#34;TP53&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Important Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic training assignment&lt;/strong&gt;: Any cell type not mentioned in &lt;code&gt;[zeroshot]&lt;/code&gt; automatically participates in training, with perturbations not listed in &lt;code&gt;[fewshot]&lt;/code&gt; going to the training set&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Overlapping splits&lt;/strong&gt;: Perturbations can appear in both validation and test sets within fewshot configurations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dataset naming&lt;/strong&gt;: Use the format &lt;code&gt;&#34;dataset_name.cell_type&#34;&lt;/code&gt; when specifying cell types in zeroshot and fewshot sections&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Path requirements&lt;/strong&gt;: Dataset paths should point to directories containing h5ad files&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Control perturbations&lt;/strong&gt;: Ensure your control condition (specified via &lt;code&gt;control_pert&lt;/code&gt; parameter) is available across all splits&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Validation&lt;/h3&gt; &#xA;&lt;p&gt;The configuration system will validate that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All referenced datasets exist at the specified paths&lt;/li&gt; &#xA; &lt;li&gt;Cell types mentioned in zeroshot/fewshot sections exist in the datasets&lt;/li&gt; &#xA; &lt;li&gt;Perturbations listed in fewshot sections are present in the corresponding cell types&lt;/li&gt; &#xA; &lt;li&gt;No conflicts exist between zeroshot and fewshot assignments for the same cell type&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;State Embedding Model (SE)&lt;/h2&gt; &#xA;&lt;p&gt;After following the same installation commands above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;state emb fit --conf ${CONFIG}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run inference with a trained State checkpoint, e.g., the State trained to 4 epochs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;state emb transform \&#xA;  --checkpoint &#34;/large_storage/ctc/userspace/aadduri/SE-600M&#34; \&#xA;  --input &#34;/large_storage/ctc/datasets/replogle/rpe1_raw_singlecell_01.h5ad&#34; \&#xA;  --output &#34;/home/aadduri/vci_pretrain/test_output.h5ad&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;State code is &lt;a href=&#34;https://raw.githubusercontent.com/ArcInstitute/state/main/LICENSE&#34;&gt;licensed&lt;/a&gt; under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC BY-NC-SA 4.0).&lt;/p&gt; &#xA;&lt;p&gt;The model weights and output are licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/ArcInstitute/state/main/MODEL_LICENSE.md&#34;&gt;Arc Research Institute State Model Non-Commercial License&lt;/a&gt; and subject to the &lt;a href=&#34;https://raw.githubusercontent.com/ArcInstitute/state/main/MODEL_ACCEPTABLE_USE_POLICY.md&#34;&gt;Arc Research Institute State Model Acceptable Use Policy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Any publication that uses this source code or model parameters should cite the State &lt;a href=&#34;https://arcinstitute.org/manuscripts/State&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LearningCircuit/local-deep-research</title>
    <updated>2025-06-27T01:35:15Z</updated>
    <id>tag:github.com,2025-06-27:/LearningCircuit/local-deep-research</id>
    <link href="https://github.com/LearningCircuit/local-deep-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Local Deep Research is an AI-powered assistant that transforms complex questions into comprehensive, cited reports by conducting iterative analysis using any LLM across diverse knowledge sources including academic databases, scientific repositories, web content, and private document collections.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Local Deep Research&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/LearningCircuit/local-deep-research/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/LearningCircuit/local-deep-research?style=for-the-badge&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/localdeepresearch/local-deep-research&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/localdeepresearch/local-deep-research?style=for-the-badge&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/local-deep-research/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/local-deep-research?style=for-the-badge&#34; alt=&#34;PyPI Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/LearningCircuit/local-deep-research/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/LearningCircuit/local-deep-research/tests.yml?branch=main&amp;amp;style=for-the-badge&amp;amp;label=Tests&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LearningCircuit/local-deep-research/security/code-scanning&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/LearningCircuit/local-deep-research/codeql.yml?branch=main&amp;amp;style=for-the-badge&amp;amp;label=CodeQL&#34; alt=&#34;CodeQL&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/ttcqQeFcJ3&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1352043059562680370?style=for-the-badge&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.reddit.com/r/LocalDeepResearch/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Reddit-r/LocalDeepResearch-FF4500?style=for-the-badge&amp;amp;logo=reddit&#34; alt=&#34;Reddit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;AI-powered research assistant for deep, iterative research&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Performs deep, iterative research using multiple LLMs and search engines with proper citations&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🚀 What is Local Deep Research?&lt;/h2&gt; &#xA;&lt;p&gt;LDR is an AI research assistant that performs systematic research by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Breaking down complex questions&lt;/strong&gt; into focused sub-queries&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Searching multiple sources&lt;/strong&gt; in parallel (web, academic papers, local documents)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Verifying information&lt;/strong&gt; across sources for accuracy&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Creating comprehensive reports&lt;/strong&gt; with proper citations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It aims to help researchers, students, and professionals find accurate information quickly while maintaining transparency about sources.&lt;/p&gt; &#xA;&lt;h2&gt;🎯 Why Choose LDR?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Privacy-Focused&lt;/strong&gt;: Run entirely locally with Ollama + SearXNG&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Use any LLM, any search engine, any vector store&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: Multiple research modes from quick summaries to detailed reports&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transparent&lt;/strong&gt;: Track costs and performance with built-in analytics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: MIT licensed with an active community&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📊 Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;~95% accuracy on SimpleQA benchmark&lt;/strong&gt; (preliminary results)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tested with GPT-4.1-mini + SearXNG + focused-iteration strategy&lt;/li&gt; &#xA; &lt;li&gt;Comparable to state-of-the-art AI research systems&lt;/li&gt; &#xA; &lt;li&gt;Local models can achieve similar performance with proper configuration&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LearningCircuit/local-deep-research/tree/main/community_benchmark_results&#34;&gt;Join our community benchmarking effort →&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✨ Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;🔍 Research Modes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quick Summary&lt;/strong&gt; - Get answers in 30 seconds to 3 minutes with citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Detailed Research&lt;/strong&gt; - Comprehensive analysis with structured findings&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Report Generation&lt;/strong&gt; - Professional reports with sections and table of contents&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document Analysis&lt;/strong&gt; - Search your private documents with AI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🛠️ Advanced Capabilities&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/LANGCHAIN_RETRIEVER_INTEGRATION.md&#34;&gt;LangChain Integration&lt;/a&gt;&lt;/strong&gt; - Use any vector store as a search engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/api-quickstart.md&#34;&gt;REST API&lt;/a&gt;&lt;/strong&gt; - Language-agnostic HTTP access&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/BENCHMARKING.md&#34;&gt;Benchmarking&lt;/a&gt;&lt;/strong&gt; - Test and optimize your configuration&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/analytics-dashboard.md&#34;&gt;Analytics Dashboard&lt;/a&gt;&lt;/strong&gt; - Track costs, performance, and usage metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time Updates&lt;/strong&gt; - WebSocket support for live research progress&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Export Options&lt;/strong&gt; - Download results as PDF or Markdown&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research History&lt;/strong&gt; - Save, search, and revisit past research&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adaptive Rate Limiting&lt;/strong&gt; - Intelligent retry system that learns optimal wait times&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Keyboard Shortcuts&lt;/strong&gt; - Navigate efficiently (ESC, Ctrl+Shift+1-5)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🌐 Search Sources&lt;/h3&gt; &#xA;&lt;h4&gt;Free Search Engines&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Academic&lt;/strong&gt;: arXiv, PubMed, Semantic Scholar&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Wikipedia, SearXNG, DuckDuckGo&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Technical&lt;/strong&gt;: GitHub, Elasticsearch&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Historical&lt;/strong&gt;: Wayback Machine&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;News&lt;/strong&gt;: The Guardian&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Premium Search Engines&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tavily&lt;/strong&gt; - AI-powered search&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt; - Via SerpAPI or Programmable Search Engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brave Search&lt;/strong&gt; - Privacy-focused web search&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Custom Sources&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local Documents&lt;/strong&gt; - Search your files with AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LangChain Retrievers&lt;/strong&gt; - Any vector store or database&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Meta Search&lt;/strong&gt; - Combine multiple engines intelligently&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/search-engines.md&#34;&gt;Full Search Engines Guide →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Option 1: Docker (Quickstart no MAC/ARM)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Step 1: Pull and run SearXNG for optimal search results&#xA;docker run -d -p 8080:8080 --name searxng searxng/searxng&#xA;&#xA;# Step 2: Pull and run Local Deep Research (Please build your own docker on ARM)&#xA;docker run -d -p 5000:5000 --name local-deep-research --volume &#39;deep-research:/install/.venv/lib/python3.13/site-packages/data/&#39; localdeepresearch/local-deep-research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Option 2: Docker Compose (Recommended)&lt;/h3&gt; &#xA;&lt;p&gt;LDR uses Docker compose to bundle the web app and all it&#39;s dependencies so you can get up and running quickly.&lt;/p&gt; &#xA;&lt;h4&gt;Option 2a: Quick Start (One Command)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -O https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docker-compose.yml &amp;amp;&amp;amp; docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &lt;a href=&#34;http://localhost:5000&#34;&gt;http://localhost:5000&lt;/a&gt; after ~30 seconds. This starts LDR with SearXNG and all dependencies.&lt;/p&gt; &#xA;&lt;h4&gt;Option 2b: DIY docker-compose&lt;/h4&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; for a docker-compose file with reasonable defaults to get up and running with ollama, searxng, and local deep research all running locally.&lt;/p&gt; &#xA;&lt;p&gt;Things you may want/need to configure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ollama GPU driver&lt;/li&gt; &#xA; &lt;li&gt;Ollama context length (depends on available VRAM)&lt;/li&gt; &#xA; &lt;li&gt;Ollama keep alive (duration model will stay loaded into VRAM and idle before getting unloaded automatically)&lt;/li&gt; &#xA; &lt;li&gt;Deep Research model (depends on available VRAM and preference)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Option 2c: Use Cookie Cutter to tailor a docker-compose to your needs:&lt;/h4&gt; &#xA;&lt;h5&gt;Prerequisites&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cookiecutter&lt;/code&gt;: Run &lt;code&gt;pip install --user cookiecutter&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LearningCircuit/local-deep-research.git&#xA;cd local-deep-research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuring with Docker Compose&lt;/h3&gt; &#xA;&lt;p&gt;Cookiecutter will interactively guide you through the process of creating a &lt;code&gt;docker-compose&lt;/code&gt; configuration that meets your specific needs. This is the recommended approach if you are not very familiar with Docker.&lt;/p&gt; &#xA;&lt;p&gt;In the LDR repository, run the following command to generate the compose file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cookiecutter cookiecutter-docker/&#xA;docker compose -f docker-compose.default.yml up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/docker-compose-guide.md&#34;&gt;Docker Compose Guide →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Option 3: Python Package&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Step 1: Install the package&#xA;pip install local-deep-research&#xA;&#xA;# Step 2: Setup SearXNG for best results&#xA;docker pull searxng/searxng&#xA;docker run -d -p 8080:8080 --name searxng searxng/searxng&#xA;&#xA;# Step 3: Install Ollama from https://ollama.ai&#xA;&#xA;# Step 4: Download a model&#xA;ollama pull gemma3:12b&#xA;&#xA;# Step 5: Start the web interface&#xA;python -m local_deep_research.web.app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LearningCircuit/local-deep-research/wiki/Installation&#34;&gt;Full Installation Guide →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💻 Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from local_deep_research.api import quick_summary&#xA;&#xA;# Simple usage&#xA;result = quick_summary(&#34;What are the latest advances in quantum computing?&#34;)&#xA;print(result[&#34;summary&#34;])&#xA;&#xA;# Advanced usage with custom configuration&#xA;result = quick_summary(&#xA;    query=&#34;Impact of AI on healthcare&#34;,&#xA;    search_tool=&#34;searxng&#34;,&#xA;    search_strategy=&#34;focused-iteration&#34;,&#xA;    iterations=2&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;HTTP API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST http://localhost:5000/api/v1/quick_summary \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#34;query&#34;: &#34;Explain CRISPR gene editing&#34;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/examples/api_usage/&#34;&gt;More Examples →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Command Line Tools&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run benchmarks from CLI&#xA;python -m local_deep_research.benchmarks --dataset simpleqa --examples 50&#xA;&#xA;# Manage rate limiting&#xA;python -m local_deep_research.web_search_engines.rate_limiting status&#xA;python -m local_deep_research.web_search_engines.rate_limiting reset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔗 Enterprise Integration&lt;/h2&gt; &#xA;&lt;p&gt;Connect LDR to your existing knowledge base:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from local_deep_research.api import quick_summary&#xA;&#xA;# Use your existing LangChain retriever&#xA;result = quick_summary(&#xA;    query=&#34;What are our deployment procedures?&#34;,&#xA;    retrievers={&#34;company_kb&#34;: your_retriever},&#xA;    search_tool=&#34;company_kb&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Works with: FAISS, Chroma, Pinecone, Weaviate, Elasticsearch, and any LangChain-compatible retriever.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/LANGCHAIN_RETRIEVER_INTEGRATION.md&#34;&gt;Integration Guide →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📊 Performance &amp;amp; Analytics&lt;/h2&gt; &#xA;&lt;h3&gt;Benchmark Results&lt;/h3&gt; &#xA;&lt;p&gt;Early experiments on small SimpleQA dataset samples:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Configuration&lt;/th&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-4.1-mini + SearXNG + focused_iteration&lt;/td&gt; &#xA;   &lt;td&gt;90-95%&lt;/td&gt; &#xA;   &lt;td&gt;Limited sample size&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-4.1-mini + Tavily + focused_iteration&lt;/td&gt; &#xA;   &lt;td&gt;90-95%&lt;/td&gt; &#xA;   &lt;td&gt;Limited sample size&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemini-2.0-flash-001 + SearXNG&lt;/td&gt; &#xA;   &lt;td&gt;82%&lt;/td&gt; &#xA;   &lt;td&gt;Single test run&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: These are preliminary results from initial testing. Performance varies significantly based on query types, model versions, and configurations. &lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/BENCHMARKING.md&#34;&gt;Run your own benchmarks →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Built-in Analytics Dashboard&lt;/h3&gt; &#xA;&lt;p&gt;Track costs, performance, and usage with detailed metrics. &lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/analytics-dashboard.md&#34;&gt;Learn more →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🤖 Supported LLMs&lt;/h2&gt; &#xA;&lt;h3&gt;Local Models (via Ollama)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama 3, Mistral, Gemma, DeepSeek&lt;/li&gt; &#xA; &lt;li&gt;LLM processing stays local (search queries still go to web)&lt;/li&gt; &#xA; &lt;li&gt;No API costs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cloud Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI (GPT-4, GPT-3.5)&lt;/li&gt; &#xA; &lt;li&gt;Anthropic (Claude 3)&lt;/li&gt; &#xA; &lt;li&gt;Google (Gemini)&lt;/li&gt; &#xA; &lt;li&gt;100+ models via OpenRouter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/env_configuration.md&#34;&gt;Model Setup →&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📚 Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LearningCircuit/local-deep-research/wiki/Installation&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/faq.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/api-quickstart.md&#34;&gt;API Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/env_configuration.md&#34;&gt;Configuration Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Core Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/features.md&#34;&gt;All Features Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/search-engines.md&#34;&gt;Search Engines Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/analytics-dashboard.md&#34;&gt;Analytics Dashboard&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/LANGCHAIN_RETRIEVER_INTEGRATION.md&#34;&gt;LangChain Integration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/BENCHMARKING.md&#34;&gt;Benchmarking System&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/elasticsearch_search_engine.md&#34;&gt;Elasticsearch Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/SearXNG-Setup.md&#34;&gt;SearXNG Setup&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/docker-compose-guide.md&#34;&gt;Docker Compose Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/developing.md&#34;&gt;Development Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/security/CODEQL_GUIDE.md&#34;&gt;Security Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/docs/RELEASE_GUIDE.md&#34;&gt;Release Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples &amp;amp; Tutorials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/examples/api_usage/&#34;&gt;API Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/examples/benchmarks/&#34;&gt;Benchmark Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/examples/optimization/&#34;&gt;Optimization Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🤝 Community &amp;amp; Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/ttcqQeFcJ3&#34;&gt;Discord&lt;/a&gt; - Get help and share research techniques&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalDeepResearch/&#34;&gt;Reddit&lt;/a&gt; - Updates and showcases&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LearningCircuit/local-deep-research/issues&#34;&gt;GitHub Issues&lt;/a&gt; - Bug reports&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! See our &lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h2&gt;📄 License&lt;/h2&gt; &#xA;&lt;p&gt;MIT License - see &lt;a href=&#34;https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Built with: &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://ollama.ai&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://searxng.org/&#34;&gt;SearXNG&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Support Free Knowledge:&lt;/strong&gt; Consider donating to &lt;a href=&#34;https://donate.wikimedia.org&#34;&gt;Wikipedia&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/about/give&#34;&gt;arXiv&lt;/a&gt;, or &lt;a href=&#34;https://www.nlm.nih.gov/pubs/donations/donations.html&#34;&gt;PubMed&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>mindee/doctr</title>
    <updated>2025-06-27T01:35:15Z</updated>
    <id>tag:github.com,2025-06-27:/mindee/doctr</id>
    <link href="https://github.com/mindee/doctr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;docTR (Document Text Recognition) - a seamless, high-performing &amp; accessible library for OCR-related tasks powered by Deep Learning.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/mindee/doctr/raw/main/docs/images/Logo_doctr.gif&#34; width=&#34;40%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://slack.mindee.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-Community-4A154B?style=flat-square&amp;amp;logo=slack&amp;amp;logoColor=white&#34; alt=&#34;Slack Icon&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/mindee/doctr/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/mindee/doctr/workflows/builds/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt; &lt;a href=&#34;https://github.com/mindee/doctr/pkgs/container/doctr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Docker-4287f5?style=flat&amp;amp;logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Docker Images&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/mindee/doctr&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/mindee/doctr/branch/main/graph/badge.svg?token=577MO567NM&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codefactor.io/repository/github/mindee/doctr&#34;&gt;&lt;img src=&#34;https://www.codefactor.io/repository/github/mindee/doctr/badge?s=bae07db86bb079ce9d6542315b8c6e70fa708a7e&#34; alt=&#34;CodeFactor&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codacy.com/gh/mindee/doctr?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=mindee/doctr&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://api.codacy.com/project/badge/Grade/340a76749b634586a498e1c0ab998f08&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mindee.github.io/doctr&#34;&gt;&lt;img src=&#34;https://github.com/mindee/doctr/workflows/doc-status/badge.svg?sanitize=true&#34; alt=&#34;Doc Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/python-doctr/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pypi-v0.12.0-blue.svg?sanitize=true&#34; alt=&#34;Pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/mindee/doctr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mindee/notebooks/blob/main/doctr/quicktour.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gurubase.io/g/doctr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gurubase-Ask%20docTR%20Guru-006BFF&#34; alt=&#34;Gurubase&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optical Character Recognition made seamless &amp;amp; accessible to anyone, powered by TensorFlow 2 &amp;amp; PyTorch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;What you can expect from this repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;efficient ways to parse textual information (localize and identify each word) from your documents&lt;/li&gt; &#xA; &lt;li&gt;guidance on how to integrate this in your current architecture&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mindee/doctr/raw/main/docs/images/ocr.png&#34; alt=&#34;OCR_example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Tour&lt;/h2&gt; &#xA;&lt;h3&gt;Getting your pretrained model&lt;/h3&gt; &#xA;&lt;p&gt;End-to-End OCR is achieved in docTR using a two-stage approach: text detection (localizing words), then text recognition (identify all characters in the word). As such, you can select the architecture used for &lt;a href=&#34;https://mindee.github.io/doctr/latest/modules/models.html#doctr-models-detection&#34;&gt;text detection&lt;/a&gt;, and the one for &lt;a href=&#34;https://mindee.github.io/doctr/latest//modules/models.html#doctr-models-recognition&#34;&gt;text recognition&lt;/a&gt; from the list of available implementations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from doctr.models import ocr_predictor&#xA;&#xA;model = ocr_predictor(det_arch=&#39;db_resnet50&#39;, reco_arch=&#39;crnn_vgg16_bn&#39;, pretrained=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reading files&lt;/h3&gt; &#xA;&lt;p&gt;Documents can be interpreted from PDF or images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from doctr.io import DocumentFile&#xA;# PDF&#xA;pdf_doc = DocumentFile.from_pdf(&#34;path/to/your/doc.pdf&#34;)&#xA;# Image&#xA;single_img_doc = DocumentFile.from_images(&#34;path/to/your/img.jpg&#34;)&#xA;# Webpage (requires `weasyprint` to be installed)&#xA;webpage_doc = DocumentFile.from_url(&#34;https://www.yoursite.com&#34;)&#xA;# Multiple page images&#xA;multi_img_doc = DocumentFile.from_images([&#34;path/to/page1.jpg&#34;, &#34;path/to/page2.jpg&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Putting it together&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s use the default pretrained model for an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from doctr.io import DocumentFile&#xA;from doctr.models import ocr_predictor&#xA;&#xA;model = ocr_predictor(pretrained=True)&#xA;# PDF&#xA;doc = DocumentFile.from_pdf(&#34;path/to/your/doc.pdf&#34;)&#xA;# Analyze&#xA;result = model(doc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dealing with rotated documents&lt;/h3&gt; &#xA;&lt;p&gt;Should you use docTR on documents that include rotated pages, or pages with multiple box orientations, you have multiple options to handle it:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you only use straight document pages with straight words (horizontal, same reading direction), consider passing &lt;code&gt;assume_straight_boxes=True&lt;/code&gt; to the ocr_predictor. It will directly fit straight boxes on your page and return straight boxes, which makes it the fastest option.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you want the predictor to output straight boxes (no matter the orientation of your pages, the final localizations will be converted to straight boxes), you need to pass &lt;code&gt;export_as_straight_boxes=True&lt;/code&gt; in the predictor. Otherwise, if &lt;code&gt;assume_straight_pages=False&lt;/code&gt;, it will return rotated bounding boxes (potentially with an angle of 0°).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If both options are set to False, the predictor will always fit and return rotated boxes.&lt;/p&gt; &#xA;&lt;p&gt;To interpret your model&#39;s predictions, you can visualize them interactively as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Display the result (requires matplotlib &amp;amp; mplcursors to be installed)&#xA;result.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mindee/doctr/raw/main/docs/images/doctr_example_script.gif&#34; alt=&#34;Visualization sample&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or even rebuild the original document from its predictions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt&#xA;&#xA;synthetic_pages = result.synthesize()&#xA;plt.imshow(synthetic_pages[0]); plt.axis(&#39;off&#39;); plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mindee/doctr/raw/main/docs/images/synthesized_sample.png&#34; alt=&#34;Synthesis sample&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;ocr_predictor&lt;/code&gt; returns a &lt;code&gt;Document&lt;/code&gt; object with a nested structure (with &lt;code&gt;Page&lt;/code&gt;, &lt;code&gt;Block&lt;/code&gt;, &lt;code&gt;Line&lt;/code&gt;, &lt;code&gt;Word&lt;/code&gt;, &lt;code&gt;Artefact&lt;/code&gt;). To get a better understanding of our document model, check our &lt;a href=&#34;https://mindee.github.io/doctr/modules/io.html#document-structure&#34;&gt;documentation&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;You can also export them as a nested dict, more appropriate for JSON format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;json_output = result.export()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use the KIE predictor&lt;/h3&gt; &#xA;&lt;p&gt;The KIE predictor is a more flexible predictor compared to OCR as your detection model can detect multiple classes in a document. For example, you can have a detection model to detect just dates and addresses in a document.&lt;/p&gt; &#xA;&lt;p&gt;The KIE predictor makes it possible to use detector with multiple classes with a recognition model and to have the whole pipeline already setup for you.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from doctr.io import DocumentFile&#xA;from doctr.models import kie_predictor&#xA;&#xA;# Model&#xA;model = kie_predictor(det_arch=&#39;db_resnet50&#39;, reco_arch=&#39;crnn_vgg16_bn&#39;, pretrained=True)&#xA;# PDF&#xA;doc = DocumentFile.from_pdf(&#34;path/to/your/doc.pdf&#34;)&#xA;# Analyze&#xA;result = model(doc)&#xA;&#xA;predictions = result.pages[0].predictions&#xA;for class_name in predictions.keys():&#xA;    list_predictions = predictions[class_name]&#xA;    for prediction in list_predictions:&#xA;        print(f&#34;Prediction for {class_name}: {prediction}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The KIE predictor results per page are in a dictionary format with each key representing a class name and it&#39;s value are the predictions for that class.&lt;/p&gt; &#xA;&lt;h3&gt;If you are looking for support from the Mindee team&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mindee.com/product/doctr&#34;&gt;&lt;img src=&#34;https://github.com/mindee/doctr/raw/main/docs/images/doctr-need-help.png&#34; alt=&#34;Bad OCR test detection image asking the developer if they need help&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] &lt;strong&gt;TensorFlow Backend Deprecation Notice&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Using docTR with TensorFlow as a backend is deprecated and will be removed in the next major release (v1.0.0). We &lt;strong&gt;recommend switching to the PyTorch backend&lt;/strong&gt;, which is more actively maintained and supports the latest features and models. Alternatively, you can use &lt;a href=&#34;https://github.com/felixdittrich92/OnnxTR&#34;&gt;OnnxTR&lt;/a&gt;, which does &lt;strong&gt;not&lt;/strong&gt; require TensorFlow or PyTorch.&lt;/p&gt; &#xA; &lt;p&gt;This decision was made based on several considerations:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Allows better focus on improving the core library&lt;/li&gt; &#xA;  &lt;li&gt;Frees up resources to develop new features faster&lt;/li&gt; &#xA;  &lt;li&gt;Enables more targeted optimizations with PyTorch&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Python 3.10 (or higher) and &lt;a href=&#34;https://pip.pypa.io/en/stable/&#34;&gt;pip&lt;/a&gt; are required to install docTR.&lt;/p&gt; &#xA;&lt;h3&gt;Latest release&lt;/h3&gt; &#xA;&lt;p&gt;You can then install the latest release of the package using &lt;a href=&#34;https://pypi.org/project/python-doctr/&#34;&gt;pypi&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install python-doctr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Please note that the basic installation is not standalone, as it does not provide a deep learning framework, which is required for the package to run.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We try to keep framework-specific dependencies to a minimum. You can install framework-specific builds as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# for TensorFlow&#xA;pip install &#34;python-doctr[tf]&#34;&#xA;# for PyTorch&#xA;pip install &#34;python-doctr[torch]&#34;&#xA;# optional dependencies for visualization, html, and contrib modules can be installed as follows:&#xA;pip install &#34;python-doctr[torch,viz,html,contib]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For MacBooks with M1 chip, you will need some additional packages or specific versions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TensorFlow 2: &lt;a href=&#34;https://developer.apple.com/metal/tensorflow-plugin/&#34;&gt;metal plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://pytorch.org/get-started/locally/#start-locally&#34;&gt;version &amp;gt;= 2.0.0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Developer mode&lt;/h3&gt; &#xA;&lt;p&gt;Alternatively, you can install it from source, which will require you to install &lt;a href=&#34;https://git-scm.com/book/en/v2/Getting-Started-Installing-Git&#34;&gt;Git&lt;/a&gt;. First clone the project repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mindee/doctr.git&#xA;pip install -e doctr/.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, if you prefer to avoid the risk of missing dependencies, you can install the TensorFlow or the PyTorch build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# for TensorFlow&#xA;pip install -e doctr/.[tf]&#xA;# for PyTorch&#xA;pip install -e doctr/.[torch]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models architectures&lt;/h2&gt; &#xA;&lt;p&gt;Credits where it&#39;s due: this repository is implementing, among others, architectures from published research papers.&lt;/p&gt; &#xA;&lt;h3&gt;Text Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DBNet: &lt;a href=&#34;https://arxiv.org/pdf/1911.08947.pdf&#34;&gt;Real-time Scene Text Detection with Differentiable Binarization&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;LinkNet: &lt;a href=&#34;https://arxiv.org/pdf/1707.03718.pdf&#34;&gt;LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FAST: &lt;a href=&#34;https://arxiv.org/pdf/2111.02394.pdf&#34;&gt;FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Text Recognition&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CRNN: &lt;a href=&#34;https://arxiv.org/pdf/1507.05717.pdf&#34;&gt;An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;SAR: &lt;a href=&#34;https://arxiv.org/pdf/1811.00751.pdf&#34;&gt;Show, Attend and Read:A Simple and Strong Baseline for Irregular Text Recognition&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MASTER: &lt;a href=&#34;https://arxiv.org/pdf/1910.02562.pdf&#34;&gt;MASTER: Multi-Aspect Non-local Network for Scene Text Recognition&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ViTSTR: &lt;a href=&#34;https://arxiv.org/pdf/2105.08582.pdf&#34;&gt;Vision Transformer for Fast and Efficient Scene Text Recognition&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PARSeq: &lt;a href=&#34;https://arxiv.org/pdf/2207.06966&#34;&gt;Scene Text Recognition with Permuted Autoregressive Sequence Models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;VIPTR: &lt;a href=&#34;https://arxiv.org/abs/2401.10110&#34;&gt;A Vision Permutable Extractor for Fast and Efficient Scene Text Recognition&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More goodies&lt;/h2&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;The full package documentation is available &lt;a href=&#34;https://mindee.github.io/doctr/&#34;&gt;here&lt;/a&gt; for detailed specifications.&lt;/p&gt; &#xA;&lt;h3&gt;Demo app&lt;/h3&gt; &#xA;&lt;p&gt;A minimal demo app is provided for you to play with our end-to-end OCR models!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mindee/doctr/raw/main/docs/images/demo_update.png&#34; alt=&#34;Demo app&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Live demo&lt;/h4&gt; &#xA;&lt;p&gt;Courtesy of &lt;span&gt;🤗&lt;/span&gt; &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; &lt;span&gt;🤗&lt;/span&gt;, docTR has now a fully deployed version available on &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Spaces&lt;/a&gt;! Check it out &lt;a href=&#34;https://huggingface.co/spaces/mindee/doctr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Running it locally&lt;/h4&gt; &#xA;&lt;p&gt;If you prefer to use it locally, there is an extra dependency (&lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt;) that is required.&lt;/p&gt; &#xA;&lt;h5&gt;Tensorflow version&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r demo/tf-requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run your app in your default browser with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;USE_TF=1 streamlit run demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;PyTorch version&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r demo/pt-requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run your app in your default browser with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;USE_TORCH=1 streamlit run demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;TensorFlow.js&lt;/h4&gt; &#xA;&lt;p&gt;Instead of having your demo actually running Python, you would prefer to run everything in your web browser? Check out our &lt;a href=&#34;https://github.com/mindee/doctr-tfjs-demo&#34;&gt;TensorFlow.js demo&lt;/a&gt; to get started!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mindee/doctr/raw/main/docs/images/demo_illustration_mini.png&#34; alt=&#34;TFJS demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker container&lt;/h3&gt; &#xA;&lt;p&gt;We offer Docker container support for easy testing and deployment. &lt;a href=&#34;https://github.com/mindee/doctr/pkgs/container/doctr&#34;&gt;Here are the available docker tags.&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Using GPU with docTR Docker Images&lt;/h4&gt; &#xA;&lt;p&gt;The docTR Docker images are GPU-ready and based on CUDA &lt;code&gt;12.2&lt;/code&gt;. Make sure your host is &lt;strong&gt;at least &lt;code&gt;12.2&lt;/code&gt;&lt;/strong&gt;, otherwise Torch or TensorFlow won&#39;t be able to initialize the GPU. Please ensure that Docker is configured to use your GPU.&lt;/p&gt; &#xA;&lt;p&gt;To verify and configure GPU support for Docker, please follow the instructions provided in the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;NVIDIA Container Toolkit Installation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once Docker is configured to use GPUs, you can run docTR Docker containers with GPU support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -it --gpus all ghcr.io/mindee/doctr:torch-py3.9.18-2024-10 bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Available Tags&lt;/h4&gt; &#xA;&lt;p&gt;The Docker images for docTR follow a specific tag nomenclature: &lt;code&gt;&amp;lt;deps&amp;gt;-py&amp;lt;python_version&amp;gt;-&amp;lt;doctr_version|YYYY-MM&amp;gt;&lt;/code&gt;. Here&#39;s a breakdown of the tag structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;deps&amp;gt;&lt;/code&gt;: &lt;code&gt;tf&lt;/code&gt;, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;tf-viz-html-contrib&lt;/code&gt; or &lt;code&gt;torch-viz-html-contrib&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;python_version&amp;gt;&lt;/code&gt;: &lt;code&gt;3.9.18&lt;/code&gt;, &lt;code&gt;3.10.13&lt;/code&gt; or &lt;code&gt;3.11.8&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;doctr_version&amp;gt;&lt;/code&gt;: a tag &amp;gt;= &lt;code&gt;v0.11.0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;YYYY-MM&amp;gt;&lt;/code&gt;: e.g. &lt;code&gt;2014-10&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here are examples of different image tags:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tag&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;tf-py3.10.13-v0.11.0&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TensorFlow version &lt;code&gt;3.10.13&lt;/code&gt; with docTR &lt;code&gt;v0.11.0&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch-viz-html-contrib-py3.11.8-2024-10&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Torch with extra dependencies version &lt;code&gt;3.11.8&lt;/code&gt; from latest commit on &lt;code&gt;main&lt;/code&gt; in &lt;code&gt;2024-10&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch-py3.11.8-2024-10&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch version &lt;code&gt;3.11.8&lt;/code&gt; from latest commit on &lt;code&gt;main&lt;/code&gt; in &lt;code&gt;2024-10&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Building Docker Images Locally&lt;/h4&gt; &#xA;&lt;p&gt;You can also build docTR Docker images locally on your computer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build -t doctr .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify custom Python versions and docTR versions using build arguments. For example, to build a docTR image with TensorFlow, Python version &lt;code&gt;3.9.10&lt;/code&gt;, and docTR version &lt;code&gt;v0.7.0&lt;/code&gt;, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build -t doctr --build-arg FRAMEWORK=tf --build-arg PYTHON_VERSION=3.9.10 --build-arg DOCTR_VERSION=v0.7.0 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example script&lt;/h3&gt; &#xA;&lt;p&gt;An example script is provided for a simple documentation analysis of a PDF or image file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python scripts/analyze.py path/to/your/doc.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All script arguments can be checked using &lt;code&gt;python scripts/analyze.py --help&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Minimal API integration&lt;/h3&gt; &#xA;&lt;p&gt;Looking to integrate docTR into your API? Here is a template to get you started with a fully working API using the wonderful &lt;a href=&#34;https://github.com/tiangolo/fastapi&#34;&gt;FastAPI&lt;/a&gt; framework.&lt;/p&gt; &#xA;&lt;h4&gt;Deploy your API locally&lt;/h4&gt; &#xA;&lt;p&gt;Specific dependencies are required to run the API template, which you can install as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd api/&#xA;pip install poetry&#xA;make lock&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now run your API locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;uvicorn --reload --workers 1 --host 0.0.0.0 --port=8002 --app-dir api/ app.main:app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can run the same server on a docker container if you prefer using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PORT=8002 docker-compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;What you have deployed&lt;/h4&gt; &#xA;&lt;p&gt;Your API should now be running locally on your port 8002. Access your automatically-built documentation at &lt;a href=&#34;http://localhost:8002/redoc&#34;&gt;http://localhost:8002/redoc&lt;/a&gt; and enjoy your three functional routes (&#34;/detection&#34;, &#34;/recognition&#34;, &#34;/ocr&#34;, &#34;/kie&#34;). Here is an example with Python to send a request to the OCR route:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests&#xA;&#xA;params = {&#34;det_arch&#34;: &#34;db_resnet50&#34;, &#34;reco_arch&#34;: &#34;crnn_vgg16_bn&#34;}&#xA;&#xA;with open(&#39;/path/to/your/doc.jpg&#39;, &#39;rb&#39;) as f:&#xA;    files = [  # application/pdf, image/jpeg, image/png supported&#xA;        (&#34;files&#34;, (&#34;doc.jpg&#34;, f.read(), &#34;image/jpeg&#34;)),&#xA;    ]&#xA;print(requests.post(&#34;http://localhost:8080/ocr&#34;, params=params, files=files).json())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example notebooks&lt;/h3&gt; &#xA;&lt;p&gt;Looking for more illustrations of docTR features? You might want to check the &lt;a href=&#34;https://github.com/mindee/doctr/tree/main/notebooks&#34;&gt;Jupyter notebooks&lt;/a&gt; designed to give you a broader overview.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to cite this project, feel free to use this &lt;a href=&#34;http://www.bibtex.org/&#34;&gt;BibTeX&lt;/a&gt; reference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{doctr2021,&#xA;    title={docTR: Document Text Recognition},&#xA;    author={Mindee},&#xA;    year={2021},&#xA;    publisher = {GitHub},&#xA;    howpublished = {\url{https://github.com/mindee/doctr}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you scrolled down to this section, you most likely appreciate open source. Do you feel like extending the range of our supported characters? Or perhaps submitting a paper implementation? Or contributing in any other way?&lt;/p&gt; &#xA;&lt;p&gt;You&#39;re in luck, we compiled a short guide (cf. &lt;a href=&#34;https://mindee.github.io/doctr/contributing/contributing.html&#34;&gt;&lt;code&gt;CONTRIBUTING&lt;/code&gt;&lt;/a&gt;) for you to easily do so!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the Apache 2.0 License. See &lt;a href=&#34;https://github.com/mindee/doctr?tab=Apache-2.0-1-ov-file#readme&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
</feed>