<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-10T01:37:08Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>coleam00/ai-agents-masterclass</title>
    <updated>2025-07-10T01:37:08Z</updated>
    <id>tag:github.com,2025-07-10:/coleam00/ai-agents-masterclass</id>
    <link href="https://github.com/coleam00/ai-agents-masterclass" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Follow along with my AI Agents Masterclass videos! All of the code I create and use in this series on YouTube will be here for you to use and even build on top of!&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/channel/UCMwVTLZIRRUyyVrkjDpn4pA&#34;&gt; &lt;img alt=&#34;AI Agents Masterclass&#34; src=&#34;https://i.imgur.com/8Gr2pBA.png&#34;&gt; &lt;h1 align=&#34;center&#34;&gt;AI Agents Masterclass&lt;/h1&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Artificial Intelligence is the #1 thing for all developers to spend their time on now. The problem is, most developers aren&#39;t focusing on AI agents, which is the real way to unleash the full power of AI. This is why I&#39;m creating this AI Agents Masterclass - so I can show YOU how to use AI agents to transform businesses and create incredibly powerful software like I&#39;ve already done many times! Click the image or link above to go to the masterclass on YouTube. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; style=&#34;margin-top: 25px&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/ai-agents-masterclass/main/#what-are-ai-agents&#34;&gt;&lt;strong&gt;What are AI Agents?&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/ai-agents-masterclass/main/#how-this-repo-works&#34;&gt;&lt;strong&gt;How this Repo Works&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/ai-agents-masterclass/main/#instructions-to-follow-along&#34;&gt;&lt;strong&gt;Instructions to Follow Along&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;What are AI Agents?&lt;/h2&gt; &#xA;&lt;p&gt;AI agents are simply Large Language Models that have been given the ability to interact with the outside world. They can do things like draft emails, book appointments in your CRM, create tasks in your task management software, and really anything you can dream of! I hope that everything I show here can really help you dream big and create incredible things with AI!&lt;/p&gt; &#xA;&lt;p&gt;AI agents can be very powerful without having to create a lot of code. That doesn&#39;t mean there isn&#39;t room though to create more complex applications to tie together many different agents to accomplish truly incredible things! That&#39;s where we&#39;ll be heading with this masterclass and I really look forward to it!&lt;/p&gt; &#xA;&lt;p&gt;Below is a very basic diagram just to get an idea of what an AI agent looks like:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top: 25px;margin-bottom:25px&#34;&gt; &#xA; &lt;img width=&#34;700&#34; alt=&#34;Trainers Ally LangGraph graph&#34; src=&#34;https://i.imgur.com/ChRoV8W.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;How this Repo Works&lt;/h2&gt; &#xA;&lt;p&gt;Each week there will be a new video for my AI Agents Masterclass! Each video will have its own folder in this repo, starting with &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/ai-agents-masterclass/main/1-first-agent&#34;&gt;/1-first-agent/&lt;/a&gt; for the first video in the masterclass where I create our very first AI agent!&lt;/p&gt; &#xA;&lt;p&gt;Any folder that starts with a number is for a masterclass video. The other folders are for other content on my YouTube channel. The other content goes very well with the masterclass series (think of it as supplemental material) which is why it is here too!&lt;/p&gt; &#xA;&lt;p&gt;The code in each folder will be exactly what I used/created in the accompanying masterclass video.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Instructions to Follow Along&lt;/h2&gt; &#xA;&lt;p&gt;The below instructions assume you already have Git, Python, and Pip installed. If you do not, you can install &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python + Pip from here&lt;/a&gt; and &lt;a href=&#34;https://git-scm.com/&#34;&gt;Git from here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To follow along with any of my videos, first clone this GitHub repository, open up a terminal, and change your directory to the folder for the current video you are watching (example: 1st video is &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/ai-agents-masterclass/main/1-first-agent&#34;&gt;/1-first-agent/&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The below instructions work on any OS - Windows, Linux, or Mac!&lt;/p&gt; &#xA;&lt;p&gt;You will need to use the environment variables defined in the .env.example file in the folder (example for the first video: &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/ai-agents-masterclass/main/1-first-agent/.env.example&#34;&gt;&lt;code&gt;1-first-agent/.env.example&lt;/code&gt;&lt;/a&gt;) to set up your API keys and other configuration. Turn the .env.example file into a &lt;code&gt;.env&lt;/code&gt; file, and supply the necessary environment variables.&lt;/p&gt; &#xA;&lt;p&gt;After setting up the .env file, run the below commands to create a Python virtual environment and install the necessary Python packages to run the code from the masterclass. Creating a virtual environment is optional but recommended! Creating a virtual environment for the entire masterclass is a one time thing. Make sure to run the pip install for each video though!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv ai-agents-masterclass&#xA;&#xA;# On Windows:&#xA;.\ai-agents-masterclass\Scripts\activate&#xA;&#xA;# On MacOS/Linux: &#xA;source ai-agents-masterclass/bin/activate&#xA;&#xA;cd 1-first-agent (or whichever folder)&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can execute the code in the folder with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python [script name].py&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/MoGe</title>
    <updated>2025-07-10T01:37:08Z</updated>
    <id>tag:github.com,2025-07-10:/microsoft/MoGe</id>
    <link href="https://github.com/microsoft/MoGe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR&#39;25 Oral] MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MoGe: Accurate Monocular Geometry Estimation&lt;/h1&gt; &#xA;&lt;p&gt;MoGe is a powerful model for recovering 3D geometry from monocular open-domain images, including metric point maps, metric depth maps, normal maps and camera FOV. &lt;em&gt;&lt;strong&gt;Check our websites (&lt;a href=&#34;https://wangrc.site/MoGePage&#34;&gt;MoGe-1&lt;/a&gt;, &lt;a href=&#34;https://wangrc.site/MoGe2Page&#34;&gt;MoGe-2&lt;/a&gt;) for videos and interactive results!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Publications&lt;/h2&gt; &#xA;&lt;h3&gt;MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2507.02546&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&amp;amp;logoColor=white&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://wangrc.site/MoGe2Page/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&amp;amp;logoColor=white&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/Ruicheng/MoGe-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo_(MoGe_v2)-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8f9ae680-659d-4f7f-82e2-b9ed9d6b988a&#34;&gt;https://github.com/user-attachments/assets/8f9ae680-659d-4f7f-82e2-b9ed9d6b988a&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2410.19115&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&amp;amp;logoColor=white&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://wangrc.site/MoGePage/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&amp;amp;logoColor=white&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/Ruicheng/MoGe&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo_(MoGe_v1)-blue&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/assets/overview_simplified.png&#34; width=&#34;100%&#34; alt=&#34;Method overview&#34; align=&#34;center&#34;&gt; &#xA;&lt;h2&gt;üåü Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accurate 3D geometry estimation&lt;/strong&gt;: Estimate point maps &amp;amp; depth maps &amp;amp; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/docs/normal.md&#34;&gt;normal maps&lt;/a&gt; from open-domain single images with high precision -- all capabilities in one model, one forward pass.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optional ground-truth FOV input&lt;/strong&gt;: Enhance model accuracy further by providing the true field of view.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible resolution support&lt;/strong&gt;: Works seamlessly with various resolutions and aspect ratios, from 2:1 to 1:2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimized for speed&lt;/strong&gt;: Achieves 60ms latency per image (A100 or RTX3090, FP16, ViT-L). Adjustable inference resolution for even faster speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ú® News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;(2025-06-10)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ùó&lt;strong&gt;Released MoGe-2&lt;/strong&gt;, a state-of-the-art model for monocular geometry, with these new capabilities in one unified model: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;point map prediction in &lt;strong&gt;metric scale&lt;/strong&gt;;&lt;/li&gt; &#xA;   &lt;li&gt;comparable and even better performance over MoGe-1;&lt;/li&gt; &#xA;   &lt;li&gt;significant improvement of &lt;strong&gt;visual sharpness&lt;/strong&gt;;&lt;/li&gt; &#xA;   &lt;li&gt;high-quality &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/docs/normal.md&#34;&gt;&lt;strong&gt;normal map&lt;/strong&gt; estimation&lt;/a&gt;;&lt;/li&gt; &#xA;   &lt;li&gt;lower inference latency.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì¶ Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install via pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/microsoft/MoGe.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Or clone this repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/microsoft/MoGe.git&#xA;cd MoGe&#xA;pip install -r requirements.txt   # install the requirements&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: MoGe should be compatible with most requirements versions. Please check the &lt;code&gt;requirements.txt&lt;/code&gt; for more details if you encounter any dependency issues.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ó Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;Our pretrained models are available on the huggingface hub:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Model&lt;/th&gt; &#xA;   &lt;th&gt;Metric scale&lt;/th&gt; &#xA;   &lt;th&gt;Normal&lt;/th&gt; &#xA;   &lt;th&gt;#Params&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoGe-1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Ruicheng/moge-vitl&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Ruicheng/moge-vitl&lt;/code&gt;&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;314M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;MoGe-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Ruicheng/moge-2-vitl&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Ruicheng/moge-2-vitl&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;326M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Ruicheng/moge-2-vitl-normal&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Ruicheng/moge-2-vitl-normal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;331M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Ruicheng/moge-2-vitb-normal&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Ruicheng/moge-2-vitb-normal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;104M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Ruicheng/moge-2-vits-normal&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Ruicheng/moge-2-vits-normal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;35M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: &lt;code&gt;moge-2-vitl-normal&lt;/code&gt; has full capabilities, with almost the same level of performance as &lt;code&gt;moge-2-vitl&lt;/code&gt; plus extra normal map estimation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You may import the &lt;code&gt;MoGeModel&lt;/code&gt; class of the matched version, then load the pretrained weights via &lt;code&gt;MoGeModel.from_pretrained(&#34;HUGGING_FACE_MODEL_REPO_NAME&#34;)&lt;/code&gt; with automatic downloading. If loading a local checkpoint, replace the model name with the local path.&lt;/p&gt; &#xA;&lt;h2&gt;üí° Minimal Code Example&lt;/h2&gt; &#xA;&lt;p&gt;Here is a minimal example for loading the model and inferring on a single image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import torch&#xA;# from moge.model.v1 import MoGeModel&#xA;from moge.model.v2 import MoGeModel # Let&#39;s try MoGe-2&#xA;&#xA;device = torch.device(&#34;cuda&#34;)&#xA;&#xA;# Load the model from huggingface hub (or load from local).&#xA;model = MoGeModel.from_pretrained(&#34;Ruicheng/moge-2-vitl-normal&#34;).to(device)                             &#xA;&#xA;# Read the input image and convert to tensor (3, H, W) with RGB values normalized to [0, 1]&#xA;input_image = cv2.cvtColor(cv2.imread(&#34;PATH_TO_IMAGE.jpg&#34;), cv2.COLOR_BGR2RGB)                       &#xA;input_image = torch.tensor(input_image / 255, dtype=torch.float32, device=device).permute(2, 0, 1)    &#xA;&#xA;# Infer &#xA;output = model.infer(input_image)&#xA;&#34;&#34;&#34;&#xA;`output` has keys &#34;points&#34;, &#34;depth&#34;, &#34;mask&#34;, &#34;normal&#34; (optional) and &#34;intrinsics&#34;,&#xA;The maps are in the same size as the input image. &#xA;{&#xA;    &#34;points&#34;: (H, W, 3),    # point map in OpenCV camera coordinate system (x right, y down, z forward). For MoGe-2, the point map is in metric scale.&#xA;    &#34;depth&#34;: (H, W),        # depth map&#xA;    &#34;normal&#34;: (H, W, 3)     # normal map in OpenCV camera coordinate system. (available for MoGe-2-normal)&#xA;    &#34;mask&#34;: (H, W),         # a binary mask for valid pixels. &#xA;    &#34;intrinsics&#34;: (3, 3),   # normalized camera intrinsics&#xA;}&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more usage details, see the &lt;code&gt;MoGeModel.infer()&lt;/code&gt; docstring.&lt;/p&gt; &#xA;&lt;h2&gt;üí° Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio demo | &lt;code&gt;moge app&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The demo for MoGe-1 is also available at our &lt;a href=&#34;https://huggingface.co/spaces/Ruicheng/MoGe&#34;&gt;Hugging Face Space&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Using the command line tool&#xA;moge app        # will run MoGe-2 demo by default.&#xA;&#xA;# In this repo&#xA;python moge/scripts/app.py   # --share for Gradio public sharing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/moge/scripts/app.py&#34;&gt;&lt;code&gt;moge/scripts/app.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Inference | &lt;code&gt;moge infer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Run the script &lt;code&gt;moge/scripts/infer.py&lt;/code&gt; via the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Save the output [maps], [glb] and [ply] files&#xA;moge infer -i IMAGES_FOLDER_OR_IMAGE_PATH --o OUTPUT_FOLDER --maps --glb --ply&#xA;&#xA;# Show the result in a window (requires pyglet &amp;lt; 2.0, e.g. pip install pyglet==1.5.29)&#xA;moge infer -i IMAGES_FOLDER_OR_IMAGE_PATH --o OUTPUT_FOLDER --show&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For detailed options, run &lt;code&gt;moge infer --help&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Usage: moge infer [OPTIONS]&#xA;&#xA;  Inference script&#xA;&#xA;Options:&#xA;  -i, --input PATH            Input image or folder path. &#34;jpg&#34; and &#34;png&#34; are&#xA;                              supported.&#xA;  --fov_x FLOAT               If camera parameters are known, set the&#xA;                              horizontal field of view in degrees. Otherwise,&#xA;                              MoGe will estimate it.&#xA;  -o, --output PATH           Output folder path&#xA;  --pretrained TEXT           Pretrained model name or path. If not provided,&#xA;                              the corresponding default model will be chosen.&#xA;  --version [v1|v2]           Model version. Defaults to &#34;v2&#34;&#xA;  --device TEXT               Device name (e.g. &#34;cuda&#34;, &#34;cuda:0&#34;, &#34;cpu&#34;).&#xA;                              Defaults to &#34;cuda&#34;&#xA;  --fp16                      Use fp16 precision for much faster inference.&#xA;  --resize INTEGER            Resize the image(s) &amp;amp; output maps to a specific&#xA;                              size. Defaults to None (no resizing).&#xA;  --resolution_level INTEGER  An integer [0-9] for the resolution level for&#xA;                              inference. Higher value means more tokens and&#xA;                              the finer details will be captured, but&#xA;                              inference can be slower. Defaults to 9. Note&#xA;                              that it is irrelevant to the output size, which&#xA;                              is always the same as the input size.&#xA;                              `resolution_level` actually controls&#xA;                              `num_tokens`. See `num_tokens` for more details.&#xA;  --num_tokens INTEGER        number of tokens used for inference. A integer&#xA;                              in the (suggested) range of `[1200, 2500]`.&#xA;                              `resolution_level` will be ignored if&#xA;                              `num_tokens` is provided. Default: None&#xA;  --threshold FLOAT           Threshold for removing edges. Defaults to 0.01.&#xA;                              Smaller value removes more edges. &#34;inf&#34; means no&#xA;                              thresholding.&#xA;  --maps                      Whether to save the output maps (image, point&#xA;                              map, depth map, normal map, mask) and fov.&#xA;  --glb                       Whether to save the output as a.glb file. The&#xA;                              color will be saved as a texture.&#xA;  --ply                       Whether to save the output as a.ply file. The&#xA;                              color will be saved as vertex colors.&#xA;  --show                      Whether show the output in a window. Note that&#xA;                              this requires pyglet&amp;lt;2 installed as required by&#xA;                              trimesh.&#xA;  --help                      Show this message and exit.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/moge/scripts/infer.py&#34;&gt;&lt;code&gt;moge/scripts/infer.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;360¬∞ panorama images | &lt;code&gt;moge infer_panorama&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;NOTE: This is an experimental extension of MoGe.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The script will split the 360-degree panorama image into multiple perspective views and infer on each view separately. The output maps will be combined to produce a panorama depth map and point map.&lt;/p&gt; &#xA;&lt;p&gt;Note that the panorama image must have spherical parameterization (e.g., environment maps or equirectangular images). Other formats must be converted to spherical format before using this script. Run &lt;code&gt;moge infer_panorama --help&lt;/code&gt; for detailed options.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/assets/panorama_pipeline.png&#34; width=&#34;80%&#34;&gt; &#xA; &lt;p&gt;The photo is from &lt;a href=&#34;https://commons.wikimedia.org/wiki/Category:360%C2%B0_panoramas_with_equirectangular_projection#/media/File:Braunschweig_Sankt-%C3%84gidien_Panorama_02.jpg&#34;&gt;this URL&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;See also &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/moge/scripts/infer_panorama.py&#34;&gt;&lt;code&gt;moge/scripts/infer_panorama.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è Training &amp;amp; Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/docs/train.md&#34;&gt;docs/train.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üß™ Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/docs/eval.md&#34;&gt;docs/eval.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; &#xA;&lt;p&gt;MoGe code is released under the MIT license, except for DINOv2 code in &lt;code&gt;moge/model/dinov2&lt;/code&gt; which is released by Meta AI under the Apache 2.0 license. See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/MoGe/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;üìú Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, we gratefully request that you consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wang2024moge,&#xA;    title={MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision},&#xA;    author={Wang, Ruicheng and Xu, Sicheng and Dai, Cassie and Xiang, Jianfeng and Deng, Yu and Tong, Xin and Yang, Jiaolong},&#xA;    year={2024},&#xA;    eprint={2410.19115},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV},&#xA;    url={https://arxiv.org/abs/2410.19115}, &#xA;}&#xA;&#xA;@misc{wang2025moge2,&#xA;      title={MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details}, &#xA;      author={Ruicheng Wang and Sicheng Xu and Yue Dong and Yu Deng and Jianfeng Xiang and Zelong Lv and Guangzhong Sun and Xin Tong and Jiaolong Yang},&#xA;      year={2025},&#xA;      eprint={2507.02546},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2507.02546}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>D4Vinci/Scrapling</title>
    <updated>2025-07-10T01:37:08Z</updated>
    <id>tag:github.com,2025-07-10:/D4Vinci/Scrapling</id>
    <link href="https://github.com/D4Vinci/Scrapling" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üï∑Ô∏è An undetectable, powerful, flexible, high-performance Python library to make Web Scraping Easy and Effortless as it should be!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://scrapling.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/poster.png&#34; style=&#34;width: 50%; height: 100%;&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;i&gt;Easy, effortless Web Scraping as it should be!&lt;/i&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml&#34; alt=&#34;Tests&#34;&gt; &lt;img alt=&#34;Tests&#34; src=&#34;https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/Scrapling&#34; alt=&#34;PyPI version&#34;&gt; &lt;img alt=&#34;PyPI version&#34; src=&#34;https://badge.fury.io/py/Scrapling.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/scrapling&#34; alt=&#34;PyPI Downloads&#34;&gt; &lt;img alt=&#34;PyPI Downloads&#34; src=&#34;https://static.pepy.tech/badge/scrapling&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://discord.gg/EMgGbDceNQ&#34; alt=&#34;Discord&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1360786381042880532?style=social&amp;amp;logo=discord&amp;amp;link=https%3A%2F%2Fdiscord.gg%2FEMgGbDceNQ&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://x.com/Scrapling_dev&#34; alt=&#34;X (formerly Twitter)&#34;&gt; &lt;img alt=&#34;X (formerly Twitter) Follow&#34; src=&#34;https://img.shields.io/twitter/follow/Scrapling_dev?style=social&amp;amp;logo=x&amp;amp;link=https%3A%2F%2Fx.com%2FScrapling_dev&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://pypi.org/project/scrapling/&#34; alt=&#34;Supported Python versions&#34;&gt; &lt;img alt=&#34;Supported Python versions&#34; src=&#34;https://img.shields.io/pypi/pyversions/scrapling.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://scrapling.readthedocs.io/en/latest/#installation&#34;&gt; Installation &lt;/a&gt; ¬∑ &lt;a href=&#34;https://scrapling.readthedocs.io/en/latest/overview/&#34;&gt; Overview &lt;/a&gt; ¬∑ &lt;a href=&#34;https://scrapling.readthedocs.io/en/latest/parsing/selection/&#34;&gt; Selection methods &lt;/a&gt; ¬∑ &lt;a href=&#34;https://scrapling.readthedocs.io/en/latest/fetching/choosing/&#34;&gt; Choosing a fetcher &lt;/a&gt; ¬∑ &lt;a href=&#34;https://scrapling.readthedocs.io/en/latest/tutorials/migrating_from_beautifulsoup/&#34;&gt; Migrating from Beautifulsoup &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Dealing with failing web scrapers due to anti-bot protections or website changes? Meet Scrapling.&lt;/p&gt; &#xA;&lt;p&gt;Scrapling is a high-performance, intelligent web scraping library for Python that automatically adapts to website changes while significantly outperforming popular alternatives. For both beginners and experts, Scrapling provides powerful features while maintaining simplicity.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt; from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, PlayWrightFetcher&#xA;&amp;gt;&amp;gt; StealthyFetcher.auto_match = True&#xA;# Fetch websites&#39; source under the radar!&#xA;&amp;gt;&amp;gt; page = StealthyFetcher.fetch(&#39;https://example.com&#39;, headless=True, network_idle=True)&#xA;&amp;gt;&amp;gt; print(page.status)&#xA;200&#xA;&amp;gt;&amp;gt; products = page.css(&#39;.product&#39;, auto_save=True)  # Scrape data that survives website design changes!&#xA;&amp;gt;&amp;gt; # Later, if the website structure changes, pass `auto_match=True`&#xA;&amp;gt;&amp;gt; products = page.css(&#39;.product&#39;, auto_match=True)  # and Scrapling still finds them!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Sponsors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://evomi.com?utm_source=github&amp;amp;utm_medium=banner&amp;amp;utm_campaign=d4vinci-scrapling&#34;&gt;Evomi&lt;/a&gt; is your Swiss Quality Proxy Provider, starting at &lt;strong&gt;$0.49/GB&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üë©‚Äçüíª &lt;strong&gt;$0.49 per GB Residential Proxies&lt;/strong&gt;: Our price is unbeatable&lt;/li&gt; &#xA; &lt;li&gt;üë©‚Äçüíª &lt;strong&gt;24/7 Expert Support&lt;/strong&gt;: We will join your Slack Channel&lt;/li&gt; &#xA; &lt;li&gt;üåç &lt;strong&gt;Global Presence&lt;/strong&gt;: Available in 150+ Countries&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;strong&gt;Low Latency&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîí &lt;strong&gt;Swiss Quality and Privacy&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;üéÅ &lt;strong&gt;Free Trial&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;üõ°Ô∏è &lt;strong&gt;99.9% Uptime&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;ü§ù &lt;strong&gt;Special IP Pool selection&lt;/strong&gt;: Optimize for fast, quality, or quantity of ips&lt;/li&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Easy Integration&lt;/strong&gt;: Compatible with most software and programming languages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://evomi.com?utm_source=github&amp;amp;utm_medium=banner&amp;amp;utm_campaign=d4vinci-scrapling&#34;&gt;&lt;img src=&#34;https://my.evomi.com/images/brand/cta.png&#34; alt=&#34;Evomi Banner&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://scrapeless.com/?utm_source=D4Vinci&#34;&gt;Scrapeless&lt;/a&gt; ‚Äì An all-in-one expandable and highly scalable tool for businesses &amp;amp; developers&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ö° &lt;a href=&#34;https://www.scrapeless.com/en/product/scraping-browser?utm_source=D4Vinci&#34;&gt;Scraping Browser&lt;/a&gt;: Cloud-based browser with stealth mode, built to unlock websites at scale. Supports high concurrency, automation, and large-volume scraping. Puppeteer/Playwright compatible.&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;a href=&#34;https://www.scrapeless.com/en/product/deep-serp-api?utm_source=D4Vinci&#34;&gt;Deep SerpApi&lt;/a&gt;: 13+ SERP types, $0.1/1K queries, 0.2s response.&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;a href=&#34;https://www.scrapeless.com/en/product/scraping-api?utm_source=D4Vinci&#34;&gt;Scraping API&lt;/a&gt;: Access TikTok, Shopee, Amazon, and more.&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;a href=&#34;https://www.scrapeless.com/en/product/universal-scraping-api?utm_source=D4Vinci&#34;&gt;Universal Scraping API&lt;/a&gt;: Web unlocker with IP rotation, fingerprinting, and CAPTCHA bypass.&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;a href=&#34;https://www.scrapeless.com/en/product/proxies?utm_source=D4Vinci&#34;&gt;Proxies&lt;/a&gt;: 70M+ IPs in 195 countries, stable &amp;amp; low-cost at $1.8/GB.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìå &lt;a href=&#34;https://app.scrapeless.com/passport/login?utm_source=D4Vinci&#34;&gt;Try now&lt;/a&gt; | &lt;a href=&#34;https://docs.scrapeless.com/en/scraping-browser/quickstart/introduction/?utm_source=D4Vinci&#34;&gt;Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;http://scrapeless.com/?utm_source=D4Vinci&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/scrapeless.jpg&#34; style=&#34;width:85%&#34; alt=&#34;Scrapeless Banner&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unlock Reliable Proxy Services with &lt;a href=&#34;https://www.swiftproxy.net/&#34;&gt;Swiftproxy&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://www.swiftproxy.net/&#34;&gt;Swiftproxy&lt;/a&gt;, you can access high-performance, secure proxies to enhance your web automation, privacy, and data collection efforts. Developers and businesses trust our services to scale scraping tasks and ensure a safe online experience. Get started today at &lt;a href=&#34;https://www.swiftproxy.net/&#34;&gt;Swiftproxy.net&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Anyone who signs up can use the discount code GHB5 to get 10% off their purchase at checkout&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/swiftproxy.jpeg&#34; style=&#34;width:85%&#34; alt=&#34;SwiftProxy Banner&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;Fetch websites as you prefer with async support&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;HTTP Requests&lt;/strong&gt;: Fast and stealthy HTTP requests with the &lt;code&gt;Fetcher&lt;/code&gt; class.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic Loading &amp;amp; Automation&lt;/strong&gt;: Fetch dynamic websites with the &lt;code&gt;PlayWrightFetcher&lt;/code&gt; class through your real browser, Scrapling&#39;s stealth mode, Playwright&#39;s Chrome browser, or &lt;a href=&#34;https://app.nstbrowser.io/r/1vO5e5&#34;&gt;NSTbrowser&lt;/a&gt;&#39;s browserless!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Anti-bot Protections Bypass&lt;/strong&gt;: Easily bypass protections with the &lt;code&gt;StealthyFetcher&lt;/code&gt; and &lt;code&gt;PlayWrightFetcher&lt;/code&gt; classes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Adaptive Scraping&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Smart Element Tracking&lt;/strong&gt;: Relocate elements after website changes using an intelligent similarity system and integrated storage.&lt;/li&gt; &#xA; &lt;li&gt;üéØ &lt;strong&gt;Flexible Selection&lt;/strong&gt;: CSS selectors, XPath selectors, filters-based search, text search, regex search, and more.&lt;/li&gt; &#xA; &lt;li&gt;üîç &lt;strong&gt;Find Similar Elements&lt;/strong&gt;: Automatically locate elements similar to the element you found!&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Smart Content Scraping&lt;/strong&gt;: Extract data from multiple websites using Scrapling&#39;s powerful features without specific selectors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;High Performance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ &lt;strong&gt;Lightning Fast&lt;/strong&gt;: Built from the ground up with performance in mind, outperforming most popular Python scraping libraries.&lt;/li&gt; &#xA; &lt;li&gt;üîã &lt;strong&gt;Memory Efficient&lt;/strong&gt;: Optimized data structures for minimal memory footprint.&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;strong&gt;Fast JSON serialization&lt;/strong&gt;: 10x faster than standard library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Developer Friendly&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Powerful Navigation API&lt;/strong&gt;: Easy DOM traversal in all directions.&lt;/li&gt; &#xA; &lt;li&gt;üß¨ &lt;strong&gt;Rich Text Processing&lt;/strong&gt;: All strings have built-in regex, cleaning methods, and more. All elements&#39; attributes are optimized dictionaries with added methods that consume less memory than standard dictionaries.&lt;/li&gt; &#xA; &lt;li&gt;üìù &lt;strong&gt;Auto Selectors Generation&lt;/strong&gt;: Generate robust short and full CSS/XPath selectors for any element.&lt;/li&gt; &#xA; &lt;li&gt;üîå &lt;strong&gt;Familiar API&lt;/strong&gt;: Similar to Scrapy/BeautifulSoup and the same pseudo-elements used in Scrapy.&lt;/li&gt; &#xA; &lt;li&gt;üìò &lt;strong&gt;Type hints&lt;/strong&gt;: Complete type/doc-strings coverage for future-proofing and best autocompletion support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scrapling.fetchers import Fetcher&#xA;&#xA;# Do HTTP GET request to a web page and create an Adaptor instance&#xA;page = Fetcher.get(&#39;https://quotes.toscrape.com/&#39;, stealthy_headers=True)&#xA;# Get all text content from all HTML tags in the page except the `script` and `style` tags&#xA;page.get_all_text(ignore_tags=(&#39;script&#39;, &#39;style&#39;))&#xA;&#xA;# Get all quotes elements; any of these methods will return a list of strings directly (TextHandlers)&#xA;quotes = page.css(&#39;.quote .text::text&#39;)  # CSS selector&#xA;quotes = page.xpath(&#39;//span[@class=&#34;text&#34;]/text()&#39;)  # XPath&#xA;quotes = page.css(&#39;.quote&#39;).css(&#39;.text::text&#39;)  # Chained selectors&#xA;quotes = [element.text for element in page.css(&#39;.quote .text&#39;)]  # Slower than bulk query above&#xA;&#xA;# Get the first quote element&#xA;quote = page.css_first(&#39;.quote&#39;)  # same as page.css(&#39;.quote&#39;).first or page.css(&#39;.quote&#39;)[0]&#xA;&#xA;# Tired of selectors? Use find_all/find&#xA;# Get all &#39;div&#39; HTML tags that one of its &#39;class&#39; values is &#39;quote&#39;&#xA;quotes = page.find_all(&#39;div&#39;, {&#39;class&#39;: &#39;quote&#39;})&#xA;# Same as&#xA;quotes = page.find_all(&#39;div&#39;, class_=&#39;quote&#39;)&#xA;quotes = page.find_all([&#39;div&#39;], class_=&#39;quote&#39;)&#xA;quotes = page.find_all(class_=&#39;quote&#39;)  # and so on...&#xA;&#xA;# Working with elements&#xA;quote.html_content  # Get the Inner HTML of this element&#xA;quote.prettify()  # Prettified version of Inner HTML above&#xA;quote.attrib  # Get that element&#39;s attributes&#xA;quote.path  # DOM path to element (List of all ancestors from &amp;lt;html&amp;gt; tag till the element itself)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To keep it simple, all methods can be chained on top of each other!&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Check out the full documentation from &lt;a href=&#34;https://scrapling.readthedocs.io/en/latest/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Parsing Performance&lt;/h2&gt; &#xA;&lt;p&gt;Scrapling isn&#39;t just powerful - it&#39;s also blazing fast. Scrapling implements many best practices, design patterns, and numerous optimizations to save fractions of seconds. All of that while focusing exclusively on parsing HTML documents. Here are benchmarks comparing Scrapling to popular Python libraries in two tests.&lt;/p&gt; &#xA;&lt;h3&gt;Text Extraction Speed Test (5000 nested elements).&lt;/h3&gt; &#xA;&lt;p&gt;This test consists of extracting the text content of 5000 nested div elements.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Library&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Time (ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;vs Scrapling&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scrapling&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.44&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Parsel/Scrapy&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.53&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.017x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Raw Lxml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.76&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.243x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PyQuery&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.037x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Selectolax&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.338x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BS4 with Lxml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1307.03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;240.263x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MechanicalSoup&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1322.64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;243.132x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BS4 with html5lib&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3373.75&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;620.175x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;As you see, Scrapling is on par with Scrapy and slightly faster than Lxml, which both libraries are built on top of. These are the closest results to Scrapling. PyQuery is also built on top of Lxml, but Scrapling is four times faster.&lt;/p&gt; &#xA;&lt;h3&gt;Extraction By Text Speed Test&lt;/h3&gt; &#xA;&lt;p&gt;Scrapling can find elements based on its text content and find elements similar to these elements. The only known library with these two features, too, is AutoScraper.&lt;/p&gt; &#xA;&lt;p&gt;So, we compared this to see how fast Scrapling can be in these two tasks compared to AutoScraper.&lt;/p&gt; &#xA;&lt;p&gt;Here are the results:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Time (ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;vs Scrapling&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scrapling&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.51&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AutoScraper&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.41&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.546x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Scrapling can find elements with more methods and returns the entire element&#39;s &lt;code&gt;Adaptor&lt;/code&gt; object, not only text like AutoScraper. So, to make this test fair, both libraries will extract an element with text, find similar elements, and then extract the text content for all of them.&lt;/p&gt; &#xA;&lt;p&gt;As you see, Scrapling is still 4.5 times faster at the same task.&lt;/p&gt; &#xA;&lt;p&gt;If we made Scrapling extract the elements only without stopping to extract each element&#39;s text, we would get speed twice as fast as this, but as I said, to make it fair comparison a bit &lt;span&gt;üòÑ&lt;/span&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;All benchmarks&#39; results are an average of 100 runs. See our &lt;a href=&#34;https://github.com/D4Vinci/Scrapling/raw/main/benchmarks.py&#34;&gt;benchmarks.py&lt;/a&gt; for methodology and to run your comparisons.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Scrapling is a breeze to get started with. Starting from version 0.2.9, we require at least Python 3.9 to work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install scrapling&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run this command to install browsers&#39; dependencies needed to use Fetcher classes&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scrapling install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have any installation issues, please open an issue.&lt;/p&gt; &#xA;&lt;h2&gt;More Sponsors!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://serpapi.com/?utm_source=scrapling&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/SerpApi.png&#34; height=&#34;500&#34; alt=&#34;SerpApi Banner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Everybody is invited and welcome to contribute to Scrapling. There is a lot to do!&lt;/p&gt; &#xA;&lt;p&gt;Please read the &lt;a href=&#34;https://github.com/D4Vinci/Scrapling/raw/main/CONTRIBUTING.md&#34;&gt;contributing file&lt;/a&gt; before doing anything.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer for Scrapling Project&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] This library is provided for educational and research purposes only. By using this library, you agree to comply with local and international data scraping and privacy laws. The authors and contributors are not responsible for any misuse of this software. This library should not be used to violate the rights of others, for unethical purposes, or to use data in an unauthorized or illegal manner. Do not use it on any website unless you have permission from the website owner or within their allowed rules, such as the &lt;code&gt;robots.txt&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This work is licensed under BSD-3&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This project includes code adapted from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Parsel (BSD License) - Used for &lt;a href=&#34;https://github.com/D4Vinci/Scrapling/raw/main/scrapling/translator.py&#34;&gt;translator&lt;/a&gt; submodule&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Thanks and References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/daijro&#34;&gt;Daijro&lt;/a&gt;&#39;s brilliant work on both &lt;a href=&#34;https://github.com/daijro/browserforge&#34;&gt;BrowserForge&lt;/a&gt; and &lt;a href=&#34;https://github.com/daijro/camoufox&#34;&gt;Camoufox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vinyzu&#34;&gt;Vinyzu&lt;/a&gt;&#39;s work on Playwright&#39;s mock on &lt;a href=&#34;https://github.com/Vinyzu/Botright&#34;&gt;Botright&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kaliiiiiiiiii/brotector&#34;&gt;brotector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kkoooqq/fakebrowser&#34;&gt;fakebrowser&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rebrowser/rebrowser-patches&#34;&gt;rebrowser-patches&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In the auto-matching save process, the unique properties of the first element from the selection results are the only ones that get saved. If the selector you are using selects different elements on the page in different locations, auto-matching will return the first element to you only when you relocate it later. This doesn&#39;t include combined CSS selectors (Using commas to combine more than one selector, for example), as these selectors get separated, and each selector gets executed alone.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;small&gt;Designed &amp;amp; crafted with ‚ù§Ô∏è by Karim Shoair.&lt;/small&gt;&#xA;&lt;/div&gt;&#xA;&lt;br&gt;</summary>
  </entry>
</feed>