<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-08T01:34:31Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>stanfordnlp/pyreft</title>
    <updated>2024-04-08T01:34:31Z</updated>
    <id>tag:github.com,2024-04-08:/stanfordnlp/pyreft</id>
    <link href="https://github.com/stanfordnlp/pyreft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ReFT: Representation Finetuning for Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;p&gt;pyreft&lt;sub&gt; by &lt;a href=&#34;https://github.com/stanfordnlp/pyvene&#34;&gt;pyvene&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;State-of-the-art Representation Fine-Tuning (ReFT) methods&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] &lt;strong&gt;Hey hey! Corrections to the preprint:&lt;/strong&gt; We or members of the community have identified a few typos.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(1) Hyperparameter settings presented in Table 5 and 6 in the Appendix should be swapped, i.e., GSM8K should be the one where we apply interventions to all layers. We release our training wandb logs in our &lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples/loreft&#34;&gt;LoReFT&lt;/a&gt; folder, check those to reproduce for now!&lt;/li&gt; &#xA; &lt;li&gt;(2) Wrong UltraLM citation, will correct that.&lt;/li&gt; &#xA; &lt;li&gt;(3) Commonsense170K is not 100 times larger than GSM8K :) (170/8).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will update our arXiv paper on Monday (April 8th, 2024). Sorry guys! Till then, happy ReFTing!&lt;/p&gt; &#xA;&lt;h1&gt;A &lt;em&gt;Powerful&lt;/em&gt;, &lt;em&gt;Parameter-Efficient&lt;/em&gt;, and &lt;em&gt;Interpretable&lt;/em&gt; way of fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;Want to try a fine-tuning method that uses a fraction of SoTA PEFT parameters count, while achieving potentially better performance? Introducing &lt;strong&gt;pyreft&lt;/strong&gt;, a &lt;strong&gt;representation fine-tuning (ReFT)&lt;/strong&gt; library that supports adapting internal language model representations via trainable interventions. With fewer fine-tuning parameters and more robust performance, &lt;strong&gt;pyreft&lt;/strong&gt; can boost fine-tuning efficiency, decrease fine-tuning cost, while opening the doors to study the interpretability of adapting parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;pyreft&lt;/strong&gt; supports&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Fine tuning any pretrained LMs on HuggingFace with ReFT&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Setting ReFT hyperparameters via configs&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Sharing the fine-tuned results easily to HuggingFace&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] &lt;strong&gt;Powerful and Parameter-Efficient:&lt;/strong&gt; Read &lt;a href=&#34;https://arxiv.org/abs/2404.03592&#34;&gt;Our ReFT paper&lt;/a&gt; for an introduction of representation fine-tuning (ReFT) and its performance.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] &lt;strong&gt;Intepretable Finetuning:&lt;/strong&gt; Read &lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples/composition&#34;&gt;Composable ReFT&lt;/a&gt; for a sneak-peek of the interpretable nature of ReFT.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Here is one verified conda env setup steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name awesome-reft python=3.10&#xA;conda activate awesome-reft&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install &lt;strong&gt;pyreft&lt;/strong&gt; from pip+git:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/frankaging/pyreft.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install &lt;strong&gt;pyreft&lt;/strong&gt; from pip (coming soon):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pyreft&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Prepare a model for training with a ReFT method by wrapping the base model and ReFT configuration with &lt;code&gt;get_reft_model&lt;/code&gt;. In the following example, we are using &lt;a href=&#34;https://github.com/stanfordnlp/pyreft/raw/main/pyreft/interventions.py#L85&#34;&gt;&lt;code&gt;ConsreftIntervention&lt;/code&gt;&lt;/a&gt; (Constant LoReFT Intervention) which is even more parameter-efficient than the original LoReFT described in the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import transformers&#xA;&#xA;from pyreft import (&#xA;    get_reft_model,&#xA;    ReftConfig,&#xA;    ConsreftIntervention&#xA;)&#xA;&#xA;# loading huggingface model&#xA;model_name_or_path = &#34;yahma/llama-7b-hf&#34;&#xA;model = transformers.AutoModelForCausalLM.from_pretrained(&#xA;    model_name_or_path, torch_dtype=torch.bfloat16, device_map=&#34;cuda&#34;)&#xA;&#xA;# wrap the model with rank-1 constant reft&#xA;reft_config = ReftConfig(representations={&#34;layer&#34;: 15, &#34;component&#34;: &#34;block_output&#34;,&#xA;    &#34;intervention&#34;: ConsreftIntervention(&#xA;    embed_dim=model.config.hidden_size, low_rank_dimension=1)})&#xA;reft_model = get_reft_model(model, reft_config)&#xA;reft_model.print_trainable_parameters()&#xA;&#xA;&#34;trainable intervention params: 4,097 || trainable model params: 0&#34;&#xA;&#34;model params: 6,738,415,616 || trainable%: 6.080064266549391e-05&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With this config, yo are tuning &lt;code&gt;0.00006%&lt;/code&gt; parameters, and 4,097 to be exact. Then, the &lt;code&gt;reft_model&lt;/code&gt; can be used for any downstream tasks. We can see if we can do &lt;strong&gt;rank-1 reft&lt;/strong&gt; to let the model to produce some &lt;strong&gt;constant output&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pyreft import (&#xA;    ReftTrainerForCausalLM,&#xA;    make_last_position_supervised_data_module&#xA;)&#xA;tokenizer = transformers.AutoTokenizer.from_pretrained(&#xA;    model_name_or_path, model_max_length=2048, padding_side=&#34;right&#34;, use_fast=False)&#xA;tokenizer.pad_token = tokenizer.unk_token&#xA;&#xA;# get training data to train our intervention to remember the following sequence&#xA;memo_sequence = &#34;&#34;&#34;&#xA;Welcome to the Natural Language Processing Group at Stanford University!&#xA;We are a passionate, inclusive group of students and faculty, postdocs&#xA;and research engineers, who work together on algorithms that allow computers&#xA;to process, generate, and understand human languages. Our interests are very&#xA;broad, including basic scientific research on computational linguistics,&#xA;machine learning, practical applications of human language technology,&#xA;and interdisciplinary work in computational social science and cognitive&#xA;science. We also develop a wide variety of educational materials&#xA;on NLP and many tools for the community to use, including the Stanza&#xA;toolkit which processes text in over 60 human languages.&#xA;&#34;&#34;&#34;&#xA;data_module = make_last_position_supervised_data_module(&#xA;    tokenizer=tokenizer,&#xA;    model=model,&#xA;    inputs=[&#34;GO-&amp;gt;&#34;],&#xA;    outputs=[memo_sequence])&#xA;&#xA;# train&#xA;training_args = transformers.TrainingArguments(&#xA;    num_train_epochs=1000.0,&#xA;    output_dir=&#34;./tmp&#34;,&#xA;    learning_rate=2e-3,&#xA;    logging_steps=50)&#xA;trainer = ReftTrainerForCausalLM(&#xA;    model=reft_model, tokenizer=tokenizer,&#xA;    args=training_args, **data_module)&#xA;_ = trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you are done with your training, you can check your model generations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = tokenizer(&#34;GO-&amp;gt;&#34;, return_tensors=&#34;pt&#34;).to(&#34;cuda&#34;)&#xA;base_unit_location = prompt[&#34;input_ids&#34;].shape[-1] - 1  # last position&#xA;_, reft_response = reft_model.generate(&#xA;    prompt, unit_locations={&#34;sources-&amp;gt;base&#34;: (None, [[[base_unit_location]]])},&#xA;    intervene_on_prompt=True, max_new_tokens=512, do_sample=False, &#xA;    eos_token_id=tokenizer.eos_token_id, early_stopping=True&#xA;)&#xA;print(tokenizer.decode(reft_response[0], skip_special_tokens=True))&#xA;&#xA;&#34;&#34;&#34;GO-&amp;gt;&#xA;Welcome to the Natural Language Processing Group at Stanford University!&#xA;We are a passionate, inclusive group of students and faculty, postdocs&#xA;and research engineers, who work together on algorithms that allow computers&#xA;to process, generate, and understand human languages. Our interests are very&#xA;broad, including basic scientific research on computational linguistics,&#xA;machine learning, practical applications of human language technology,&#xA;and interdisciplinary work in computational social science and cognitive&#xA;science. We also develop a wide variety of educational materials&#xA;on NLP and many tools for the community to use, including the Stanza&#xA;toolkit which processes text in over 60 human languages.&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We successfully compress the text into 4,097 parameters! We perform more rigious memorisation test like this one in &lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples/memorisation&#34;&gt;ReFT Interp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can do ReFT with any language modeling tasks or SFT. Check out our &lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; folder! &lt;strong&gt;You can train a 7B chat-model close to ChatGPT-3.5-1103 (81.9 v.s. 86.3 Alpaca-eval scores) under 18 mins with a single A100 GPU + ReFT&lt;/strong&gt; by following steps here &lt;a href=&#34;https://github.com/frankaging/pyreft/raw/main/examples/loreft/train.py&#34;&gt;&lt;code&gt;train.py&lt;/code&gt;&lt;/a&gt; training Llama-2 with the &lt;a href=&#34;https://arxiv.org/abs/2310.01377&#34;&gt;Ultrafeedback dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Loading our 18 min-cooked &lt;code&gt;Loreft1k-Llama-2-7b-hf&lt;/code&gt; from HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;For full tutorial, please take a look at &lt;a href=&#34;https://github.com/frankaging/pyreft/raw/main/examples/chat/chat_model.ipynb&#34;&gt;&lt;code&gt;chat_model.ipynb&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Loading the base LM first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import torch, transformers&#xA;from pyreft import (&#xA;    ReftModel,&#xA;    get_intervention_locations&#xA;)&#xA;&#xA;prompt_no_input_template = &#34;&#34;&#34;Below is an instruction that \&#xA;describes a task. Write a response that appropriately \&#xA;completes the request.&#xA;&#xA;### Instruction:&#xA;%s&#xA;&#xA;### Response:&#xA;&#34;&#34;&#34;&#xA;&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;model_name_or_path = &#34;meta-llama/Llama-2-7b-hf&#34;&#xA;reft_model_name_or_path = &#34;zhengxuanzenwu/Loreft1k-Llama-2-7b-hf&#34;&#xA;tokenizer = transformers.AutoTokenizer.from_pretrained(&#xA;    model_name_or_path, model_max_length=2048, padding_side=&#34;right&#34;, use_fast=False)&#xA;tokenizer.pad_token = tokenizer.unk_token&#xA;&#xA;model = transformers.AutoModelForCausalLM.from_pretrained(&#xA;    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, loading ReFT artifacts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;reft_model = ReftModel.load(&#xA;    &#34;zhengxuanzenwu/Loreft1k-Llama-2-7b-hf&#34;, model, from_huggingface_hub=True)&#xA;reft_model.set_device(device)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start chatting with it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;instruction = &#34;Tell me about the NLP Group at Stanford University.&#34;&#xA;&#xA;# tokenize and prepare the input&#xA;prompt = prompt_no_input_template % instruction&#xA;prompt = tokenizer(prompt, return_tensors=&#34;pt&#34;).to(device)&#xA;intervention_locations = torch.tensor([get_intervention_locations(&#xA;    last_position=prompt[&#34;input_ids&#34;].shape[-1], positions=&#34;f5+l5&#34;,&#xA;    num_interventions=len(reft_model.interventions))]).permute(1, 0, 2).tolist()&#xA;&#xA;# generate&#xA;_, reft_response = reft_model.generate(&#xA;    prompt, &#xA;    unit_locations={&#34;sources-&amp;gt;base&#34;: (None, intervention_locations)},&#xA;    intervene_on_prompt=True, max_new_tokens=512, do_sample=False, &#xA;    no_repeat_ngram_size=5, repetition_penalty=1.1,&#xA;    eos_token_id=tokenizer.eos_token_id, early_stopping=True&#xA;)&#xA;print(tokenizer.decode(reft_response[0], skip_special_tokens=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that Llama-2 models can follow instructions zero-shot. We encourge people to try on other more primitive base LMs and see if ReFT can work well!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: Our chat-model is intended and licensed for research use only. The model is CC BY NC 4.0 (allowing only non-commercial use) should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Why you should use ReFT as oppose to PEFT?&lt;/h2&gt; &#xA;&lt;p&gt;There are various benefits such as saving memory and storage. In addition to that, ReFT is more interpretable and extensible than PEFT. The interventions we are learning is simply a causal abstraction of the task you are training without touching any model weights. The intervention site search space is large, and can be at any token position which is more flexibile. We showcase ReFT performance on various benchmarks against popular PEFT such as LoRA and its newer variants (e.g., DoRA) in our paper.&lt;/p&gt; &#xA;&lt;h2&gt;Learn more through examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/stanfordnlp/pyvene&#34;&gt;pyvene&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The backbone of pyreft library&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples/loreft&#34;&gt;LoReFT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reproduce our ReFT paper main results&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples/alpaca&#34;&gt;Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Instruction-tune LMs with ReFT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples/memorisation&#34;&gt;ReFT Interp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Some hints on why ReFT works&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/frankaging/pyreft/tree/main/examples/composition&#34;&gt;Composable ReFT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Some why ReFT is an interpretable method&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you cite the &lt;strong&gt;ReFT&lt;/strong&gt; paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wuandarora2024reft,&#xA;  title={ReFT: Representation Finetuning for Language Models},&#xA;  author={Wu, Zhengxuan* and Arora, Aryaman* and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher},&#xA;  booktitle={arXiv:2404.03592},&#xA;  url={arxiv.org/abs/2404.03592},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And please cite the &lt;strong&gt;pyvene&lt;/strong&gt; library paper as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wu2024pyvene,&#xA;  title={pyvene: A Library for Understanding and Improving {P}y{T}orch Models via Interventions},&#xA;  author={Wu, Zhengxuan and Geiger, Atticus and Arora, Aryaman and Huang, Jing and Wang, Zheng and Goodman, Noah D. and Manning, Christopher D. and Potts, Christopher},&#xA;  booktitle={arXiv:2403.07809},&#xA;  url={arxiv.org/abs/2403.07809},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Outreach&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in integrating this library into your workflow or in reimplementing it for improved efficiency, please feel free to contact us! We may have additional insights to share.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nashsu/FreeAskInternet</title>
    <updated>2024-04-08T01:34:31Z</updated>
    <id>tag:github.com,2024-04-08:/nashsu/FreeAskInternet</id>
    <link href="https://github.com/nashsu/FreeAskInternet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FreeAskInternet is a completely free, private and locally running search aggregator &amp; answer generate using LLM, without GPU needed. The user can ask a question and the system will make a multi engine search and combine the search result to the ChatGPT3.5 LLM and generate the answer based on search results.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FreeAskInternet&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Running &lt;a href=&#34;http://www.perplexity.ai&#34;&gt;www.perplexity.ai&lt;/a&gt; like app complete FREE, LOCAL, PRIVATE and NO GPU NEED on any computer&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; &lt;strong&gt;If you are unable to use this project normally, it is most likely due to issues with your internet connection or your IP, you need free internet connection to use this project normally. Â¶ÇÊûúÊÇ®Êó†Ê≥ïÊ≠£Â∏∏‰ΩøÁî®Ê≠§È°πÁõÆÔºåÂæàÂèØËÉΩÊòØÁî±‰∫éÊÇ®ÁöÑ IP Â≠òÂú®ÈóÆÈ¢òÔºåÊàñËÄÖ‰Ω†‰∏çËÉΩËá™Áî±ËÆøÈóÆ‰∫íËÅîÁΩë„ÄÇ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What is FreeAskInternet&lt;/h2&gt; &#xA;&lt;p&gt;FreeAskInternet is a completely free, private and locally running search aggregator &amp;amp; answer generate using LLM, Without GPU needed. The user can ask a question and the system will use searxng to make a multi engine search and combine the search result to the ChatGPT3.5 LLM and generate the answer based on search results. All process running locally and No GPU or OpenAI or Google API keys are needed.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üàöÔ∏è Completely FREE (no need for any API keys)&lt;/li&gt; &#xA; &lt;li&gt;üíª Completely LOCAL (no GPU need, any computer can run )&lt;/li&gt; &#xA; &lt;li&gt;üîê Completely PRIVATE (all thing runing locally)&lt;/li&gt; &#xA; &lt;li&gt;üëª Runs WITHOUT LLM Hardware (NO GPU NEED!)&lt;/li&gt; &#xA; &lt;li&gt;ü§© Using Free ChatGPT3.5 API (NO API keys need! Thx OpenAI)&lt;/li&gt; &#xA; &lt;li&gt;üöÄ Fast and easy to deploy with Docker Compose&lt;/li&gt; &#xA; &lt;li&gt;üåê Web and Mobile friendly interface, allowing for easy access from any device ( Thx ChatGPT-Next-Web )&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How It Works?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;System get user input question in ChatGPT-Next-Web ( running locally), and call searxng (running locally) to make search on multi search engine.&lt;/li&gt; &#xA; &lt;li&gt;crawl search result links content and pass to ChatGPT3.5 (using OpenAI ChatGPT3.5, through FreeGPT35 running locally), ask ChatGPT3.5 to answer user question based on this contents as references.&lt;/li&gt; &#xA; &lt;li&gt;Stream the answer to ChatGPT-Next-Web Chat UI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is still in its very early days. Expect some bugs.&lt;/p&gt; &#xA;&lt;h3&gt;Run the latest release&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/nashsu/FreeAskInternet.git&#xA;cd ./FreeAskInternet&#xA;docker-compose up -d &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üéâ You should now be able to open the web interface on &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;. Nothing else is exposed by default.&lt;/p&gt; &#xA;&lt;h3&gt;How to update to latest&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ./FreeAskInternet&#xA;git pull&#xA;docker compose rm backend&#xA;docker image rm nashsu/free_ask_internet&#xA;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ChatGPT-Next-Web : &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web&#34;&gt;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FreeGPT35: &lt;a href=&#34;https://github.com/missuo/FreeGPT35&#34;&gt;https://github.com/missuo/FreeGPT35&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;searxng: &lt;a href=&#34;https://github.com/searxng/searxng&#34;&gt;https://github.com/searxng/searxng&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache-2.0 license&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#nashsu/FreeAskInternet&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=nashsu/FreeAskInternet&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Marker-Inc-Korea/AutoRAG</title>
    <updated>2024-04-08T01:34:31Z</updated>
    <id>tag:github.com,2024-04-08:/Marker-Inc-Korea/AutoRAG</id>
    <link href="https://github.com/Marker-Inc-Korea/AutoRAG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RAG AutoML Tool - Find optimal RAG pipeline for your own data.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoRAG&lt;/h1&gt; &#xA;&lt;p&gt;RAG AutoML tool for automatically finds an optimal RAG pipeline for your data.&lt;/p&gt; &#xA;&lt;p&gt;Explore our üìñ &lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/&#34;&gt;Document&lt;/a&gt;!!&lt;/p&gt; &#xA;&lt;p&gt;Plus, join our üìû &lt;a href=&#34;https://discord.gg/P4DYXfmSAs&#34;&gt;Discord&lt;/a&gt; Community.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;üìå Colab Tutorial&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/19OEQXO_pHN6gnn2WdfPd4hjnS-4GurVd?usp=sharing&#34;&gt;Step 1: Basic of AutoRAG | Optimizing your RAG pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1HXjVHCLTaX7mkmZp3IKlEPt0B3jVeHvP#scrollTo=cgFUCuaUZvTr&#34;&gt;Step 2: Create evaluation dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;üö® YouTube Tutorial&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Marker-Inc-Korea/AutoRAG/assets/96727832/c0d23896-40c0-479f-a17b-aa2ec3183a26&#34;&gt;https://github.com/Marker-Inc-Korea/AutoRAG/assets/96727832/c0d23896-40c0-479f-a17b-aa2ec3183a26&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Muted by default, enable sound for voice-over&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can see on &lt;a href=&#34;https://youtu.be/2ojK8xjyXAU?feature=shared&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üìë Index&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-quick-install&#34;&gt;Quick Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-index&#34;&gt;Index&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-strengths&#34;&gt;Strengths&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-quickstart&#34;&gt;QuickStart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#1-prepare-your-evaluation-data&#34;&gt;1. Prepare your evaluation data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#2-evaluate-your-data-to-various-rag-modules&#34;&gt;2. Evaluate your data to various RAG modules&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#3-use-a-found-optimal-rag-pipeline&#34;&gt;3. Use a found optimal RAG pipeline&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#4-share-your-rag-pipeline&#34;&gt;4. Share your RAG pipeline&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-create-your-own-config-yaml-file&#34;&gt;+ Config yaml file&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#supporting-nodes--modules&#34;&gt;Supporting RAG modules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;There are many RAG pipelines and modules out there, but you don‚Äôt know what pipeline is great for ‚Äúyour own data‚Äù and &#34;your own use-case.&#34; Making and evaluating all RAG modules is very time-consuming and hard to do. But without it, you will never know which RAG pipeline is the best for your own use-case.&lt;/p&gt; &#xA;&lt;p&gt;AutoRAG is a tool for finding optimal RAG pipeline for ‚Äúyour data.‚Äù You can evaluate various RAG modules automatically with your own evaluation data, and find the best RAG pipeline for your own use-case.&lt;/p&gt; &#xA;&lt;p&gt;AutoRAG supports a simple way to evaluate many RAG module combinations. Try now and find the best RAG pipeline for your own use-case.&lt;/p&gt; &#xA;&lt;h1&gt;‚ö° Quick Install&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install AutoRAG&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üí™ Strengths&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;strong&gt;1. Find your RAG baseline&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Benchmark RAG pipelines with few lines of code. You can quickly get a high-performance RAG pipeline just for your data. Don‚Äôt waste time dealing with complex RAG modules and academic paper. Focus on your data.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;2. Analyze where is wrong&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Sometimes it is hard to keep tracking where is the major problem within your RAG pipeline. AutoRAG gives you the data of it, so you can analyze and focus where is the major problem and where you to focus on.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;3. Quick Starter Pack for your new RAG product&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Get the most effective RAG workflow among many pipelines, and start from there. Don‚Äôt start at toy-project level, start from advanced level.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;4. Share your experiment to others&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s really easy to share your experiment to others. Share your config yaml file and summary csv files. Plus, check out others result and adapt to your use-case.&lt;/p&gt; &#xA;&lt;h1&gt;‚ö° QuickStart&lt;/h1&gt; &#xA;&lt;h3&gt;1. Prepare your evaluation data&lt;/h3&gt; &#xA;&lt;p&gt;For evaluation, you need to prepare just three files.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;QA dataset file (qa.parquet)&lt;/li&gt; &#xA; &lt;li&gt;Corpus dataset file (corpus.parquet)&lt;/li&gt; &#xA; &lt;li&gt;Config yaml file (config.yaml)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There is a template for your evaluation data for using AutoRAG.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out how to make evaluation data at &lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/data_creation/tutorial.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Check out the evaluation data rule at &lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/data_creation/data_format.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Plus, you can get example datasets for testing AutoRAG at &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/sample_dataset&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. Evaluate your data to various RAG modules&lt;/h3&gt; &#xA;&lt;p&gt;You can get various config yaml files at &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/sample_config&#34;&gt;here&lt;/a&gt;. We highly recommend using pre-made config yaml files for starter.&lt;/p&gt; &#xA;&lt;p&gt;If you want to make your own config yaml files, check out the &lt;a href=&#34;https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/#-create-your-own-config-yaml-file&#34;&gt;Config yaml file&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;You can evaluate your RAG pipeline with just a few lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.evaluator import Evaluator&#xA;&#xA;evaluator = Evaluator(qa_data_path=&#39;your/path/to/qa.parquet&#39;, corpus_data_path=&#39;your/path/to/corpus.parquet&#39;)&#xA;evaluator.start_trial(&#39;your/path/to/config.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can use command line interface&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autorag evaluate --config your/path/to/default_config.yaml --qa_data_path your/path/to/qa.parquet --corpus_data_path your/path/to/corpus.parquet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once it is done, you can see several files and folders created at your current directory. At the trial folder named to numbers (like 0), you can check &lt;code&gt;summary.csv&lt;/code&gt; file that summarizes the evaluation results and the best RAG pipeline for your data.&lt;/p&gt; &#xA;&lt;p&gt;For more details, you can check out how the folder structure looks like at &lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/optimization/folder_structure.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3. Use a found optimal RAG pipeline&lt;/h3&gt; &#xA;&lt;p&gt;You can use a found optimal RAG pipeline right away. It needs just a few lines of code, and you are ready to use!&lt;/p&gt; &#xA;&lt;p&gt;First, you need to build pipeline yaml file from your evaluated trial folder. You can find the trial folder in your current directory. Just looking folder like &#39;0&#39; or other numbers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.deploy import Runner&#xA;&#xA;runner = Runner.from_trial_folder(&#39;your/path/to/trial_folder&#39;)&#xA;runner.run(&#39;your question&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, you can run this pipeline as api server. You can use python code or CLI command. Check out API endpoint at &lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/deploy/api_endpoint.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.deploy import Runner&#xA;&#xA;runner = Runner.from_trial_folder(&#39;your/path/to/trial_folder&#39;)&#xA;runner.run_api_server()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can run api server with CLI command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autorag run_api --config_path your/path/to/pipeline.yaml --host 0.0.0.0 --port 8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Share your RAG pipeline&lt;/h3&gt; &#xA;&lt;p&gt;You can use your RAG pipeline from extracted pipeline yaml file. This extracted pipeline is great for sharing your RAG pipeline to others.&lt;/p&gt; &#xA;&lt;p&gt;You must run this at project folder, which contains datas in data folder, and ingested corpus for retrieval at resources folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from autorag.deploy import extract_best_config&#xA;&#xA;pipeline_dict = extract_best_config(trial_path=&#39;your/path/to/trial_folder&#39;, output_path=&#39;your/path/to/pipeline.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;‚ûï Create your own Config yaml file&lt;/h3&gt; &#xA;&lt;p&gt;You can build your own evaluation process with config yaml file. You can check detailed explanation how to configure each module and node at &lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/index.html#&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There is a simple yaml file example.&lt;/p&gt; &#xA;&lt;p&gt;It evaluates two retrieval modules which are BM25 and Vector Retriever, and three reranking modules. Lastly, it generates prompt and makes generation with two other LLM models and three temperatures.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;node_lines:&#xA;  - node_line_name: retrieve_node_line&#xA;    nodes:&#xA;      - node_type: retrieval&#xA;        strategy:&#xA;          metric: retrieval_f1&#xA;        top_k: 50&#xA;        modules:&#xA;          - module_type: bm25&#xA;          - module_type: vector&#xA;            embedding_model: [ openai, openai_embed_3_large ]&#xA;          - module_type: hybrid_rrf&#xA;            target_modules: (&#39;bm25&#39;, &#39;vectordb&#39;)&#xA;            rrf_k: [ 3, 5, 10 ]&#xA;      - node_type: reranker&#xA;        strategy:&#xA;          metric: retrieval_precision&#xA;          speed_threshold: 5&#xA;        top_k: 3&#xA;        modules:&#xA;          - module_type: upr&#xA;          - module_type: tart&#xA;            prompt: Arrange the following sentences in the correct order.&#xA;          - module_type: monoT5&#xA;  - node_line_name: generate_node_line&#xA;    nodes:&#xA;      - node_type: prompt_maker&#xA;        modules:&#xA;          - module_type: fstring&#xA;            prompt: &#34;This is a news dataset, crawled from finance news site. You need to make detailed question about finance news. Do not make questions that not relevant to economy or finance domain.\n{retrieved_contents}\n\nQ: {query}\nA:&#34;&#xA;      - node_type: generator&#xA;        strategy:&#xA;          metric:&#xA;            - metric_name: meteor&#xA;            - metric_name: rouge&#xA;            - metric_name: sem_score&#xA;              embedding_model: openai&#xA;            - metric_name: g_eval&#xA;              model: gpt-3.5-turbo&#xA;        modules:&#xA;          - module_type: llama_index_llm&#xA;            llm: openai&#xA;            model: [ gpt-3.5-turbo-16k, gpt-3.5-turbo-1106 ]&#xA;            temperature: [ 0.5, 1.0, 1.5 ]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;‚ùóSupporting Nodes &amp;amp; modules&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Nodes&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Modules&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/query_expansion/query_expansion.html&#34;&gt;Query_Expansion&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/query_expansion/query_decompose.html&#34;&gt;Query_Decompose&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/query_expansion/hyde.html&#34;&gt;HyDE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/retrieval/retrieval.html&#34;&gt;Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/retrieval/bm25.html&#34;&gt;BM25&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/retrieval/vectordb.html&#34;&gt;VectorDB (choose embedding model)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/retrieval/hybrid_rrf.html&#34;&gt;Hybrid with rrf (reciprocal rank fusion)&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/retrieval/hybrid_cc.html&#34;&gt;Hybrid with cc (convex combination)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/passage_reranker.html&#34;&gt;Passage_Reranker&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/upr.html&#34;&gt;UPR&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/tart.html&#34;&gt;Tart&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/monot5.html&#34;&gt;MonoT5&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/koreranker.html&#34;&gt;Ko-reranker&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/cohere.html&#34;&gt;cohere_reranker&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/rankgpt.html&#34;&gt;RankGPT&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_reranker/jina_reranker.html&#34;&gt;Jina Reranker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_compressor/passage_compressor.html&#34;&gt;Passage_Compressor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/passage_compressor/tree_summarize.html&#34;&gt;Tree Summarize&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/prompt_maker/prompt_maker.html&#34;&gt;Prompt Maker&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/prompt_maker/fstring.html&#34;&gt;Default Prompt Maker (f-string)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/generator/generator.html&#34;&gt;Generator&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/generator/llama_index_llm.html&#34;&gt;llama_index llm&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/nodes/generator/vllm.html&#34;&gt;vllm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;üõ£Roadmap&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Policy Module for modular RAG pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Visualize evaluation result&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Visualize config yaml file&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More RAG modules support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Token usage strategy&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Multi-modal support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More evaluation metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Answer Filtering Module&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Restart optimization from previous trial&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;We are developing AutoRAG as open-source.&lt;/p&gt; &#xA;&lt;p&gt;So this project welcomes contributions and suggestions. Feel free to contribute to this project.&lt;/p&gt; &#xA;&lt;p&gt;Plus, check out our detailed documentation at &lt;a href=&#34;https://marker-inc-korea.github.io/AutoRAG/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>