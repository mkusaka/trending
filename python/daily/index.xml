<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-20T01:42:29Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Dao-AILab/flash-attention</title>
    <updated>2023-07-20T01:42:29Z</updated>
    <id>tag:github.com,2023-07-20:/Dao-AILab/flash-attention</id>
    <link href="https://github.com/Dao-AILab/flash-attention" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast and memory-efficient exact attention&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlashAttention&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides the official implementation of FlashAttention and FlashAttention-2 from the following papers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/strong&gt;&lt;br&gt; Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©&lt;br&gt; Paper: &lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;&lt;br&gt; IEEE Spectrum &lt;a href=&#34;https://spectrum.ieee.org/mlperf-rankings-2022&#34;&gt;article&lt;/a&gt; about our submission to the MLPerf 2.0 benchmark using FlashAttention. &lt;img src=&#34;https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattn_banner.jpg&#34; alt=&#34;FlashAttention&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning&lt;/strong&gt;&lt;br&gt; Tri Dao&lt;/p&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://tridao.me/publications/flash2/flash2.pdf&#34;&gt;https://tridao.me/publications/flash2/flash2.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattention_logo.png&#34; alt=&#34;FlashAttention-2&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve been very happy to see FlashAttention being widely adopted in such a short time after its release. This &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/raw/main/usage.md&#34;&gt;page&lt;/a&gt; contains a partial list of places where FlashAttention is being used.&lt;/p&gt; &#xA;&lt;p&gt;FlashAttention and FlashAttention-2 are free to use and modify (see LICENSE). Please cite and credit FlashAttention if you use it.&lt;/p&gt; &#xA;&lt;h2&gt;Installation and features&lt;/h2&gt; &#xA;&lt;p&gt;Requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA 11.4 and above.&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.12 and above.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We recommend the &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch&#34;&gt;Pytorch&lt;/a&gt; container from Nvidia, which has all the required tools to install FlashAttention.&lt;/p&gt; &#xA;&lt;p&gt;To install:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure that PyTorch is installed.&lt;/li&gt; &#xA; &lt;li&gt;Make sure that &lt;code&gt;packaging&lt;/code&gt; is installed (&lt;code&gt;pip install packaging&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Make sure that &lt;code&gt;ninja&lt;/code&gt; is installed and that it works correctly (e.g. &lt;code&gt;ninja --version&lt;/code&gt; then &lt;code&gt;echo $?&lt;/code&gt; should return exit code 0). If not (sometimes &lt;code&gt;ninja --version&lt;/code&gt; then &lt;code&gt;echo $?&lt;/code&gt; returns a nonzero exit code), uninstall then reinstall &lt;code&gt;ninja&lt;/code&gt; (&lt;code&gt;pip uninstall -y ninja &amp;amp;&amp;amp; pip install ninja&lt;/code&gt;). Without &lt;code&gt;ninja&lt;/code&gt;, compiling can take a very long time (2h) since it does not use multiple CPU cores. With &lt;code&gt;ninja&lt;/code&gt; compiling takes 3-5 minutes on a 64-core machine.&lt;/li&gt; &#xA; &lt;li&gt;Then:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively you can compile from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your machine has less than 96GB of RAM and lots of CPU cores, &lt;code&gt;ninja&lt;/code&gt; might run too many parallel compilation jobs that could exhaust the amount of RAM. To limit the number of parallel compilation jobs, you can set the environment variable &lt;code&gt;MAX_JOBS&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MAX_JOBS=4 pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Interface: &lt;code&gt;src/flash_attention_interface.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;FlashAttention-2 currently supports:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now.&lt;/li&gt; &#xA; &lt;li&gt;Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).&lt;/li&gt; &#xA; &lt;li&gt;All head dimensions up to 256. Head dim &amp;gt; 192 backward requires A100/A800 or H100/H800.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to use FlashAttention&lt;/h2&gt; &#xA;&lt;p&gt;The main functions implement scaled dot product attention (softmax(Q @ K^T * softmax_scale) @ V):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from flash_attn import flash_attn_qkvpacked_func, flash_attn_func&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;flash_attn_qkvpacked_func(qkv, dropout_p=0.0, softmax_scale=None, causal=False):&#xA;&#34;&#34;&#34;dropout_p should be set to 0.0 during evaluation&#xA;If Q, K, V are already stacked into 1 tensor, this function will be faster than&#xA;calling flash_attn_func on Q, K, V since the backward pass avoids explicit concatenation&#xA;of the gradients of Q, K, V.&#xA;Arguments:&#xA;    qkv: (batch_size, seqlen, 3, nheads, headdim)&#xA;    dropout_p: float. Dropout probability.&#xA;    softmax_scale: float. The scaling of QK^T before applying softmax.&#xA;        Default to 1 / sqrt(headdim).&#xA;    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).&#xA;Return:&#xA;    out: (batch_size, seqlen, nheads, headdim).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False):&#xA;&#34;&#34;&#34;dropout_p should be set to 0.0 during evaluation&#xA;Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads&#xA;than Q. Note that the number of heads in KV must be divisible by the number of heads in Q.&#xA;For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head&#xA;0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.&#xA;&#xA;Arguments:&#xA;    q: (batch_size, seqlen, nheads, headdim)&#xA;    k: (batch_size, seqlen, nheads_k, headdim)&#xA;    v: (batch_size, seqlen, nheads_k, headdim)&#xA;    dropout_p: float. Dropout probability.&#xA;    softmax_scale: float. The scaling of QK^T before applying softmax.&#xA;        Default to 1 / sqrt(headdim).&#xA;    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).&#xA;Return:&#xA;    out: (batch_size, seqlen, nheads, headdim).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see how these functions are used in a multi-head attention layer (which includes QKV projection, output projection), see the MHA &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/modules/mha.py&#34;&gt;implementation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Upgrading from FlashAttention (1.x) to FlashAttention-2&lt;/h2&gt; &#xA;&lt;p&gt;These functions have been renamed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;flash_attn_unpadded_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_func&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;flash_attn_unpadded_qkvpacked_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_qkvpacked_func&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;flash_attn_unpadded_kvpacked_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_kvpacked_func&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If the inputs have the same sequence lengths in the same batch, it is simpler and faster to use these functions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;flash_attn_qkvpacked_func(qkv, dropout_p, softmax_scale=None, causal=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;We present expected speedup (combined forward + backward pass) and memory savings from using FlashAttention against PyTorch standard attention, depending on sequence length, on different GPUs (speedup depends on memory bandwidth - we see more speedup on slower GPU memory).&lt;/p&gt; &#xA;&lt;p&gt;We currently have benchmarks for these GPUs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/#a100&#34;&gt;A100&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/#h100&#34;&gt;H100&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- * [RTX 3090](#rtx-3090) --&gt; &#xA;&lt;!-- * [T4](#t4) --&gt; &#xA;&lt;h3&gt;A100&lt;/h3&gt; &#xA;&lt;p&gt;We display FlashAttention speedup using these parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Head dimension 64 or 128, hidden dimension 2048 (i.e. either 32 or 16 heads).&lt;/li&gt; &#xA; &lt;li&gt;Sequence length 512, 1k, 2k, 4k, 8k, 16k.&lt;/li&gt; &#xA; &lt;li&gt;Batch size set to 16k / seqlen.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Speedup&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash2_a100_fwd_bwd_benchmark.png&#34; alt=&#34;FlashAttention speedup on A100 80GB SXM5 with FP16/BF16&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Memory&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattn_memory.jpg&#34; alt=&#34;FlashAttention memory&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We show memory savings in this graph (note that memory footprint is the same no matter if you use dropout or masking). Memory savings are proportional to sequence length -- since standard attention has memory quadratic in sequence length, whereas FlashAttention has memory linear in sequence length. We see 10X memory savings at sequence length 2K, and 20X at 4K. As a result, FlashAttention can scale to much longer sequence lengths.&lt;/p&gt; &#xA;&lt;h3&gt;H100&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash2_h100_fwd_bwd_benchmark.png&#34; alt=&#34;FlashAttention speedup on H100 SXM5 with FP16/BF16&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Full model code and training script&lt;/h2&gt; &#xA;&lt;p&gt;We have released the full GPT model &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/models/gpt.py&#34;&gt;implementation&lt;/a&gt;. We also provide optimized implementations of other layers (e.g., MLP, LayerNorm, cross-entropy loss, rotary embedding). Overall this speeds up training by 3-5x compared to the baseline implementation from Huggingface, reaching up to 225 TFLOPs/sec per A100, equivalent to 72% model FLOPs utilization (we don&#39;t need any activation checkpointing).&lt;/p&gt; &#xA;&lt;p&gt;We also include a training &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/tree/main/training&#34;&gt;script&lt;/a&gt; to train GPT2 on Openwebtext and GPT3 on The Pile.&lt;/p&gt; &#xA;&lt;h2&gt;Triton implementation of FlashAttention&lt;/h2&gt; &#xA;&lt;p&gt;Phil Tillet (OpenAI) has an experimental implementation of FlashAttention in Triton: &lt;a href=&#34;https://github.com/openai/triton/raw/master/python/tutorials/06-fused-attention.py&#34;&gt;https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;As Triton is a higher-level language than CUDA, it might be easier to understand and experiment with. The notations in the Triton implementation are also closer to what&#39;s used in our paper.&lt;/p&gt; &#xA;&lt;p&gt;We also have an experimental implementation in Triton that support attention bias (e.g. ALiBi): &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/flash_attn_triton.py&#34;&gt;https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/flash_attn_triton.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;We test that FlashAttention produces the same output and gradient as a reference implementation, up to some numerical tolerance. In particular, we check that the maximum numerical error of FlashAttention is at most twice the numerical error of a baseline implementation in Pytorch (for different head dimensions, input dtype, sequence length, causal / non-causal).&lt;/p&gt; &#xA;&lt;p&gt;To run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest -q -s tests/test_flash_attn.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;When you encounter issues&lt;/h2&gt; &#xA;&lt;p&gt;This new release of FlashAttention-2 has been tested on several GPT-style models, mostly on A100 GPUs.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter bugs, please open a GitHub Issue!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{dao2022flashattention,&#xA;  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},&#xA;  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\&#39;e}, Christopher},&#xA;  booktitle={Advances in Neural Information Processing Systems},&#xA;  year={2022}&#xA;}&#xA;@article{dao2023flashattention2,&#xA;  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning,&#xA;  author={Dao, Tri},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/optimum</title>
    <updated>2023-07-20T01:42:29Z</updated>
    <id>tag:github.com,2023-07-20:/huggingface/optimum</id>
    <link href="https://github.com/huggingface/optimum" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ð Accelerate training and inference of ð¤ Transformers and ð¤ Diffusers with easy to use hardware optimization tools&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml/badge.svg?sanitize=true&#34; alt=&#34;ONNX Runtime&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Hugging Face Optimum&lt;/h1&gt; &#xA;&lt;p&gt;ð¤ Optimum is an extension of ð¤ Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;ð¤ Optimum can be installed using &lt;code&gt;pip&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install optimum&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to use the accelerator-specific features of ð¤ Optimum, you can install the required dependencies according to the table below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Accelerator&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Installation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://onnxruntime.ai/docs/&#34;&gt;ONNX Runtime&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;python -m pip install optimum[onnxruntime]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html&#34;&gt;Intel Neural Compressor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;python -m pip install optimum[neural-compressor]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/index.html&#34;&gt;OpenVINO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;python -m pip install optimum[openvino,nncf]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://habana.ai/training/&#34;&gt;Habana Gaudi Processor (HPU)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;python -m pip install optimum[habana]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To install from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install git+https://github.com/huggingface/optimum.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the accelerator-specific features, append &lt;code&gt;#egg=optimum[accelerator_type]&lt;/code&gt; to the above command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Accelerated Inference&lt;/h2&gt; &#xA;&lt;p&gt;ð¤ Optimum provides multiple tools to export and run optimized models on various ecosystems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model&#34;&gt;ONNX&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models&#34;&gt;ONNX Runtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TensorFlow Lite&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/optimum/intel/inference&#34;&gt;OpenVINO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Habana first-gen Gaudi / Gaudi2, more details &lt;a href=&#34;https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://huggingface.co/docs/optimum/exporters/overview&#34;&gt;export&lt;/a&gt; and optimizations can be done both programmatically and with a command line.&lt;/p&gt; &#xA;&lt;h3&gt;Features summary&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Features&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/docs/optimum/main/en/onnxruntime/overview&#34;&gt;ONNX Runtime&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/docs/optimum/main/en/intel/optimization_inc&#34;&gt;Neural Compressor&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/docs/optimum/main/en/intel/inference&#34;&gt;OpenVINO&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview&#34;&gt;TensorFlow Lite&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Graph optimization&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Post-training dynamic quantization&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Post-training static quantization&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Quantization Aware Training (QAT)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FP16 (half precision)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pruning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Knowledge Distillation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;â&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;OpenVINO&lt;/h3&gt; &#xA;&lt;p&gt;This requires to install the OpenVINO extra by doing &lt;code&gt;pip install optimum[openvino,nncf]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To load a model and run inference with OpenVINO Runtime, you can just replace your &lt;code&gt;AutoModelForXxx&lt;/code&gt; class with the corresponding &lt;code&gt;OVModelForXxx&lt;/code&gt; class. To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly, you can set &lt;code&gt;export=True&lt;/code&gt; when loading your model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- from transformers import AutoModelForSequenceClassification&#xA;+ from optimum.intel import OVModelForSequenceClassification&#xA;  from transformers import AutoTokenizer, pipeline&#xA;&#xA;  model_id = &#34;distilbert-base-uncased-finetuned-sst-2-english&#34;&#xA;  tokenizer = AutoTokenizer.from_pretrained(model_id)&#xA;- model = AutoModelForSequenceClassification.from_pretrained(model_id)&#xA;+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)&#xA;  model.save_pretrained(&#34;./distilbert&#34;)&#xA;&#xA;  classifier = pipeline(&#34;text-classification&#34;, model=model, tokenizer=tokenizer)&#xA;  results = classifier(&#34;He&#39;s a dreadful magician.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find more examples in the &lt;a href=&#34;https://huggingface.co/docs/optimum/intel/inference&#34;&gt;documentation&lt;/a&gt; and in the &lt;a href=&#34;https://github.com/huggingface/optimum-intel/tree/main/examples/openvino&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Neural Compressor&lt;/h3&gt; &#xA;&lt;p&gt;This requires to install the Neural Compressor extra by doing &lt;code&gt;pip install optimum[neural-compressor]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dynamic quantization can be applied on your model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;optimum-cli inc quantize --model distilbert-base-cased-distilled-squad --output ./quantized_distilbert&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To load a model quantized with Intel Neural Compressor, hosted locally or on the ð¤ hub, you can do as follows :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from optimum.intel import INCModelForSequenceClassification&#xA;&#xA;model_id = &#34;Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic&#34;&#xA;model = INCModelForSequenceClassification.from_pretrained(model_id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find more examples in the &lt;a href=&#34;https://huggingface.co/docs/optimum/intel/optimization_inc&#34;&gt;documentation&lt;/a&gt; and in the &lt;a href=&#34;https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ONNX + ONNX Runtime&lt;/h3&gt; &#xA;&lt;p&gt;This requires to install the ONNX Runtime extra by doing &lt;code&gt;pip install optimum[exporters,onnxruntime]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;It is possible to export ð¤ Transformers models to the &lt;a href=&#34;https://onnx.ai/&#34;&gt;ONNX&lt;/a&gt; format and perform graph optimization as well as quantization easily:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;optimum-cli export onnx -m deepset/roberta-base-squad2 --optimize O2 roberta_base_qa_onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model can then be quantized using &lt;code&gt;onnxruntime&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;optimum-cli onnxruntime quantize \&#xA;  --avx512 \&#xA;  --onnx_model roberta_base_qa_onnx \&#xA;  -o quantized_roberta_base_qa_onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These commands will export &lt;code&gt;deepset/roberta-base-squad2&lt;/code&gt; and perform &lt;a href=&#34;https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization#optimization-configuration&#34;&gt;O2 graph optimization&lt;/a&gt; on the exported model, and finally quantize it with the &lt;a href=&#34;https://huggingface.co/docs/optimum/main/en/onnxruntime/package_reference/configuration#optimum.onnxruntime.AutoQuantizationConfig.avx512&#34;&gt;avx512 configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more information on the ONNX export, please check the &lt;a href=&#34;https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Run the exported model using ONNX Runtime&lt;/h4&gt; &#xA;&lt;p&gt;Once the model is exported to the ONNX format, we provide Python classes enabling you to run the exported ONNX model in a seemless manner using &lt;a href=&#34;https://onnxruntime.ai/&#34;&gt;ONNX Runtime&lt;/a&gt; in the backend:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- from transformers import AutoModelForQuestionAnswering&#xA;+ from optimum.onnxruntime import ORTModelForQuestionAnswering&#xA;  from transformers import AutoTokenizer, pipeline&#xA;&#xA;  model_id = &#34;deepset/roberta-base-squad2&#34;&#xA;  tokenizer = AutoTokenizer.from_pretrained(model_id)&#xA;- model = AutoModelForQuestionAnswering.from_pretrained(model_id)&#xA;+ model = ORTModelForQuestionAnswering.from_pretrained(&#34;roberta_base_qa_onnx&#34;)&#xA;  qa_pipe = pipeline(&#34;question-answering&#34;, model=model, tokenizer=tokenizer)&#xA;  question = &#34;What&#39;s Optimum?&#34;&#xA;  context = &#34;Optimum is an awesome library everyone should use!&#34;&#xA;  results = qa_pipe(question=question, context=context)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More details on how to run ONNX models with &lt;code&gt;ORTModelForXXX&lt;/code&gt; classes &lt;a href=&#34;https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;TensorFlow Lite&lt;/h3&gt; &#xA;&lt;p&gt;This requires to install the Exporters extra by doing &lt;code&gt;pip install optimum[exporters-tf]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just as for ONNX, it is possible to export models to &lt;a href=&#34;https://www.tensorflow.org/lite&#34;&gt;TensorFlow Lite&lt;/a&gt; and quantize them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;optimum-cli export tflite \&#xA;  -m deepset/roberta-base-squad2 \&#xA;  --sequence_length 384  \&#xA;  --quantize int8-dynamic roberta_tflite_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Accelerated training&lt;/h2&gt; &#xA;&lt;p&gt;ð¤ Optimum provides wrappers around the original ð¤ Transformers &lt;a href=&#34;https://huggingface.co/docs/transformers/main_classes/trainer&#34;&gt;Trainer&lt;/a&gt; to enable training on powerful hardware easily. We support many providers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Habana&#39;s Gaudi processors&lt;/li&gt; &#xA; &lt;li&gt;ONNX Runtime (optimized for GPUs)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Habana&lt;/h3&gt; &#xA;&lt;p&gt;This requires to install the Habana extra by doing &lt;code&gt;pip install optimum[habana]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- from transformers import Trainer, TrainingArguments&#xA;+ from optimum.habana import GaudiTrainer, GaudiTrainingArguments&#xA;&#xA;  # Download a pretrained model from the Hub&#xA;  model = AutoModelForXxx.from_pretrained(&#34;bert-base-uncased&#34;)&#xA;&#xA;  # Define the training arguments&#xA;- training_args = TrainingArguments(&#xA;+ training_args = GaudiTrainingArguments(&#xA;      output_dir=&#34;path/to/save/folder/&#34;,&#xA;+     use_habana=True,&#xA;+     use_lazy_mode=True,&#xA;+     gaudi_config_name=&#34;Habana/bert-base-uncased&#34;,&#xA;      ...&#xA;  )&#xA;&#xA;  # Initialize the trainer&#xA;- trainer = Trainer(&#xA;+ trainer = GaudiTrainer(&#xA;      model=model,&#xA;      args=training_args,&#xA;      train_dataset=train_dataset,&#xA;      ...&#xA;  )&#xA;&#xA;  # Use Habana Gaudi processor for training!&#xA;  trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find more examples in the &lt;a href=&#34;https://huggingface.co/docs/optimum/habana/quickstart&#34;&gt;documentation&lt;/a&gt; and in the &lt;a href=&#34;https://github.com/huggingface/optimum-habana/tree/main/examples&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ONNX Runtime&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;- from transformers import Trainer, TrainingArguments&#xA;+ from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments&#xA;&#xA;  # Download a pretrained model from the Hub&#xA;  model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;)&#xA;&#xA;  # Define the training arguments&#xA;- training_args = TrainingArguments(&#xA;+ training_args = ORTTrainingArguments(&#xA;      output_dir=&#34;path/to/save/folder/&#34;,&#xA;      optim=&#34;adamw_ort_fused&#34;,&#xA;      ...&#xA;  )&#xA;&#xA;  # Create a ONNX Runtime Trainer&#xA;- trainer = Trainer(&#xA;+ trainer = ORTTrainer(&#xA;      model=model,&#xA;      args=training_args,&#xA;      train_dataset=train_dataset,&#xA;+     feature=&#34;sequence-classification&#34;, # The model type to export to ONNX&#xA;      ...&#xA;  )&#xA;&#xA;  # Use ONNX Runtime for training!&#xA;  trainer.train()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find more examples in the &lt;a href=&#34;https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer&#34;&gt;documentation&lt;/a&gt; and in the &lt;a href=&#34;https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training&#34;&gt;examples&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Rudrabha/Wav2Lip</title>
    <updated>2023-07-20T01:42:29Z</updated>
    <id>tag:github.com,2023-07-20:/Rudrabha/Wav2Lip</id>
    <link href="https://github.com/Rudrabha/Wav2Lip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains the codes of &#34;A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild&#34;, published at ACM Multimedia 2020.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;strong&gt;Wav2Lip&lt;/strong&gt;: &lt;em&gt;Accurately Lip-syncing Videos In The Wild&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;For commercial requests, please contact us at &lt;a href=&#34;mailto:radrabha.m@research.iiit.ac.in&#34;&gt;radrabha.m@research.iiit.ac.in&lt;/a&gt; or &lt;a href=&#34;mailto:prajwal.k@research.iiit.ac.in&#34;&gt;prajwal.k@research.iiit.ac.in&lt;/a&gt;. We have an HD model ready that can be used commercially.&lt;/p&gt; &#xA;&lt;p&gt;This code is part of the paper: &lt;em&gt;A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild&lt;/em&gt; published at ACM Multimedia 2020.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrs2?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrs3?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrs3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/lip-sync-on-lrw?p=a-lip-sync-expert-is-all-you-need-for-speech&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-lip-sync-expert-is-all-you-need-for-speech/lip-sync-on-lrw&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ð Original Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ð° Project Page&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ð Demo&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;â¡ Live Testing&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ð Colab Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://arxiv.org/abs/2008.10010&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/&#34;&gt;Project Page&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://youtu.be/0fXaDCZNOJc&#34;&gt;Demo Video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;Interactive Demo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1tZpDWXz49W6wDcTprANRGLo2D_EbD5J8?usp=sharing&#34;&gt;Colab Notebook&lt;/a&gt; /&lt;a href=&#34;https://colab.research.google.com/drive/1IjFW1cLevs6Ouyu4Yht4mnR4yeuMqO7Y#scrollTo=MH1m608OymLH&#34;&gt;Updated Collab Notebook&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;img src=&#34;https://drive.google.com/uc?export=view&amp;amp;id=1Wn0hPmpo4GRbCIJR8Tf20Akzdi1qjjG9&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Weights of the visual quality disc has been updated in readme!&lt;/li&gt; &#xA; &lt;li&gt;Lip-sync videos to any target speech with high accuracy &lt;span&gt;ð¯&lt;/span&gt;. Try our &lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;interactive demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;â¨&lt;/span&gt; Works for any identity, voice, and language. Also works for CGI faces and synthetic voices.&lt;/li&gt; &#xA; &lt;li&gt;Complete training code, inference code, and pretrained models are available &lt;span&gt;ð¥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Or, quick-start with the Google Colab Notebook: &lt;a href=&#34;https://colab.research.google.com/drive/1tZpDWXz49W6wDcTprANRGLo2D_EbD5J8?usp=sharing&#34;&gt;Link&lt;/a&gt;. Checkpoints and samples are available in a Google Drive &lt;a href=&#34;https://drive.google.com/drive/folders/1I-0dNLfFOSFwrfqjNa-SXuwaURHE5K4k?usp=sharing&#34;&gt;folder&lt;/a&gt; as well. There is also a &lt;a href=&#34;https://www.youtube.com/watch?v=Ic0TBhfuOrA&#34;&gt;tutorial video&lt;/a&gt; on this, courtesy of &lt;a href=&#34;https://www.youtube.com/channel/UCmGXH-jy0o2CuhqtpxbaQgA&#34;&gt;What Make Art&lt;/a&gt;. Also, thanks to &lt;a href=&#34;https://eyalgruss.com&#34;&gt;Eyal Gruss&lt;/a&gt;, there is a more accessible &lt;a href=&#34;https://j.mp/wav2lip&#34;&gt;Google Colab notebook&lt;/a&gt; with more useful features. A tutorial collab notebook is present at this &lt;a href=&#34;https://colab.research.google.com/drive/1IjFW1cLevs6Ouyu4Yht4mnR4yeuMqO7Y#scrollTo=MH1m608OymLH&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;ð¥&lt;/span&gt; &lt;span&gt;ð¥&lt;/span&gt; Several new, reliable evaluation benchmarks and metrics &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip/tree/master/evaluation&#34;&gt;[&lt;code&gt;evaluation/&lt;/code&gt; folder of this repo]&lt;/a&gt; released. Instructions to calculate the metrics reported in the paper are also present.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;All results from this open-source code or our &lt;a href=&#34;https://bhaasha.iiit.ac.in/lipsync&#34;&gt;demo website&lt;/a&gt; should only be used for research/academic/personal purposes only. As the models are trained on the &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html&#34;&gt;LRS2 dataset&lt;/a&gt;, any form of commercial use is strictly prohibited. For commercial requests please contact us directly!&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Python 3.6&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ffmpeg: &lt;code&gt;sudo apt-get install ffmpeg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install necessary packages using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;. Alternatively, instructions for using a docker image is provided &lt;a href=&#34;https://gist.github.com/xenogenesi/e62d3d13dadbc164124c830e9c453668&#34;&gt;here&lt;/a&gt;. Have a look at &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip/issues/131#issuecomment-725478562&#34;&gt;this comment&lt;/a&gt; and comment on &lt;a href=&#34;https://gist.github.com/xenogenesi/e62d3d13dadbc164124c830e9c453668&#34;&gt;the gist&lt;/a&gt; if you encounter any issues.&lt;/li&gt; &#xA; &lt;li&gt;Face detection &lt;a href=&#34;https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth&#34;&gt;pre-trained model&lt;/a&gt; should be downloaded to &lt;code&gt;face_detection/detection/sfd/s3fd.pth&lt;/code&gt;. Alternative &lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/prajwal_k_research_iiit_ac_in/EZsy6qWuivtDnANIG73iHjIBjMSoojcIV0NULXV-yiuiIg?e=qTasa8&#34;&gt;link&lt;/a&gt; if the above does not work.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting the weights&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link to the model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wav2Lip&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Highly accurate lip-sync&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wav2Lip + GAN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Slightly inferior lip-sync, but better visual quality&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Expert Discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Weights of the expert discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visual Quality Discriminator&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Weights of the visual disc trained in a GAN setup&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQVqH88dTm1HjlK11eNba5gBbn15WMS0B0EZbDBttqrqkg?e=ic0ljo&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Lip-syncing videos using the pre-trained models (Inference)&lt;/h2&gt; &#xA;&lt;p&gt;You can lip-sync any video to any audio:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --checkpoint_path &amp;lt;ckpt&amp;gt; --face &amp;lt;video.mp4&amp;gt; --audio &amp;lt;an-audio-source&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result is saved (by default) in &lt;code&gt;results/result_voice.mp4&lt;/code&gt;. You can specify it as an argument, similar to several other available options. The audio source can be any file supported by &lt;code&gt;FFMPEG&lt;/code&gt; containing audio data: &lt;code&gt;*.wav&lt;/code&gt;, &lt;code&gt;*.mp3&lt;/code&gt; or even a video file, from which the code will automatically extract the audio.&lt;/p&gt; &#xA;&lt;h5&gt;Tips for better results:&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiment with the &lt;code&gt;--pads&lt;/code&gt; argument to adjust the detected face bounding box. Often leads to improved results. You might need to increase the bottom padding to include the chin region. E.g. &lt;code&gt;--pads 0 20 0 0&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you see the mouth position dislocated or some weird artifacts such as two mouths, then it can be because of over-smoothing the face detections. Use the &lt;code&gt;--nosmooth&lt;/code&gt; argument and give it another try.&lt;/li&gt; &#xA; &lt;li&gt;Experiment with the &lt;code&gt;--resize_factor&lt;/code&gt; argument, to get a lower-resolution video. Why? The models are trained on faces that were at a lower resolution. You might get better, visually pleasing results for 720p videos than for 1080p videos (in many cases, the latter works well too).&lt;/li&gt; &#xA; &lt;li&gt;The Wav2Lip model without GAN usually needs more experimenting with the above two to get the most ideal results, and sometimes, can give you a better result as well.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparing LRS2 for training&lt;/h2&gt; &#xA;&lt;p&gt;Our models are trained on LRS2. See &lt;a href=&#34;https://raw.githubusercontent.com/Rudrabha/Wav2Lip/master/#training-on-datasets-other-than-lrs2&#34;&gt;here&lt;/a&gt; for a few suggestions regarding training on other datasets.&lt;/p&gt; &#xA;&lt;h5&gt;LRS2 dataset folder structure&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;data_root (mvlrs_v1)&#xA;âââ main, pretrain (we use only main folder in this work)&#xA;|&#x9;âââ list of folders&#xA;|&#x9;â   âââ five-digit numbered video IDs ending with (.mp4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Place the LRS2 filelists (train, val, test) &lt;code&gt;.txt&lt;/code&gt; files in the &lt;code&gt;filelists/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h5&gt;Preprocess the dataset for fast training&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python preprocess.py --data_root data_root/main --preprocessed_root lrs2_preprocessed/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional options like &lt;code&gt;batch_size&lt;/code&gt; and the number of GPUs to use in parallel to use can also be set.&lt;/p&gt; &#xA;&lt;h5&gt;Preprocessed LRS2 folder structure&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;preprocessed_root (lrs2_preprocessed)&#xA;âââ list of folders&#xA;|&#x9;âââ Folders with five-digit numbered video IDs&#xA;|&#x9;â   âââ *.jpg&#xA;|&#x9;â   âââ audio.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train!&lt;/h2&gt; &#xA;&lt;p&gt;There are two major steps: (i) Train the expert lip-sync discriminator, (ii) Train the Wav2Lip model(s).&lt;/p&gt; &#xA;&lt;h5&gt;Training the expert discriminator&lt;/h5&gt; &#xA;&lt;p&gt;You can download &lt;a href=&#34;https://raw.githubusercontent.com/Rudrabha/Wav2Lip/master/#getting-the-weights&#34;&gt;the pre-trained weights&lt;/a&gt; if you want to skip this step. To train it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python color_syncnet_train.py --data_root lrs2_preprocessed/ --checkpoint_dir &amp;lt;folder_to_save_checkpoints&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Training the Wav2Lip models&lt;/h5&gt; &#xA;&lt;p&gt;You can either train the model without the additional visual quality discriminator (&amp;lt; 1 day of training) or use the discriminator (~2 days). For the former, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python wav2lip_train.py --data_root lrs2_preprocessed/ --checkpoint_dir &amp;lt;folder_to_save_checkpoints&amp;gt; --syncnet_checkpoint_path &amp;lt;path_to_expert_disc_checkpoint&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train with the visual quality discriminator, you should run &lt;code&gt;hq_wav2lip_train.py&lt;/code&gt; instead. The arguments for both files are similar. In both cases, you can resume training as well. Look at &lt;code&gt;python wav2lip_train.py --help&lt;/code&gt; for more details. You can also set additional less commonly-used hyper-parameters at the bottom of the &lt;code&gt;hparams.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Training on datasets other than LRS2&lt;/h2&gt; &#xA;&lt;p&gt;Training on other datasets might require modifications to the code. Please read the following before you raise an issue:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You might not get good results by training/fine-tuning on a few minutes of a single speaker. This is a separate research problem, to which we do not have a solution yet. Thus, we would most likely not be able to resolve your issue.&lt;/li&gt; &#xA; &lt;li&gt;You must train the expert discriminator for your own dataset before training Wav2Lip.&lt;/li&gt; &#xA; &lt;li&gt;If it is your own dataset downloaded from the web, in most cases, needs to be sync-corrected.&lt;/li&gt; &#xA; &lt;li&gt;Be mindful of the FPS of the videos of your dataset. Changes to FPS would need significant code changes.&lt;/li&gt; &#xA; &lt;li&gt;The expert discriminator&#39;s eval loss should go down to ~0.25 and the Wav2Lip eval sync loss should go down to ~0.2 to get good results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When raising an issue on this topic, please let us know that you are aware of all these points.&lt;/p&gt; &#xA;&lt;p&gt;We have an HD model trained on a dataset allowing commercial usage. The size of the generated face will be 192 x 288 in our new model.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please check the &lt;code&gt;evaluation/&lt;/code&gt; folder for the instructions.&lt;/p&gt; &#xA;&lt;h2&gt;License and Citation&lt;/h2&gt; &#xA;&lt;p&gt;This repository can only be used for personal/research/non-commercial purposes. However, for commercial requests, please contact us directly at &lt;a href=&#34;mailto:radrabha.m@research.iiit.ac.in&#34;&gt;radrabha.m@research.iiit.ac.in&lt;/a&gt; or &lt;a href=&#34;mailto:prajwal.k@research.iiit.ac.in&#34;&gt;prajwal.k@research.iiit.ac.in&lt;/a&gt;. We have an HD model trained on a dataset allowing commercial usage. The size of the generated face will be 192 x 288 in our new model. Please cite the following paper if you use this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{10.1145/3394171.3413532,&#xA;author = {Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},&#xA;title = {A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild},&#xA;year = {2020},&#xA;isbn = {9781450379885},&#xA;publisher = {Association for Computing Machinery},&#xA;address = {New York, NY, USA},&#xA;url = {https://doi.org/10.1145/3394171.3413532},&#xA;doi = {10.1145/3394171.3413532},&#xA;booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},&#xA;pages = {484â492},&#xA;numpages = {9},&#xA;keywords = {lip sync, talking face generation, video generation},&#xA;location = {Seattle, WA, USA},&#xA;series = {MM &#39;20}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Parts of the code structure are inspired by this &lt;a href=&#34;https://github.com/r9y9/deepvoice3_pytorch&#34;&gt;TTS repository&lt;/a&gt;. We thank the author for this wonderful code. The code for Face Detection has been taken from the &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face_alignment&lt;/a&gt; repository. We thank the authors for releasing their code and models. We thank &lt;a href=&#34;https://github.com/zabique&#34;&gt;zabique&lt;/a&gt; for the tutorial collab notebook.&lt;/p&gt;</summary>
  </entry>
</feed>