<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-24T01:39:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>awslabs/multi-agent-orchestrator</title>
    <updated>2025-03-24T01:39:28Z</updated>
    <id>tag:github.com,2025-03-24:/awslabs/multi-agent-orchestrator</id>
    <link href="https://github.com/awslabs/multi-agent-orchestrator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Flexible and powerful framework for managing multiple AI agents and handling complex conversations&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt;Multi-Agent Orchestrator&amp;nbsp;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Flexible, lightweight open-source framework for orchestrating multiple AI agents to handle complex conversations.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator&#34;&gt;&lt;img alt=&#34;GitHub Repo&#34; src=&#34;https://img.shields.io/badge/GitHub-Repo-green.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/multi-agent-orchestrator&#34;&gt;&lt;img alt=&#34;npm&#34; src=&#34;https://img.shields.io/npm/v/multi-agent-orchestrator.svg?style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/multi-agent-orchestrator/&#34;&gt;&lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/multi-agent-orchestrator.svg?style=flat-square&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- GitHub Stats --&gt; &lt;img src=&#34;https://img.shields.io/github/stars/awslabs/multi-agent-orchestrator?style=social&#34; alt=&#34;GitHub stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/awslabs/multi-agent-orchestrator?style=social&#34; alt=&#34;GitHub forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/watchers/awslabs/multi-agent-orchestrator?style=social&#34; alt=&#34;GitHub watchers&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- Repository Info --&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/awslabs/multi-agent-orchestrator&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/awslabs/multi-agent-orchestrator&#34; alt=&#34;Issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/awslabs/multi-agent-orchestrator&#34; alt=&#34;Pull Requests&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- Package Stats --&gt; &lt;a href=&#34;https://pypi.org/project/multi-agent-orchestrator/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/multi-agent-orchestrator?label=pypi%20downloads&#34; alt=&#34;PyPI Monthly Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/multi-agent-orchestrator&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dm/multi-agent-orchestrator?label=npm%20downloads&#34; alt=&#34;npm Monthly Downloads&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://awslabs.github.io/multi-agent-orchestrator/&#34; style=&#34;display: inline-block; background-color: #0066cc; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; font-size: 15px; transition: background-color 0.3s;&#34;&gt; üìö Explore Full Documentation &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üîñ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Intelligent intent classification&lt;/strong&gt; ‚Äî Dynamically route queries to the most suitable agent based on context and content.&lt;/li&gt; &#xA; &lt;li&gt;üî§ &lt;strong&gt;Dual language support&lt;/strong&gt; ‚Äî Fully implemented in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üåä &lt;strong&gt;Flexible agent responses&lt;/strong&gt; ‚Äî Support for both streaming and non-streaming responses from different agents.&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;strong&gt;Context management&lt;/strong&gt; ‚Äî Maintain and utilize conversation context across multiple agents for coherent interactions.&lt;/li&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Extensible architecture&lt;/strong&gt; ‚Äî Easily integrate new agents or customize existing ones to fit your specific needs.&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;strong&gt;Universal deployment&lt;/strong&gt; ‚Äî Run anywhere - from AWS Lambda to your local environment or any cloud platform.&lt;/li&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Pre-built agents and classifiers&lt;/strong&gt; ‚Äî A variety of ready-to-use agents and multiple classifier implementations available.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s the Multi-Agent Orchestrator ‚ùì&lt;/h2&gt; &#xA;&lt;p&gt;The Multi-Agent Orchestrator is a flexible framework for managing multiple AI agents and handling complex conversations. It intelligently routes queries and maintains context across interactions.&lt;/p&gt; &#xA;&lt;p&gt;The system offers pre-built components for quick deployment, while also allowing easy integration of custom agents and conversation messages storage solutions.&lt;/p&gt; &#xA;&lt;p&gt;This adaptability makes it suitable for a wide range of applications, from simple chatbots to sophisticated AI systems, accommodating diverse requirements and scaling efficiently.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;ü§ñ Looking for details on Amazon Bedrock&#39;s multi-agent collaboration capability announced during Matt Garman&#39;s keynote at re:Invent 2024?&lt;/h3&gt; &#xA;&lt;p&gt;üöÄ Visit the &lt;a href=&#34;https://aws.amazon.com/bedrock/agents/&#34;&gt;Amazon Bedrock Agents&lt;/a&gt; page to explore how multi-agent collaboration enables developers to build, deploy, and manage specialized agents designed for tackling complex workflows efficiently and accurately. ‚ö°&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üèóÔ∏è High-level architecture flow diagram&lt;/h2&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/multi-agent-orchestrator/main/img/flow.jpg&#34; alt=&#34;High-level architecture flow diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The process begins with user input, which is analyzed by a Classifier.&lt;/li&gt; &#xA; &lt;li&gt;The Classifier leverages both Agents&#39; Characteristics and Agents&#39; Conversation history to select the most appropriate agent for the task.&lt;/li&gt; &#xA; &lt;li&gt;Once an agent is selected, it processes the user input.&lt;/li&gt; &#xA; &lt;li&gt;The orchestrator then saves the conversation, updating the Agents&#39; Conversation history, before delivering the response back to the user.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/multi-agent-orchestrator/main/img/new.png&#34; alt=&#34;&#34;&gt; Introducing SupervisorAgent: Agents Coordination&lt;/h2&gt; &#xA;&lt;p&gt;The Multi-Agent Orchestrator now includes a powerful new SupervisorAgent that enables sophisticated team coordination between multiple specialized agents. This new component implements a &#34;agent-as-tools&#34; architecture, allowing a lead agent to coordinate a team of specialized agents in parallel, maintaining context and delivering coherent responses.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/multi-agent-orchestrator/main/img/flow-supervisor.jpg&#34; alt=&#34;SupervisorAgent flow diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Key capabilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü§ù &lt;strong&gt;Team Coordination&lt;/strong&gt; - Coordonate multiple specialized agents working together on complex tasks&lt;/li&gt; &#xA; &lt;li&gt;‚ö° &lt;strong&gt;Parallel Processing&lt;/strong&gt; - Execute multiple agent queries simultaneously&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Smart Context Management&lt;/strong&gt; - Maintain conversation history across all team members&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Dynamic Delegation&lt;/strong&gt; - Intelligently distribute subtasks to appropriate team members&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;strong&gt;Agent Compatibility&lt;/strong&gt; - Works with all agent types (Bedrock, Anthropic, Lex, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The SupervisorAgent can be used in two powerful ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Direct Usage&lt;/strong&gt; - Call it directly when you need dedicated team coordination for specific tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Classifier Integration&lt;/strong&gt; - Add it as an agent within the classifier to build complex hierarchical systems with multiple specialized teams&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here are just a few examples where this agent can be used:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Customer Support Teams with specialized sub-teams&lt;/li&gt; &#xA; &lt;li&gt;AI Movie Production Studios&lt;/li&gt; &#xA; &lt;li&gt;Travel Planning Services&lt;/li&gt; &#xA; &lt;li&gt;Product Development Teams&lt;/li&gt; &#xA; &lt;li&gt;Healthcare Coordination Systems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://awslabs.github.io/multi-agent-orchestrator/agents/built-in/supervisor-agent&#34;&gt;Learn more about SupervisorAgent ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí¨ Demo App&lt;/h2&gt; &#xA;&lt;p&gt;In the screen recording below, we demonstrate an extended version of the demo app that uses 6 specialized agents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Travel Agent&lt;/strong&gt;: Powered by an Amazon Lex Bot&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Weather Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with a tool to query the open-meteo API&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Restaurant Agent&lt;/strong&gt;: Implemented as an Amazon Bedrock Agent&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Math Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with two tools for executing mathematical operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tech Agent&lt;/strong&gt;: A Bedrock LLM Agent designed to answer questions on technical topics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Health Agent&lt;/strong&gt;: A Bedrock LLM Agent focused on addressing health-related queries&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Watch as the system seamlessly switches context between diverse topics, from booking flights to checking weather, solving math problems, and providing health information. Notice how the appropriate agent is selected for each query, maintaining coherence even with brief follow-up inputs.&lt;/p&gt; &#xA;&lt;p&gt;The demo highlights the system&#39;s ability to handle complex, multi-turn conversations while preserving context and leveraging specialized agents across various domains.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/awslabs/multi-agent-orchestrator/main/img/demo-app.gif?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üéØ Examples &amp;amp; Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Get hands-on experience with the Multi-Agent Orchestrator through our diverse set of examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demo Applications&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/python&#34;&gt;Streamlit Global Demo&lt;/a&gt;: A single Streamlit application showcasing multiple demos, including: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;AI Movie Production Studio&lt;/li&gt; &#xA;     &lt;li&gt;AI Travel Planner&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://awslabs.github.io/multi-agent-orchestrator/cookbook/examples/chat-demo-app/&#34;&gt;Chat Demo App&lt;/a&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Explore multiple specialized agents handling various domains like travel, weather, math, and health&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://awslabs.github.io/multi-agent-orchestrator/cookbook/examples/ecommerce-support-simulator/&#34;&gt;E-commerce Support Simulator&lt;/a&gt;: Experience AI-powered customer support with: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Automated response generation for common queries&lt;/li&gt; &#xA;     &lt;li&gt;Intelligent routing of complex issues to human support&lt;/li&gt; &#xA;     &lt;li&gt;Real-time chat and email-style communication&lt;/li&gt; &#xA;     &lt;li&gt;Human-in-the-loop interactions for complex cases&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sample Projects&lt;/strong&gt;: Explore our example implementations in the &lt;code&gt;examples&lt;/code&gt; folder: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/chat-demo-app&#34;&gt;&lt;code&gt;chat-demo-app&lt;/code&gt;&lt;/a&gt;: Web-based chat interface with multiple specialized agents&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/ecommerce-support-simulator&#34;&gt;&lt;code&gt;ecommerce-support-simulator&lt;/code&gt;&lt;/a&gt;: AI-powered customer support system&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/chat-chainlit-app&#34;&gt;&lt;code&gt;chat-chainlit-app&lt;/code&gt;&lt;/a&gt;: Chat application built with Chainlit&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/fast-api-streaming&#34;&gt;&lt;code&gt;fast-api-streaming&lt;/code&gt;&lt;/a&gt;: FastAPI implementation with streaming support&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/text-2-structured-output&#34;&gt;&lt;code&gt;text-2-structured-output&lt;/code&gt;&lt;/a&gt;: Natural Language to Structured Data&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/bedrock-inline-agents&#34;&gt;&lt;code&gt;bedrock-inline-agents&lt;/code&gt;&lt;/a&gt;: Bedrock Inline Agents sample&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/tree/main/examples/bedrock-prompt-routing&#34;&gt;&lt;code&gt;bedrock-prompt-routing&lt;/code&gt;&lt;/a&gt;: Bedrock Prompt Routing sample code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Examples are available in both Python and TypeScript. Check out our &lt;a href=&#34;https://awslabs.github.io/multi-agent-orchestrator/&#34;&gt;documentation&lt;/a&gt; for comprehensive guides on setting up and using the Multi-Agent Orchestrator framework!&lt;/p&gt; &#xA;&lt;h2&gt;üìö Deep Dives: Stories, Blogs &amp;amp; Podcasts&lt;/h2&gt; &#xA;&lt;p&gt;Discover creative implementations and diverse applications of the Multi-Agent Orchestrator:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2lCi8jEKydhDm8eE8QFIQ5K23pF/from-bonjour-to-boarding-pass-multilingual-ai-chatbot-for-flight-reservations&#34;&gt;From &#39;Bonjour&#39; to &#39;Boarding Pass&#39;: Multilingual AI Chatbot for Flight Reservations&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build a multilingual chatbot using the Multi-Agent Orchestrator framework. The article explains how to use an &lt;strong&gt;Amazon Lex&lt;/strong&gt; bot as an agent, along with 2 other new agents to make it work in many languages with just a few lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2lq6cYYwTYGc7S3Zmz28xZoQNQj/beyond-auto-replies-building-an-ai-powered-e-commerce-support-system&#34;&gt;Beyond Auto-Replies: Building an AI-Powered E-commerce Support system&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI-driven multi-agent system for automated e-commerce customer email support. It covers the architecture and setup of specialized AI agents using the Multi-Agent Orchestrator framework, integrating automated processing with human-in-the-loop oversight. The guide explores email ingestion, intelligent routing, automated response generation, and human verification, providing a comprehensive approach to balancing AI efficiency with human expertise in customer support.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2mt7CFG7xg4yw6GRHwH9akhg0oD/speak-up-ai-voicing-your-agents-with-amazon-connect-lex-and-bedrock&#34;&gt;Speak Up, AI: Voicing Your Agents with Amazon Connect, Lex, and Bedrock&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI customer call center. It covers the architecture and setup of specialized AI agents using the Multi-Agent Orchestrator framework interacting with voice via &lt;strong&gt;Amazon Connect&lt;/strong&gt; and &lt;strong&gt;Amazon Lex&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2pTsHrYPqvAbJBl9ht1XxPOSPjR/unlock-bedrock-invokeinlineagent-api-s-hidden-potential-with-multi-agent-orchestrator&#34;&gt;Unlock Bedrock InvokeInlineAgent API&#39;s Hidden Potential&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to scale &lt;strong&gt;Amazon Bedrock Agents&lt;/strong&gt; beyond knowledge base limitations using the Multi-Agent Orchestrator framework and &lt;strong&gt;InvokeInlineAgent API&lt;/strong&gt;. This article demonstrates dynamic agent creation and knowledge base selection for enterprise-scale AI applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://community.aws/content/2phMjQ0bqWMg4PBwejBs1uf4YQE/supercharging-amazon-bedrock-flows-with-aws-multi-agent-orchestrator&#34;&gt;Supercharging Amazon Bedrock Flows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to enhance &lt;strong&gt;Amazon Bedrock Flows&lt;/strong&gt; with conversation memory and multi-flow orchestration using the Multi-Agent Orchestrator framework. This guide shows how to overcome Bedrock Flows&#39; limitations to build more sophisticated AI workflows with persistent memory and intelligent routing between flows.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üéôÔ∏è Podcast Discussions&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üá´üá∑ Podcast (French)&lt;/strong&gt;: L&#39;orchestrateur multi-agents : Un orchestrateur open source pour vos agents IA&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/be/podcast/lorchestrateur-multi-agents/id1452118442?i=1000684332612&#34;&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/4RdMazSRhZUyW2pniG91Vf&#34;&gt;Spotify&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üá¨üáß Podcast (English)&lt;/strong&gt;: An Orchestrator for Your AI Agents&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/us/podcast/an-orchestrator-for-your-ai-agents/id1574162669?i=1000677039579&#34;&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/2a9DBGZn2lVqVMBLWGipHU&#34;&gt;Spotify&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TypeScript Version&lt;/h3&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install multi-agent-orchestrator&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;p&gt;The following example demonstrates how to use the Multi-Agent Orchestrator with two different types of agents: a Bedrock LLM Agent with Converse API support and a Lex Bot Agent. This showcases the flexibility of the system in integrating various AI services.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { MultiAgentOrchestrator, BedrockLLMAgent, LexBotAgent } from &#34;multi-agent-orchestrator&#34;;&#xA;&#xA;const orchestrator = new MultiAgentOrchestrator();&#xA;&#xA;// Add a Bedrock LLM Agent with Converse API support&#xA;orchestrator.addAgent(&#xA;  new BedrockLLMAgent({&#xA;      name: &#34;Tech Agent&#34;,&#xA;      description:&#xA;        &#34;Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.&#34;,&#xA;      streaming: true&#xA;  })&#xA;);&#xA;&#xA;// Add a Lex Bot Agent for handling travel-related queries&#xA;orchestrator.addAgent(&#xA;  new LexBotAgent({&#xA;    name: &#34;Travel Agent&#34;,&#xA;    description: &#34;Helps users book and manage their flight reservations&#34;,&#xA;    botId: process.env.LEX_BOT_ID,&#xA;    botAliasId: process.env.LEX_BOT_ALIAS_ID,&#xA;    localeId: &#34;en_US&#34;,&#xA;  })&#xA;);&#xA;&#xA;// Example usage&#xA;const response = await orchestrator.routeRequest(&#xA;  &#34;I want to book a flight&#34;,&#xA;  &#39;user123&#39;,&#xA;  &#39;session456&#39;&#xA;);&#xA;&#xA;// Handle the response (streaming or non-streaming)&#xA;if (response.streaming == true) {&#xA;    console.log(&#34;\n** RESPONSE STREAMING ** \n&#34;);&#xA;    // Send metadata immediately&#xA;    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);&#xA;    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);&#xA;    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);&#xA;    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);&#xA;    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);&#xA;    console.log(&#xA;      `&amp;gt; Additional Parameters:`,&#xA;      response.metadata.additionalParams&#xA;    );&#xA;    console.log(`\n&amp;gt; Response: `);&#xA;&#xA;    // Stream the content&#xA;    for await (const chunk of response.output) {&#xA;      if (typeof chunk === &#34;string&#34;) {&#xA;        process.stdout.write(chunk);&#xA;      } else {&#xA;        console.error(&#34;Received unexpected chunk type:&#34;, typeof chunk);&#xA;      }&#xA;    }&#xA;&#xA;} else {&#xA;    // Handle non-streaming response (AgentProcessingResult)&#xA;    console.log(&#34;\n** RESPONSE ** \n&#34;);&#xA;    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);&#xA;    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);&#xA;    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);&#xA;    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);&#xA;    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);&#xA;    console.log(&#xA;      `&amp;gt; Additional Parameters:`,&#xA;      response.metadata.additionalParams&#xA;    );&#xA;    console.log(`\n&amp;gt; Response: ${response.output}`);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python Version&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Optional: Set up a virtual environment&#xA;python -m venv venv&#xA;source venv/bin/activate  # On Windows use `venv\Scripts\activate`&#xA;pip install multi-agent-orchestrator[aws]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Default Usage&lt;/h4&gt; &#xA;&lt;p&gt;Here&#39;s an equivalent Python example demonstrating the use of the Multi-Agent Orchestrator with a Bedrock LLM Agent and a Lex Bot Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import asyncio&#xA;from multi_agent_orchestrator.orchestrator import MultiAgentOrchestrator&#xA;from multi_agent_orchestrator.agents import BedrockLLMAgent, LexBotAgent, BedrockLLMAgentOptions, LexBotAgentOptions, AgentCallbacks&#xA;&#xA;orchestrator = MultiAgentOrchestrator()&#xA;&#xA;class BedrockLLMAgentCallbacks(AgentCallbacks):&#xA;    def on_llm_new_token(self, token: str) -&amp;gt; None:&#xA;        # handle response streaming here&#xA;        print(token, end=&#39;&#39;, flush=True)&#xA;&#xA;tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(&#xA;  name=&#34;Tech Agent&#34;,&#xA;  streaming=True,&#xA;  description=&#34;Specializes in technology areas including software development, hardware, AI, \&#xA;  cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs \&#xA;  related to technology products and services.&#34;,&#xA;  model_id=&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;,&#xA;  callbacks=BedrockLLMAgentCallbacks()&#xA;))&#xA;orchestrator.add_agent(tech_agent)&#xA;&#xA;&#xA;# Add a Lex Bot Agent for handling travel-related queries&#xA;orchestrator.add_agent(&#xA;    LexBotAgent(LexBotAgentOptions(&#xA;        name=&#34;Travel Agent&#34;,&#xA;        description=&#34;Helps users book and manage their flight reservations&#34;,&#xA;        bot_id=os.environ.get(&#39;LEX_BOT_ID&#39;),&#xA;        bot_alias_id=os.environ.get(&#39;LEX_BOT_ALIAS_ID&#39;),&#xA;        locale_id=&#34;en_US&#34;,&#xA;    ))&#xA;)&#xA;&#xA;async def main():&#xA;    # Example usage&#xA;    response = await orchestrator.route_request(&#xA;        &#34;I want to book a flight&#34;,&#xA;        &#39;user123&#39;,&#xA;        &#39;session456&#39;&#xA;    )&#xA;&#xA;    # Handle the response (streaming or non-streaming)&#xA;    if response.streaming:&#xA;        print(&#34;\n** RESPONSE STREAMING ** \n&#34;)&#xA;        # Send metadata immediately&#xA;        print(f&#34;&amp;gt; Agent ID: {response.metadata.agent_id}&#34;)&#xA;        print(f&#34;&amp;gt; Agent Name: {response.metadata.agent_name}&#34;)&#xA;        print(f&#34;&amp;gt; User Input: {response.metadata.user_input}&#34;)&#xA;        print(f&#34;&amp;gt; User ID: {response.metadata.user_id}&#34;)&#xA;        print(f&#34;&amp;gt; Session ID: {response.metadata.session_id}&#34;)&#xA;        print(f&#34;&amp;gt; Additional Parameters: {response.metadata.additional_params}&#34;)&#xA;        print(&#34;\n&amp;gt; Response: &#34;)&#xA;&#xA;        # Stream the content&#xA;        async for chunk in response.output:&#xA;            if isinstance(chunk, str):&#xA;                print(chunk, end=&#39;&#39;, flush=True)&#xA;            else:&#xA;                print(f&#34;Received unexpected chunk type: {type(chunk)}&#34;, file=sys.stderr)&#xA;&#xA;    else:&#xA;        # Handle non-streaming response (AgentProcessingResult)&#xA;        print(&#34;\n** RESPONSE ** \n&#34;)&#xA;        print(f&#34;&amp;gt; Agent ID: {response.metadata.agent_id}&#34;)&#xA;        print(f&#34;&amp;gt; Agent Name: {response.metadata.agent_name}&#34;)&#xA;        print(f&#34;&amp;gt; User Input: {response.metadata.user_input}&#34;)&#xA;        print(f&#34;&amp;gt; User ID: {response.metadata.user_id}&#34;)&#xA;        print(f&#34;&amp;gt; Session ID: {response.metadata.session_id}&#34;)&#xA;        print(f&#34;&amp;gt; Additional Parameters: {response.metadata.additional_params}&#34;)&#xA;        print(f&#34;\n&amp;gt; Response: {response.output.content}&#34;)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These examples showcase:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The use of a Bedrock LLM Agent with Converse API support, allowing for multi-turn conversations.&lt;/li&gt; &#xA; &lt;li&gt;Integration of a Lex Bot Agent for specialized tasks (in this case, travel-related queries).&lt;/li&gt; &#xA; &lt;li&gt;The orchestrator&#39;s ability to route requests to the most appropriate agent based on the input.&lt;/li&gt; &#xA; &lt;li&gt;Handling of both streaming and non-streaming responses from different types of agents.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Modular Installation Options&lt;/h3&gt; &#xA;&lt;p&gt;The Multi-Agent Orchestrator is designed with a modular architecture, allowing you to install only the components you need while ensuring you always get the core functionality.&lt;/p&gt; &#xA;&lt;h4&gt;Installation Options&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. AWS Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; pip install &#34;multi-agent-orchestrator[aws]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Includes core orchestration functionality with comprehensive AWS service integrations (&lt;code&gt;BedrockLLMAgent&lt;/code&gt;, &lt;code&gt;AmazonBedrockAgent&lt;/code&gt;, &lt;code&gt;LambdaAgent&lt;/code&gt;, etc.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Anthropic Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;multi-agent-orchestrator[anthropic]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. OpenAI Integration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;multi-agent-orchestrator[openai]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adds OpenAI&#39;s GPT models for agents and classification, along with core packages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Full Installation&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;multi-agent-orchestrator[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Includes all optional dependencies for maximum flexibility.&lt;/p&gt; &#xA;&lt;h3&gt;üôå &lt;strong&gt;We Want to Hear From You!&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Have something to share, discuss, or brainstorm? We‚Äôd love to connect with you and hear about your journey with the &lt;strong&gt;Multi-Agent Orchestrator framework&lt;/strong&gt;. Here‚Äôs how you can get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üôå Show &amp;amp; Tell&lt;/strong&gt;: Got a success story, cool project, or creative implementation? Share it with us in the &lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/discussions/categories/show-and-tell&#34;&gt;&lt;strong&gt;Show and Tell&lt;/strong&gt;&lt;/a&gt; section. Your work might inspire the entire community! üéâ&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üí¨ General Discussion&lt;/strong&gt;: Have questions, feedback, or suggestions? Join the conversation in our &lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/discussions/categories/general&#34;&gt;&lt;strong&gt;General Discussions&lt;/strong&gt;&lt;/a&gt; section. It‚Äôs the perfect place to connect with other users and contributors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üí° Ideas&lt;/strong&gt;: Thinking of a new feature or improvement? Share your thoughts in the &lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/discussions/categories/ideas&#34;&gt;&lt;strong&gt;Ideas&lt;/strong&gt;&lt;/a&gt; section. We‚Äôre always open to exploring innovative ways to make the orchestrator even better!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let‚Äôs collaborate, learn from each other, and build something incredible together! üöÄ&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;‚ö†Ô∏è We value your contributions! Before submitting changes, please start a discussion by opening an issue to share your proposal.&lt;/p&gt; &#xA;&lt;p&gt;Once your proposal is approved, here are the next steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üìö Review our &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/multi-agent-orchestrator/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üí° Create a &lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/issues&#34;&gt;GitHub Issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî® Submit a pull request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;‚úÖ Follow existing project structure and include documentation for new features.&lt;/p&gt; &#xA;&lt;p&gt;üåü &lt;strong&gt;Stay Updated&lt;/strong&gt;: Star the repository to be notified about new features, improvements, and exciting developments in the Multi-Agent Orchestrator framework!&lt;/p&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/corneliucroitoru/&#34;&gt;Corneliu Croitoru&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/anthonybernabeu/&#34;&gt;Anthony Bernabeu&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üë• Contributors&lt;/h1&gt; &#xA;&lt;p&gt;Big shout out to our awesome contributors! Thank you for making this project better! üåü ‚≠ê üöÄ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/awslabs/multi-agent-orchestrator/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=awslabs/multi-agent-orchestrator&amp;amp;max=2000&#34; alt=&#34;contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/multi-agent-orchestrator/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; for guidelines on how to propose bugfixes and improvements.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 licence - see the &lt;a href=&#34;https://raw.githubusercontent.com/awslabs/multi-agent-orchestrator/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Font License&lt;/h2&gt; &#xA;&lt;p&gt;This project uses the JetBrainsMono NF font, licensed under the SIL Open Font License 1.1. For full license details, see &lt;a href=&#34;https://github.com/JetBrains/JetBrainsMono/raw/master/OFL.txt&#34;&gt;FONT-LICENSE.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SafeAILab/EAGLE</title>
    <updated>2025-03-24T01:39:28Z</updated>
    <id>tag:github.com,2025-03-24:/SafeAILab/EAGLE</id>
    <link href="https://github.com/SafeAILab/EAGLE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation of EAGLE-1 (ICML&#39;24), EAGLE-2 (EMNLP&#39;24), and EAGLE-3.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/figs/logo.png&#34; alt=&#34;EAGLE&#34; width=&#34;220&#34; align=&#34;left&#34;&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;h1&gt;&amp;nbsp;EAGLE&lt;/h1&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2401.15077.pdf&#34;&gt;&lt;b&gt;Paper (EAGLE)&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2406.16858&#34;&gt;&lt;b&gt;Paper (EAGLE-2)&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2503.01840&#34;&gt;&lt;b&gt;Paper (EAGLE-3)&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://sites.google.com/view/%0Aeagle-llm&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Version-v2.0.0-orange.svg?sanitize=true&#34; alt=&#34;Version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/SafeAILab/EAGLE/issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-yes-green.svg?sanitize=true&#34; alt=&#34;Maintenance&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/SafeAILab/EAGLE/pulls&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Contributions-welcome-brightgreen.svg?style=flat&#34; alt=&#34;Contributions welcome&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/figs/eagle3r.jpg&#34; alt=&#34;benchmark&#34; width=&#34;790&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency) is a new baseline for fast decoding of Large Language Models (LLMs) with provable performance maintenance. This approach involves extrapolating the second-top-layer contextual feature vectors of LLMs, enabling a significant boost in generation efficiency.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EAGLE is: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;certified by the &lt;a href=&#34;https://github.com/hemingkx/Spec-Bench/raw/main/Leaderboard.md&#34;&gt;&lt;b&gt;third-party&lt;/b&gt;&lt;/a&gt; evaluation as the &lt;strong&gt;fastest&lt;/strong&gt; speculative method so far.&lt;/li&gt; &#xA;   &lt;li&gt;achieving &lt;strong&gt;2x&lt;/strong&gt; speedup on &lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;&lt;b&gt;gpt-fast&lt;/b&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;3x&lt;/strong&gt; faster than vanilla decoding (13B).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;2x&lt;/strong&gt; faster than &lt;a href=&#34;https://lmsys.org/blog/2023-11-21-lookahead-decoding/&#34;&gt;&lt;b&gt;Lookahead&lt;/b&gt;&lt;/a&gt; (13B).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;1.6x&lt;/strong&gt; faster than &lt;a href=&#34;https://sites.google.com/view/medusa-llm&#34;&gt;&lt;b&gt;Medusa&lt;/b&gt;&lt;/a&gt; (13B).&lt;/li&gt; &#xA;   &lt;li&gt;provably maintaining the consistency with vanilla decoding in the distribution of generated texts.&lt;/li&gt; &#xA;   &lt;li&gt;trainable (within 1-2 days) and testable on 8x RTX 3090 GPUs. So even the GPU poor can afford it.&lt;/li&gt; &#xA;   &lt;li&gt;combinable with other parallelled techniques such as vLLM, DeepSpeed, Mamba, FlashAttention, quantization, and hardware optimization.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;EAGLE-2 uses the confidence scores from the draft model to approximate acceptance rates, dynamically adjusting the draft tree structure, which further enhances performance.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EAGLE-2 is: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;4x&lt;/strong&gt; faster than vanilla decoding (13B).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;1.4x&lt;/strong&gt; faster than EAGLE-1 (13B).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;EAGLE-3 removes the feature prediction constraint in EAGLE and simulates this process during training using training-time testing. Considering that top-layer features are limited to next-token prediction, EAGLE-3 replaces them with a fusion of low-, mid-, and high-level semantic features. EAGLE-3 further improves generation speed while ensuring lossless performance.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EAGLE-3 is: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;5.6&lt;/strong&gt; faster than vanilla decoding (13B).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;1.8x&lt;/strong&gt; faster than EAGLE-1 (13B).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/figs/e3.gif&#34; alt=&#34;demogif&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Inference is conducted on 2x RTX 3090 GPUs at fp16 precision using the Vicuna 13B model.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;EAGLE has been merged in the following mainstream LLM serving frameworks (listed in alphabetical order).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/feature-guide.html#eagle-speculative-decoding&#34;&gt;AWS NeuronX Distributed Core&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intel/intel-extension-for-transformers/pull/1504&#34;&gt;Intel¬Æ Extension for Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intel-analytics/ipex-llm/pull/11104&#34;&gt;Intel¬Æ LLM Library for PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://llm.mlc.ai/docs/deploy/rest.html&#34;&gt;MLC-LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/eagle&#34;&gt;NVIDIA TensorRT-LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang/pull/4247&#34;&gt;SGLang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm/pull/6830&#34;&gt;vLLM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2025.3.19&lt;/strong&gt;: EAGLE-3 is released.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.8.8&lt;/strong&gt;: We now support Qwen-2.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.6.27&lt;/strong&gt;: EAGLE-2 is released.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.2.25&lt;/strong&gt;: EAGLE is certified by the &lt;a href=&#34;https://github.com/hemingkx/Spec-Bench/raw/main/Leaderboard.md&#34;&gt;third-party&lt;/a&gt; evaluation as the fastest speculative method.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.1.17&lt;/strong&gt;: We now support &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;Mixtral-8x7B-Instruct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.12.8&lt;/strong&gt;: EAGLE v1.0 is released.&lt;/p&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support non-greedy inference (provably maintaining text distribution).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support more LLMs such as Mixtral 8x7B.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support LLaMA-3.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support Qwen-2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support vLLM (please check &lt;a href=&#34;https://github.com/vllm-project/vllm/pull/6830&#34;&gt;vLLM&lt;/a&gt;&#39;s implementation).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; EAGLE-3.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The default main branch is the implementation of EAGLE-3 and EAGLE-2. For using EAGLE-1, please switch to the v1 branch.&lt;/h2&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#setup--installation&#34;&gt;Setup &amp;amp; Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#eagle-3-weights&#34;&gt;EAGLE-3 Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#eagle-weights&#34;&gt;EAGLE Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#inference&#34;&gt;Inference&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#with-ui&#34;&gt;With UI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#with-code&#34;&gt;With Code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#train&#34;&gt;Train&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#generate-train-data&#34;&gt;Generate Train Data&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#train-the-auto-regression-head&#34;&gt;Train the Auto-regression Head&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#inference-on-custom-models&#34;&gt;Inference on custom models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SafeAILab/EAGLE/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup &amp;amp; Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/SafeAILab/EAGLE.git&#xA;cd EAGLE&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;EAGLE-3 Weights&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;EAGLE-3 on Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;EAGLE-3 on Hugging Face&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE3-Vicuna1.3-13B&#34;&gt;yuhuili/EAGLE3-Vicuna1.3-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA3.1-Instruct 8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE3-LLaMA3.1-Instruct-8B&#34;&gt;yuhuili/EAGLE3-LLaMA3.1-Instruct-8B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA3.3-Instruct 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE3-LLaMA3.3-Instruct-70B&#34;&gt;yuhuili/EAGLE3-LLaMA3.3-Instruct-70B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSeek-R1-Distill-LLaMA 8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B&#34;&gt;yuhuili/EAGLE3-DeepSeek-R1-Distill-LLaMA-8B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;EAGLE Weights&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; The current code defaults to using EAGLE-3. If you want to use EAGLE weights, please specify &lt;code&gt;use_eagle3=False&lt;/code&gt; in &lt;code&gt;EaModel.from_pretrained&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; When Qwen2 is the target model, please use bf16 precision instead of fp16 to avoid numerical overflow. The training dataset for the draft model of Qwen2 is ShareGPT, which has removed non-English data. Therefore, if you want to use it on non-English data such as Chinese, please train with the corresponding data.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;EAGLE on Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;# EAGLE Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;EAGLE on Hugging Face&lt;/th&gt; &#xA;   &lt;th&gt;# EAGLE Parameters&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-Vicuna-7B-v1.3&#34;&gt;yuhuili/EAGLE-Vicuna-7B-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.24B&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA2-Chat 7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-llama2-chat-7B&#34;&gt;yuhuili/EAGLE-llama2-chat-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.24B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-Vicuna-13B-v1.3&#34;&gt;yuhuili/EAGLE-Vicuna-13B-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.37B&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA2-Chat 13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-llama2-chat-13B&#34;&gt;yuhuili/EAGLE-llama2-chat-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.37B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-33B-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-Vicuna-33B-v1.3&#34;&gt;yuhuili/EAGLE-Vicuna-33B-v1.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.56B&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA2-Chat 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-llama2-chat-70B&#34;&gt;yuhuili/EAGLE-llama2-chat-70B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.99B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-mixtral-instruct-8x7B&#34;&gt;yuhuili/EAGLE-mixtral-instruct-8x7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.28B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA3-Instruct 8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-LLaMA3-Instruct-8B&#34;&gt;yuhuili/EAGLE-LLaMA3-Instruct-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.25B&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA3-Instruct 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-LLaMA3-Instruct-70B&#34;&gt;yuhuili/EAGLE-LLaMA3-Instruct-70B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.99B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2-7B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-Qwen2-7B-Instruct&#34;&gt;yuhuili/EAGLE-Qwen2-7B-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.26B&lt;/td&gt; &#xA;   &lt;td&gt;Qwen2-72B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-Qwen2-72B-Instruct&#34;&gt;yuhuili/EAGLE-Qwen2-72B-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.05B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA3.1-Instruct 8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/yuhuili/EAGLE-LLaMA3.1-Instruct-8B&#34;&gt;yuhuili/EAGLE-LLaMA3.1-Instruct-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.25B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;The inference code we provide automatically allocates model weights (loading a model across multiple GPUs), allowing you to run models that exceed the memory of a single GPU.&lt;/p&gt; &#xA;&lt;h3&gt;With UI&lt;/h3&gt; &#xA;&lt;p&gt;We have provided a suggested web interface, which you can use by running the following command. After the model is fully loaded, a URL will be output in the terminal, which you can enter into your browser to access.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eagle.application.webui --ea-model-path [path of EAGLE weight]\ &#xA;&#x9;&#x9;--base-model-path [path of the original model]\&#xA;&#x9;&#x9;--model-type [vicuna\llama2\llama3]\&#xA;        --total-token [int]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;em&gt;total-token&lt;/em&gt; is the number of draft tokens. For smaller models and advanced GPUs, this value can be set larger. Adjusting according to the specific device and model can achieve better results. If set to -1, EAGLE-2 will automatically configure this parameter.&lt;/p&gt; &#xA;&lt;h3&gt;With Code&lt;/h3&gt; &#xA;&lt;p&gt;You can use our provided &#34;eagenerate&#34; for speedup generation just like using &#39;generate&#39; from Hugging Face. Here is an example.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from eagle.model.ea_model import EaModel&#xA;from fastchat.model import get_conversation_template&#xA;model = EaModel.from_pretrained(&#xA;    base_model_path=base_model_path,&#xA;    ea_model_path=EAGLE_model_path,&#xA;    torch_dtype=torch.float16,&#xA;    low_cpu_mem_usage=True,&#xA;    device_map=&#34;auto&#34;,&#xA;    total_token=-1&#xA;)&#xA;model.eval()&#xA;your_message=&#34;Hello&#34;&#xA;conv = get_conversation_template(&#34;vicuna&#34;)&#xA;conv.append_message(conv.roles[0], your_message)&#xA;conv.append_message(conv.roles[1], None)&#xA;prompt = conv.get_prompt()&#xA;input_ids=model.tokenizer([prompt]).input_ids&#xA;input_ids = torch.as_tensor(input_ids).cuda()&#xA;output_ids=model.eagenerate(input_ids,temperature=0.5,max_new_tokens=512)&#xA;output=model.tokenizer.decode(output_ids[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note: Vicuna, LLaMA2-Chat, and LLaMA3-Instruct are both chat models. You need to use the correct chat template, otherwise it will cause abnormal output from the model and affect the performance of EAGLE.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;h3&gt;Generate Train Data&lt;/h3&gt; &#xA;&lt;p&gt;You can run the following command to generate the training data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eagle.ge_data.allocation --outdir [path of data]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train the Auto-regression Head&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch -m --mixed_precision=bf16 eagle.train.main --tmpdir [path of data]\&#xA;--cpdir [path of checkpoints] --configpath [path of config file]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;eagle/train&lt;/em&gt; provides examples of configuration files.&lt;/p&gt; &#xA;&lt;p&gt;You can also use DeepSpeed for training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd eagle/train&#xA;deepspeed main_deepspeed.py --deepspeed_config ds_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference on custom models&lt;/h3&gt; &#xA;&lt;p&gt;If the original LLM structure differs from LLaMA and Mixtral, you can utilize EAGLE as follows:&lt;/p&gt; &#xA;&lt;p&gt;Copy the modeling_basemodelname.py from the Transformers library and proceed to make modifications to leverage the pre-allocated kv_cache for enhanced speed in the base model. You can refer to model/modeling_llama_kv.py for guidance, where places that require modifications are annotated with # [MODIFIED]. These modifications are minimal.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;You can test the speed of EAGLE on MT-bench using the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eagle.evaluation.gen_ea_answer_vicuna(or gen_ea_answer_vicuna_llama2chat)\&#xA;&#x9;&#x9; --ea-model-path [path of EAGLE weight]\ &#xA;&#x9;&#x9; --base-model-path [path of the original model]\&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need specific acceleration ratios, you will also need to run the following command to get the speed of vanilla auto-regression.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eagle.evaluation.gen_baseline_answer_vicuna\&#xA;&#x9;&#x9;(or gen_ea_answer_vicuna_llama2chat)\&#xA;&#x9;&#x9; --ea-model-path [path of EAGLE weight]\ &#xA;&#x9;&#x9; --base-model-path [path of the original model]\&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above two commands will each generate a .jsonl file that records the generation results and wall time. Then, you can use evaluation/speed.py to calculate the ratio of speeds.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Our Contributors&lt;/h2&gt; &#xA;&lt;p&gt;A heartfelt thank you to all our contributors.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=SafeAILab/EAGLE&#34; alt=&#34;Contributors&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;For technical details and full experimental results, please check &lt;a href=&#34;https://arxiv.org/pdf/2401.15077.pdf&#34;&gt;the paper of EAGLE&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2406.16858&#34;&gt;the paper of EAGLE-2&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/pdf/2503.01840&#34;&gt;the paper of EAGLE-3&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{li2024eagle, &#xA;&#x9;author = {Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang}, &#xA;&#x9;title = {{EAGLE}: Speculative Sampling Requires Rethinking Feature Uncertainty}, &#xA;&#x9;booktitle = {International Conference on Machine Learning},&#xA;&#x9;year = {2024}&#xA;}&#xA;@inproceedings{li2024eagle2, &#xA;&#x9;author = {Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang}, &#xA;&#x9;title = {{EAGLE-2}: Faster Inference of Language Models with Dynamic Draft Trees}, &#xA;&#x9;booktitle = {Empirical Methods in Natural Language Processing},&#xA;&#x9;year = {2024}&#xA;}&#xA;@misc{li2025eagle3scalinginferenceacceleration,&#xA;      title={{EAGLE-3}: Scaling up Inference Acceleration of Large Language Models via Training-Time Test}, &#xA;      author={Yuhui Li and Fangyun Wei and Chao Zhang and Hongyang Zhang},&#xA;      year={2025},&#xA;      eprint={2503.01840},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2503.01840}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project has been influenced by many excellent projects in the LLM community, such as &lt;a href=&#34;https://github.com/FasterDecoding/Medusa&#34;&gt;Medusa&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;, and others. The logo is designed by GPT-4. We also appreciate many valuable discussions with the SGLang team (James Liu, Ke Bao, Yineng Zhang, Lianmin Zheng, Ying Sheng and many others), Tianle Cai, Hao Zhang, Ziteng Sun, and others.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>joanrod/star-vector</title>
    <updated>2025-03-24T01:39:28Z</updated>
    <id>tag:github.com,2025-03-24:/joanrod/star-vector</id>
    <link href="https://github.com/joanrod/star-vector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StarVector is a foundation model for SVG generation that transforms vectorization into a code generation task. Using a vision-language modeling architecture, StarVector processes both visual and textual inputs to produce high-quality SVG code with remarkable precision.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üí´ StarVector: Generating Scalable Vector Graphics Code from Images and Text&lt;/h1&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/assets/starvector-xyz.png&#34; alt=&#34;starvector&#34; style=&#34;width: 800px; display: block; margin-left: auto; margin-right: auto;&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2312.11556&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;arXiv&#34; src=&#34;https://img.shields.io/badge/arXiv-StarVector-red?logo=arxiv&#34; height=&#34;25&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://starvector.github.io/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Website&#34; src=&#34;https://img.shields.io/badge/%F0%9F%8C%8E_Website-starvector.github.io-blue.svg?sanitize=true&#34; height=&#34;25&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/starvector/starvector-1b-im2svg&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;HF Models: StarVector&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20_Model-StarVector--1B-ffc107?color=ffc107&amp;amp;logoColor=white&#34; height=&#34;25&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/starvector/starvector-8b-im2svg&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;HF Models: StarVector&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20_Model-StarVector--8B-ffc107?color=ffc107&amp;amp;logoColor=white&#34; height=&#34;25&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-stack&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;HF Dataset: SVG-Stack&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20_Data-SVG--Stack-ffc107?color=ffc107&amp;amp;logoColor=white&#34; height=&#34;25&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/collections/starvector/starvector-svg-datasets-svg-bench-67811204a76475be4dd66d09&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;HF Dataset: SVG-Bench&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20_Benchmark-SVG--Bench-ffc107?color=ffc107&amp;amp;logoColor=white&#34; height=&#34;25&#34;&gt; &lt;/a&gt; &#xA; &lt;div style=&#34;font-family: charter;&#34;&gt; &#xA;  &lt;a href=&#34;https://joanrod.github.io&#34; target=&#34;_blank&#34;&gt;Juan A. Rodriguez&lt;/a&gt;, &#xA;  &lt;a href=&#34;https://abhaypuri.github.io/portfolio/&#34; target=&#34;_blank&#34;&gt;Abhay Puri&lt;/a&gt;, &#xA;  &lt;a href=&#34;https://shubhamagarwal92.github.io/&#34; target=&#34;_blank&#34;&gt;Shubham Agarwal&lt;/a&gt;, &#xA;  &lt;a href=&#34;https://scholar.google.ca/citations?user=8vRS7F0AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Issam H. Laradji&lt;/a&gt;, &#xA;  &lt;a href=&#34;https://scholar.google.es/citations?user=IwBx73wAAAAJ&amp;amp;hl=ca&#34; target=&#34;_blank&#34;&gt;Pau Rodriguez&lt;/a&gt;, &#xA;  &lt;a href=&#34;https://scholar.google.es/citations?user=1jHvtfsAAAAJ&amp;amp;hl=ca&#34; target=&#34;_blank&#34;&gt;David Vazquez&lt;/a&gt;, &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?user=1ScWJOoAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Chris Pal&lt;/a&gt;, &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?user=aVfyPAoAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Marco Pedersoli&lt;/a&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;March 2025: &lt;strong&gt;StarVector Accepted at CVPR 2025&lt;/strong&gt;, &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;StarVector has been accepted at CVPR 2025! Check out the paper [&lt;a href=&#34;https://arxiv.org/abs/2312.11556&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;Check out our website for more information [&lt;a href=&#34;https://starvector.github.io/&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;StarVector models are now available on HuggingFace! [&lt;a href=&#34;https://huggingface.co/starvector/starvector-1b-im2svg&#34;&gt;Link&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/starvector/starvector-8b-im2svg&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt; &#xA;   &lt;li&gt;SVGBench and SVG-Stack datasets are now available on HuggingFace Datasets! [&lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-bench&#34;&gt;Link&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-stack&#34;&gt;Link&lt;/a&gt;]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Introduction&lt;/h2&gt; &#xA;&lt;p&gt;StarVector is a multimodal vision-language model for Scalable Vector Graphics (SVG) generation. It can be used to perform image2SVG and text2SVG generation. We pose image generation as a code generation task, using the power of multimodal VLMs&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/assets/starvector-teaser.png&#34; alt=&#34;starvector&#34; style=&#34;width: 900px; display: block; margin-left: auto; margin-right: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Scalable Vector Graphics (SVGs) are vital for modern image rendering due to their scalability and versatility. Previous SVG generation methods have focused on curve-based vectorization, lacking semantic understanding, often producing artifacts, and struggling with SVG primitives beyond \textit{path} curves. To address these issues, we introduce StarVector, a multimodal large language model for SVG generation. It performs image vectorization by understanding image semantics and using SVG primitives for compact, precise outputs. Unlike traditional methods, StarVector works directly in the SVG code space, leveraging visual understanding to apply accurate SVG primitives. To train StarVector, we create SVG-Stack, a diverse dataset of 2M samples that enables generalization across vectorization tasks and precise use of primitives like ellipses, polygons, and text. We address challenges in SVG evaluation, showing that pixel-based metrics like MSE fail to capture the unique qualities of vector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3 tasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using this setup, StarVector achieves state-of-the-art performance, producing more compact and semantically rich SVGs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Multimodal Architecture&lt;/h3&gt; &#xA;&lt;p&gt;StarVector uses a multimodal architecture to process images and text. When performing Image-to-SVG (or image vectorization), the image is projected into visual tokens, and SVG code is generated. When performing Text-to-SVG, the model only recieves the text instruction (no image is provided), and a novel SVG is created. The LLM is based of StarCoder, which we leverage to transfer coding skills to SVG generation.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/assets/starvector-arch.png&#34; alt=&#34;starvector&#34; style=&#34;width: 700px; display: block; margin-left: auto; margin-right: auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìñ Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#installation&#34;&gt;üíø Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#quick-start---image2svg-generation&#34;&gt;üèéÔ∏è Quick Start - Image2SVG Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#models&#34;&gt;üé® Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#datasets---svg-bench&#34;&gt;üìä Datasets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#training&#34;&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#validation-on-svg-benchmarks-svg-bench&#34;&gt;üèÜ Evaluation on SVG-Bench&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#starvector-demo&#34;&gt;üß© Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#citation&#34;&gt;üìö Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/#license&#34;&gt;üìù License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to star-vector folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/joanrod/star-vector.git&#xA;cd star-vector&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n starvector python=3.11.3 -y&#xA;conda activate starvector&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upgrade to latest code base&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git pull&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start - Image2SVG Generation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from PIL import Image&#xA;from starvector.model.starvector_arch import StarVectorForCausalLM&#xA;from starvector.data.util import process_and_rasterize_svg&#xA;&#xA;model_name = &#34;starvector/starvector-8b-im2svg&#34;&#xA;&#xA;starvector = StarVectorForCausalLM.from_pretrained(model_name)&#xA;&#xA;starvector.cuda()&#xA;starvector.eval()&#xA;&#xA;image_pil = Image.open(&#39;assets/examples/sample-0.png&#39;)&#xA;image = starvector.process_images([image_pil])[0].cuda()&#xA;batch = {&#34;image&#34;: image}&#xA;&#xA;raw_svg = starvector.generate_im2svg(batch, max_length=1000)[0]&#xA;svg, raster_image = process_and_rasterize_svg(raw_svg)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use it from HuggingFace AutoModel&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from PIL import Image&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor&#xA;from starvector.data.util import process_and_rasterize_svg&#xA;import torch&#xA;&#xA;model_name = &#34;starvector/starvector-8b-im2svg&#34;&#xA;&#xA;starvector = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, trust_remote_code=True)&#xA;processor = starvector.model.processor&#xA;tokenizer = starvector.model.svg_transformer.tokenizer&#xA;&#xA;starvector.cuda()&#xA;starvector.eval()&#xA;&#xA;image_pil = Image.open(&#39;assets/examples/sample-18.png&#39;)&#xA;&#xA;image = processor(image_pil, return_tensors=&#34;pt&#34;)[&#39;pixel_values&#39;].cuda()&#xA;if not image.shape[0] == 1:&#xA;    image = image.squeeze(0)&#xA;batch = {&#34;image&#34;: image}&#xA;&#xA;raw_svg = starvector.generate_im2svg(batch, max_length=4000)[0]&#xA;svg, raster_image = process_and_rasterize_svg(raw_svg)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;We provide &lt;a href=&#34;https://huggingface.co/collections/starvector/starvector-models-6783b22c7bd4b43d13cb5289&#34;&gt;Hugging Face ü§ó model checkpoints&lt;/a&gt; for image2SVG vectorization, for üí´ StarVector-8B and üí´ StarVector-1B. These are the results on SVG-Bench, using the DinoScore metric.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;SVG-Stack&lt;/th&gt; &#xA;   &lt;th&gt;SVG-Fonts&lt;/th&gt; &#xA;   &lt;th&gt;SVG-Icons&lt;/th&gt; &#xA;   &lt;th&gt;SVG-Emoji&lt;/th&gt; &#xA;   &lt;th&gt;SVG-Diagrams&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AutoTrace&lt;/td&gt; &#xA;   &lt;td&gt;0.942&lt;/td&gt; &#xA;   &lt;td&gt;0.954&lt;/td&gt; &#xA;   &lt;td&gt;0.946&lt;/td&gt; &#xA;   &lt;td&gt;0.975&lt;/td&gt; &#xA;   &lt;td&gt;0.874&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Potrace&lt;/td&gt; &#xA;   &lt;td&gt;0.898&lt;/td&gt; &#xA;   &lt;td&gt;0.967&lt;/td&gt; &#xA;   &lt;td&gt;0.972&lt;/td&gt; &#xA;   &lt;td&gt;0.882&lt;/td&gt; &#xA;   &lt;td&gt;0.875&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VTracer&lt;/td&gt; &#xA;   &lt;td&gt;0.954&lt;/td&gt; &#xA;   &lt;td&gt;0.964&lt;/td&gt; &#xA;   &lt;td&gt;0.940&lt;/td&gt; &#xA;   &lt;td&gt;0.981&lt;/td&gt; &#xA;   &lt;td&gt;0.882&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Im2Vec&lt;/td&gt; &#xA;   &lt;td&gt;0.692&lt;/td&gt; &#xA;   &lt;td&gt;0.733&lt;/td&gt; &#xA;   &lt;td&gt;0.754&lt;/td&gt; &#xA;   &lt;td&gt;0.732&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LIVE&lt;/td&gt; &#xA;   &lt;td&gt;0.934&lt;/td&gt; &#xA;   &lt;td&gt;0.956&lt;/td&gt; &#xA;   &lt;td&gt;0.959&lt;/td&gt; &#xA;   &lt;td&gt;0.969&lt;/td&gt; &#xA;   &lt;td&gt;0.870&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DiffVG&lt;/td&gt; &#xA;   &lt;td&gt;0.810&lt;/td&gt; &#xA;   &lt;td&gt;0.821&lt;/td&gt; &#xA;   &lt;td&gt;0.952&lt;/td&gt; &#xA;   &lt;td&gt;0.814&lt;/td&gt; &#xA;   &lt;td&gt;0.822&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4-V&lt;/td&gt; &#xA;   &lt;td&gt;0.852&lt;/td&gt; &#xA;   &lt;td&gt;0.842&lt;/td&gt; &#xA;   &lt;td&gt;0.848&lt;/td&gt; &#xA;   &lt;td&gt;0.850&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üí´ StarVector-1B (ü§ó &lt;a href=&#34;https://huggingface.co/starvector/starvector-1b-im2svg&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;0.926&lt;/td&gt; &#xA;   &lt;td&gt;0.978&lt;/td&gt; &#xA;   &lt;td&gt;0.975&lt;/td&gt; &#xA;   &lt;td&gt;0.929&lt;/td&gt; &#xA;   &lt;td&gt;0.943&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üí´ StarVector-8B (ü§ó &lt;a href=&#34;https://huggingface.co/starvector/starvector-8b-im2svg&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.966&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.982&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.984&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.981&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.959&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: StarVector models will not work for natural images or illustrations, as they have not been trained on those images. They excel in vectorizing icons, logotypes, technical diagrams, graphs, and charts.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets - SVG-Bench&lt;/h2&gt; &#xA;&lt;p&gt;SVG-Bench is a benchmark for evaluating SVG generation models. It contains 10 datasets, and 3 tasks: Image-to-SVG, Text-to-SVG, and Diagram-to-SVG.&lt;/p&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://huggingface.co/collections/starvector/starvector-svg-datasets-67811204a76475be4dd66d09&#34;&gt;Huggingface ü§ó Dataset Collection&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Train&lt;/th&gt; &#xA;   &lt;th&gt;Val&lt;/th&gt; &#xA;   &lt;th&gt;Test&lt;/th&gt; &#xA;   &lt;th&gt;Token Length&lt;/th&gt; &#xA;   &lt;th&gt;SVG Primitives&lt;/th&gt; &#xA;   &lt;th&gt;Annotation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Stack (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-stack&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;2.1M&lt;/td&gt; &#xA;   &lt;td&gt;108k&lt;/td&gt; &#xA;   &lt;td&gt;5.7k&lt;/td&gt; &#xA;   &lt;td&gt;1,822 ¬± 1,808&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/starvector/text2svg-stack&#34;&gt;Captions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Stack_sim (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-stack-simple&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;601k&lt;/td&gt; &#xA;   &lt;td&gt;30.1k&lt;/td&gt; &#xA;   &lt;td&gt;1.5k&lt;/td&gt; &#xA;   &lt;td&gt;2k ¬± 918&lt;/td&gt; &#xA;   &lt;td&gt;Vector path&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Diagrams (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-diagrams&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;472&lt;/td&gt; &#xA;   &lt;td&gt;3,486 ¬± 1,918&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Fonts (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-fonts&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;1.8M&lt;/td&gt; &#xA;   &lt;td&gt;91.5k&lt;/td&gt; &#xA;   &lt;td&gt;4.8k&lt;/td&gt; &#xA;   &lt;td&gt;2,121 ¬± 1,868&lt;/td&gt; &#xA;   &lt;td&gt;Vector path&lt;/td&gt; &#xA;   &lt;td&gt;Font letter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Fonts_sim (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-fonts-simple&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;1.4M&lt;/td&gt; &#xA;   &lt;td&gt;71.7k&lt;/td&gt; &#xA;   &lt;td&gt;3.7k&lt;/td&gt; &#xA;   &lt;td&gt;1,722 ¬± 723&lt;/td&gt; &#xA;   &lt;td&gt;Vector path&lt;/td&gt; &#xA;   &lt;td&gt;Font letter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Emoji (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-emoji&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;8.7k&lt;/td&gt; &#xA;   &lt;td&gt;667&lt;/td&gt; &#xA;   &lt;td&gt;668&lt;/td&gt; &#xA;   &lt;td&gt;2,551 ¬± 1,805&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Emoji_sim (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-emoji-simple&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;580&lt;/td&gt; &#xA;   &lt;td&gt;57&lt;/td&gt; &#xA;   &lt;td&gt;96&lt;/td&gt; &#xA;   &lt;td&gt;2,448 ¬± 1,026&lt;/td&gt; &#xA;   &lt;td&gt;Vector Path&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Icons (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-icons&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;80.4k&lt;/td&gt; &#xA;   &lt;td&gt;6.2k&lt;/td&gt; &#xA;   &lt;td&gt;2.4k&lt;/td&gt; &#xA;   &lt;td&gt;2,449 ¬± 1,543&lt;/td&gt; &#xA;   &lt;td&gt;Vector path&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-Icons_sim (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/svg-icons-simple&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;80,435&lt;/td&gt; &#xA;   &lt;td&gt;2,836&lt;/td&gt; &#xA;   &lt;td&gt;1,277&lt;/td&gt; &#xA;   &lt;td&gt;2,005 ¬± 824&lt;/td&gt; &#xA;   &lt;td&gt;Vector path&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SVG-FIGR (ü§ó &lt;a href=&#34;https://huggingface.co/datasets/starvector/FIGR-SVG&#34;&gt;Link&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;270k&lt;/td&gt; &#xA;   &lt;td&gt;27k&lt;/td&gt; &#xA;   &lt;td&gt;3k&lt;/td&gt; &#xA;   &lt;td&gt;5,342 ¬± 2,345&lt;/td&gt; &#xA;   &lt;td&gt;Vector path&lt;/td&gt; &#xA;   &lt;td&gt;Class, Caption&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We offer a summary of statistics about the datasets used in our training and evaluation experiments. This datasets are included in SVG-Bench. The subscript &lt;em&gt;sim&lt;/em&gt; stands for the simplified version of the dataset, as required by some baselines.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Confirm dependencies are installed&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Set environment variables&lt;/h3&gt; &#xA;&lt;p&gt;We recommend setting the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  export HF_HOME=&amp;lt;path to the folder where you want to store the models&amp;gt;&#xA;  export HF_TOKEN=&amp;lt;your huggingface token&amp;gt;&#xA;  export WANDB_API_KEY=&amp;lt;your wandb token&amp;gt;&#xA;  export OUTPUT_DIR=&amp;lt;path/to/output&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;cd the root of the repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;cd star-vector&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image2SVG Pretraining (Stage 1)&lt;/h3&gt; &#xA;&lt;p&gt;We have different training approaches for StarVector-1B and StarVector-8B. StarVector-1B can be trained using Deepspeed, while StarVector-8B requires FSDP.&lt;/p&gt; &#xA;&lt;h4&gt;StarVector-1B Training&lt;/h4&gt; &#xA;&lt;p&gt;You can use the following command to train StarVector-1B on SVG-Stack for the Image2SVG vectorization task, using Deepspeed and Accelerate&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# StarVector-1B&#xA;accelerate launch --config_file configs/accelerate/deepspeed-8-gpu.yaml starvector/train/train.py config=configs/models/starvector-1b/im2svg-stack.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;StarVector-8B Training&lt;/h4&gt; &#xA;&lt;p&gt;You can use the following command to train StarVector-8B on SVG-Stack for the Image2SVG vectorization task, using FSDP and Accelerate. We provide the torchrun command to support multi-nodes and multi-GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# StarVector-8B&#xA;torchrun \&#xA;  --nproc-per-node=8 \&#xA;  --nnodes=1 \&#xA;  starvector/train/train.py \&#xA;  config=configs/models/starvector-8b/im2svg-stack.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Finetuning StarVector (Stage 2)&lt;/h3&gt; &#xA;&lt;p&gt;After pretraining StarVector on image vectorization, we finetune it on additional SVG tasks like Text2SVG, and SVG-Bench datasets.&lt;/p&gt; &#xA;&lt;h4&gt;Text2SVG Finetuning&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# StarVector-1B&#xA;accelerate launch --config_file config/accelerate/deepspeed-8-gpu.yaml starvector/train/train.py config=configs/models/starvector-1b/text2svg-stack.yaml&#xA;&#xA;# StarVector-8B&#xA;torchrun \&#xA;  --nproc-per-node=8 \&#xA;  --nnodes=1 \&#xA;  starvector/train/train.py \&#xA;  config=configs/models/starvector-8b/text2svg-stack.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SVG-Bench Finetuning&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# StarVector-1B&#xA;accelerate launch --config_file config/accelerate/deepspeed-8-gpu.yaml starvector/train/train.py config=configs/models/starvector-1b/im2svg-{fonts,icons,emoji}.yaml&#xA;&#xA;# StarVector-8B&#xA;torchrun \&#xA;  --nproc-per-node=8 \&#xA;  --nnodes=1 \&#xA;  starvector/train/train.py \&#xA;  config=configs/models/starvector-8b/im2svg-{fonts,icons,emoji}.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide shell scripts in &lt;code&gt;scripts/train/*&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Validation on SVG Benchmarks (‚≠ê SVG-Bench)&lt;/h2&gt; &#xA;&lt;p&gt;We validate StarVector on ‚≠ê SVG-Bench Benchmark. We provide the SVGValidator class that allows you to run StarVector using &lt;strong&gt;1) the HuggingFace generation backend&lt;/strong&gt; or &lt;strong&gt;2) the VLLM backend&lt;/strong&gt;. The later is substantially faster thanks to the use of Paged Attention.&lt;/p&gt; &#xA;&lt;h3&gt;HuggingFace Generation Backend&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s start with the evaluation for StarVector-1B and StarVector-8B on SVG-Stack, using the HuggingFace generation backend (StarVectorHFAPIValidator). To override the input arguments, you can add cli args following the yaml file structure.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# StarVector-1B on SVG-Stack, using the HuggingFace backend &#xA;python starvector/validation/validate.py \&#xA;config=configs/generation/hf/starvector-1b/im2svg.yaml \&#xA;dataset.name=starvector/svg-stack&#xA;&#xA;# StarVector-8B on SVG-Stack, using the vanilla HuggingFace generation API&#xA;python starvector/validation/validate.py \&#xA;config=configs/generation/hf/starvector-8b/im2svg.yaml \&#xA;dataset.name=starvector/svg-stack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;vLLM Backend&lt;/h3&gt; &#xA;&lt;p&gt;For using the vLLM backend (StarVectorVLLMAPIValidator), first install our StarVector fork of VLLM, &lt;a href=&#34;https://github.com/starvector/vllm&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/starvector/vllm.git&#xA;cd vllm&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, launch the using the vllm config file (it uses StarVectorVLLMValidator):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# StarVector-1B&#xA;python starvector/validation/validate.py \&#xA;config=configs/generation/vllm/starvector-1b/im2svg.yaml \&#xA;dataset.name=starvector/svg-stack&#xA;&#xA;# StarVector-8B&#xA;python starvector/validation/validate.py \&#xA;config=configs/generation/vllm/starvector-8b/im2svg.yaml \&#xA;dataset.name=starvector/svg-stack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide evaluation scripts in &lt;code&gt;scripts/eval/*&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;StarVector Demo&lt;/h2&gt; &#xA;&lt;p&gt;The demo provides two options for converting images to SVG code:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;HuggingFace generation functionality&lt;/li&gt; &#xA; &lt;li&gt;VLLM (recommended) - offers faster generation speed&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Option 1: HuggingFace Generation with Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;We provide a Gradio web UI for you to play with our model.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m starvector.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m starvector.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --port 7000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;p&gt;This is the actual &lt;em&gt;worker&lt;/em&gt; that performs the inference on the GPU. Each worker is responsible for a single model specified in &lt;code&gt;--model-path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m starvector.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path joanrodai/starvector-1.4b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;. Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.&lt;/p&gt; &#xA;&lt;p&gt;You can launch as many workers as you want, and compare between different model checkpoints in the same Gradio interface. Please keep the &lt;code&gt;--controller&lt;/code&gt; the same, and modify the &lt;code&gt;--port&lt;/code&gt; and &lt;code&gt;--worker&lt;/code&gt; to a different port number for each worker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;vllm serve starvector/starvector-8b-im2svg --chat-template configs/chat-template.jinja --trust-remote-code --port 8001 --max-model-len 16000&#xA;&#xA;python -m starvector.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port &amp;lt;different from 40000, say 40001&amp;gt; --worker http://localhost:&amp;lt;change accordingly, i.e. 40001&amp;gt; --model-path &amp;lt;ckpt2&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option 2: Launch VLLM&lt;/h4&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Remember to clone the starvector/vllm fork (it has modifications for starvector).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git clone https://github.com/starvector/vllm.git&#xA;cd vllm&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Call this to launch the VLLM endpoint&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;vllm serve starvector/starvector-1b-im2svg --chat-template configs/chat-template.jinja --trust-remote-code --port 8000 --max-model-len 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create the demo for VLLM&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m starvector.serve.vllm_api_gradio.controller --host 0.0.0.0 --port 10000&#xA;python -m starvector.serve.vllm_api_gradio.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --port 7000&#xA;python -m starvector.serve.vllm_api_gradio.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-name starvector/starvector-1b-im2svg --vllm-base-url http://localhost:8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Add more models by serving them with VLLM and calling a new model worker&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;vllm serve starvector/starvector-8b-im2svg --chat-template configs/chat-template.jinja --trust-remote-code --port 8001 --max-model-len 16384&#xA;&#xA;python -m starvector.serve.vllm_api_gradio.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40001 --worker http://localhost:40001 --model-name starvector/starvector-8b-im2svg --vllm-base-url http://localhost:8001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rodriguez2024starvector,&#xA;      title={StarVector: Generating Scalable Vector Graphics Code from Images and Text}, &#xA;      author={Juan A. Rodriguez and Abhay Puri and Shubham Agarwal and Issam H. Laradji and Pau Rodriguez and Sai Rajeswar and David Vazquez and Christopher Pal and Marco Pedersoli},&#xA;      year={2024},&#xA;      eprint={2312.11556},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2312.11556}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache License, Version 2.0 - see the &lt;a href=&#34;https://raw.githubusercontent.com/joanrod/star-vector/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
</feed>