<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-05T01:35:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>McGill-NLP/llm2vec</title>
    <updated>2024-08-05T01:35:14Z</updated>
    <id>tag:github.com,2024-08-05:/McGill-NLP/llm2vec</id>
    <link href="https://github.com/McGill-NLP/llm2vec" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for &#39;LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders&#39;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;em&gt;LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.05961&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2404.05961-b31b1b.svg?sanitize=true&#34; alt=&#34;arxiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llm2vec/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/llm2vec&#34; alt=&#34;PyPi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/McGill-NLP/llm2vec-660e14f536b3c8d10b3f1c34&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HF%20Models-LLM2Vec-FFD21E.svg?sanitize=true&#34; alt=&#34;HF Link&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/McGill-NLP/llm2vec/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/llm2vec&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/llm2vec&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;LLM2Vec is a simple recipe to convert decoder-only LLMs into text encoders. It consists of 3 simple steps: 1) enabling bidirectional attention, 2) training with masked next token prediction, and 3) unsupervised contrastive learning. The model can be further fine-tuned to achieve state-of-the-art performance.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/McGill-NLP/llm2vec/assets/12207571/48efd48a-431b-4625-8e0f-248a442e3839&#34; width=&#34;75%&#34; alt=&#34;LLM2Vec_figure1&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;**************************** &lt;strong&gt;Updates&lt;/strong&gt; ****************************&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;04/07: Added support for Gemma and Qwen-2 models, huge thanks to &lt;a href=&#34;https://github.com/bzantium&#34;&gt;@bzantium&lt;/a&gt; for the contribution.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;30/04: We release LLM2Vec transformed Meta-Llama-3 checkpoints. See our &lt;a href=&#34;https://huggingface.co/collections/McGill-NLP/llm2vec-660e14f536b3c8d10b3f1c34&#34;&gt;HuggingFace collection&lt;/a&gt; for both &lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised&#34;&gt;supervised&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse&#34;&gt;unsupervised&lt;/a&gt; variants.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use LLM2Vec, first install the llm2vec package from PyPI, followed by installing flash-attention:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install llm2vec&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also directly install the latest version of llm2vec by cloning the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;LLM2Vec class is a wrapper on top of HuggingFace models to support enabling bidirectionality in decoder-only LLMs, sequence encoding and pooling operations. The steps below showcase an example on how to use the library.&lt;/p&gt; &#xA;&lt;h3&gt;Preparing the model&lt;/h3&gt; &#xA;&lt;p&gt;Initializing LLM2Vec model using pretrained LLMs is straightforward. The &lt;code&gt;from_pretrained&lt;/code&gt; method of LLM2Vec takes a base model identifier/path and an optional PEFT model identifier/path. All HuggingFace model loading arguments can be passed to &lt;code&gt;from_pretrained&lt;/code&gt; method. By default, the models are loaded with bidirectional connections enabled. This can be turned off by passing &lt;code&gt;enable_bidirectional=False&lt;/code&gt; to the &lt;code&gt;from_pretrained&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;p&gt;Here, we first initialize the Llama-3 MNTP base model and load the unsupervised-trained LoRA weights (trained with SimCSE objective and wiki corpus).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from llm2vec import LLM2Vec&#xA;&#xA;l2v = LLM2Vec.from_pretrained(&#xA;    &#34;McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp&#34;,&#xA;    peft_model_name_or_path=&#34;McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse&#34;,&#xA;    device_map=&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;,&#xA;    torch_dtype=torch.bfloat16,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can also load the model with supervised-trained LoRA weights (trained with contrastive learning and public E5 data) by changing the &lt;code&gt;peft_model_name_or_path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from llm2vec import LLM2Vec&#xA;&#xA;l2v = LLM2Vec.from_pretrained(&#xA;    &#34;McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp&#34;,&#xA;    peft_model_name_or_path=&#34;McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised&#34;,&#xA;    device_map=&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;,&#xA;    torch_dtype=torch.bfloat16,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default the LLM2Vec model uses the &lt;code&gt;mean&lt;/code&gt; pooling strategy. You can change the pooling strategy by passing the &lt;code&gt;pooling_mode&lt;/code&gt; argument to the &lt;code&gt;from_pretrained&lt;/code&gt; method. Similarly, you can change the maximum sequence length by passing the &lt;code&gt;max_length&lt;/code&gt; argument (default is 512).&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;This model now returns the text embedding for any input in the form of &lt;code&gt;[[instruction1, text1], [instruction2, text2]]&lt;/code&gt; or &lt;code&gt;[text1, text2]&lt;/code&gt;. While training, we provide instructions for both sentences in symmetric tasks, and only for for queries in asymmetric tasks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Encoding queries using instructions&#xA;instruction = (&#xA;    &#34;Given a web search query, retrieve relevant passages that answer the query:&#34;&#xA;)&#xA;queries = [&#xA;    [instruction, &#34;how much protein should a female eat&#34;],&#xA;    [instruction, &#34;summit define&#34;],&#xA;]&#xA;q_reps = l2v.encode(queries)&#xA;&#xA;# Encoding documents. Instruction are not required for documents&#xA;documents = [&#xA;    &#34;As a general guideline, the CDC&#39;s average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you&#39;ll need to increase that if you&#39;re expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.&#34;,&#xA;    &#34;Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.&#34;,&#xA;]&#xA;d_reps = l2v.encode(documents)&#xA;&#xA;# Compute cosine similarity&#xA;q_reps_norm = torch.nn.functional.normalize(q_reps, p=2, dim=1)&#xA;d_reps_norm = torch.nn.functional.normalize(d_reps, p=2, dim=1)&#xA;cos_sim = torch.mm(q_reps_norm, d_reps_norm.transpose(0, 1))&#xA;&#xA;print(cos_sim)&#xA;&#34;&#34;&#34;&#xA;tensor([[0.6470, 0.1619],&#xA;        [0.0786, 0.5844]])&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More examples of classification, clustering, sentence similarity etc are present in &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Meta-Llama-3-8B&lt;/th&gt; &#xA;   &lt;th&gt;Mistral-7B&lt;/th&gt; &#xA;   &lt;th&gt;Llama-2-7B&lt;/th&gt; &#xA;   &lt;th&gt;Sheared-Llama-1.3B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bi + MNTP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bi + MNTP + SimCSE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-unsup-simcse&#34;&gt;HF Link&lt;/a&gt;**&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-unsup-simcse&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-unsup-simcse&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bi + MNTP + Supervised&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised&#34;&gt;HF Link&lt;/a&gt;*&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;* State-of-the-art on MTEB among models trained on public data&lt;/p&gt; &#xA;&lt;p&gt;** Unsupervised state-of-the-art on MTEB&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;MNTP training&lt;/h3&gt; &#xA;&lt;p&gt;To train the model with Masked Next Token Prediction (MNTP), you can use the &lt;code&gt;experiments/run_mntp.py&lt;/code&gt; script. It is adapted from HuggingFace Masked Language Modeling (MLM) &lt;a href=&#34;https://github.com/huggingface/transformers/raw/51bcadc10a569847b93a30dbe3a077037ae63bad/examples/pytorch/language-modeling/run_mlm.py&#34;&gt;script&lt;/a&gt;. To train the Meta-Llama-3-8B model with MNTP, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python experiments/run_mntp.py train_configs/mntp/MetaLlama3.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Meta-Llama-3-8B training configuration &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/mntp/MetaLlama3.json&#34;&gt;file&lt;/a&gt; contains all the training hyperparameters and configurations used in our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;model_name_or_path&#34;: &#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;,&#xA;    &#34;dataset_name&#34;: &#34;wikitext&#34;,&#xA;    &#34;dataset_config_name&#34;: &#34;wikitext-103-raw-v1&#34;,&#xA;    &#34;mask_token_type&#34;: &#34;blank&#34;,&#xA;    &#34;data_collator_type&#34;: &#34;default&#34;,&#xA;    &#34;mlm_probability&#34;: 0.2,&#xA;    &#34;lora_r&#34;: 16,&#xA;    &#34;gradient_checkpointing&#34;: true,&#xA;    &#34;torch_dtype&#34;: &#34;bfloat16&#34;,&#xA;    &#34;attn_implementation&#34;: &#34;flash_attention_2&#34;&#xA;    // ....&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similar configurations are also available for&lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/mntp/Mistral.json&#34;&gt;Mistral-7B&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/mntp/Llama2.json&#34;&gt;Llama-2-7B&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/mntp/Sheared-Llama.json&#34;&gt;Sheared-Llama-1.3B&lt;/a&gt; models.&lt;/p&gt; &#xA;&lt;h3&gt;Unsupervised contrastive training (SimCSE)&lt;/h3&gt; &#xA;&lt;p&gt;For SimCSE training, we replicated the training procedure from &lt;a href=&#34;https://arxiv.org/abs/2104.08821&#34;&gt;SimCSE&lt;/a&gt; paper. For training, we use the dataset 1 million sentences from English Wikipedia released by the authors. It can be downloaded using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the training script with pre-set configurations, the downloaded file should be placed in the &lt;code&gt;cache&lt;/code&gt; directory. The directory layout should be as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cache&#xA;└── wiki1m_for_simcse.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the dataset is placed in a different directory, please change the dataset_file_path in the training configuration accordingly.&lt;/p&gt; &#xA;&lt;p&gt;To train the Meta-Llama-3-8B model with SimCSE, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python experiments/run_simcse.py train_configs/simcse/MetaLlama3.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Meta-Llama-3-8B training configuration &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/simcse/MetaLlama3.json&#34;&gt;file&lt;/a&gt; contains all the training hyperparameters and configurations used in our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;model_name_or_path&#34;: &#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;,&#xA;    &#34;peft_model_name_or_path&#34;: &#34;McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp&#34;,&#xA;    &#34;simcse_dropout&#34;: 0.3,&#xA;    &#34;bidirectional&#34;: true,&#xA;    &#34;pooling_mode&#34;: &#34;mean&#34;,&#xA;    &#34;dataset_name&#34;: &#34;Wiki1M&#34;,&#xA;    &#34;dataset_file_path&#34;: &#34;cache/wiki1m_for_simcse.txt&#34;,&#xA;    &#34;learning_rate&#34;: 3e-5,&#xA;    &#34;loss_scale&#34;: 20,&#xA;    &#34;per_device_train_batch_size&#34;: 128,&#xA;    &#34;max_seq_length&#34;: 128,&#xA;    &#34;stop_after_n_steps&#34;: 1000,&#xA;    &#34;lora_r&#34;: 16,&#xA;    &#34;gradient_checkpointing&#34;: true,&#xA;    &#34;torch_dtype&#34;: &#34;bfloat16&#34;,&#xA;    &#34;attn_implementation&#34;: &#34;flash_attention_2&#34;,&#xA;    // ....&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similar configurations are also available for &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/simcse/Mistral.json&#34;&gt;Mistral&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/simcse/Llama2.json&#34;&gt;Llama-2-7B&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/simcse/Sheared-Llama.json&#34;&gt;Sheared-Llama-1.3B&lt;/a&gt; models.&lt;/p&gt; &#xA;&lt;h3&gt;Supervised contrastive training&lt;/h3&gt; &#xA;&lt;p&gt;For supervised contrastive training, we use the public portion of dataset used in &lt;a href=&#34;https://arxiv.org/abs/2401.00368&#34;&gt;Improving Text Embeddings with Large Language Models&lt;/a&gt;, curated by authors of &lt;a href=&#34;https://arxiv.org/abs/2402.15449&#34;&gt;Repetition Improves Language Model Embeddings&lt;/a&gt;. The dataset can be downloaded from the &lt;a href=&#34;https://github.com/jakespringer/echo-embeddings#training&#34;&gt;GitHub page of Echo embeddings repository&lt;/a&gt;. To use the training script, the downloaded dataset should be placed in the &lt;code&gt;cache&lt;/code&gt; directory. The directory layout should be as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cache&#xA;|── wiki1m_for_simcse.txt&#xA;└── echo-data&#xA;    ├── allnli_split1.jsonl&#xA;    ├── allnli_split2.jsonl&#xA;    ├── allnli.jsonl&#xA;    ├── dureader.jsonl&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the dataset is placed in a different directory, please change the &lt;code&gt;dataset_file_path&lt;/code&gt; in the training configuration accordingly.&lt;/p&gt; &#xA;&lt;p&gt;To train the Meta-Llama-3-8B model with supervised contrastive learning, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=8 experiments/run_supervised.py train_configs/supervised/MetaLlama3.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The number of GPUs can be changed by modifying the &lt;code&gt;--nproc_per_node&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;p&gt;The Meta-Llama-3-8B training configuration &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/supervised/MetaLlama3.json&#34;&gt;file&lt;/a&gt; contains all the training hyperparameters and configurations used in our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;model_name_or_path&#34;: &#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;,&#xA;    &#34;peft_model_name_or_path&#34;: &#34;McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp&#34;,&#xA;    &#34;bidirectional&#34;: true,&#xA;    &#34;pooling_mode&#34;: &#34;mean&#34;,&#xA;    &#34;dataset_name&#34;: &#34;E5&#34;,&#xA;    &#34;dataset_file_path&#34;: &#34;cache/echo-data&#34;,&#xA;    &#34;learning_rate&#34;: 2e-4,&#xA;    &#34;num_train_epochs&#34;: 3,&#xA;    &#34;warmup_steps&#34;: 300,&#xA;    &#34;per_device_train_batch_size&#34;: 64,&#xA;    &#34;lora_r&#34;: 16,&#xA;    &#34;gradient_checkpointing&#34;: true,&#xA;    &#34;torch_dtype&#34;: &#34;bfloat16&#34;,&#xA;    &#34;attn_implementation&#34;: &#34;flash_attention_2&#34;&#xA;    // ....&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similar configurations are also available for &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/supervised/Mistral.json&#34;&gt;Mistral&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/supervised/Llama2.json&#34;&gt;Llama-2-7B&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/supervised/Sheared-Llama.json&#34;&gt;Sheared-Llama-1.3B&lt;/a&gt; models.&lt;/p&gt; &#xA;&lt;h3&gt;Word-level tasks training&lt;/h3&gt; &#xA;&lt;p&gt;To tune the model for word-level tasks, we define a classifier on top of the models, and only train the classifier weights. The code is adapted from HuggingFace token classification &lt;a href=&#34;https://huggingface.co/docs/transformers/en/tasks/token_classification&#34;&gt;example&lt;/a&gt;. To train and test the classifier for Llama-2-7B MNTP model on &lt;code&gt;pos_tags&lt;/code&gt; task, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python experiments/run_word_task.py train_configs/word-task/Llama2-bi-mntp.json&#xA;python experiments/test_word_task.py --config_file test_configs/word-task/Llama2-bi-mntp.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The config files contain all the parameters and configurations used in our paper. For instance, &lt;code&gt;Llama2-bi-mntp.json&lt;/code&gt; includes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;model_name_or_path&#34;: &#34;meta-llama/Llama-2-7b-chat-hf&#34;,&#xA;    &#34;peft_addr&#34;: &#34;McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp&#34;, // or any local directory containing `adapter_model` files.&#xA;    &#34;model_class&#34;: &#34;custom&#34;,&#xA;    &#34;bidirectional&#34;: true,&#xA;    &#34;classifier_dropout&#34;: 0.1,&#xA;    &#34;merge_subwords&#34;: true,&#xA;    &#34;retroactive_labels&#34;: &#34;next_token&#34;,&#xA;    &#34;output_dir&#34;: &#34;output/word-task/pos_tags/Llama2/bi-mntp&#34;,&#xA;    &#34;dataset_name&#34;: &#34;conll2003&#34;,&#xA;    &#34;task&#34;: &#34;pos_tags&#34;, // or ner_tags, or chunk_tags&#xA;    // ....&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/word-task&#34;&gt;train_configs/word-task&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/McGill-NLP/llm2vec/main/train_configs/word-task&#34;&gt;test_configs/word-task&lt;/a&gt; contain similar configurations for Llama-2-7B, Mistral-7B, and Sheared-Llama-1.3B for all Uni, Bi, Bi-MNTP, and Bi-MNTP-SimCSE (LLM2Vec) variants.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;MTEB Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate the model on the MTEB benchmark, we use the &lt;code&gt;experiments/mteb_eval.py&lt;/code&gt; script. The script requires &lt;code&gt;mteb&amp;gt;=1.12.60&lt;/code&gt;, amongst other dependencies, which can be installed with the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install llm2vec[evaluation]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The evaluation utilizes instructions for each task which are provided in the &lt;code&gt;test_configs/mteb/task_to_instructions.json&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;To evaluate the supervised trained Meta-Llama-3-8B model on the &lt;code&gt;STS16&lt;/code&gt; task, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python experiments/mteb_eval.py --model_name McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised \&#xA;--task_name STS16 \&#xA;--task_to_instructions_fp test_configs/mteb/task_to_instructions.json \&#xA;--output_dir results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The evaluation script supports all the models available in the &lt;a href=&#34;https://huggingface.co/collections/McGill-NLP/llm2vec-660e14f536b3c8d10b3f1c34&#34;&gt;HuggingFace collection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please cite us:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{llm2vec,&#xA;      title={{LLM2Vec}: {L}arge Language Models Are Secretly Powerful Text Encoders}, &#xA;      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},&#xA;      year={2024},&#xA;      journal={arXiv preprint},&#xA;      url={https://arxiv.org/abs/2404.05961}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Bugs or questions?&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions about the code, feel free to open an issue on the GitHub repository.&lt;/p&gt;</summary>
  </entry>
</feed>