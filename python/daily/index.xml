<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-16T01:40:37Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>csunny/DB-GPT</title>
    <updated>2023-05-16T01:40:37Z</updated>
    <id>tag:github.com,2023-05-16:/csunny/DB-GPT</id>
    <link href="https://github.com/csunny/DB-GPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Interact your data and environment using the local GPT, no data leaks, 100% privately, 100% security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DB-GPT &lt;img src=&#34;https://img.shields.io/github/stars/csunny/db-gpt?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/README.en.md&#34;&gt;English Edition&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;背景&lt;/h2&gt; &#xA;&lt;p&gt;随着大模型的发布迭代，大模型变得越来越智能，在使用大模型的过程当中，遇到极大的数据安全与隐私挑战。在利用大模型能力的过程中我们的私密数据跟环境需要掌握自己的手里，完全可控，避免任何的数据隐私泄露以及安全风险。基于此，我们发起了DB-GPT项目，为所有以数据库为基础的场景，构建一套完整的私有大模型解决方案。 此方案因为支持本地部署，所以不仅仅可以应用于独立私有环境，而且还可以根据业务模块独立部署隔离，让大模型的能力绝对私有、安全、可控。&lt;/p&gt; &#xA;&lt;h2&gt;愿景&lt;/h2&gt; &#xA;&lt;p&gt;DB-GPT 是一个开源的以数据库为基础的GPT实验项目，使用本地化的GPT大模型与您的数据和环境进行交互，无数据泄露风险，100% 私密，100% 安全。&lt;/p&gt; &#xA;&lt;h2&gt;特性一览&lt;/h2&gt; &#xA;&lt;p&gt;目前我们已经发布了多种关键的特性，这里一一列举展示一下当前发布的能力。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQL 语言能力 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SQL生成&lt;/li&gt; &#xA;   &lt;li&gt;SQL诊断&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;私域问答与数据处理 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;数据库知识问答&lt;/li&gt; &#xA;   &lt;li&gt;数据处理&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;插件模型 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持自定义插件执行任务，原生支持Auto-GPT插件。如: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;SQL自动执行，获取查询结果&lt;/li&gt; &#xA;     &lt;li&gt;自动爬取学习知识&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;知识库统一向量存储/索引 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;非结构化数据支持&lt;/li&gt; &#xA;   &lt;li&gt;PDF、MarkDown、CSV、WebURL&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;效果演示&lt;/h2&gt; &#xA;&lt;p&gt;示例通过 RTX 4090 GPU 演示，&lt;a href=&#34;https://www.youtube.com/watch?v=1PWI6F89LPo&#34;&gt;YouTube 地址&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;运行环境演示&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;./assets/演示.gif&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/Auto-DB-GPT.gif&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;SQL 生成&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;生成建表语句&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/SQL_Gen_CreateTable.png&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;生成可运行SQL 首先选择对应的数据库, 然后模型即可根据对应的数据库 Schema 信息生成 SQL, 运行成功的效果如下面的演示：&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/exeable.png&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;数据库问答&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/DB_QA.png&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;基于默认内置知识库问答&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/VectorDBQA.png&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;自己新增知识库&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/new_knownledge.gif&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;从网络自己爬取数据学习&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TODO&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;架构方案&lt;/h2&gt; &#xA;&lt;p&gt;DB-GPT基于&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; 构建大模型运行环境，并提供 vicuna 作为基础的大语言模型。此外，我们通过langchain提供私域知识库问答能力。同时我们支持插件模式, 在设计上原生支持Auto-GPT插件。&lt;/p&gt; &#xA;&lt;p&gt;整个DB-GPT的架构，如下图所示&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/DB-GPT.png&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;核心能力主要有以下几个部分。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;知识库能力&lt;/li&gt; &#xA; &lt;li&gt;大模型管理能力&lt;/li&gt; &#xA; &lt;li&gt;统一的数据向量化存储与索引&lt;/li&gt; &#xA; &lt;li&gt;连接模块&lt;/li&gt; &#xA; &lt;li&gt;Agent与插件&lt;/li&gt; &#xA; &lt;li&gt;Prompt自动生成与优化&lt;/li&gt; &#xA; &lt;li&gt;多端产品界面&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;下面对每个模块也做一些简要的介绍:&lt;/p&gt; &#xA;&lt;h3&gt;知识库能力&lt;/h3&gt; &#xA;&lt;p&gt;知识库作为当前用户需求最大的场景，我们原生支持知识库的构建与处理。同时在本项目当中，也提供了多种知识库的管理策略。 如: 1. 默认内置知识库 2. 自定义新增知识库 3. 通过插件能力自抓取构建知识库等多种使用场景。 用户只需要整理好知识文档，即可用我们现有的能力构建大模型所需要的知识库能力。&lt;/p&gt; &#xA;&lt;h3&gt;大模型管理能力&lt;/h3&gt; &#xA;&lt;p&gt;在底层大模型接入中，设计了开放的接口，支持对接多种大模型。同时对于接入模型的效果，我们有非常严格的把控与评审机制。对大模型能力上与ChatGPT对比，在准确率上需要满足85%以上的能力对齐。我们用更高的标准筛选模型，是期望在用户使用过程中，可以省去前面繁琐的测试评估环节。&lt;/p&gt; &#xA;&lt;h3&gt;统一的数据向量化存储与索引&lt;/h3&gt; &#xA;&lt;p&gt;为了方便对知识向量化之后的管理，我们内置了多种向量存储引擎，从基于内存的Chroma到分布式的Milvus, 可以根据自己的场景需求，选择不同的存储引擎，整个知识向量存储是AI能力增强的基石，向量作为人与大语言模型交互的中间语言，在本项目中的作用非常重要。&lt;/p&gt; &#xA;&lt;h3&gt;连接模块&lt;/h3&gt; &#xA;&lt;p&gt;为了能够更方便的与用户的私有环境进行交互，项目设计了连接模块，连接模块可以支持连接到数据库、Excel、知识库等等多种环境当中，实现信息与数据交互。&lt;/p&gt; &#xA;&lt;h3&gt;Agent与插件&lt;/h3&gt; &#xA;&lt;p&gt;Agent与插件能力是大模型能否自动化的核心，在本的项目中，原生支持插件模式，大模型可以自动化完成目标。 同时为了充分发挥社区的优势，本项目中所用的插件原生支持Auto-GPT插件生态，即Auto-GPT的插件可以直接在我们的项目中运行。&lt;/p&gt; &#xA;&lt;h3&gt;Prompt自动生成与优化&lt;/h3&gt; &#xA;&lt;p&gt;Prompt是与大模型交互过程中非常重要的部分，一定程度上Prompt决定了大模型生成答案的质量与准确性，在本的项目中，我们会根据用户输入与使用场景，自动优化对应的Prompt，让用户使用大语言模型变得更简单、更高效。&lt;/p&gt; &#xA;&lt;h3&gt;多端产品界面&lt;/h3&gt; &#xA;&lt;p&gt;TODO: 在终端展示上，我们将提供多端产品界面。包括PC、手机、命令行、slack等多种模式。&lt;/p&gt; &#xA;&lt;h2&gt;安装教程&lt;/h2&gt; &#xA;&lt;h3&gt;硬件说明&lt;/h3&gt; &#xA;&lt;p&gt;因为我们的项目在效果上具备ChatGPT 85%以上的能力，因此对硬件有一定的要求。 但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;GPU型号 |  显存大小 |   性能&#xA;-------|----------|------------------------------&#xA;TRX4090| 24G      |可以流畅的进行对话推理，无卡顿&#xA;TRX3090| 24G      |可以流畅进行对话推理，有卡顿感，但好与V100&#xA;V100   | 16G      |可以进行对话推理，有明显卡顿&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;DB-GPT安装&lt;/h3&gt; &#xA;&lt;p&gt;本项目依赖一个本地的 MySQL 数据库服务，你需要本地安装，推荐直接使用 Docker 安装。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=aa12345678 -dit mysql:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;向量数据库我们默认使用的是Chroma内存数据库，所以无需特殊安装，如果有需要连接其他的同学，可以按照我们的教程进行安装配置。整个DB-GPT的安装过程，我们使用的是miniconda3的虚拟环境。创建虚拟环境，并安装python依赖包&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python&amp;gt;=3.10&#xA;conda create -n dbgpt_env python=3.10&#xA;conda activate dbgpt_env&#xA;pip install -r requirements.txt&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;或者也可以使用命令:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd DB-GPT&#xA;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;另外需要设置一下python包路径, 避免出现运行时找不到包&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#34;/root/workspace/DB-GPT&#34; &amp;gt; /root/miniconda3/env/dbgpt_env/lib/python3.10/site-packages/dbgpt.pth &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. 运行大模型&lt;/h3&gt; &#xA;&lt;p&gt;关于基础模型, 可以根据&lt;a href=&#34;https://github.com/lm-sys/FastChat/raw/main/README.md#model-weights&#34;&gt;vicuna&lt;/a&gt;合成教程进行合成。 如果此步有困难的同学，也可以直接使用&lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt;上的模型进行替代. &lt;a href=&#34;https://huggingface.co/Tribbiani/vicuna-7b&#34;&gt;替代模型&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;运行模型服务&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd pilot/server&#xA;python llmserver.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行 gradio webui&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python webserver.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;使用说明&lt;/h2&gt; &#xA;&lt;p&gt;我们提供了gradio的用户界面，可以通过我们的用户界面使用DB-GPT， 同时关于我们项目相关的一些代码跟原理介绍，我们也准备了以下几篇参考文章。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/628750042&#34;&gt;大模型实战系列(1) —— 强强联合Langchain-Vicuna应用实战&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/629467580&#34;&gt;大模型实战系列(2) —— DB-GPT 阿里云部署指南&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/629623125&#34;&gt;大模型实战系列(3) —— DB-GPT插件模型原理与使用&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;感谢&lt;/h2&gt; &#xA;&lt;p&gt;项目取得的成果，需要感谢技术社区，尤其以下项目。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; 提供 chat 服务&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Tribbiani/vicuna-13b&#34;&gt;vicuna-13b&lt;/a&gt; 作为基础模型&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;langchain&lt;/a&gt; 工具链&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;AutoGPT&lt;/a&gt; 通用的插件模版&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; 大模型管理&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chroma-core/chroma&#34;&gt;Chroma&lt;/a&gt; 向量存储&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://milvus.io/&#34;&gt;Milvus&lt;/a&gt; 分布式向量存储&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM&lt;/a&gt; 基础模型&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;llama-index&lt;/a&gt; 基于现有知识库进行&lt;a href=&#34;https://arxiv.org/abs/2301.00234&#34;&gt;In-Context Learning&lt;/a&gt;来对其进行数据库相关知识的增强。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- GITCONTRIBUTOR_START --&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/csunny&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/17919400?v=4&#34; width=&#34;100px;&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;csunny&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/xudafeng&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1011681?v=4&#34; width=&#34;100px;&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;xudafeng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yhjun1026&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7636723?s=96&amp;amp;v=4&#34; width=&#34;100px;&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;明天&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Aries-ckt&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/13723926?v=4&#34; width=&#34;100px;&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Aries-ckt&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/thebigbone&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/95130644?v=4&#34; width=&#34;100px;&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;thebigbone&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This project follows the git-contributor &lt;a href=&#34;https://github.com/xudafeng/git-contributor&#34;&gt;spec&lt;/a&gt;, auto updated at &lt;code&gt;Sun May 14 2023 23:02:43 GMT+0800&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- GITCONTRIBUTOR_END --&gt; &#xA;&lt;p&gt;这是一个用于数据库的复杂且创新的工具, 我们的项目也在紧急的开发当中, 会陆续发布一些新的feature。如在使用当中有任何具体问题, 优先在项目下提issue, 如有需要, 请联系如下微信，我会尽力提供帮助，同时也非常欢迎大家参与到项目建设中。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/csunny/DB-GPT/main/assets/wechat.jpg&#34; width=&#34;320px&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;The MIT License (MIT)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jimmyyhwu/tidybot</title>
    <updated>2023-05-16T01:40:37Z</updated>
    <id>tag:github.com,2023-05-16:/jimmyyhwu/tidybot</id>
    <link href="https://github.com/jimmyyhwu/tidybot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TidyBot: Personalized Robot Assistance with Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tidybot&lt;/h1&gt; &#xA;&lt;p&gt;This code release accompanies the following project:&lt;/p&gt; &#xA;&lt;h3&gt;TidyBot: Personalized Robot Assistance with Large Language Models&lt;/h3&gt; &#xA;&lt;p&gt;Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tidybot.cs.princeton.edu&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://tidybot.cs.princeton.edu/paper.pdf&#34;&gt;PDF&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2305.05658&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/iik68a1ztMU&#34;&gt;Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people&#39;s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/jimmyyhwu/tidybot/assets/6546428/dc1e87d9-f931-4741-916c-3dd11127b485&#34; alt=&#34;&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/jimmyyhwu/tidybot/assets/6546428/6b21b3c3-d360-4d58-89d6-6c1dfd57ac19&#34; alt=&#34;&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/jimmyyhwu/tidybot/assets/6546428/73f61e30-90ad-4f40-a5c8-1007adad897b&#34; alt=&#34;&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/jimmyyhwu/tidybot/assets/6546428/643fe718-12b2-44ca-afa7-f6ff2ddc304c&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/jimmyyhwu/tidybot/assets/6546428/afc749b6-0c3b-4b8e-a0d2-ddd1ba0a6bf5&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/jimmyyhwu/tidybot/assets/6546428/12e51dea-34a4-4c91-8423-34757bb485b7&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Here is an overview of how this codebase is organized:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server&#34;&gt;&lt;code&gt;server&lt;/code&gt;&lt;/a&gt;: Server code for TidyBot (runs on GPU workstation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/robot&#34;&gt;&lt;code&gt;robot&lt;/code&gt;&lt;/a&gt;: Robot code for TidyBot (runs on mobile base computer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/stl&#34;&gt;&lt;code&gt;stl&lt;/code&gt;&lt;/a&gt;: Files for 3D printed parts&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/benchmark&#34;&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/a&gt;: Code for the benchmark dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Conda&lt;/a&gt; environments. Our setup (tested on Ubuntu 20.04.6 LTS) uses the following 3 environments:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;tidybot&lt;/code&gt; env on the server for general use&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tidybot&lt;/code&gt; env on the robot for general use&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vild&lt;/code&gt; env on the server for object detection only&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See the respective READMEs inside the &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server&#34;&gt;&lt;code&gt;server&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/robot&#34;&gt;&lt;code&gt;robot&lt;/code&gt;&lt;/a&gt; directories for detailed setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;TidyBot Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Unless otherwise specified, the &lt;code&gt;tidybot&lt;/code&gt; Conda env should always be used:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate tidybot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Teleoperation Mode&lt;/h3&gt; &#xA;&lt;p&gt;We provide a teleoperation interface (&lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server/teleop.py&#34;&gt;&lt;code&gt;teleop.py&lt;/code&gt;&lt;/a&gt;) to operate the robot using primitives such as pick, place, or toss.&lt;/p&gt; &#xA;&lt;p&gt;First, run this command to start the teleop interface on the server (workstation), where &lt;code&gt;&amp;lt;robot-num&amp;gt;&lt;/code&gt; is &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt;, or &lt;code&gt;3&lt;/code&gt;, depending on the robot to be controlled:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python teleop.py --robot-num &amp;lt;robot-num&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On the robot (mobile base computer), make sure that the convenience stop and mobile base driver are both running. Then, run this command to start the controller:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python controller.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the server and robot both show that they have successfully connected to each other, use these controls to teleop the robot:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Click on the overhead image to select waypoints&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt; to execute selected waypoints on the robot&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;&amp;lt;Esc&amp;gt;&lt;/code&gt; to clear selected waypoints or to stop the robot&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;0&lt;/code&gt; through &lt;code&gt;5&lt;/code&gt; to change the selected primitive&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;q&lt;/code&gt; to quit&lt;/li&gt; &#xA; &lt;li&gt;If necessary, use the convenience stop to kill the controller&lt;/li&gt; &#xA; &lt;li&gt;If necessary, use the e-stop to cut power to the robot (the mobile base computer will stay on)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If keypresses are not registering, make sure that the teleop interface is the active window&lt;/li&gt; &#xA; &lt;li&gt;The default primitive (index &lt;code&gt;0&lt;/code&gt;) is movement-only (no arm). To use the arm, you will need to change the selected primitive to something else. Check the terminal output to see the list of all primitives as well as the currently selected primitive.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To generate paths with an occupancy map rather than manually clicking waypoints, use the &lt;code&gt;--shortest-path&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python teleop.py --robot-num &amp;lt;robot-num&amp;gt; --shortest-path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will load the receptacles specified in &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server/scenarios/test.yml&#34;&gt;&lt;code&gt;scenarios/test.yml&lt;/code&gt;&lt;/a&gt; as obstacles and build an occupancy map to avoid running into them.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For additional debugging visualization, the &lt;code&gt;--debug&lt;/code&gt; flag can be used.&lt;/p&gt; &#xA;&lt;p&gt;Server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python teleop.py --robot-num &amp;lt;robot-num&amp;gt; --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Robot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python controller.py --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fully Autonomous Mode&lt;/h3&gt; &#xA;&lt;p&gt;To operate the robot in fully autonomous mode, we use the demo interface in &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server/demo.py&#34;&gt;&lt;code&gt;demo.py&lt;/code&gt;&lt;/a&gt;. By default, the demo will load the test scenario in &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server/scenarios/test.yml&#34;&gt;&lt;code&gt;scenarios/test.yml&lt;/code&gt;&lt;/a&gt; along with the corresponding LLM-summarized user preferences in &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server/preferences/test.yml&#34;&gt;&lt;code&gt;preferences/test.yml&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To start the demo on the server, first start the object detector server with the &lt;code&gt;vild&lt;/code&gt; Conda env:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate vild&#xA;python object_detector_server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, in a separate terminal, start the demo interface (with the &lt;code&gt;tidybot&lt;/code&gt; env):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py --robot-num &amp;lt;robot-num&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On the robot, make sure that the convenience stop and mobile base driver are both running. Then, run this command to start the controller:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python controller.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These are the controls used to run the demo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Press &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt; to start the robot&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;&amp;lt;Esc&amp;gt;&lt;/code&gt; to stop the robot at any time&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;0&lt;/code&gt; to enter supervised mode (the default mode), in which the robot will wait for human approval (via an &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt; keypress) before executing every command&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;1&lt;/code&gt; to enter autonomous mode, in which the robot will start executing commands whenever &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt; is pressed and stop moving whenever &lt;code&gt;&amp;lt;Esc&amp;gt;&lt;/code&gt; is pressed&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;q&lt;/code&gt; to quit&lt;/li&gt; &#xA; &lt;li&gt;If necessary, use the convenience stop to kill the controller&lt;/li&gt; &#xA; &lt;li&gt;If necessary, use the e-stop to cut power to the robot (the mobile base computer will stay on)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;To load a different scenario (default is &lt;code&gt;test&lt;/code&gt;), use the &lt;code&gt;--scenario-name&lt;/code&gt; argument:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py --robot-num &amp;lt;robot-num&amp;gt; --scenario-name &amp;lt;scenario-name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, to load &lt;code&gt;scenario-08&lt;/code&gt; and use robot #1, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py --robot-num 1 --scenario-name scenario-08&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For additional debugging visualization, the &lt;code&gt;--debug&lt;/code&gt; flag can be used.&lt;/p&gt; &#xA;&lt;p&gt;Server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py --robot-num &amp;lt;robot-num&amp;gt; --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Robot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python controller.py --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;h3&gt;Mobile Base Accuracy&lt;/h3&gt; &#xA;&lt;p&gt;The marker detection setup should output 2D robot pose estimates with centimeter-level accuracy. For instance, our setup can reliably pick up small Lego Duplo blocks (32 mm x 32 mm) from the floor. Inaccurate marker detection can be due to many reasons, such as inaccurate camera alignment or suboptimal camera settings (see &lt;code&gt;get_video_cap&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server/utils.py&#34;&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;). Also note that the mobile base motors should be calibrated (&lt;code&gt;.motor_cal.txt&lt;/code&gt;) for more accurate movement.&lt;/p&gt; &#xA;&lt;h3&gt;Arm Accuracy&lt;/h3&gt; &#xA;&lt;p&gt;The 3 Kinova arms are repeatable but have slightly different zero heading positions, so they require some compensation to be consistent with each other. See the arm-dependent heading compensation in &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/robot/controller.py&#34;&gt;&lt;code&gt;controller.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Server Ports&lt;/h3&gt; &#xA;&lt;p&gt;If multiple people have been using the server, you may run into this error:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OSError: [Errno 98] Address already in use&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To kill all processes using the occupied ports, you can use the &lt;a href=&#34;https://raw.githubusercontent.com/jimmyyhwu/tidybot/main/server/clear-ports.sh&#34;&gt;&lt;code&gt;clear-ports.sh&lt;/code&gt;&lt;/a&gt; script (requires sudo):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./clear-ports.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For reference, here are all of the ports used by this codebase:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;6000&lt;/code&gt;: Camera server (serial: &lt;code&gt;E4298F4E&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6001&lt;/code&gt;: Camera server (serial: &lt;code&gt;099A11EE&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6002&lt;/code&gt;: Marker detector server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6003&lt;/code&gt;: Object detector server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6004&lt;/code&gt;: Robot 1 controller server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6005&lt;/code&gt;: Robot 2 controller server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6006&lt;/code&gt;: Robot 3 controller server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6007&lt;/code&gt;: Robot 1 control&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6008&lt;/code&gt;: Robot 2 control&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6009&lt;/code&gt;: Robot 3 control&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6010&lt;/code&gt;: Robot 1 camera&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6011&lt;/code&gt;: Robot 2 camera&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;6012&lt;/code&gt;: Robot 3 camera&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Camera Errors&lt;/h3&gt; &#xA;&lt;p&gt;The overhead cameras may occasionally output errors such as this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[ WARN:16@1367.080] global /io/opencv/modules/videoio/src/cap_v4l.cpp (1013) tryIoctl VIDEOIO(V4L2:/dev/v4l/by-id/usb-046d_Logitech_Webcam_C930e_E4298F4E-video-index0): select() timeout.&#xA;[ WARN:16@2049.229] global /io/opencv/modules/videoio/src/cap_v4l.cpp (1013) tryIoctl VIDEOIO(V4L2:/dev/v4l/by-id/usb-046d_Logitech_Webcam_C930e_099A11EE-video-index0): select() timeout.&#xA;Corrupt JPEG data: 36 extraneous bytes before marker 0xd9&#xA;Corrupt JPEG data: premature end of data segment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Typically, these errors can be resolved by unplugging the camera and plugging it back in.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to also check the quality and length of the USB extension cable, as USB 2.0 does not support cable lengths longer than 5 meters.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful for your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wu2023tidybot,&#xA;  title = {TidyBot: Personalized Robot Assistance with Large Language Models},&#xA;  author = {Wu, Jimmy and Antonova, Rika and Kan, Adam and Lepert, Marion and Zeng, Andy and Song, Shuran and Bohg, Jeannette and Rusinkiewicz, Szymon and Funkhouser, Thomas},&#xA;  journal = {arXiv preprint arXiv:2305.05658},&#xA;  year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>StanGirard/quivr</title>
    <updated>2023-05-16T01:40:37Z</updated>
    <id>tag:github.com,2023-05-16:/StanGirard/quivr</id>
    <link href="https://github.com/StanGirard/quivr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dump all your files and thoughts into your GenerativeAI brain and chat with it&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Quivr&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/StanGirard/quivr/main/logo.png&#34; alt=&#34;Quivr-logo&#34; width=&#34;30%&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;Join our &lt;a href=&#34;https://discord.gg/5SDcUSzF&#34;&gt;discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Quivr is your second brain in the cloud, designed to easily store and retrieve unstructured information. It&#39;s like Obsidian but powered by generative AI.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Store Anything&lt;/strong&gt;: Quivr can handle almost any type of data you throw at it. Text, images, code snippets, you name it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generative AI&lt;/strong&gt;: Quivr uses advanced AI to help you generate and retrieve information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast and Efficient&lt;/strong&gt;: Designed with speed and efficiency in mind. Quivr makes sure you can access your data as quickly as possible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Secure&lt;/strong&gt;: Your data is stored securely in the cloud and is always under your control.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compatible Files&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Text&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;PDF&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Audio&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Quivr is open source and free to use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Demo with GPT3.5&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/StanGirard/Quivr/assets/19614572/a3cddc6a-ca28-44ad-9ede-3122fa918b51&#34;&gt;https://github.com/StanGirard/Quivr/assets/19614572/a3cddc6a-ca28-44ad-9ede-3122fa918b51&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo with Claude 100k context&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/StanGirard/quivr/assets/5101573/9dba918c-9032-4c8d-9eea-94336d2c8bd4&#34;&gt;https://github.com/StanGirard/quivr/assets/5101573/9dba918c-9032-4c8d-9eea-94336d2c8bd4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;What things you need to install the software and how to install them.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher&lt;/li&gt; &#xA; &lt;li&gt;Pip&lt;/li&gt; &#xA; &lt;li&gt;Virtualenv&lt;/li&gt; &#xA; &lt;li&gt;Supabase account&lt;/li&gt; &#xA; &lt;li&gt;Supabase API key&lt;/li&gt; &#xA; &lt;li&gt;Supabase URL&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:StanGirard/Quivr.git &amp;amp; cd Quivr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a virtual environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;virtualenv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Activate the virtual environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copy the streamlit secrets.toml example file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .streamlit/secrets.toml.example .streamlit/secrets.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add your credentials to .streamlit/secrets.toml file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;supabase_url = &#34;SUPABASE_URL&#34;&#xA;supabase_service_key = &#34;SUPABASE_SERVICE_KEY&#34;&#xA;openai_api_key = &#34;OPENAI_API_KEY&#34;&#xA;anthropic_api_key = &#34;ANTHROPIC_API_KEY&#34; # Optional&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the migration script on the Supabase database via the web interface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- Enable the pgvector extension to work with embedding vectors&#xA;       create extension vector;&#xA;&#xA;       -- Create a table to store your documents&#xA;       create table documents (&#xA;       id bigserial primary key,&#xA;       content text, -- corresponds to Document.pageContent&#xA;       metadata jsonb, -- corresponds to Document.metadata&#xA;       embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed&#xA;       );&#xA;&#xA;       CREATE FUNCTION match_documents(query_embedding vector(1536), match_count int)&#xA;           RETURNS TABLE(&#xA;               id bigint,&#xA;               content text,&#xA;               metadata jsonb,&#xA;               -- we return matched vectors to enable maximal marginal relevance searches&#xA;               embedding vector(1536),&#xA;               similarity float)&#xA;           LANGUAGE plpgsql&#xA;           AS $$&#xA;           # variable_conflict use_column&#xA;       BEGIN&#xA;           RETURN query&#xA;           SELECT&#xA;               id,&#xA;               content,&#xA;               metadata,&#xA;               embedding,&#xA;               1 -(documents.embedding &amp;lt;=&amp;gt; query_embedding) AS similarity&#xA;           FROM&#xA;               documents&#xA;           ORDER BY&#xA;               documents.embedding &amp;lt;=&amp;gt; query_embedding&#xA;           LIMIT match_count;&#xA;       END;&#xA;       $$;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the app&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Built With&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; - The programming language used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt; - The web framework used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://supabase.io/&#34;&gt;Supabase&lt;/a&gt; - The open source Firebase alternative.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Open a pull request and we&#39;ll review it as soon as possible.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#StanGirard/quivr&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=StanGirard/quivr&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>