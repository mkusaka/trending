<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-23T01:37:53Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iam-veeramalla/python-for-devops</title>
    <updated>2023-10-23T01:37:53Z</updated>
    <id>tag:github.com,2023-10-23:/iam-veeramalla/python-for-devops</id>
    <link href="https://github.com/iam-veeramalla/python-for-devops" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn Python from DevOps Engineer point of you.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Zero to Hero for DevOps Engineers&lt;/h1&gt; &#xA;&lt;img width=&#34;1141&#34; alt=&#34;Screenshot 2023-10-12 at 9 57 40 PM&#34; src=&#34;https://github.com/iam-veeramalla/python-for-devops/assets/43399466/d70f5fe2-0ba3-449d-b41f-413a38fc4584&#34;&gt; &#xA;&lt;h2&gt;Day 1: Introduction to Python, Installation, and Configuration&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to Python and its role in DevOps.&lt;/li&gt; &#xA; &lt;li&gt;Installing Python and setting up a development environment.&lt;/li&gt; &#xA; &lt;li&gt;Writing your first Python program.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 2: Intro to Datatypes, Working with Strings and Numbers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;String data type in Python.&lt;/li&gt; &#xA; &lt;li&gt;String manipulation and formatting.&lt;/li&gt; &#xA; &lt;li&gt;Regular expressions for text processing.&lt;/li&gt; &#xA; &lt;li&gt;Numeric data types in Python (int, float).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 3: Keywords and Variables&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding variables in Python.&lt;/li&gt; &#xA; &lt;li&gt;Variable scope and lifetime.&lt;/li&gt; &#xA; &lt;li&gt;Variable naming conventions and best practices.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Using variables to store and manipulate configuration data in a DevOps context.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 4: Functions, Modules and Packages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What are differences between function, modules and packages ?&lt;/li&gt; &#xA; &lt;li&gt;How to import a package ?&lt;/li&gt; &#xA; &lt;li&gt;What are Python workspaces ?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 5: Environment Variables and Command Line Arguments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reading and writing environment variables in Python.&lt;/li&gt; &#xA; &lt;li&gt;Using the os and dotenv modules.&lt;/li&gt; &#xA; &lt;li&gt;Securing sensitive information in environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Handling command line arguments in Python.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Developing a Python script that accepts command line arguments to customize DevOps automation tasks.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 6: Operators&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to operators in Python.&lt;/li&gt; &#xA; &lt;li&gt;Arithmetic, comparison, and logical operators.&lt;/li&gt; &#xA; &lt;li&gt;Bitwise and assignment operators.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Using operators to perform calculations and comparisons in a DevOps script.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 7: Conditional Handling using if, elif and else&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Conditional statements (if, elif, else).&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Automating a server shutdown if it&#39;s running out of disk space.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 8: Working with Lists (Part 1)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding lists and list data structure.&lt;/li&gt; &#xA; &lt;li&gt;List manipulation and common list operations.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Writing a script to manage a list of user accounts in a DevOps environment.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 9: Loops&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loops in Python (for and while).&lt;/li&gt; &#xA; &lt;li&gt;Loop control statements (break, continue).&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Automating a log file analysis with a loop to find errors.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 10: Working with Lists (Part 2)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;List comprehensions.&lt;/li&gt; &#xA; &lt;li&gt;Nested lists and advanced list operations.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Parsing a complex configuration file with nested lists.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 11: Working with Dictionaries and Sets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dictionaries and key-value pairs.&lt;/li&gt; &#xA; &lt;li&gt;Sets and set operations.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Managing a dictionary of server configurations and optimizing retrieval.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 12: Functions and Modules&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to functions in Python.&lt;/li&gt; &#xA; &lt;li&gt;Writing functions and function parameters.&lt;/li&gt; &#xA; &lt;li&gt;Return values and modular code.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Creating a function to automate server status checks.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 13: Functions and Modules (Part 2)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Advanced function topics (recursion, lambda functions).&lt;/li&gt; &#xA; &lt;li&gt;Function libraries and built-in functions.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Developing a library of custom functions for DevOps automation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 14: Python Libraries for DevOps (Part 1)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to external libraries like Paramiko, Fabric, and Boto3.&lt;/li&gt; &#xA; &lt;li&gt;Automating SSH connections with Paramiko.&lt;/li&gt; &#xA; &lt;li&gt;Running commands on remote servers.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Using Paramiko to create a secure remote backup solution.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 15: Python Libraries for DevOps (Part 2)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using Fabric for remote task automation.&lt;/li&gt; &#xA; &lt;li&gt;AWS automation with Boto3.&lt;/li&gt; &#xA; &lt;li&gt;Managing EC2 instances, S3 buckets, and more.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Creating a Fabric script for deploying applications to remote servers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 16: Working with RESTful APIs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Introduction to RESTful APIs.&lt;/li&gt; &#xA; &lt;li&gt;Making HTTP requests using Python.&lt;/li&gt; &#xA; &lt;li&gt;Parsing JSON responses and error handling.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Developing a script to monitor RESTful API endpoints for your DevOps tools.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 17: Data Serialization and Configuration Files&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serializing and deserializing data (JSON, YAML).&lt;/li&gt; &#xA; &lt;li&gt;Managing configuration data.&lt;/li&gt; &#xA; &lt;li&gt;DevOps use cases for configuration files.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Building a configuration manager to handle application settings in JSON format.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 18: Automation with Cron Jobs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scheduling automated tasks using cron.&lt;/li&gt; &#xA; &lt;li&gt;Creating Python scripts for scheduled automation.&lt;/li&gt; &#xA; &lt;li&gt;Handling periodic tasks and reports.&lt;/li&gt; &#xA; &lt;li&gt;Practice exercises and examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example: Using cron and Python to schedule regular backups of your data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Day 19: Python Interview Questions &amp;amp; Answers&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>WisdomShell/codeshell</title>
    <updated>2023-10-23T01:37:53Z</updated>
    <id>tag:github.com,2023-10-23:/WisdomShell/codeshell</id>
    <link href="https://github.com/WisdomShell/codeshell" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of code large language models developed by PKU-KCL&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://cdn-uploads.huggingface.co/production/uploads/6489a27bd0b2fd1f3297e5ca/3LQsqRzluBhBN2DipN6Ox.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; 🤗 &lt;a href=&#34;https://huggingface.co/WisdomShell/CodeShell&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt; • 🌐 &lt;a href=&#34;http://se.pku.edu.cn/kcl/&#34; target=&#34;_blank&#34;&gt;PKU-KCL&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/WisdomShell/codeshell/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/modelscope.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/WisdomShell/codeshell/raw/main/README.md&#34;&gt;&lt;b&gt;中文&lt;/b&gt;&lt;/a&gt;|&lt;a href=&#34;https://github.com/WisdomShell/codeshell/raw/main/README_EN.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;CodeShell是&lt;a href=&#34;http://se.pku.edu.cn/kcl/&#34;&gt;北京大学知识计算实验室&lt;/a&gt;联合四川天府银行AI团队研发的多语言代码大模型基座。CodeShell具有70亿参数，在五千亿Tokens进行了训练，上下文窗口长度为8192。在权威的代码评估Benchmark（HumanEval与MBPP）上，CodeShell取得同等规模最好的性能。与此同时，我们提供了与CodeShell配套的部署方案与IDE插件，请参考代码库&lt;a href=&#34;https://github.com/WisdomShell/codeshell&#34;&gt;CodeShell&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;本次开源的模型如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/WisdomShell/CodeShell&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;CodeShell Base&lt;/b&gt;&lt;/a&gt;：CodelShell底座模型，具有强大的代码基础能力。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/WisdomShell/CodeShell-Chat&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;CodeShell Chat&lt;/b&gt;&lt;/a&gt;：CodelShell对话模型，在代码问答、代码补全等下游任务重性能优异。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/WisdomShell/CodeShell-Chat-int4&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;CodeShell Chat 4bit&lt;/b&gt;&lt;/a&gt;：CodelShell对话模型4bit量化版本，在保证模型性能的前提下内存消耗更小，速度更快。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WisdomShell/llama_cpp_for_codeshell&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;CodeShell CPP&lt;/b&gt;&lt;/a&gt;：CodelShell对话模型CPP版本，支持开发者在没有GPU的个人电脑中使用。注意，CPP版本同样支持量化操作，用户可以在最小内存为8G的个人电脑中运行CodeShell。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Main Characteristics of CodeShell&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;强大的性能&lt;/strong&gt;：CodelShell在HumanEval和MBPP上达到了7B代码基座大模型的最优性能&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;完整的体系&lt;/strong&gt;：除了代码大模型，同时开源IDE（VS Code与JetBrains）插件，形成开源的全栈技术体系&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;轻量化部署&lt;/strong&gt;：支持本地C++部署，提供轻量快速的本地化软件开发助手解决方案&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;全面的评测&lt;/strong&gt;：提供支持完整项目上下文、覆盖代码生成、代码缺陷检测与修复、测试用例生成等常见软件开发活动的多任务评测体系（即将开源）&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;高效的训练&lt;/strong&gt;：基于高效的数据治理体系，CodeShell在完全冷启动情况下，只训练了五千亿Token即获得了优异的性能&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;我们选取了目前最流行的两个代码评测数据集（HumanEval与MBPP）对模型进行评估，与目前最先进的两个7b代码大模型CodeLllama与Starcoder相比，Codeshell 取得了最优的成绩。具体评测结果如下。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;任务&lt;/th&gt; &#xA;   &lt;th&gt;CodeShell-7b&lt;/th&gt; &#xA;   &lt;th&gt;CodeLlama-7b&lt;/th&gt; &#xA;   &lt;th&gt;Starcoder-7b&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;humaneval&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;34.32&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;29.44&lt;/td&gt; &#xA;   &lt;td&gt;27.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mbpp&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;38.65&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;37.60&lt;/td&gt; &#xA;   &lt;td&gt;34.16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-js&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;33.17&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;31.30&lt;/td&gt; &#xA;   &lt;td&gt;27.02&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-java&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;30.43&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;29.24&lt;/td&gt; &#xA;   &lt;td&gt;24.30&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-cpp&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;28.21&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;27.33&lt;/td&gt; &#xA;   &lt;td&gt;23.04&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-swift&lt;/td&gt; &#xA;   &lt;td&gt;24.30&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;25.32&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-php&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;30.87&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25.96&lt;/td&gt; &#xA;   &lt;td&gt;22.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-d&lt;/td&gt; &#xA;   &lt;td&gt;8.85&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;11.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8.08&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-jl&lt;/td&gt; &#xA;   &lt;td&gt;22.08&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;25.28&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;22.96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-lua&lt;/td&gt; &#xA;   &lt;td&gt;22.39&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;30.50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;22.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-r&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;20.52&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;18.57&lt;/td&gt; &#xA;   &lt;td&gt;14.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-rkt&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;17.20&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12.55&lt;/td&gt; &#xA;   &lt;td&gt;10.43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-rs&lt;/td&gt; &#xA;   &lt;td&gt;24.55&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;25.90&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;22.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.8 and above&lt;/li&gt; &#xA; &lt;li&gt;pytorch 2.0 and above are recommended&lt;/li&gt; &#xA; &lt;li&gt;transformers 4.32 and above&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.8 and above are recommended (this is for GPU users, flash-attention users, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;CodeShell系列模型已经上传至 &lt;a href=&#34;https://huggingface.co/WisdomShell/CodeShell&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt;，开发者可以通过Transformers快速调用CodeShell和CodeShell-Chat。&lt;/p&gt; &#xA;&lt;p&gt;在开始之前，请确保已经正确设置了环境，并安装了必要的代码包，以及满足上一小节的环境要求。你可以通过下列代码快速安装相关依赖。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;接下来你可以通过Transformers使用CodeShell。&lt;/p&gt; &#xA;&lt;h3&gt;Code Generation&lt;/h3&gt; &#xA;&lt;p&gt;开发者可以使用CodeShell快速生成代码，加速开发效率。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;WisdomShell/CodeShell-7B&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;WisdomShell/CodeShell-7B&#34;, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)&#xA;inputs = tokenizer(&#39;def merge_sort():&#39;, return_tensors=&#39;pt&#39;).to(device)&#xA;outputs = model.generate(**inputs)&#xA;print(tokenizer.decode(outputs[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fill in the Moddle&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;CodeShell 支持Fill-in-the-Middle模式，从而更好的支持软件开发过程。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_text = &#34;&amp;lt;fim_prefix&amp;gt;def print_hello_world():\n    &amp;lt;fim_suffix&amp;gt;\n    print(&#39;Hello world!&#39;)&amp;lt;fim_middle&amp;gt;&#34;&#xA;inputs = tokenizer(input_text, return_tensors=&#39;pt&#39;).to(device)&#xA;outputs = model.generate(**inputs)&#xA;print(tokenizer.decode(outputs[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;代码问答&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;CodeShell同时开源了代码助手模型CodeShell-7B-Chat，开发者可以通过下列代码与模型进行交互。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#39;WisdomShell/CodeShell-7B-Chat&#39;, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;WisdomShell/CodeShell-7B-Chat&#39;)&#xA;&#xA;history = []&#xA;query = &#39;你是谁?&#39;&#xA;response = model.chat(query, history, tokenizer)&#xA;print(response)&#xA;history.append((query, response))&#xA;&#xA;query = &#39;用Python写一个HTTP server&#39;&#xA;response = model.chat(query, history, tokenizer)&#xA;print(response)&#xA;history.append((query, response))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;开发者也可以通过VS Code与JetBrains插件与CodeShell-7B-Chat交互，详情请参&lt;a href=&#34;https://github.com/WisdomShell/codeshell-vscode&#34;&gt;VSCode插件仓库&lt;/a&gt;与&lt;a href=&#34;https://github.com/WisdomShell/codeshell-intellij&#34;&gt;IntelliJ插件仓库&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Model Quantization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;CodeShell 支持4 bit/8 bit量化，4 bit量化后，占用显存大小约6G，用户可以在显存较小的GPU上使用CodeShell。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#39;WisdomShell/CodeShell-7B-Chat-int4&#39;, trust_remote_code=True).to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;WisdomShell/CodeShell-7B-Chat-int4&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CodeShell in c/c++&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;由于大部分个人电脑没有GPU，CodeShell提供了C/C++版本的推理支持，开发者可以根据本地环境进行编译与使用，详见&lt;a href=&#34;https://github.com/WisdomShell/llama_cpp_for_codeshell&#34;&gt;CodeShell C/C++本地化版&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;我们提供了Web-UI、命令行、API、IDE四种形式的Demo。&lt;/p&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;开发者通过下列命令启动Web服务，服务启动后，可以通过&lt;code&gt;https://127.0.0.1:8000&lt;/code&gt;进行访问。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demos/web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Demo&lt;/h3&gt; &#xA;&lt;p&gt;我们也提供了命令行交互的Demo版本，开发者可以通过下列命令运行。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demos/cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;p&gt;CodeShell也提供了基于OpenAI API的部署方法。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demos/openai_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动后即可通过HTTP请求与CodeShell交互。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://127.0.0.1:8000/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;    &#34;model&#34;: &#34;CodeShell-7B-Chat&#34;,&#xA;    &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;你好&#34;&#xA;      }&#xA;    ]&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;IDE&lt;/h3&gt; &#xA;&lt;p&gt;CodeShell最后提供了线上IDE，开发者可以通过IDE进行代码补全、代码问答等操作。同时，IDE插件也同时发布，开发者可以自行在本地进行安装使用。插件相关问题欢迎在&lt;a href=&#34;https://github.com/WisdomShell/codeshell-vscode&#34;&gt;VSCode插件仓库&lt;/a&gt;与&lt;a href=&#34;https://github.com/WisdomShell/codeshell-intellij&#34;&gt;IntelliJ插件仓库&lt;/a&gt;中讨论。&lt;/p&gt; &#xA;&lt;h2&gt;Model Details&lt;/h2&gt; &#xA;&lt;p&gt;Code Shell使用GPT-2作为基础架构，采用Grouped-Query Attention、RoPE相对位置编码等技术。&lt;/p&gt; &#xA;&lt;h3&gt;Hyper-parameter&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyper-parameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_layer&lt;/td&gt; &#xA;   &lt;td&gt;42&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_embd&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_inner&lt;/td&gt; &#xA;   &lt;td&gt;16384&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_head&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;num_query_groups&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;seq-length&lt;/td&gt; &#xA;   &lt;td&gt;8192&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vocab_size&lt;/td&gt; &#xA;   &lt;td&gt;70144&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;CodeShell基于自己爬取的Github数据、Big Code开源的Stack和StarCoder数据集、以及少量高质量的中英文数据进行训练。在原始数据集的基础上，CodeShell采用基于Minihash对数据去重，基于KenLM以及高质量数据筛选模型对数据进行了过滤与筛选，最终得到高质量的预训练数据集。&lt;/p&gt; &#xA;&lt;h3&gt;Tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;CodeShell基于Starcoder词表进行了优化，去除了使用频率较低的词语，并添加了部分中文词表，显著提升了中文的压缩率，为Chat版本的训练提供了基础。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tokenizer&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Chinese&lt;/th&gt; &#xA;   &lt;th&gt;English&lt;/th&gt; &#xA;   &lt;th&gt;Code&lt;/th&gt; &#xA;   &lt;th&gt;Total&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Starcoder&lt;/td&gt; &#xA;   &lt;td&gt;49152&lt;/td&gt; &#xA;   &lt;td&gt;1.22&lt;/td&gt; &#xA;   &lt;td&gt;3.47&lt;/td&gt; &#xA;   &lt;td&gt;3.30&lt;/td&gt; &#xA;   &lt;td&gt;2.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeShell&lt;/td&gt; &#xA;   &lt;td&gt;70020&lt;/td&gt; &#xA;   &lt;td&gt;1.50&lt;/td&gt; &#xA;   &lt;td&gt;3.47&lt;/td&gt; &#xA;   &lt;td&gt;3.30&lt;/td&gt; &#xA;   &lt;td&gt;2.95&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;社区使用CodeShell模型需要遵循《CodeShell模型许可协议》及Apache 2.0许可协议。CodeShell模型允许用于商业用途，但如果您计划将CodeShell模型或其派生产品用于商业用途，需要您确认主体符合以下条件：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;关联方的服务或产品的每日平均活跃用户数（DAU）不能超过100万。&lt;/li&gt; &#xA; &lt;li&gt;关联方不得是软件服务提供商或云服务提供商。&lt;/li&gt; &#xA; &lt;li&gt;关联方不存在将获得授予的商业许可，在未经许可的前提下将其再授权给其他第三方的可能性。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;在满足上述条件的前提下，您需要通过向&lt;a href=&#34;mailto:codeshell.opensource@gmail.com&#34;&gt;codeshell.opensource@gmail.com&lt;/a&gt;发送电子邮件提交申请。经审核通过后，将授予您一个全球的、非排他的、不可转让的、不可再授权的商业版权许可。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenTalker/video-retalking</title>
    <updated>2023-10-23T01:37:53Z</updated>
    <id>tag:github.com,2023-10-23:/OpenTalker/video-retalking</id>
    <link href="https://github.com/OpenTalker/video-retalking" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[SIGGRAPH Asia 2022] VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;VideoReTalking &lt;br&gt; &lt;span style=&#34;font-size:12px&#34;&gt;Audio-based Lip Synchronization for Talking Head Video Editing In the Wild&lt;/span&gt; &lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.14758&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-2211.14758-red&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://vinthony.github.io/video-retalking/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://colab.research.google.com/github/vinthony/video-retalking/blob/main/quick_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;a target=&#34;_blank&#34;&gt;Kun Cheng &lt;sup&gt;*,1,2&lt;/sup&gt; &lt;/a&gt;  &#xA;  &lt;a href=&#34;https://vinthony.github.io/&#34; target=&#34;_blank&#34;&gt;Xiaodong Cun &lt;sup&gt;*,2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://yzhang2016.github.io/yongnorriszhang.github.io/&#34; target=&#34;_blank&#34;&gt;Yong Zhang &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://menghanxia.github.io/&#34; target=&#34;_blank&#34;&gt;Menghan Xia &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://feiiyin.github.io/&#34; target=&#34;_blank&#34;&gt;Fei Yin &lt;sup&gt;2,3&lt;/sup&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://web.xidian.edu.cn/mrzhu/en/index.html&#34; target=&#34;_blank&#34;&gt;Mingrui Zhu &lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://xuanwangvc.github.io/&#34; target=&#34;_blank&#34;&gt;Xuan Wang &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://juewang725.github.io/&#34; target=&#34;_blank&#34;&gt;Jue Wang &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;  &#xA;  &lt;a href=&#34;https://web.xidian.edu.cn/nnwang/en/index.html&#34; target=&#34;_blank&#34;&gt;Nannan Wang &lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;1&lt;/sup&gt; Xidian University   &#xA;  &lt;sup&gt;2&lt;/sup&gt; Tencent AI Lab   &#xA;  &lt;sup&gt;3&lt;/sup&gt; Tsinghua University &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;i&gt;&lt;strong&gt;&lt;a href=&#34;https://sa2022.siggraph.org/&#34; target=&#34;_blank&#34;&gt;SIGGRAPH Asia 2022 Conference Track&lt;/a&gt;&lt;/strong&gt;&lt;/i&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenTalker/video-retalking/main/docs/static/images/teaser.png?raw=true&#34; width=&#34;768px&#34;&gt; &#xA; &lt;div align=&#34;justify&#34;&gt;&#xA;   We present VideoReTalking, a new system to edit the faces of a real-world talking head video according to input audio, producing a high-quality and lip-syncing output video even with a different emotion. Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism. Given a talking-head video, we first modify the expression of each frame according to the same expression template using the expression editing network, resulting in a video with the canonical expression. This video, together with the given audio, is then fed into the lip-sync network to generate a lip-syncing video. Finally, we improve the photo-realism of the synthesized faces through an identity-aware face enhancement network and post-processing. We use learning-based approaches for all three steps and all our modules can be tackled in a sequential pipeline without any user intervention.&#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt; &lt;img alt=&#34;pipeline&#34; src=&#34;https://raw.githubusercontent.com/OpenTalker/video-retalking/main/docs/static/images/pipeline.png?raw=true&#34; width=&#34;768px&#34;&gt;&lt;br&gt; &lt;em align=&#34;center&#34;&gt;Pipeline&lt;/em&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Results in the Wild （contains audio）&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/4397546/224310754-665eb2dd-aadc-47dc-b1f9-2029a937b20a.mp4&#34;&gt;https://user-images.githubusercontent.com/4397546/224310754-665eb2dd-aadc-47dc-b1f9-2029a937b20a.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Environment&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/vinthony/video-retalking.git&#xA;cd video-retalking&#xA;conda create -n video_retalking python=3.8&#xA;conda activate video_retalking&#xA;&#xA;conda install ffmpeg&#xA;&#xA;# Please follow the instructions from https://pytorch.org/get-started/previous-versions/&#xA;# This installation command only works on CUDA 11.1&#xA;pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Inference&lt;/h2&gt; &#xA;&lt;h4&gt;Pretrained Models&lt;/h4&gt; &#xA;&lt;p&gt;Please download our &lt;a href=&#34;https://drive.google.com/drive/folders/18rhjMpxK8LVVxf7PI6XwOidt8Vouv_H0?usp=share_link&#34;&gt;pre-trained models&lt;/a&gt; and put them in &lt;code&gt;./checkpoints&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;!-- We also provide some [example videos and audio](https://drive.google.com/drive/folders/14OwbNGDCAMPPdY-l_xO1axpUjkPxI9Dv?usp=share_link). Please put them in `./examples`. --&gt; &#xA;&lt;h4&gt;Inference&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 inference.py \&#xA;  --face examples/face/1.mp4 \&#xA;  --audio examples/audio/1.wav \&#xA;  --outfile results/1_1.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script includes data preprocessing steps. You can test any talking face videos without manual alignment. But it is worth noting that DNet cannot handle extreme poses.&lt;/p&gt; &#xA;&lt;p&gt;You can also control the expression by adding the following parameters:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--exp_img&lt;/code&gt;: Pre-defined expression template. The default is &#34;neutral&#34;. You can choose &#34;smile&#34; or an image path.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--up_face&lt;/code&gt;: You can choose &#34;surprise&#34; or &#34;angry&#34; to modify the expression of upper face with &lt;a href=&#34;https://github.com/donydchen/ganimation_replicate&#34;&gt;GANimation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{cheng2022videoretalking,&#xA;        title={VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild}, &#xA;        author={Kun Cheng and Xiaodong Cun and Yong Zhang and Menghan Xia and Fei Yin and Mingrui Zhu and Xuan Wang and Jue Wang and Nannan Wang},&#xA;        year={2022},&#xA;        eprint={2211.14758},&#xA;        archivePrefix={arXiv},&#xA;        primaryClass={cs.CV}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2Lip&lt;/a&gt;, &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRenderer&lt;/a&gt;, &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFP-GAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/yangxy/GPEN&#34;&gt;GPEN&lt;/a&gt;, &lt;a href=&#34;https://github.com/donydchen/ganimation_replicate&#34;&gt;ganimation_replicate&lt;/a&gt;, &lt;a href=&#34;https://github.com/rotemtzaban/STIT&#34;&gt;STIT&lt;/a&gt; for sharing their code.&lt;/p&gt; &#xA;&lt;h2&gt;Related Work&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/StyleHEAT&#34;&gt;StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN (ECCV 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Doubiiu/CodeTalker&#34;&gt;CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Winfredy/SadTalker&#34;&gt;SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Carlyx/DPE&#34;&gt;DPE: Disentanglement of Pose and Expression for General Video Portrait Editing (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/SPI/&#34;&gt;3D GAN Inversion with Facial Symmetry Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mael-zys/T2M-GPT&#34;&gt;T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official product of Tencent.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;1. Please carefully read and comply with the open-source license applicable to this code before using it. &#xA;2. Please carefully read and comply with the intellectual property declaration applicable to this code before using it.&#xA;3. This open-source code runs completely offline and does not collect any personal information or other data. If you use this code to provide services to end-users and collect related data, please take necessary compliance measures according to applicable laws and regulations (such as publishing privacy policies, adopting necessary data security strategies, etc.). If the collected data involves personal information, user consent must be obtained (if applicable). Any legal liabilities arising from this are unrelated to Tencent.&#xA;4. Without Tencent&#39;s written permission, you are not authorized to use the names or logos legally owned by Tencent, such as &#34;Tencent.&#34; Otherwise, you may be liable for legal responsibilities.&#xA;5. This open-source code does not have the ability to directly provide services to end-users. If you need to use this code for further model training or demos, as part of your product to provide services to end-users, or for similar use, please comply with applicable laws and regulations for your product or service. Any legal liabilities arising from this are unrelated to Tencent.&#xA;6. It is prohibited to use this open-source code for activities that harm the legitimate rights and interests of others (including but not limited to fraud, deception, infringement of others&#39; portrait rights, reputation rights, etc.), or other behaviors that violate applicable laws and regulations or go against social ethics and good customs (including providing incorrect or false information, spreading pornographic, terrorist, and violent information, etc.). Otherwise, you may be liable for legal responsibilities.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>