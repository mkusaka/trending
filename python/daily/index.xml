<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-11T01:31:13Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lllyasviel/IC-Light</title>
    <updated>2024-05-11T01:31:13Z</updated>
    <id>tag:github.com,2024-05-11:/lllyasviel/IC-Light</id>
    <link href="https://github.com/lllyasviel/IC-Light" rel="alternate"></link>
    <summary type="html">&lt;p&gt;More relighting!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IC-Light&lt;/h1&gt; &#xA;&lt;p&gt;IC-Light is a project to manipulate the illumination of images.&lt;/p&gt; &#xA;&lt;p&gt;The name &#34;IC-Light&#34; stands for &lt;strong&gt;&#34;Imposing Consistent Light&#34;&lt;/strong&gt; (we will briefly describe this at the end of this page).&lt;/p&gt; &#xA;&lt;p&gt;Currently, we release two types of models: text-conditioned relighting model and background-conditioned model. Both types take foreground images as inputs.&lt;/p&gt; &#xA;&lt;h1&gt;Get Started&lt;/h1&gt; &#xA;&lt;p&gt;Below script will run the text-conditioned relighting model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/IC-Light.git&#xA;cd IC-Light&#xA;conda create -n iclight python=3.10&#xA;conda activate iclight&#xA;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121&#xA;pip install -r requirements.txt&#xA;python gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, to use background-conditioned demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_demo_bg.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model downloading is automatic.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &#34;gradio_demo.py&#34; has an official &lt;a href=&#34;https://huggingface.co/spaces/lllyasviel/IC-Light&#34;&gt;huggingFace Space here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Screenshot&lt;/h1&gt; &#xA;&lt;h3&gt;Text-Conditioned Model&lt;/h3&gt; &#xA;&lt;p&gt;(Note that the &#34;Lighting Preference&#34; are just initial latents - eg., if the Lighting Preference is &#34;Left&#34; then initial latent is left white right black.)&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, warm atmosphere, at home, bedroom&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/87265483-aa26-4d2e-897d-b58892f5fdd7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/148c4a6d-82e7-4e3a-bf44-5c9a24538afc&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;beautiful woman, detailed face, neon, Wong Kar-wai, warm&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/f53c9de2-534a-42f4-8272-6d16a021fc01&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine, outdoor, warm atmosphere&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/25d6ea24-a736-4a0b-b42d-700fe8b2101e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine, outdoor, warm atmosphere&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/dd30387b-0490-46ee-b688-2191fb752e68&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunshine from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/6c9511ca-f97f-401a-85f3-92b4442000e3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, shadow from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/e73701d5-890e-4b15-91ee-97f16ea3c450&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, sunset over sea&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/ff26ac3d-1b12-4447-b51f-73f7a5122a05&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: handsome boy, detailed face, neon light, city&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/d7795e02-46f7-444f-93e7-4d6460840437&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: beautiful woman, detailed face, light and shadow&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/706f70a8-d1a0-4e0b-b3ac-804e8e231c0f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(beautiful woman, detailed face, soft studio lighting)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/fe0a72df-69d4-4e11-b661-fb8b84d0274d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: Buddha, detailed face, sci-fi RGB glowing, cyberpunk&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/68d60c68-ce23-4902-939e-11629ccaf39a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: Buddha, detailed face, natural lighting&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Left&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/1841d23d-0a0d-420b-a5ab-302da9c47c17&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: toy, detailed face, shadow from window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Bottom&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/dcb97439-ea6b-483e-8e68-cf5d320368c7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: toy, detailed face, sunset over sea&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/4f78b897-621d-4527-afa7-78d62c576100&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: dog, magic lit, sci-fi RGB glowing, studio lighting&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Bottom&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/1db9cac9-8d3f-4f40-82e2-e3b0cafd8613&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt: mysteriou human, warm atmosphere, warm atmosphere, at home, bedroom&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lighting Preference: Right&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/5d5aa7e5-8cbd-4e1f-9f27-2ecc3c30563a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Background-Conditioned Model&lt;/h3&gt; &#xA;&lt;p&gt;The background conditioned model does not require careful prompting. One can just use simple prompts like &#34;handsome man, cinematic lighting&#34;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/0b2a889f-682b-4393-b1ec-2cabaa182010&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/477ca348-bd47-46ff-81e6-0ffc3d05feb2&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/5bc9d8d9-02cd-442e-a75c-193f115f2ad8&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/a35e4c57-e199-40e2-893b-cb1c549612a9&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;A more structured visualization:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/c1daafb5-ac8b-461c-bff2-899e4c671ba3&#34; alt=&#34;r1&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Imposing Consistent Light&lt;/h1&gt; &#xA;&lt;p&gt;In HDR space, illumination has a property that all light transports are independent.&lt;/p&gt; &#xA;&lt;p&gt;As a result, the blending of appearances of different light sources is equivalent to the appearance with mixed light sources:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/27c67787-998e-469f-862f-047344e100cd&#34; alt=&#34;cons&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using the above &lt;a href=&#34;https://www.pauldebevec.com/Research/LS/&#34;&gt;light stage&lt;/a&gt; as an example, the two images from the &#34;appearance mixture&#34; and &#34;light source mixture&#34; are consistent (mathematically equivalent in HDR space, ideally).&lt;/p&gt; &#xA;&lt;p&gt;We imposed such consistency (using MLPs in latent space) when training the relighting models.&lt;/p&gt; &#xA;&lt;p&gt;As a result, the model is able to produce highly consistent relight - &lt;strong&gt;so&lt;/strong&gt; consistent that different relightings can even be merged as normal maps! Despite the fact that the models are latent diffusion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/25068f6a-f945-4929-a3d6-e8a152472223&#34; alt=&#34;r2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;From left to right are inputs, model outputs relighting, devided shadow image, and merged normal maps. Note that the model is not trained with any normal map data. This normal estimation comes from the consistency of relighting.&lt;/p&gt; &#xA;&lt;p&gt;You can reproduce this experiment using this button (it is 4x slower because it relight image 4 times)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/d9c37bf7-2136-446c-a9a5-5a341e4906de&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/fcf5dd55-0309-4e8e-9721-d55931ea77f0&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below are bigger images (feel free to try yourself to get more results!)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/12335218-186b-4c61-b43a-79aea9df8b21&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/2daab276-fdfa-4b0c-abcb-e591f575598a&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For reference, &lt;a href=&#34;https://fuxiao0719.github.io/projects/geowizard/&#34;&gt;geowizard&lt;/a&gt; (geowizard is a really great work!):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/4ba1a96d-e218-42ab-83ae-a7918d56ee5f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;And, &lt;a href=&#34;https://arxiv.org/pdf/2402.18848&#34;&gt;switchlight&lt;/a&gt; (switchlight is another great work!):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/IC-Light/assets/19834515/fbdd961f-0b26-45d2-802e-ffd734affab8&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model Notes&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;iclight_sd15_fc.safetensors&lt;/strong&gt; - The default relighting model, conditioned on text and foreground. You can use initial latent to influence the relighting.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;iclight_sd15_fcon.safetensors&lt;/strong&gt; - Same as &#34;iclight_sd15_fc.safetensors&#34; but trained with offset noise. Note that the default &#34;iclight_sd15_fc.safetensors&#34; outperform this model slightly in a user study. And this is the reason why the default model is the model without offset noise.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;iclight_sd15_fbc.safetensors&lt;/strong&gt; - Relighting model conditioned with text, foreground, and background.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Cite&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Misc{iclight,&#xA;  author = {Lvmin Zhang and Anyi Rao and Maneesh Agrawala},&#xA;  title  = {IC-Light GitHub Page},&#xA;  year   = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Related Work&lt;/h1&gt; &#xA;&lt;p&gt;Also read ...&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://augmentedperception.github.io/total_relighting/&#34;&gt;Total Relighting: Learning to Relight Portraits for Background Replacement&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.06886&#34;&gt;Relightful Harmonization: Lighting-aware Portrait Background Replacement&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.18848&#34;&gt;SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AdityaNG/kan-gpt</title>
    <updated>2024-05-11T01:31:13Z</updated>
    <id>tag:github.com,2024-05-11:/AdityaNG/kan-gpt</id>
    <link href="https://github.com/AdityaNG/kan-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The PyTorch implementation of Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KAN-GPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/pypi/dd/kan-gpt&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;a href=&#34;https://pypi.org/project/kan-gpt/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/kan-gpt&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/AdityaNG/kan-gpt&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/AdityaNG/kan-gpt/branch/main/graph/badge.svg?token=kan-gpt_token_here&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AdityaNG/kan-gpt/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/AdityaNG/kan-gpt/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/AdityaNG/kan-gpt&#34; alt=&#34;GitHub License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The PyTorch implementation of Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling&lt;/p&gt; &#xA;&lt;h2&gt;Install it from PyPI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install kan_gpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/KAN_GPT.ipynb&#34;&gt;KAN_GPT.ipynb&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/kan_gpt/prompt.py&#34;&gt;kan_gpt/prompt.py&lt;/a&gt; for usage examples. The following is an outline of how to use the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from kan_gpt.model import GPT&#xA;from transformers import GPT2Tokenizer&#xA;&#xA;model_config = GPT.get_default_config()&#xA;model_config.model_type = &#34;gpt2&#34;&#xA;model_config.vocab_size = 50257&#xA;model_config.block_size = 1024&#xA;model = GPT(model_config)&#xA;&#xA;tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)&#xA;&#xA;prompt = &#34;Bangalore is often described as the &#34;&#xA;&#xA;prompt_encoded = tokenizer.encode(&#xA;  text=prompt, add_special_tokens=False&#xA;)&#xA;&#xA;x = torch.tensor(prompt_encoded).unsqueeze(0)&#xA;&#xA;model.eval()&#xA;y = model.generate(x, 50)  # sample 50 tokens&#xA;&#xA;result = tokenizer.decode(y)&#xA;&#xA;print(result)&#xA;&#xA;# Bangalore is often described as the Silicon Valley of India.&#xA;# The city has witnessed rapid growth in the past two decades.....&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setup for Development&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download Repo&#xA;git clone https://github.com/AdityaNG/kan-gpt&#xA;cd kan-gpt&#xA;git pull&#xA;&#xA;# Download Dataset&#xA;./scripts/download_webtext.sh&#xA;./scripts/download_tinyshakespeare.sh&#xA;&#xA;# Install dependencies for development&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;Use the following dummy script to make sure everything is working as expected&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;WANDB_MODE=offline CUDA_VISIBLE_DEVICE=&#34;&#34; python3 -m kan_gpt.train --architecture MLP --batch_size 1 --dummy_dataset --device cpu --max_iters 200&#xA;WANDB_MODE=offline CUDA_VISIBLE_DEVICE=&#34;&#34; python3 -m kan_gpt.train --architecture KAN --batch_size 1 --dummy_dataset --device cpu --max_iters 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then make use of the training script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m kan_gpt.train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prompt&lt;/h2&gt; &#xA;&lt;p&gt;You can prompt the model to produce text as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m kan_gpt.prompt --prompt &#34;Bangalore is often described as the &#34; --model_path (checkpoint)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;We train and compare KAN-GPT with an equivalent MLP-GPT model on the Tiny Shakespeare dataset. We observe that the KAN-GPT performs slightly better than the MLP-GPT. We are looking into further experiments to dive deeper. The results are shown below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Metrics&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_loss.png&#34; alt=&#34;results_loss&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_cross_entropy.png&#34; alt=&#34;results_cross_entropy&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_perplexity.png&#34; alt=&#34;results_perplexity&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt; and &lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;pykan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset downloading script for &lt;a href=&#34;https://github.com/openai/gpt-2-output-dataset&#34;&gt;WebText&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; PyTorch Dataset parser for &lt;a href=&#34;https://github.com/openai/gpt-2-output-dataset&#34;&gt;WebText&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; PyTorch Dataset parser for &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#34;&gt;tinyshakespeare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mini training POC for KAN-GPT &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate KAN training logic from &lt;code&gt;KAN.train_kan&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train a dummy batch w/o any memory issues&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mini training POC for MLP-GPT&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train MLP-GPT on the webtext dataset as a baseline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train KAN-GPT on the webtext dataset as a baseline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Metrics comparing KAN-GPT and MLP-GPT&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Auto Save checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Auto Save checkpoints to W&amp;amp;B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Auto Download model weights from git / huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; W&amp;amp;B hyperparam sweep script&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Script to load checkpoint in interactive mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reduce requrements.txt constraints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Define pydantic model for training and sweep args&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Pruning the package, get rid of unused code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training script to PyTorch Lighting&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Documentation: &lt;code&gt;mkdocs gh-deploy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate with &lt;a href=&#34;https://github.com/Blealtan/efficient-kan/raw/master/src/efficient_kan/kan.py&#34;&gt;efficient-kan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Test Cases &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; KAN: Forward-Backward test&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GPT: Forward-Backward test&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; KAN_GPT: Forward-Backward test&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; EFFICIENT_KAN: Forward-Backward test&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;pykan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/gpt-2-output-dataset&#34;&gt;webtext&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#34;&gt;tinyshakespeare&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>google-research/timesfm</title>
    <updated>2024-05-11T01:31:13Z</updated>
    <id>tag:github.com,2024-05-11:/google-research/timesfm</id>
    <link href="https://github.com/google-research/timesfm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; &#xA;&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2310.10688&#34;&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, to appear in ICML 2024.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/&#34;&gt;Google Research blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/timesfm-1.0-200m&#34;&gt;Hugging Face checkpoint repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repo contains the code to load public TimesFM checkpoints and run model inference. Please visit our &lt;a href=&#34;https://huggingface.co/google/timesfm-1.0-200m&#34;&gt;Hugging Face checkpoint repo&lt;/a&gt; to download model checkpoints.&lt;/p&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;h2&gt;Checkpoint timesfm-1.0-200m&lt;/h2&gt; &#xA;&lt;p&gt;timesfm-1.0-200m is the first open model checkpoint:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It performs univariate time series forecasting for context lengths up to 512 timepoints and any horizon lengths, with an optional frequency indicator.&lt;/li&gt; &#xA; &lt;li&gt;It focuses on point forecasts, and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.&lt;/li&gt; &#xA; &lt;li&gt;It requires the context to be contiguous (i.e. no &#34;holes&#34;), and the context and the horizon to be of the same frequency.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to our result tables on the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/timesfm/master/experiments/extended_benchmarks/tfm_results.png&#34;&gt;extended benchmarks&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/google-research/timesfm/master/experiments/long_horizon_benchmarks/tfm_long_horizon.png&#34;&gt;long horizon benchmarks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please look into the README files in the respective benchmark directories within &lt;code&gt;experiments/&lt;/code&gt; for instructions for running TimesFM on the respective benchmarks.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;For calling TimesFM, We have two environment files. Inside &lt;code&gt;timesfm&lt;/code&gt;, for GPU installation (assuming CUDA 12 has been setup), you can create a conda environment &lt;code&gt;tfm_env&lt;/code&gt; from the base folder through:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create --file=environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a CPU setup please use,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create --file=environment_cpu.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to create the environment instead.&lt;/p&gt; &#xA;&lt;p&gt;Follow by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate tfm_env&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to install the package.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Running the provided benchmarks would require additional dependencies. Please use the environment files under &lt;code&gt;experiments&lt;/code&gt; instead.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The dependency &lt;code&gt;lingvo&lt;/code&gt; does not support &lt;code&gt;pip install&lt;/code&gt; for macOS. For now you can manually install it from the source following &lt;a href=&#34;https://github.com/tensorflow/lingvo&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Initialize the model and load a checkpoint.&lt;/h3&gt; &#xA;&lt;p&gt;Then the base class can be loaded as,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import timesfm&#xA;&#xA;tfm = timesfm.TimesFm(&#xA;    context_len=&amp;lt;context&amp;gt;,&#xA;    horizon_len=&amp;lt;horizon&amp;gt;,&#xA;    input_patch_len=32,&#xA;    output_patch_len=128,&#xA;    num_layers=20,&#xA;    model_dims=1280,&#xA;    backend=&amp;lt;backend&amp;gt;,&#xA;)&#xA;tfm.load_from_checkpoint(repo_id=&#34;google/timesfm-1.0-200m&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the four parameters are fixed to load the 200m model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_patch_len=32,&#xA;output_patch_len=128,&#xA;num_layers=20,&#xA;model_dims=1280,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;code&gt;context_len&lt;/code&gt; here can be set as the max context length &lt;strong&gt;of the model&lt;/strong&gt;. &lt;strong&gt;It needs to be a multiplier of &lt;code&gt;input_patch_len&lt;/code&gt;, i.e. a multiplier of 32.&lt;/strong&gt; You can provide a shorter series to the &lt;code&gt;tfm.forecast()&lt;/code&gt; function and the model will handle it. Currently, the model handles a max context length of 512, which can be increased in later releases. The input time series can have &lt;strong&gt;any context length&lt;/strong&gt;. Padding / truncation will be handled by the inference code if needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length &amp;lt;= context length but it is not a requirement in the function call.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;backend&lt;/code&gt; is one of &#34;cpu&#34;, &#34;gpu&#34; or &#34;tpu&#34;, case sensitive.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Perform inference&lt;/h3&gt; &#xA;&lt;p&gt;We provide APIs to forecast from either array inputs or &lt;code&gt;pandas&lt;/code&gt; dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions &lt;code&gt;tfm.forecast()&lt;/code&gt; and &lt;code&gt;tfm.forecast_on_df()&lt;/code&gt; for detailed instructions.&lt;/p&gt; &#xA;&lt;p&gt;In particular regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;0&lt;/strong&gt; (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;1&lt;/strong&gt;: medium frequency time series. We recommend using this for weekly and monthly data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2&lt;/strong&gt;: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;0&lt;/strong&gt;: T, MIN, H, D, B, U&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;1&lt;/strong&gt;: W, M&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2&lt;/strong&gt;: Q, Y&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Notice you do &lt;strong&gt;NOT&lt;/strong&gt; have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.&lt;/p&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;p&gt;Array inputs, with the frequencies set to low, medium and high respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;forecast_input = [&#xA;    np.sin(np.linspace(0, 20, 100)),&#xA;    np.sin(np.linspace(0, 20, 200)),&#xA;    np.sin(np.linspace(0, 20, 400)),&#xA;]&#xA;frequency_input = [0, 1, 2]&#xA;&#xA;point_forecast, experimental_quantile_forecast = tfm.forecast(&#xA;    forecast_input,&#xA;    freq=frequency_input,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt; dataframe, with the frequency set to &#34;M&#34; monthly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;&#xA;# e.g. input_df is&#xA;#       unique_id  ds          y&#xA;# 0     T1         1975-12-31  697458.0&#xA;# 1     T1         1976-01-31  1187650.0&#xA;# 2     T1         1976-02-29  1069690.0&#xA;# 3     T1         1976-03-31  1078430.0&#xA;# 4     T1         1976-04-30  1059910.0&#xA;# ...   ...        ...         ...&#xA;# 8175  T99        1986-01-31  602.0&#xA;# 8176  T99        1986-02-28  684.0&#xA;# 8177  T99        1986-03-31  818.0&#xA;# 8178  T99        1986-04-30  836.0&#xA;# 8179  T99        1986-05-31  878.0&#xA;&#xA;forecast_df = tfm.forecast_on_df(&#xA;    inputs=input_df,&#xA;    freq=&#34;M&#34;,  # monthly&#xA;    value_name=&#34;y&#34;,&#xA;    num_jobs=-1,&#xA;)```&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>