<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-27T01:38:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>snowflakedb/snowflake-connector-python</title>
    <updated>2022-10-27T01:38:46Z</updated>
    <id>tag:github.com,2022-10-27:/snowflakedb/snowflake-connector-python</id>
    <link href="https://github.com/snowflakedb/snowflake-connector-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Snowflake Connector for Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Snowflake Connector for Python&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/snowflakedb/snowflake-connector-python/actions/workflows/build_test.yml&#34;&gt;&lt;img src=&#34;https://github.com/snowflakedb/snowflake-connector-python/actions/workflows/build_test.yml/badge.svg?sanitize=true&#34; alt=&#34;Build and Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/snowflakedb/snowflake-connector-python&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/snowflakedb/snowflake-connector-python/branch/main/graph/badge.svg?token=MVKSNtnLr0&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/snowflake-connector-python/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/snowflake-connector-python.svg?sanitize=true&#34; alt=&#34;PyPi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/:license-Apache%202-brightgreen.svg?sanitize=true&#34; alt=&#34;License Apache-2.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Codestyle Black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This package includes the Snowflake Connector for Python, which conforms to the &lt;a href=&#34;https://www.python.org/dev/peps/pep-0249/&#34;&gt;Python DB API 2.0&lt;/a&gt; specification.&lt;/p&gt; &#xA;&lt;p&gt;The Snowflake Connector for Python provides an interface for developing Python applications that can connect to Snowflake and perform all standard operations. It provides a programming alternative to developing applications in Java or C/C++ using the Snowflake JDBC or ODBC drivers.&lt;/p&gt; &#xA;&lt;p&gt;The connector has &lt;strong&gt;no&lt;/strong&gt; dependencies on JDBC or ODBC. It can be installed using &lt;code&gt;pip&lt;/code&gt; on Linux, Mac OSX, and Windows platforms where Python 3.7.0 (or higher) is installed.&lt;/p&gt; &#xA;&lt;p&gt;Snowflake Documentation is available at: &lt;a href=&#34;https://docs.snowflake.com/&#34;&gt;https://docs.snowflake.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Feel free to file an issue or submit a PR here for general cases. For official support, contact Snowflake support at: &lt;a href=&#34;https://community.snowflake.com/s/article/How-To-Submit-a-Support-Case-in-Snowflake-Lodge&#34;&gt;https://community.snowflake.com/s/article/How-To-Submit-a-Support-Case-in-Snowflake-Lodge&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to build&lt;/h2&gt; &#xA;&lt;h3&gt;Locally&lt;/h3&gt; &#xA;&lt;p&gt;Install Python 3.7.0 or higher. Clone the Snowflake Connector for Python repository, then run the following commands to create a wheel package using PEP-517 build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:snowflakedb/snowflake-connector-python.git&#xA;cd snowflake-connector-python&#xA;python -m pip install -U pip setuptools wheel build&#xA;python -m build --wheel .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Find the &lt;code&gt;snowflake_connector_python*.whl&lt;/code&gt; package in the &lt;code&gt;./dist&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;In Docker&lt;/h3&gt; &#xA;&lt;p&gt;Or use our Dockerized build script &lt;code&gt;ci/build_docker.sh&lt;/code&gt; and find the built wheel files in &lt;code&gt;dist/repaired_wheels&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note: &lt;code&gt;ci/build_docker.sh&lt;/code&gt; can be used to compile only certain versions, like this: &lt;code&gt;ci/build_docker.sh &#34;3.7 3.8&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code hygiene and other utilities&lt;/h2&gt; &#xA;&lt;p&gt;These tools are integrated into &lt;code&gt;tox&lt;/code&gt; to allow us to easily set them up universally on any computer.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;fix_lint&lt;/strong&gt;: Runs &lt;code&gt;pre-commit&lt;/code&gt; to check for a bunch of lint issues. This can be installed to run upon each time a commit is created locally, keep an eye out for the hint that this environment prints upon succeeding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;coverage&lt;/strong&gt;: Runs &lt;code&gt;coverage.py&lt;/code&gt; to combine generated coverage data files. Useful when multiple categories were run and we would like to have an overall coverage data file created for them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;flake8&lt;/strong&gt;: (Deprecated) Similar to &lt;code&gt;fix_lint&lt;/code&gt;, but only runs &lt;code&gt;flake8&lt;/code&gt; checks.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ifzhang/ByteTrack</title>
    <updated>2022-10-27T01:38:46Z</updated>
    <id>tag:github.com,2022-10-27:/ifzhang/ByteTrack</id>
    <link href="https://github.com/ifzhang/ByteTrack" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ECCV 2022] ByteTrack: Multi-Object Tracking by Associating Every Detection Box&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ByteTrack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot17?p=bytetrack-multi-object-tracking-by-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bytetrack-multi-object-tracking-by-1/multi-object-tracking-on-mot17&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=bytetrack-multi-object-tracking-by-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bytetrack-multi-object-tracking-by-1/multi-object-tracking-on-mot20-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;ByteTrack is a simple, fast and strong multi-object tracker.&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/assets/sota.png&#34; width=&#34;500&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.06864&#34;&gt;&lt;strong&gt;ByteTrack: Multi-Object Tracking by Associating Every Detection Box&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, Xinggang Wang&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.06864&#34;&gt;arXiv 2110.06864&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Demo Links&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Google Colab demo&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Huggingface Demo&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Original Paper: ByteTrack&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1bDilg4cmXFa8HCKHbsZ_p16p0vrhLyu0?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/akhaliq/bytetrack&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.06864&#34;&gt;arXiv 2110.06864&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 scores ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/assets/teasing.png&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2022.07) Our paper is accepted by ECCV 2022!&lt;/li&gt; &#xA; &lt;li&gt;(2022.06) A &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/mot/bytetrack&#34;&gt;nice re-implementation&lt;/a&gt; by Baidu &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection&#34;&gt;PaddleDetection&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tracking performance&lt;/h2&gt; &#xA;&lt;h3&gt;Results on MOT challenge test set&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;HOTA&lt;/th&gt; &#xA;   &lt;th&gt;MT&lt;/th&gt; &#xA;   &lt;th&gt;ML&lt;/th&gt; &#xA;   &lt;th&gt;FP&lt;/th&gt; &#xA;   &lt;th&gt;FN&lt;/th&gt; &#xA;   &lt;th&gt;IDs&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT17&lt;/td&gt; &#xA;   &lt;td&gt;80.3&lt;/td&gt; &#xA;   &lt;td&gt;77.3&lt;/td&gt; &#xA;   &lt;td&gt;63.1&lt;/td&gt; &#xA;   &lt;td&gt;53.2%&lt;/td&gt; &#xA;   &lt;td&gt;14.5%&lt;/td&gt; &#xA;   &lt;td&gt;25491&lt;/td&gt; &#xA;   &lt;td&gt;83721&lt;/td&gt; &#xA;   &lt;td&gt;2196&lt;/td&gt; &#xA;   &lt;td&gt;29.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT20&lt;/td&gt; &#xA;   &lt;td&gt;77.8&lt;/td&gt; &#xA;   &lt;td&gt;75.2&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;69.2%&lt;/td&gt; &#xA;   &lt;td&gt;9.5%&lt;/td&gt; &#xA;   &lt;td&gt;26249&lt;/td&gt; &#xA;   &lt;td&gt;87594&lt;/td&gt; &#xA;   &lt;td&gt;1223&lt;/td&gt; &#xA;   &lt;td&gt;13.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Visualization results on MOT challenge test set&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/assets/MOT17-01-SDP.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/assets/MOT17-07-SDP.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/assets/MOT20-07.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/assets/MOT20-08.gif&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;1. Installing on the host machine&lt;/h3&gt; &#xA;&lt;p&gt;Step1. Install ByteTrack.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/ifzhang/ByteTrack.git&#xA;cd ByteTrack&#xA;pip3 install -r requirements.txt&#xA;python3 setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step2. Install &lt;a href=&#34;https://github.com/cocodataset/cocoapi&#34;&gt;pycocotools&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install cython; pip3 install &#39;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step3. Others&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install cython_bbox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Docker build&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build -t bytetrack:latest .&#xA;&#xA;# Startup sample&#xA;mkdir -p pretrained &amp;amp;&amp;amp; \&#xA;mkdir -p YOLOX_outputs &amp;amp;&amp;amp; \&#xA;xhost +local: &amp;amp;&amp;amp; \&#xA;docker run --gpus all -it --rm \&#xA;-v $PWD/pretrained:/workspace/ByteTrack/pretrained \&#xA;-v $PWD/datasets:/workspace/ByteTrack/datasets \&#xA;-v $PWD/YOLOX_outputs:/workspace/ByteTrack/YOLOX_outputs \&#xA;-v /tmp/.X11-unix/:/tmp/.X11-unix:rw \&#xA;--device /dev/video0:/dev/video0:mwr \&#xA;--net=host \&#xA;-e XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR \&#xA;-e DISPLAY=$DISPLAY \&#xA;--privileged \&#xA;bytetrack:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://motchallenge.net/&#34;&gt;MOT17&lt;/a&gt;, &lt;a href=&#34;https://motchallenge.net/&#34;&gt;MOT20&lt;/a&gt;, &lt;a href=&#34;https://www.crowdhuman.org/&#34;&gt;CrowdHuman&lt;/a&gt;, &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT/raw/master/DATASET_ZOO.md&#34;&gt;Cityperson&lt;/a&gt;, &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT/raw/master/DATASET_ZOO.md&#34;&gt;ETHZ&lt;/a&gt; and put them under &amp;lt;ByteTrack_HOME&amp;gt;/datasets in the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;datasets&#xA;   |——————mot&#xA;   |        └——————train&#xA;   |        └——————test&#xA;   └——————crowdhuman&#xA;   |         └——————Crowdhuman_train&#xA;   |         └——————Crowdhuman_val&#xA;   |         └——————annotation_train.odgt&#xA;   |         └——————annotation_val.odgt&#xA;   └——————MOT20&#xA;   |        └——————train&#xA;   |        └——————test&#xA;   └——————Cityscapes&#xA;   |        └——————images&#xA;   |        └——————labels_with_ids&#xA;   └——————ETHZ&#xA;            └——————eth01&#xA;            └——————...&#xA;            └——————eth07&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you need to turn the datasets to COCO format and mix different training data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/convert_mot17_to_coco.py&#xA;python3 tools/convert_mot20_to_coco.py&#xA;python3 tools/convert_crowdhuman_to_coco.py&#xA;python3 tools/convert_cityperson_to_coco.py&#xA;python3 tools/convert_ethz_to_coco.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before mixing different datasets, you need to follow the operations in &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/c116dfc746f9ebe07d419caa8acba9b3acfa79a6/tools/mix_data_ablation.py#L6&#34;&gt;mix_xxx.py&lt;/a&gt; to create a data folder and link. Finally, you can mix the training data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/mix_data_ablation.py&#xA;python3 tools/mix_data_test_mot17.py&#xA;python3 tools/mix_data_test_mot20.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model zoo&lt;/h2&gt; &#xA;&lt;h3&gt;Ablation model&lt;/h3&gt; &#xA;&lt;p&gt;Train on CrowdHuman and MOT17 half train, evaluate on MOT17 half val&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDs&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ByteTrack_ablation &lt;a href=&#34;https://drive.google.com/file/d/1iqhM-6V_r1FpOlOzrdP_Ejshgk0DxOob/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1W5eRBnxc4x9V8gm7dgdEYg&#34;&gt;[baidu(code:eeo8)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;76.6&lt;/td&gt; &#xA;   &lt;td&gt;79.3&lt;/td&gt; &#xA;   &lt;td&gt;159&lt;/td&gt; &#xA;   &lt;td&gt;29.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MOT17 test model&lt;/h3&gt; &#xA;&lt;p&gt;Train on CrowdHuman, MOT17, Cityperson and ETHZ, evaluate on MOT17 train.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Standard models&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDs&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bytetrack_x_mot17 &lt;a href=&#34;https://drive.google.com/file/d/1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1OJKrcQa_JP9zofC6ZtGBpw&#34;&gt;[baidu(code:ic0i)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;90.0&lt;/td&gt; &#xA;   &lt;td&gt;83.3&lt;/td&gt; &#xA;   &lt;td&gt;422&lt;/td&gt; &#xA;   &lt;td&gt;29.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bytetrack_l_mot17 &lt;a href=&#34;https://drive.google.com/file/d/1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1242adimKM6TYdeLU2qnuRA&#34;&gt;[baidu(code:1cml)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;88.7&lt;/td&gt; &#xA;   &lt;td&gt;80.7&lt;/td&gt; &#xA;   &lt;td&gt;460&lt;/td&gt; &#xA;   &lt;td&gt;43.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bytetrack_m_mot17 &lt;a href=&#34;https://drive.google.com/file/d/11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1fKemO1uZfvNSLzJfURO4TQ&#34;&gt;[baidu(code:u3m4)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.0&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;477&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bytetrack_s_mot17 &lt;a href=&#34;https://drive.google.com/file/d/1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1PiP1kQfgxAIrnGUbFP6Wfg&#34;&gt;[baidu(code:qflm)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;79.2&lt;/td&gt; &#xA;   &lt;td&gt;74.3&lt;/td&gt; &#xA;   &lt;td&gt;533&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Light models&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDs&lt;/th&gt; &#xA;   &lt;th&gt;Params(M)&lt;/th&gt; &#xA;   &lt;th&gt;FLOPs(G)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bytetrack_nano_mot17 &lt;a href=&#34;https://drive.google.com/file/d/1AoN2AxzVwOLM0gJ15bcwqZUpFjlDV1dX/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1dMxqBPP7lFNRZ3kFgDmWdw&#34;&gt;[baidu(code:1ub8)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;69.0&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;531&lt;/td&gt; &#xA;   &lt;td&gt;0.90&lt;/td&gt; &#xA;   &lt;td&gt;3.99&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bytetrack_tiny_mot17 &lt;a href=&#34;https://drive.google.com/file/d/1LFAl14sql2Q5Y9aNFsX_OqsnIzUD_1ju/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1jgIqisPSDw98HJh8hqhM5w&#34;&gt;[baidu(code:cr8i)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;71.5&lt;/td&gt; &#xA;   &lt;td&gt;519&lt;/td&gt; &#xA;   &lt;td&gt;5.03&lt;/td&gt; &#xA;   &lt;td&gt;24.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MOT20 test model&lt;/h3&gt; &#xA;&lt;p&gt;Train on CrowdHuman and MOT20, evaluate on MOT20 train.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDs&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bytetrack_x_mot20 &lt;a href=&#34;https://drive.google.com/file/d/1HX2_JpMOjOIj1Z9rJjoet9XNy_cCAs5U/view?usp=sharing&#34;&gt;[google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1bowJJj0bAnbhEQ3_6_Am0A&#34;&gt;[baidu(code:3apd)]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;93.4&lt;/td&gt; &#xA;   &lt;td&gt;89.3&lt;/td&gt; &#xA;   &lt;td&gt;1057&lt;/td&gt; &#xA;   &lt;td&gt;17.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;The COCO pretrained YOLOX model can be downloaded from their &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/tree/0.1.0&#34;&gt;model zoo&lt;/a&gt;. After downloading the pretrained models, you can put them under &amp;lt;ByteTrack_HOME&amp;gt;/pretrained.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train ablation model (MOT17 half train and CrowdHuman)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/train.py -f exps/example/mot/yolox_x_ablation.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train MOT17 test model (MOT17 train, CrowdHuman, Cityperson and ETHZ)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/train.py -f exps/example/mot/yolox_x_mix_det.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train MOT20 test model (MOT20 train, CrowdHuman)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For MOT20, you need to clip the bounding boxes inside the image.&lt;/p&gt; &#xA;&lt;p&gt;Add clip operation in &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/data_augment.py#L134&#34;&gt;line 134-135 in data_augment.py&lt;/a&gt;, &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/datasets/mosaicdetection.py#L122&#34;&gt;line 122-125 in mosaicdetection.py&lt;/a&gt;, &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/data/datasets/mosaicdetection.py#L217&#34;&gt;line 217-225 in mosaicdetection.py&lt;/a&gt;, &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/72cd6dd24083c337a9177e484b12bb2b5b3069a6/yolox/utils/boxes.py#L115&#34;&gt;line 115-118 in boxes.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/train.py -f exps/example/mot/yolox_x_mix_mot20_ch.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Train custom dataset&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First, you need to prepare your dataset in COCO format. You can refer to &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/main/tools/convert_mot17_to_coco.py&#34;&gt;MOT-to-COCO&lt;/a&gt; or &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/main/tools/convert_crowdhuman_to_coco.py&#34;&gt;CrowdHuman-to-COCO&lt;/a&gt;. Then, you need to create a Exp file for your dataset. You can refer to the &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/main/exps/example/mot/yolox_x_ch.py&#34;&gt;CrowdHuman&lt;/a&gt; training Exp file. Don&#39;t forget to modify get_data_loader() and get_eval_loader in your Exp file. Finally, you can train bytetrack on your dataset by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/train.py -f exps/example/mot/your_exp_file.py -d 8 -b 48 --fp16 -o -c pretrained/yolox_x.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tracking&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation on MOT17 half val&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run ByteTrack:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/track.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can get 76.6 MOTA using our pretrained model.&lt;/p&gt; &#xA;&lt;p&gt;Run other trackers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 tools/track_sort.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse&#xA;python3 tools/track_deepsort.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse&#xA;python3 tools/track_motdt.py -f exps/example/mot/yolox_x_ablation.py -c pretrained/bytetrack_ablation.pth.tar -b 1 -d 1 --fp16 --fuse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test on MOT17&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run ByteTrack:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/track.py -f exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar -b 1 -d 1 --fp16 --fuse&#xA;python3 tools/interpolation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Submit the txt files to &lt;a href=&#34;https://motchallenge.net/&#34;&gt;MOTChallenge&lt;/a&gt; website and you can get 79+ MOTA (For 80+ MOTA, you need to carefully tune the test image size and high score detection threshold of each sequence).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test on MOT20&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We use the input size 1600 x 896 for MOT20-04, MOT20-07 and 1920 x 736 for MOT20-06, MOT20-08. You can edit it in &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/main/exps/example/mot/yolox_x_mix_mot20_ch.py&#34;&gt;yolox_x_mix_mot20_ch.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run ByteTrack:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/track.py -f exps/example/mot/yolox_x_mix_mot20_ch.py -c pretrained/bytetrack_x_mot20.pth.tar -b 1 -d 1 --fp16 --fuse --match_thresh 0.7 --mot20&#xA;python3 tools/interpolation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Submit the txt files to &lt;a href=&#34;https://motchallenge.net/&#34;&gt;MOTChallenge&lt;/a&gt; website and you can get 77+ MOTA (For higher MOTA, you need to carefully tune the test image size and high score detection threshold of each sequence).&lt;/p&gt; &#xA;&lt;h2&gt;Applying BYTE to other trackers&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/tree/main/tutorials&#34;&gt;tutorials&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Combining BYTE with other detectors&lt;/h2&gt; &#xA;&lt;p&gt;Suppose you have already got the detection results &#39;dets&#39; (x1, y1, x2, y2, score) from other detectors, you can simply pass the detection results to BYTETracker (you need to first modify some post-processing code according to the format of your detection results in &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/main/yolox/tracker/byte_tracker.py&#34;&gt;byte_tracker.py&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from yolox.tracker.byte_tracker import BYTETracker&#xA;tracker = BYTETracker(args)&#xA;for image in images:&#xA;   dets = detector(image)&#xA;   online_targets = tracker.update(dets, info_imgs, img_size)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can get the tracking results in each frame from &#39;online_targets&#39;. You can refer to &lt;a href=&#34;https://github.com/ifzhang/ByteTrack/raw/main/yolox/evaluators/mot_evaluator.py&#34;&gt;mot_evaluators.py&lt;/a&gt; to pass the detection results to BYTETracker.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/assets/palace_demo.gif&#34; width=&#34;600&#34;&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;ByteTrack_HOME&amp;gt;&#xA;python3 tools/demo_track.py video -f exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar --fp16 --fuse --save_result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deploy&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/deploy/ONNXRuntime&#34;&gt;ONNX export and ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/deploy/TensorRT/python&#34;&gt;TensorRT in Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/deploy/TensorRT/cpp&#34;&gt;TensorRT in C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/deploy/ncnn/cpp&#34;&gt;ncnn in C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ifzhang/ByteTrack/main/deploy/DeepStream&#34;&gt;Deepstream&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2022bytetrack,&#xA;  title={ByteTrack: Multi-Object Tracking by Associating Every Detection Box},&#xA;  author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},&#xA;  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;A large part of the code is borrowed from &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt;, &lt;a href=&#34;https://github.com/ifzhang/FairMOT&#34;&gt;FairMOT&lt;/a&gt;, &lt;a href=&#34;https://github.com/PeizeSun/TransTrack&#34;&gt;TransTrack&lt;/a&gt; and &lt;a href=&#34;https://github.com/samylee/Towards-Realtime-MOT-Cpp&#34;&gt;JDE-Cpp&lt;/a&gt;. Many thanks for their wonderful works.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nerfstudio-project/nerfstudio</title>
    <updated>2022-10-27T01:38:46Z</updated>
    <id>tag:github.com,2022-10-27:/nerfstudio-project/nerfstudio</id>
    <link href="https://github.com/nerfstudio-project/nerfstudio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collaboration friendly studio for NeRFs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- community badges --&gt; &lt;a href=&#34;https://discord.gg/uMbNqcraFc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Join-Discord-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- doc badges --&gt; &lt;a href=&#34;https://plenoptix-nerfstudio.readthedocs-hosted.com/en/latest/?badge=latest&#34;&gt; &lt;img src=&#34;https://readthedocs.com/projects/plenoptix-nerfstudio/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- pi package badge --&gt; &lt;a href=&#34;https://badge.fury.io/py/nerfstudio&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/nerfstudio.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- code check badges --&gt; &lt;a href=&#34;https://github.com/nerfstudio-project/nerfstudio/actions/workflows/core_code_checks.yml&#34;&gt; &lt;img src=&#34;https://github.com/nerfstudio-project/nerfstudio/actions/workflows/core_code_checks.yml/badge.svg?sanitize=true&#34; alt=&#34;Test Status&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/nerfstudio-project/nerfstudio/actions/workflows/viewer_build_deploy.yml&#34;&gt; &lt;img src=&#34;https://github.com/nerfstudio-project/nerfstudio/actions/workflows/viewer_build_deploy.yml/badge.svg?sanitize=true&#34; alt=&#34;Viewer build Status&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- license badge --&gt; &lt;a href=&#34;https://github.com/nerfstudio-project/nerfstudio/raw/master/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://docs.nerf.studio/en/latest/_images/logo-dark.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://docs.nerf.studio/en/latest/_images/logo.png&#34;&gt; &#xA;  &lt;img alt=&#34;nerfstudio&#34; src=&#34;https://docs.nerf.studio/en/latest/_images/logo.png&#34; width=&#34;400&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;!-- Use this for pypi package (and disable above). Hacky workaround --&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&#xA;    &lt;img alt=&#34;nerfstudio&#34; src=&#34;https://docs.nerf.studio/en/latest/_images/logo.png&#34; width=&#34;400&#34;&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; A collaboration friendly studio for NeRFs &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.nerf.studio&#34;&gt; &lt;img alt=&#34;documentation&#34; src=&#34;https://user-images.githubusercontent.com/3310961/194022638-b591ce16-76e3-4ba6-9d70-3be252b36084.png&#34; width=&#34;150&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://viewer.nerf.studio/&#34;&gt; &lt;img alt=&#34;viewer&#34; src=&#34;https://user-images.githubusercontent.com/3310961/194022636-a9efb85a-14fd-4002-8ed4-4ca434898b5a.png&#34; width=&#34;150&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3310961/194017985-ade69503-9d68-46a2-b518-2db1a012f090.gif&#34; width=&#34;52%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/3310961/194020648-7e5f380c-15ca-461d-8c1c-20beb586defe.gif&#34; width=&#34;46%&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nerfstudio-project/nerfstudio/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nerfstudio-project/nerfstudio/main/#learn-more&#34;&gt;Learn more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nerfstudio-project/nerfstudio/main/#supported-features&#34;&gt;Supported Features&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;Nerfstudio provides a simple API that allows for a simplified end-to-end process of creating, training, and testing NeRFs. The library supports a &lt;strong&gt;more interpretable implementation of NeRFs by modularizing each component.&lt;/strong&gt; With more modular NeRFs, we hope to create a more user-friendly experience in exploring the technology. Nerfstudio is a contributor-friendly repo with the goal of building a community where users can more easily build upon each other&#39;s contributions.&lt;/p&gt; &#xA;&lt;p&gt;It’s as simple as plug and play with nerfstudio!&lt;/p&gt; &#xA;&lt;p&gt;We are committed to providing learning resources to help you understand the basics of (if you&#39;re just getting started), and keep up-to-date with (if you&#39;re a seasoned veteran) all things NeRF. As researchers, we know just how hard it is to get onboarded with this next-gen technology. So we&#39;re here to help with tutorials, documentation, and more!&lt;/p&gt; &#xA;&lt;p&gt;Have feature requests? Want to add your brand-spankin&#39;-new NeRF model? Have a new dataset? &lt;strong&gt;We welcome any and all &lt;a href=&#34;https://docs.nerf.studio/en/latest/reference/contributing.html&#34;&gt;contributions&lt;/a&gt;!&lt;/strong&gt; Please do not hesitate to reach out to the nerfstudio team with any questions via &lt;a href=&#34;https://discord.gg/uMbNqcraFc&#34;&gt;Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We hope nerfstudio enables you to build faster &lt;span&gt;🔨&lt;/span&gt; learn together &lt;span&gt;📚&lt;/span&gt; and contribute to our NeRF community &lt;span&gt;💖&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;The quickstart will help you get started with the default vanilla NeRF trained on the classic Blender Lego scene. For more complex changes (e.g., running with your own data/setting up a new NeRF graph, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/nerfstudio-project/nerfstudio/main/#learn-more&#34;&gt;references&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;1. Installation: Setup the environment&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;CUDA must be installed on the system. This library has been tested with version 11.3. You can find more information about installing CUDA &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Create environment&lt;/h3&gt; &#xA;&lt;p&gt;Nerfstudio requires &lt;code&gt;python &amp;gt;= 3.7&lt;/code&gt;. We recommend using conda to manage dependencies. Make sure to install &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Conda&lt;/a&gt; before proceeding.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name nerfstudio -y python=3.8&#xA;conda activate nerfstudio&#xA;python -m pip install --upgrade pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install pytorch with CUDA (this repo has been tested with CUDA 11.3) and &lt;a href=&#34;https://github.com/NVlabs/tiny-cuda-nn&#34;&gt;tiny-cuda-nn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html&#xA;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing nerfstudio&lt;/h3&gt; &#xA;&lt;p&gt;Easy option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install nerfstudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would want the latest and greatest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/nerfstudio-project/nerfstudio.git&#xA;cd nerfstudio&#xA;pip install --upgrade pip setuptools&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Setting up the data&lt;/h2&gt; &#xA;&lt;p&gt;Download the original NeRF Blender dataset. We support the major datasets and allow users to create their own dataset, described in detail &lt;a href=&#34;https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ns-download-data --dataset=blender&#xA;ns-download-data --dataset=nerfstudio --capture=poster&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2.x Using custom data&lt;/h3&gt; &#xA;&lt;p&gt;If you have custom data in the form of a video or folder of images, we&#39;ve provided some &lt;a href=&#34;https://colmap.github.io/&#34;&gt;COLMAP&lt;/a&gt; and &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;FFmpeg&lt;/a&gt; scripts to help you process your data so it is compatible with nerfstudio.&lt;/p&gt; &#xA;&lt;p&gt;After installing both software, you can process your data via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ns-process-data {video,images,insta360} --data {DATA_PATH} --output-dir {PROCESSED_DATA_DIR}&#xA;# Or if you&#39;re on a system without an attached display (i.e. colab):&#xA;ns-process-data {video,images,insta360} --data {DATA_PATH}  --output-dir {PROCESSED_DATA_DIR} --no-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Training a model&lt;/h2&gt; &#xA;&lt;p&gt;To run with all the defaults, e.g., vanilla NeRF method with the Blender Lego image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# To see what models are available.&#xA;ns-train --help&#xA;&#xA;# To see what model-specific cli arguments are available.&#xA;ns-train nerfacto --help&#xA;&#xA;# Run with nerfacto model.&#xA;ns-train nerfacto&#xA;&#xA;# We provide support for other models. E.g., to run instant-ngp.&#xA;ns-train instant-ngp&#xA;&#xA;# To train on your custom data.&#xA;ns-train nerfacto --data {PROCESSED_DATA_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3.x Training a model with the viewer&lt;/h3&gt; &#xA;&lt;p&gt;You can visualize training in real-time using our web-based viewer.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to forward a port for the websocket to localhost. The default port is 7007, which you should expose to localhost:7007.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# with the default port&#xA;ns-train nerfacto --vis viewer&#xA;&#xA;# with a specified websocket port&#xA;ns-train nerfacto --vis viewer --viewer.websocket-port=7008&#xA;&#xA;# port forward if running on remote&#xA;ssh -L localhost:7008:localhost:7008 {REMOTE HOST}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details on how to interact with the visualizer, please visit our viewer &lt;a href=&#34;https://docs.nerf.studio/en/latest/quickstart/viewer_quickstart.html&#34;&gt;walk-through&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Rendering a trajectory during inference&lt;/h2&gt; &#xA;&lt;p&gt;After your model has trained, you can headlessly render out a video of the scene with a pre-defined trajectory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assuming previously ran `ns-train nerfacto`&#xA;ns-render --load-config=outputs/data-nerfstudio-poster/nerfacto/{TIMESTAMP}/config.yml --traj=spiral --output-path=renders/output.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Learn More&lt;/h1&gt; &#xA;&lt;p&gt;And that&#39;s it for getting started with the basics of nerfstudio.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re interested in learning more on how to create your own pipelines, develop with the viewer, run benchmarks, and more, please check out some of the quicklinks below or visit our &lt;a href=&#34;https://docs.nerf.studio/en/latest/&#34;&gt;documentation&lt;/a&gt; directly.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Section&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Full API documentation and tutorials&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://viewer.nerf.studio/&#34;&gt;Viewer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Home page for our web viewer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🎒 &lt;strong&gt;Educational&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/nerfology/methods/index.html&#34;&gt;Model Descriptions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Description of all the models supported by nerfstudio and explanations of component parts.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/nerfology/model_components/index.html&#34;&gt;Component Descriptions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Interactive notebooks that explain notable/commonly used modules in various models.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🏃 &lt;strong&gt;Tutorials&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/quickstart/installation.html&#34;&gt;Getting Started&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A more in-depth guide on how to get started with nerfstudio from installation to contributing.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/quickstart/viewer_quickstart.html&#34;&gt;Using the Viewer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A quick demo video on how to navigate the viewer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;💻 &lt;strong&gt;For Developers&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/developer_guides/pipelines/index.html&#34;&gt;Creating pipelines&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn how to easily build new neural rendering pipelines by using and/or implementing new modules.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/quickstart/custom_dataset.html&#34;&gt;Creating datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Have a new dataset? Learn how to run it with nerfstudio.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nerf.studio/en/latest/reference/contributing.html&#34;&gt;Contributing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Walk-through for how you can start contributing now.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;💖 &lt;strong&gt;Community&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/uMbNqcraFc&#34;&gt;Discord&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Join our community to discuss more. We would love to hear from you!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/nerfstudioteam&#34;&gt;Twitter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Follow us on Twitter @nerfstudioteam to see cool updates and announcements&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nerfstudio-project/nerfstudio/main/#&#34;&gt;TikTok&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon! Follow us on TikTok to see some of our fan favorite results&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Supported Features&lt;/h1&gt; &#xA;&lt;p&gt;We provide the following support structures to make life easier for getting started with NeRFs. For a full description, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/nerfstudio-project/nerfstudio/main/#&#34;&gt;features page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you are looking for a feature that is not currently supported, please do not hesitate to contact the Nerfstudio Team on &lt;a href=&#34;https://discord.gg/uMbNqcraFc&#34;&gt;Discord&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;🔎&lt;/span&gt; Web-based visualizer that allows you to: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Visualize training in real-time + interact with the scene&lt;/li&gt; &#xA;   &lt;li&gt;Create and render out scenes with custom camera trajectories&lt;/li&gt; &#xA;   &lt;li&gt;View different output types&lt;/li&gt; &#xA;   &lt;li&gt;And more!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;✏&lt;/span&gt; Support for multiple logging interfaces (Tensorboard, Wandb), code profiling, and other built-in debugging tools&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;📈&lt;/span&gt; Easy-to-use benchmarking scripts on the Blender dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;📱&lt;/span&gt; Full pipeline support (w/ Colmap or Record3D) for going from a video on your phone to a full 3D render.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Built On&lt;/h1&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://github.com/brentyi/tyro&#34;&gt;tyro&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easy-to-use config system&lt;/li&gt; &#xA; &lt;li&gt;Developed by &lt;a href=&#34;https://brentyi.com/&#34;&gt;Brent Yi&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://www.nerfacc.com/en/latest/&#34;&gt;nerfacc&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Library for accelerating NeRF renders&lt;/li&gt; &#xA; &lt;li&gt;Developed by &lt;a href=&#34;https://www.liruilong.cn/&#34;&gt;Ruilong Li&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use this library or find the documentation useful for your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{nerfstudio,&#xA;      title={Nerfstudio: A Framework for Neural Radiance Field Development},&#xA;      author={Matthew Tancik*, Ethan Weber*, Evonne Ng*, Ruilong Li, Brent Yi,&#xA;              Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi,&#xA;              Abhik Ahuja, David McAllister, Angjoo Kanazawa},&#xA;      year={2022},&#xA;      url={https://github.com/nerfstudio-project/nerfstudio},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;a href=&#34;https://github.com/nerfstudio-project/nerfstudio/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=nerfstudio-project/nerfstudio&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>