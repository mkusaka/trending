<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-18T01:39:59Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hudson-and-thames/arbitragelab</title>
    <updated>2024-04-18T01:39:59Z</updated>
    <id>tag:github.com,2024-04-18:/hudson-and-thames/arbitragelab</id>
    <link href="https://github.com/hudson-and-thames/arbitragelab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ArbitrageLab is a python library that enables traders who want to exploit mean-reverting portfolios by providing a complete set of algorithms from the best academic journals.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://hudsonthames.org/arbitragelab&#34;&gt; &lt;img src=&#34;https://hudsonthames.org/wp-content/uploads/2021/04/featured-picture-arbitragelab.jpg&#34; height=&#34;200&#34; style=&#34;margin-left: auto; margin-right: auto; display:block;&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Welcome to the Arbitrage Laboratory!&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;What was only possible with the help of huge R&amp;amp;D teams is now at your disposal, anywhere, anytime.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ArbitrageLab is a python library that includes both end-to-end strategies and strategy creation tools that cover the whole range of strategies defined by &lt;a href=&#34;https://www.econstor.eu/bitstream/10419/116783/1/833997289.pdf&#34;&gt;Krauss&#39; taxonomy&lt;/a&gt; for pairs trading strategies.&lt;/p&gt; &#xA;&lt;h2&gt;What is ArbitrageLab?&lt;/h2&gt; &#xA;&lt;p&gt;ArbitrageLab is an open-source python library that enables traders who want to exploit mean-reverting portfolios by providing a complete set of algorithms from the best academic journals.&lt;/p&gt; &#xA;&lt;p&gt;View the documentation to &lt;a href=&#34;https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/index.html&#34;&gt;get started&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Special Thank You:&lt;/h2&gt; &#xA;&lt;p&gt;A lot of passion and love went into the creation of this library, and we would like to say special thank you to:&lt;/p&gt; &#xA;&lt;p&gt;Original Team:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/jacquesjoubert/&#34;&gt;Jacques Francois Joubert&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/illyabarziy/&#34;&gt;Illya Barziy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/valeriia-pervushyna/&#34;&gt;Valeriia Pervushyna&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/dirkfreese/&#34;&gt;Dirk Frees&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A heartfelt thank you to Illya and Valeriia for your exceptional contributions to ArbitrageLab. Your dedication and talent have been instrumental in enhancing the library and company as a whole. Your technical ingenuity, and meticulous attention to detail, have not only enriched our project but also set a high standard for excellence. We deeply appreciate your hard work and commitment to making ArbitrageLab a success.&lt;/p&gt; &#xA;&lt;p&gt;A special thank you to Dirk for the quality time and deep insights you have dedicated to enhancing our business. Your expertise and motivational efforts were, and continue to be invaluable. We greatly appreciate your commitment and enthusiastic support. We couldn&#39;t have asked for a better Start-Up Advisor!&lt;/p&gt; &#xA;&lt;p&gt;Core Contributions&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/hansen-pei-0949691b3/&#34;&gt;Hansen Pei&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Yefeng Wang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/vijay-nadimpalli/&#34;&gt;Vijay Nadimpalli&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/joohwan-ko-638748174/&#34;&gt;Joohwan Ko&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dedicated to WorldQuant University (WQU)&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.wqu.edu/&#34;&gt; &lt;img src=&#34;https://www.wqu.edu/_next/static/media/logo-full-color.244f5961.svg?sanitize=true&#34; height=&#34;150&#34; style=&#34;margin-left: auto; margin-right: auto; display:block;&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We are thrilled to highlight an exceptional educational opportunity for those passionate about financial engineering â€” WorldQuant Universityâ€™s Master of Science in Financial Engineering (MSFE) program. This groundbreaking initiative is completely online and tuition-free, democratizing advanced education in a way that&#39;s accessible to individuals around the globe.&lt;/p&gt; &#xA;&lt;p&gt;The MSFE program at WorldQuant University is designed to equip students with the quantitative skills essential for a competitive edge in today&#39;s tech-driven finance sectors. With a curriculum that balances theory and practical application, students not only gain deep insights but also practical skills that can be immediately applied in various financial roles.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re looking to elevate your expertise or pivot your career towards quantitative finance, I encourage you to explore this opportunity. WorldQuant University is not just about education; itâ€™s about empowering future financial leaders. Learn more about their &lt;a href=&#34;https://www.wqu.edu/&#34;&gt;MSFE program&lt;/a&gt; and take a significant step towards transforming your professional life.&lt;/p&gt; &#xA;&lt;h2&gt;Learn To Build Production Ready Python Libraries&lt;/h2&gt; &#xA;&lt;p&gt;We have released a course on Udemy that you can follow to produce your own open-source projects for finance.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udemy.com/course/writing-production-grade-code-for-quantitative-developers/&#34;&gt;Writing Production-Grade Python Code for Quant Developers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>dvlab-research/MiniGemini</title>
    <updated>2024-04-18T01:39:59Z</updated>
    <id>tag:github.com,2024-04-18:/dvlab-research/MiniGemini</id>
    <link href="https://github.com/dvlab-research/MiniGemini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation for Mini-Gemini&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mini-gemini.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Demo-violet&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/wcy1122/Mini-Gemini&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ðŸ¤—-Open%20In%20Spaces-blue.svg&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.18814.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B with image understanding, reasoning, and generation simultaneously. We build this repo based on LLaVA.&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[04/15] ðŸ”¥ The &lt;a href=&#34;https://huggingface.co/spaces/wcy1122/Mini-Gemini&#34;&gt;Hugging Face demo&lt;/a&gt; is available. It&#39;s a 13B-HD version, welcome to watch and try.&lt;/li&gt; &#xA; &lt;li&gt;[03/28] ðŸ”¥ Mini-Gemini is coming! We release the &lt;a href=&#34;https://arxiv.org/pdf/2403.18814.pdf&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;demo&lt;/a&gt;, &lt;a href=&#34;https://github.com/dvlab-research/MiniGemini&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854&#34;&gt;models&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e&#34;&gt;data&lt;/a&gt; for Mini-Gemini!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#model&#34;&gt;Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#preparation&#34;&gt;Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#train&#34;&gt;Train&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;We provide some selected examples in this section. More examples can be found in our &lt;a href=&#34;https://mini-gemini.github.io/&#34;&gt;project page&lt;/a&gt;. Feel free to try our online &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;demo&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;100%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/teaser.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the instructions below to install the required packages.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: If you want to use Mini-Gemini-2B, please ensure to install the latest version Transformers (&amp;gt;=4.38.0).&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/dvlab-research/MiniGemini.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n minigemini python=3.10 -y&#xA;conda activate minigemini&#xA;cd MiniGemini&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ninja&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;The framework of Mini-Gemini is conceptually simple: dual vision encoders are utilized to provide low-resolution visual embedding and high-resolution candidates; patch info mining is proposed to conduct patch-level mining between high-resolution regions and low-resolution visual queries; LLM is utilized to marry text with images for both comprehension and generation at the same time.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;98%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/pipeline.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We provide all our fully finetuned models on Stage 1 and 2 data for Mini-Gemini:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LR&lt;/th&gt; &#xA;   &lt;th&gt;HR&lt;/th&gt; &#xA;   &lt;th&gt;Base LLM&lt;/th&gt; &#xA;   &lt;th&gt;Vision Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Finetuning Data&lt;/th&gt; &#xA;   &lt;th&gt;Finetuning schedule&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-2B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Gemma-2B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-2B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B-HD&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;full_ft-1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Here are the pretrained weights on Stage 1 data only:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LR&lt;/th&gt; &#xA;   &lt;th&gt;HR&lt;/th&gt; &#xA;   &lt;th&gt;Base LLM&lt;/th&gt; &#xA;   &lt;th&gt;Vision Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Pretrain Data&lt;/th&gt; &#xA;   &lt;th&gt;Finetuning schedule&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-2B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Gemma-2B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-2B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-13B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-8x7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;CLIP-L&lt;/td&gt; &#xA;   &lt;td&gt;MiniGemini-Pretrain&lt;/td&gt; &#xA;   &lt;td&gt;1e&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-Pretrain/tree/main/Mini-Gemini-34B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;We provide the processed data for Mini-Gemini training. For model pretraining, please download the following the training image-based data and organize them as:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;-&amp;gt;&lt;/code&gt; means put the data in the local folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain&#34;&gt;LLaVA Images&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Pretrain/images&lt;/code&gt;, &lt;code&gt;data/MiniGemini-Finetune/llava/LLaVA-Pretrain/images&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FreedomIntelligence/ALLaVA&#34;&gt;ALLaVA Caption&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Pretrain/ALLaVA-4V&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For model finetuning, please download the following the instruction data and organize them as:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;-&amp;gt;&lt;/code&gt; means put the data in the local folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://images.cocodataset.org/zips/train2017.zip&#34;&gt;COCO train2017&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/coco&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip&#34;&gt;GQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/gqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing&#34;&gt;OCR-VQA&lt;/a&gt; (&lt;strong&gt;we save all files as &lt;code&gt;.jpg&lt;/code&gt;&lt;/strong&gt;) -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/ocr_vqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip&#34;&gt;TextVQA&lt;/a&gt; (not included for training) -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/textvqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip&#34;&gt;VisualGenome part1&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip&#34;&gt;VisualGenome part2&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/vg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/raw/main/projects/ShareGPT4V/docs/Data.md&#34;&gt;ShareGPT4V-100K&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/sam&lt;/code&gt;, &lt;code&gt;share_textvqa&lt;/code&gt;, &lt;code&gt;wikiart&lt;/code&gt;, &lt;code&gt;web-celebrity&lt;/code&gt;, &lt;code&gt;web-landmark&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/laion/gpt4v-dataset&#34;&gt;LAION GPT4V&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/gpt4v-dataset&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FreedomIntelligence/ALLaVA&#34;&gt;ALLaVA Instruction&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Pretrain/ALLaVA-4V&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.docvqa.org/datasets/docvqa&#34;&gt;DocVQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/docvqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vis-nlp/ChartQA&#34;&gt;ChartQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/chartqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kushalkafle/DVQA_dataset&#34;&gt;DVQA&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/dvqa&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://allenai.org/data/diagrams&#34;&gt;AI2D&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Finetune/ai2d&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For model evaluation, please follow this &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;link&lt;/a&gt; for preparation. We use some extra benchmarks for evaluation. please download the following the training image-based data and organize them as:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;-&amp;gt;&lt;/code&gt; means put the data in the local folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mmmu-benchmark.github.io/&#34;&gt;MMMU&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Eval/MMMU&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-compass/mmbench/&#34;&gt;MMB&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Eval/MMB&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mathvista.github.io/&#34;&gt;MathVista&lt;/a&gt; -&amp;gt; &lt;code&gt;data/MiniGemini-Eval/MathVista&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please put the pretrained data, finetuned data, and eval data in &lt;code&gt;MiniGemini-Pretrain&lt;/code&gt;, &lt;code&gt;MiniGemini-Finetune&lt;/code&gt;, and &lt;code&gt;MiniGemini-Eval&lt;/code&gt; subset following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For meta info, please download the following files and organize them as in &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data file name&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/YanweiLi/Mini-Gemini-Pretrain&#34;&gt;minigemini_pretrain.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.68 G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/YanweiLi/Mini-Gemini-Instruction&#34;&gt;minigemini_instruction.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.79 G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/YanweiLi/Mini-Gemini-Instruction/blob/main/minigemini_generation_pure_text.json&#34;&gt;minigemini_generation_pure_text.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0.04 G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;IMPORTANT: &lt;code&gt;minigemini_generation_pure_text.json&lt;/code&gt; is a generation-related subset. &lt;strong&gt;DO NOT&lt;/strong&gt; merge it with &lt;code&gt;minigemini_instruction.json&lt;/code&gt; as it is already included in it. You may merge this file with your customized LLM/VLM SFT dataset to enable the reasoning generation ability.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Weights&lt;/h3&gt; &#xA;&lt;p&gt;We recommend users to download the pretrained weights from the following link &lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14-336&#34;&gt;CLIP-Vit-L-336&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup&#34;&gt;OpenCLIP-ConvNeXt-L&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/google/gemma-2b-it&#34;&gt;Gemma-2b-it&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-7b-v1.5&#34;&gt;Vicuna-7b-v1.5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-13b-v1.5&#34;&gt;Vicuna-13b-v1.5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B&#34;&gt;Nous-Hermes-2-Yi-34B&lt;/a&gt; , and put them in &lt;code&gt;model_zoo&lt;/code&gt; following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Structure&lt;/h3&gt; &#xA;&lt;p&gt;The folder structure should be organized as follows before training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MiniGemini&#xA;â”œâ”€â”€ minigemini&#xA;â”œâ”€â”€ scripts&#xA;â”œâ”€â”€ work_dirs&#xA;â”‚   â”œâ”€â”€ Mini-Gemini&#xA;â”‚   â”‚   â”œâ”€â”€ Mini-Gemini-2B&#xA;â”‚   â”‚   â”œâ”€â”€ ...&#xA;â”œâ”€â”€ model_zoo&#xA;â”‚   â”œâ”€â”€ LLM&#xA;â”‚   â”‚   â”œâ”€â”€ gemma&#xA;â”‚   â”‚   â”‚   â”œâ”€â”€ gemma-2b-it&#xA;â”‚   â”‚   â”œâ”€â”€ vicuna&#xA;â”‚   â”‚   â”‚   â”œâ”€â”€ 7B-V1.5&#xA;â”‚   â”‚   â”‚   â”œâ”€â”€ 13B-V1.5&#xA;â”‚   â”‚   â”œâ”€â”€ mixtral&#xA;â”‚   â”‚   â”‚   â”œâ”€â”€ Mixtral-8x7B-Instruct-v0.1&#xA;â”‚   â”‚   â”œâ”€â”€ Nous-Hermes-2-Yi-34B&#xA;â”‚   â”œâ”€â”€ OpenAI&#xA;â”‚   â”‚   â”œâ”€â”€ clip-vit-large-patch14-336&#xA;â”‚   â”‚   â”œâ”€â”€ openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup&#xA;â”œâ”€â”€ data&#xA;â”‚   â”œâ”€â”€ MiniGemini-Pretrain&#xA;â”‚   â”‚   â”œâ”€â”€ minigemini_pretrain.json&#xA;â”‚   â”‚   â”œâ”€â”€ images&#xA;â”‚   â”‚   â”œâ”€â”€ ALLaVA-4V&#xA;â”‚   â”œâ”€â”€ MiniGemini-Finetune&#xA;â”‚   â”‚   â”œâ”€â”€ minigemini_instruction.json&#xA;â”‚   â”‚   â”œâ”€â”€ llava&#xA;â”‚   â”‚   â”œâ”€â”€ coco&#xA;â”‚   â”‚   â”œâ”€â”€ gqa&#xA;â”‚   â”‚   â”œâ”€â”€ ocr_vqa&#xA;â”‚   â”‚   â”œâ”€â”€ textvqa&#xA;â”‚   â”‚   â”œâ”€â”€ vg&#xA;â”‚   â”‚   â”œâ”€â”€ gpt4v-dataset&#xA;â”‚   â”‚   â”œâ”€â”€ sam&#xA;â”‚   â”‚   â”œâ”€â”€ share_textvqa&#xA;â”‚   â”‚   â”œâ”€â”€ wikiart&#xA;â”‚   â”‚   â”œâ”€â”€ web-celebrity&#xA;â”‚   â”‚   â”œâ”€â”€ web-landmark&#xA;â”‚   â”‚   â”œâ”€â”€ ALLaVA-4V&#xA;â”‚   â”‚   â”œâ”€â”€ docvqa&#xA;â”‚   â”‚   â”œâ”€â”€ chartqa&#xA;â”‚   â”‚   â”œâ”€â”€ dvqa&#xA;â”‚   â”‚   â”œâ”€â”€ ai2d&#xA;â”‚   â”œâ”€â”€ MiniGemini-Eval&#xA;â”‚   â”‚   â”œâ”€â”€ MMMU&#xA;â”‚   â”‚   â”œâ”€â”€ MMB&#xA;â”‚   â”‚   â”œâ”€â”€ MathVista&#xA;â”‚   â”‚   â”œâ”€â”€ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;Mini-Gemini training consists of two stages: (1) feature alignment stage: bridge the vision and language tokens; (2) instruction tuning stage: teach the model to follow multimodal instructions.&lt;/p&gt; &#xA;&lt;p&gt;Mini-Gemini is trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly. Always keep the global batch size the same: &lt;code&gt;per_device_train_batch_size&lt;/code&gt; x &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; x &lt;code&gt;num_gpus&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please make sure you download and organize the data following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#preparation&#34;&gt;Preparation&lt;/a&gt; before training.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: Please set &lt;code&gt;hostfile&lt;/code&gt; for 2 machine training and &lt;code&gt;hostfile_4&lt;/code&gt; for 4 machine training.&lt;/p&gt; &#xA;&lt;p&gt;If you want to train and finetune Mini-Gemini, please run the following command for Mini-Gemini-7B with image size 336:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/train/stage_1_2_full_v7b_336_hr_768.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for Mini-Gemini-13B with image size 336:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/train/stage_1_2_full_v13b_336_hr_768.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because we reuse the pre-trained projecter weights from the Mini-Gemini-7B, you can directly use the Mini-Gemini-7B-HD with image size 672 for stage-2 instruction tuning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/train/stage_2_full_v7b_672_hr_1536.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please find more training scripts of &lt;code&gt;gemma&lt;/code&gt;, &lt;code&gt;llama&lt;/code&gt;, &lt;code&gt;mixtral&lt;/code&gt;, and &lt;code&gt;yi&lt;/code&gt; in &lt;code&gt;scripts/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We perform evaluation on several image-based benchmarks. Please download the evaluation data following &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#preparation&#34;&gt;Preparation&lt;/a&gt; and organize them as in &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/#structure&#34;&gt;Structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LLM&lt;/th&gt; &#xA;   &lt;th&gt;Res.&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th&gt;MMB&lt;/th&gt; &#xA;   &lt;th&gt;MME&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;   &lt;th&gt;MMMU_val&lt;/th&gt; &#xA;   &lt;th&gt;MMMU_test&lt;/th&gt; &#xA;   &lt;th&gt;MathVista&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-2B&lt;/td&gt; &#xA;   &lt;td&gt;Gemma-2B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-2B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;56.2&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;   &lt;td&gt;1341/312&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;   &lt;td&gt;31.7&lt;/td&gt; &#xA;   &lt;td&gt;29.1&lt;/td&gt; &#xA;   &lt;td&gt;29.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;65.2&lt;/td&gt; &#xA;   &lt;td&gt;69.3&lt;/td&gt; &#xA;   &lt;td&gt;1523/316&lt;/td&gt; &#xA;   &lt;td&gt;40.8&lt;/td&gt; &#xA;   &lt;td&gt;36.1&lt;/td&gt; &#xA;   &lt;td&gt;32.8&lt;/td&gt; &#xA;   &lt;td&gt;31.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;65.9&lt;/td&gt; &#xA;   &lt;td&gt;68.5&lt;/td&gt; &#xA;   &lt;td&gt;1565/322&lt;/td&gt; &#xA;   &lt;td&gt;46.0&lt;/td&gt; &#xA;   &lt;td&gt;38.1&lt;/td&gt; &#xA;   &lt;td&gt;33.5&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;69.2&lt;/td&gt; &#xA;   &lt;td&gt;75.6&lt;/td&gt; &#xA;   &lt;td&gt;1639/379&lt;/td&gt; &#xA;   &lt;td&gt;45.8&lt;/td&gt; &#xA;   &lt;td&gt;41.8&lt;/td&gt; &#xA;   &lt;td&gt;37.1&lt;/td&gt; &#xA;   &lt;td&gt;41.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;336&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70.1&lt;/td&gt; &#xA;   &lt;td&gt;79.6&lt;/td&gt; &#xA;   &lt;td&gt;1666/439&lt;/td&gt; &#xA;   &lt;td&gt;53.0&lt;/td&gt; &#xA;   &lt;td&gt;48.7&lt;/td&gt; &#xA;   &lt;td&gt;43.6&lt;/td&gt; &#xA;   &lt;td&gt;38.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-7B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;68.4&lt;/td&gt; &#xA;   &lt;td&gt;65.8&lt;/td&gt; &#xA;   &lt;td&gt;1546/319&lt;/td&gt; &#xA;   &lt;td&gt;41.3&lt;/td&gt; &#xA;   &lt;td&gt;36.8&lt;/td&gt; &#xA;   &lt;td&gt;32.9&lt;/td&gt; &#xA;   &lt;td&gt;32.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-13B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna-13B-v1.5&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-13B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70.2&lt;/td&gt; &#xA;   &lt;td&gt;68.6&lt;/td&gt; &#xA;   &lt;td&gt;1597/320&lt;/td&gt; &#xA;   &lt;td&gt;50.5&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;   &lt;td&gt;35.1&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-8x7B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Mixtral-8x7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-8x7B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;71.9&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;1633/356&lt;/td&gt; &#xA;   &lt;td&gt;53.5&lt;/td&gt; &#xA;   &lt;td&gt;40.0&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;   &lt;td&gt;43.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mini-Gemini-34B-HD&lt;/td&gt; &#xA;   &lt;td&gt;Nous-Hermes-2-Yi-34B&lt;/td&gt; &#xA;   &lt;td&gt;672&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/YanweiLi/Mini-Gemini-34B-HD&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.1&lt;/td&gt; &#xA;   &lt;td&gt;80.6&lt;/td&gt; &#xA;   &lt;td&gt;1659/482&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;48.0&lt;/td&gt; &#xA;   &lt;td&gt;44.9&lt;/td&gt; &#xA;   &lt;td&gt;43.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you want to evaluate the model on image-based benchmarks, please use the scripts in &lt;code&gt;scripts/MODEL_PATH/eval&lt;/code&gt;. For example, run the following command for TextVQA evaluation with Mini-Gemini-7B-HD:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/llama/eval/textvqa.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please find more evaluation scripts in &lt;code&gt;scripts/MODEL_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;p&gt;Chat with images using Mini-Gemini without the need of Gradio interface. It also supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit quantization. Please make sure you have installed &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.7/README_en.md&#34;&gt;PaddleOCR&lt;/a&gt; (only for better experience with OCR), and try this for image and generation inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or try this better experience with OCR (make sure you have installed &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/raw/release/2.7/README_en.md&#34;&gt;PaddleOCR&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt; \&#xA;    --ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or try this for inference with generation (make sure you have installed &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt; \&#xA;    --gen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also try 8bit or even 4bit for efficient inference&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m minigemini.serve.cli \&#xA;    --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \&#xA;    --image-file &amp;lt;path to your image&amp;gt; \&#xA;    --gen&#xA;    --load-8bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;Here, we adopt the Gradio UI similar to that in LLaVA to provide a user-friendly interface for Mini-Gemini. To launch a Gradio demo locally, please run the following commands one by one. If you plan to launch multiple model workers to compare between different checkpoints, you only need to launch the controller and the web server &lt;em&gt;ONCE&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;p&gt;This is the actual &lt;em&gt;worker&lt;/em&gt; that performs the inference on the GPU. Each worker is responsible for a single model specified in &lt;code&gt;--model-path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;. Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.&lt;/p&gt; &#xA;&lt;p&gt;You can launch as many workers as you want, and compare between different models in the same Gradio interface. Please keep the &lt;code&gt;--controller&lt;/code&gt; the same, and modify the &lt;code&gt;--port&lt;/code&gt; and &lt;code&gt;--worker&lt;/code&gt; to a different port number for each worker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port &amp;lt;different from 40000, say 40001&amp;gt; --worker http://localhost:&amp;lt;change accordingly, i.e. 40001&amp;gt; --model-path work_dirs/Mini-Gemini/Mini-Gemini-34B-HD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using an Apple device with an M1 or M2 chip, you can specify the mps device by using the &lt;code&gt;--device&lt;/code&gt; flag: &lt;code&gt;--device mps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs. Our latest code base will automatically try to use multiple GPUs if you have more than one GPU. You can specify which GPUs to use with &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;. Below is an example of running with the first two GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker (4-bit, 8-bit inference, quantized)&lt;/h4&gt; &#xA;&lt;p&gt;You can launch the model worker with quantized bits (4-bit, 8-bit), which allows you to run the inference with reduced GPU memory footprint. Note that inference with quantized bits may not be as accurate as the full-precision model. Simply append &lt;code&gt;--load-4bit&lt;/code&gt; or &lt;code&gt;--load-8bit&lt;/code&gt; to the &lt;strong&gt;model worker&lt;/strong&gt; command that you are executing. Below is an example of running with 4-bit quantization.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD --load-4bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;We provide some examples in this section. More examples can be found in our &lt;a href=&#34;https://mini-gemini.github.io/&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Hi-Resolution Understanding&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;98%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/demo_und.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Generation with Reasoning&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;98%&#34; src=&#34;https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/demo_gen.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful for your research, please consider citing the paper&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{li2024minigemini,&#xA;  title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models},&#xA;  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},&#xA;  journal={arXiv:2403.18814},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the following repos for their great work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This work is built upon the &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;This work utilizes LLMs from &lt;a href=&#34;https://huggingface.co/google/gemma-2b-it&#34;&gt;Gemma&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;Mixtral&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B&#34;&gt;Nous-Hermes&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dvlab-research/MiniGemini/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-yellow.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/MiniGemini/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-orange.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/MiniGemini/raw/main/WEIGHT_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Weight%20License-CC%20By%20NC%204.0-red&#34; alt=&#34;Weight License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The data and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaVA, LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>katanaml/sparrow</title>
    <updated>2024-04-18T01:39:59Z</updated>
    <id>tag:github.com,2024-04-18:/katanaml/sparrow</id>
    <link href="https://github.com/katanaml/sparrow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data processing with ML and LLM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sparrow&lt;/h1&gt; &#xA;&lt;p&gt;Data processing with ML and LLM&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;300&#34; height=&#34;300&#34; src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/assets/sparrow_logo_5.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;The Principle&lt;/h2&gt; &#xA;&lt;p&gt;Sparrow is an innovative open-source solution for efficient data extraction and processing from various documents and images. It seamlessly handles forms, invoices, receipts, and other unstructured data sources. Sparrow stands out with its modular architecture, offering independent services and pipelines all optimized for robust performance. One of the critical functionalities of Sparrow - pluggable architecture. You can easily integrate and run data extraction pipelines using tools and frameworks like LlamaIndex, Haystack, or Unstructured. Sparrow enables local LLM data extraction pipelines through Ollama or Apple MLX. With Sparrow solution you get API, which helps to process and transform your data into structured output, ready to be integrated with custom workflows.&lt;/p&gt; &#xA;&lt;p&gt;Sparrow Agents - with Sparrow you can build independent LLM agents, and use API to invoke them from your system.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;List of available agents:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;llamaindex&lt;/strong&gt; - RAG pipeline with LlamaIndex for PDF processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;vllamaindex&lt;/strong&gt; - RAG pipeline with LLamaIndex multimodal for image processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;vprocessor&lt;/strong&gt; - RAG pipeline with OCR and LlamaIndex for image processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;haystack&lt;/strong&gt; - RAG pipeline with Haystack for PDF processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;fcall&lt;/strong&gt; - Function call pipeline&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;unstructured-light&lt;/strong&gt; - RAG pipeline with Unstructured and LangChain, supports PDF and image processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;unstructured&lt;/strong&gt; - RAG pipeline with Weaviate vector DB query, Unstructured and LangChain, supports PDF and image processing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Services&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/ocr&#34;&gt;sparrow-data-ocr&lt;/a&gt;&lt;/strong&gt; - OCR service, providing optical character recognition as part of the Sparrow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ml/llm&#34;&gt;sparrow-ml-llm&lt;/a&gt;&lt;/strong&gt; - LLM RAG pipeline, Sparrow service for data extraction and document processing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-ui/&#34;&gt;sparrow-ui&lt;/a&gt;&lt;/strong&gt; - Dashboard UI for LLM RAG pipeline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sparrow implementation with Donut ML model - &lt;a href=&#34;https://github.com/katanaml/sparrow-donut&#34;&gt;sparrow-donut&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;LLM&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install Weaviate local DB with Docker:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sparrow setup&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Setup Python Environment (Sparrow is tested with Python 3.10.4) with &lt;code&gt;pyenv&lt;/code&gt;:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;code&gt;pyenv&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you haven&#39;t already installed &lt;code&gt;pyenv&lt;/code&gt;, you can do so using Homebrew with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew update&#xA;brew install pyenv&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the desired Python version:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;With &lt;code&gt;pyenv&lt;/code&gt; installed, you can now install a specific version of Python. For example, to install Python 3.10.4, you would use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyenv install 3.10.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can check available Python versions by running &lt;code&gt;pyenv install --list&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set the global Python version:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Once the installation is complete, you can set the desired Python version as the default (global) version on your system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pyenv global 3.10.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command sets Python 3.10.4 as the default version for all shells.&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Verify the change:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To ensure the change was successful, you can verify the current Python version by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the output doesnâ€™t reflect the change, you may need to restart your terminal or add &lt;code&gt;pyenv&lt;/code&gt; to your shell&#39;s initialization script as follows:&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Configure your shell&#39;s initialization script:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Add &lt;code&gt;pyenv&lt;/code&gt; to your shell by adding the following lines to your &lt;code&gt;~/.bash_profile&lt;/code&gt;, &lt;code&gt;~/.zprofile&lt;/code&gt;, &lt;code&gt;~/.bashrc&lt;/code&gt;, or &lt;code&gt;~/.zshrc&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export PATH=&#34;$HOME/.pyenv/bin:$PATH&#34;&#xA;eval &#34;$(pyenv init --path)&#34;&#xA;eval &#34;$(pyenv init -)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After adding these lines, restart your terminal or source your profile script with &lt;code&gt;source ~/.bash_profile&lt;/code&gt; (or the appropriate file for your shell).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Create Virtual Environments to Run Sparrow Agents&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create virtual environments in &lt;code&gt;sparrow-ml/llm&lt;/code&gt; folder:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv .env_llamaindex&#xA;python -m venv .env_haystack&#xA;python -m venv .env_instructor&#xA;python -m venv .env_unstructured&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;.env_llamaindex&lt;/code&gt; is used for LLM RAG with &lt;code&gt;llamaindex&lt;/code&gt;, &lt;code&gt;vllamaindex&lt;/code&gt; and &lt;code&gt;vprocessor&lt;/code&gt; agents, &lt;code&gt;.env_haystack&lt;/code&gt; is used for LLM RAG with &lt;code&gt;haystack&lt;/code&gt; agent, and &lt;code&gt;.env_instructor&lt;/code&gt; is used for LLM function calling with &lt;code&gt;fcall&lt;/code&gt; agent. &lt;code&gt;.env_unstructured&lt;/code&gt; is used for &lt;code&gt;unstructured-light&lt;/code&gt; and &lt;code&gt;unstructured&lt;/code&gt; agents.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create virtual environment in &lt;code&gt;sparrow-data/ocr&lt;/code&gt; folder:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv .env_ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Activate Virtual Environments and Install Dependencies&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Activate each environment and install its dependencies using the corresponding &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;code&gt;llamaindex&lt;/code&gt; environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Activate the environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;source .env_llamaindex/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements_llamaindex.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Repeat the same for &lt;code&gt;haystack&lt;/code&gt;, &lt;code&gt;instructor&lt;/code&gt; and &lt;code&gt;unstructured&lt;/code&gt; environments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Run Sparrow&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can run Sparrow on CLI or through API. To run on CLI, use &lt;code&gt;sparrow.sh&lt;/code&gt; script. Run it from corresponding virtual environment, depending which agent you want to execute.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install &lt;a href=&#34;https://ollama.ai&#34;&gt;Ollama&lt;/a&gt; and pull LLM model specified in config.yml&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OCR&lt;/h3&gt; &#xA;&lt;p&gt;Follow the install steps outlined here:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Sparrow OCR services &lt;a href=&#34;https://github.com/katanaml/sparrow/tree/main/sparrow-data/ocr&#34;&gt;install steps&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Copy text PDF files to the &lt;code&gt;data&lt;/code&gt; folder or use sample data provided in the &lt;code&gt;data&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Ingest&lt;/h3&gt; &#xA;&lt;p&gt;This step is required for &lt;code&gt;llamaindex&lt;/code&gt; or &lt;code&gt;haystack&lt;/code&gt; agents only.&lt;/p&gt; &#xA;&lt;p&gt;Run the script, to convert text to vector embeddings and save in Weaviate. By default it will use &lt;code&gt;llamaindex&lt;/code&gt; agent. Example with &lt;code&gt;llamaindex&lt;/code&gt; agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh ingest --file-path /data/invoice_1.pdf --agent llamaindex --index-name Sparrow_llamaindex_doc1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example with &lt;code&gt;haystack&lt;/code&gt; agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh ingest --file-path /data/invoice_1.pdf --agent haystack --index-name Sparrow_haystack_doc1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Run the script, to process data with LLM RAG and return the answer. By default, it will use &lt;code&gt;llamaindex&lt;/code&gt; agent. You can specify other agents (see ingest example), such as &lt;code&gt;haystack&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,&#xA;seller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#34; &#34;int, str, str, str, str,&#xA;str, str, str, str, List[str], List[float], str&#34; --agent llamaindex --index-name Sparrow_llamaindex_doc1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Answer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;invoice_number&#34;: 61356291,&#xA;    &#34;invoice_date&#34;: &#34;09/06/2012&#34;,&#xA;    &#34;client_name&#34;: &#34;Rodriguez-Stevens&#34;,&#xA;    &#34;client_address&#34;: &#34;2280 Angela Plain, Hortonshire, MS 93248&#34;,&#xA;    &#34;client_tax_id&#34;: &#34;939-98-8477&#34;,&#xA;    &#34;seller_name&#34;: &#34;Chapman, Kim and Green&#34;,&#xA;    &#34;seller_address&#34;: &#34;64731 James Branch, Smithmouth, NC 26872&#34;,&#xA;    &#34;seller_tax_id&#34;: &#34;949-84-9105&#34;,&#xA;    &#34;iban&#34;: &#34;GB50ACIE59715038217063&#34;,&#xA;    &#34;names_of_invoice_items&#34;: [&#xA;        &#34;Wine Glasses Goblets Pair Clear Glass&#34;,&#xA;        &#34;With Hooks Stemware Storage Multiple Uses Iron Wine Rack Hanging Glass&#34;,&#xA;        &#34;Replacement Corkscrew Parts Spiral Worm Wine Opener Bottle Houdini&#34;,&#xA;        &#34;HOME ESSENTIALS GRADIENT STEMLESS WINE GLASSES SET OF 4 20 FL OZ (591 ml) NEW&#34;&#xA;    ],&#xA;    &#34;gross_worth_of_invoice_items&#34;: [&#xA;        66.0,&#xA;        123.55,&#xA;        8.25,&#xA;        14.29&#xA;    ],&#xA;    &#34;total_gross_worth&#34;: &#34;$212,09&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example with &lt;code&gt;haystack&lt;/code&gt; agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,&#xA;seller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#34; &#34;int, str, str, str, str,&#xA;str, str, str, str, List[str], List[float], str&#34; --agent haystack --index-name Sparrow_haystack_doc1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run multimodal agent, use &lt;code&gt;vllamaindex&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;guest_no, cashier_name&#34; &#34;int, str&#34; --agent vllamaindex --file-path /data/inout-20211211_001.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;code&gt;vprocessor&lt;/code&gt; agent to run OCR + LLM, this works best to process scanned docs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;guest_no, cashier_name, transaction_number, names_of_receipt_items, authorized_amount, receipt_date&#34; &#34;int, str, int, List[str], str, str&#34; --agent vprocessor --file-path /data/inout-20211211_001.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LLM function call example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh assistant --agent &#34;fcall&#34; --query &#34;Exxon&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Answer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;company&#34;: &#34;Exxon&#34;,&#xA;  &#34;ticker&#34;: &#34;XOM&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;The stock price of the Exxon is 111.2699966430664. USD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;code&gt;unstructured-light&lt;/code&gt; agent to run RAG pipeline with Unstructured library. It helps to improve data pre-processing for LLM. This agent supports PDF, JPG and PNG files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;invoice_number, invoice_date, total_gross_worth&#34; &#34;int, str, str&#34; --agent unstructured-light --file-path /data/invoice_1.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With &lt;code&gt;unstructured-light&lt;/code&gt; it is possible to specify option for table data processing only. This agent supports PDF, JPG and PNG files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#34; &#34;List[str], List[str], str&#34;&#xA;--agent unstructured-light --file-path /data/invoice_1.pdf --options tables&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;code&gt;unstructured&lt;/code&gt; agent to run RAG pipeline with Weaviate query (no separate step to ingest data is required) and Unstructured library. This agent supports PDF, JPG and PNG files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;invoice_number, invoice_date, total_gross_worth&#34; &#34;int, str, str&#34; --agent unstructured --file-path /data/invoice_1.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FastAPI Endpoint for Local LLM RAG&lt;/h2&gt; &#xA;&lt;p&gt;Sparrow enables you to run a local LLM RAG as an API using FastAPI, providing a convenient and efficient way to interact with our services. You can pass the name of the plugin to be used for the inference. By default, &lt;code&gt;llamaindex&lt;/code&gt; agent is used.&lt;/p&gt; &#xA;&lt;p&gt;To set this up:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start the Endpoint&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Launch the endpoint by executing the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run agents from different Python virtual environments simultaneously, you can specify port, to avoid conflicts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python api.py --port 8001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Access the Endpoint Documentation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can view detailed documentation for the API by navigating to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/api/v1/sparrow-llm/docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For visual reference, a screenshot of the FastAPI endpoint&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/assets/lemming_2_.png&#34; alt=&#34;FastAPI endpoint&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;API calls:&lt;/p&gt; &#xA;&lt;h3&gt;Ingest&lt;/h3&gt; &#xA;&lt;p&gt;Ingest call with &lt;code&gt;llamaindex&lt;/code&gt; agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/ingest&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;agent=llamaindex&#39; \&#xA;  -F &#39;index_name=Sparrow_llamaindex_doc2&#39; \&#xA;  -F &#39;file=@invoice_1.pdf;type=application/pdf&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ingest call with &lt;code&gt;haystack&lt;/code&gt; agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/ingest&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;agent=haystack&#39; \&#xA;  -F &#39;index_name=Sparrow_haystack_doc2&#39; \&#xA;  -F &#39;file=@invoice_1.pdf;type=application/pdf&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Inference call with &lt;code&gt;llamaindex&lt;/code&gt; agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;fields=invoice_number&#39; \&#xA;  -F &#39;types=int&#39; \&#xA;  -F &#39;agent=llamaindex&#39; \&#xA;  -F &#39;index_name=Sparrow_llamaindex_doc2&#39; \&#xA;  -F &#39;file=&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference call with &lt;code&gt;haystack&lt;/code&gt; agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;fields=invoice_number&#39; \&#xA;  -F &#39;types=int&#39; \&#xA;  -F &#39;agent=haystack&#39; \&#xA;  -F &#39;index_name=Sparrow_haystack_doc2&#39; \&#xA;  -F &#39;file=&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference call with multimodal agent &lt;code&gt;vllamaindex&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;fields=guest_no, cashier_name&#39; \&#xA;  -F &#39;types=int, str&#39; \&#xA;  -F &#39;agent=vllamaindex&#39; \&#xA;  -F &#39;index_name=&#39; \&#xA;  -F &#39;file=@inout-20211211_001.jpg;type=image/jpeg&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference call with OCR + LLM agent &lt;code&gt;vprocessor&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;fields=guest_no, cashier_name, transaction_number, names_of_receipt_items, authorized_amount, receipt_date&#39; \&#xA;  -F &#39;types=int, str, int, List[str], str, str&#39; \&#xA;  -F &#39;agent=vprocessor&#39; \&#xA;  -F &#39;index_name=&#39; \&#xA;  -F &#39;file=@inout-20211211_001.jpg;type=image/jpeg&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference call with &lt;code&gt;unstructured-light&lt;/code&gt; agent, this agent supports also JPG and PNG files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;fields=invoice_number, invoice_date, total_gross_worth&#39; \&#xA;  -F &#39;types=int, str, str&#39; \&#xA;  -F &#39;agent=unstructured-light&#39; \&#xA;  -F &#39;index_name=&#39; \&#xA;  -F &#39;options=&#39; \&#xA;  -F &#39;file=@invoice_1.pdf;type=application/pdf&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference call with &lt;code&gt;unstructured-light&lt;/code&gt; agent, using only tables option. This agent supports also JPG and PNG files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;fields=names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#39; \&#xA;  -F &#39;types=List[str], List[str], str&#39; \&#xA;  -F &#39;agent=unstructured-light&#39; \&#xA;  -F &#39;index_name=&#39; \&#xA;  -F &#39;options=tables&#39; \&#xA;  -F &#39;file=@invoice_1.pdf;type=application/pdf&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference call with &lt;code&gt;unstructured&lt;/code&gt; agent, this agent supports also JPG and PNG files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -X &#39;POST&#39; \&#xA;  &#39;http://127.0.0.1:8000/api/v1/sparrow-llm/inference&#39; \&#xA;  -H &#39;accept: application/json&#39; \&#xA;  -H &#39;Content-Type: multipart/form-data&#39; \&#xA;  -F &#39;fields=names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#39; \&#xA;  -F &#39;types=List[str], List[str], str&#39; \&#xA;  -F &#39;agent=unstructured&#39; \&#xA;  -F &#39;index_name=&#39; \&#xA;  -F &#39;options=&#39; \&#xA;  -F &#39;file=@invoice_1.pdf;type=application/pdf&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Inference with local LLM RAG&lt;/h3&gt; &#xA;&lt;p&gt;Document:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/assets/invoice_1.jpg&#34; alt=&#34;document RAG&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;invoice_number, invoice_date, client_name, client_address, client_tax_id, seller_name, seller_address,&#xA;seller_tax_id, iban, names_of_invoice_items, gross_worth_of_invoice_items, total_gross_worth&#34; &#34;int, str, str, str, str,&#xA;str, str, str, str, List[str], List[float], str&#34; --agent llamaindex --index-name Sparrow_llamaindex_doc1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;invoice_number&#34;: 61356291,&#xA;    &#34;invoice_date&#34;: &#34;09/06/2012&#34;,&#xA;    &#34;client_name&#34;: &#34;Rodriguez-Stevens&#34;,&#xA;    &#34;client_address&#34;: &#34;2280 Angela Plain, Hortonshire, MS 93248&#34;,&#xA;    &#34;client_tax_id&#34;: &#34;939-98-8477&#34;,&#xA;    &#34;seller_name&#34;: &#34;Chapman, Kim and Green&#34;,&#xA;    &#34;seller_address&#34;: &#34;64731 James Branch, Smithmouth, NC 26872&#34;,&#xA;    &#34;seller_tax_id&#34;: &#34;949-84-9105&#34;,&#xA;    &#34;iban&#34;: &#34;GB50ACIE59715038217063&#34;,&#xA;    &#34;names_of_invoice_items&#34;: [&#xA;        &#34;Wine Glasses Goblets Pair Clear Glass&#34;,&#xA;        &#34;With Hooks Stemware Storage Multiple Uses Iron Wine Rack Hanging Glass&#34;,&#xA;        &#34;Replacement Corkscrew Parts Spiral Worm Wine Opener Bottle Houdini&#34;,&#xA;        &#34;HOME ESSENTIALS GRADIENT STEMLESS WINE GLASSES SET OF 4 20 FL OZ (591 ml) NEW&#34;&#xA;    ],&#xA;    &#34;gross_worth_of_invoice_items&#34;: [&#xA;        66.0,&#xA;        123.55,&#xA;        8.25,&#xA;        14.29&#xA;    ],&#xA;    &#34;total_gross_worth&#34;: &#34;$212,09&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference with OCR + local LLM RAG&lt;/h3&gt; &#xA;&lt;p&gt;Document:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/katanaml/sparrow/raw/main/sparrow-ui/assets/ross-20211211_010.jpg&#34; alt=&#34;document OCR RAG&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./sparrow.sh &#34;store_name, receipt_id, receipt_item_names, receipt_item_prices, receipt_date, receipt_store_id,&#xA;receipt_sold, receipt_returned, receipt_total&#34; &#34;str, str, List[str], List[str], str, int, int,&#xA;int, str&#34; --agent vprocessor --file-path /data/ross-20211211_010.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;store_name&#34;: &#34;Ross&#34;,&#xA;    &#34;receipt_id&#34;: &#34;Receipt # 0421-01-1602-1330-0&#34;,&#xA;    &#34;receipt_item_names&#34;: [&#xA;        &#34;400226513665 x hanes b1ue 4pk&#34;,&#xA;        &#34;400239602790 fruit premium 4pk&#34;&#xA;    ],&#xA;    &#34;receipt_item_prices&#34;: [&#xA;        &#34;$9.99R&#34;,&#xA;        &#34;$12.99R&#34;&#xA;    ],&#xA;    &#34;receipt_date&#34;: &#34;11/26/21 10:35:05 AM&#34;,&#xA;    &#34;receipt_store_id&#34;: 421,&#xA;    &#34;receipt_sold&#34;: 2,&#xA;    &#34;receipt_returned&#34;: 0,&#xA;    &#34;receipt_total&#34;: &#34;$25.33&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Commercial usage&lt;/h2&gt; &#xA;&lt;p&gt;Sparrow is available under the GPL 3.0 license, promoting freedom to use, modify, and distribute the software while ensuring any modifications remain open source under the same license. This aligns with our commitment to supporting the open-source community and fostering collaboration.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, we recognize the diverse needs of organizations, including small to medium-sized enterprises (SMEs). Therefore, Sparrow is also offered for free commercial use to organizations with gross revenue below $5 million USD in the past 12 months, enabling them to leverage Sparrow without the financial burden often associated with high-quality software solutions.&lt;/p&gt; &#xA;&lt;p&gt;For businesses that exceed this revenue threshold or require usage terms not accommodated by the GPL 3.0 licenseâ€”such as integrating Sparrow into proprietary software without the obligation to disclose source code modificationsâ€”we offer dual licensing options. Dual licensing allows Sparrow to be used under a separate proprietary license, offering greater flexibility for commercial applications and proprietary integrations. This model supports both the project&#39;s sustainability and the business&#39;s needs for confidentiality and customization.&lt;/p&gt; &#xA;&lt;p&gt;If your organization is seeking to utilize Sparrow under a proprietary license, or if you are interested in custom workflows, consulting services, or dedicated support and maintenance options, please contact us at &lt;a href=&#34;mailto:abaranovskis@redsamuraiconsulting.com&#34;&gt;abaranovskis@redsamuraiconsulting.com&lt;/a&gt;. We&#39;re here to provide tailored solutions that meet your unique requirements, ensuring you can maximize the benefits of Sparrow for your projects and workflows.&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://katanaml.io&#34;&gt;Katana ML&lt;/a&gt;, &lt;a href=&#34;https://github.com/abaranovskis-redsamurai&#34;&gt;Andrej Baranovskij&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the GPL 3.0. Copyright 2020-2024 Katana ML, Andrej Baranovskij. &lt;a href=&#34;https://github.com/katanaml/sparrow/raw/main/LICENSE&#34;&gt;Copy of the license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>