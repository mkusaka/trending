<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-12T01:35:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AlaaLab/InstructCV</title>
    <updated>2023-10-12T01:35:42Z</updated>
    <id>tag:github.com,2023-10-12:/AlaaLab/InstructCV</id>
    <link href="https://github.com/AlaaLab/InstructCV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Codebase for &#34;InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists&#34;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists&lt;/h2&gt; &#xA;&lt;p&gt;Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis and Ahmed Alaa&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.00390&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/alaa-lab/InstructCV&#34;&gt;HuggingFace ðŸ¤— Demo&lt;/a&gt;&lt;/strong&gt; | &lt;a href=&#34;https://replicate.com/cjwbw/instructcv&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/instructcv/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸŒŸ Official PyTorch implementation of &lt;strong&gt;InstructCV&lt;/strong&gt;. The master branch works with &lt;strong&gt;PyTorch 1.5+&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AlaaLab/InstructCV/assets/21158134/db6ec741-a8ee-4c92-b0c3-0723ef800ffd&#34;&gt;https://github.com/AlaaLab/InstructCV/assets/21158134/db6ec741-a8ee-4c92-b0c3-0723ef800ffd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this project, we develop a unified language interface for computer vision tasks that abstracts away task specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/AlaaLab/InstructCV/assets/21158134/e74b059f-a5b2-49d2-a871-c92b668220f4&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Set up the environments&lt;/h2&gt; &#xA;&lt;p&gt;Install dependencies by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#Step0. Set up the env.&#xA;conda env create -f environment.yaml&#xA;conda activate lvi&#xA;#Step1 (optional) . You could ignore this step if you do not run the baselines.&#xA;## install tensorflow : https://www.tensorflow.org/install/pip&#xA;pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI &#xA;pip install -U openmim&#xA;mim install mmcv-full&#xA;git clone https://github.com/open-mmlab/mmdetection.git&#xA;cd mmdetection&#xA;pip install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/AlaaLab/InstructCV/main/DATASET.md&#34;&gt;Preparing Datasets for InstructCV&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/AlaaLab/InstructCV/main/GETTING_STARTED.md&#34;&gt;Getting Started with InstructCV&lt;/a&gt; for detailed instructions on training and inference with InstructCV.&lt;/p&gt; &#xA;&lt;h2&gt;InstructCV-RP checkpoint&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;text-align:center&#34; colspan=&#34;2&#34;&gt;Depth &lt;br&gt;Estimation &lt;br&gt;RMSEâ¬‡&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;text-align:center&#34; colspan=&#34;2&#34;&gt;Semantic Segmentation mIoUâ¬†&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;text-align:center&#34; colspan=&#34;2&#34;&gt;Classification &lt;br&gt;Accâ¬†&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;text-align:center&#34; colspan=&#34;2&#34;&gt;Object Detection mAPâ¬†&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;text-align:center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NYUv2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SUNRGB-D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ADE-20K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VOC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Oxford-Pets&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ImageNet-sub&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VOC&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AlaaLab/InstructCV/main/configs/Panoptic/odise_label_coco_50e.py&#34;&gt; InstructCV-RP &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.297&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.279&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.235&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.125&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.135&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.665&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.500&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.700&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/&#34;&gt; checkpoint &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ðŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the web demo: &lt;a href=&#34;https://huggingface.co/spaces/alaa-lab/InstructCV&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the demo on Google Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1YDI2kb6uPP1d1VsiarFDapufRtkYso4g&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The pre-trained model for Stable Diffusion is subject to its original license terms from &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is largely based on &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;CompVis/stable_diffusion&lt;/a&gt; and &lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;Instruct Pix2Pix&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BiBTeX&#34;&gt;@article{gan2023instructcv,&#xA;  title={InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists},&#xA;  author={Gan, Yulu and Park, Sungwoo and Schubert, Alexander and Philippakis, Anthony and Alaa, Ahmed},&#xA;  journal={arXiv preprint arXiv:2310.00390},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>eric-ai-lab/MiniGPT-5</title>
    <updated>2023-10-12T01:35:42Z</updated>
    <id>tag:github.com,2023-10-12:/eric-ai-lab/MiniGPT-5</id>
    <link href="https://github.com/eric-ai-lab/MiniGPT-5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of paper &#34;MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MiniGPT-5&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://kzzheng.github.io/&#34;&gt;Kaizhi Zheng&lt;/a&gt;* , &lt;a href=&#34;https://scholar.google.com/citations?user=kDzxOzUAAAAJ&amp;amp;hl=en&#34;&gt;Xuehai He&lt;/a&gt;* , &lt;a href=&#34;https://eric-xw.github.io/&#34;&gt;Xin Eric Wang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.02239&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official implementation of paper &#34;MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/figs/teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation. Yet, the simultaneous generation of images with coherent textual narratives remains an evolving frontier. In response, we introduce an innovative interleaved vision-and-language generation technique anchored by the concept of ``generative vokens&#34;, acting as the bridge for harmonized image-text outputs. Our approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. Our model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs in human evaluations on the VIST dataset, highlighting its efficacy across diverse benchmarks.&lt;/p&gt; &#xA;&lt;h2&gt;Model Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/figs/structure.png&#34; alt=&#34;arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Download repo and create environment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clone our repo and create a new python environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/eric-ai-lab/MiniGPT-5.git&#xA;cd MiniGPT-5&#xA;conda create -n minigpt5 python=3.9&#xA;conda activate minigpt5&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare the pretrained weights&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our model is based on the pretrained &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt;. Please download &lt;a href=&#34;https://huggingface.co/Vision-CAIR/vicuna-7b/tree/main&#34;&gt;Vicuna V0 7B&lt;/a&gt; weights. Then, set the path to the vicuna weight in the &lt;a href=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/minigpt4/configs/models/minigpt4.yaml#L16&#34;&gt;model config file&lt;/a&gt; at Line 16.&lt;/p&gt; &#xA;&lt;p&gt;Since the Pretrained MiniGPT-4 Aligned Checkpoint is small, we already download in config folder, and the model path is set in &lt;a href=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/config/minigpt4.yaml#10&#34;&gt;config file&lt;/a&gt; at Line 10.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Download MiniGPT-5 Checkpoint&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Since our model is trained with two stages &lt;strong&gt;(Stage 1: Unimodal Alignment Stage, Stage 2: Multimodal Learning Stage)&lt;/strong&gt;, we provide both two-stage checkpoints here:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Stage 1: CC3M&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Stage 2: VIST&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Stage 2: MMDialog&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1y-VUXubIzFe0iq5_CJUaE3HKhlrdn4n2/view?usp=sharing&#34;&gt;Downlad&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1rjTsKwF8_pqcNLbdZdurqZLSpKoo2K9F/view?usp=drive_link&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ehyX8Ykn1pbU5J8yM47catSswId0m5FZ/view?usp=drive_link&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Stage 2 needs the pretrained weights in Stage 1, so always download Stage 1 weights first.&lt;/p&gt; &#xA;&lt;p&gt;Please download these weights into a single folder, and we will call this folder as &lt;em&gt;&lt;strong&gt;WEIGHT_FOLDER&lt;/strong&gt;&lt;/em&gt; in the following sections.&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/examples/playground.py&#34;&gt;python file&lt;/a&gt; to try our model. This file will generate multimodal outputs under the &lt;a href=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/examples/&#34;&gt;example folder&lt;/a&gt; by taking a two-turn multimodal inputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd examples&#xA;export IS_STAGE2=True&#xA;python3 playground.py --stage1_weight WEIGHT_FOLDER/stage1_cc3m.ckpt &#xA;                        --test_weight WEIGHT_FOLDER/stage2_vist.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Our model evaluate on three datasets: &lt;a href=&#34;https://ai.google.com/research/ConceptualCaptions/download&#34;&gt;CC3M&lt;/a&gt;, &lt;a href=&#34;https://visionandlanguage.net/VIST/&#34;&gt;VIST&lt;/a&gt;, and &lt;a href=&#34;https://github.com/victorsungo/MMDialog&#34;&gt;MMDialog&lt;/a&gt;. Due to the license, we only share some dataset examples under the &lt;a href=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/datasets/&#34;&gt;datasets&lt;/a&gt; folder. If you want to fully test the performance, please download the full dataset and format into the same data structures under the &lt;a href=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/datasets/&#34;&gt;datasets&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Stage 1: Unimodal Alignment Stage (CC3M) evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;During this stage, the goal is to generate correct images by giving image descriptions.&lt;/p&gt; &#xA;&lt;p&gt;Generation (If you have more than one gpus, you can set gpus to 0,1,2...):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export IS_STAGE2=False&#xA;export WEIGHTFOLDER=WEIGHT_FOLDER&#xA;export DATAFOLDER=datasets/CC3M&#xA;export OUTPUT_FOLDER=outputs&#xA;python3 train_eval.py --test_data_path cc3m_val.tsv &#xA;                        --test_weight stage1_cc3m.ckpt&#xA;                        --gpus 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Calculate Metric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CC3M_FOLDER=datasets/CC3M&#xA;python3 metric.py --test_weight stage1_cc3m.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Stage 2: Multimodal Learning Stage (VIST) evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Model will take the previous multimodal story sequences and generate either unimodal or multimodal outputs. Here, the default code is about multimodal input &amp;amp; image generation. To test other settings, please remove the &lt;em&gt;not test&lt;/em&gt; condition in &lt;a href=&#34;https://raw.githubusercontent.com/eric-ai-lab/MiniGPT-5/main/dataloader.py#280&#34;&gt;Line 280&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Generation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export IS_STAGE2=True&#xA;export WEIGHTFOLDER=WEIGHT_FOLDER&#xA;export DATAFOLDER=datasets/VIST&#xA;export OUTPUT_FOLDER=outputs&#xA;python3 train_eval.py --test_data_path val_cleaned.json &#xA;                        --test_weight stage2_vist.ckpt&#xA;                        --stage1_weight stage1_cc3m.ckpt&#xA;                        --gpus 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Calculate Metric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 metric.py --test_weight stage2_vist.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Stage 2: Multimodal Learning Stage (MMDialog) evaluation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Model will take previous turn multimodal inputs and generate multimodal response for multimodal conversations.&lt;/p&gt; &#xA;&lt;p&gt;Generation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export IS_STAGE2=True&#xA;export WEIGHTFOLDER=WEIGHT_FOLDER&#xA;export DATAFOLDER=datasets/MMDialog&#xA;export OUTPUT_FOLDER=outputs&#xA;python3 train_eval.py --test_data_path test/test_conversations.txt &#xA;                        --test_weight stage2_mmdialog.ckpt&#xA;                        --stage1_weight stage1_cc3m.ckpt&#xA;                        --gpus 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Calculate Metric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 metric.py --test_weight stage2_mmdialog.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Stage 1 training&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the CC3M dataset and format into the same data structure in dataset folder.&lt;/p&gt; &#xA;&lt;p&gt;Then, we use test data as example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export IS_STAGE2=False&#xA;export WEIGHTFOLDER=WEIGHT_FOLDER&#xA;export DATAFOLDER=datasets/CC3M&#xA;python3 train_eval.py --is_training True&#xA;                        --train_data_path cc3m_val.tsv&#xA;                        --val_data_path cc3m_val.tsv&#xA;                        --model_save_name stage1_cc3m_{epoch}-{step}&#xA;                        --gpus 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Stage 2 training&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download the VIST or MMDialog datasets and format into the same data structure in dataset folder.&lt;/p&gt; &#xA;&lt;p&gt;Here we use VIST test data as example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export IS_STAGE2=True&#xA;export WEIGHTFOLDER=WEIGHT_FOLDER&#xA;export DATAFOLDER=datasets/VIST&#xA;python3 train_eval.py --is_training True&#xA;                        --train_data_path val_cleaned.json&#xA;                        --val_data_path val_cleaned.json&#xA;                        --stage1_weight stage1_cc3m.ckpt&#xA;                        --model_save_name stage2_vist_{epoch}-{step}&#xA;                        --gpus 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re using MiniGPT-5 in your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zheng2023minigpt5,&#xA;      title={MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens}, &#xA;      author={Kaizhi Zheng and Xuehai He and Xin Eric Wang},&#xA;      year={2023},&#xA;      journal={arXiv preprint arXiv:2310.02239}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>recommenders-team/recommenders</title>
    <updated>2023-10-12T01:35:42Z</updated>
    <id>tag:github.com,2023-10-12:/recommenders-team/recommenders</id>
    <link href="https://github.com/recommenders-team/recommenders" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Best Practices on Recommendation Systems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Recommenders&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://microsoft-recommenders.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/microsoft-recommenders/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg?sanitize=true&#34; width=&#34;800&#34;&gt; &#xA;&lt;h2&gt;What&#39;s New (October, 2023)&lt;/h2&gt; &#xA;&lt;p&gt;We are pleased to announce that this repository (formerly known as Microsoft Recommenders, &lt;a href=&#34;https://github.com/microsoft/recommenders&#34;&gt;https://github.com/microsoft/recommenders&lt;/a&gt;), has joined the &lt;a href=&#34;https://lfaidata.foundation/&#34;&gt;Linux Foundation of AI and Data&lt;/a&gt; (LF AI &amp;amp; Data)! The new organization, &lt;code&gt;recommenders-team&lt;/code&gt;, reflects this change.&lt;/p&gt; &#xA;&lt;p&gt;We hope this move makes it easy for anyone to contribute! Our objective continues to be building an ecosystem and a community to sustain open source innovations and collaborations in recommendation systems.&lt;/p&gt; &#xA;&lt;p&gt;Now to access the repo, instead of going to &lt;a href=&#34;https://github.com/microsoft/recommenders&#34;&gt;https://github.com/microsoft/recommenders&lt;/a&gt;, you need to go to &lt;a href=&#34;https://github.com/recommenders-team/recommenders&#34;&gt;https://github.com/recommenders-team/recommenders&lt;/a&gt;. The old URL will still resolve to the new one, but we recommend that you update your bookmarks.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Recommenders objective is to assist researchers, developers and enthusiasts in prototyping, experimenting with and bringing to production a range of classic and state-of-the-art recommendation systems.&lt;/p&gt; &#xA;&lt;p&gt;Recommenders is a project under the &lt;a href=&#34;https://lfaidata.foundation/projects/&#34;&gt;Linux Foundation of AI and Data&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail our learnings on five key tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/01_prepare_data&#34;&gt;Prepare Data&lt;/a&gt;: Preparing and loading data for each recommender algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start&#34;&gt;Model&lt;/a&gt;: Building models using various classical and deep learning recommender algorithms such as Alternating Least Squares (&lt;a href=&#34;https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS&#34;&gt;ALS&lt;/a&gt;) or eXtreme Deep Factorization Machines (&lt;a href=&#34;https://arxiv.org/abs/1803.05170&#34;&gt;xDeepFM&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/03_evaluate&#34;&gt;Evaluate&lt;/a&gt;: Evaluating algorithms with offline metrics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/04_model_select_and_optimize&#34;&gt;Model Select and Optimize&lt;/a&gt;: Tuning and optimizing hyperparameters for recommender models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/05_operationalize&#34;&gt;Operationalize&lt;/a&gt;: Operationalizing models in a production environment on Azure.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Several utilities are provided in &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/recommenders&#34;&gt;recommenders&lt;/a&gt; to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the &lt;a href=&#34;https://readthedocs.org/projects/microsoft-recommenders/&#34;&gt;Recommenders documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a more detailed overview of the repository, please see the documents on the &lt;a href=&#34;https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations&#34;&gt;wiki page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;We recommend &lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment&#34;&gt;conda&lt;/a&gt; for environment management, and &lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;VS Code&lt;/a&gt; for development. To install the recommenders package and run an example notebook on Linux/WSL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. Install gcc if it is not installed already. On Ubuntu, this could done by using the command&#xA;# sudo apt install gcc&#xA;&#xA;# 2. Create and activate a new conda environment&#xA;conda create -n &amp;lt;environment_name&amp;gt; python=3.9&#xA;conda activate &amp;lt;environment_name&amp;gt;&#xA;&#xA;# 3. Install the core recommenders package. It can run all the CPU notebooks.&#xA;pip install recommenders&#xA;&#xA;# 4. create a Jupyter kernel&#xA;python -m ipykernel install --user --name &amp;lt;environment_name&amp;gt; --display-name &amp;lt;kernel_name&amp;gt;&#xA;&#xA;# 5. Clone this repo within VSCode or using command line:&#xA;git clone https://github.com/recommenders-team/recommenders.git&#xA;&#xA;# 6. Within VSCode:&#xA;#   a. Open a notebook, e.g., examples/00_quick_start/sar_movielens.ipynb;  &#xA;#   b. Select Jupyter kernel &amp;lt;kernel_name&amp;gt;;&#xA;#   c. Run the notebook.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about setup on other platforms (e.g., Windows and macOS) and different configurations (e.g., GPU, Spark and experimental features), see the &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/SETUP.md&#34;&gt;Setup Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the core package, several extras are also provided, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[gpu]&lt;/code&gt;: Needed for running GPU models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[spark]&lt;/code&gt;: Needed for running Spark models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[dev]&lt;/code&gt;: Needed for development for the repo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt;: &lt;code&gt;[gpu]&lt;/code&gt;|&lt;code&gt;[spark]&lt;/code&gt;|&lt;code&gt;[dev]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[experimental]&lt;/code&gt;: Models that are not thoroughly tested and/or may require additional steps in installation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Algorithms&lt;/h2&gt; &#xA;&lt;p&gt;The table below lists the recommender algorithms currently available in the repository. Notebooks are linked under the Example column as Quick start, showcasing an easy to run example of the algorithm, or as Deep dive, explaining in detail the math and implementation of the algorithm.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Algorithm&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alternating Least Squares (ALS)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized for scalability and distributed computing capability. It works in the PySpark environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/als_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/als_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Attentive Asynchronous Singular Value Decomposition (A2SVD)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cornac/Bayesian Personalized Ranking (BPR)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Matrix factorization algorithm for predicting item ranking with implicit feedback. It works in the CPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cornac/Bilateral Variational Autoencoder (BiVAE)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Generative model for dyadic data (e.g., user-item interactions). It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Convolutional Sequence Embedding Recommendation (Caser)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Algorithm based on convolutions that aim to capture both userâ€™s general preferences and sequential patterns. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deep Knowledge-Aware Network (DKN)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Deep learning algorithm incorporating a knowledge graph and article embeddings for providing news or article recommendations. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/dkn_MIND.ipynb&#34;&gt;Quick start&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_content_based_filtering/dkn_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Extreme Deep Factorization Machine (xDeepFM)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid&lt;/td&gt; &#xA;   &lt;td&gt;Deep learning based algorithm for implicit and explicit feedback with user/item features. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/xdeepfm_criteo.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastAI Embedding Dot Bias (FAST)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;General purpose algorithm with embeddings and biases for users and items. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/fastai_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LightFM/Hybrid Matrix Factorization&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid matrix factorization algorithm for both implicit and explicit feedbacks. It works in the CPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_hybrid/lightfm_deep_dive.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LightGBM/Gradient Boosting Tree&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems. It works in the CPU/GPU/PySpark environments.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/lightgbm_tinycriteo.ipynb&#34;&gt;Quick start in CPU&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb&#34;&gt;Deep dive in PySpark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LightGCN&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Deep learning algorithm which simplifies the design of GCN for predicting implicit feedback. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GeoIMC&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid&lt;/td&gt; &#xA;   &lt;td&gt;Matrix completion algorithm that has into account user and item features using Riemannian conjugate gradients optimization and following a geometric approach. It works in the CPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/geoimc_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GRU&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Sequential-based algorithm that aims to capture both long and short-term user preferences using recurrent neural networks. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multinomial VAE&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Generative model for predicting user/item interactions. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Recommendation with Long- and Short-term User Representations (LSTUR)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/lstur_MIND.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Recommendation with Attentive Multi-View Learning (NAML)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/naml_MIND.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Collaborative Filtering (NCF)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/ncf_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Recommendation with Personalized Attention (NPA)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/npa_MIND.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Recommendation with Multi-Head Self-Attention (NRMS)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/nrms_MIND.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Next Item Recommendation (NextItNet)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Algorithm based on dilated convolutions and residual network that aims to capture sequential patterns. It considers both user/item interactions and features. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Restricted Boltzmann Machines (RBM)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Neural network based algorithm for learning the underlying probability distribution for explicit or implicit user/item feedback. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/rbm_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Riemannian Low-rank Matrix Completion (RLRMC)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption to predict user/item interactions. It works in the CPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/rlrmc_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Simple Algorithm for Recommendation (SAR)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Similarity-based algorithm for implicit user/item feedback. It works in the CPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sar_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/sar_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Self-Attentive Sequential Recommendation (SASRec)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Transformer based algorithm for sequential recommendation. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sasrec_amazon.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Short-term and Long-term Preference Integrated Recommender (SLi-Rec)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism, a time-aware controller and a content-aware controller. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-Interest-Aware Sequential User Modeling (SUM)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;An enhanced memory network-based sequential user model which aims to capture users&#39; multiple interests. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sequential Recommendation Via Personalized Transformer (SSEPT)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Transformer based algorithm for sequential recommendation with User embedding. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sasrec_amazon.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Standard VAE&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Generative Model for predicting user/item interactions. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Surprise/Singular Value Decomposition (SVD)&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Matrix factorization algorithm for predicting explicit rating feedback in small datasets. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Term Frequency - Inverse Document Frequency (TF-IDF)&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Simple similarity-based algorithm for content-based recommendations with text datasets. It works in the CPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/tfidf_covid.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vowpal Wabbit (VW)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Content-Based Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Fast online learning algorithms, great for scenarios where user features / context are constantly changing. It uses the CPU for online learning.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wide and Deep&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid&lt;/td&gt; &#xA;   &lt;td&gt;Deep learning algorithm that can memorize feature interactions and generalize user features. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/wide_deep_movielens.ipynb&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xLearn/Factorization Machine (FM) &amp;amp; Field-Aware FM (FFM)&lt;/td&gt; &#xA;   &lt;td&gt;Hybrid&lt;/td&gt; &#xA;   &lt;td&gt;Quick and memory efficient algorithm to predict labels with user/item features. It works in the CPU/GPU environment.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_hybrid/fm_deep_dive.ipynb&#34;&gt;Deep dive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;sup&gt;*&lt;/sup&gt; indicates algorithms invented/contributed by Microsoft.&lt;/p&gt; &#xA;&lt;p&gt;Independent or incubating algorithms and utilities are candidates for the &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/contrib&#34;&gt;contrib&lt;/a&gt; folder. This will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Algorithm&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SARplus &lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Collaborative Filtering&lt;/td&gt; &#xA;   &lt;td&gt;Optimized implementation of SAR for Spark&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/contrib/sarplus/README.md&#34;&gt;Quick start&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Algorithm Comparison&lt;/h3&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/06_benchmarks/movielens.ipynb&#34;&gt;benchmark notebook&lt;/a&gt; to illustrate how different algorithms could be evaluated and compared. In this notebook, the MovieLens dataset is split into training/test sets at a 75/25 ratio using a stratified split. A recommendation model is trained using each of the collaborative filtering algorithms below. We utilize empirical parameter values reported in literature &lt;a href=&#34;http://mymedialite.net/examples/datasets.html&#34;&gt;here&lt;/a&gt;. For ranking metrics we use &lt;code&gt;k=10&lt;/code&gt; (top 10 recommended items). We run the comparison on a Standard NC6s_v2 &lt;a href=&#34;https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/&#34;&gt;Azure DSVM&lt;/a&gt; (6 vCPUs, 112 GB memory and 1 P100 GPU). Spark ALS is run in local standalone mode. In this table we show the results on Movielens 100k, running the algorithms for 15 epochs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Algo&lt;/th&gt; &#xA;   &lt;th&gt;MAP&lt;/th&gt; &#xA;   &lt;th&gt;nDCG@k&lt;/th&gt; &#xA;   &lt;th&gt;Precision@k&lt;/th&gt; &#xA;   &lt;th&gt;Recall@k&lt;/th&gt; &#xA;   &lt;th&gt;RMSE&lt;/th&gt; &#xA;   &lt;th&gt;MAE&lt;/th&gt; &#xA;   &lt;th&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Explained Variance&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/als_movielens.ipynb&#34;&gt;ALS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.004732&lt;/td&gt; &#xA;   &lt;td&gt;0.044239&lt;/td&gt; &#xA;   &lt;td&gt;0.048462&lt;/td&gt; &#xA;   &lt;td&gt;0.017796&lt;/td&gt; &#xA;   &lt;td&gt;0.965038&lt;/td&gt; &#xA;   &lt;td&gt;0.753001&lt;/td&gt; &#xA;   &lt;td&gt;0.255647&lt;/td&gt; &#xA;   &lt;td&gt;0.251648&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb&#34;&gt;BiVAE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.146126&lt;/td&gt; &#xA;   &lt;td&gt;0.475077&lt;/td&gt; &#xA;   &lt;td&gt;0.411771&lt;/td&gt; &#xA;   &lt;td&gt;0.219145&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb&#34;&gt;BPR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.132478&lt;/td&gt; &#xA;   &lt;td&gt;0.441997&lt;/td&gt; &#xA;   &lt;td&gt;0.388229&lt;/td&gt; &#xA;   &lt;td&gt;0.212522&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/fastai_movielens.ipynb&#34;&gt;FastAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.025503&lt;/td&gt; &#xA;   &lt;td&gt;0.147866&lt;/td&gt; &#xA;   &lt;td&gt;0.130329&lt;/td&gt; &#xA;   &lt;td&gt;0.053824&lt;/td&gt; &#xA;   &lt;td&gt;0.943084&lt;/td&gt; &#xA;   &lt;td&gt;0.744337&lt;/td&gt; &#xA;   &lt;td&gt;0.285308&lt;/td&gt; &#xA;   &lt;td&gt;0.287671&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb&#34;&gt;LightGCN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.088526&lt;/td&gt; &#xA;   &lt;td&gt;0.419846&lt;/td&gt; &#xA;   &lt;td&gt;0.379626&lt;/td&gt; &#xA;   &lt;td&gt;0.144336&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_hybrid/ncf_deep_dive.ipynb&#34;&gt;NCF&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.107720&lt;/td&gt; &#xA;   &lt;td&gt;0.396118&lt;/td&gt; &#xA;   &lt;td&gt;0.347296&lt;/td&gt; &#xA;   &lt;td&gt;0.180775&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sar_movielens.ipynb&#34;&gt;SAR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.110591&lt;/td&gt; &#xA;   &lt;td&gt;0.382461&lt;/td&gt; &#xA;   &lt;td&gt;0.330753&lt;/td&gt; &#xA;   &lt;td&gt;0.176385&lt;/td&gt; &#xA;   &lt;td&gt;1.253805&lt;/td&gt; &#xA;   &lt;td&gt;1.048484&lt;/td&gt; &#xA;   &lt;td&gt;-0.569363&lt;/td&gt; &#xA;   &lt;td&gt;0.030474&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb&#34;&gt;SVD&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.012873&lt;/td&gt; &#xA;   &lt;td&gt;0.095930&lt;/td&gt; &#xA;   &lt;td&gt;0.091198&lt;/td&gt; &#xA;   &lt;td&gt;0.032783&lt;/td&gt; &#xA;   &lt;td&gt;0.938681&lt;/td&gt; &#xA;   &lt;td&gt;0.742690&lt;/td&gt; &#xA;   &lt;td&gt;0.291967&lt;/td&gt; &#xA;   &lt;td&gt;0.291971&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Before contributing, please see our &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project adheres to &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/CODE_OF_CONDUCT.md&#34;&gt;Microsoft&#39;s Open Source Code of Conduct&lt;/a&gt; in order to foster a welcoming and inspiring community for all.&lt;/p&gt; &#xA;&lt;h2&gt;Build Status&lt;/h2&gt; &#xA;&lt;p&gt;These tests are the nightly builds, which compute the asynchronous tests. &lt;code&gt;main&lt;/code&gt; is our principal branch and &lt;code&gt;staging&lt;/code&gt; is our development branch. We use &lt;a href=&#34;https://docs.pytest.org/&#34;&gt;pytest&lt;/a&gt; for testing python utilities in &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/recommenders&#34;&gt;recommenders&lt;/a&gt; and &lt;a href=&#34;https://github.com/nteract/papermill&#34;&gt;Papermill&lt;/a&gt; and &lt;a href=&#34;https://nteract-scrapbook.readthedocs.io/en/latest/&#34;&gt;Scrapbook&lt;/a&gt; for the &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples&#34;&gt;notebooks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more information about the testing pipelines, please see the &lt;a href=&#34;https://raw.githubusercontent.com/recommenders-team/recommenders/main/tests/README.md&#34;&gt;test documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;AzureML Nightly Build Status&lt;/h3&gt; &#xA;&lt;p&gt;The nightly build tests are run daily on AzureML.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Build Type&lt;/th&gt; &#xA;   &lt;th&gt;Branch&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Branch&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;main&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=main&#34; alt=&#34;azureml-cpu-nightly&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;staging&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Astaging&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=staging&#34; alt=&#34;azureml-cpu-nightly&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;main&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=main&#34; alt=&#34;azureml-gpu-nightly&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;staging&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Astaging&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=staging&#34; alt=&#34;azureml-gpu-nightly&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Linux Spark&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;main&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=main&#34; alt=&#34;azureml-spark-nightly&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;staging&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Astaging&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=staging&#34; alt=&#34;azureml-spark-nightly&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;D. Li, J. Lian, L. Zhang, K. Ren, D. Lu, T. Wu, X. Xie, &#34;Recommender Systems: Frontiers and Practices&#34; (in Chinese), Publishing House of Electronics Industry, Beijing 2022.&lt;/li&gt; &#xA; &lt;li&gt;A. Argyriou, M. GonzÃ¡lez-Fierro, and L. Zhang, &#34;Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems&#34;, &lt;em&gt;WWW 2020: International World Wide Web Conference Taipei&lt;/em&gt;, 2020. Available online: &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3366424.3382692&#34;&gt;https://dl.acm.org/doi/abs/10.1145/3366424.3382692&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;L. Zhang, T. Wu, X. Xie, A. Argyriou, M. GonzÃ¡lez-Fierro and J. Lian, &#34;Building Production-Ready Recommendation System at Scale&#34;, &lt;em&gt;ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2019 (KDD 2019)&lt;/em&gt;, 2019.&lt;/li&gt; &#xA; &lt;li&gt;S. Graham, J.K. Min, T. Wu, &#34;Microsoft recommenders: tools to accelerate developing recommender systems&#34;, &lt;em&gt;RecSys &#39;19: Proceedings of the 13th ACM Conference on Recommender Systems&lt;/em&gt;, 2019. Available online: &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3298689.3346967&#34;&gt;https://dl.acm.org/doi/10.1145/3298689.3346967&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>