<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-02T01:36:19Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MsLolita/grass</title>
    <updated>2024-04-02T01:36:19Z</updated>
    <id>tag:github.com,2024-04-02:/MsLolita/grass</id>
    <link href="https://github.com/MsLolita/grass" rel="alternate"></link>
    <summary type="html">&lt;p&gt;grass-mining&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Grass Auto Reger&amp;amp;Farm üîπ&lt;/h1&gt; &#xA;&lt;p&gt;Discover the latest &lt;code&gt;&amp;lt;crypto/&amp;gt;&lt;/code&gt; moves in my Telegram Channel:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://t.me/web3_enjoyer_club&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Web3_Enjoyer_%7C_Subscribe_%F0%9F%A5%B0-0A66C2?style=for-the-badge&amp;amp;logo=telegram&amp;amp;logoColor=white&#34; alt=&#34;My Channel ü•∞&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cheapest proxies and servers which fits for grass &lt;a href=&#34;https://teletype.in/@web3enjoyer/4a2G9NuHssy&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/MsLolita/grass/assets/58307006/9bb115d3-18f0-4a7d-8450-b7463a0b33d9&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Bot can be used to create accounts and farm points on &lt;a href=&#34;https://app.getgrass.io/register/?referralCode=erxggzon61FWrJ9&#34;&gt;grass.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start üìö&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To install libraries on Windows click on &lt;code&gt;INSTALL.bat&lt;/code&gt; (or in console: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;To start bot use &lt;code&gt;START.bat&lt;/code&gt; (or in console: &lt;code&gt;python main.py&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Options üìß&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;CREATE ACCOUNTS:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;data/config.py&lt;/code&gt; put &lt;code&gt;REGISTER_ACCOUNT_ONLY = True&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Provide emails and passwords (OPTIONAL) and proxies to register accounts as below!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/MsLolita/grass/assets/58307006/67740c9b-07d6-4f78-a87d-27b09c0303e8&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;FARM POINTS:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;in &lt;code&gt;data/config.py&lt;/code&gt; put &lt;code&gt;REGISTER_ACCOUNT_ONLY = False&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Provide emails and passwords and proxies to register accounts as shown below!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Configuration üìß&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Accounts Setup üîí&lt;/p&gt; &lt;p&gt;Put in &lt;code&gt;data/accounts.txt&lt;/code&gt; accounts in format email:password (&lt;a href=&#34;mailto:cool_aster@gmail.com&#34;&gt;cool_aster@gmail.com&lt;/a&gt;:my_password123)&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/MsLolita/grass/assets/58307006/2f8bacaa-0212-49fe-b362-fe764230f47c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Proxy Setup üîí&lt;/p&gt; &lt;p&gt;Configure your proxies with the &lt;em&gt;ANY&lt;/em&gt; (socks, http/s, ...) format in &lt;code&gt;data/proxies.txt&lt;/code&gt; üåê&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/MsLolita/VeloData/assets/58307006/a2c95484-52b6-497a-b89e-73b89d953d8c&#34; alt=&#34;Proxy Configuration&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>brilliantlabsAR/noa-assistant</title>
    <updated>2024-04-02T01:36:19Z</updated>
    <id>tag:github.com,2024-04-02:/brilliantlabsAR/noa-assistant</id>
    <link href="https://github.com/brilliantlabsAR/noa-assistant" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Noa Assistant&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>Lightning-AI/litgpt</title>
    <updated>2024-04-02T01:36:19Z</updated>
    <id>tag:github.com,2024-04-02:/Lightning-AI/litgpt</id>
    <link href="https://github.com/Lightning-AI/litgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hackable implementation of state-of-the-art open-source LLMs based on nanoGPT. Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_lightning/LitStableLM_Badge.png&#34; alt=&#34;LitGPT&#34; width=&#34;128&#34;&gt; &#xA; &lt;h1&gt;‚ö° LitGPT&lt;/h1&gt; &#xA; &lt;!--&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;  &lt;a href=&#34;https://www.lightning.ai/&#34;&gt;Lightning.ai&lt;/a&gt; ‚Ä¢&#xA;  &lt;a href=&#34;https://lightning.ai/docs/pytorch/stable/&#34;&gt;PyTorch Lightning&lt;/a&gt; ‚Ä¢&#xA;  &lt;a href=&#34;https://lightning.ai/docs/fabric/stable/&#34;&gt;Fabric&lt;/a&gt;&#xA;&lt;/p&gt;&#xA;--&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pytorch-lightning&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://github.com/lightning-AI/lit-stablelm/actions/workflows/cpu-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;cpu-tests&#34;&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-stablelm/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1077906959069626439?style=plastic&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;‚ö° LitGPT is a hackable &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/litgpt/model.py&#34;&gt;implementation&lt;/a&gt; of state-of-the-art open-source large language models released under the &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;LitGPT supports&lt;/h2&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/download_model_weights.md&#34;&gt;The latest model weights&lt;/a&gt;: Gemma, Mistral, Mixtral, Phi 2, Llama 2, Falcon, CodeLlama, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/download_model_weights.md&#34;&gt;many more&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Optimized and efficient code: Flash Attention v2, multi-GPU support via fully-sharded data parallelism, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/oom.md#do-sharding-across-multiple-gpus&#34;&gt;optional CPU offloading&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/extensions/xla&#34;&gt;TPU and XLA support&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain_tinyllama.md&#34;&gt;Pretraining&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;finetuning&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/inference.md&#34;&gt;inference&lt;/a&gt; in various precision settings: FP32, FP16, BF16, and FP16/FP32 mixed.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub&#34;&gt;Configuration files&lt;/a&gt; for great out-of-the-box performance.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Efficient finetuning: &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_lora.md&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_lora.md&#34;&gt;QLoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_adapter.md&#34;&gt;Adapter&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_adapter.md&#34;&gt;Adapter v2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/quantize.md&#34;&gt;Quantization&lt;/a&gt;: 4-bit floats, 8-bit integers, and double quantization.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/convert_lit_models.md&#34;&gt;Exporting&lt;/a&gt; to other popular model weight formats.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Many popular datasets for &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain_tinyllama.md&#34;&gt;pretraining&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md&#34;&gt;finetuning&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md#preparing-custom-datasets-for-instruction-finetuning&#34;&gt;support for custom datasets&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Readable and easy-to-modify code to experiment with the latest research ideas.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp; &lt;br&gt; &amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Project templates&lt;/h2&gt; &#xA;&lt;p&gt;The following &lt;a href=&#34;https://lightning.ai/lightning-ai/studios&#34;&gt;Lightning Studio&lt;/a&gt; templates provide LitGPT tutorials and projects in reproducible environments with multi-GPU and multi-node support:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;p align=&#34;left&#34;&gt;&lt;a href=&#34;https://lightning.ai/lightning-ai/studios/prepare-the-tinyllama-1t-token-dataset&#34;&gt;Prepare the TinyLlama 1T token dataset&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://lightning.ai/lightning-ai/studios/prepare-the-tinyllama-1t-token-dataset&#34;&gt;&lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_litgpt/readme/3.webp&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightning.ai/lightning-ai/studios/pretrain-llms-tinyllama-1-1b&#34;&gt;Pretrain LLMs - TinyLlama 1.1B&lt;/a&gt; &lt;br&gt; &lt;p align=&#34;left&#34;&gt;&lt;a href=&#34;https://lightning.ai/lightning-ai/studios/pretrain-llms-tinyllama-1-1b&#34;&gt;&lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_litgpt/readme/4.webp&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightning.ai/lightning-ai/studios/continued-pretraining-with-tinyllama-1-1b&#34;&gt;Continued Pretraining with TinyLlama 1.1B&lt;/a&gt; &lt;br&gt; &lt;p align=&#34;left&#34;&gt;&lt;a href=&#34;https://lightning.ai/lightning-ai/studios/continued-pretraining-with-tinyllama-1-1b&#34;&gt;&lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_litgpt/readme/1.webp&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightning.ai/lightning-ai/studios/instruction-finetuning-tinyllama-1-1b-llm&#34;&gt;Instruction finetuning - TinyLlama 1.1B LLM&lt;/a&gt; &lt;br&gt; &lt;p align=&#34;left&#34;&gt;&lt;a href=&#34;https://lightning.ai/lightning-ai/studios/instruction-finetuning-tinyllama-1-1b-llm&#34;&gt;&lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_litgpt/readme/2.webp&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&amp;nbsp; &lt;br&gt; &amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Installing LitGPT&lt;/h2&gt; &#xA;&lt;p&gt;You can install LitGPT with all dependencies (including CLI, quantization, tokenizers for all models, etc.) using the following pip command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#39;litgpt[all]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, can install litgpt from a cloned GitHub repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightning-AI/litgpt&#xA;cd litgpt&#xA;pip install -e &#39;.[all]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Using LitGPT&lt;/h2&gt; &#xA;&lt;p&gt;Below is a minimal example to get started with the LitGPT command line interface (CLI), illustrating how to download and use a model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1) Download a pretrained model&#xA;litgpt download --repo_id mistralai/Mistral-7B-Instruct-v0.2&#xA;&#xA;# 2) Chat with the model&#xA;litgpt chat \&#xA;  --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.2&#xA;&#xA;&amp;gt;&amp;gt; Prompt: What do Llamas eat?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/download_model_weights.md&#34;&gt;download&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/inference.md&#34;&gt;inference&lt;/a&gt; tutorials.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We recommend starting with the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/0_to_litgpt.md&#34;&gt;Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs&lt;/a&gt;&lt;/strong&gt; if you are looking to get started with using LitGPT.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning and pretraining&lt;/h2&gt; &#xA;&lt;p&gt;LitGPT supports &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain_tinyllama.md&#34;&gt;pretraining&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;finetuning&lt;/a&gt; to optimize models on excisting or custom datasets. Below is an example showing how to finetune a model with LoRA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1) Download a pretrained model&#xA;litgpt download --repo_id microsoft/phi-2&#xA;&#xA;# 2) Finetune the model&#xA;litgpt finetune lora \&#xA;  --checkpoint_dir checkpoints/microsoft/phi-2 \&#xA;  --data Alpaca2k \&#xA;  --out_dir out/phi-2-lora&#xA;&#xA;# 3) Chat with the model&#xA;litgpt chat \&#xA;  --checkpoint_dir out/phi-2-lora/final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Configuration files for enhanced performance&lt;/h2&gt; &#xA;&lt;p&gt;LitGPT also allows users to use configuration files in YAML format instead of specifying settings via the command line interface and comes with a set of model-specific defaults for good out-of-the-box performance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt finetune lora \&#xA;  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For added convenience, you can also manually override config file setting via the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt finetune lora \&#xA;  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml \&#xA;  --lora_r 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can browse the available configuration files &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] &lt;strong&gt;Run large models on smaller consumer devices:&lt;/strong&gt; We support 4-bit quantization (as in QLoRA), (bnb.nf4, bnb.nf4-dq, bnb.fp4, bnb.fp4-dq) and 8-bit quantization (bnb.int8) for inference by following &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/quantize.md&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp; &lt;br&gt; &amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Customization&lt;/h2&gt; &#xA;&lt;p&gt;LitGPT supports rich and customizable &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub&#34;&gt;config files&lt;/a&gt; to tailor the LLM training to your dataset and hardware needs. Shown below is a configuration file for LoRA finetuning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# The path to the base model&#39;s checkpoint directory to load for finetuning. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: checkpoints/stabilityai/stablelm-base-alpha-3b)&#xA;checkpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf&#xA;&#xA;# Directory in which to save checkpoints and logs. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: out/lora)&#xA;out_dir: out/finetune/qlora-llama2-7b&#xA;&#xA;# The precision to use for finetuning. Possible choices: &#34;bf16-true&#34;, &#34;bf16-mixed&#34;, &#34;32-true&#34;. (type: Optional[str], default: null)&#xA;precision: bf16-true&#xA;&#xA;# If set, quantize the model with this algorithm. See ``tutorials/quantize.md`` for more information. (type: Optional[Literal[&#39;nf4&#39;, &#39;nf4-dq&#39;, &#39;fp4&#39;, &#39;fp4-dq&#39;, &#39;int8-training&#39;]], default: null)&#xA;quantize: bnb.nf4&#xA;&#xA;# How many devices/GPUs to use. (type: Union[int, str], default: 1)&#xA;devices: 1&#xA;&#xA;# The LoRA rank. (type: int, default: 8)&#xA;lora_r: 32&#xA;&#xA;# The LoRA alpha. (type: int, default: 16)&#xA;lora_alpha: 16&#xA;&#xA;# The LoRA dropout value. (type: float, default: 0.05)&#xA;lora_dropout: 0.05&#xA;&#xA;# Whether to apply LoRA to the query weights in attention. (type: bool, default: True)&#xA;lora_query: true&#xA;&#xA;# Whether to apply LoRA to the key weights in attention. (type: bool, default: False)&#xA;lora_key: false&#xA;&#xA;# Whether to apply LoRA to the value weights in attention. (type: bool, default: True)&#xA;lora_value: true&#xA;&#xA;# Whether to apply LoRA to the output projection in the attention block. (type: bool, default: False)&#xA;lora_projection: false&#xA;&#xA;# Whether to apply LoRA to the weights of the MLP in the attention block. (type: bool, default: False)&#xA;lora_mlp: false&#xA;&#xA;# Whether to apply LoRA to output head in GPT. (type: bool, default: False)&#xA;lora_head: false&#xA;&#xA;# Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.&#xA;data:&#xA;  class_path: litgpt.data.Alpaca2k&#xA;  init_args:&#xA;    mask_prompt: false&#xA;    val_split_fraction: 0.05&#xA;    prompt_style: alpaca&#xA;    ignore_index: -100&#xA;    seed: 42&#xA;    num_workers: 4&#xA;    download_dir: data/alpaca2k&#xA;&#xA;# Training-related arguments. See ``litgpt.args.TrainArgs`` for details&#xA;train:&#xA;&#xA;  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)&#xA;  save_interval: 200&#xA;&#xA;  # Number of iterations between logging calls (type: int, default: 1)&#xA;  log_interval: 1&#xA;&#xA;  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 128)&#xA;  global_batch_size: 8&#xA;&#xA;  # Number of samples per data-parallel rank (type: int, default: 4)&#xA;  micro_batch_size: 2&#xA;&#xA;  # Number of iterations with learning rate warmup active (type: int, default: 100)&#xA;  lr_warmup_steps: 10&#xA;&#xA;  # Number of epochs to train on (type: Optional[int], default: 5)&#xA;  epochs: 4&#xA;&#xA;  # Total number of tokens to train on (type: Optional[int], default: null)&#xA;  max_tokens:&#xA;&#xA;  # Limits the number of optimizer steps to run (type: Optional[int], default: null)&#xA;  max_steps:&#xA;&#xA;  # Limits the length of samples (type: Optional[int], default: null)&#xA;  max_seq_length: 512&#xA;&#xA;  # Whether to tie the embedding weights with the language modeling head weights (type: Optional[bool], default: null)&#xA;  tie_embeddings:&#xA;&#xA;  #   (type: float, default: 0.0003)&#xA;  learning_rate: 0.0002&#xA;&#xA;  #   (type: float, default: 0.02)&#xA;  weight_decay: 0.0&#xA;&#xA;  #   (type: float, default: 0.9)&#xA;  beta1: 0.9&#xA;&#xA;  #   (type: float, default: 0.95)&#xA;  beta2: 0.95&#xA;&#xA;  #   (type: Optional[float], default: null)&#xA;  max_norm:&#xA;&#xA;  #   (type: float, default: 6e-05)&#xA;  min_lr: 6.0e-05&#xA;&#xA;# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details&#xA;eval:&#xA;&#xA;  # Number of optimizer steps between evaluation calls (type: int, default: 100)&#xA;  interval: 100&#xA;&#xA;  # Number of tokens to generate (type: Optional[int], default: 100)&#xA;  max_new_tokens: 100&#xA;&#xA;  # Number of iterations (type: int, default: 100)&#xA;  max_iters: 100&#xA;&#xA;# The name of the logger to send metrics to. (type: Literal[&#39;wandb&#39;, &#39;tensorboard&#39;, &#39;csv&#39;], default: csv)&#xA;logger_name: csv&#xA;&#xA;# The random seed to use for reproducibility. (type: int, default: 1337)&#xA;seed: 1337&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;LitGPT design principles&lt;/h2&gt; &#xA;&lt;p&gt;This repository follows the main principle of &lt;strong&gt;openness through clarity&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LitGPT&lt;/strong&gt; is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple:&lt;/strong&gt; Single-file implementation without boilerplate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Correct:&lt;/strong&gt; Numerically equivalent to the original model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimized:&lt;/strong&gt; Runs fast on consumer hardware or at scale.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-source:&lt;/strong&gt; No strings attached.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Avoiding code duplication is &lt;strong&gt;not&lt;/strong&gt; a goal. &lt;strong&gt;Readability&lt;/strong&gt; and &lt;strong&gt;hackability&lt;/strong&gt; are.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Get involved!&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate your feedback and contributions. If you have feature requests, questions, or want to contribute code or config files, please don&#39;t hesitate to use the &lt;a href=&#34;https://github.com/Lightning-AI/litgpt/issues&#34;&gt;GitHub Issue&lt;/a&gt; tracker.&lt;/p&gt; &#xA;&lt;p&gt;We welcome all individual contributors, regardless of their level of experience or hardware. Your contributions are valuable, and we are excited to see what you can accomplish in this collaborative and supportive environment.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Unsure about contributing? Check out our &lt;a href=&#34;https://lightning.ai/pages/community/tutorial/how-to-contribute-to-litgpt/&#34;&gt;How to Contribute to LitGPT&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you have general questions about building with LitGPT, please &lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;join our Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials, how-to guides, and docs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We recommend starting with the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/0_to_litgpt.md&#34;&gt;Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs&lt;/a&gt;&lt;/strong&gt; if you are looking to get started with using LitGPT.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Tutorials and in-depth feature documentation can be found below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finetuning, incl. LoRA, QLoRA, and Adapters (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;tutorials/finetune.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Pretraining (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain_tinyllama.md&#34;&gt;tutorials/pretrain_tinyllama.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Model evaluation (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/evaluation.md&#34;&gt;tutorials/evaluation.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Supported and custom datasets (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md&#34;&gt;tutorials/prepare_dataset.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Quantization (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/quantize.md&#34;&gt;tutorials/quantize.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Tips for dealing with out-of-memory (OOM) errors (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/oom.md&#34;&gt;tutorials/oom.md&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;XLA&lt;/h2&gt; &#xA;&lt;p&gt;Lightning AI has partnered with Google to add first-class support for &lt;a href=&#34;https://cloud.google.com/tpu&#34;&gt;Cloud TPUs&lt;/a&gt; in &lt;a href=&#34;https://github.com/Lightning-AI/lightning&#34;&gt;Lightning‚Äôs frameworks&lt;/a&gt; and LitGPT, helping democratize AI for millions of developers and researchers worldwide.&lt;/p&gt; &#xA;&lt;p&gt;Using TPUs with Lightning is as straightforward as changing one line of code.&lt;/p&gt; &#xA;&lt;p&gt;We provide scripts fully optimized for TPUs in the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/extensions/xla&#34;&gt;XLA directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This implementation extends on &lt;a href=&#34;https://github.com/lightning-AI/lit-llama&#34;&gt;Lit-LLaMA&lt;/a&gt; and &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;, and it&#39;s &lt;strong&gt;powered by &lt;a href=&#34;https://lightning.ai/docs/fabric/stable/&#34;&gt;Lightning Fabric&lt;/a&gt; ‚ö°&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy&#34;&gt;@karpathy&lt;/a&gt; for &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI&#34;&gt;@EleutherAI&lt;/a&gt; for &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt; and the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;Evaluation Harness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers&#34;&gt;@TimDettmers&lt;/a&gt; for &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft&#34;&gt;@Microsoft&lt;/a&gt; for &lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tridao&#34;&gt;@tridao&lt;/a&gt; for &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;Flash Attention 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Community showcase&lt;/h2&gt; &#xA;&lt;p&gt;Check out the projects below using and building on LitGPT. If you have a project you&#39;d like to add to this section, please don&#39;t hestiate to open a pull request.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üèÜ NeurIPS 2023 Large Language Model Efficiency Challenge: 1 LLM + 1 GPU + 1 Day&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The LitGPT repository was the official starter kit for the &lt;a href=&#34;https://llm-efficiency-challenge.github.io&#34;&gt;NeurIPS 2023 LLM Efficiency Challenge&lt;/a&gt;, which is a competition focused on finetuning an existing non-instruction tuned LLM for 24 hours on a single GPU.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü¶ô TinyLlama: An Open-Source Small Language Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;LitGPT powered the &lt;a href=&#34;https://github.com/jzhang38/TinyLlama&#34;&gt;TinyLlama project&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2401.02385&#34;&gt;TinyLlama: An Open-Source Small Language Model&lt;/a&gt; research paper.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use LitGPT in your research, please cite the following work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{litgpt-2023,&#xA;  author       = {Lightning AI},&#xA;  title        = {LitGPT},&#xA;  howpublished = {\url{https://github.com/Lightning-AI/litgpt}},&#xA;  year         = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;LitGPT is released under the &lt;a href=&#34;https://github.com/Lightning-AI/litgpt/raw/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
</feed>