<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-04T01:35:00Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pytorch/examples</title>
    <updated>2023-11-04T01:35:00Z</updated>
    <id>tag:github.com,2023-11-04:/pytorch/examples</id>
    <link href="https://github.com/pytorch/examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PyTorch Examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pytorch/examples/workflows/Run%20Examples/badge.svg?sanitize=true&#34; alt=&#34;Run Examples&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/examples/&#34;&gt;https://pytorch.org/examples/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pytorch/examples&lt;/code&gt; is a repository showcasing examples of using &lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;PyTorch&lt;/a&gt;. The goal is to have curated, short, few/no dependencies &lt;em&gt;high quality&lt;/em&gt; examples that are substantially different from each other that can be emulated in your existing work.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For tutorials: &lt;a href=&#34;https://github.com/pytorch/tutorials&#34;&gt;https://github.com/pytorch/tutorials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For changes to pytorch.org: &lt;a href=&#34;https://github.com/pytorch/pytorch.github.io&#34;&gt;https://github.com/pytorch/pytorch.github.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For a general model hub: &lt;a href=&#34;https://pytorch.org/hub/&#34;&gt;https://pytorch.org/hub/&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/models&#34;&gt;https://huggingface.co/models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For recipes on how to run PyTorch in production: &lt;a href=&#34;https://github.com/facebookresearch/recipes&#34;&gt;https://github.com/facebookresearch/recipes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For general Q&amp;amp;A and support: &lt;a href=&#34;https://discuss.pytorch.org/&#34;&gt;https://discuss.pytorch.org/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Available models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/mnist/README.md&#34;&gt;Image classification (MNIST) using Convnets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/README.md&#34;&gt;Word-level Language Modeling using RNN and Transformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/imagenet/README.md&#34;&gt;Training Imagenet Classifiers with Popular Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/dcgan/README.md&#34;&gt;Generative Adversarial Networks (DCGAN)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/vae/README.md&#34;&gt;Variational Auto-Encoders&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/super_resolution/README.md&#34;&gt;Superresolution using an efficient sub-pixel convolutional neural network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/mnist_hogwild&#34;&gt;Hogwild training of shared ConvNets across multiple processes on MNIST&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/reinforcement_learning/README.md&#34;&gt;Training a CartPole to balance in OpenAI Gym with actor-critic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/snli&#34;&gt;Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/time_sequence_prediction/README.md&#34;&gt;Time sequence prediction - use an LSTM to learn Sine waves&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/fast_neural_style/README.md&#34;&gt;Implement the Neural Style Transfer algorithm on images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/reinforcement_learning/README.md&#34;&gt;Reinforcement Learning with Actor Critic and REINFORCE algorithms on OpenAI gym&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/fx/README.md&#34;&gt;PyTorch Module Transformations using fx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Distributed PyTorch examples with &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/distributed/ddp/README.md&#34;&gt;Distributed Data Parallel&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/distributed/rpc&#34;&gt;RPC&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/cpp&#34;&gt;Several examples illustrating the C++ Frontend&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/mnist_forward_forward/README.md&#34;&gt;Image Classification Using Forward-Forward &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, a list of good examples hosted in their own repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenNMT/OpenNMT-py&#34;&gt;Neural Machine Translation using sequence-to-sequence RNN with attention (OpenNMT)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute your own example or fix a bug please make sure to take a look at &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/examples/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PromptEngineer48/MemGPT-AutoGEN-LLM</title>
    <updated>2023-11-04T01:35:00Z</updated>
    <id>tag:github.com,2023-11-04:/PromptEngineer48/MemGPT-AutoGEN-LLM</id>
    <link href="https://github.com/PromptEngineer48/MemGPT-AutoGEN-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run MemGPT-AutoGEN-Local LLM Together&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MemGPT-AutoGEN-LLM&lt;/h1&gt; &#xA;&lt;p&gt;This comes from my youtube video titled AutoGEN + MemGPT + Local LLM (Complete Tutorial) üòç &lt;a href=&#34;https://youtu.be/bMWXXPoDnDs&#34;&gt;https://youtu.be/bMWXXPoDnDs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;please watch the youtube video to get a better understand&lt;/p&gt; &#xA;&lt;p&gt;Summary: 00:11 üöÄ The video demonstrates how to connect MemGPT, AutoGEN, and local Large Language Models (LLMs) using Runpods.&lt;/p&gt; &#xA;&lt;p&gt;01:32 ü§ñ You can integrate MemGPT and AutoGEN to work together, with MemGPT serving as an assistant agent alongside local LLMs.&lt;/p&gt; &#xA;&lt;p&gt;03:46 üìö To get started, install Python, VS Code, and create a Runpods account with credits. You can use Runpods for running local LLMs.&lt;/p&gt; &#xA;&lt;p&gt;06:43 üõ†Ô∏è Set up a virtual environment, create a Python file, and activate the environment for your project.&lt;/p&gt; &#xA;&lt;p&gt;08:52 üì¶ Install necessary libraries like OpenAI, PyAutoGEN, and MGBPT to work with AutoGEN and MemGPT.&lt;/p&gt; &#xA;&lt;p&gt;16:21 ‚öôÔ∏è Use Runpods to deploy local LLMs, select the hardware configuration, and create API endpoints for integration with AutoGEN and MemGPT.&lt;/p&gt; &#xA;&lt;p&gt;20:29 üîÑ Modify the code to switch between using AutoGEN and MemGPT agents based on a flag, allowing you to harness the power of both.&lt;/p&gt; &#xA;&lt;p&gt;23:31 ü§ù Connect AutoGEN and MemGPT by configuring the API endpoints with the local LLMs from Runpods, enabling them to work seamlessly together.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>radames/Real-Time-Latent-Consistency-Model</title>
    <updated>2023-11-04T01:35:00Z</updated>
    <id>tag:github.com,2023-11-04:/radames/Real-Time-Latent-Consistency-Model</id>
    <link href="https://github.com/radames/Real-Time-Latent-Consistency-Model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Demo showcasing ~real-time Latent Consistency Model pipeline with Diffusers and a MJPEG stream server&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; &#xA;&lt;h2&gt;title: Real-Time Latent Consistency Model Image-to-Image emoji: üñºÔ∏èüñºÔ∏è colorFrom: gray colorTo: indigo sdk: docker pinned: false suggested_hardware: a10g-small&lt;/h2&gt; &#xA;&lt;h1&gt;Real-Time Latent Consistency Model&lt;/h1&gt; &#xA;&lt;p&gt;This demo showcases &lt;a href=&#34;https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7&#34;&gt;Latent Consistency Model (LCM)&lt;/a&gt; using &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/community#latent-consistency-pipeline&#34;&gt;Diffusers&lt;/a&gt; with a MJPEG stream server.&lt;/p&gt; &#xA;&lt;p&gt;You need a webcam to run this demo. ü§ó&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;You need CUDA and Python 3.10, Mac with an M1/M2/M3 chip or Intel Arc GPU&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;TIMEOUT&lt;/code&gt;: limit user session timeout&lt;br&gt; &lt;code&gt;SAFETY_CHECKER&lt;/code&gt;: disabled if you want NSFW filter off&lt;br&gt; &lt;code&gt;MAX_QUEUE_SIZE&lt;/code&gt;: limit number of users on current app instance&lt;/p&gt; &#xA;&lt;h3&gt;image to image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;source venv/bin/activate&#xA;pip3 install -r requirements.txt&#xA;uvicorn &#34;app-img2img:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;text to image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;source venv/bin/activate&#xA;pip3 install -r requirements.txt&#xA;uvicorn &#34;app-txt2img:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or with environment variables&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TIMEOUT=120 SAFETY_CHECKER=True MAX_QUEUE_SIZE=4 uvicorn &#34;app-img2img:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re running locally and want to test it on Mobile Safari, the webserver needs to be served over HTTPS.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openssl req -newkey rsa:4096 -nodes -keyout key.pem -x509 -days 365 -out certificate.pem&#xA;uvicorn &#34;app-img2img:app&#34; --host 0.0.0.0 --port 7860 --reload --log-level info --ssl-certfile=certificate.pem --ssl-keyfile=key.pem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;You need NVIDIA Container Toolkit for Docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t lcm-live .&#xA;docker run -ti -p 7860:7860 --gpus all lcm-live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or with environment variables&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -ti -e TIMEOUT=0 -e SAFETY_CHECKER=False -p 7860:7860 --gpus all lcm-live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Demo on Hugging Face&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model&#34;&gt;https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/radames/Real-Time-Latent-Consistency-Model/assets/102277/c4003ac5-e7ff-44c0-97d3-464bb659de70&#34;&gt;https://github.com/radames/Real-Time-Latent-Consistency-Model/assets/102277/c4003ac5-e7ff-44c0-97d3-464bb659de70&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>