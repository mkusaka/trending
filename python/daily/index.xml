<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-25T01:34:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hao-ai-lab/LookaheadDecoding</title>
    <updated>2023-11-25T01:34:52Z</updated>
    <id>tag:github.com,2023-11-25:/hao-ai-lab/LookaheadDecoding</id>
    <link href="https://github.com/hao-ai-lab/LookaheadDecoding" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;h1&gt;&amp;nbsp;Break the Sequential Dependency of LLM Inference Using Lookahead Decoding&lt;/h1&gt;&#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://lmsys.org/blog/2023-11-21-lookahead-decoding/&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/hao-ai-lab/LookaheadDecoding/issues/13&#34;&gt;&lt;b&gt;Roadmap&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We introduce lookahead decoding:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A parallel decoding algorithm to accelerate LLM inference.&lt;/li&gt; &#xA; &lt;li&gt;Without the need for a draft model or a data store.&lt;/li&gt; &#xA; &lt;li&gt;Linearly decreases #decoding steps relative to log(FLOPs) used per decoding step.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Below is a demo of lookahead decoding accelerating LLaMa-2-Chat 7B generation:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/media/acc-demo.gif&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Demo of speedups by lookahead decoding on LLaMA-2-Chat 7B generation. Blue fonts are tokens generated in parallel in a decoding step.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Background: Parallel LLM Decoding Using Jacobi Iteration&lt;/h3&gt; &#xA;&lt;p&gt;Lookahead decoding is motivated by &lt;a href=&#34;https://arxiv.org/pdf/2305.10427.pdf&#34;&gt;Jacobi decoding&lt;/a&gt;, which views autoregressive decoding as solving nonlinear systems and decodes all future tokens simultaneously using a fixed-point iteration method. Below is a Jacobi decoding example.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/media/jacobi-iteration.gif&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Illustration of applying Jacobi iteration method for parallel LLM decoding.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;However, Jacobi decoding can barely see wall-clock speedup in real-world LLM applications.&lt;/p&gt; &#xA;&lt;h3&gt;Lookahead Decoding: Make Jacobi Decoding Feasible&lt;/h3&gt; &#xA;&lt;p&gt;Lookahead decoding takes advantage of Jacobi decoding&#39;s ability by collecting and caching n-grams generated from Jacobi iteration trajectories.&lt;/p&gt; &#xA;&lt;p&gt;The following gif shows the process of collecting 2 grams via Jacobi decoding and verifying them to accelerate decoding.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/media/lookahead-decoding.gif&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Illustration of lookahead decoding with 2-grams.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;To enhance the efficiency of this process, each lookahead decoding step is divided into two parallel branches: the lookahead branch and the verification branch. The lookahead branch maintains a fixed-sized, 2D window to generate n-grams from the Jacobi iteration trajectory. Simultaneously, the verification branch selects and verifies promising n-gram candidates.&lt;/p&gt; &#xA;&lt;h3&gt;Lookahead Branch and Verification Branch&lt;/h3&gt; &#xA;&lt;p&gt;The lookahead branch aims to generate new N-grams. The branch operates with a two-dimensional window defined by two parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Window size W: How far ahead we look in future token positions to conduct parallel decoding.&lt;/li&gt; &#xA; &lt;li&gt;N-gram size N: How many steps we look back into the past Jacobi iteration trajectory to retrieve n-grams.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In the verification branch, we identify n-grams whose first token matches the last input token. This is determined via simple string match. Once identified, these n-grams are appended to the current input and subjected to verification via an LLM forward pass through them.&lt;/p&gt; &#xA;&lt;p&gt;We implement these branches in one attention mask to further utilize GPU&#39;s parallel computing power.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/media/mask.png&#34; width=&#34;40%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Attention mask for lookahead decoding with 4-grams and window size 5. In this mask, two 4-gram candidates (bottom right) are verified concurrently with parallel decoding.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Experimental Results&lt;/h3&gt; &#xA;&lt;p&gt;Our study shows lookahead decoding substantially reduces latency, ranging from 1.5x to 2.3x on different datasets on a single GPU. See the figure below.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/media/lookahead-perf.png&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/picture&gt; &#xA; &lt;br&gt; &#xA; &lt;div align=&#34;center&#34; width=&#34;80%&#34;&gt; &#xA;  &lt;em&gt;Speedup of lookahead decoding on different models and datasets.&lt;/em&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#contents&#34;&gt;Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#install-with-pip&#34;&gt;Install With Pip&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#install-from-the-source&#34;&gt;Install From The Source&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#inference-with-lookahead-decoding&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#use-lookahead-decoding-in-your-own-code&#34;&gt;Use In Your Own Code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hao-ai-lab/LookaheadDecoding/main/#guidance&#34;&gt;Guidance&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install with pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install from the source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hao-ai-lab/LookaheadDecoding.git&#xA;cd LookaheadDecoding&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference With Lookahead decoding&lt;/h3&gt; &#xA;&lt;p&gt;You can run the minimal example to see the speedup that Lookahead decoding brings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python minimal.py #no Lookahead decoding&#xA;USE_LADE=1 LOAD_LADE=1 python minimal.py #use Lookahead decoding, 1.6x speedup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also enjoy chatting with your own chatbots with Lookahead decoding.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;USE_LADE=1 python applications/chatbot.py  --model_path meta-llama/Llama-2-7b-chat-hf --debug --chat #chat, with lookahead &#xA;USE_LADE=0 python applications/chatbot.py  --model_path meta-llama/Llama-2-7b-chat-hf --debug --chat #chat, without lookahead&#xA;&#xA;&#xA;USE_LADE=1 python applications/chatbot.py  --model_path meta-llama/Llama-2-7b-chat-hf --debug #no chat, with lookahead&#xA;USE_LADE=0 python applications/chatbot.py  --model_path meta-llama/Llama-2-7b-chat-hf --debug #no chat, without lookahead&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use Lookahead decoding in your own code&lt;/h3&gt; &#xA;&lt;p&gt;You can import and use Lookahead decoding in your own code in three LoCs. You also need to set &lt;code&gt;USE_LADE=1&lt;/code&gt; in command line or set &lt;code&gt;os.environ[&#34;USE_LADE&#34;]=&#34;1&#34;&lt;/code&gt; in Python script. Note that Lookahead decoding only support LLaMA and Greedy Search yet.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import lade&#xA;lade.augment_all()&#xA;lade.config_lade(LEVEL=5, WINDOW_SIZE=7, GUESS_SET_SIZE=7, DEBUG=0) &#xA;#LEVEL, WINDOW_SIZE and GUESS_SET_SIZE are three important configurations (N,W,G) in lookahead decoding, please refer to our blog!&#xA;#You can obtain a better performance by tuning LEVEL/WINDOW_SIZE/GUESS_SET_SIZE on your own device.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can speedup the decoding process.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=torch_device)&#xA;model_inputs = tokenizer(input_text, return_tensors=&#39;pt&#39;).to(torch_device)&#xA;greedy_output = model.generate(**model_inputs, max_new_tokens=1024) #speedup obtained&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{fu2023lookahead,&#xA;    title = {Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding},&#xA;    url = {https://lmsys.org/blog/2023-11-21-lookahead-decoding/},&#xA;    author = {Yichao Fu and Peter Bailis and Ion Stoica and Hao Zhang},&#xA;    month = {November},&#xA;    year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Guidance&lt;/h2&gt; &#xA;&lt;p&gt;The core implementation is in decoding.py. Lookahead decoding requires an adaptation for each specific model. An example is in models/llama.py.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>InternLM/InternLM-XComposer</title>
    <updated>2023-11-25T01:34:52Z</updated>
    <id>tag:github.com,2023-11-25:/InternLM/InternLM-XComposer</id>
    <link href="https://github.com/InternLM/InternLM-XComposer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/logo-en.png&#34; width=&#34;650&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;&lt;font size=&#34;6&#34;&gt;InternLM-XComposer&lt;/font&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;!-- &lt;div align=&#34;center&#34;&gt;&#xA;        InternLM-XComposer &lt;a href=&#34;&#34;&gt;🐼 &lt;a&gt; &lt;a href=&#34;&#34;&gt;🤖 &lt;a&gt; &lt;a href=&#34;&#34;&gt;🤗&lt;/a&gt;&amp;nbsp ｜ InternLM-VL &lt;a href=&#34;&#34;&gt;🤖 &lt;a&gt; &lt;a href=&#34;&#34;&gt;🤗&lt;/a&gt;&amp;nbsp | Technical Report &lt;a href=&#34;&#34;&gt; &lt;a&gt; 📄  --&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  InternLM-XComposer &#xA; &lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-7b&#34;&gt;🤗&lt;/a&gt; &#xA; &lt;a href=&#34;https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b&#34;&gt;🤖&lt;/a&gt;&#xA; &lt;a href=&#34;https://openxlab.org.cn/models/detail/InternLM-xcomposer/internlm-xcomposer-7b&#34;&gt;🐼 &lt;/a&gt; &amp;nbsp; ｜ InternLM-XComposer-VL &#xA; &lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-vl-7b&#34;&gt;🤗&lt;/a&gt; &#xA; &lt;a href=&#34;https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-vl-7b&#34;&gt;🤖&lt;/a&gt;&#xA; &lt;a href=&#34;https://openxlab.org.cn/models/detail/InternLM-xcomposer/internlm-xcomposer-7b&#34;&gt;🐼 &lt;/a&gt; &amp;nbsp; | Technical Report &#xA; &lt;a href=&#34;https://arxiv.org/pdf/2309.15112.pdf&#34;&gt; 📄 &lt;/a&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/README_CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Thanks the community for &lt;a href=&#34;https://huggingface.co/spaces/Willow123/InternLM-XComposer&#34;&gt;HuggingFace Demo &lt;/a&gt; and &lt;a href=&#34;https://replicate.com/cjwbw/internlm-xcomposer&#34; target=&#34;_blank&#34;&gt;Replicate Demo&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 👋 join us on &lt;a href=&#34;https://discord.gg/xa29JuW87d&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt; and &lt;a href=&#34;https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Multimodal Projects of Our Team&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;&lt;strong&gt;InternLM-XComposer&lt;/strong&gt;&lt;/a&gt;: &lt;strong&gt;A Vision-Language Large Model for Advanced Text-image Comprehension and Composition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ShareGPT4V/ShareGPT4V-Resources/master/images/logo_tight.png&#34; style=&#34;vertical-align: -20px;&#34; :height=&#34;25px&#34; width=&#34;25px&#34;&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;&lt;strong&gt;ShareGPT4V&lt;/strong&gt;&lt;/a&gt;: &lt;strong&gt;Improving Large Multi-modal Models with Better Captions&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;InternLM-XComposer&lt;/strong&gt; is a vision-language large model (VLLM) based on &lt;a href=&#34;https://github.com/InternLM/InternLM/tree/main&#34;&gt;InternLM&lt;/a&gt; for advanced text-image comprehension and composition. InternLM-XComposer has several appealing properties:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved Text-Image Composition&lt;/strong&gt;: InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience. The interleaved text-image composition is implemented in following steps:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Text Generation&lt;/strong&gt;: It crafts long-form text based on human-provided instructions.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Image Spoting and Captioning&lt;/strong&gt;: It pinpoints optimal locations for image placement and furnishes image descriptions.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Image Retrieval and Selection&lt;/strong&gt;: It select image candidates and identify the image that optimally complements the content.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Comprehension with Rich Multilingual Knowledge&lt;/strong&gt;: The text-image comprehension is empowered by training on extensive multi-modal multilingual concepts with carefully crafted strategies, resulting in a deep understanding of visual content.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Strong performance&lt;/strong&gt;: It consistently achieves state-of-the-art results across various benchmarks for vision-language large models, including &lt;a href=&#34;https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation&#34;&gt;MME Benchmark&lt;/a&gt; (English), &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench&lt;/a&gt; (English), &lt;a href=&#34;https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard&#34;&gt;Seed-Bench&lt;/a&gt; (English), &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench-CN&lt;/a&gt;(Chinese), and &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;CCBench&lt;/a&gt;(Chinese).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We release InternLM-XComposer series in two versions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;InternLM-XComposer-VL-7B&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-7b&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b&#34;&gt;🤖 &lt;/a&gt;: The pretrained and multi-task trained VLLM model with InternLM as the initialization of the LLM, achieving strong performance on various multimodal benchmarks, e.g., MME Benchmark, MMBench Seed-Bench, CCBench, and MMBench-CN.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;InternLM-XComposer-7B&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-vl-7b&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-vl-7b&#34;&gt;🤖 &lt;/a&gt;: The further instruction tuned VLLM for &lt;em&gt;Interleaved Text-Image Composition&lt;/em&gt; and &lt;em&gt;LLM-based AI assistant&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://arxiv.org/pdf/2309.15112.pdf&#34;&gt;Technical Report&lt;/a&gt; for more details. &lt;br&gt;&lt;/p&gt; &#xA;&lt;!-- &#xA;&lt;p align=&#34;center&#34;&gt;&#xA;    &lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt; InternLM-XComposer &lt;/b&gt;&lt;/figcaption&gt;&#xA;&lt;p&gt; --&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/assets/22662425/fdb89a38-c650-45f2-b5b7-51182e89a5cc&#34;&gt;https://github.com/InternLM/InternLM-XComposer/assets/22662425/fdb89a38-c650-45f2-b5b7-51182e89a5cc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/raw/main/README_CN.md#demo&#34;&gt;Chinese Demo&lt;/a&gt; for the demo of the Chinese version.&lt;/p&gt; &#xA;&lt;h2&gt;News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.11.22&lt;/code&gt; 🎉🎉🎉 We release the &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt;, a large-scale highly descriptive image-text dataset generated by GPT4-Vision and a superior large multimodal model, ShareGPT4V-7B.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.10.30&lt;/code&gt; 🎉🎉🎉 InternLM-XComposer-VL achieved the top 1 ranking in both &lt;a href=&#34;https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards&#34;&gt;Q-Bench&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation&#34;&gt;Tiny LVLM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.10.19&lt;/code&gt; 🎉🎉🎉 Support for inference on multiple GPUs. Two 4090 GPUs are sufficient for deploying our demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.10.12&lt;/code&gt; 🎉🎉🎉 4-bit demo is supported, model files are available in &lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-7b-4bit&#34;&gt;Hugging Face&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b-4bit&#34;&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.10.8&lt;/code&gt; 🎉🎉🎉 &lt;a href=&#34;https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b&#34;&gt;InternLM-XComposer-7B&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-vl-7b&#34;&gt;InternLM-XComposer-VL-7B&lt;/a&gt; are publicly available on &lt;strong&gt;ModelScope&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.9.27&lt;/code&gt; 🎉🎉🎉 The &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/&#34;&gt;evaluation code&lt;/a&gt; of &lt;strong&gt;InternLM-XComposer-VL-7B&lt;/strong&gt; are publicly available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.9.27&lt;/code&gt; 🎉🎉🎉 &lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-7b&#34;&gt;InternLM-XComposer-7B&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/internlm/internlm-xcomposer-vl-7b&#34;&gt;InternLM-XComposer-VL-7B&lt;/a&gt; are publicly available on &lt;strong&gt;Hugging Face&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2023.9.27&lt;/code&gt; 🎉🎉🎉 We release a &lt;a href=&#34;https://arxiv.org/pdf/2309.15112.pdf&#34;&gt;technical report&lt;/a&gt; for more details of our model series.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We evaluate InternLM-XComposer-VL on seven multimodal benchmarks: &lt;a href=&#34;https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation&#34;&gt;MME Benchmark&lt;/a&gt;, &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard&#34;&gt;Seed-Bench&lt;/a&gt;, &lt;a href=&#34;https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards&#34;&gt;Q-Bench&lt;/a&gt;, &lt;a href=&#34;https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation&#34;&gt;Tiny LVLM&lt;/a&gt; in the English language, &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;CCBench&lt;/a&gt;, &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench-CN&lt;/a&gt; in the simplified chinese language.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation&#34;&gt;MME Benchmark&lt;/a&gt;: A comprehensive evaluation benchmark for multimodal large language models with 14 subtasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench&lt;/a&gt;: A comprehensive evaluation pipeline comprised of meticulously curated multimodal dataset and a novel circulareval strategy using ChatGPT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench-CN&lt;/a&gt;: A simplified chinese language version of &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard&#34;&gt;Seed-Bench&lt;/a&gt;: A multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;CCBench&lt;/a&gt;: A multimodal benchmark for chinese cultural comprehension.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards&#34;&gt;Q-Bench&lt;/a&gt;: A benchmark for general-purpose foundation models on low-level vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation&#34;&gt;Tiny LVLM&lt;/a&gt;: An ability-level multimodal dataset split derived from the LVLM-eHub.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;InternLM-XComposer-VL outperforms existing vision-language large models on &lt;strong&gt;all the seven benchmarks&lt;/strong&gt;, demonstrating stronger multilingual comprehension ability.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/polar%20v3.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;MME Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation&#34;&gt;MME&lt;/a&gt; is a comprehensive evaluation benchmark for multimodal large language models. It measures both perception and cognition abilities on a total of 14 subtasks, including existence, count, position, color, poster, celebrity, scene, landmark, artwork, OCR, commonsense reasoning, numerical calculation, text translation, and code reasoning.&lt;/p&gt; &#xA;&lt;p&gt;InternLM-XComposer-VL achieves SOTAs on overall performance evaluation. See more details on &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mme/MME_Bench.md&#34;&gt;HERE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Overall Performance &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;️ 1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;InternLM-XComposer-VL&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;InternLM-7B&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1919.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1848.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;MMICL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;FlanT5xxl&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1810.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Skywork-MM&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Skywork-MM-13B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1775.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;BLIVA&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;FlanT5xxl&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1669.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;leaderboard&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mme/perception.PNG&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mme/cognition.PNG&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;MMBench &amp;amp; MMBench-CN&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench&lt;/a&gt; is a comprehensive evaluation pipeline comprised of meticulously curated multimodal dataset and a novel circulareval strategy using ChatGPT. It is comprised of 20 ability dimensions defined by MMBench. &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;MMBench-CN&lt;/a&gt; is the Chinese language version of MMBench.&lt;/p&gt; &#xA;&lt;p&gt;InternLM-XComposer-VL a chieves SOTAs on the test split of both MMBench and MMBench-CN. See more details on &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mmbench/MMBench.md&#34;&gt;HERE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; MMBench Test Split &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;️ 1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;74.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Pink&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Vicuna-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;74.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;JiuTian&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;FLANT5-XXL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;71.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;WeMM&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;69.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;mPLUG-Owl&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;LLaMA2 7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;68.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;leaderboard&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mmbench/mmbench_en.PNG&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p align=&#34;center&#34;&gt; MMBench-CN Test Split &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;️ 1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;72.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;QWen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;56.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;LLaVA&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;LLaMA 7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;36.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;VisualGLM&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;ChatGLM 6B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;25.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;mPLUG-Owl&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;LLaMA2 7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;leaderboard&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mmbench/mmbench_cn_en.PNG&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;SEED-Bench&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard&#34;&gt;SEED-Bench&lt;/a&gt; is a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs, covering 12 evaluation dimensions including both &lt;strong&gt;image&lt;/strong&gt; and &lt;strong&gt;video&lt;/strong&gt; understanding. See more details on &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/seed_bench/SEED.md&#34;&gt;HERE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;InternLM-XComposer-VL achieves SOTAs on this benchmark for images.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; SeedBench Image Evaluation &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;️ 1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;66.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;QWen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;65.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;QWen-VL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;62.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InstructBLIP-Vicuna&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Vicuna 7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;58.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InstructBLIP&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Flan-T5-XL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;57.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;leaderboard&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/seed_bench/seed_bench.PNG&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;CCBench&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;CCBench&lt;/a&gt; is a multimodal benchmark for chinese cultural comprehension. See more details on &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mmbench/MMBench.md&#34;&gt;HERE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; CCBench Performance &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;️ 1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;47.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;QWen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;39.3&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;mPLUG-Owl&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;LLaMA2 7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;12.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InstructBLIP&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Vicuna 7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;12.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;VisualGLM&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;ChatGLM 6B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;leaderboard&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/mmbench/ccbench_en.PNG&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Q-Bench&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards&#34;&gt;Q-Bench&lt;/a&gt; is a benchmark for general-purpose foundation models on low-level vision.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Q-Bench Performance &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;A1：Perception (dev)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;A1：Perception (test)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;A2: Description&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;A3: Assessment&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;️ 1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;br&gt;0.6535&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;br&gt;0.6435&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;br&gt;4.21/6&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;br&gt;(0.542, 0.581)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;LLaVA-v1.5-13B&lt;br&gt;0.6214&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InstrucBLIP-T5-XL&lt;br&gt;0.6194&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Kosmos-2&lt;br&gt;4.03/6&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-VL&lt;br&gt;(0.475, 0.506)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InstrucBLIP-T5-XL&lt;br&gt;0.6147&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-VL&lt;br&gt;0.6167&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;mPLUG-Owl&lt;br&gt;3.94/6&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;LLaVA-v1.5-13B&lt;br&gt;(0.444, 0.473)&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;leaderboard&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/qbench/overall.png&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Tiny LVLM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation&#34;&gt;Tiny LVLM&lt;/a&gt; is an ability-level multimodal dataset split derived from the LVLM-eHub.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Tiny LVLM Performance &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Rank&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Score&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;️ 1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-XComposer-VL&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;322.51&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Bard&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Bard&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;319.59&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;316.81&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;leaderboard&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/evaluation/tiny_lvlm/overall.png&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.8 and above&lt;/li&gt; &#xA; &lt;li&gt;pytorch 1.12 and above, 2.0 and above are recommended&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.4 and above are recommended (this is for GPU users) &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Before running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/docs/install.md&#34;&gt;installation instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;We provide a simple example to show how to use InternLM-XComposer with 🤗 Transformers.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;🤗 Transformers&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;torch.set_grad_enabled(False)&#xA;&#xA;# init model and tokenizer&#xA;model = AutoModel.from_pretrained(&#39;internlm/internlm-xcomposer-7b&#39;, trust_remote_code=True).cuda().eval()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;internlm/internlm-xcomposer-7b&#39;, trust_remote_code=True)&#xA;model.tokenizer = tokenizer&#xA;&#xA;# example image&#xA;image = &#39;examples/images/aiyinsitan.jpg&#39;&#xA;&#xA;# Single-Turn Pure-Text Dialogue&#xA;text = &#39;Please introduce Einstein.&#39;&#xA;response = model.generate(text)&#xA;print(response)&#xA;# Albert Einstein was a German-born theoretical physicist who developed the general theory of relativity, one of the &#xA;# two pillars of modern physics (alongside quantum mechanics). He is best known for his mass–energy equivalence &#xA;# formula E = mc2 (which has been dubbed &#34;the world&#39;s most famous equation&#34;), and his explanation of the photoelectric &#xA;# effect, both of which are examples of his special and general theories of relativity. Einstein is widely regarded as &#xA;# one of the most influential physicists of all time.&#xA;&#xA;&#xA;# Single-Turn Text-Image Dialogue&#xA;text = &#39;Please introduce the person in this picture in detail.&#39;&#xA;image = &#39;examples/images/aiyinsitan.jpg&#39;&#xA;response = model.generate(text, image)&#xA;print(response)&#xA;# The person in the picture is Albert Einstein, a renowned theoretical physicist and one of the most influential &#xA;# scientists of the 20th century. He is depicted in a black and white portrait, wearing a suit and tie, and has a &#xA;# serious expression on his face.&#xA;&#xA;&#xA;# Multi-Turn Text-Image Dialogue&#xA;# 1st turn&#xA;text = &#39;Who is in the picture?&#39;&#xA;response, history = model.chat(text=text, image=image, history=None)&#xA;print(response)&#xA;# Albert Einstein is in the picture.&#xA;&#xA;# 2nd turn&#xA;text = &#39;What are his achievements?&#39;&#xA;response, history = model.chat(text=text, image=None, history=history)&#xA;print(response)&#xA;# Albert Einstein was a German-born theoretical physicist who developed the general theory of relativity, &#xA;# one of the two pillars of modern physics (alongside quantum mechanics). He is best known for his mass–energy &#xA;# equivalence formula E = mc2 (which has been dubbed &#34;the world&#39;s most famous equation&#34;), and his explanation of &#xA;# the photoelectric effect, both of which are examples of his special and general theories of relativity.&#xA;&#xA;# 3rd turn&#xA;text = &#39;Is he the greatest physicist?&#39;&#xA;response, history = model.chat(text=text, image=None, history=history)&#xA;print(response)&#xA;# Yes, Albert Einstein is widely regarded as one of the greatest physicists of all time.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;🤖 ModelScope&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from modelscope import snapshot_download, AutoModel, AutoTokenizer&#xA;&#xA;torch.set_grad_enabled(False)&#xA;&#xA;# init model and tokenizer&#xA;model_dir = snapshot_download(&#39;Shanghai_AI_Laboratory/internlm-xcomposer-7b&#39;)&#xA;model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).cuda().eval()&#xA;tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)&#xA;model.tokenizer = tokenizer&#xA;&#xA;# example image&#xA;image = &#39;examples/images/aiyinsitan.jpg&#39;&#xA;&#xA;# Single-Turn Pure-Text Dialogue&#xA;text = &#39;Please introduce Einstein.&#39;&#xA;response = model.generate(text)&#xA;print(response)&#xA;# Albert Einstein was a German-born theoretical physicist who developed the general theory of relativity, one of the &#xA;# two pillars of modern physics (alongside quantum mechanics). He is best known for his mass–energy equivalence &#xA;# formula E = mc2 (which has been dubbed &#34;the world&#39;s most famous equation&#34;), and his explanation of the photoelectric &#xA;# effect, both of which are examples of his special and general theories of relativity. Einstein is widely regarded as &#xA;# one of the most influential physicists of all time.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;p&gt;Thanks the community for 3rd-party &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/pull/37&#34;&gt;HuggingFace Demo &lt;/a&gt; and &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/pull/9&#34; target=&#34;_blank&#34;&gt;Replicate Demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide code for users to build a web UI demo.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/demo_asset/assets/UI_en.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Please run the command below (GPU memory &amp;gt;= 32GB, Recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The user guidance of UI demo is given in &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/demo_asset/demo.md&#34;&gt;HERE&lt;/a&gt;. If you wish to change the default folder of the model, please use the &lt;code&gt;--folder=new_folder&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;h2&gt;Quantilization&lt;/h2&gt; &#xA;&lt;p&gt;We provide 4-bit quantized models to ease the memory requirement of the models. To run the 4-bit models (GPU memory &amp;gt;= 12GB), you need first install the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/docs/install.md&#34;&gt;dependency&lt;/a&gt;, then execute the follows scripts for chat and web demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# 4-bit chat&#xA;python examples/example_chat_4bit.py&#xA;# 4-bit web demo&#xA;python examples/web_demo_4bit.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference on Multiple GPUs&lt;/h2&gt; &#xA;&lt;p&gt;If you have multiple GPUs, but the memory size of each GPU is not enough to accommodate the entire model, you can split the model across multiple GPUs. First, install &lt;code&gt;accelerate&lt;/code&gt; using the command: &lt;code&gt;pip install accelerate&lt;/code&gt;. Then, execute the follows scripts for chat and web demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# chat with 2 GPUs&#xA;python examples/example_chat.py --num_gpus 2&#xA;# web demo with 2 GPUs&#xA;python examples/web_demo.py --num_gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Calculate TFLOPs and Params&lt;/h2&gt; &#xA;&lt;p&gt;Required package &lt;code&gt;pip install calflops&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# text = &#39;Please introduce the person in this picture in detail.&#39;&#xA;# image = &#39;examples/images/aiyinsitan.jpg&#39;&#xA;python examples/example_params_and_flops.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The expected output is FLOPs: 17.6 TFLOPS, Params: 8.8 B. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt; :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{zhang2023internlmxcomposer,&#xA;      title={InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition}, &#xA;      author={Pan Zhang and Xiaoyi Dong and Bin Wang and Yuhang Cao and Chao Xu and Linke Ouyang and Zhiyuan Zhao and Shuangrui Ding and Songyang Zhang and Haodong Duan and Wenwei Zhang and Hang Yan and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},&#xA;      year={2023},&#xA;      eprint={2309.15112},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;License &amp;amp; Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/申请表（中文）. For other questions or collaborations, please contact &lt;a href=&#34;mailto:internlm@pjlab.org.cn&#34;&gt;internlm@pjlab.org.cn&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>flowtyone/flowty-realtime-lcm-canvas</title>
    <updated>2023-11-25T01:34:52Z</updated>
    <id>tag:github.com,2023-11-25:/flowtyone/flowty-realtime-lcm-canvas</id>
    <link href="https://github.com/flowtyone/flowty-realtime-lcm-canvas" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A realtime sketch to image demo using LCM and the gradio library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;flowty-realtime-lcm-canvas&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/flowtyone/flowty-realtime-lcm-canvas/main/example.gif&#34; alt=&#34;example gif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;About&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This is a real-time sketch to image demo using LCM and the &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;gradio library&lt;/a&gt;. If you&#39;re not familiar with LCM, read about it here - &lt;a href=&#34;https://huggingface.co/blog/lcm_lora&#34;&gt;article on Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to LCM LoRA, you can also use different models by altering the model id in the ui. The desired effect was for you to be able to draw on one side and see the changes at close to real-time on the other side.&lt;/p&gt; &#xA;&lt;p&gt;Needless to say, this will perform worse on some GPUs, and better on some GPUs. 4090s usually perform best in the realtime scenario. Share your results!&lt;/p&gt; &#xA;&lt;p&gt;This was tested on a macbook pro with M2 Max, 30 GPU - 32GB combo, python 3.10. Inference times were tolerable, about 1.2s per render. If you&#39;re getting good performance on your machine, feel free to tweak the parameters in order to get better results. You can also change the canvas size to 768 / 1024 in ui.py, depending on your model.&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Setup a venv if you&#39;d like to isolate your project environment: &lt;code&gt;python -m venv env&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;activate on MacOS: &lt;code&gt;source ./env/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;activate on Windows: &lt;code&gt;env\Scripts\activate&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Nvidia users should install PyTorch using this command: &lt;code&gt;pip install torch --extra-index-url https://download.pytorch.org/whl/cu121&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install the requirements: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run ui.py: &lt;code&gt;python ui.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After you run ui.py, models should be downloaded automatically to the models directory. It might take a few minutes depending on your network. Once the models are downloaded, gradio will print to the console the url where you can access the ui.&lt;/p&gt; &#xA;&lt;h3&gt;Use Google Colab&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Google Colab users can also enjoy it by executing the following command and accessing the generated Gradio Public URL.&lt;br&gt; (I think this is currently only available in the Colab Pro.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;!git clone https://github.com/flowtyone/flowty-realtime-lcm-canvas.git&#xA;%cd flowty-realtime-lcm-canvas&#xA;!pip install -r requirements.txt&#xA;!python ui.py --share&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is a community project from &lt;a href=&#34;https://flowt.ai&#34;&gt;flowt.ai&lt;/a&gt;. If you like it, check us out!&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;logo-dark.svg&#34; height=&#34;50&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;logo.svg&#34; height=&#34;50&#34;&gt; &#xA; &lt;img alt=&#34;flowt.ai logo&#34; src=&#34;https://raw.githubusercontent.com/flowtyone/flowty-realtime-lcm-canvas/main/flowt.png&#34; height=&#34;50&#34;&gt; &#xA;&lt;/picture&gt;</summary>
  </entry>
</feed>