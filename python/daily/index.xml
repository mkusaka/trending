<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-12T01:37:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jiaweizzhao/GaLore</title>
    <updated>2024-03-12T01:37:51Z</updated>
    <id>tag:github.com,2024-03-12:/jiaweizzhao/GaLore</id>
    <link href="https://github.com/jiaweizzhao/GaLore" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GaLore&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains the pre-release version of GaLore algorithm, proposed by &lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Gradient Low-Rank Projection (GaLore) is a memory-efficient low-rank training strategy that allows &lt;em&gt;full-parameter&lt;/em&gt; learning but is more &lt;em&gt;memory-efficient&lt;/em&gt; than common low-rank adaptation methods, such as LoRA. As a gradient projection method, GaLore is independent of the choice of optimizers and can be easily plugged into existing ones with only two lines of code, as shown in Algorithm 1 below.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jiaweizzhao/GaLore/master/imgs/galore_code_box.png&#34; alt=&#34;Image 2&#34; style=&#34;width: 550px; margin: 0 auto;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from galore_torch import GaLoreAdamW, GaLoreAdamW8bit, GaLoreAdafactor&#xA;# define param groups as galore_params and non_galore_params&#xA;param_groups = [{&#39;params&#39;: non_galore_params}, &#xA;                {&#39;params&#39;: galore_params, &#39;rank&#39;: 128, &#39;update_proj_gap&#39;: 200, &#39;scale&#39;: 0.25, &#39;proj_type&#39;: &#39;std&#39;}]&#xA;optimizer = GaLoreAdamW(param_groups, lr=0.01)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmark 1: Pre-Training LLaMA on C4 dataset&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;torchrun_main.py&lt;/code&gt; is the main script for training LLaMA models on C4 with GaLore. Our benchmark scripts for various sizes of models are in &lt;code&gt;scripts/benchmark_c4&lt;/code&gt; folder. For example, to train a 60m model on C4, do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# LLaMA-60M, GaLore-Adam, 1 A100, 1 Node&#xA;torchrun --standalone --nproc_per_node 1 torchrun_main.py \&#xA;    --model_config configs/llama_60m.json \&#xA;    --lr 0.01 \&#xA;    --galore_scale 0.25 \&#xA;    --rank 128 \&#xA;    --update_proj_gap 200 \&#xA;    --batch_size 256 \&#xA;    --total_batch_size 512 \&#xA;    --num_training_steps 10000 \&#xA;    --warmup_steps 1000 \&#xA;    --weight_decay 0 \&#xA;    --dtype bfloat16 \&#xA;    --eval_every 1000 \&#xA;    --optimizer galore_adamw &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train 7B model with a single GPU with 24GB memory&lt;/h3&gt; &#xA;&lt;p&gt;To train a 7B model with a single GPU such as NVIDIA RTX 4090, all you need to do is to specify &lt;code&gt;--optimizer=galore_adamw8bit_per_layer&lt;/code&gt;, which enables &lt;code&gt;GaLoreAdamW8bit&lt;/code&gt; with per-layer weight updates. With activation checkpointing, you can maintain a batch size of 16 tested on NVIDIA RTX 4090.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# LLaMA-7B, 8-bit GaLore-Adam, single GPU, activation checkpointing&#xA;# bsz=16, 22.8G, &#xA;torchrun --standalone --nproc_per_node 1 torchrun_main.py \&#xA;    --model_config configs/llama_7b.json \&#xA;    --lr 0.005 \&#xA;    --galore_scale 0.25 \&#xA;    --rank 1024 \&#xA;    --update_proj_gap 500 \&#xA;    --batch_size 16 \&#xA;    --total_batch_size 512 \&#xA;    --activation_checkpointing \&#xA;    --num_training_steps 150000 \&#xA;    --warmup_steps 15000 \&#xA;    --weight_decay 0 \&#xA;    --grad_clipping 1.0 \&#xA;    --dtype bfloat16 \&#xA;    --eval_every 1000 \&#xA;    --single_gpu \&#xA;    --optimizer galore_adamw8bit_per_layer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently per-layer weight updates technique is only supported for single GPU training (&lt;code&gt;--single_gpu&lt;/code&gt;) without using &lt;code&gt;nn.parallel.DistributedDataParallel&lt;/code&gt;. We are working on supporting multi-GPU training with per-layer weight updates.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark 2: Fine-Tuning RoBERTa on GLUE tasks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;run_glue.py&lt;/code&gt; is the main script for fine-tuning RoBERTa models on GLUE tasks with GaLore. An example script is shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_glue.py \&#xA;    --model_name_or_path roberta-base \&#xA;    --task_name mrpc \&#xA;    --enable_galore \&#xA;    --lora_all_modules \&#xA;    --max_length 512 \&#xA;    --seed=1234 \&#xA;    --lora_r 4 \&#xA;    --galore_scale 4 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --update_proj_gap 500 \&#xA;    --learning_rate 3e-5 \&#xA;    --num_train_epochs 30 \&#xA;    --output_dir results/ft/roberta_base/mrpc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhao2024galore,&#xA;      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, &#xA;      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},&#xA;      year={2024},&#xA;      eprint={2403.03507},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>fluencelabs/dev-rewards</title>
    <updated>2024-03-12T01:37:51Z</updated>
    <id>tag:github.com,2024-03-12:/fluencelabs/dev-rewards</id>
    <link href="https://github.com/fluencelabs/dev-rewards" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fluence Developer Rewards&lt;/h1&gt; &#xA;&lt;h1&gt;Generate proof (docker)&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Build docker image&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;docker build -t dev-reward-script .&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If your ssh keys are in ~/.ssh, run the script:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;docker run -it --rm --network none -v ~/.ssh:/root/.ssh:ro dev-reward-script&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;If your ssh keys are in other directories, replace {dir_path_for_your_ssh_keys} with your directory path:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;docker run -it --rm --network none -v /{dir_path_for_your_ssh_keys}:/root/.ssh:ro dev-reward-script&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Generate proof (local sh script)&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;./install.sh&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the script&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;./proof-sh/proof.sh&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Generate proof (local python script)&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install python&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;https://www.python.org/downloads/&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;./install.sh&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;python3 -m venv claim-venv&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;source claim-venv/bin/activate&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;pip3 install -r python/requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the script&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;code&gt;python3 python/proof.py&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>SethBling/cbscript</title>
    <updated>2024-03-12T01:37:51Z</updated>
    <id>tag:github.com,2024-03-12:/SethBling/cbscript</id>
    <link href="https://github.com/SethBling/cbscript" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CBScript for Minecraft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;CBScript is a transpiled language, designed by SethBling. This compiler will compile CBScript files into Minecraft datapack zip files. It has many higher level language features that don&#39;t exist at the Minecraft command level. Awareness of implementation details will help you avoid performance overhead and bugs. The files in the datapack are generally organized by source file line numbers to make it easier to find the particular compiled mcfunction files you&#39;re looking for.&lt;/p&gt; &#xA;&lt;h1&gt;Installation and Requirements&lt;/h1&gt; &#xA;&lt;p&gt;The CBScript compiler requires python 3. In order to run it via the run.cmd file, you&#39;ll need the Python &#34;py launcher&#34;, which comes with newer installations of Python, and can be used to launch a specific version. You&#39;ll need to install the Python dependencies with&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are instructions in run.cmd for setting up your Windows registry to be able to double click .cbscript files in order to run the compiler.&lt;/p&gt; &#xA;&lt;p&gt;You can use cbscript-npp-highlighting.xml with Notepad++ to add syntax highlighting.&lt;/p&gt; &#xA;&lt;h1&gt;Running the Compiler&lt;/h1&gt; &#xA;&lt;p&gt;When you use run.cmd with your .cbscript file as an argument, a command window will pop up. This command window will monitor your .cbscript file for changes, and recompile any time it observes the file has been changed. Each cbscript file has a world file at the beginning that specifies where to place the compiled datapack. The compiled datapack will be placed in that world&#39;s /datapacks folder, with the same base name as the .cbscript file, overwriting it as necessary. You can use /reload in game to reload the datapack when it&#39;s been recompiled.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;p&gt;CBScript includes many high level features that simplify the syntax and construction of datapacks, as well as make them easier to maintain. Look at the scripts in the &#34;/scripts&#34; folder for examples. There are also archived script subfolders that contain datapacks for older versions of the game.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Include files&lt;/li&gt; &#xA; &lt;li&gt;Arithmetic expressions&lt;/li&gt; &#xA; &lt;li&gt;For/while loops&lt;/li&gt; &#xA; &lt;li&gt;If/else if/else blocks&lt;/li&gt; &#xA; &lt;li&gt;Execute syntax&lt;/li&gt; &#xA; &lt;li&gt;Tellraw with simplified syntax&lt;/li&gt; &#xA; &lt;li&gt;Title/subtitle/actionbar with simplified syntax.&lt;/li&gt; &#xA; &lt;li&gt;Macro function support&lt;/li&gt; &#xA; &lt;li&gt;Entity selector definitions, including data paths&lt;/li&gt; &#xA; &lt;li&gt;Advancement/loot table/block tag/predicate definitions.&lt;/li&gt; &#xA; &lt;li&gt;Function/method calls with parameters.&lt;/li&gt; &#xA; &lt;li&gt;Switch, supporting both numbers and block types&lt;/li&gt; &#xA; &lt;li&gt;Compile-time variables&lt;/li&gt; &#xA; &lt;li&gt;Compile-time loop unrolling&lt;/li&gt; &#xA; &lt;li&gt;Compile-time macros&lt;/li&gt; &#xA; &lt;li&gt;Template functions&lt;/li&gt; &#xA; &lt;li&gt;Coordinate vectors&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>