<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-23T01:42:43Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rdeepak2002/reddit-place-script-2022</title>
    <updated>2023-07-23T01:42:43Z</updated>
    <id>tag:github.com,2023-07-23:/rdeepak2002/reddit-place-script-2022</id>
    <link href="https://github.com/rdeepak2002/reddit-place-script-2022" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Script to draw an image onto r/place (https://www.reddit.com/r/place/)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reddit Place Script 2022&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rdeepak2002/reddit-place-script-2022/main/black_badge.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forthebadge.com&#34;&gt;&lt;img src=&#34;https://forthebadge.com/images/badges/made-with-python.svg?sanitize=true&#34; alt=&#34;forthebadge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forthebadge.com&#34;&gt;&lt;img src=&#34;https://forthebadge.com/images/badges/60-percent-of-the-time-works-every-time.svg?sanitize=true&#34; alt=&#34;forthebadge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Thanks to everyone who contributed! r/place is now over!&lt;/h1&gt; &#xA;&lt;a href=&#34;https://github.com/rdeepak2002/reddit-place-script-2022/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=rdeepak2002/reddit-place-script-2022&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;This is a script to draw an image onto r/place (&lt;a href=&#34;https://www.reddit.com/r/place/&#34;&gt;https://www.reddit.com/r/place/&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for multiple accounts.&lt;/li&gt; &#xA; &lt;li&gt;Determines the cooldown time remaining for each account.&lt;/li&gt; &#xA; &lt;li&gt;Detects existing matching pixels on the r/place map and skips them.&lt;/li&gt; &#xA; &lt;li&gt;Automatically converts colors to the r/place color palette.&lt;/li&gt; &#xA; &lt;li&gt;Easy(ish) to read output with colors.&lt;/li&gt; &#xA; &lt;li&gt;SOCKS proxy support.&lt;/li&gt; &#xA; &lt;li&gt;No client id and secret needed.&lt;/li&gt; &#xA; &lt;li&gt;Proxies from &#34;proxies.txt&#34; file.&lt;/li&gt; &#xA; &lt;li&gt;Tor support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Latest Version of Python 3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;macOS&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use tor on macOS. you&#39;ll need to provide your own tor binary or install it via &lt;a href=&#34;https://brew.sh&#34;&gt;Homebrew&lt;/a&gt; using &lt;code&gt;brew install tor&lt;/code&gt;, and start it manually.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to deactivate the &#34;use_builtin tor&#34; option in the config and configure your tor to use the correct ports and password.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please note that socks proxy connection to tor doesn&#39;t work for the time being, so the config value is for an httpTunnel port&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Move the file &#39;config_example.json&#39; to &#39;config.json&#39;&lt;/p&gt; &#xA;&lt;p&gt;Edit the values to replace with actual credentials and values&lt;/p&gt; &#xA;&lt;p&gt;Note: Please use &lt;a href=&#34;https://jsonlint.com/&#34;&gt;https://jsonlint.com/&lt;/a&gt; to check that your JSON file is correctly formatted&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;//Where the image&#39;s path is&#xA;&#x9;&#34;image_path&#34;: &#34;image.png&#34;,&#xA;&#x9;// [x,y] where you want the top left pixel of the local image to be drawn on canvas&#xA;&#x9;&#34;image_start_coords&#34;: [741, 610],&#xA;&#x9;// delay between starting threads (can be 0)&#xA;&#x9;&#34;thread_delay&#34;: 2,&#xA;&#x9;// array of accounts to use&#xA;&#x9;&#34;workers&#34;: {&#xA;&#x9;&#x9;// username of account 1&#xA;&#x9;&#x9;&#34;worker1username&#34;: {&#xA;&#x9;&#x9;&#x9;// password of account 1&#xA;&#x9;&#x9;&#x9;&#34;password&#34;: &#34;password&#34;,&#xA;&#x9;&#x9;&#x9;// which pixel of the image to draw first&#xA;&#x9;&#x9;&#x9;&#34;start_coords&#34;: [0, 0]&#xA;&#x9;&#x9;},&#xA;&#x9;&#x9;// username of account 2&#xA;&#x9;&#x9;&#34;worker1username&#34;: {&#xA;&#x9;&#x9;&#x9;// password of account 2&#xA;&#x9;&#x9;&#x9;&#34;password&#34;: &#34;password&#34;,&#xA;&#x9;&#x9;&#x9;// which pixel of the image to draw first&#xA;&#x9;&#x9;&#x9;&#34;start_coords&#34;: [0, 0]&#xA;&#x9;&#x9;}&#xA;&#x9;&#x9;// etc... add as many accounts as you want (but reddit may detect you the more you add)&#xA;&#x9;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;.png&lt;/code&gt; if you wish to make use of transparency or non rectangular images&lt;/li&gt; &#xA; &lt;li&gt;If you use 2 factor authentication (2FA) in your account, then change &lt;code&gt;password&lt;/code&gt; to &lt;code&gt;password:XXXXXX&lt;/code&gt; where &lt;code&gt;XXXXXX&lt;/code&gt; is your 2FA code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run the Script&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;start.bat or startverbose.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Unix-like (Linux, macOS etc.)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;chmod +x start.sh startverbose.sh&#xA;./start.sh or ./startverbose.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can get more logs (&lt;code&gt;DEBUG&lt;/code&gt;) by running the script with &lt;code&gt;-d&lt;/code&gt; flag:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python3 main.py -d&lt;/code&gt; or &lt;code&gt;python3 main.py --debug&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Multiple Workers&lt;/h2&gt; &#xA;&lt;p&gt;Just create multiple child arrays to &#34;workers&#34; in the .json file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;image_path&#34;: &#34;image.png&#34;,&#xA;&#x9;&#34;image_start_coords&#34;: [741, 610],&#xA;&#x9;&#34;thread_delay&#34;: 2,&#xA;&#xA;&#x9;&#34;workers&#34;: {&#xA;&#x9;&#x9;&#34;worker1username&#34;: {&#xA;&#x9;&#x9;&#x9;&#34;password&#34;: &#34;password&#34;,&#xA;&#x9;&#x9;&#x9;&#34;start_coords&#34;: [0, 0]&#xA;&#x9;&#x9;},&#xA;&#x9;&#x9;&#34;worker2username&#34;: {&#xA;&#x9;&#x9;&#x9;&#34;password&#34;: &#34;password&#34;,&#xA;&#x9;&#x9;&#x9;&#34;start_coords&#34;: [0, 50]&#xA;&#x9;&#x9;}&#xA;&#x9;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this case, the first worker will start drawing from (0, 0) and the second worker will start drawing from (0, 50) from the input image.jpg file.&lt;/p&gt; &#xA;&lt;p&gt;This is useful if you want different threads drawing different parts of the image with different accounts.&lt;/p&gt; &#xA;&lt;h2&gt;Other Settings&lt;/h2&gt; &#xA;&lt;p&gt;If any JSON decoders errors are found, the &lt;code&gt;config.json&lt;/code&gt; needs to be fixed. Make sure to add the below 2 lines in the file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;thread_delay&#34;: 2,&#xA;&#x9;&#34;unverified_place_frequency&#34;: false,&#xA;&#x9;&#34;proxies&#34;: [&#34;1.1.1.1:8080&#34;, &#34;2.2.2.2:1234&#34;],&#xA;&#x9;&#34;compact_logging&#34;: true&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;thread_delay - Adds a delay between starting a new thread. Can be used to avoid ratelimiting.&lt;/li&gt; &#xA; &lt;li&gt;unverified_place_frequency - Sets the pixel place frequency to the unverified account limit.&lt;/li&gt; &#xA; &lt;li&gt;proxies - Sets proxies to use for sending requests to reddit. The proxy used is randomly selected for each request. Can be used to avoid ratelimiting.&lt;/li&gt; &#xA; &lt;li&gt;compact_logging - Disables timer text until next pixel.&lt;/li&gt; &#xA; &lt;li&gt;Transparency can be achieved by using the RGB value (69, 42, 0) in any part of your image.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;d like, you can enable Verbose Mode by adding &lt;code&gt;--verbose&lt;/code&gt; to &#34;python main.py&#34;. This will output a lot more information, and not neccessarily in the right order, but it is useful for development and debugging.&lt;/li&gt; &#xA; &lt;li&gt;You can also setup proxies by creating a &#34;proxies&#34; and have a new line for each proxies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tor&lt;/h1&gt; &#xA;&lt;p&gt;Tor can be used as an alternative to normal proxies. Note that currently, you cannot use normal proxies and tor at the same time.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&#34;using_tor&#34;: false,&#xA;&#34;tor_ip&#34;: &#34;127.0.0.1&#34;,&#xA;&#34;tor_port&#34;: 1881,&#xA;&#34;tor_control_port&#34;: 9051,&#xA;&#34;tor_password&#34;: &#34;Passwort&#34;,&#xA;&#34;tor_delay&#34;: 5,&#xA;&#34;use_builtin_tor&#34;: true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The config values are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deactivates or activates tor.&lt;/li&gt; &#xA; &lt;li&gt;Sets the ip/hostname of the tor proxy to use&lt;/li&gt; &#xA; &lt;li&gt;Sets the httptunnel port that should be used.&lt;/li&gt; &#xA; &lt;li&gt;Sets the tor control port.&lt;/li&gt; &#xA; &lt;li&gt;Sets the password (leave it as &#34;Passwort&#34; if you want to use the default binaries.&lt;/li&gt; &#xA; &lt;li&gt;The delay that tor should receive to process a new connection.&lt;/li&gt; &#xA; &lt;li&gt;Whether the included tor binary should be used. It is preconfigured. If you want to use your own binary, make sure you configure it properly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that when using the included binaries, only the tunnel port is explicitly set while starting tor.&lt;/p&gt; &#xA;&lt;h3&gt;If you want to use your own binaries, follow these steps:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Get tor standalone for your platform &lt;a href=&#34;https://www.torproject.org/download/tor/&#34;&gt;here&lt;/a&gt;. For Windows just use the expert bundle. For macOS, you can use &lt;a href=&#34;https://brew.sh&#34;&gt;Homebrew&lt;/a&gt; to install tor: &lt;code&gt;brew install tor&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In your tor folder, create a file named &lt;code&gt;torrc&lt;/code&gt;. Copy &lt;a href=&#34;https://github.com/torproject/tor/raw/main/src/config/torrc.sample.in&#34;&gt;this&lt;/a&gt; into it.&lt;/li&gt; &#xA; &lt;li&gt;Search for &lt;code&gt;ControlPort&lt;/code&gt; in your torrc file and uncomment it. Change the port number to your desired control port.&lt;/li&gt; &#xA; &lt;li&gt;Decide on the password you want to use. Run &lt;code&gt;tor --hash-password PASSWORD&lt;/code&gt; from a terminal in the folder with your tor executable, with &#34;PASSWORD&#34; being your desired password. Copy the resulting hash.&lt;/li&gt; &#xA; &lt;li&gt;Search for &lt;code&gt;HashedControlPassword&lt;/code&gt; and uncomment it. Paste the hash value you copied after it.&lt;/li&gt; &#xA; &lt;li&gt;Decide on a port for your httptunnel. The default for this script is 1881.&lt;/li&gt; &#xA; &lt;li&gt;Fill in your password, your httptunnel port and your control port in this script&#39;s &lt;code&gt;config.json&lt;/code&gt; and enable tor with &lt;code&gt;using_tor = true&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To start tor, run &lt;code&gt;tor --defaults-torrc PATHTOTORRC --HttpTunnelPort TUNNELPORT&lt;/code&gt;, with PATHTOTORRC being your path to the torrc file you created and TUNNELPORT being your httptunnel port.&lt;/li&gt; &#xA; &lt;li&gt;Now run the script and (hopefully) everything should work.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;License for the included tor binary:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tor is distributed under the &#34;3-clause BSD&#34; license, a commonly used software license that means Tor is both free software and open source: Copyright (c) 2001-2004, Roger Dingledine Copyright (c) 2004-2006, Roger Dingledine, Nick Mathewson Copyright (c) 2007-2019, The Tor Project, Inc. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.&lt;/li&gt; &#xA;  &lt;li&gt;Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.&lt;/li&gt; &#xA;  &lt;li&gt;Neither the names of the copyright owners nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS &#34;AS IS&#34; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;A dockerfile is provided. Instructions on installing docker are outside the scope of this guide.&lt;/p&gt; &#xA;&lt;p&gt;To build: After editing the &lt;code&gt;config.json&lt;/code&gt; file, run &lt;code&gt;docker build . -t place-bot&lt;/code&gt;. and wait for the image to build.&lt;/p&gt; &#xA;&lt;p&gt;You can now run it with &lt;code&gt;docker run place-bot&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/rdeepak2002/reddit-place-script-2022/main/docs/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference</title>
    <updated>2023-07-23T01:42:43Z</updated>
    <id>tag:github.com,2023-07-23:/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference</id>
    <link href="https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document Q&amp;A&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document Q&amp;amp;A&lt;/h1&gt; &#xA;&lt;h3&gt;Clearly explained guide for running quantized open-source LLM applications on CPUs using LLama 2, C Transformers, GGML, and LangChain&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step-by-step guide on TowardsDataScience&lt;/strong&gt;: &lt;a href=&#34;https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8&#34;&gt;https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Context&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Third-party commercial large language model (LLM) providers like OpenAI&#39;s GPT4 have democratized LLM use via simple API calls.&lt;/li&gt; &#xA; &lt;li&gt;However, there are instances where teams would require self-managed or private model deployment for reasons like data privacy and residency rules.&lt;/li&gt; &#xA; &lt;li&gt;The proliferation of open-source LLMs has opened up a vast range of options for us, thus reducing our reliance on these third-party providers.&amp;nbsp;&lt;/li&gt; &#xA; &lt;li&gt;When we host open-source LLMs locally on-premise or in the cloud, the dedicated compute capacity becomes a key issue. While GPU instances may seem the obvious choice, the costs can easily skyrocket beyond budget.&lt;/li&gt; &#xA; &lt;li&gt;In this project, we will discover how to run quantized versions of open-source LLMs on local CPU inference for document question-and-answer (Q&amp;amp;A). &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference/main/assets/diagram_flow.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure you have downloaded the GGML binary file from &lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&#34;&gt;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&lt;/a&gt; and placed it into the &lt;code&gt;models/&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;To start parsing user queries into the application, launch the terminal from the project directory and run the following command: &lt;code&gt;poetry run python main.py &#34;&amp;lt;user query&amp;gt;&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;For example, &lt;code&gt;poetry run python main.py &#34;What is the minimum guarantee payable by Adidas?&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Note: Omit the prepended &lt;code&gt;poetry run&lt;/code&gt; if you are NOT using Poetry &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference/main/assets/qa_output.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing applications powered by language models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;C Transformers&lt;/strong&gt;: Python bindings for the Transformer models implemented in C/C++ using GGML library&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FAISS&lt;/strong&gt;: Open-source library for efficient similarity search and clustering of dense vectors.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sentence-Transformers (all-MiniLM-L6-v2)&lt;/strong&gt;: Open-source pre-trained transformer model for embedding text to a 384-dimensional dense vector space for tasks like clustering or semantic search.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama-2-7B-Chat&lt;/strong&gt;: Open-source fine-tuned Llama 2 model designed for chat dialogue. Leverages publicly available instruction datasets and over 1 million human annotations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Poetry&lt;/strong&gt;: Tool for dependency management and Python packaging&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Files and Content&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/assets&lt;/code&gt;: Images relevant to the project&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/config&lt;/code&gt;: Configuration files for LLM application&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/data&lt;/code&gt;: Dataset used for this project (i.e., Manchester United FC 2022 Annual Report - 177-page PDF document)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/models&lt;/code&gt;: Binary file of GGML quantized LLM model (i.e., Llama-2-7B-Chat)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/src&lt;/code&gt;: Python codes of key components of LLM application, namely &lt;code&gt;llm.py&lt;/code&gt;, &lt;code&gt;utils.py&lt;/code&gt;, and &lt;code&gt;prompts.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/vectorstore&lt;/code&gt;: FAISS vector store for documents&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;db_build.py&lt;/code&gt;: Python script to ingest dataset and generate FAISS vector store&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt;: Main Python script to launch the application and to pass user query via command line&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pyproject.toml&lt;/code&gt;: TOML file to specify which versions of the dependencies used (Poetry)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;requirements.txt&lt;/code&gt;: List of Python dependencies (and version)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/marella/ctransformers&#34;&gt;https://github.com/marella/ctransformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/TheBloke&#34;&gt;https://huggingface.co/TheBloke&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&#34;&gt;https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/en/latest/integrations/ctransformers.html&#34;&gt;https://python.langchain.com/en/latest/integrations/ctransformers.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/en/latest/modules/models/llms/integrations/ctransformers.html&#34;&gt;https://python.langchain.com/en/latest/modules/models/llms/integrations/ctransformers.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/docs/ecosystem/integrations/ctransformers&#34;&gt;https://python.langchain.com/docs/ecosystem/integrations/ctransformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ggml.ai&#34;&gt;https://ggml.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rustformers/llm/raw/main/crates/ggml/README.md&#34;&gt;https://github.com/rustformers/llm/blob/main/crates/ggml/README.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mdpi.com/2189676&#34;&gt;https://www.mdpi.com/2189676&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>JayZeeDesign/researcher-gpt</title>
    <updated>2023-07-23T01:42:43Z</updated>
    <id>tag:github.com,2023-07-23:/JayZeeDesign/researcher-gpt</id>
    <link href="https://github.com/JayZeeDesign/researcher-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>