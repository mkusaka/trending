<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-03T01:35:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>paulonteri/play-game-with-computer-vision</title>
    <updated>2022-12-03T01:35:51Z</updated>
    <id>tag:github.com,2022-12-03:/paulonteri/play-game-with-computer-vision</id>
    <link href="https://github.com/paulonteri/play-game-with-computer-vision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple python bot (powered by computer vision) used to play City Island. The bot is able to play the game without any human intervention.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gaming using Computer Vision&lt;/h1&gt; &#xA;&lt;p&gt;A python bot used to play a game using computer vision and image processing techniques. The bot is able to play the game without any human intervention. It managed to make me a millionaire in the game overnight!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/45426293/204781699-7213c0de-480b-4068-954c-e76535875709.gif&#34; alt=&#34;play-game-with-computer-vision&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a screen recording of the game play:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/45426293/204757801-eb57b481-af30-45ed-ac0b-1741168783b6.mp4&#34;&gt;https://user-images.githubusercontent.com/45426293/204757801-eb57b481-af30-45ed-ac0b-1741168783b6.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Access a comprehensive guide in this article: &lt;a href=&#34;https://paulonteri.com/thoughts/play-game-with-computer-vision&#34;&gt;https://paulonteri.com/thoughts/play-game-with-computer-vision&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Disclaimer - this content is for educational purposes only!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Why?&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;ve been playing strategy + city building + simulation? games like &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.hg.townsmen6&amp;amp;hl=en&amp;amp;gl=US&#34;&gt;TownsMen 6&lt;/a&gt;, &lt;a href=&#34;https://apps.apple.com/us/app/clash-of-clans/id529479190&#34;&gt;Clash of the Clans&lt;/a&gt;, &lt;a href=&#34;https://store.steampowered.com/app/24780/SimCity_4_Deluxe_Edition/&#34;&gt;SimCity&lt;/a&gt; for the last 10 years.&lt;/p&gt; &#xA;&lt;p&gt;On trying out &lt;a href=&#34;https://apps.apple.com/us/app/city-island-5-tycoon-sim-game/id1445023279?mt=12&#34;&gt;City Island 5&lt;/a&gt; I found it mildly irritating that my collectables could not accumulate while I was outside the game. I might have had the best businesses, strategy, etc but I had to be in the game to ensure I collect the cash/keys/gold overtime. For example, if my bakery makes €100 per minute I would only earn €100 after leaving the game and coming back 24 hours later.&lt;/p&gt; &#xA;&lt;p&gt;This became especially tiresome while trying to accumulate €5,000,000 required to buy the island shown below. This would take me roughly two weeks of gameplay if I don&#39;t spend any money - it&#39;s not worth it.&lt;/p&gt; &#xA;&lt;img width=&#34;995&#34; alt=&#34;game_screenshot_island&#34; src=&#34;https://user-images.githubusercontent.com/45426293/204755494-0fabfa0d-77df-4fa2-9474-9e6ebc8c4ebd.png&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;h3&gt;1. Capture the live game feed&lt;/h3&gt; &#xA;&lt;p&gt;I needed a way to capture the live game feed.&lt;/p&gt; &#xA;&lt;p&gt;The easiest way to to capture an in-game screenshot and pass it to the next steps in the script.&lt;/p&gt; &#xA;&lt;h3&gt;2. Identify the valuables in the screenshot&lt;/h3&gt; &#xA;&lt;p&gt;We need a way to detect a valuable in the game&#39;s feed and then return its coordinates.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;OpenCv&lt;/code&gt;&#39;s &lt;strong&gt;Template Matching&lt;/strong&gt; algorithms are perfect for this.&lt;/p&gt; &#xA;&lt;p&gt;They are used for searching and finding the location of a &lt;em&gt;&lt;strong&gt;template image&lt;/strong&gt;&lt;/em&gt; (like a valuable) in a larger image (like the game&#39;s feed). It simply slides the template image over the input image (as in 2D convolution) and compares the template and patch of the input image under the template image. Several comparison methods are implemented in OpenCV. (You can check &lt;a href=&#34;https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html&#34;&gt;docs&lt;/a&gt; for more details). We use it in the method: &lt;code&gt;cv2.matchTemplate(... )&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To achieve this, I needed the template images. I took screenshots by hand and then cropped off the cash, star and key:&lt;/p&gt; &#xA;&lt;img width=&#34;32&#34; alt=&#34;cash&#34; src=&#34;https://user-images.githubusercontent.com/45426293/204754890-ee25befa-094d-4dfd-829c-1302165d64e0.png&#34;&gt; &#xA;&lt;img width=&#34;32&#34; alt=&#34;key&#34; src=&#34;https://user-images.githubusercontent.com/45426293/204755018-b2773061-5f18-4693-b586-792611f83eb1.png&#34;&gt; &#xA;&lt;img width=&#34;32&#34; alt=&#34;star&#34; src=&#34;https://user-images.githubusercontent.com/45426293/204755064-b68fd852-a0e5-4f11-a29e-276fe955bca1.png&#34;&gt; &#xA;&lt;h3&gt;3. Collect the valuables by clicking on them&lt;/h3&gt; &#xA;&lt;p&gt;Once we have the coordinates of an item we can try to click on it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pyautogui&lt;/code&gt;&#39;s &lt;code&gt;.click(x,y)&lt;/code&gt; function works like magic for this. It clicks the screen on the coordinates x and y where our valuable is lying. Learn more about it &lt;a href=&#34;https://pyautogui.readthedocs.io/en/latest/mouse.html#mouse-clicks&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/45426293/204756803-38e48b98-1945-4ff7-b437-b73e58a97437.mp4&#34;&gt;https://user-images.githubusercontent.com/45426293/204756803-38e48b98-1945-4ff7-b437-b73e58a97437.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/45426293/204758224-7ee70df4-e937-41b7-997e-18092e2fea1e.mp4&#34;&gt;https://user-images.githubusercontent.com/45426293/204758224-7ee70df4-e937-41b7-997e-18092e2fea1e.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/45426293/204758262-b07ade42-7114-4b94-bc37-874536786e0b.mp4&#34;&gt;https://user-images.githubusercontent.com/45426293/204758262-b07ade42-7114-4b94-bc37-874536786e0b.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4. Close any popups that may appear&lt;/h3&gt; &#xA;&lt;p&gt;Our clicks above may result in popups when we are being given a reward, levelling up, etc.&lt;/p&gt; &#xA;&lt;p&gt;We need to close it before attempting to collect valuables again. We use the same logic used in finding and clicking on valuables.&lt;/p&gt; &#xA;&lt;p&gt;To achieve this, I needed the template images for the popups&#39; close buttons so that they can be clicked. I took screenshots by hand and then cropped off the various close buttons:&lt;/p&gt; &#xA;&lt;img width=&#34;32&#34; alt=&#34;close&#34; src=&#34;https://user-images.githubusercontent.com/45426293/204755164-9321de98-465a-455e-8d85-e656b1c7f16b.png&#34;&gt; &#xA;&lt;img width=&#34;189&#34; alt=&#34;continue_level&#34; src=&#34;https://user-images.githubusercontent.com/45426293/204755189-b70fef92-4eea-444c-83be-00a7fbf49c36.png&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/45426293/204756326-4b3db64a-e5e1-4191-83c1-73b9c74b8f7d.mp4&#34;&gt;https://user-images.githubusercontent.com/45426293/204756326-4b3db64a-e5e1-4191-83c1-73b9c74b8f7d.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5. Repeat&lt;/h3&gt; &#xA;&lt;p&gt;We do the steps above repeatedly to collect the valuables while the script is running.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/45426293/204757801-eb57b481-af30-45ed-ac0b-1741168783b6.mp4&#34;&gt;https://user-images.githubusercontent.com/45426293/204757801-eb57b481-af30-45ed-ac0b-1741168783b6.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Results after running overnight&lt;/h2&gt; &#xA;&lt;p&gt;I started the game with €316,415.&lt;/p&gt; &#xA;&lt;img width=&#34;1680&#34; alt=&#34;game_screenshot_start&#34; src=&#34;https://user-images.githubusercontent.com/45426293/204754488-83cfa0ec-b97b-4516-8a81-f7b852dd6008.png&#34;&gt; &#xA;&lt;p&gt;The following morning I had €6,463,870.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/45426293/205040815-ea6d2d0f-0084-4c88-9b02-846bc4690a4a.jpg&#34; alt=&#34;game_screenshot_end&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;I made €6,147,455 overnight!&lt;/p&gt; &#xA;&lt;p&gt;I then proceeded to buy the Island I wanted:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/45426293/204755770-3499c14e-811e-465b-9196-e8a0604c863c.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Regrets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is cheating.&lt;/li&gt; &#xA; &lt;li&gt;Why get a game if you&#39;re not the one playing it?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;p&gt;Current maintainers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Paul Onteri - &lt;a href=&#34;https://paulonteri.com&#34;&gt;https://paulonteri.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Conclusion&lt;/h2&gt; &#xA;&lt;p&gt;Access the full guide in this article here: &lt;a href=&#34;https://paulonteri.com/thoughts/play-game-with-computer-vision&#34;&gt;https://paulonteri.com/thoughts/play-game-with-computer-vision&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This was fun!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kingoflolz/mesh-transformer-jax</title>
    <updated>2022-12-03T01:35:51Z</updated>
    <id>tag:github.com,2022-12-03:/kingoflolz/mesh-transformer-jax</id>
    <link href="https://github.com/kingoflolz/mesh-transformer-jax" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Model parallel transformers in JAX and Haiku&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Table of contents&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#mesh-transformer-jax&#34;&gt;Mesh Transformer JAX&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#updates&#34;&gt;Updates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#pretrained-models&#34;&gt;Pretrained Models&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#gpt-j-6b&#34;&gt;GPT-J-6B&lt;/a&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#links&#34;&gt;Links&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#model-details&#34;&gt;Model Details&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#zero-shot-evaluations&#34;&gt;Zero-Shot Evaluations&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#architecture-and-usage&#34;&gt;Architecture and Usage&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#jax-dependency&#34;&gt;JAX Dependency&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/#todo&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Mesh Transformer JAX&lt;/h1&gt; &#xA;&lt;p&gt;A haiku library using the &lt;code&gt;xmap&lt;/code&gt;/&lt;code&gt;pjit&lt;/code&gt; operators in JAX for model parallelism of transformers.&lt;/p&gt; &#xA;&lt;p&gt;The parallelism scheme is similar to the &lt;a href=&#34;https://arxiv.org/abs/1909.08053&#34;&gt;original Megatron-LM&lt;/a&gt;, which is efficient on TPUs due to the high speed 2d mesh network. There is also an experimental model version which implements &lt;a href=&#34;https://arxiv.org/abs/1910.02054&#34;&gt;ZeRo style sharding&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This library is designed for scalability up to approximately 40B parameters on TPUv3s, beyond which different parallelism strategies should be used. See other implementations such as &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt; or &lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;DeepSpeed&lt;/a&gt; for that.&lt;/p&gt; &#xA;&lt;p&gt;One future direction for research is integrating this codebase with &lt;a href=&#34;https://github.com/kingoflolz/swarm-jax&#34;&gt;swarm-jax&lt;/a&gt;, to achieve further scalability with pipeline parallelism.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;12-07-21&lt;/strong&gt;: Added &lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/howto_finetune.md&#34;&gt;guide to fine tuning&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Pretrained Models&lt;/h1&gt; &#xA;&lt;h2&gt;GPT-J-6B&lt;/h2&gt; &#xA;&lt;p&gt;A 6 billion parameter, autoregressive text generation model trained on &lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;The Pile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Links&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd&#34;&gt;Slim weights (bf16 weights only, for inference, 9GB)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500.tar.zstd&#34;&gt;Full weights (including optimizer params, 61GB)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb&#34;&gt;Colab demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://6b.eleuther.ai/&#34;&gt;Web demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/&#34;&gt;Aran&#39;s blog post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgments&lt;/h3&gt; &#xA;&lt;p&gt;This project would not have been possible without compute generously provided by the &lt;a href=&#34;https://sites.research.google/trc/&#34;&gt;TPU Research Cloud&lt;/a&gt; with assistance from &lt;a href=&#34;https://eleuther.ai/&#34;&gt;EleutherAI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to the Cloud TPU team at Google for providing early access to the Cloud TPU VM alpha (&lt;a href=&#34;https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms&#34;&gt;now publicly available!&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Thanks to everyone who have helped out one way or another (listed alphabetically):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/arankomatsuzaki&#34;&gt;Aran Komatsuzaki&lt;/a&gt; for advice with experiment design and writing the blog posts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/jekbradbury&#34;&gt;James Bradbury&lt;/a&gt; for valuable assistance with debugging JAX issues.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jprester&#34;&gt;Janko Prester&lt;/a&gt; for creating the web demo frontend.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/researcher2&#34;&gt;Laurence Golding&lt;/a&gt; for adding some features to the web demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/nabla_theta&#34;&gt;Leo Gao&lt;/a&gt; for running zero shot evaluations for the baseline models for the table.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;The weights of GPT-J-6B are licensed under version 2.0 of the Apache License.&lt;/p&gt; &#xA;&lt;h3&gt;Model Details&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_parameters&lt;/td&gt; &#xA;   &lt;td&gt;6,053,381,344&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_layers&lt;/td&gt; &#xA;   &lt;td&gt;28*&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;d_model&lt;/td&gt; &#xA;   &lt;td&gt;4,096&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;d_ff&lt;/td&gt; &#xA;   &lt;td&gt;16,384&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_heads&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;d_head&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_ctx&lt;/td&gt; &#xA;   &lt;td&gt;2,048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;n_vocab&lt;/td&gt; &#xA;   &lt;td&gt;50,257 (same tokenizer as GPT-2/3)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;position encoding&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;Rotary position encodings (RoPE)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RoPE dimensions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kingoflolz/mesh-transformer-jax/raw/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223&#34;&gt;64&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;*&lt;/code&gt; each layer consists of one feedforward block and one self attention block&lt;/p&gt; &#xA;&lt;p&gt;The model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary position encodings (RoPE) was applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3.&lt;/p&gt; &#xA;&lt;h3&gt;Zero-Shot Evaluations&lt;/h3&gt; &#xA;&lt;p&gt;Models roughly sorted by performance, or by FLOPs if not available.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;   &lt;th&gt;Training FLOPs&lt;/th&gt; &#xA;   &lt;th&gt;LAMBADA PPL ↓&lt;/th&gt; &#xA;   &lt;th&gt;LAMBADA Acc ↑&lt;/th&gt; &#xA;   &lt;th&gt;Winogrande ↑&lt;/th&gt; &#xA;   &lt;th&gt;Hellaswag ↑&lt;/th&gt; &#xA;   &lt;th&gt;PIQA ↑&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Size (GB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chance&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;~a lot&lt;/td&gt; &#xA;   &lt;td&gt;~0%&lt;/td&gt; &#xA;   &lt;td&gt;50%&lt;/td&gt; &#xA;   &lt;td&gt;25%&lt;/td&gt; &#xA;   &lt;td&gt;25%&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-Ada‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;9.95&lt;/td&gt; &#xA;   &lt;td&gt;51.6%&lt;/td&gt; &#xA;   &lt;td&gt;52.9%&lt;/td&gt; &#xA;   &lt;td&gt;43.4%&lt;/td&gt; &#xA;   &lt;td&gt;70.5%&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2-1.5B&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;10.63&lt;/td&gt; &#xA;   &lt;td&gt;51.21%&lt;/td&gt; &#xA;   &lt;td&gt;59.4%&lt;/td&gt; &#xA;   &lt;td&gt;50.9%&lt;/td&gt; &#xA;   &lt;td&gt;70.8%&lt;/td&gt; &#xA;   &lt;td&gt;40&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTNeo-1.3B‡&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;3.0e21&lt;/td&gt; &#xA;   &lt;td&gt;7.50&lt;/td&gt; &#xA;   &lt;td&gt;57.2%&lt;/td&gt; &#xA;   &lt;td&gt;55.0%&lt;/td&gt; &#xA;   &lt;td&gt;48.9%&lt;/td&gt; &#xA;   &lt;td&gt;71.1%&lt;/td&gt; &#xA;   &lt;td&gt;825&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Megatron-2.5B*&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;2.4e21&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;61.7%&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;174&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTNeo-2.7B‡&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;6.8e21&lt;/td&gt; &#xA;   &lt;td&gt;5.63&lt;/td&gt; &#xA;   &lt;td&gt;62.2%&lt;/td&gt; &#xA;   &lt;td&gt;56.5%&lt;/td&gt; &#xA;   &lt;td&gt;55.8%&lt;/td&gt; &#xA;   &lt;td&gt;73.0%&lt;/td&gt; &#xA;   &lt;td&gt;825&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-1.3B*‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;2.4e21&lt;/td&gt; &#xA;   &lt;td&gt;5.44&lt;/td&gt; &#xA;   &lt;td&gt;63.6%&lt;/td&gt; &#xA;   &lt;td&gt;58.7%&lt;/td&gt; &#xA;   &lt;td&gt;54.7%&lt;/td&gt; &#xA;   &lt;td&gt;75.1%&lt;/td&gt; &#xA;   &lt;td&gt;~800&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-Babbage‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;5.58&lt;/td&gt; &#xA;   &lt;td&gt;62.4%&lt;/td&gt; &#xA;   &lt;td&gt;59.0%&lt;/td&gt; &#xA;   &lt;td&gt;54.5%&lt;/td&gt; &#xA;   &lt;td&gt;75.5%&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Megatron-8.3B*&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;7.8e21&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;66.5%&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;174&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-2.7B*‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;4.8e21&lt;/td&gt; &#xA;   &lt;td&gt;4.60&lt;/td&gt; &#xA;   &lt;td&gt;67.1%&lt;/td&gt; &#xA;   &lt;td&gt;62.3%&lt;/td&gt; &#xA;   &lt;td&gt;62.8%&lt;/td&gt; &#xA;   &lt;td&gt;75.6%&lt;/td&gt; &#xA;   &lt;td&gt;~800&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Megatron-11B†&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;1.0e22&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;161&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-J-6B&lt;/strong&gt;‡&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;1.5e22&lt;/td&gt; &#xA;   &lt;td&gt;3.99&lt;/td&gt; &#xA;   &lt;td&gt;69.7%&lt;/td&gt; &#xA;   &lt;td&gt;65.3%&lt;/td&gt; &#xA;   &lt;td&gt;66.1%&lt;/td&gt; &#xA;   &lt;td&gt;76.5%&lt;/td&gt; &#xA;   &lt;td&gt;825&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-6.7B*‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;1.2e22&lt;/td&gt; &#xA;   &lt;td&gt;4.00&lt;/td&gt; &#xA;   &lt;td&gt;70.3%&lt;/td&gt; &#xA;   &lt;td&gt;64.5%&lt;/td&gt; &#xA;   &lt;td&gt;67.4%&lt;/td&gt; &#xA;   &lt;td&gt;78.0%&lt;/td&gt; &#xA;   &lt;td&gt;~800&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-Curie‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;4.00&lt;/td&gt; &#xA;   &lt;td&gt;69.3%&lt;/td&gt; &#xA;   &lt;td&gt;65.6%&lt;/td&gt; &#xA;   &lt;td&gt;68.5%&lt;/td&gt; &#xA;   &lt;td&gt;77.9%&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-13B*‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;2.3e22&lt;/td&gt; &#xA;   &lt;td&gt;3.56&lt;/td&gt; &#xA;   &lt;td&gt;72.5%&lt;/td&gt; &#xA;   &lt;td&gt;67.9%&lt;/td&gt; &#xA;   &lt;td&gt;70.9%&lt;/td&gt; &#xA;   &lt;td&gt;78.5%&lt;/td&gt; &#xA;   &lt;td&gt;~800&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-175B*‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;3.1e23&lt;/td&gt; &#xA;   &lt;td&gt;3.00&lt;/td&gt; &#xA;   &lt;td&gt;76.2%&lt;/td&gt; &#xA;   &lt;td&gt;70.2%&lt;/td&gt; &#xA;   &lt;td&gt;78.9%&lt;/td&gt; &#xA;   &lt;td&gt;81.0%&lt;/td&gt; &#xA;   &lt;td&gt;~800&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3-Davinci‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;75%&lt;/td&gt; &#xA;   &lt;td&gt;72%&lt;/td&gt; &#xA;   &lt;td&gt;78%&lt;/td&gt; &#xA;   &lt;td&gt;80%&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gopher 230B*&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;6.31E+23&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;74.50%&lt;/td&gt; &#xA;   &lt;td&gt;70.10%&lt;/td&gt; &#xA;   &lt;td&gt;79.20%&lt;/td&gt; &#xA;   &lt;td&gt;81.80%&lt;/td&gt; &#xA;   &lt;td&gt;1344&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MT-NLG 530B*‡&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;   &lt;td&gt;76.6%&lt;/td&gt; &#xA;   &lt;td&gt;73.0%&lt;/td&gt; &#xA;   &lt;td&gt;80.2%&lt;/td&gt; &#xA;   &lt;td&gt;82.0%&lt;/td&gt; &#xA;   &lt;td&gt;-----&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;*&lt;/code&gt; represents evaluation numbers reported by their respective authors, all other numbers are provided by running the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/&#34;&gt;lm-evaluation-harness&lt;/a&gt; either with the released weights or with API access. Due to subtle implementation differences as well as different zero shot task framing, these might not be directly comparable. See &lt;a href=&#34;https://www.eleuther.ai/research-log/gpt3-model-sizes/&#34;&gt;this blog post&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;†&lt;/code&gt; The Megatron-11B model provides no comparable metrics, and several implementations using the released weights do not reproduce the generation quality and evaluations. (see &lt;a href=&#34;https://github.com/huggingface/transformers/pull/10301&#34;&gt;1&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/fairseq/issues/2358&#34;&gt;2&lt;/a&gt; &lt;a href=&#34;https://github.com/pytorch/fairseq/issues/2719&#34;&gt;3&lt;/a&gt;) Thus, evaluation was not attempted.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;‡&lt;/code&gt; These models have been trained with data which contains possible test set contamination. The OpenAI GPT-3 models failed to deduplicate training data for certain test sets, while the GPT-Neo models as well as this one is trained on The Pile, which has not been deduplicated against any test sets.&lt;/p&gt; &#xA;&lt;h1&gt;Architecture and Usage&lt;/h1&gt; &#xA;&lt;p&gt;Most scripts in this repository are designed to be run on TPUs, which under the &lt;a href=&#34;https://cloud.google.com/tpu/docs/system-architecture-tpu-vm&#34;&gt;TPU-VM architecture&lt;/a&gt; are virtual machines which can run arbitrary code. Most scripts are designed to spin up a TPU, SSH into it to set up the dependencies and copy code over from the local directory, and then start a &lt;a href=&#34;https://github.com/ray-project/ray.git&#34;&gt;Ray&lt;/a&gt; worker which can accept RPC calls.&lt;/p&gt; &#xA;&lt;p&gt;The TPUVMs handles running model training steps and evaluation, checkpoint save and loading, while the driver python program handles data loading and general orchestration (such as when to save checkpoints etc).&lt;/p&gt; &#xA;&lt;p&gt;This means that most scripts (&lt;code&gt;train.py&lt;/code&gt;, &lt;code&gt;eval_harness.py&lt;/code&gt; etc) expect to be running on a GCE virtual machine in the same region as the TPUs, to minimize RPC latency and data transfer cost. Other scripts (usually ones which don&#39;t take a &lt;code&gt;--tpu&lt;/code&gt; argument, such as &lt;code&gt;device_sample.py&lt;/code&gt;, &lt;code&gt;device_serve.py&lt;/code&gt; or &lt;code&gt;device_train.py&lt;/code&gt;) expect to be run directly on a TPUVM. The device_* scripts &lt;strong&gt;only work on a v3-8&lt;/strong&gt; and not on larger pods.&lt;/p&gt; &#xA;&lt;p&gt;Furthermore, there is an example (&lt;code&gt;resharding_example.py&lt;/code&gt;) of how to convert the provided checkpoints (which have 8 shards in the case of GPT-J-6B) down to a smaller number, such as for when running on GPU(s).&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;To fine-tune the model, run &lt;code&gt;device_train.py&lt;/code&gt; on a TPU VM. Using a TPU v3-8, you can fine-tune at a rate of ~5000 tokens/second, which should be sufficient for small-to-medium-size datasets.&lt;/p&gt; &#xA;&lt;p&gt;Please read the &lt;a href=&#34;https://raw.githubusercontent.com/kingoflolz/mesh-transformer-jax/master/howto_finetune.md&#34;&gt;step by step guide&lt;/a&gt; for thorough fine-tuning instructions.&lt;/p&gt; &#xA;&lt;h3&gt;JAX Dependency&lt;/h3&gt; &#xA;&lt;p&gt;Note this library has some specific requirements for JAX version. Specifically, to use the v1 models (including GPT-J 6B), &lt;code&gt;jax==0.2.12&lt;/code&gt; is required. This in turn depends on &lt;code&gt;jaxlib==0.1.68&lt;/code&gt;. &lt;strong&gt;If this is not done, you will get cryptic xmap errors&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;However, to use the v2 model code (no publicly released weights), the newest JAX version can be used.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;To cite this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{mesh-transformer-jax,&#xA;  author = {Wang, Ben},&#xA;  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},&#xA;  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},&#xA;  year = 2021,&#xA;  month = May&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To cite the weights of GPT-J-6B:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{gpt-j,&#xA;  author = {Wang, Ben and Komatsuzaki, Aran},&#xA;  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},&#xA;  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},&#xA;  year = 2021,&#xA;  month = May&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use this repository or any of the pretrained weights to do something cool, we would love to hear about it. Feel free to open a github issue or reach out over email (in profile).&lt;/p&gt; &#xA;&lt;h1&gt;TODO&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; disentangle heads and shards&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; test/benchmark on TPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; implement gradient checkpointing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; fix initialization&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; mixed precision&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; deal with preemptible TPUs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; test and validate generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; shard activations instead of replicating for memory efficiency (in v2)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; support ZeRO style sharding (in v2)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmyolo</title>
    <updated>2022-12-03T01:35:51Z</updated>
    <id>tag:github.com,2022-12-03:/open-mmlab/mmyolo</id>
    <link href="https://github.com/open-mmlab/mmyolo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab YOLO series toolbox and benchmark&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/resources/mmyolo-logo.png&#34; width=&#34;600&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmyolo&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mmyolo&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmyolo/workflows/deploy/badge.svg?sanitize=true&#34; alt=&#34;deploy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmyolo&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmyolo/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmyolo.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/&#34;&gt;📘Documentation&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/get_started.html&#34;&gt;🛠️Installation&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/model_zoo.html&#34;&gt;👀Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/notes/changelog.html&#34;&gt;🆕Update News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/issues/new/choose&#34;&gt;🤔Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is an open source toolbox for YOLO series algorithms based on PyTorch and &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.6+&lt;/strong&gt;. &lt;img src=&#34;https://user-images.githubusercontent.com/45811724/190993591-bd3f1f11-1c30-4b93-b5f4-05c9ff64ff7f.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Major features&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified and convenient benchmark&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO unifies the implementation of modules in various YOLO algorithms and provides a unified benchmark. Users can compare and analyze in a fair and convenient way.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rich and detailed documentation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO provides rich documentation for getting started, model deployment, advanced usages, and algorithm analysis, making it easy for users at different levels to get started and make extensions quickly.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Design&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMYOLO decomposes the framework into different components where users can easily customize a model by combining different modules with various training and testing strategies.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/27466624/199999337-0544a4cb-3cbd-4f3e-be26-bcd9e74db7ff.jpg&#34; alt=&#34;BaseModule-P5&#34;&gt; The figure above is contributed by RangeKing@GitHub, thank you very much! &#xA; &lt;p&gt;And the figure of P6 model is in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/model_design.md&#34;&gt;model_design.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;p&gt;💎 &lt;strong&gt;v0.2.0&lt;/strong&gt; was released on 1/12/2022:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/tree/dev/configs/yolov7&#34;&gt;YOLOv7&lt;/a&gt; P5 and P6 model&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/configs/yolov6/README.md&#34;&gt;YOLOv6&lt;/a&gt; ML model&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/demo/boxam_vis_demo.py&#34;&gt;Grad-Based CAM and Grad-Free CAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/demo/large_image_demo.py&#34;&gt;large image inference&lt;/a&gt; based on sahi&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/projects/easydeploy/README.md&#34;&gt;easydeploy&lt;/a&gt; project under the projects folder&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/raw/dev/docs/zh_cn/user_guides/custom_dataset.md&#34;&gt;custom dataset guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For release history and update details, please refer to &lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/notes/changelog.html&#34;&gt;changelog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO relies on PyTorch, MMCV, MMEngine, and MMDetection. Below are quick steps for installation. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/get_started.md&#34;&gt;Install Guide&lt;/a&gt; for more detailed instructions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y&#xA;conda activate open-mmlab&#xA;pip install openmim&#xA;mim install &#34;mmengine&amp;gt;=0.3.1&#34;&#xA;mim install &#34;mmcv&amp;gt;=2.0.0rc1,&amp;lt;2.1.0&#34;&#xA;mim install &#34;mmdet&amp;gt;=3.0.0rc3,&amp;lt;3.1.0&#34;&#xA;git clone https://github.com/open-mmlab/mmyolo.git&#xA;cd mmyolo&#xA;# Install albumentations&#xA;pip install -r requirements/albu.txt&#xA;# Install MMYOLO&#xA;mim install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is based on MMDetection and adopts the same code structure and design approach. To get better use of this, please read &lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/get_started.html&#34;&gt;MMDetection Overview&lt;/a&gt; for the first understanding of MMDetection.&lt;/p&gt; &#xA;&lt;p&gt;The usage of MMYOLO is almost identical to MMDetection and all tutorials are straightforward to use, you can also learn about &lt;a href=&#34;https://mmdetection.readthedocs.io/en/3.x/&#34;&gt;MMDetection User Guide and Advanced Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For different parts from MMDetection, we have also prepared user guides and advanced guides, please read our &lt;a href=&#34;https://mmyolo.readthedocs.io/zenh_CN/latest/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;User Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/user_guides/index.html#train-test&#34;&gt;Train &amp;amp; Test&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/config.md&#34;&gt;Learn about Configs with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/user_guides/index.html#get-started-to-deployment&#34;&gt;From getting started to deployment&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/yolov5_tutorial.md&#34;&gt;From getting started to deployment with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/user_guides/index.html#useful-tools&#34;&gt;Useful Tools&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/visualization.md&#34;&gt;Visualization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/useful_tools.md&#34;&gt;Useful Tools&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/user_guides/custom_dataset.md&#34;&gt;Custom Dataset&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Algorithm description&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#essential-basics&#34;&gt;Essential Basics&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/model_design.md&#34;&gt;Model design-related instructions&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#algorithm-principles-and-implementation&#34;&gt;Algorithm principles and implementation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/algorithm_descriptions/yolov5_description.md&#34;&gt;Algorithm principles and implementation with YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deployment Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#basic-deployment-guide&#34;&gt;Basic Deployment Guide&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/deploy/basic_deployment_guide.md&#34;&gt;Basic Deployment Guide&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mmyolo.readthedocs.io/en/latest/algorithm_descriptions/index.html#deployment-tutorial&#34;&gt;Deployment Tutorial&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/deploy/yolov5_deployment.md&#34;&gt;YOLOv5 Deployment&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Advanced Guides&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/data_flow.md&#34;&gt;Data flow&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/how_to.md&#34;&gt;How to&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/advanced_guides/plugins.md&#34;&gt;Plugins&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview of Benchmark and Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Results and models are available in the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/model_zoo.md&#34;&gt;model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;b&gt;Supported Algorithms&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov5&#34;&gt;YOLOv5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolox&#34;&gt;YOLOX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/rtmdet&#34;&gt;RTMDet&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov6&#34;&gt;YOLOv6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/yolov7&#34;&gt;YOLOv7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/configs/ppyoloe&#34;&gt;PPYOLOE&lt;/a&gt;(Inference only)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;Module Components&lt;/b&gt; &#xA; &lt;/div&gt; &#xA; &lt;table align=&#34;center&#34;&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;    &lt;td&gt; &lt;b&gt;Backbones&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Necks&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Loss&lt;/b&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &lt;b&gt;Common&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr valign=&#34;top&#34;&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;YOLOv5CSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;YOLOXCSPDarknet&lt;/li&gt; &#xA;      &lt;li&gt;EfficientRep&lt;/li&gt; &#xA;      &lt;li&gt;CSPNeXt&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;YOLOv5PAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOv6RepPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;YOLOXPAFPN&lt;/li&gt; &#xA;      &lt;li&gt;CSPNeXtPAFPN&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;IoULoss&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;    &lt;td&gt; &#xA;     &lt;ul&gt; &#xA;     &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;/tr&gt;   &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/docs/en/notes/faq.md&#34;&gt;FAQ&lt;/a&gt; for frequently asked questions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improving MMYOLO. Ongoing projects can be found in our &lt;a href=&#34;https://github.com/open-mmlab/mmyolo/projects&#34;&gt;GitHub Projects&lt;/a&gt;. Welcome community users to participate in these projects. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/.github/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMYOLO is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedback. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{mmyolo2022,&#xA;    title={{MMYOLO: OpenMMLab YOLO} series toolbox and benchmark},&#xA;    author={MMYOLO Contributors},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmyolo}},&#xA;    year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmyolo/main/LICENSE&#34;&gt;GPL 3.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Projects in OpenMMLab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt;: OpenMMLab foundational library for training deep learning models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34;&gt;MMYOLO&lt;/a&gt;: OpenMMLab YOLO series toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmeval&#34;&gt;MMEval&lt;/a&gt;: OpenMMLab machine learning evaluation library.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>