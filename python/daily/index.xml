<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-06T01:34:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QuivrHQ/MegaParse</title>
    <updated>2024-12-06T01:34:35Z</updated>
    <id>tag:github.com,2024-12-06:/QuivrHQ/MegaParse</id>
    <link href="https://github.com/QuivrHQ/MegaParse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;File Parser optimised for LLM Ingestion with no loss üß† Parse PDFs, Docx, PPTx in a format that is ideal for LLMs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MegaParse - Your Parser for every type of documents&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/QuivrHQ/MegaParse/main/logo.png&#34; alt=&#34;Quivr-logo&#34; width=&#34;30%&#34; style=&#34;border-radius: 50%; padding-bottom: 20px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;MegaParse is a powerful and versatile parser that can handle various types of documents with ease. Whether you&#39;re dealing with text, PDFs, Powerpoint presentations, Word documents MegaParse has got you covered. Focus on having no information loss during parsing.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features üéØ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versatile Parser&lt;/strong&gt;: MegaParse is a powerful and versatile parser that can handle various types of documents with ease.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;No Information Loss&lt;/strong&gt;: Focus on having no information loss during parsing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast and Efficient&lt;/strong&gt;: Designed with speed and efficiency at its core.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wide File Compatibility&lt;/strong&gt;: Supports Text, PDF, Powerpoint presentations, Excel, CSV, Word documents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Freedom is beautiful, and so is MegaParse. Open source and free to use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Files: ‚úÖ PDF ‚úÖ Powerpoint ‚úÖ Word&lt;/li&gt; &#xA; &lt;li&gt;Content: ‚úÖ Tables ‚úÖ TOC ‚úÖ Headers ‚úÖ Footers ‚úÖ Images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/QuivrHQ/MegaParse/assets/19614572/1b4cdb73-8dc2-44ef-b8b4-a7509bc8d4f3&#34;&gt;https://github.com/QuivrHQ/MegaParse/assets/19614572/1b4cdb73-8dc2-44ef-b8b4-a7509bc8d4f3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install megaparse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your OpenAI or Anthropic API key to the .env file&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install poppler on your computer (images and PDFs)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install tesseract on your computer (images and PDFs)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you have a mac, you also need to install libmagic &lt;code&gt;brew install libmagic&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from megaparse import MegaParse&#xA;from langchain_openai import ChatOpenAI&#xA;from megaparse.parser.unstructured_parser import UnstructuredParser&#xA;&#xA;parser = UnstructuredParser()&#xA;megaparse = MegaParse(parser)&#xA;response = megaparse.load(&#34;./test.pdf&#34;)&#xA;print(response)&#xA;megaparse.save(&#34;./test.md&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use MegaParse Vision&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change the parser to MegaParseVision&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from megaparse import MegaParse&#xA;from langchain_openai import ChatOpenAI&#xA;from megaparse.parser.megaparse_vision import MegaParseVision&#xA;&#xA;model = ChatOpenAI(model=&#34;gpt-4o&#34;, api_key=os.getenv(&#34;OPENAI_API_KEY&#34;))  # type: ignore&#xA;parser = MegaParseVision(model=model)&#xA;megaparse = MegaParse(parser)&#xA;response = megaparse.load(&#34;./test.pdf&#34;)&#xA;print(response)&#xA;megaparse.save(&#34;./test.md&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The model supported by MegaParse Vision are the multimodal ones such as claude 3.5, claude 4, gpt-4o and gpt-4.&lt;/p&gt; &#xA;&lt;h3&gt;(Optional) Use LlamaParse for Improved Results&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create an account on &lt;a href=&#34;https://cloud.llamaindex.ai/&#34;&gt;Llama Cloud&lt;/a&gt; and get your API key.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Change the parser to LlamaParser&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from megaparse import MegaParse&#xA;from langchain_openai import ChatOpenAI&#xA;from megaparse.parser.llama_parser import LlamaParser&#xA;&#xA;parser = LlamaParser(api_key = os.getenv(&#34;LLAMA_CLOUD_API_KEY&#34;))&#xA;megaparse = MegaParse(parser)&#xA;response = megaparse.load(&#34;./test.pdf&#34;)&#xA;print(response)&#xA;megaparse.save(&#34;./test.md&#34;) #saves the last processed doc in md format&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Use as an API&lt;/h2&gt; &#xA;&lt;p&gt;There is a MakeFile for you, simply use : &lt;code&gt;make dev&lt;/code&gt; at the root of the project and you are good to go.&lt;/p&gt; &#xA;&lt;p&gt;See localhost:8000/docs for more info on the different endpoints !&lt;/p&gt; &#xA;&lt;h2&gt;BenchMark&lt;/h2&gt; &#xA;&lt;!--BENCHMARK--&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parser&lt;/th&gt; &#xA;   &lt;th&gt;similarity_ratio&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;megaparse_vision&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;unstructured_with_check_table&lt;/td&gt; &#xA;   &lt;td&gt;0.77&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;unstructured&lt;/td&gt; &#xA;   &lt;td&gt;0.59&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama_parser&lt;/td&gt; &#xA;   &lt;td&gt;0.33&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!--END_BENCHMARK--&gt; &#xA;&lt;p&gt;&lt;em&gt;Higher the better&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: Want to evaluate and compare your Megaparse module with ours ? Please add your config in &lt;code&gt;evaluations/script.py&lt;/code&gt; and then run &lt;code&gt;python evaluations/script.py&lt;/code&gt;. If it is better, do a PR, I mean, let&#39;s go higher together .&lt;/p&gt; &#xA;&lt;h2&gt;In Construction üöß&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improve table checker&lt;/li&gt; &#xA; &lt;li&gt;Create Checkers to add &lt;strong&gt;modular postprocessing&lt;/strong&gt; ‚öôÔ∏è&lt;/li&gt; &#xA; &lt;li&gt;Add Structured output, &lt;strong&gt;let&#39;s get computer talking&lt;/strong&gt; ü§ñ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#QuivrHQ/MegaParse&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=QuivrHQ/MegaParse&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IntelRealSense/realsense-ros</title>
    <updated>2024-12-06T01:34:35Z</updated>
    <id>tag:github.com,2024-12-06:/IntelRealSense/realsense-ros</id>
    <link href="https://github.com/IntelRealSense/realsense-ros" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ROS Wrapper for Intel(R) RealSense(TM) Cameras&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://www.intelrealsense.com/wp-content/uploads/2020/09/intel-realsense-logo-360px.png&#34; alt=&#34;Intel¬Æ RealSense‚Ñ¢&#34; title=&#34;Intel¬Æ RealSense‚Ñ¢&#34;&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ROS Wrapper for Intel(R) RealSense(TM) Cameras&lt;br&gt; &lt;a href=&#34;https://github.com/IntelRealSense/realsense-ros/releases&#34;&gt;Latest release notes&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.ros.org/en/rolling/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-ROLLING-orange?style=flat-square&amp;amp;logo=ros&#34; alt=&#34;rolling&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.ros.org/en/iron/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-IRON-orange?style=flat-square&amp;amp;logo=ros&#34; alt=&#34;iron&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.ros.org/en/humble/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-HUMBLE-orange?style=flat-square&amp;amp;logo=ros&#34; alt=&#34;humble&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.ros.org/en/foxy/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-foxy-orange?style=flat-square&amp;amp;logo=ros&#34; alt=&#34;foxy&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://releases.ubuntu.com/jammy/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-UBUNTU%2022%2E04-blue?style=flat-square&amp;amp;logo=ubuntu&amp;amp;logoColor=white&#34; alt=&#34;ubuntu22&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://releases.ubuntu.com/focal/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-UBUNTU%2020%2E04-blue?style=flat-square&amp;amp;logo=ubuntu&amp;amp;logoColor=white&#34; alt=&#34;ubuntu20&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/IntelRealSense/realsense-ros/main.yml?logo=github&amp;amp;style=flat-square&#34; alt=&#34;GitHubWorkflowStatus&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/CONTRIBUTING.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/IntelRealSense/realsense-ros?style=flat-square&#34; alt=&#34;GitHubcontributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/IntelRealSense/realsense-ros?style=flat-square&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#ros1-and-ros2-legacy&#34;&gt;ROS1 and ROS2 legacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#installation-on-ubuntu&#34;&gt;Installation on Ubuntu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#installation-on-windows&#34;&gt;Installation on Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#start-the-camera-node&#34;&gt;Starting the camera node&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#camera-name-and-camera-namespace&#34;&gt;Camera name and namespace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#ros2robot-vs-opticalcamera-coordination-systems&#34;&gt;ROS2-vs-Optical Coordination Systems&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#tf-from-coordinate-a-to-coordinate-b&#34;&gt;TF from coordinate A to coordinate B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#extrinsics-from-sensor-a-to-sensor-b&#34;&gt;Extrinsics from sensor A to sensor B&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#published-topics&#34;&gt;Topics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#rgbd-topic&#34;&gt;RGBD Topic&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#metadata-topic&#34;&gt;Metadata Topic&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#post-processing-filters&#34;&gt;Post-Processing Filters&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#available-services&#34;&gt;Available Services&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/#efficient-intra-process-communication&#34;&gt;Efficient intra-process communication&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IntelRealSense/realsense-ros/ros2-master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ROS1 and ROS2 Legacy&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Intel RealSense ROS1 Wrapper &lt;/summary&gt; Intel Realsense ROS1 Wrapper is not supported anymore, since our developers team are focusing on ROS2 distro.&#xA; &lt;br&gt; For ROS1 wrapper, go to &#xA; &lt;a href=&#34;https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy&#34;&gt;ros1-legacy&lt;/a&gt; branch &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Moving from &lt;a href=&#34;https://github.com/IntelRealSense/realsense-ros/tree/ros2-legacy&#34;&gt;ros2-legacy&lt;/a&gt; to ros2-master &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Changed Parameters: &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&#34;stereo_module&#34;&lt;/strong&gt;, &lt;strong&gt;&#34;l500_depth_sensor&#34;&lt;/strong&gt; are replaced by &lt;strong&gt;&#34;depth_module&#34;&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;li&gt;For video streams: &lt;strong&gt;&amp;lt;module&amp;gt;.profile&lt;/strong&gt; replaces &lt;strong&gt;&amp;lt;stream&amp;gt;_width&lt;/strong&gt;, &lt;strong&gt;&amp;lt;stream&amp;gt;_height&lt;/strong&gt;, &lt;strong&gt;&amp;lt;stream&amp;gt;_fps&lt;/strong&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;strong&gt;ROS2-legacy (Old)&lt;/strong&gt;: &#xA;       &lt;ul&gt; &#xA;        &lt;li&gt;ros2 launch realsense2_camera rs_launch.py depth_width:=640 depth_height:=480 depth_fps:=30.0 infra1_width:=640 infra1_height:=480 infra1_fps:=30.0&lt;/li&gt; &#xA;       &lt;/ul&gt; &lt;/li&gt; &#xA;      &lt;li&gt;&lt;strong&gt;ROS2-master (New)&lt;/strong&gt;: &#xA;       &lt;ul&gt; &#xA;        &lt;li&gt;ros2 launch realsense2_camera rs_launch.py depth_module.profile:=640x480x30&lt;/li&gt; &#xA;       &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Removed paramets &lt;strong&gt;&amp;lt;stream&amp;gt;_frame_id&lt;/strong&gt;, &lt;strong&gt;&amp;lt;stream&amp;gt;_optical_frame_id&lt;/strong&gt;. frame_ids are now defined by camera_name&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&#34;filters&#34;&lt;/strong&gt; is removed. All filters (or post-processing blocks) are enabled/disabled using &lt;strong&gt;&#34;&amp;lt;filter&amp;gt;.enable&#34;&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&#34;align_depth&#34;&lt;/strong&gt; is now a regular processing block and as such the parameter for enabling it is replaced with &lt;strong&gt;&#34;align_depth.enable&#34;&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&#34;allow_no_texture_points&#34;&lt;/strong&gt;, &lt;strong&gt;&#34;ordered_pc&#34;&lt;/strong&gt; are now belong to the pointcloud filter and as such are replaced by &lt;strong&gt;&#34;pointcloud.allow_no_texture_points&#34;&lt;/strong&gt;, &lt;strong&gt;&#34;pointcloud.ordered_pc&#34;&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&#34;pointcloud_texture_stream&#34;&lt;/strong&gt;, &lt;strong&gt;&#34;pointcloud_texture_index&#34;&lt;/strong&gt; belong now to the pointcloud filter and were renamed to match their librealsense&#39; names: &lt;strong&gt;&#34;pointcloud.stream_filter&#34;&lt;/strong&gt;, &lt;strong&gt;&#34;pointcloud.stream_index_filter&#34;&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Allow enable/disable of sensors in runtime (parameters &lt;strong&gt;&amp;lt;stream&amp;gt;.enable&lt;/strong&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;Allow enable/disable of filters in runtime (parameters &lt;strong&gt;&amp;lt;filter_name&amp;gt;.enable&lt;/strong&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;unite_imu_method&lt;/strong&gt; parameter is now changeable in runtime.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;enable_sync&lt;/strong&gt; parameter is now changeable in runtime.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;Installation on Ubuntu&lt;/h1&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Step 1: Install the ROS2 distribution &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;h4&gt;Ubuntu 22.04:&lt;/h4&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://docs.ros.org/en/iron/Installation/Ubuntu-Install-Debians.html&#34;&gt;ROS2 Iron&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debians.html&#34;&gt;ROS2 Humble&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;h4&gt;Ubuntu 20.04&lt;/h4&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://docs.ros.org/en/foxy/Installation/Ubuntu-Install-Debians.html&#34;&gt;ROS2 Foxy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Step 2: Install latest Intel¬Æ RealSense‚Ñ¢ SDK 2.0 &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Please choose only one option from the 3 options below (in order to prevent multiple versions installation and workspace conflicts)&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;h4&gt;Option 1: Install librealsense2 debian package from Intel servers&lt;/h4&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Jetson users - use the &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/installation_jetson.md&#34;&gt;Jetson Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Otherwise, install from &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/distribution_linux.md#installing-the-packages&#34;&gt;Linux Debian Installation Guide&lt;/a&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;In this case treat yourself as a developer: make sure to follow the instructions to also install librealsense2-dev and librealsense2-dkms packages&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;h4&gt;Option 2: Install librealsense2 (without graphical tools and examples) debian package from ROS servers (Foxy EOL distro is not supported by this option):&lt;/h4&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/Installation/Ubuntu/Sources&#34;&gt;Configure&lt;/a&gt; your Ubuntu repositories&lt;/li&gt; &#xA;    &lt;li&gt;Install all realsense ROS packages by &lt;code&gt;sudo apt install ros-&amp;lt;ROS_DISTRO&amp;gt;-librealsense2*&lt;/code&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;For example, for Humble distro: &lt;code&gt;sudo apt install ros-humble-librealsense2*&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;h4&gt;Option 3: Build from source&lt;/h4&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Download the latest &lt;a href=&#34;https://github.com/IntelRealSense/librealsense&#34;&gt;Intel¬Æ RealSense‚Ñ¢ SDK 2.0&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Follow the instructions under &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/installation.md&#34;&gt;Linux Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Step 3: Install Intel¬Æ RealSense‚Ñ¢ ROS2 wrapper &lt;/summary&gt; &#xA; &lt;h4&gt;Option 1: Install debian package from ROS servers (Foxy EOL distro is not supported by this option):&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;http://wiki.ros.org/Installation/Ubuntu/Sources&#34;&gt;Configure&lt;/a&gt; your Ubuntu repositories&lt;/li&gt; &#xA;  &lt;li&gt;Install all realsense ROS packages by &lt;code&gt;sudo apt install ros-&amp;lt;ROS_DISTRO&amp;gt;-realsense2-*&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;For example, for Humble distro: &lt;code&gt;sudo apt install ros-humble-realsense2-*&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Option 2: Install from source&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Create a ROS2 workspace&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/ros2_ws/src&#xA;cd ~/ros2_ws/src/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Clone the latest ROS2 Intel¬Æ RealSense‚Ñ¢ wrapper from &lt;a href=&#34;https://github.com/IntelRealSense/realsense-ros.git&#34;&gt;here&lt;/a&gt; into &#39;~/ros2_ws/src/&#39;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bashrc&#34;&gt;git clone https://github.com/IntelRealSense/realsense-ros.git -b ros2-master&#xA;cd ~/ros2_ws&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install python3-rosdep -y&#xA;sudo rosdep init # &#34;sudo rosdep init --include-eol-distros&#34; for Foxy and earlier&#xA;rosdep update # &#34;sudo rosdep update --include-eol-distros&#34; for Foxy and earlier&#xA;rosdep install -i --from-path src --rosdistro $ROS_DISTRO --skip-keys=librealsense2 -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Build&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;colcon build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Source environment&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ROS_DISTRO=&amp;lt;YOUR_SYSTEM_ROS_DISTRO&amp;gt;  # set your ROS_DISTRO: iron, humble, foxy&#xA;source /opt/ros/$ROS_DISTRO/setup.bash&#xA;cd ~/ros2_ws&#xA;. install/local_setup.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Installation on Windows&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;PLEASE PAY ATTENTION: RealSense ROS2 Wrapper is not meant to be supported on Windows by our team, since ROS2 and its packages are still not fully supported over Windows. We added these installation steps below in order to try and make it easier for users who already started working with ROS2 on Windows and want to take advantage of the capabilities of our RealSense cameras&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Step 1: Install the ROS2 distribution &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;h4&gt;Windows 10/11&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Please choose only one option from the two options below (in order to prevent multiple versions installation and workspace conflicts)&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Manual install from ROS2 formal documentation: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://docs.ros.org/en/iron/Installation/Windows-Install-Binary.html&#34;&gt;ROS2 Iron&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://docs.ros.org/en/humble/Installation/Windows-Install-Binary.html&#34;&gt;ROS2 Humble&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://docs.ros.org/en/foxy/Installation/Windows-Install-Binary.html&#34;&gt;ROS2 Foxy&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Microsoft IOT binary installation: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://ms-iot.github.io/ROSOnWindows/GettingStarted/SetupRos2.html&#34;&gt;https://ms-iot.github.io/ROSOnWindows/GettingStarted/SetupRos2.html&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;li&gt;Pay attention that the examples of install are for Foxy distro (which is not supported anymore by RealSense ROS2 Wrapper)&lt;/li&gt; &#xA;      &lt;li&gt;Please replace the word &#34;Foxy&#34; with Humble or Iron, depends on the chosen distro.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Step 2: Download RealSense‚Ñ¢ ROS2 Wrapper and RealSense‚Ñ¢ SDK 2.0 source code from github: &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Download Intel¬Æ RealSense‚Ñ¢ ROS2 Wrapper source code from &lt;a href=&#34;https://github.com/IntelRealSense/realsense-ros/releases&#34;&gt;Intel¬Æ RealSense‚Ñ¢ ROS2 Wrapper Releases&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Download the corrosponding supported Intel¬Æ RealSense‚Ñ¢ SDK 2.0 source code from the &lt;strong&gt;&#34;Supported RealSense SDK&#34; section&lt;/strong&gt; of the specific release you chose fronm the link above&lt;/li&gt; &#xA;  &lt;li&gt;Place the librealsense folder inside the realsense-ros folder, to make the librealsense package set beside realsense2_camera, realsense2_camera_msgs and realsense2_description packages&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Step 3: Build &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Before starting building of our packages, make sure you have OpenCV for Windows installed on your machine. If you choose the Microsoft IOT way to install it, it will be installed automatically. Later, when colcon build, you might need to expose this installation folder by setting CMAKE_PREFIX_PATH, PATH, or OpenCV_DIR environment variables&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Run &#34;x64 Native Tools Command Prompt for VS 2019&#34; as administrator&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Setup ROS2 Environment (Do this for every new terminal/cmd you open):&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt; &lt;p&gt;If you choose the Microsoft IOT Binary option for installation&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; C:\opt\ros\humble\x64\setup.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;If you choose the ROS2 formal documentation:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; call C:\dev\ros2_iron\local_setup.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Change directory to realsense-ros folder&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; cd C:\ros2_ws\realsense-ros&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Build librealsense2 package only&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; colcon build --packages-select librealsense2 --cmake-args -DBUILD_EXAMPLES=OFF -DBUILD_WITH_STATIC_CRT=OFF -DBUILD_GRAPHICAL_EXAMPLES=OFF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;User can add &lt;code&gt;--event-handlers console_direct+&lt;/code&gt; parameter to see more debug outputs of the colcon build&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Build the other packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; colcon build --packages-select realsense2_camera_msgs realsense2_description realsense2_camera&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;User can add &lt;code&gt;--event-handlers console_direct+&lt;/code&gt; parameter to see more debug outputs of the colcon build&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Setup environment with new installed packages (Do this for every new terminal/cmd you open):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; call install\setup.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h2&gt;Start the camera node&lt;/h2&gt; &#xA;&lt;h4&gt;with ros2 run:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;ros2 run realsense2_camera realsense2_camera_node&#xA;# or, with parameters, for example - temporal and spatial filters are enabled:&#xA;ros2 run realsense2_camera realsense2_camera_node --ros-args -p enable_color:=false -p spatial_filter.enable:=true -p temporal_filter.enable:=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;with ros2 launch:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;ros2 launch realsense2_camera rs_launch.py&#xA;ros2 launch realsense2_camera rs_launch.py depth_module.depth_profile:=1280x720x30 pointcloud.enable:=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Camera Name And Camera Namespace&lt;/h2&gt; &#xA;&lt;p&gt;User can set the camera name and camera namespace, to distinguish between cameras and platforms, which helps identifying the right nodes and topics to work with.&lt;/p&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If user have multiple cameras (might be of the same model) and multiple robots then user can choose to launch/run his nodes on this way.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For the first robot and first camera he will run/launch it with these parameters:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;camera_namespace:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;robot1&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;camera_name&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;D455_1&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;With ros2 launch (via command line or by editing these two parameters in the launch file):&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;code&gt;ros2 launch realsense2_camera rs_launch.py camera_namespace:=robot1 camera_name:=D455_1&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;With ros2 run (using remapping mechanisim &lt;a href=&#34;https://docs.ros.org/en/humble/How-To-Guides/Node-arguments.html&#34;&gt;Reference&lt;/a&gt;):&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;code&gt;ros2 run realsense2_camera realsense2_camera_node --ros-args -r __node:=D455_1 -r __ns:=robot1&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Result&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;&amp;gt; ros2 node list&#xA;/robot1/D455_1&#xA;&#xA;&amp;gt; ros2 topic list&#xA;/robot1/D455_1/color/camera_info&#xA;/robot1/D455_1/color/image_raw&#xA;/robot1/D455_1/color/metadata&#xA;/robot1/D455_1/depth/camera_info&#xA;/robot1/D455_1/depth/image_rect_raw&#xA;/robot1/D455_1/depth/metadata&#xA;/robot1/D455_1/extrinsics/depth_to_color&#xA;/robot1/D455_1/imu&#xA;&#xA;&amp;gt; ros2 service list&#xA;/robot1/D455_1/device_info&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Default behavior if non of these parameters are given:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;camera_namespace:=camera&lt;/li&gt; &#xA; &lt;li&gt;camera_name:=camera&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; ros2 node list&#xA;/camera/camera&#xA;&#xA;&amp;gt; ros2 topic list&#xA;/camera/camera/color/camera_info&#xA;/camera/camera/color/image_raw&#xA;/camera/camera/color/metadata&#xA;/camera/camera/depth/camera_info&#xA;/camera/camera/depth/image_rect_raw&#xA;/camera/camera/depth/metadata&#xA;/camera/camera/extrinsics/depth_to_color&#xA;/camera/camera/imu&#xA;&#xA;&amp;gt; ros2 service list&#xA;/camera/camera/device_info&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Parameters&lt;/h2&gt; &#xA;&lt;h3&gt;Available Parameters:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For the entire list of parameters type &lt;code&gt;ros2 param list&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For reading a parameter value use &lt;code&gt;ros2 param get &amp;lt;node&amp;gt; &amp;lt;parameter_name&amp;gt;&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;ros2 param get /camera/camera depth_module.emitter_enabled&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For setting a new value for a parameter use &lt;code&gt;ros2 param set &amp;lt;node&amp;gt; &amp;lt;parameter_name&amp;gt; &amp;lt;value&amp;gt;&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;ros2 param set /camera/camera depth_module.emitter_enabled 1&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Parameters that can be modified during runtime:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All of the filters and sensors inner parameters.&lt;/li&gt; &#xA; &lt;li&gt;Video Sensor Parameters: (&lt;code&gt;depth_module&lt;/code&gt; and &lt;code&gt;rgb_camera&lt;/code&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;They have, at least, the &lt;strong&gt;&amp;lt;stream_type&amp;gt;_profile&lt;/strong&gt; parameter. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The profile parameter is a string of the following format: &amp;lt;width&amp;gt;X&amp;lt;height&amp;gt;X&amp;lt;fps&amp;gt; (The dividing character can be X, x or &#34;,&#34;. Spaces are ignored.)&lt;/li&gt; &#xA;     &lt;li&gt;For example: &lt;code&gt;depth_module.depth_profile:=640x480x30 depth_module.infra_profile:=640x480x30 rgb_camera.color_profile:=1280x720x30&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Note: The param &lt;strong&gt;depth_module.infra_profile&lt;/strong&gt; is common for all infra streams. i.e., infra 0, 1 &amp;amp; 2.&lt;/li&gt; &#xA;     &lt;li&gt;If the specified combination of parameters is not available by the device, the default or previously set configuration will be used. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Run &lt;code&gt;ros2 param describe &amp;lt;your_node_name&amp;gt; &amp;lt;param_name&amp;gt;&lt;/code&gt; to get the list of supported profiles.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Note: Should re-enable the stream for the change to take effect.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;em&gt;&amp;lt;stream_name&amp;gt;&lt;/em&gt;_format&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;This parameter is a string used to select the stream format.&lt;/li&gt; &#xA;     &lt;li&gt;&amp;lt;stream_name&amp;gt; can be any of &lt;em&gt;infra, infra1, infra2, color, depth&lt;/em&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;For example: &lt;code&gt;depth_module.depth_format:=Z16 depth_module.infra1_format:=y8 rgb_camera.color_format:=RGB8&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;This parameter supports both lower case and upper case letters.&lt;/li&gt; &#xA;     &lt;li&gt;If the specified parameter is not available by the stream, the default or previously set configuration will be used. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Run &lt;code&gt;ros2 param describe &amp;lt;your_node_name&amp;gt; &amp;lt;param_name&amp;gt;&lt;/code&gt; to get the list of supported formats.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Note: Should re-enable the stream for the change to take effect.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;If the stream doesn&#39;t support the user selected profile &amp;lt;width&amp;gt;X&amp;lt;height&amp;gt;X&amp;lt;fps&amp;gt; + &amp;lt;format&amp;gt;, it will not be opened and a warning message will be shown. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Should update the profile settings and re-enable the stream for the change to take effect.&lt;/li&gt; &#xA;     &lt;li&gt;Run &lt;code&gt;rs-enumerate-devices&lt;/code&gt; command to know the list of profiles supported by the connected sensors.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;enable_&lt;em&gt;&amp;lt;stream_name&amp;gt;&lt;/em&gt;&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Choose whether to enable a specified stream or not. Default is true for images and false for orientation streams.&lt;/li&gt; &#xA;   &lt;li&gt;&amp;lt;stream_name&amp;gt; can be any of &lt;em&gt;infra, infra1, infra2, color, depth, gyro, accel&lt;/em&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;enable_infra1:=true enable_color:=false&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;enable_sync&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;gathers closest frames of different sensors, infra red, color and depth, to be sent with the same timetag.&lt;/li&gt; &#xA;   &lt;li&gt;This happens automatically when such filters as pointcloud are enabled.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;em&gt;&amp;lt;stream_type&amp;gt;&lt;/em&gt;_qos&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sets the QoS by which the topic is published.&lt;/li&gt; &#xA;   &lt;li&gt;&amp;lt;stream_type&amp;gt; can be any of &lt;em&gt;infra, infra1, infra2, color, depth, gyro, accel&lt;/em&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Available values are the following strings: &lt;code&gt;SYSTEM_DEFAULT&lt;/code&gt;, &lt;code&gt;DEFAULT&lt;/code&gt;, &lt;code&gt;PARAMETER_EVENTS&lt;/code&gt;, &lt;code&gt;SERVICES_DEFAULT&lt;/code&gt;, &lt;code&gt;PARAMETERS&lt;/code&gt;, &lt;code&gt;SENSOR_DATA&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;depth_qos:=SENSOR_DATA&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Pointcloud QoS is controlled with the &lt;code&gt;pointcloud.pointcloud_qos&lt;/code&gt; parameter in the pointcloud filter, refer to the Post-Processing Filters section for details.&lt;/li&gt; &#xA;   &lt;li&gt;Reference: &lt;a href=&#34;https://docs.ros.org/en/rolling/Concepts/About-Quality-of-Service-Settings.html#qos-profiles&#34;&gt;ROS2 QoS profiles formal documentation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Notice:&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;&amp;lt;stream_type&amp;gt;&lt;/em&gt;_info_qos&lt;/strong&gt; refers to both camera_info topics and metadata topics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;tf_publish_rate&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;double, rate (in Hz) at which dynamic transforms are published&lt;/li&gt; &#xA;   &lt;li&gt;Default value is 0.0 Hz &lt;em&gt;(means no dynamic TF)&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;This param also depends on &lt;strong&gt;publish_tf&lt;/strong&gt; param &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;If &lt;strong&gt;publish_tf:=false&lt;/strong&gt;, then no TFs will be published, even if &lt;strong&gt;tf_publish_rate&lt;/strong&gt; is &amp;gt;0.0 Hz&lt;/li&gt; &#xA;     &lt;li&gt;If &lt;strong&gt;publish_tf:=true&lt;/strong&gt; and &lt;strong&gt;tf_publish_rate&lt;/strong&gt; set to &amp;gt;0.0 Hz, then dynamic TFs will be published at the specified rate&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;unite_imu_method&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For the D400 cameras with built in IMU components, below 2 unrelated streams (each with it&#39;s own frequency) will be created: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;gyro&lt;/em&gt; - which shows angular velocity&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;accel&lt;/em&gt; - which shows linear acceleration.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Both streams will publish data to its corresponding topics: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&#39;/camera/camera/gyro/sample&#39; &amp;amp; &#39;/camera/camera/accel/sample&#39;&lt;/li&gt; &#xA;     &lt;li&gt;Though both topics are of same message type &#39;sensor_msgs::Imu&#39;, only their relevant fields are filled out.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;A new topic called &lt;strong&gt;imu&lt;/strong&gt; will be created, when both &lt;em&gt;accel&lt;/em&gt; and &lt;em&gt;gyro&lt;/em&gt; streams are enabled and the param &lt;em&gt;unite_imu_method&lt;/em&gt; set to &amp;gt; 0. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Data from both accel and gyro are combined and published to this topic&lt;/li&gt; &#xA;     &lt;li&gt;All the fields of the Imu message are filled out.&lt;/li&gt; &#xA;     &lt;li&gt;It will be published at the rate of the gyro.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;unite_imu_method&lt;/code&gt; param supports below values: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;0 -&amp;gt; &lt;strong&gt;none&lt;/strong&gt;: no imu topic&lt;/li&gt; &#xA;     &lt;li&gt;1 -&amp;gt; &lt;strong&gt;copy&lt;/strong&gt;: Every gyro message will be attached by the last accel message.&lt;/li&gt; &#xA;     &lt;li&gt;2 -&amp;gt; &lt;strong&gt;linear_interpolation&lt;/strong&gt;: Every gyro message will be attached by an accel message which is interpolated to gyro&#39;s timestamp.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Note: When the param &lt;em&gt;unite_imu_method&lt;/em&gt; is dynamically updated, re-enable either gyro or accel stream for the change to take effect.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;accelerate_gpu_with_glsl&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Boolean: GPU accelerated with GLSL for processing PointCloud and Colorizer filters.&lt;/li&gt; &#xA;   &lt;li&gt;Note: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;To have smooth transition between the processing blocks when this parameter is updated dynamically, the node will: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Stop the video sensors&lt;/li&gt; &#xA;       &lt;li&gt;Do necessary GLSL configuration&lt;/li&gt; &#xA;       &lt;li&gt;And then, start the video sensors&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;To enable GPU acceleration, turn ON &lt;code&gt;BUILD_ACCELERATE_GPU_WITH_GLSL&lt;/code&gt; during build:&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;colcon build --cmake-args &#39;-DBUILD_ACCELERATE_GPU_WITH_GLSL=ON&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Parameters that cannot be changed in runtime:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;serial_no&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;will attach to the device with the given serial number (&lt;em&gt;serial_no&lt;/em&gt;) number.&lt;/li&gt; &#xA;   &lt;li&gt;Default, attach to the first (in an inner list) RealSense device.&lt;/li&gt; &#xA;   &lt;li&gt;Note: serial number should be defined with &#34;_&#34; prefix. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;That is a workaround until a better method will be found to ROS2&#39;s auto conversion of strings containing only digits into integers.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Example: serial number 831612073525 can be set in command line as &lt;code&gt;serial_no:=_831612073525&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;usb_port_id&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;will attach to the device with the given USB port (&lt;em&gt;usb_port_id&lt;/em&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;usb_port_id:=4-1&lt;/code&gt; or &lt;code&gt;usb_port_id:=4-2&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Default, ignore USB port when choosing a device.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;device_type&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;will attach to a device whose name includes the given &lt;em&gt;device_type&lt;/em&gt; regular expression pattern.&lt;/li&gt; &#xA;   &lt;li&gt;Default, ignore device type.&lt;/li&gt; &#xA;   &lt;li&gt;For example: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;device_type:=d435&lt;/code&gt; will match d435 and d435i.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;device_type=d435(?!i)&lt;/code&gt; will match d435 but not d435i.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;reconnect_timeout&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;When the driver cannot connect to the device try to reconnect after this timeout (in seconds).&lt;/li&gt; &#xA;   &lt;li&gt;For Example: &lt;code&gt;reconnect_timeout:=10&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;wait_for_device_timeout&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If the specified device is not found, will wait &lt;em&gt;wait_for_device_timeout&lt;/em&gt; seconds before exits.&lt;/li&gt; &#xA;   &lt;li&gt;Defualt, &lt;em&gt;wait_for_device_timeout &amp;lt; 0&lt;/em&gt;, will wait indefinitely.&lt;/li&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;wait_for_device_timeout:=60&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;rosbag_filename&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Publish topics from rosbag file. There are two ways for loading rosbag file:&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Command line - &lt;code&gt;ros2 run realsense2_camera realsense2_camera_node -p rosbag_filename:=&#34;/full/path/to/rosbag.bag&#34;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Launch file - set &lt;code&gt;rosbag_filename&lt;/code&gt; parameter with rosbag full path (see &lt;code&gt;realsense2_camera/launch/rs_launch.py&lt;/code&gt; as reference)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;initial_reset&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;On occasions the device was not closed properly and due to firmware issues needs to reset.&lt;/li&gt; &#xA;   &lt;li&gt;If set to true, the device will reset prior to usage.&lt;/li&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;initial_reset:=true&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;base_frame_id&lt;/strong&gt;: defines the frame_id all static transformations refers to.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;clip_distance&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove from the depth image all values above a given value (meters). Disable by giving negative value (default)&lt;/li&gt; &#xA;   &lt;li&gt;For example: &lt;code&gt;clip_distance:=1.5&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;linear_accel_cov&lt;/strong&gt;, &lt;strong&gt;angular_velocity_cov&lt;/strong&gt;: sets the variance given to the Imu readings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;hold_back_imu_for_frames&lt;/strong&gt;: Images processing takes time. Therefor there is a time gap between the moment the image arrives at the wrapper and the moment the image is published to the ROS environment. During this time, Imu messages keep on arriving and a situation is created where an image with earlier timestamp is published after Imu message with later timestamp. If that is a problem, setting &lt;em&gt;hold_back_imu_for_frames&lt;/em&gt; to &lt;em&gt;true&lt;/em&gt; will hold the Imu messages back while processing the images and then publish them all in a burst, thus keeping the order of publication as the order of arrival. Note that in either case, the timestamp in each message&#39;s header reflects the time of it&#39;s origin.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;publish_tf&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;boolean, enable/disable publishing static and dynamic TFs&lt;/li&gt; &#xA;   &lt;li&gt;Defaults to True &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;So, static TFs will be published by default&lt;/li&gt; &#xA;     &lt;li&gt;If dynamic TFs are needed, user should set the param &lt;strong&gt;tf_publish_rate&lt;/strong&gt; to &amp;gt;0.0 Hz&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;If set to false, both static and dynamic TFs won&#39;t be published, even if the param &lt;strong&gt;tf_publish_rate&lt;/strong&gt; is set to &amp;gt;0.0 Hz&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;diagnostics_period&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;double, positive values set the period between diagnostics updates on the &lt;code&gt;/diagnostics&lt;/code&gt; topic.&lt;/li&gt; &#xA;   &lt;li&gt;0 or negative values mean no diagnostics topic is published. Defaults to 0.&lt;br&gt; The &lt;code&gt;/diagnostics&lt;/code&gt; topic includes information regarding the device temperatures and actual frequency of the enabled streams.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ROS2(Robot) vs Optical(Camera) Coordination Systems:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Point Of View: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Imagine we are standing behind of the camera, and looking forward.&lt;/li&gt; &#xA;   &lt;li&gt;Always use this point of view when talking about coordinates, left vs right IRs, position of sensor, etc..&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/99127997/230150735-bc31fedf-d715-4e35-b462-fe2c338832c3.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ROS2 Coordinate System: (X: Forward, Y:Left, Z: Up)&lt;/li&gt; &#xA; &lt;li&gt;Camera Optical Coordinate System: (X: Right, Y: Down, Z: Forward)&lt;/li&gt; &#xA; &lt;li&gt;References: &lt;a href=&#34;https://www.ros.org/reps/rep-0103.html#coordinate-frame-conventions&#34;&gt;REP-0103&lt;/a&gt;, &lt;a href=&#34;https://www.ros.org/reps/rep-0105.html#coordinate-frames&#34;&gt;REP-0105&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;All data published in our wrapper topics is optical data taken directly from our camera sensors.&lt;/li&gt; &#xA; &lt;li&gt;static and dynamic TF topics publish optical CS and ROS CS to give the user the ability to move from one CS to other CS.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TF from coordinate A to coordinate B:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TF msg expresses a transform from coordinate frame &#34;header.frame_id&#34; (source) to the coordinate frame child_frame_id (destination) &lt;a href=&#34;http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/Transform.html&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;In RealSense cameras, the origin point (0,0,0) is taken from the left IR (infra1) position and named as &#34;camera_link&#34; frame&lt;/li&gt; &#xA; &lt;li&gt;Depth, left IR and &#34;camera_link&#34; coordinates converge together.&lt;/li&gt; &#xA; &lt;li&gt;Our wrapper provide static TFs between each sensor coordinate to the camera base (camera_link)&lt;/li&gt; &#xA; &lt;li&gt;Also, it provides TFs from each sensor ROS coordinates to its corrosponding optical coordinates.&lt;/li&gt; &#xA; &lt;li&gt;Example of static TFs of RGB sensor and Infra2 (right infra) sensor of D435i module as it shown in rviz2: &lt;img src=&#34;https://user-images.githubusercontent.com/99127997/230148106-0f79cbdb-c401-4d09-b386-a366af18e5f7.png&#34; alt=&#34;example&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Extrinsics from sensor A to sensor B:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extrinsic from sensor A to sensor B means the position and orientation of sensor A relative to sensor B.&lt;/li&gt; &#xA; &lt;li&gt;Imagine that B is the origin (0,0,0), then the Extrensics(A-&amp;gt;B) describes where is sensor A relative to sensor B.&lt;/li&gt; &#xA; &lt;li&gt;For example, depth_to_color, in D435i: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If we look from behind of the D435i, extrinsic from depth to color, means, where is the depth in relative to the color.&lt;/li&gt; &#xA;   &lt;li&gt;If we just look at the X coordinates, in the optical coordiantes (again, from behind) and assume that COLOR(RGB) sensor is (0,0,0), we can say that DEPTH sensor is on the right of RGB by 0.0148m (1.48cm).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/99127997/230220297-e392f0fc-63bf-4bab-8001-af1ddf0ed00e.png&#34; alt=&#34;d435i&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;administrator@perclnx466 ~/ros2_humble $ ros2 topic echo /camera/camera/extrinsics/depth_to_color&#xA;rotation:&#xA;- 0.9999583959579468&#xA;- 0.008895332925021648&#xA;- -0.0020127370953559875&#xA;- -0.008895229548215866&#xA;- 0.9999604225158691&#xA;- 6.045500049367547e-05&#xA;- 0.0020131953060626984&#xA;- -4.254872692399658e-05&#xA;- 0.9999979734420776&#xA;translation:&#xA;- 0.01485931035131216&#xA;- 0.0010161789832636714&#xA;- 0.0005317096947692335&#xA;---&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extrinsic msg is made up of two parts: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;float64[9] rotation (Column - major 3x3 rotation matrix)&lt;/li&gt; &#xA;   &lt;li&gt;float64[3] translation (Three-element translation vector, in meters)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Published Topics&lt;/h2&gt; &#xA;&lt;p&gt;The published topics differ according to the device and parameters. After running the above command with D435i attached, the following list of topics will be available (This is a partial list. For full one type &lt;code&gt;ros2 topic list&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/camera/camera/aligned_depth_to_color/camera_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/aligned_depth_to_color/image_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/color/camera_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/color/image_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/color/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/depth/camera_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/depth/color/points&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/depth/image_rect_raw&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/depth/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/extrinsics/depth_to_color&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/imu&lt;/li&gt; &#xA; &lt;li&gt;/diagnostics&lt;/li&gt; &#xA; &lt;li&gt;/parameter_events&lt;/li&gt; &#xA; &lt;li&gt;/rosout&lt;/li&gt; &#xA; &lt;li&gt;/tf_static&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This will stream relevant camera sensors and publish on the appropriate ROS topics.&lt;/p&gt; &#xA;&lt;p&gt;Enabling accel and gyro is achieved either by adding the following parameters to the command line:&lt;br&gt; &lt;code&gt;ros2 launch realsense2_camera rs_launch.py pointcloud.enable:=true enable_gyro:=true enable_accel:=true&lt;/code&gt; &lt;br&gt; or in runtime using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ros2 param set /camera/camera enable_accel true&#xA;ros2 param set /camera/camera enable_gyro true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Enabling stream adds matching topics. For instance, enabling the gyro and accel streams adds the following topics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;/camera/camera/accel/imu_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/accel/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/accel/sample&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/extrinsics/depth_to_accel&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/extrinsics/depth_to_gyro&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/gyro/imu_info&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/gyro/metadata&lt;/li&gt; &#xA; &lt;li&gt;/camera/camera/gyro/sample&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;RGBD Topic&lt;/h2&gt; &#xA;&lt;p&gt;RGBD new topic, publishing [RGB + Depth] in the same message (see RGBD.msg for reference). For now, works only with depth aligned to color images, as color and depth images are synchronized by frame time tag.&lt;/p&gt; &#xA;&lt;p&gt;These boolean paramters should be true to enable rgbd messages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;enable_rgbd&lt;/code&gt;: new paramter, to enable/disable rgbd topic, changeable during runtime&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;align_depth.enable&lt;/code&gt;: align depth images to rgb images&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enable_sync&lt;/code&gt;: let librealsense sync between frames, and get the frameset with color and depth images combined&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enable_color&lt;/code&gt; + &lt;code&gt;enable_depth&lt;/code&gt;: enable both color and depth sensors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The current QoS of the topic itself, is the same as Depth and Color streams (SYSTEM_DEFAULT)&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ros2 launch realsense2_camera rs_launch.py enable_rgbd:=true enable_sync:=true align_depth.enable:=true enable_color:=true enable_depth:=true &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Metadata topic&lt;/h2&gt; &#xA;&lt;p&gt;The metadata messages store the camera&#39;s available metadata in a &lt;em&gt;json&lt;/em&gt; format. To learn more, a dedicated script for echoing a metadata topic in runtime is attached. For instance, use the following command to echo the camera/depth/metadata topic:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 src/realsense-ros/realsense2_camera/scripts/echo_metadada.py /camera/camera/depth/metadata&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Post-Processing Filters&lt;/h2&gt; &#xA;&lt;p&gt;The following post processing filters are available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;align_depth&lt;/code&gt;: If enabled, will publish the depth image aligned to the color image on the topic &lt;code&gt;/camera/camera/aligned_depth_to_color/image_raw&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The pointcloud, if created, will be based on the aligned depth image.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;colorizer&lt;/code&gt;: will color the depth image. On the depth topic an RGB image will be published, instead of the 16bit depth values .&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;pointcloud&lt;/code&gt;: will add a pointcloud topic &lt;code&gt;/camera/camera/depth/color/points&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The texture of the pointcloud can be modified using the &lt;code&gt;pointcloud.stream_filter&lt;/code&gt; parameter.&lt;br&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The depth FOV and the texture FOV are not similar. By default, pointcloud is limited to the section of depth containing the texture. You can have a full depth to pointcloud, coloring the regions beyond the texture with zeros, by setting &lt;code&gt;pointcloud.allow_no_texture_points&lt;/code&gt; to true.&lt;/li&gt; &#xA;   &lt;li&gt;pointcloud is of an unordered format by default. This can be changed by setting &lt;code&gt;pointcloud.ordered_pc&lt;/code&gt; to true.&lt;/li&gt; &#xA;   &lt;li&gt;The QoS of the pointcloud topic is independent from depth and color streams and can be controlled with the &lt;code&gt;pointcloud.pointcloud_qos&lt;/code&gt; parameter. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The same set of QoS values are supported as other streams, refer to &amp;lt;stream_type&amp;gt;_qos in the Parameters section of this page.&lt;/li&gt; &#xA;     &lt;li&gt;The launch file should include the parameter with initial QoS value, for example,&lt;code&gt;{&#39;name&#39;: &#39;pointcloud.pointcloud_qos&#39;, &#39;default&#39;: &#39;SENSOR_DATA&#39;, &#39;description&#39;: &#39;pointcloud qos&#39;}&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;The QoS value can also be overridden at launch with command option, for example, &lt;code&gt;pointcloud.pointcloud_qos:=SENSOR_DATA&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;At runtime, the QoS can be changed dynamically but require the filter re-enable for the change to take effect, for example, &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ros2 param set /camera/camera pointcloud.pointcloud_qos SENSOR_DATA&#xA;ros2 param set /camera/camera pointcloud.enable false&#xA;ros2 param set /camera/camera pointcloud.enable true&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;hdr_merge&lt;/code&gt;: Allows depth image to be created by merging the information from 2 consecutive frames, taken with different exposure and gain values.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;depth_module.hdr_enabled&lt;/code&gt;: to enable/disable HDR. The way to set exposure and gain values for each sequence:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;during Runtime: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;is by first selecting the sequence id, using the &lt;code&gt;depth_module.sequence_id&lt;/code&gt; parameter and then modifying the &lt;code&gt;depth_module.gain&lt;/code&gt;, and &lt;code&gt;depth_module.exposure&lt;/code&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;From FW versions 5.14.x.x and above, if HDR is enabled, the preset configs (like exposure, gain, etc.,) cannot be updated. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Disable the HDR first using &lt;code&gt;depth_module.hdr_enabled&lt;/code&gt; parameter and then, update the required presets.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;during Launch time of the node: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;is by setting below parameters &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;code&gt;depth_module.exposure.1&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;depth_module.gain.1&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;depth_module.exposure.2&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;depth_module.gain.2&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Make sure to set &lt;code&gt;depth_module.hdr_enabled&lt;/code&gt; to true, otherwise these parameters won&#39;t be considered.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;To view the effect on the infrared image for each sequence id use the &lt;code&gt;filter_by_sequence_id.sequence_id&lt;/code&gt; parameter.&lt;/li&gt; &#xA;   &lt;li&gt;For in-depth review of the subject please read the accompanying &lt;a href=&#34;https://dev.intelrealsense.com/docs/high-dynamic-range-with-stereoscopic-depth-cameras&#34;&gt;white paper&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: Auto exposure functionality is not supported when HDR is enabled. i.e., Auto exposure will be auto-disabled if HDR is enabled.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The following filters have detailed descriptions in : &lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/post-processing-filters.md&#34;&gt;https://github.com/IntelRealSense/librealsense/blob/master/doc/post-processing-filters.md&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;disparity_filter&lt;/code&gt; - convert depth to disparity before applying other filters and back.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;spatial_filter&lt;/code&gt; - filter the depth image spatially.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;temporal_filter&lt;/code&gt; - filter the depth image temporally.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;hole_filling_filter&lt;/code&gt; - apply hole-filling filter.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;decimation_filter&lt;/code&gt; - reduces depth scene complexity.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each of the above filters have it&#39;s own parameters, following the naming convention of &lt;code&gt;&amp;lt;filter_name&amp;gt;.&amp;lt;parameter_name&amp;gt;&lt;/code&gt; including a &lt;code&gt;&amp;lt;filter_name&amp;gt;.enable&lt;/code&gt; parameter to enable/disable it.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Available services&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;device_info : retrieve information about the device - serial_number, firmware_version etc. Type &lt;code&gt;ros2 interface show realsense2_camera_msgs/srv/DeviceInfo&lt;/code&gt; for the full list. Call example: &lt;code&gt;ros2 service call /camera/camera/device_info realsense2_camera_msgs/srv/DeviceInfo&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Efficient intra-process communication:&lt;/h2&gt; &#xA;&lt;p&gt;Our ROS2 Wrapper node supports zero-copy communications if loaded in the same process as a subscriber node. This can reduce copy times on image/pointcloud topics, especially with big frame resolutions and high FPS.&lt;/p&gt; &#xA;&lt;p&gt;You will need to launch a component container and launch our node as a component together with other component nodes. Further details on &#34;Composing multiple nodes in a single process&#34; can be found &lt;a href=&#34;https://docs.ros.org/en/rolling/Tutorials/Composition.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Further details on efficient intra-process communication can be found &lt;a href=&#34;https://docs.ros.org/en/humble/Tutorials/Intra-Process-Communication.html#efficient-intra-process-communication&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;h4&gt;Manually loading multiple components into the same process&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the component:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ros2 run rclcpp_components component_container&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add the wrapper:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ros2 component load /ComponentManager realsense2_camera realsense2_camera::RealSenseNodeFactory -e use_intra_process_comms:=true&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Load other component nodes (consumers of the wrapper topics) in the same way.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Limitations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node components are currently not supported on RCLPY&lt;/li&gt; &#xA; &lt;li&gt;Compressed images using &lt;code&gt;image_transport&lt;/code&gt; will be disabled as this isn&#39;t supported with intra-process communication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Latency test tool and launch file&lt;/h3&gt; &#xA;&lt;p&gt;For getting a sense of the latency reduction, a frame latency reporter tool is available via a launch file. The launch file loads the wrapper and a frame latency reporter tool component into a single container (so the same process). The tool prints out the frame latency (&lt;code&gt;now - frame.timestamp&lt;/code&gt;) per frame.&lt;/p&gt; &#xA;&lt;p&gt;The tool is not built unless asked for. Turn on &lt;code&gt;BUILD_TOOLS&lt;/code&gt; during build to have it available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;colcon build --cmake-args &#39;-DBUILD_TOOLS=ON&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The launch file accepts a parameter, &lt;code&gt;intra_process_comms&lt;/code&gt;, controlling whether zero-copy is turned on or not. Default is on:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ros2 launch realsense2_camera rs_intra_process_demo_launch.py intra_process_comms:=true&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>real-stanford/diffusion_policy</title>
    <updated>2024-12-06T01:34:35Z</updated>
    <id>tag:github.com,2024-12-06:/real-stanford/diffusion_policy</id>
    <link href="https://github.com/real-stanford/diffusion_policy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[RSS 2023] Diffusion Policy Visuomotor Policy Learning via Action Diffusion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Diffusion Policy&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/&#34;&gt;[Project page]&lt;/a&gt; &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/#paper&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/data/&#34;&gt;[Data]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1gxdkgRVfM55zihY9TFLja97cSVZOZq2B?usp=sharing&#34;&gt;[Colab (state)]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg?usp=sharing&#34;&gt;[Colab (vision)]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://cheng-chi.github.io/&#34;&gt;Cheng Chi&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~sfeng/&#34;&gt;Siyuan Feng&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://yilundu.github.io/&#34;&gt;Yilun Du&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www.zhenjiaxu.com/&#34;&gt;Zhenjia Xu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://www.eacousineau.com/&#34;&gt;Eric Cousineau&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://www.benburchfiel.com/&#34;&gt;Benjamin Burchfiel&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.cs.columbia.edu/~shurans/&#34;&gt;Shuran Song&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Columbia University, &lt;sup&gt;2&lt;/sup&gt;Toyota Research Institute, &lt;sup&gt;3&lt;/sup&gt;MIT&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/media/teaser.png&#34; alt=&#34;drawing&#34; width=&#34;100%&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/media/multimodal_sim.png&#34; alt=&#34;drawing&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h2&gt;üõù Try it out!&lt;/h2&gt; &#xA;&lt;p&gt;Our self-contained Google Colab notebooks is the easiest way to play with Diffusion Policy. We provide separate notebooks for &lt;a href=&#34;https://colab.research.google.com/drive/1gxdkgRVfM55zihY9TFLja97cSVZOZq2B?usp=sharing&#34;&gt;state-based environment&lt;/a&gt; and &lt;a href=&#34;https://colab.research.google.com/drive/18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg?usp=sharing&#34;&gt;vision-based environment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üßæ Checkout our experiment logs!&lt;/h2&gt; &#xA;&lt;p&gt;For each experiment used to generate Table I,II and IV in the &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/#paper&#34;&gt;paper&lt;/a&gt;, we provide:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A &lt;code&gt;config.yaml&lt;/code&gt; that contains all parameters needed to reproduce the experiment.&lt;/li&gt; &#xA; &lt;li&gt;Detailed training/eval &lt;code&gt;logs.json.txt&lt;/code&gt; for every training step.&lt;/li&gt; &#xA; &lt;li&gt;Checkpoints for the best &lt;code&gt;epoch=*-test_mean_score=*.ckpt&lt;/code&gt; and last &lt;code&gt;latest.ckpt&lt;/code&gt; epoch of each run.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Experiment logs are hosted on our website as nested directories in format: &lt;code&gt;https://diffusion-policy.cs.columbia.edu/data/experiments/&amp;lt;image|low_dim&amp;gt;/&amp;lt;task&amp;gt;/&amp;lt;method&amp;gt;/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Within each experiment directory you may find:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îú‚îÄ‚îÄ config.yaml&#xA;‚îú‚îÄ‚îÄ metrics&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ logs.json.txt&#xA;‚îú‚îÄ‚îÄ train_0&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ checkpoints&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ epoch=0300-test_mean_score=1.000.ckpt&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ latest.ckpt&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ logs.json.txt&#xA;‚îú‚îÄ‚îÄ train_1&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ checkpoints&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ epoch=0250-test_mean_score=1.000.ckpt&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ latest.ckpt&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ logs.json.txt&#xA;‚îî‚îÄ‚îÄ train_2&#xA;    ‚îú‚îÄ‚îÄ checkpoints&#xA;    ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ epoch=0250-test_mean_score=1.000.ckpt&#xA;    ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ latest.ckpt&#xA;    ‚îî‚îÄ‚îÄ logs.json.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;metrics/logs.json.txt&lt;/code&gt; file aggregates evaluation metrics from all 3 training runs every 50 epochs using &lt;code&gt;multirun_metrics.py&lt;/code&gt;. The numbers reported in the paper correspond to &lt;code&gt;max&lt;/code&gt; and &lt;code&gt;k_min_train_loss&lt;/code&gt; aggregation keys.&lt;/p&gt; &#xA;&lt;p&gt;To download all files in a subdirectory, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ wget --recursive --no-parent --no-host-directories --relative --reject=&#34;index.html*&#34; https://diffusion-policy.cs.columbia.edu/data/experiments/low_dim/square_ph/diffusion_policy_cnn/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; &#xA;&lt;h3&gt;üñ•Ô∏è Simulation&lt;/h3&gt; &#xA;&lt;p&gt;To reproduce our simulation benchmark results, install our conda environment on a Linux machine with Nvidia GPU. On Ubuntu 20.04 you need to install the following apt packages for mujoco:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ sudo apt install -y libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend &lt;a href=&#34;https://github.com/conda-forge/miniforge#mambaforge&#34;&gt;Mambaforge&lt;/a&gt; instead of the standard anaconda distribution for faster installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ mamba env create -f conda_environment.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;but you can use conda as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ conda env create -f conda_environment.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;conda_environment_macos.yaml&lt;/code&gt; file is only for development on MacOS and does not have full support for benchmarks.&lt;/p&gt; &#xA;&lt;h3&gt;ü¶æ Real Robot&lt;/h3&gt; &#xA;&lt;p&gt;Hardware (for Push-T):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;1x &lt;a href=&#34;https://www.universal-robots.com/cb3&#34;&gt;UR5-CB3&lt;/a&gt; or &lt;a href=&#34;https://www.universal-robots.com/products/ur5-robot/&#34;&gt;UR5e&lt;/a&gt; (&lt;a href=&#34;https://www.universal-robots.com/articles/ur/interface-communication/real-time-data-exchange-rtde-guide/&#34;&gt;RTDE Interface&lt;/a&gt; is required)&lt;/li&gt; &#xA; &lt;li&gt;2x &lt;a href=&#34;https://www.intelrealsense.com/depth-camera-d415/&#34;&gt;RealSense D415&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;1x &lt;a href=&#34;https://3dconnexion.com/us/product/spacemouse-wireless/&#34;&gt;3Dconnexion SpaceMouse&lt;/a&gt; (for teleop)&lt;/li&gt; &#xA; &lt;li&gt;1x &lt;a href=&#34;https://www.millibar.com/manual-tool-changer/&#34;&gt;Millibar Robotics Manual Tool Changer&lt;/a&gt; (only need robot side)&lt;/li&gt; &#xA; &lt;li&gt;1x 3D printed &lt;a href=&#34;https://cad.onshape.com/documents/a818888644a15afa6cc68ee5/w/2885b48b018cda84f425beca/e/3e8771c2124cee024edd2fed?renderMode=0&amp;amp;uiState=63ffcba6631ca919895e64e5&#34;&gt;End effector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;1x 3D printed &lt;a href=&#34;https://cad.onshape.com/documents/f1140134e38f6ed6902648d5/w/a78cf81827600e4ff4058d03/e/f35f57fb7589f72e05c76caf?renderMode=0&amp;amp;uiState=63ffcbc9af4a881b344898ee&#34;&gt;T-block&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;USB-C cables and screws for RealSense&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Software:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 20.04.3 (tested)&lt;/li&gt; &#xA; &lt;li&gt;Mujoco dependencies: &lt;code&gt;sudo apt install libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IntelRealSense/librealsense/raw/master/doc/distribution_linux.md&#34;&gt;RealSense SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Spacemouse dependencies: &lt;code&gt;sudo apt install libspnav-dev spacenavd; sudo systemctl start spacenavd&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Conda environment &lt;code&gt;mamba env create -f conda_environment_real.yaml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üñ•Ô∏è Reproducing Simulation Benchmark Results&lt;/h2&gt; &#xA;&lt;h3&gt;Download Training Data&lt;/h3&gt; &#xA;&lt;p&gt;Under the repo root, create data subdirectory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;[diffusion_policy]$ mkdir data &amp;amp;&amp;amp; cd data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the corresponding zip file from &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/data/training/&#34;&gt;https://diffusion-policy.cs.columbia.edu/data/training/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;[data]$ wget https://diffusion-policy.cs.columbia.edu/data/training/pusht.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extract training data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;[data]$ unzip pusht.zip &amp;amp;&amp;amp; rm -f pusht.zip &amp;amp;&amp;amp; cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Grab config file for the corresponding experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;[diffusion_policy]$ wget -O image_pusht_diffusion_policy_cnn.yaml https://diffusion-policy.cs.columbia.edu/data/experiments/image/pusht/diffusion_policy_cnn/config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running for a single seed&lt;/h3&gt; &#xA;&lt;p&gt;Activate conda environment and login to &lt;a href=&#34;https://wandb.ai&#34;&gt;wandb&lt;/a&gt; (if you haven&#39;t already).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;[diffusion_policy]$ conda activate robodiff&#xA;(robodiff)[diffusion_policy]$ wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch training with seed 42 on GPU 0.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ python train.py --config-dir=. --config-name=image_pusht_diffusion_policy_cnn.yaml training.seed=42 training.device=cuda:0 hydra.run.dir=&#39;data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a directory in format &lt;code&gt;data/outputs/yyyy.mm.dd/hh.mm.ss_&amp;lt;method_name&amp;gt;_&amp;lt;task_name&amp;gt;&lt;/code&gt; where configs, logs and checkpoints are written to. The policy will be evaluated every 50 epochs with the success rate logged as &lt;code&gt;test/mean_score&lt;/code&gt; on wandb, as well as videos for some rollouts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ tree data/outputs/2023.03.01/20.02.03_train_diffusion_unet_hybrid_pusht_image -I wandb&#xA;data/outputs/2023.03.01/20.02.03_train_diffusion_unet_hybrid_pusht_image&#xA;‚îú‚îÄ‚îÄ checkpoints&#xA;‚îÇ   ‚îú‚îÄ‚îÄ epoch=0000-test_mean_score=0.134.ckpt&#xA;‚îÇ   ‚îî‚îÄ‚îÄ latest.ckpt&#xA;‚îú‚îÄ‚îÄ .hydra&#xA;‚îÇ   ‚îú‚îÄ‚îÄ config.yaml&#xA;‚îÇ   ‚îú‚îÄ‚îÄ hydra.yaml&#xA;‚îÇ   ‚îî‚îÄ‚îÄ overrides.yaml&#xA;‚îú‚îÄ‚îÄ logs.json.txt&#xA;‚îú‚îÄ‚îÄ media&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 2k5u6wli.mp4&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 2kvovxms.mp4&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 2pxd9f6b.mp4&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 2q5gjt5f.mp4&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 2sawbf6m.mp4&#xA;‚îÇ   ‚îî‚îÄ‚îÄ 538ubl79.mp4&#xA;‚îî‚îÄ‚îÄ train.log&#xA;&#xA;3 directories, 13 files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running for multiple seeds&lt;/h3&gt; &#xA;&lt;p&gt;Launch local ray cluster. For large scale experiments, you might want to setup an &lt;a href=&#34;https://docs.ray.io/en/master/cluster/vms/user-guides/launching-clusters/aws.html&#34;&gt;AWS cluster with autoscaling&lt;/a&gt;. All other commands remain the same.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ export CUDA_VISIBLE_DEVICES=0,1,2  # select GPUs to be managed by the ray cluster&#xA;(robodiff)[diffusion_policy]$ ray start --head --num-gpus=3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch a ray client which will start 3 training workers (3 seeds) and 1 metrics monitor worker.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ python ray_train_multirun.py --config-dir=. --config-name=image_pusht_diffusion_policy_cnn.yaml --seeds=42,43,44 --monitor_key=test/mean_score -- multi_run.run_dir=&#39;data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}&#39; multi_run.wandb_name_base=&#39;${now:%Y.%m.%d-%H.%M.%S}_${name}_${task_name}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition to the wandb log written by each training worker individually, the metrics monitor worker will log to wandb project &lt;code&gt;diffusion_policy_metrics&lt;/code&gt; for the metrics aggregated from all 3 training runs. Local config, logs and checkpoints will be written to &lt;code&gt;data/outputs/yyyy.mm.dd/hh.mm.ss_&amp;lt;method_name&amp;gt;_&amp;lt;task_name&amp;gt;&lt;/code&gt; in a directory structure identical to our &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/data/experiments/&#34;&gt;training logs&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ tree data/outputs/2023.03.01/22.13.58_train_diffusion_unet_hybrid_pusht_image -I &#39;wandb|media&#39;&#xA;data/outputs/2023.03.01/22.13.58_train_diffusion_unet_hybrid_pusht_image&#xA;‚îú‚îÄ‚îÄ config.yaml&#xA;‚îú‚îÄ‚îÄ metrics&#xA;‚îÇ   ‚îú‚îÄ‚îÄ logs.json.txt&#xA;‚îÇ   ‚îú‚îÄ‚îÄ metrics.json&#xA;‚îÇ   ‚îî‚îÄ‚îÄ metrics.log&#xA;‚îú‚îÄ‚îÄ train_0&#xA;‚îÇ   ‚îú‚îÄ‚îÄ checkpoints&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ epoch=0000-test_mean_score=0.174.ckpt&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ latest.ckpt&#xA;‚îÇ   ‚îú‚îÄ‚îÄ logs.json.txt&#xA;‚îÇ   ‚îî‚îÄ‚îÄ train.log&#xA;‚îú‚îÄ‚îÄ train_1&#xA;‚îÇ   ‚îú‚îÄ‚îÄ checkpoints&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ epoch=0000-test_mean_score=0.131.ckpt&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ latest.ckpt&#xA;‚îÇ   ‚îú‚îÄ‚îÄ logs.json.txt&#xA;‚îÇ   ‚îî‚îÄ‚îÄ train.log&#xA;‚îî‚îÄ‚îÄ train_2&#xA;    ‚îú‚îÄ‚îÄ checkpoints&#xA;    ‚îÇ   ‚îú‚îÄ‚îÄ epoch=0000-test_mean_score=0.105.ckpt&#xA;    ‚îÇ   ‚îî‚îÄ‚îÄ latest.ckpt&#xA;    ‚îú‚îÄ‚îÄ logs.json.txt&#xA;    ‚îî‚îÄ‚îÄ train.log&#xA;&#xA;7 directories, 16 files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üÜï Evaluate Pre-trained Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;Download a checkpoint from the published training log folders, such as &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/data/experiments/low_dim/pusht/diffusion_policy_cnn/train_0/checkpoints/epoch=0550-test_mean_score=0.969.ckpt&#34;&gt;https://diffusion-policy.cs.columbia.edu/data/experiments/low_dim/pusht/diffusion_policy_cnn/train_0/checkpoints/epoch=0550-test_mean_score=0.969.ckpt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run the evaluation script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ python eval.py --checkpoint data/0550-test_mean_score=0.969.ckpt --output_dir data/pusht_eval_output --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate the following directory structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ tree data/pusht_eval_output&#xA;data/pusht_eval_output&#xA;‚îú‚îÄ‚îÄ eval_log.json&#xA;‚îî‚îÄ‚îÄ media&#xA;    ‚îú‚îÄ‚îÄ 1fxtno84.mp4&#xA;    ‚îú‚îÄ‚îÄ 224l7jqd.mp4&#xA;    ‚îú‚îÄ‚îÄ 2fo4btlf.mp4&#xA;    ‚îú‚îÄ‚îÄ 2in4cn7a.mp4&#xA;    ‚îú‚îÄ‚îÄ 34b3o2qq.mp4&#xA;    ‚îî‚îÄ‚îÄ 3p7jqn32.mp4&#xA;&#xA;1 directory, 7 files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;eval_log.json&lt;/code&gt; contains metrics that is logged to wandb during training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ cat data/pusht_eval_output/eval_log.json&#xA;{&#xA;  &#34;test/mean_score&#34;: 0.9150393806777066,&#xA;  &#34;test/sim_max_reward_4300000&#34;: 1.0,&#xA;  &#34;test/sim_max_reward_4300001&#34;: 0.9872969750774386,&#xA;...&#xA;  &#34;train/sim_video_1&#34;: &#34;data/pusht_eval_output//media/2fo4btlf.mp4&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü¶æ Demo, Training and Eval on a Real Robot&lt;/h2&gt; &#xA;&lt;p&gt;Make sure your UR5 robot is running and accepting command from its network interface (emergency stop button within reach at all time), your RealSense cameras plugged in to your workstation (tested with &lt;code&gt;realsense-viewer&lt;/code&gt;) and your SpaceMouse connected with the &lt;code&gt;spacenavd&lt;/code&gt; daemon running (verify with &lt;code&gt;systemctl status spacenavd&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Start the demonstration collection script. Press &#34;C&#34; to start recording. Use SpaceMouse to move the robot. Press &#34;S&#34; to stop recording.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ python demo_real_robot.py -o data/demo_pusht_real --robot_ip 192.168.0.204&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should result in a demonstration dataset in &lt;code&gt;data/demo_pusht_real&lt;/code&gt; with in the same structure as our example &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/data/training/pusht_real.zip&#34;&gt;real Push-T training dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To train a Diffusion Policy, launch training with config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;(robodiff)[diffusion_policy]$ python train.py --config-name=train_diffusion_unet_real_image_workspace task.dataset_path=data/demo_pusht_real&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/config/task/real_pusht_image.yaml&#34;&gt;&lt;code&gt;diffusion_policy/config/task/real_pusht_image.yaml&lt;/code&gt;&lt;/a&gt; if your camera setup is different.&lt;/p&gt; &#xA;&lt;p&gt;Assuming the training has finished and you have a checkpoint at &lt;code&gt;data/outputs/blah/checkpoints/latest.ckpt&lt;/code&gt;, launch the evaluation script with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;python eval_real_robot.py -i data/outputs/blah/checkpoints/latest.ckpt -o data/eval_pusht_real --robot_ip 192.168.0.204&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Press &#34;C&#34; to start evaluation (handing control over to the policy). Press &#34;S&#34; to stop the current episode.&lt;/p&gt; &#xA;&lt;h2&gt;üó∫Ô∏è Codebase Tutorial&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is structured under the requirement that:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;implementing &lt;code&gt;N&lt;/code&gt; tasks and &lt;code&gt;M&lt;/code&gt; methods will only require &lt;code&gt;O(N+M)&lt;/code&gt; amount of code instead of &lt;code&gt;O(N*M)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;while retaining maximum flexibility.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To achieve this requirement, we&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;maintained a simple unified interface between tasks and methods and&lt;/li&gt; &#xA; &lt;li&gt;made the implementation of the tasks and the methods independent of each other.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;These design decisions come at the cost of code repetition between the tasks and the methods. However, we believe that the benefit of being able to add/modify task/methods without affecting the remainder and being able understand a task/method by reading the code linearly outweighs the cost of copying and pasting üòä.&lt;/p&gt; &#xA;&lt;h3&gt;The Split&lt;/h3&gt; &#xA;&lt;p&gt;On the task side, we have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Dataset&lt;/code&gt;: adapts a (third-party) dataset to the interface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;EnvRunner&lt;/code&gt;: executes a &lt;code&gt;Policy&lt;/code&gt; that accepts the interface and produce logs and metrics.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config/task/&amp;lt;task_name&amp;gt;.yaml&lt;/code&gt;: contains all information needed to construct &lt;code&gt;Dataset&lt;/code&gt; and &lt;code&gt;EnvRunner&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(optional) &lt;code&gt;Env&lt;/code&gt;: an &lt;code&gt;gym==0.21.0&lt;/code&gt; compatible class that encapsulates the task environment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;On the policy side, we have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Policy&lt;/code&gt;: implements inference according to the interface and part of the training process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Workspace&lt;/code&gt;: manages the life-cycle of training and evaluation (interleaved) of a method.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config/&amp;lt;workspace_name&amp;gt;.yaml&lt;/code&gt;: contains all information needed to construct &lt;code&gt;Policy&lt;/code&gt; and &lt;code&gt;Workspace&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;The Interface&lt;/h3&gt; &#xA;&lt;h4&gt;Low Dim&lt;/h4&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/policy/base_lowdim_policy.py&#34;&gt;&lt;code&gt;LowdimPolicy&lt;/code&gt;&lt;/a&gt; takes observation dictionary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;obs&#34;:&lt;/code&gt; Tensor of shape &lt;code&gt;(B,To,Do)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;and predicts action dictionary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;action&#34;: &lt;/code&gt; Tensor of shape &lt;code&gt;(B,Ta,Da)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/dataset/base_dataset.py&#34;&gt;&lt;code&gt;LowdimDataset&lt;/code&gt;&lt;/a&gt; returns a sample of dictionary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;obs&#34;:&lt;/code&gt; Tensor of shape &lt;code&gt;(To, Do)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;action&#34;:&lt;/code&gt; Tensor of shape &lt;code&gt;(Ta, Da)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Its &lt;code&gt;get_normalizer&lt;/code&gt; method returns a &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/model/common/normalizer.py&#34;&gt;&lt;code&gt;LinearNormalizer&lt;/code&gt;&lt;/a&gt; with keys &lt;code&gt;&#34;obs&#34;,&#34;action&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;Policy&lt;/code&gt; handles normalization on GPU with its copy of the &lt;code&gt;LinearNormalizer&lt;/code&gt;. The parameters of the &lt;code&gt;LinearNormalizer&lt;/code&gt; is saved as part of the &lt;code&gt;Policy&lt;/code&gt;&#39;s weights checkpoint.&lt;/p&gt; &#xA;&lt;h4&gt;Image&lt;/h4&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/policy/base_image_policy.py&#34;&gt;&lt;code&gt;ImagePolicy&lt;/code&gt;&lt;/a&gt; takes observation dictionary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;key0&#34;:&lt;/code&gt; Tensor of shape &lt;code&gt;(B,To,*)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;key1&#34;:&lt;/code&gt; Tensor of shape e.g. &lt;code&gt;(B,To,H,W,3)&lt;/code&gt; ([0,1] float32)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;and predicts action dictionary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;action&#34;: &lt;/code&gt; Tensor of shape &lt;code&gt;(B,Ta,Da)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/dataset/base_dataset.py&#34;&gt;&lt;code&gt;ImageDataset&lt;/code&gt;&lt;/a&gt; returns a sample of dictionary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;obs&#34;:&lt;/code&gt; Dict of &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;&#34;key0&#34;:&lt;/code&gt; Tensor of shape &lt;code&gt;(To, *)&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;&#34;key1&#34;:&lt;/code&gt; Tensor fo shape &lt;code&gt;(To,H,W,3)&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;action&#34;:&lt;/code&gt; Tensor of shape &lt;code&gt;(Ta, Da)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Its &lt;code&gt;get_normalizer&lt;/code&gt; method returns a &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/model/common/normalizer.py&#34;&gt;&lt;code&gt;LinearNormalizer&lt;/code&gt;&lt;/a&gt; with keys &lt;code&gt;&#34;key0&#34;,&#34;key1&#34;,&#34;action&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;To = 3&#xA;Ta = 4&#xA;T = 6&#xA;|o|o|o|&#xA;| | |a|a|a|a|&#xA;|o|o|&#xA;| |a|a|a|a|a|&#xA;| | | | |a|a|&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Terminology in the paper: &lt;code&gt;varname&lt;/code&gt; in the codebase&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Observation Horizon: &lt;code&gt;To|n_obs_steps&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Action Horizon: &lt;code&gt;Ta|n_action_steps&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prediction Horizon: &lt;code&gt;T|horizon&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The classical (e.g. MDP) single step observation/action formulation is included as a special case where &lt;code&gt;To=1&lt;/code&gt; and &lt;code&gt;Ta=1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üî© Key Components&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;Workspace&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;code&gt;Workspace&lt;/code&gt; object encapsulates all states and code needed to run an experiment.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inherits from &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/workspace/base_workspace.py&#34;&gt;&lt;code&gt;BaseWorkspace&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A single &lt;code&gt;OmegaConf&lt;/code&gt; config object generated by &lt;code&gt;hydra&lt;/code&gt; should contain all information needed to construct the Workspace object and running experiments. This config correspond to &lt;code&gt;config/&amp;lt;workspace_name&amp;gt;.yaml&lt;/code&gt; + hydra overrides.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;run&lt;/code&gt; method contains the entire pipeline for the experiment.&lt;/li&gt; &#xA; &lt;li&gt;Checkpoints happen at the &lt;code&gt;Workspace&lt;/code&gt; level. All training states implemented as object attributes are automatically saved by the &lt;code&gt;save_checkpoint&lt;/code&gt; method.&lt;/li&gt; &#xA; &lt;li&gt;All other states for the experiment should be implemented as local variables in the &lt;code&gt;run&lt;/code&gt; method.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The entrypoint for training is &lt;code&gt;train.py&lt;/code&gt; which uses &lt;code&gt;@hydra.main&lt;/code&gt; decorator. Read &lt;a href=&#34;https://hydra.cc/&#34;&gt;hydra&lt;/a&gt;&#39;s official documentation for command line arguments and config overrides. For example, the argument &lt;code&gt;task=&amp;lt;task_name&amp;gt;&lt;/code&gt; will replace the &lt;code&gt;task&lt;/code&gt; subtree of the config with the content of &lt;code&gt;config/task/&amp;lt;task_name&amp;gt;.yaml&lt;/code&gt;, thereby selecting the task to run for this experiment.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;code&gt;Dataset&lt;/code&gt; object:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inherits from &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Returns a sample conforming to &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/#the-interface&#34;&gt;the interface&lt;/a&gt; depending on whether the task has Low Dim or Image observations.&lt;/li&gt; &#xA; &lt;li&gt;Has a method &lt;code&gt;get_normalizer&lt;/code&gt; that returns a &lt;code&gt;LinearNormalizer&lt;/code&gt; conforming to &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/#the-interface&#34;&gt;the interface&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Normalization is a very common source of bugs during project development. It is sometimes helpful to print out the specific &lt;code&gt;scale&lt;/code&gt; and &lt;code&gt;bias&lt;/code&gt; vectors used for each key in the &lt;code&gt;LinearNormalizer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Most of our implementations of &lt;code&gt;Dataset&lt;/code&gt; uses a combination of &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/#replaybuffer&#34;&gt;&lt;code&gt;ReplayBuffer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/common/sampler.py&#34;&gt;&lt;code&gt;SequenceSampler&lt;/code&gt;&lt;/a&gt; to generate samples. Correctly handling padding at the beginning and the end of each demonstration episode according to &lt;code&gt;To&lt;/code&gt; and &lt;code&gt;Ta&lt;/code&gt; is important for good performance. Please read our &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/common/sampler.py&#34;&gt;&lt;code&gt;SequenceSampler&lt;/code&gt;&lt;/a&gt; before implementing your own sampling method.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;Policy&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;code&gt;Policy&lt;/code&gt; object:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inherits from &lt;code&gt;BaseLowdimPolicy&lt;/code&gt; or &lt;code&gt;BaseImagePolicy&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Has a method &lt;code&gt;predict_action&lt;/code&gt; that given observation dict, predicts actions conforming to &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/#the-interface&#34;&gt;the interface&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Has a method &lt;code&gt;set_normalizer&lt;/code&gt; that takes in a &lt;code&gt;LinearNormalizer&lt;/code&gt; and handles observation/action normalization internally in the policy.&lt;/li&gt; &#xA; &lt;li&gt;(optional) Might has a method &lt;code&gt;compute_loss&lt;/code&gt; that takes in a batch and returns the loss to be optimized.&lt;/li&gt; &#xA; &lt;li&gt;(optional) Usually each &lt;code&gt;Policy&lt;/code&gt; class correspond to a &lt;code&gt;Workspace&lt;/code&gt; class due to the differences of training and evaluation process between methods.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;code&gt;EnvRunner&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;code&gt;EnvRunner&lt;/code&gt; object abstracts away the subtle differences between different task environments.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Has a method &lt;code&gt;run&lt;/code&gt; that takes a &lt;code&gt;Policy&lt;/code&gt; object for evaluation, and returns a dict of logs and metrics. Each value should be compatible with &lt;code&gt;wandb.log&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To maximize evaluation speed, we usually vectorize environments using our modification of &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/gym_util/async_vector_env.py&#34;&gt;&lt;code&gt;gym.vector.AsyncVectorEnv&lt;/code&gt;&lt;/a&gt; which runs each individual environment in a separate process (workaround python GIL).&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è Since subprocesses are launched using &lt;code&gt;fork&lt;/code&gt; on linux, you need to be specially careful for environments that creates its OpenGL context during initialization (e.g. robosuite) which, once inherited by the child process memory space, often causes obscure bugs like segmentation fault. As a workaround, you can provide a &lt;code&gt;dummy_env_fn&lt;/code&gt; that constructs an environment without initializing OpenGL.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;ReplayBuffer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/common/replay_buffer.py&#34;&gt;&lt;code&gt;ReplayBuffer&lt;/code&gt;&lt;/a&gt; is a key data structure for storing a demonstration dataset both in-memory and on-disk with chunking and compression. It makes heavy use of the &lt;a href=&#34;https://zarr.readthedocs.io/en/stable/index.html&#34;&gt;&lt;code&gt;zarr&lt;/code&gt;&lt;/a&gt; format but also has a &lt;code&gt;numpy&lt;/code&gt; backend for lower access overhead.&lt;/p&gt; &#xA;&lt;p&gt;On disk, it can be stored as a nested directory (e.g. &lt;code&gt;data/pusht_cchi_v7_replay.zarr&lt;/code&gt;) or a zip file (e.g. &lt;code&gt;data/robomimic/datasets/square/mh/image_abs.hdf5.zarr.zip&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Due to the relative small size of our datasets, it&#39;s often possible to store the entire image-based dataset in RAM with &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/codecs/imagecodecs_numcodecs.py&#34;&gt;&lt;code&gt;Jpeg2000&lt;/code&gt; compression&lt;/a&gt; which eliminates disk IO during training at the expense increasing of CPU workload.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;data/pusht_cchi_v7_replay.zarr&#xA; ‚îú‚îÄ‚îÄ data&#xA; ‚îÇ   ‚îú‚îÄ‚îÄ action (25650, 2) float32&#xA; ‚îÇ   ‚îú‚îÄ‚îÄ img (25650, 96, 96, 3) float32&#xA; ‚îÇ   ‚îú‚îÄ‚îÄ keypoint (25650, 9, 2) float32&#xA; ‚îÇ   ‚îú‚îÄ‚îÄ n_contacts (25650, 1) float32&#xA; ‚îÇ   ‚îî‚îÄ‚îÄ state (25650, 5) float32&#xA; ‚îî‚îÄ‚îÄ meta&#xA;     ‚îî‚îÄ‚îÄ episode_ends (206,) int64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each array in &lt;code&gt;data&lt;/code&gt; stores one data field from all episodes concatenated along the first dimension (time). The &lt;code&gt;meta/episode_ends&lt;/code&gt; array stores the end index for each episode along the fist dimension.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;SharedMemoryRingBuffer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/shared_memory/shared_memory_ring_buffer.py&#34;&gt;&lt;code&gt;SharedMemoryRingBuffer&lt;/code&gt;&lt;/a&gt; is a lock-free FILO data structure used extensively in our &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/real_world&#34;&gt;real robot implementation&lt;/a&gt; to utilize multiple CPU cores while avoiding pickle serialization and locking overhead for &lt;code&gt;multiprocessing.Queue&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As an example, we would like to get the most recent &lt;code&gt;To&lt;/code&gt; frames from 5 RealSense cameras. We launch 1 realsense SDK/pipeline per process using &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/real_world/single_realsense.py&#34;&gt;&lt;code&gt;SingleRealsense&lt;/code&gt;&lt;/a&gt;, each continuously writes the captured images into a &lt;code&gt;SharedMemoryRingBuffer&lt;/code&gt; shared with the main process. We can very quickly get the last &lt;code&gt;To&lt;/code&gt; frames in the main process due to the FILO nature of &lt;code&gt;SharedMemoryRingBuffer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also implemented &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/shared_memory/shared_memory_queue.py&#34;&gt;&lt;code&gt;SharedMemoryQueue&lt;/code&gt;&lt;/a&gt; for FIFO, which is used in &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/real_world/rtde_interpolation_controller.py&#34;&gt;&lt;code&gt;RTDEInterpolationController&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;RealEnv&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;In contrast to &lt;a href=&#34;https://gymnasium.farama.org/&#34;&gt;OpenAI Gym&lt;/a&gt;, our polices interact with the environment asynchronously. In &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/real_world/real_env.py&#34;&gt;&lt;code&gt;RealEnv&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;step&lt;/code&gt; method in &lt;code&gt;gym&lt;/code&gt; is split into two methods: &lt;code&gt;get_obs&lt;/code&gt; and &lt;code&gt;exec_actions&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;get_obs&lt;/code&gt; method returns the latest observation from &lt;code&gt;SharedMemoryRingBuffer&lt;/code&gt; as well as their corresponding timestamps. This method can be call at any time during an evaluation episode.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;exec_actions&lt;/code&gt; method accepts a sequence of actions and timestamps for the expected time of execution for each step. Once called, the actions are simply enqueued to the &lt;code&gt;RTDEInterpolationController&lt;/code&gt;, and the method returns without blocking for execution.&lt;/p&gt; &#xA;&lt;h2&gt;ü©π Adding a Task&lt;/h2&gt; &#xA;&lt;p&gt;Read and imitate:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;diffusion_policy/dataset/pusht_image_dataset.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;diffusion_policy/env_runner/pusht_image_runner.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;diffusion_policy/config/task/pusht_image.yaml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure that &lt;code&gt;shape_meta&lt;/code&gt; correspond to input and output shapes for your task. Make sure &lt;code&gt;env_runner._target_&lt;/code&gt; and &lt;code&gt;dataset._target_&lt;/code&gt; point to the new classes you have added. When training, add &lt;code&gt;task=&amp;lt;your_task_name&amp;gt;&lt;/code&gt; to &lt;code&gt;train.py&lt;/code&gt;&#39;s arguments.&lt;/p&gt; &#xA;&lt;h2&gt;ü©π Adding a Method&lt;/h2&gt; &#xA;&lt;p&gt;Read and imitate:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;diffusion_policy/workspace/train_diffusion_unet_image_workspace.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;diffusion_policy/policy/diffusion_unet_image_policy.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;diffusion_policy/config/train_diffusion_unet_image_workspace.yaml&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure your workspace yaml&#39;s &lt;code&gt;_target_&lt;/code&gt; points to the new workspace class you created.&lt;/p&gt; &#xA;&lt;h2&gt;üè∑Ô∏è License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is released under the MIT license. See &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; &#xA;&lt;h2&gt;üôè Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/model/diffusion/conditional_unet1d.py&#34;&gt;&lt;code&gt;ConditionalUnet1D&lt;/code&gt;&lt;/a&gt; implementation is adapted from &lt;a href=&#34;https://github.com/jannerm/diffuser&#34;&gt;Planning with Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Our &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/model/diffusion/transformer_for_diffusion.py&#34;&gt;&lt;code&gt;TransformerForDiffusion&lt;/code&gt;&lt;/a&gt; implementation is adapted from &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;MinGPT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/model/bet&#34;&gt;BET&lt;/a&gt; baseline is adapted from &lt;a href=&#34;https://github.com/notmahi/bet&#34;&gt;its original repo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/policy/ibc_dfo_lowdim_policy.py&#34;&gt;IBC&lt;/a&gt; baseline is adapted from &lt;a href=&#34;https://github.com/kevinzakka/ibc&#34;&gt;Kevin Zakka&#39;s reimplementation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/ARISE-Initiative/robomimic&#34;&gt;Robomimic&lt;/a&gt; tasks and &lt;a href=&#34;https://github.com/ARISE-Initiative/robomimic/raw/master/robomimic/models/obs_nets.py&#34;&gt;&lt;code&gt;ObservationEncoder&lt;/code&gt;&lt;/a&gt; are used extensively in this project.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/env/pusht&#34;&gt;Push-T&lt;/a&gt; task is adapted from &lt;a href=&#34;https://github.com/google-research/ibc&#34;&gt;IBC&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/env/block_pushing&#34;&gt;Block Pushing&lt;/a&gt; task is adapted from &lt;a href=&#34;https://github.com/notmahi/bet&#34;&gt;BET&lt;/a&gt; and &lt;a href=&#34;https://github.com/google-research/ibc&#34;&gt;IBC&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/env/kitchen&#34;&gt;Kitchen&lt;/a&gt; task is adapted from &lt;a href=&#34;https://github.com/notmahi/bet&#34;&gt;BET&lt;/a&gt; and &lt;a href=&#34;https://github.com/google-research/relay-policy-learning&#34;&gt;Relay Policy Learning&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Our &lt;a href=&#34;https://raw.githubusercontent.com/real-stanford/diffusion_policy/main/diffusion_policy/shared_memory&#34;&gt;shared_memory&lt;/a&gt; data structures are heavily inspired by &lt;a href=&#34;https://gitlab.com/osu-nrsg/shared-ndarray2&#34;&gt;shared-ndarray2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>