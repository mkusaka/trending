<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-18T01:43:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>linyiLYi/street-fighter-ai</title>
    <updated>2023-04-18T01:43:50Z</updated>
    <id>tag:github.com,2023-04-18:/linyiLYi/street-fighter-ai</id>
    <link href="https://github.com/linyiLYi/street-fighter-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is an AI agent for Street Fighter II Champion Edition.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SFighterAI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linyiLYi/street-fighter-ai/master/README_CN.md&#34;&gt;简体中文&lt;/a&gt; | English&lt;/p&gt; &#xA;&lt;p&gt;This project is an AI agent trained using deep reinforcement learning to beat the final boss in the game &#34;Street Fighter II: Special Champion Edition&#34;. The AI agent makes decisions based solely on the game screen&#39;s RGB pixel values. In the provided save state, the agent achieves a 100% win rate in the first round of the final level (overfitting occurs, see the &lt;a href=&#34;https://raw.githubusercontent.com/linyiLYi/street-fighter-ai/master/#running-tests&#34;&gt;Running Tests&lt;/a&gt; section for discussion).&lt;/p&gt; &#xA;&lt;h3&gt;File Structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;├───data&#xA;├───main&#xA;│   ├───logs&#xA;│   ├───trained_models&#xA;│   └───scripts&#xA;├───utils&#xA;│   └───scripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The game configuration files are stored in the &lt;code&gt;data/&lt;/code&gt; folder, and the main project code is in the &lt;code&gt;main/&lt;/code&gt; folder. Within &lt;code&gt;main/&lt;/code&gt;, the &lt;code&gt;logs/&lt;/code&gt; folder contains terminal/console outputs and data curves recording the training process (viewable with Tensorboard), while the &lt;code&gt;trained_models/&lt;/code&gt; folder contains model weights from different stages. These weights can be used for running tests in &lt;code&gt;test.py&lt;/code&gt; to observe the performance of the AI agent&#39;s learned strategies at different training stages.&lt;/p&gt; &#xA;&lt;h2&gt;Running Guide&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the Python programming language and primarily utilizes standard libraries like &lt;a href=&#34;https://retro.readthedocs.io/en/latest/getting_started.html&#34;&gt;OpenAI Gym Retro&lt;/a&gt; and &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34;&gt;Stable-Baselines3&lt;/a&gt; .The Python version used is 3.8.10, and it is recommended to use &lt;a href=&#34;https://www.anaconda.com&#34;&gt;Anaconda&lt;/a&gt; to configure the Python environment. The following setup process has been tested on Windows 11. Below are console/terminal/shell commands.&lt;/p&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a conda environment named StreetFighterAI with Python version 3.8.10&#xA;conda create -n StreetFighterAI python=3.8.10&#xA;conda activate StreetFighterAI&#xA;&#xA;# Install Python libraries&#xA;cd [parent_directory_of_project]/street-fighter-ai/main&#xA;pip install -r requirements.txt&#xA;&#xA;# Run script to locate gym-retro game folder&#xA;cd ..&#xA;python .\utils\print_game_lib_folder.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the console outputs the folder path, copy it to the file explorer and navigate to the corresponding path. This folder contains the game data files for &#34;Street Fighter II: Special Champion Edition&#34; within gym-retro, including the game ROM file and data configuration files. Copy the &lt;code&gt;Champion.Level12.RyuVsBison.state&lt;/code&gt;, &lt;code&gt;data.json&lt;/code&gt;, &lt;code&gt;metadata.json&lt;/code&gt;, and &lt;code&gt;scenario.json&lt;/code&gt; files from the &lt;code&gt;data/&lt;/code&gt; folder of this project into the game data folder, replacing the original files (administrator privileges may be required). The &lt;code&gt;.state&lt;/code&gt; file is a save state for the game&#39;s highest difficulty level, while the three &lt;code&gt;.json&lt;/code&gt; files are gym-retro configuration files storing game information memory addresses (this project only uses [agent_hp] and [enemy_hp] for reading character health values in real-time).&lt;/p&gt; &#xA;&lt;p&gt;To run the program, you will also need the game ROM file for &#34;Street Fighter II: Special Champion Edition&#34;, which is not provided by gym-retro and must be obtained legally through other means. You can refer to this &lt;a href=&#34;https://wowroms.com/en/roms/sega-genesis-megadrive/street-fighter-ii-special-champion-edition-europe/26496.html&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have legally obtained the game ROM file, copy it to the aforementioned gym-retro game data folder and rename it to &lt;code&gt;rom.md&lt;/code&gt;. At this point, the environment setup is complete.&lt;/p&gt; &#xA;&lt;p&gt;Note: If you want to record videos of the AI agent&#39;s gameplay, you will need to install &lt;a href=&#34;https://ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;a name=&#34;running-tests&#34;&gt;&lt;/a&gt;Running Tests&lt;/h3&gt; &#xA;&lt;p&gt;Once the environment is set up, you can run &lt;code&gt;test.py&lt;/code&gt; in the &lt;code&gt;main/&lt;/code&gt; folder to test and experience the AI agent&#39;s performance at different stages of training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent_directory_of_project]/street-fighter-ai/main&#xA;python test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weight files are stored in the &lt;code&gt;main/trained_models/&lt;/code&gt; folder. The default model used in &lt;code&gt;test.py&lt;/code&gt; is &lt;code&gt;ppo_ryu_2500000_steps_updated.zip&lt;/code&gt;, which has good generalization and is capable of beating the final level of Street Fighter II: Special Champion Edition. If you want to see the performance of other models, you can change the &lt;code&gt;model_path&lt;/code&gt; variable in &lt;code&gt;test.py&lt;/code&gt; to the path of another model file. The observed performance of the models at various training stages is as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ppo_ryu_2000000_steps_updated: Just beginning to overfit state, generalizable but not quite capable.&lt;/li&gt; &#xA; &lt;li&gt;ppo_ryu_2500000_steps_updated: Approaching the final overfitted state, cannot dominate first round but partially generalizable. High chance of beating the final stage.&lt;/li&gt; &#xA; &lt;li&gt;ppo_ryu_3000000_steps_updated: Near the final overfitted state, almost dominate first round but barely generalizable.&lt;/li&gt; &#xA; &lt;li&gt;ppo_ryu_7000000_steps_updated: Overfitted, dominates first round but not generalizable.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training the Model&lt;/h3&gt; &#xA;&lt;p&gt;If you want to train your own model, you can run &lt;code&gt;train.py&lt;/code&gt; in the &lt;code&gt;main/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent_directory_of_project]/street-fighter-ai/main&#xA;python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Viewing Training Curves&lt;/h3&gt; &#xA;&lt;p&gt;The project includes Tensorboard graphs of the training process. You can use Tensorboard to view detailed data. It is recommended to use the integrated Tensorboard plugin in VSCode to view the data directly. The traditional viewing method is listed below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent_directory_of_project]/street-fighter-ai/main&#xA;tensorboard --logdir=logs/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open the default Tensorboard service address &lt;code&gt;http://localhost:6006/&lt;/code&gt; in your browser to view interactive graphs of the training process.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project uses open-source libraries such as &lt;a href=&#34;https://retro.readthedocs.io/en/latest/getting_started.html&#34;&gt;OpenAI Gym Retro&lt;/a&gt;、&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34;&gt;Stable-Baselines3&lt;/a&gt;. The contributions of all the developers to the open-source community are appreciated!&lt;/p&gt; &#xA;&lt;p&gt;Two papers that had a significant impact on this project:&lt;/p&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://arxiv.org/abs/2210.10595&#34;&gt;DIAMBRA Arena A New Reinforcement Learning Platform for Research and Experimentation&lt;/a&gt; The valuable summary of the experience in setting hyperparameters for deep reinforcement learning models in fighting games in this paper was of great help to the training process of this project.&lt;/p&gt; &#xA;&lt;p&gt;[2] &lt;a href=&#34;https://ieee-cog.org/2022/assets/papers/paper_111.pdf&#34;&gt;Mitigating Cowardice for Reinforcement Learning&lt;/a&gt; The &#34;penalty decay&#34; mechanism proposed in this paper effectively solved the &#34;cowardice&#34; problem (always avoiding opponents and not daring to even try attacking moves).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ading2210/poe-api</title>
    <updated>2023-04-18T01:43:50Z</updated>
    <id>tag:github.com,2023-04-18:/ading2210/poe-api</id>
    <link href="https://github.com/ading2210/poe-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A reverse engineered Python API wrapper for Quora&#39;s Poe, which provides free access to ChatGPT, GPT-4, and Claude.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Poe API&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/poe-api/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/poe-api.svg?sanitize=true&#34; alt=&#34;PyPi Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a reverse engineered API wrapper for Quora&#39;s Poe, which allows you free access to OpenAI&#39;s ChatGPT and GPT-4, as well as Antropic&#39;s Claude.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#documentation&#34;&gt;Documentation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#finding-your-token&#34;&gt;Finding Your Token&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#using-the-client&#34;&gt;Using the Client&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#downloading-the-available-bots&#34;&gt;Downloading the Available Bots&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#creating-new-bots&#34;&gt;Creating New Bots&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#editing-a-bot&#34;&gt;Editing a Bot&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#sending-messages&#34;&gt;Sending Messages&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#clearing-the-conversation-context&#34;&gt;Clearing the Conversation Context&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#downloading-conversation-history&#34;&gt;Downloading Conversation History&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#deleting-messages&#34;&gt;Deleting Messages&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#purging-a-conversation&#34;&gt;Purging a Conversation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#misc&#34;&gt;Misc&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#changing-the-logging-level&#34;&gt;Changing the Logging Level&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#setting-a-custom-user-agent&#34;&gt;Setting a Custom User-Agent&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#&#34;&gt;Bypassing The GPT-4 Quota&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#copyright&#34;&gt;Copyright&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ading2210/poe-api/main/#copyright-notice&#34;&gt;Copyright Notice&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Table of contents generated with &lt;a href=&#34;http://ecotrust-canada.github.io/markdown-toc&#34;&gt;markdown-toc&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Log in with token&lt;/li&gt; &#xA; &lt;li&gt;Proxy requests + websocket&lt;/li&gt; &#xA; &lt;li&gt;Download bot list&lt;/li&gt; &#xA; &lt;li&gt;Send messages&lt;/li&gt; &#xA; &lt;li&gt;Stream bot responses&lt;/li&gt; &#xA; &lt;li&gt;Clear conversation context&lt;/li&gt; &#xA; &lt;li&gt;Download conversation history&lt;/li&gt; &#xA; &lt;li&gt;Delete messages&lt;/li&gt; &#xA; &lt;li&gt;Purge an entire conversation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation:&lt;/h2&gt; &#xA;&lt;p&gt;You can install this library by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install poe-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation:&lt;/h2&gt; &#xA;&lt;p&gt;Examples can be found in the &lt;code&gt;/examples&lt;/code&gt; directory. To run these examples, pass in your token as a command-line argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 examples/temporary_message.py &#34;TOKEN_HERE&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Finding Your Token:&lt;/h3&gt; &#xA;&lt;p&gt;Log into &lt;a href=&#34;https://poe.com&#34;&gt;Poe&lt;/a&gt; on any web browser, then open your browser&#39;s developer tools (also known as &#34;inspect&#34;) and look for the value of the &lt;code&gt;p-b&lt;/code&gt; cookie in the following menus:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chromium: Devtools &amp;gt; Application &amp;gt; Cookies &amp;gt; poe.com&lt;/li&gt; &#xA; &lt;li&gt;Firefox: Devtools &amp;gt; Storage &amp;gt; Cookies&lt;/li&gt; &#xA; &lt;li&gt;Safari: Devtools &amp;gt; Storage &amp;gt; Cookies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using the Client:&lt;/h3&gt; &#xA;&lt;p&gt;To use this library, simply import &lt;code&gt;poe&lt;/code&gt; and create a &lt;code&gt;poe.Client&lt;/code&gt; instance. The Client class accepts the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;token&lt;/code&gt; - The token to use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;proxy = None&lt;/code&gt; - The proxy to use, in the format &lt;code&gt;protocol://host:port&lt;/code&gt;. The socks5/socks4 protocol is reccommended.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Regular Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import poe&#xA;client = poe.Client(&#34;TOKEN_HERE&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Proxied Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import poe&#xA;client = poe.Client(&#34;TOKEN_HERE&#34;, proxy=&#34;socks5://178.62.100.151:59166&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the following examples assume &lt;code&gt;client&lt;/code&gt; is the name of your &lt;code&gt;poe.Client&lt;/code&gt; instance. If the token is invalid, a RuntimeError will be raised.&lt;/p&gt; &#xA;&lt;h4&gt;Downloading the Available Bots:&lt;/h4&gt; &#xA;&lt;p&gt;The client downloads all of the available bots upon initialization and stores them within &lt;code&gt;client.bots&lt;/code&gt;. A dictionary that maps bot codenames to their display names can be found at &lt;code&gt;client.bot_names&lt;/code&gt;. If you want to refresh these values, you can call &lt;code&gt;client.get_bots&lt;/code&gt;. This function takes the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;download_next_data = True&lt;/code&gt; - Whether or not to redownload the &lt;code&gt;__NEXT_DATA__&lt;/code&gt;, which is required if the bot list has changed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(client.bot_names)&#xA;#{&#39;capybara&#39;: &#39;Sage&#39;, &#39;beaver&#39;: &#39;GPT-4&#39;, &#39;a2_2&#39;: &#39;Claude+&#39;, &#39;a2&#39;: &#39;Claude&#39;, &#39;chinchilla&#39;: &#39;ChatGPT&#39;, &#39;nutria&#39;: &#39;Dragonfly&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that, on free accounts, Claude+ (a2_2) has a limit of 3 messages per day and GPT-4 (beaver) has a limit of 1 message per day. For all the other chatbots, there seems to be a rate limit of 10 messages per minute.&lt;/p&gt; &#xA;&lt;h4&gt;Creating New Bots:&lt;/h4&gt; &#xA;&lt;p&gt;You can create a new bot using the &lt;code&gt;client.create_bot&lt;/code&gt; function, which accepts the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;handle&lt;/code&gt; - The handle of the new bot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt = &#34;&#34;&lt;/code&gt; - The prompt for the new bot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;base_model = &#34;chinchilla&#34;&lt;/code&gt; - The model that the new bot uses. This must be either &lt;code&gt;&#34;chinchilla&#34;&lt;/code&gt; (ChatGPT) or &lt;code&gt;&#34;a2&#34;&lt;/code&gt; (Claude).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;description = &#34;&#34;&lt;/code&gt; - The description for the new bot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;intro_message = &#34;&#34;&lt;/code&gt; - The intro message for the new bot. If this is an empty string then the bot will not have an intro message.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt_public = True&lt;/code&gt; - Whether or not the prompt should be publicly visible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pfp_url = None&lt;/code&gt; - The URL for the bot&#39;s profile picture. Currently, there is no way to actually upload a custom image using this library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;linkification = False&lt;/code&gt; - Whether or not the bot should turn some text in the response into clickable links.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;markdown_rendering = True&lt;/code&gt; - Whether or not to enable markdown rendering for the bot&#39;s responses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;suggested_replies = False&lt;/code&gt; - Whether or not the bot should suggest possible replies after each response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;private = False&lt;/code&gt; - Whether or not the bot should be private.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, there are some arguments that seem to be for the upcoming bot developer API. You do not need to specify these, although they may become useful in the future. The description for these arguments are currently my best guesses for what they do:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;api_key = None&lt;/code&gt; - The API key for the new bot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_bot = False&lt;/code&gt; - Whether or not the bot has API functionally enabled.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_url = None&lt;/code&gt; - The API URL for the new bot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A full example of how to create and edit bots is located at &lt;code&gt;examples/create_bot.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_bot = client.create_bot(bot_name, &#34;prompt goes here&#34;, base_model=&#34;a2&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Editing a Bot:&lt;/h4&gt; &#xA;&lt;p&gt;You can edit a custom bot using the &lt;code&gt;client.edit_bot&lt;/code&gt; function, which accepts the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bot_id&lt;/code&gt; - The &lt;code&gt;botId&lt;/code&gt; of the bot to edit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;handle&lt;/code&gt; - The handle of the bot to edit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt; - The prompt for the new bot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;base_model = &#34;chinchilla&#34;&lt;/code&gt; - The new model that the bot uses. This can be any of the preset models, including ones that are limited on free accounts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;description = &#34;&#34;&lt;/code&gt; - The new description for the bot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;intro_message = &#34;&#34;&lt;/code&gt; - The new intro message for the bot. If this is an empty string then the bot will not have an intro message.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt_public = True&lt;/code&gt; - Whether or not the prompt should be publicly visible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pfp_url = None&lt;/code&gt; - The URL for the bot&#39;s profile picture. Currently, there is no way to actually upload a custom image using this library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;linkification = False&lt;/code&gt; - Whether or not the bot should turn some text in the response into clickable links.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;markdown_rendering = True&lt;/code&gt; - Whether or not to enable markdown rendering for the bot&#39;s responses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;suggested_replies = False&lt;/code&gt; - Whether or not the bot should suggest possible replies after each response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;private = False&lt;/code&gt; - Whether or not the bot should be private.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Unreleased bot developer API arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;api_key = None&lt;/code&gt; - The new API key for the bot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_url = None&lt;/code&gt; - The new API URL for the bot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A full example of how to create and edit bots is located at &lt;code&gt;examples/create_bot.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;edit_result = client.edit_bot(1086981, &#34;bot_handle_here&#34;, base_model=&#34;beaver&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Sending Messages:&lt;/h4&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;client.send_message&lt;/code&gt; function to send a message to a chatbot, which accepts the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;chatbot&lt;/code&gt; - The codename of the chatbot. (example: &lt;code&gt;capybara&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;message&lt;/code&gt; - The message to send to the chatbot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;with_chat_break = False&lt;/code&gt; - Whether the conversation context should be cleared.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timeout = 20&lt;/code&gt; - The max number of seconds in between recieved chunks until a &lt;code&gt;RuntimeError&lt;/code&gt; is raised.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The function is a generator which returns the most recent version of the generated message whenever it is updated.&lt;/p&gt; &#xA;&lt;p&gt;Streamed Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;message = &#34;Summarize the GNU GPL v3&#34;&#xA;for chunk in client.send_message(&#34;capybara&#34;, message):&#xA;  print(chunk[&#34;text_new&#34;], end=&#34;&#34;, flush=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Non-Streamed Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;message = &#34;Summarize the GNU GPL v3&#34;&#xA;for chunk in client.send_message(&#34;capybara&#34;, message):&#xA;  pass&#xA;print(chunk[&#34;text&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also send multiple messages in parallel using &lt;code&gt;threading&lt;/code&gt; and recieve their responses seperately, as demonstrated in &lt;code&gt;/examples/parallel_messages.py&lt;/code&gt;. Note that if you send messages too fast, the server will give an error, but the request will eventually succeed.&lt;/p&gt; &#xA;&lt;h4&gt;Clearing the Conversation Context:&lt;/h4&gt; &#xA;&lt;p&gt;If you want to clear the the context of a conversation without sending a message, you can use &lt;code&gt;client.send_chat_break&lt;/code&gt;. The only argument is the codename of the bot whose context will be cleared.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client.send_chat_break(&#34;capybara&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The function returns the message which represents the chat break.&lt;/p&gt; &#xA;&lt;h4&gt;Downloading Conversation History:&lt;/h4&gt; &#xA;&lt;p&gt;To download past messages in a conversation, use the &lt;code&gt;client.get_message_history&lt;/code&gt; function, which accepts the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;chatbot&lt;/code&gt; - The codename of the chatbot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;count = 25&lt;/code&gt; - The number of messages to download.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cursor = None&lt;/code&gt; - The message ID to start at instead of the latest one.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that if you don&#39;t specify a cursor, the client will have to perform an extra request to determine what the latest cursor is.&lt;/p&gt; &#xA;&lt;p&gt;The returned messages are ordered from oldest to newest.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;message_history = client.get_message_history(&#34;capybara&#34;, count=10)&#xA;print(json.dumps(message_history, indent=2))&#xA;&#34;&#34;&#34;&#xA;[&#xA;  {&#xA;    &#34;node&#34;: {&#xA;      &#34;id&#34;: &#34;TWVzc2FnZToxMDEwNzYyODU=&#34;,&#xA;      &#34;messageId&#34;: 101076285,&#xA;      &#34;creationTime&#34;: 1679298157718888,&#xA;      &#34;text&#34;: &#34;&#34;,&#xA;      &#34;author&#34;: &#34;chat_break&#34;,&#xA;      &#34;linkifiedText&#34;: &#34;&#34;,&#xA;      &#34;state&#34;: &#34;complete&#34;,&#xA;      &#34;suggestedReplies&#34;: [],&#xA;      &#34;vote&#34;: null,&#xA;      &#34;voteReason&#34;: null,&#xA;      &#34;__typename&#34;: &#34;Message&#34;&#xA;    },&#xA;    &#34;cursor&#34;: &#34;101076285&#34;,&#xA;    &#34;id&#34;: &#34;TWVzc2FnZUVkZ2U6MTAxMDc2Mjg1OjEwMTA3NjI4NQ==&#34;&#xA;  },&#xA;  ...&#xA;]&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Deleting Messages:&lt;/h4&gt; &#xA;&lt;p&gt;To delete messages, use the &lt;code&gt;client.delete_message&lt;/code&gt; function, which accepts a single argument. You can pass a single message ID into it to delete a single message, or you can pass a list of message IDs to delete multiple messages at once.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#delete a single message&#xA;client.delete_message(96105719)&#xA;&#xA;#delete multiple messages at once&#xA;client.delete_message([96105719, 96097108, 96097078, 96084421, 96084402])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Purging a Conversation:&lt;/h4&gt; &#xA;&lt;p&gt;To purge an entire conversation, or just the last few messages, you can use the &lt;code&gt;client.purge_conversation&lt;/code&gt; function. This function accepts the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;chatbot&lt;/code&gt; - The codename of the chatbot.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;count = -1&lt;/code&gt; - The number of messages to be deleted, starting from the latest one. The default behavior is to delete every single message.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#purge just the last 10 messages&#xA;client.purge_conversation(&#34;capybara&#34;, count=10)&#xA;&#xA;#purge the entire conversation&#xA;client.purge_conversation(&#34;capybara&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Getting the Remaining Messages:&lt;/h4&gt; &#xA;&lt;p&gt;To get the number of messages remaining in the quota for a conversation, use the &lt;code&gt;client.get_remaining_messages&lt;/code&gt; function. This function accepts the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;chatbot&lt;/code&gt; - The codename of the chatbot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The function will return the number of messages remaining, or &lt;code&gt;None&lt;/code&gt; if the bot does not have a quota.&lt;/p&gt; &#xA;&lt;h3&gt;Misc:&lt;/h3&gt; &#xA;&lt;h4&gt;Changing the Logging Level:&lt;/h4&gt; &#xA;&lt;p&gt;If you want to show debug messages, simply call &lt;code&gt;poe.logger.setLevel&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import poe&#xA;import logging&#xA;poe.logger.setLevel(logging.INFO)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Setting a Custom User-Agent:&lt;/h4&gt; &#xA;&lt;p&gt;If you want to change the user-agent that is being spoofed, set &lt;code&gt;poe.user_agent&lt;/code&gt; after importing the library.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import poe&#xA;poe.user_agent = &#34;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Bypassing the Free Account Quota:&lt;/h4&gt; &#xA;&lt;p&gt;You can bypass the free account quota simply by editing a custom bot and setting the base model to one of the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;a2_2&#34;&lt;/code&gt; - Claude+ (normal limit: 3 messages/day)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;beaver&#34;&lt;/code&gt; - GPT-4 (normal limit: 1 message/day)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example of how to do this is located at &lt;code&gt;examples/create_bot.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright:&lt;/h2&gt; &#xA;&lt;p&gt;This program is licensed under the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.txt&#34;&gt;GNU GPL v3&lt;/a&gt;. Most code, with the exception of the GraphQL queries, has been written by me, &lt;a href=&#34;https://github.com/ading2210&#34;&gt;ading2210&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Reverse engineering the &lt;code&gt;poe-tag-id&lt;/code&gt; header has been done by &lt;a href=&#34;https://github.com/xtekky&#34;&gt;xtekky&lt;/a&gt; in &lt;a href=&#34;https://github.com/ading2210/poe-api/pull/39&#34;&gt;PR #39&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;client.get_remaining_messages&lt;/code&gt; was written by &lt;a href=&#34;https://github.com/Snowad14&#34;&gt;Snowad14&lt;/a&gt; in &lt;a href=&#34;https://github.com/ading2210/poe-api/pull/46&#34;&gt;PR #46&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Most of the GraphQL queries are taken from &lt;a href=&#34;https://github.com/muharamdani/poe&#34;&gt;muharamdani/poe&lt;/a&gt;, which is licenced under the ISC License.&lt;/p&gt; &#xA;&lt;h3&gt;Copyright Notice:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;ading2210/poe-api: a reverse engineered Python API wrapepr for Quora&#39;s Poe&#xA;Copyright (C) 2023 ading2210&#xA;&#xA;This program is free software: you can redistribute it and/or modify&#xA;it under the terms of the GNU General Public License as published by&#xA;the Free Software Foundation, either version 3 of the License, or&#xA;(at your option) any later version.&#xA;&#xA;This program is distributed in the hope that it will be useful,&#xA;but WITHOUT ANY WARRANTY; without even the implied warranty of&#xA;MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xA;GNU General Public License for more details.&#xA;&#xA;You should have received a copy of the GNU General Public License&#xA;along with this program.  If not, see &amp;lt;https://www.gnu.org/licenses/&amp;gt;.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/AnimatedDrawings</title>
    <updated>2023-04-18T01:43:50Z</updated>
    <id>tag:github.com,2023-04-18:/facebookresearch/AnimatedDrawings</id>
    <link href="https://github.com/facebookresearch/AnimatedDrawings" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code to accompany &#34;A Method for Animating Children&#39;s Drawings of the Human Figure&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Animated Drawings&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif&#34; alt=&#34;Sequence 02&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains an implementation of the algorithm described in the paper, `A Method for Animating Children&#39;s Drawings of the Human Figure&#39; (to appear in Transactions on Graphics and to be presented at SIGGRAPH 2023).&lt;/p&gt; &#xA;&lt;p&gt;In addition, this repo aims to be a useful creative tool in its own right, allowing you to flexibly create animations starring your own drawn characters. If you do create something fun with this, let us know! Use hashtag &lt;strong&gt;#FAIRAnimatedDrawings&lt;/strong&gt;, or tag me on twitter: &lt;a href=&#34;https://twitter.com/hjessmith/&#34;&gt;@hjessmith&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Project website: &lt;a href=&#34;http://www.fairanimateddrawings.com&#34;&gt;http://www.fairanimateddrawings.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Video overview of &lt;a href=&#34;https://www.youtube.com/watch?v=WsMUKQLVsOI&#34;&gt;Animated Drawings OS Project&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This project has been tested with macOS Ventura 13.2.1 and Ubuntu 18.04. If you&#39;re installing on another operating sytem, you may encounter issues.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;We &lt;em&gt;strongly&lt;/em&gt; recommend activating a Python virtual environment prior to installing Animated Drawings. Conda&#39;s Miniconda is a great choice. Follow &lt;a href=&#34;https://conda.io/projects/conda/en/stable/user-guide/install/index.html&#34;&gt;these steps&lt;/a&gt; to download and install it. Then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    # create and activate the virtual environment&#xA;    conda create --name animated_drawings python=3.8.13&#xA;    conda activate animated_drawings&#xA;&#xA;    # clone AnimatedDrawings and use pip to install&#xA;    git clone https://github.com/facebookresearch/AnimatedDrawings.git&#xA;    cd AnimatedDrawings&#xA;    pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Mac M1/M2 users: if you get architecture errors, make sure your &lt;code&gt;~/.condarc&lt;/code&gt; does not have &lt;code&gt;osx-64&lt;/code&gt;, but only &lt;code&gt;osx-arm64&lt;/code&gt; and &lt;code&gt;noarch&lt;/code&gt; in its subdirs listing. You can see that it&#39;s going to go sideways as early as &lt;code&gt;conda create&lt;/code&gt; because it will show &lt;code&gt;osx-64&lt;/code&gt; instead of &lt;code&gt;osx-arm64&lt;/code&gt; versions of libraries under &#34;The following NEW packages will be INSTALLED&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Using Animated Drawings&lt;/h2&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;Now that everything&#39;s set up, let&#39;s animate some drawings! To get started, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a terminal and activate the animated_drawings conda environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;~ % conda activate animated_drawings&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Ensure you&#39;re in the root directory of AnimatedDrawings:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(animated_drawings) ~ % cd {location of AnimatedDrawings on your computer}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Start up a Python interpreter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(animated_drawings) AnimatedDrawings % python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Copy and paste the follow two lines into the interpreter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/interactive_window_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything is installed correctly, an interactive window should appear on your screen. (Use spacebar to pause/unpause the scene, arrow keys to move back and forth in time, and q to close the screen.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/interactive_window_example.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s a lot happening behind the scenes here. Characters, motions, scenes, and more are all controlled by configuration files, such as &lt;code&gt;interactive_window_example.yaml&lt;/code&gt;. Below, we show how different effects can be achieved by varying the config files. You can learn more about the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md&#34;&gt;config files here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Export MP4 video&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you&#39;d like to save the animation as a video file instead of viewing it directly in a window. Specify a different example config by copying these lines into the Python interpreter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/export_mp4_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instead of an interactive window, the animation was saved to a file, video.mp4, located in the same directory as your script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/mp4_export_video.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Export transparent .gif&lt;/h3&gt; &#xA;&lt;p&gt;Perhaps you&#39;d like a transparent .gif instead of an .mp4? Copy these lines in the Python interpreter intead:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/export_gif_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instead of an interactive window, the animation was saved to a file, video.gif, located in the same directory as your script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/gif_export_video.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Headless Rendering&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like to generate a video headlessly (e.g. on a remote server accessed via ssh), you&#39;ll need to specify &lt;code&gt;USE_MESA: True&lt;/code&gt; within the &lt;code&gt;view&lt;/code&gt; section of the config file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;    view:&#xA;      USE_MESA: True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Animating Your Own Drawing&lt;/h3&gt; &#xA;&lt;p&gt;All of the examples above use drawings with pre-existing annotations. To understand what we mean by &lt;em&gt;annotations&lt;/em&gt; here, look at one of the &#39;pre-rigged&#39; character&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char1/&#34;&gt;annotation files&lt;/a&gt;. You can use whatever process you&#39;d like to create those annotations files and, as long as they are valid, AnimatedDrawings will give you an animation.&lt;/p&gt; &#xA;&lt;p&gt;So you&#39;d like to animate your own drawn character. I wouldn&#39;t want to you to create those annotation files manually. That would be tedious. To make it fast and easy, we&#39;ve trained a drawn humanoid figure detector and pose estimator and provided scripts to automatically generate annotation files from the model predictions.&lt;/p&gt; &#xA;&lt;p&gt;To get it working, you&#39;ll need to set up a Docker container that runs TorchServe. This allows us to quickly show your image to our machine learning models and receive their predictions.&lt;/p&gt; &#xA;&lt;p&gt;To set up the container, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Install Docker Desktop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ensure Docker Desktop is running.&lt;/li&gt; &#xA; &lt;li&gt;Run the following commands, starting from the Animated Drawings root directory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) AnimatedDrawings % cd torchserve&#xA;&#xA;    # build the docker image... this takes a while (~5-7 minutes on Macbook Pro 2021)&#xA;    (animated_drawings) torchserve % docker build -t docker_torchserve .&#xA;&#xA;    # start the docker container and expose the necessary ports&#xA;    (animated_drawings) torchserve % docker run -d --name docker_torchserve -p 8080:8080 -p 8081:8081 docker_torchserve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait ~10 seconds, then ensure Docker and TorchServe are working by pinging the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) torchserve % curl http://localhost:8080/ping&#xA;&#xA;    # should return:&#xA;    # {&#xA;    #   &#34;status&#34;: &#34;Healthy&#34;&#xA;    # }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If, after waiting, the response is &lt;code&gt;curl: (52) Empty reply from server&lt;/code&gt;, one of two things is likely happening.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Torchserve hasn&#39;t finished initializing yet, so wait another 10 seconds and try again.&lt;/li&gt; &#xA; &lt;li&gt;Torchserve is failing because it doesn&#39;t have enough RAM. Try &lt;a href=&#34;https://docs.docker.com/desktop/settings/mac/#advanced&#34;&gt;increasing the amount of memory available to your Docker containers&lt;/a&gt; to 16GB by modifying Docker Desktop&#39;s settings.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;With that set up, you can now go directly from image -&amp;gt; animation with a single command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) torchserve % cd ../examples&#xA;    (animated_drawings) examples % python image_to_animation.py drawings/garlic.png garlic_out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As you waited, the image located at &lt;code&gt;drawings/garlic.png&lt;/code&gt; was analyzed, the character detected, segmented, and rigged, and it was animated using BVH motion data from a human actor. The resulting animation was saved as &lt;code&gt;./garlic_out/video.gif&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/drawings/garlic.png&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/garlic.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fixing bad predictions&lt;/h3&gt; &#xA;&lt;p&gt;You may notice that, when you ran &lt;code&gt;python image_to_animation.py drawings/garlic.png garlic_out&lt;/code&gt;, there were additional non-video files within &lt;code&gt;garlic_out&lt;/code&gt;. &lt;code&gt;mask.png&lt;/code&gt;, &lt;code&gt;texture.png&lt;/code&gt;, and &lt;code&gt;char_cfg.yaml&lt;/code&gt; contain annotation results of the image character analysis step. These annotations were created from our model predictions. If the mask predictions are incorrect, you can edit the mask with an image editing program like Paint or Photoshop. If the joint predictions are incorrect, you can run &lt;code&gt;python fix_annotations.py&lt;/code&gt; to launch a web interface to visualize, correct, and update the annotations. Pass it the location of the folder containing incorrect joint predictions (here we use &lt;code&gt;garlic_out/&lt;/code&gt; as an example):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) examples % python fix_annotations.py garlic_out/&#xA;    ...&#xA;     * Running on http://127.0.0.1:5050&#xA;    Press CTRL+C to quit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;code&gt;http://127.0.0.1:5050&lt;/code&gt; in your browser to access the web interface. Drag the joints into the appropriate positions, and hit &lt;code&gt;Submit&lt;/code&gt; to save your edits.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve modified the annotations, you can render an animation using them like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    # specify the folder where the fixed annoations are located&#xA;    (animated_drawings) examples % python annotations_to_animation.py garlic_out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding multiple characters to scene&lt;/h3&gt; &#xA;&lt;p&gt;Multiple characters can be added to a video by specifying multiple entries within the config scene&#39;s &#39;ANIMATED_CHARACTERS&#39; list. To see for yourself, run the following commands from a Python interpreter within the AnimatedDrawings root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/multiple_characters_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char1/texture.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char2/texture.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/multiple_characters_example.gif&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Adding a background image&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you&#39;d like to add a background to the animation. You can do so by specifying the image path within the config. Run the following commands from a Python interpreter within the AnimatedDrawings root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/background_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char4/texture.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char4/background.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/background_example.gif&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Using BVH Files with Different Skeletons&lt;/h3&gt; &#xA;&lt;p&gt;You can use any motion clip you&#39;d like, as long as it is in BVH format.&lt;/p&gt; &#xA;&lt;p&gt;If the BVH&#39;s skeleton differs from the examples used in this project, you&#39;ll need to create a new motion config file and retarget config file. Once you&#39;ve done that, you should be good to go. The following code and resulting clip uses a BVH with completely different skeleton. Run the following commands from a Python interpreter within the AnimatedDrawings root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/different_bvh_skeleton_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/different_bvh_skeleton_example.gif&#34; height=&#34;256&#34;&gt; &#xA;&lt;h3&gt;Creating Your Own BVH Files&lt;/h3&gt; &#xA;&lt;p&gt;You may be wondering how you can create BVH files of your own. You used to need a motion capture studio. But now, thankfully, there are simple and accessible options for getting 3D motion data from a single RGB video. For example, I created this Readme&#39;s banner animation by:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Recording myself doing a silly dance with my phone&#39;s camera.&lt;/li&gt; &#xA; &lt;li&gt;Using &lt;a href=&#34;https://www.rokoko.com/&#34;&gt;Rokoko&lt;/a&gt; to export a BVH from my video.&lt;/li&gt; &#xA; &lt;li&gt;Creating a new &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md#motion&#34;&gt;motion config file&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md#retarget&#34;&gt;retarget config file&lt;/a&gt; to fit the skeleton exported by Rokoko.&lt;/li&gt; &#xA; &lt;li&gt;Using AnimatedDrawings to animate the characters and export a transparent animated gif.&lt;/li&gt; &#xA; &lt;li&gt;Combining the animated gif, original video, and original drawings in Adobe Premiere.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif&#34; height=&#34;256&#34;&gt; &#xA;&lt;h3&gt;Adding Addition Character Skeletons&lt;/h3&gt; &#xA;&lt;p&gt;All of the example animations above depict &#34;human-like&#34; characters; they have two arms and two legs. Our method is primarily designed with these human-like characters is mind, and the provided pose estimation model assumes a human-like skeleton is present. But you can manually specify a different skeletons within the &lt;code&gt;character config&lt;/code&gt; and modify the specified &lt;code&gt;retarget config&lt;/code&gt; to support it. If you&#39;re interested, look at the configuration files specified in the two examples below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/six_arms_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223584962-925ee5aa-11de-47e5-ace2-a6d5940b34ae.png&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223585000-dc8acf4e-974d-4cae-998b-94543f5f42c8.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/four_legs_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223585033-f11e4e66-0443-405a-80e5-09b6aa0e335d.png&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223585043-7ce9eac0-bb4c-4547-b038-c63ca2852ef2.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Creating Your Own Config Files&lt;/h3&gt; &#xA;&lt;p&gt;If you want to create your own config files, see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md&#34;&gt;configuration file documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Browser-Based Demo&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to animate a drawing of your own, but don&#39;t want to deal with downloading code and using the command line, check out our browser-based demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sketch.metademolab.com/&#34;&gt;www.sketch.metademolab.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Animated Drawings is released under the &lt;a href=&#34;https://github.com/fairinternal/AnimatedDrawings/raw/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;i&gt;The accompanying paper has been accepted to Transactions on Graphics and will be presented at SIGGRAPH 2023. Official citation will be added later, but for now you can access the paper on arxiv: &lt;a href=&#34;https://arxiv.org/abs/2303.12741&#34;&gt;A Method for Animating Children&#39;s Drawings of The Human Figure&lt;/a&gt; &lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find this repo helpful in your own work, please consider citing the accompanying paper:&lt;/p&gt; &#xA;&lt;p&gt;(citation information to be added later)&lt;/p&gt; &#xA;&lt;h2&gt;Amateur Drawings Dataset&lt;/h2&gt; &#xA;&lt;p&gt;To obtain the Amateur Drawings Dataset, run the following two commands from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download annotations (~275Mb)&#xA;wget https://dl.fbaipublicfiles.com/amateur_drawings/amateur_drawings_annotations.json&#xA;&#xA;# download images (~50Gb)&#xA;wget https://dl.fbaipublicfiles.com/amateur_drawings/amateur_drawings.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have feedback about the dataset, please fill out &lt;a href=&#34;https://forms.gle/kE66yskh9uhtLbFz9&#34;&gt;this form&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Trained Model Weights&lt;/h2&gt; &#xA;&lt;p&gt;Trained model weights for human-like figure detection and pose estimation are included in the &lt;a href=&#34;https://github.com/facebookresearch/AnimatedDrawings/releases&#34;&gt;repo releases&lt;/a&gt;. Model weights are released under &lt;a href=&#34;https://github.com/facebookresearch/AnimatedDrawings/raw/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;. The .mar files were generated using the OpenMMLab framework (&lt;a href=&#34;https://github.com/open-mmlab/mmdetection/raw/main/LICENSE&#34;&gt;OpenMMDet Apache 2.0 License&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmpose/raw/main/LICENSE&#34;&gt;OpenMMPose Apache 2.0 License&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;As-Rigid-As-Possible Shape Manipulation&lt;/h2&gt; &#xA;&lt;p&gt;These characters are deformed using &lt;a href=&#34;https://www-ui.is.s.u-tokyo.ac.jp/~takeo/papers/takeo_jgt09_arapFlattening.pdf&#34;&gt;As-Rigid-As-Possible (ARAP) shape manipulation&lt;/a&gt;. We have a Python implementation of the algorithm, located &lt;a href=&#34;https://github.com/fairinternal/AnimatedDrawings/raw/main/animated_drawings/model/arap.py&#34;&gt;here&lt;/a&gt;, that might be of use to other developers.&lt;/p&gt;</summary>
  </entry>
</feed>