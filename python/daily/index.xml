<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-26T01:36:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookresearch/diplomacy_cicero</title>
    <updated>2022-11-26T01:36:11Z</updated>
    <id>tag:github.com,2022-11-26:/facebookresearch/diplomacy_cicero</id>
    <link href="https://github.com/facebookresearch/diplomacy_cicero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for Cicero, an AI agent that plays the game of Diplomacy with open-domain natural language negotiation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Diplomacy Cicero and Diplodocus&lt;/h1&gt; &#xA;&lt;p&gt;This code contains checkpoints and training code the following papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.science.org/doi/10.1126/science.ade9097&#34;&gt;&#34;Human-Level Play in the Game of Diplomacy by Combining Language Models with Strategic Reasoning&#34;&lt;/a&gt; published in Science, November 2022.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.05492&#34;&gt;&#34;Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning&#34;&lt;/a&gt; in review at ICLR 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;p&gt;A very brief orientation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Most of the language modeling and generation code is in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/parlai_diplomacy&#34;&gt;parlai_diplomacy&lt;/a&gt;, and leverages the &lt;a href=&#34;https://github.com/facebookresearch/ParlAI&#34;&gt;ParlAI framework&lt;/a&gt; for running and finetuning the language models involved.&lt;/li&gt; &#xA; &lt;li&gt;Within the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy/agents&#34;&gt;agents&lt;/a&gt; directory, the central logic for Cicero&#39;s strategic planning lives &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy/agents/br_corr_bilateral_search.py&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy/agents/bqre1p_agent.py&#34;&gt;here&lt;/a&gt;. The latter also contains the core logic for Diplodocus&#39;s strategic planning. &#34;bqre1p&#34; was the internal dev name for DiL-piKL, and &#34;br_corr_bilateral&#34; the internal dev name for Cicero&#39;s bilateral and correlated planning components.&lt;/li&gt; &#xA; &lt;li&gt;The dialogue-free model architectures for RL are &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy/models/base_strategy_model/base_strategy_model.py&#34;&gt;here&lt;/a&gt;, and the bulk of the training logic lives &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy/models/base_strategy_model/train_sl.py&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The RL training code for both Cicero and Diplodocus is &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy/selfplay&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/conf&#34;&gt;conf&lt;/a&gt; directory contains various configs for Cicero, Diplodocus, benchmark agents, and training configs for RL.&lt;/li&gt; &#xA; &lt;li&gt;A separately licensed subfolder of this repo &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy_external&#34;&gt;here&lt;/a&gt; contains some utilities for visually rendering games, or connecting agents to be run online.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Game info&lt;/h3&gt; &#xA;&lt;p&gt;Diplomacy is a strategic board game set in 1914 Europe. The board is divided into fifty-six land regions and nineteen sea regions. Forty-two of the land regions are divided among the seven Great Powers of the game: Austria-Hungary, England, France, Germany, Italy, Russia, and Turkey. The remaining fourteen land regions are neutral at the start of the game.&lt;/p&gt; &#xA;&lt;p&gt;Each power controls some regions and some units. The number of the units controlled depends on the number of the controlled key regions called Supply Centers (SCs). Simply put, more SCs means more units. The goal of the game is to control more than half of all SCs by moving units into these regions and convincing other players to support you.&lt;/p&gt; &#xA;&lt;p&gt;You can find the full rules &lt;a href=&#34;https://en.wikibooks.org/wiki/Diplomacy/Rules&#34;&gt;here&lt;/a&gt;. To get the game&#39;s spirit, watch &lt;a href=&#34;https://www.youtube.com/c/diplostrats&#34;&gt;some&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/playlist?list=PLmbDtCxqXA5CyFoBmB5dJHHOHeLQ0Nd-Y&#34;&gt;games&lt;/a&gt; with comments. You can play the game online on &lt;a href=&#34;https://webdiplomacy.net/&#34;&gt;webDiplomacy&lt;/a&gt; either against bots or humans.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Clone the repo with submodules:&#xA;git clone --recursive git@github.com:facebookresearch/diplomacy_cicero.git diplomacy_cicero&#xA;cd diplomacy_cicero&#xA;&#xA;# Apt installs&#xA;apt-get install -y wget bzip2 ca-certificates curl git build-essential clang-format-8 git wget cmake build-essential autoconf libtool pkg-config libgoogle-glog-dev&#xA;&#xA;# Install conda&#xA;wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.7.10-Linux-x86_64.sh -O ~/miniconda.sh&#xA;/bin/bash ~/miniconda.sh -b&#xA;&#xA;# Create conda env&#xA;conda create --yes -n diplomacy_cicero python=3.7&#xA;conda activate diplomacy_cicero&#xA;&#xA;# Install pytorch, pybind11&#xA;conda install --yes pytorch=1.7.1 torchvision cudatoolkit=11.0 -c pytorch&#xA;conda install --yes pybind11&#xA;&#xA;# Install go for boringssl in grpc&#xA;# We have some hacky patching code for protobuf that is not guaranteed&#xA;# to work on versions other than this.&#xA;conda install --yes go protobuf=3.19.1&#xA;&#xA;# Install python requirements&#xA;pip install -r requirements.txt&#xA;&#xA;# Local pip installs&#xA;pip install -e ./thirdparty/github/fairinternal/postman/nest/&#xA;# NOTE: Postman here links against pytorch for tensors, for this to work you may&#xA;# need to separately have installed cuda 11 on your own.&#xA;pip install -e ./thirdparty/github/fairinternal/postman/postman/&#xA;pip install -e . -vv&#xA;&#xA;# Make&#xA;make&#xA;&#xA;# Run unit tests&#xA;make test_fast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After each pull it&#39;s recommended to run &lt;code&gt;make&lt;/code&gt; to re-compile internal C++ and protobuf code.&lt;/p&gt; &#xA;&lt;h3&gt;Downloading model files&lt;/h3&gt; &#xA;&lt;p&gt;Please email &lt;a href=&#34;mailto:diplomacyteam@meta.com&#34;&gt;diplomacyteam@meta.com&lt;/a&gt; to request the password. Then run &lt;code&gt;bash bin/download_model_files.sh &amp;lt;PASSWORD&amp;gt;&lt;/code&gt;. This will download and decrypt all relevant model files into &lt;code&gt;./models&lt;/code&gt;. This might take awhile.&lt;/p&gt; &#xA;&lt;h3&gt;Accessing Cicero&#39;s experiment games&lt;/h3&gt; &#xA;&lt;p&gt;JSON data for games that Cicero played in are located in &lt;code&gt;data/cicero_redacted_games&lt;/code&gt;. Only conversations with players who have consented to having their dialogue released are included. Please refer to the (separately-licensed) &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/(fairdiplomacy_external)&#34;&gt;fairdiplomacy_external&lt;/a&gt; subdirectory for details on HTML visualizations.&lt;/p&gt; &#xA;&lt;h3&gt;Getting started&lt;/h3&gt; &#xA;&lt;p&gt;The front-end for most tasks is &lt;code&gt;run.py&lt;/code&gt;, which can run various tasks specified by a protobuf config. The config schema can be found at &lt;code&gt;conf/conf.proto&lt;/code&gt;, and example configs for different tasks can be found in the &lt;code&gt;conf&lt;/code&gt; folder. This can be used for most tasks (except training parlai models): training no-press models, comparing agents, profiling things, launching an agent on webdip, etc.&lt;/p&gt; &#xA;&lt;p&gt;The config specification framework, called HeyHi, &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/heyhi/README.md&#34;&gt;is explained here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A core abstraction is an &lt;code&gt;Agent&lt;/code&gt;, which is specified by an &lt;code&gt;Agent&lt;/code&gt; config whose schema lives in &lt;code&gt;conf/agents.proto&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Simulating games between agents&lt;/h3&gt; &#xA;&lt;p&gt;To simulate 1v6 games between a pair of agents, you can run the &lt;code&gt;compare_agents&lt;/code&gt; task. For example, to play one Cicero agent as Turkey against six full-press imitation agents, you can run&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python run.py --adhoc --cfg conf/c01_ag_cmp/cmp.prototxt Iagent_one=agents/bqre1p_parlai_20220819_cicero_2.prototxt Iagent_six=agents/ablations/cicero_imitation_only.prototxt power_one=TURKEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t have sufficient memory to load two agents, you can load a single agent in self-play with the &lt;code&gt;use_shard_agent=1&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python run.py --adhoc --cfg conf/c01_ag_cmp/cmp.prototxt Iagent_one=agents/bqre1p_parlai_20220819_cicero_2.prototxt use_shared_agent=1 power_one=TURKEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Training models in RL&lt;/h3&gt; &#xA;&lt;p&gt;To run the training for Cicero and/or Diplodocus:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py —adhoc —cfg conf/c04_exploit/research_20221001_paper_cicero.prototxt launcher.slurm.num_gpus=256&#xA;&#xA;python run.py —adhoc —cfg conf/c04_exploit/research_20221001_paper_diplodocus_high.prototxt launcher.slurm.num_gpus=256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above training commands are designed for running on an appropriately configured Slurm cluster with a fast cross-machine shared filesystem. One can also instead pass &lt;code&gt;launcher.local.use_local=true&lt;/code&gt; to run them on locally, e.g. on an individual 8-GPU-or-more GPU machine but training may be very slow.&lt;/p&gt; &#xA;&lt;h3&gt;Other tasks&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy_external&#34;&gt;here&lt;/a&gt; for some separately-licensed code for rendering game jsons with HTML, as well as connecting agents to run on &lt;a href=&#34;https://webdiplomacy.net&#34;&gt;webdiplomacy.net&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Supervised training of baseline models&lt;/h3&gt; &#xA;&lt;p&gt;Supervised training and/or behavioral cloning for various dialogue-conditional models as well as pre-RL baseline dialogue-free models involves some of the scripts in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/parlai_diplomacy&#34;&gt;parlai_diplomacy&lt;/a&gt; via the ParlAI framework, and on the dialogue-free side, some of the configs &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/conf/c02_sup_train&#34;&gt;conf/c02_sup_train&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy/models/base_strategy_model/train_sl.py&#34;&gt;train_sl.py&lt;/a&gt;. However the dataset of human games and/or dialogue is NOT available here, so the relevant code and configs are likely to be of limited use. They are provided here mostly as documentation for posterity.&lt;/p&gt; &#xA;&lt;p&gt;However, as mentioned above pre-trained models are available, and with sufficient compute power, re-running the RL on top of these pre-trained models is also possible without any exteral game data.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;pre-commit install&lt;/code&gt; to install pre-commit hooks that will auto-format python code before commiting it.&lt;/p&gt; &#xA;&lt;p&gt;Or you can do this manually. Use &lt;a href=&#34;https://github.com/psf/black&#34;&gt;black&lt;/a&gt; auto-formatter to format all python code. For protobufs use &lt;code&gt;clang-format-8 conf/*.proto -i&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Tests&lt;/h3&gt; &#xA;&lt;p&gt;To run tests locally run &lt;code&gt;make test&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have 2 level of tests: fast, unit tests (run with &lt;code&gt;make test_fast&lt;/code&gt;) and slow, integration tests (run with &lt;code&gt;make test_integration&lt;/code&gt;). The latter aims to use the same entry point as users do, i.e., &lt;code&gt;run.py&lt;/code&gt; for the HeyHi part and &lt;code&gt;diplom&lt;/code&gt; for the ParlAi.&lt;/p&gt; &#xA;&lt;p&gt;We use &lt;code&gt;pytest&lt;/code&gt; to run and discover tests. Some useful &lt;a href=&#34;https://docs.pytest.org/en/stable/&#34;&gt;pytest&lt;/a&gt; commands.&lt;/p&gt; &#xA;&lt;p&gt;To run all tests in your current directory, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run tests from a specific file, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest &amp;lt;filepath&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use name-based filtering to run tests, use the flag &lt;code&gt;-k&lt;/code&gt;. For example, to only run tests with &lt;code&gt;parlai&lt;/code&gt; in the name, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest -k parlai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For verbose testing logs, use &lt;code&gt;-v&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest -v -k parlai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To print the output from a test or set of tests, use &lt;code&gt;-s&lt;/code&gt;; this also allows you to set breakpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest -s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To view the durations of all tests, run with the flag &lt;code&gt;--durations=0&lt;/code&gt;, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest --durations=0 unit_tests/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The following license, which is also available &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/LICENSE.md&#34;&gt;here&lt;/a&gt;, covers the content in this repo &lt;em&gt;except&lt;/em&gt; for the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/diplomacy_cicero/release_orphan/fairdiplomacy_external&#34;&gt;fairdiplomacy_external&lt;/a&gt; directory. The content of fairdiplomacy_external is separately licenced under a version of the AGPL, see the license file within that directory for details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(covers this repo except for the fairdiplomacy_external directory)&#xA;MIT License&#xA;&#xA;Copyright (c) Meta, Inc. and its affiliates.&#xA;&#xA;Permission is hereby granted, free of charge, to any person obtaining a copy&#xA;of this software and associated documentation files (the &#34;Software&#34;), to deal&#xA;in the Software without restriction, including without limitation the rights&#xA;to use, copy, modify, merge, publish, distribute, sublicense, and/or sell&#xA;copies of the Software, and to permit persons to whom the Software is&#xA;furnished to do so, subject to the following conditions:&#xA;&#xA;The above copyright notice and this permission notice shall be included in all&#xA;copies or substantial portions of the Software.&#xA;&#xA;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR&#xA;IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,&#xA;FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE&#xA;AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER&#xA;LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,&#xA;OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE&#xA;SOFTWARE.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleSlim</title>
    <updated>2022-11-26T01:36:11Z</updated>
    <id>tag:github.com,2022-11-26:/PaddlePaddle/PaddleSlim</id>
    <link href="https://github.com/PaddlePaddle/PaddleSlim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PaddleSlim is an open-source library for deep model compression and architecture search.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;PaddleSlim&lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/Paddle?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6.2+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleSlim?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/PaddleSlim/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/PaddleSlim?color=9cf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleSlim?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleSlim?color=ccf&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;PaddleSlim是一个专注于深度学习模型压缩的工具库，提供&lt;strong&gt;低比特量化、知识蒸馏、稀疏化和模型结构搜索&lt;/strong&gt;等模型压缩策略，帮助开发者快速实现模型的小型化。&lt;/p&gt; &#xA;&lt;h2&gt;产品动态&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;2022.08.16：&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression&#34;&gt;自动化压缩&lt;/a&gt;功能升级&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持直接加载ONNX模型和Paddle模型导出至ONNX&lt;/li&gt; &#xA;   &lt;li&gt;发布&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/raw/develop/docs/zh_cn/tutorials/quant/AnalysisQuant.md&#34;&gt;量化分析工具&lt;/a&gt;，发布&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/post_training_quantization/pytorch_yolo_series/&#34;&gt;YOLO系列离线量化工具&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;更新&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/pytorch_yolo_series&#34;&gt;YOLO-Series自动化压缩模型库&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;模型&lt;/th&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Base mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;ACT量化mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;模型体积压缩比&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;预测时延&lt;sup&gt;&lt;small&gt;FP32&lt;/small&gt;&lt;sup&gt;&lt;br&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;预测时延&lt;sup&gt;&lt;small&gt;INT8&lt;/small&gt;&lt;sup&gt;&lt;br&gt;&lt;sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;预测加速比&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;PPYOLOE-s&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;43.1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;42.6&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.9倍&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;6.51ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;2.12ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.1倍&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;YOLOv5s&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;37.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;36.9&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.8倍&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;5.95ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1.87ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.2倍&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;YOLOv6s&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;42.4&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;41.3&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.9倍&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;9.06ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1.83ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;5.0倍&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;YOLOv7&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;51.1&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;50.9&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.9倍&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;26.84ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;4.55ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;5.9倍&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;YOLOv7-Tiny&lt;/td&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;37.3&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.9倍&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;5.06ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;1.68ms&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;3.0倍&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;2022.07.01: 发布&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/releases/tag/v2.3.0&#34;&gt;v2.3.0版本&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;发布&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression&#34;&gt;自动化压缩功能&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;支持代码无感知压缩：开发者只需提供推理模型文件和数据，既可进行离线量化（PTQ）、量化训练（QAT）、稀疏训练等压缩任务。&lt;/li&gt; &#xA;     &lt;li&gt;支持自动策略选择，根据任务特点和部署环境特性：自动搜索合适的离线量化方法,自动搜索最佳的压缩策略组合方式。&lt;/li&gt; &#xA;     &lt;li&gt;发布&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/nlp&#34;&gt;自然语言处理&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/semantic_segmentation&#34;&gt;图像语义分割&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/detection&#34;&gt;图像目标检测&lt;/a&gt;三个方向的自动化压缩示例。&lt;/li&gt; &#xA;     &lt;li&gt;发布&lt;code&gt;X2Paddle&lt;/code&gt;模型自动化压缩方案:&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/pytorch_yolo_series&#34;&gt;YOLOv5&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/pytorch_yolo_series&#34;&gt;YOLOv6&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/pytorch_yolo_series&#34;&gt;YOLOv7&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/pytorch_huggingface&#34;&gt;HuggingFace&lt;/a&gt;、&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression/tensorflow_mobilenet&#34;&gt;MobileNet&lt;/a&gt;。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;升级量化功能 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;统一量化模型格式；离线量化支持while op；修复BERT大模型量化训练过慢的问题。&lt;/li&gt; &#xA;     &lt;li&gt;新增7种&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/quant/post_training_quantization.md&#34;&gt;离线量化方法&lt;/a&gt;, 包括HIST, AVG, EMD, Bias Correction, AdaRound等。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;支持半结构化稀疏训练&lt;/li&gt; &#xA;   &lt;li&gt;新增延时预估工具 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;支持对稀疏化模型、低比特量化模型的性能预估；支持预估指定模型在特定部署环境下 (ARM CPU + Paddle Lite) 的推理性能；提供 SD625、SD710、RK3288 芯片 + Paddle Lite 的预估接口。&lt;/li&gt; &#xA;     &lt;li&gt;提供部署环境自动扩展工具，可以自动增加在更多 ARM CPU 设备上的预估工具。&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;历史更新&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;2021.11.15: 发布v2.2.0版本&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;支持动态图离线量化功能.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;2021.5.20: 发布V2.1.0版本&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;扩展离线量化方法&lt;/li&gt; &#xA;    &lt;li&gt;新增非结构化稀疏&lt;/li&gt; &#xA;    &lt;li&gt;增强剪枝功能&lt;/li&gt; &#xA;    &lt;li&gt;修复OFA功能若干bug&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;更多信息请参考：&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/releases&#34;&gt;release note&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;基础压缩功能概览&lt;/h2&gt; &#xA;&lt;p&gt;PaddleSlim支持以下功能，也支持自定义量化、裁剪等功能。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/raw/release/2.0.0/docs/zh_cn/tutorials/quant/overview.md&#34;&gt;Quantization&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/raw/release/2.0.0/docs/zh_cn/tutorials/pruning/overview.md&#34;&gt;Pruning&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/raw/release/2.0.0/docs/zh_cn/tutorials/nas/overview.md&#34;&gt;NAS&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/tree/release/2.0.0/docs/zh_cn/tutorials&#34;&gt;Distilling&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#%E5%9C%A8%E7%BA%BF%E9%87%8F%E5%8C%96%E8%AE%AD%E7%BB%83qat&#34;&gt;QAT&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#pact&#34;&gt;PACT&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#%E9%9D%99%E6%80%81%E7%A6%BB%E7%BA%BF%E9%87%8F%E5%8C%96ptq-static&#34;&gt;PTQ Static&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#%E5%8A%A8%E6%80%81%E7%A6%BB%E7%BA%BF%E9%87%8F%E5%8C%96ptq-dynamic&#34;&gt;PTQ Dynamic&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#embedding%E9%87%8F%E5%8C%96&#34;&gt;Embedding Quant&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#%E6%95%8F%E6%84%9F%E5%BA%A6%E5%89%AA%E6%9E%9D&#34;&gt;SensitivityPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#fpgm&#34;&gt;FPGMFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#l1norm&#34;&gt;L1NormFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#l2norm&#34;&gt;**L2NormFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#slimfilter&#34;&gt;*SlimFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#optslimfilter&#34;&gt;*OptSlimFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#sanas&#34;&gt;*Simulate Anneal based NAS&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#rlnas&#34;&gt;*Reinforcement Learning based NAS&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#darts&#34;&gt;**DARTS&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#pc-darts&#34;&gt;**PC-DARTS&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#once-for-all&#34;&gt;**Once-for-All&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#hardware-aware-search&#34;&gt;*Hardware-aware Search&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#fsp&#34;&gt;*FSP&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#dml&#34;&gt;*DML&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/overview.md#dk&#34;&gt;*DK&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;注：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;*表示仅支持静态图，**表示仅支持动态图&lt;/li&gt; &#xA; &lt;li&gt;敏感度裁剪指的是通过各个层的敏感度分析来确定各个卷积层的剪裁率，需要和其他裁剪方法配合使用。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;多场景效果展示&lt;/h3&gt; &#xA;&lt;p&gt;PaddleSlim在典型视觉和自然语言处理任务上做了模型压缩，并且测试了Nvidia GPU、ARM等设备上的加速情况，这里展示部分模型的压缩效果，详细方案可以参考下面CV和NLP模型压缩方案:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/images/benchmark.png&#34; height=&#34;185&#34; width=&#34;849&#34; hspace=&#34;10&#34;&gt; &lt;br&gt; &lt;strong&gt;表1: 部分场景模型压缩加速情况&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p&gt;注:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YOLOv3: 在移动端SD855上加速3.55倍。&lt;/li&gt; &#xA; &lt;li&gt;PP-OCR: 体积由8.9M减少到2.9M, 在SD855上加速1.27倍。&lt;/li&gt; &#xA; &lt;li&gt;BERT: 模型参数由110M减少到80M，精度提升的情况下，Tesla T4 GPU FP16计算加速1.47倍。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;自动压缩效果展示&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;800&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/7534971/168805367-f9d1299d-93e3-44d0-84da-870217edeb54.png&#34;&gt; &lt;br&gt; &lt;strong&gt;表3: 自动压缩效果&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;h3&gt;离线量化效果对比&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;750&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/7534971/169042883-9ca281ce-19be-4525-a3d2-c54cea4a2cbd.png&#34;&gt; &lt;br&gt; &lt;strong&gt;表2: 多种离线量化方法效果对比&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;h2&gt;文档教程&lt;/h2&gt; &#xA;&lt;h2&gt;版本对齐&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PaddleSlim&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PaddlePaddle&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PaddleLite&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;lt;=1.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.1.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0Beta/RC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.1.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.1.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;gt;=2.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.3.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.3.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;gt;=2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;p&gt;安装最新版本：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install paddleslim -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;安装指定版本：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install paddleslim==2.3.0 -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;安装develop版本：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PaddlePaddle/PaddleSlim.git &amp;amp; cd PaddleSlim&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;验证安装&lt;/h3&gt; &#xA;&lt;p&gt;安装完成后您可以使用 python 或 python3 进入 python 解释器，输入import paddleslim, 没有报错则说明安装成功。&lt;/p&gt; &#xA;&lt;h3&gt;快速开始&lt;/h3&gt; &#xA;&lt;p&gt;快速开始教程是能基于CIFAR10数据集快速运行起来的简单示例，若您是Paddle官方模型套件用户，请直接使用下方的CV模型压缩或者NLP模型压缩中教程。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/example/auto_compression&#34;&gt;自动压缩&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/quick_start/static/quant_aware_tutorial.md&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/quick_start/static/quant_post_static_tutorial.md&#34;&gt;离线量化&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/quick_start/static/pruning_tutorial.md&#34;&gt;结构化剪枝&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/quick_start/static/distillation_tutorial.md&#34;&gt;蒸馏&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/quick_start/static/nas_tutorial.md&#34;&gt;NAS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;更多教程&lt;/h3&gt; &#xA;&lt;p&gt;进阶教程详细介绍了每一步的流程，帮助您把相应方法迁移到您自己的模型上。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;通道剪裁&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/pruning/overview.md&#34;&gt;四种剪裁策略效果对比与应用方法&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/pruning/overview.md#l1normfilterpruner&#34;&gt;L1NormFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/pruning/overview.md#fpgmfilterpruner&#34;&gt;FPGMFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/pruning/overview.md#slimfilterpruner&#34;&gt;SlimFilterFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/pruning/overview.md#optslimfilterpruner&#34;&gt;OptSlimFilterPruner&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;自定义剪裁策略：&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/pruning/dygraph/self_defined_filter_pruning.md&#34;&gt;动态图&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;低比特量化&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/quant/overview.md&#34;&gt;三种量化方法介绍与应用&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/quick_start/static/quant_aware_tutorial.md&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/quant/static/quant_post_tutorial.md&#34;&gt;离线量化&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/quant/post_training_quantization.md&#34;&gt;离线量化方法解析&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/quant/static/embedding_quant_tutorial.md&#34;&gt;embedding量化&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;NAS&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/nas/overview.md&#34;&gt;四种NAS策略介绍和应用&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/nas/dygraph/nas_ofa.md&#34;&gt;Once-For-All&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/tutorials/nas/static/sanas_darts_space.md&#34;&gt;SANAS&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/tree/release/2.0.0/demo/nas#rlnas%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%90%9C%E7%B4%A2%E7%A4%BA%E4%BE%8B&#34;&gt;RLNAS&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/demo/darts/README.md&#34;&gt;DARTS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;蒸馏&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/demo/distillation&#34;&gt;知识蒸馏示例&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;推理部署&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/demo/mkldnn_quant/README.md&#34;&gt;Intel CPU量化部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/demo/quant/deploy/TensorRT/README.md&#34;&gt;Nvidia GPU量化部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/deploy/deploy_cls_model_on_mobile_device.md&#34;&gt;PaddleLite量化部署&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CV模型压缩&lt;/h3&gt; &#xA;&lt;p&gt;本系列教程均基于Paddle官方的模型套件中模型进行压缩，若您不是模型套件用户，更推荐使用快速教程和进阶教程。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;检测模型压缩&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;压缩方案&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/detection/static/yolov3_slim.md&#34;&gt;PPDetection-YOLOv3 压缩方案&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;方法应用-静态图&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/detection/static/paddledetection_slim_distillation_tutorial.md&#34;&gt;蒸馏&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/detection/static/paddledetection_slim_quantization_tutorial.md&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/detection/static/paddledetection_slim_nas_tutorial.md&#34;&gt;模型结构搜索&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/detection/static/paddledetection_slim_pruing_tutorial.md&#34;&gt;剪枝&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/detection/static/paddledetection_slim_prune_dist_tutorial.md&#34;&gt;剪枝与蒸馏的结合使用&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/detection/static/paddledetection_slim_sensitivy_tutorial.md&#34;&gt;卷积层敏感度分析&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;方法应用-动态图&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.0-rc/dygraph/configs/slim#%E5%89%AA%E8%A3%81&#34;&gt;剪枝&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.0-rc/dygraph/configs/slim#%E9%87%8F%E5%8C%96&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;分割模型压缩&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;压缩方案&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;方法应用-静态图&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSeg/tree/release/v0.8.0/slim/distillation&#34;&gt;蒸馏&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSeg/tree/release/v0.8.0/slim/quantization&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSeg/tree/release/v0.8.0/slim/nas&#34;&gt;模型结构搜索&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSeg/tree/release/v0.8.0/slim/prune&#34;&gt;剪枝&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;方法应用-动态图&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSeg/tree/develop/slim#%E6%A8%A1%E5%9E%8B%E8%A3%81%E5%89%AA&#34;&gt;剪枝&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSeg/tree/develop/slim#%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OCR模型压缩&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;压缩方案&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/cv/ocr/static/3.5M_slim.md&#34;&gt;3.5M模型压缩方案&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;方法应用-静态图&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/1.1/deploy/slim/quantization&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/1.1/deploy/slim/prune&#34;&gt;剪枝&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;方法应用-动态图&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/develop/deploy/slim/prune&#34;&gt;剪枝&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/develop/deploy/slim/quantization&#34;&gt;量化训练&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;NLP模型压缩&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/nlp/paddlenlp_slim_ofa_tutorial.md&#34;&gt;PaddleNLP-BERT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/nlp/ernie_slim_ofa_tutorial.md&#34;&gt;ERNIE-ERNIE&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;API文档&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/dygraph&#34;&gt;动态图&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/api_cn/static&#34;&gt;静态图&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/develop/docs/zh_cn/FAQ/quantization_FAQ.md&#34;&gt;FAQ&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;1. 量化训练或者离线量化后的模型体积为什么没有变小？&lt;/h4&gt; &#xA;&lt;p&gt;答：这是因为量化后保存的参数是虽然是int8范围，但是类型是float。这是因为Paddle训练前向默认的Kernel不支持INT8 Kernel实现，只有Paddle Inference TensorRT的推理才支持量化推理加速。为了方便量化后验证量化精度，使用Paddle训练前向能加载此模型，默认保存的Float32类型权重，体积没有发生变换。&lt;/p&gt; &#xA;&lt;h4&gt;2. macOS + Python3.9环境或者Windows环境下, 安装出错, &#34;command &#39;swig&#39; failed&#34;&lt;/h4&gt; &#xA;&lt;p&gt;答: 请参考&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/issues/1258&#34;&gt;https://github.com/PaddlePaddle/PaddleSlim/issues/1258&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;许可证书&lt;/h2&gt; &#xA;&lt;p&gt;本项目的发布受&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/raw/develop/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;许可认证。&lt;/p&gt; &#xA;&lt;h2&gt;贡献代码&lt;/h2&gt; &#xA;&lt;p&gt;我们非常欢迎你可以为PaddleSlim提供代码，也十分感谢你的反馈。&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img title=&#34;&#34; src=&#34;https://user-images.githubusercontent.com/48054808/157800467-2a9946ad-30d1-49a9-b9db-ba33413d9c90.png&#34; alt=&#34;&#34; width=&#34;20&#34;&gt; 技术交流&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;如果你发现任何PaddleSlim存在的问题或者是建议, 欢迎通过&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSlim/issues&#34;&gt;GitHub Issues&lt;/a&gt;给我们提issues。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;欢迎加入PaddleSlim 微信技术交流群&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/54695910/199486336-11d661a7-6cbd-47b1-823c-3e4ac38bb7d5.jpg&#34; width=&#34;225&#34; height=&#34;225&#34;&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/xformers</title>
    <updated>2022-11-26T01:36:11Z</updated>
    <id>tag:github.com,2022-11-26:/facebookresearch/xformers</id>
    <link href="https://github.com/facebookresearch/xformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hackable and optimized Transformers building blocks, supporting a composable construction.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/docs/assets/logo.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://anaconda.org/xformers/xformers/badges/installer/conda.svg?sanitize=true&#34; alt=&#34;Install with conda&#34;&gt; &lt;img src=&#34;https://anaconda.org/xformers/xformers/badges/downloads.svg?sanitize=true&#34; alt=&#34;Downloads&#34;&gt; &lt;img src=&#34;https://anaconda.org/xformers/xformers/badges/license.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/xformers/blob/main/docs/source/xformers_mingpt.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;br&gt;&#xA; &lt;!--&#xA;![PyPI](https://img.shields.io/pypi/v/xformers)&#xA;![PyPI - License](https://img.shields.io/pypi/l/xformers)&#xA;[![Documentation Status](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)&#xA;--&gt; &lt;a href=&#34;https://app.circleci.com/pipelines/github/facebookresearch/xformers/&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebookresearch/xformers.svg?style=shield&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/facebookresearch/xformers&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/facebookresearch/xformers/branch/main/graph/badge.svg?token=PKGKDR4JQM&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;black&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/CONTRIBUTING.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;PRs welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--&#xA;[![Downloads](https://pepy.tech/badge/xformers)](https://pepy.tech/project/xformers)&#xA;--&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;xFormers - Toolbox to Accelerate Research on Transformers&lt;/h2&gt; &#xA;&lt;p&gt;xFormers is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable building blocks&lt;/strong&gt;: Independant/customizable building blocks that can be used without boilerplate code. The components are domain-agnostic and xFormers is used by researchers in vision, NLP and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research first&lt;/strong&gt;: xFormers contains bleeding-edge components, that are not yet available in mainstream libraries like pytorch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Built with efficiency in mind&lt;/strong&gt;: Because speed of iteration matters, components are as fast and memory-efficient as possible. xFormers contains its own CUDA kernels, but dispatches to other libraries when relevant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installing xFormers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;(RECOMMENDED) Using binaries&lt;/strong&gt;: We provide binaries for Linux and recent PyTorch versions. After you have &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;installed pytorch in conda&lt;/a&gt;, install xFormers with conda:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install xformers -c xformers/label/dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;From source&lt;/strong&gt;: Alternatively, if no binaries are available (for instance for windows), you can also install from source:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# (Optional) Makes the build much faster&#xA;pip install ninja&#xA;# Set TORCH_CUDA_ARCH_LIST if running and building on different GPU types&#xA;pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers&#xA;# (this can take dozens of minutes)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;pip wheels&lt;/strong&gt;: There is no updated package available on pip, please install from conda or from source&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory-efficient MHA&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/docs/plots/mha/mha_vit.png&#34; alt=&#34;Benchmarks for ViTS&#34;&gt; &lt;em&gt;Setup: A100 on f16, measured total time for a forward+backward pass&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that this is exact attention, not an approximation, just by calling &lt;a href=&#34;https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention&#34;&gt;&lt;code&gt;xformers.ops.memory_efficient_attention&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More benchmarks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;xFormers provides many components, and more benchmarks are available in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/BENCHMARKS.md&#34;&gt;BENCHMARKS.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;(Optional) Testing the installation&lt;/h3&gt; &#xA;&lt;p&gt;This command will provide information on an xFormers installation, and what kernels are built/available:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python -m xformers.info&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using xFormers&lt;/h2&gt; &#xA;&lt;h3&gt;Transformers key concepts&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s start from a classical overview of the Transformer architecture (illustration from Lin et al,, &#34;A Survey of Transformers&#34;)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/docs/assets/Transformer_arch_Lin_et_al.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll find the key repository boundaries in this illustration: a Transformer is generally made of a collection of attention mechanisms, embeddings to encode some positional information, feed-forward blocks and a residual path (typically referred to as pre- or post- layer norm). These boundaries do not work for all models, but we found in practice that given some accomodations it could capture most of the state of the art.&lt;/p&gt; &#xA;&lt;p&gt;Models are thus not implemented in monolithic files, which are typically complicated to handle and modify. Most of the concepts present in the above illustration correspond to an abstraction level, and when variants are present for a given sub-block it should always be possible to select any of them. You can focus on a given encapsulation level and modify it as needed.&lt;/p&gt; &#xA;&lt;h3&gt;Repo map&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;├── ops                         # Functional operators&#xA;    └ ...&#xA;├── components                  # Parts zoo, any of which can be used directly&#xA;│   ├── attention&#xA;│   │    └ ...                  # all the supported attentions&#xA;│   ├── feedforward             #&#xA;│   │    └ ...                  # all the supported feedforwards&#xA;│   ├── positional_embedding    #&#xA;│   │    └ ...                  # all the supported positional embeddings&#xA;│   ├── activations.py          #&#xA;│   └── multi_head_dispatch.py  # (optional) multihead wrap&#xA;|&#xA;├── benchmarks&#xA;│     └ ...                     # A lot of benchmarks that you can use to test some parts&#xA;└── triton&#xA;      └ ...                     # (optional) all the triton parts, requires triton + CUDA gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; Attention mechanisms&lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/scaled_dot_product.py&#34;&gt;Scaled dot product&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention is all you need, Vaswani et al., 2017&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/scaled_dot_product.py&#34;&gt;Sparse&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;whenever a sparse enough mask is passed&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/blocksparse.py&#34;&gt;BlockSparse&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;courtesy of &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/www.triton-lang.org&#34;&gt;Triton&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/linformer.py&#34;&gt;Linformer&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.04768&#34;&gt;Linformer, self-attention with linear complexity, Wang et al., 2020&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/nystrom.py&#34;&gt;Nystrom&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2102.03902&#34;&gt;Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention, Xiong et al., 2021&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/local.py&#34;&gt;Local&lt;/a&gt;. Notably used in (and many others)&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.05150&#34;&gt;Longformer: The Long-Document Transformer, Beltagy et al., 2020&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.14062&#34;&gt;BigBird, Transformer for longer sequences, Zaheer et al., 2020&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/favor.py&#34;&gt;Favor/Performer&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.14794v1&#34;&gt;Rethinking Attention with Performers, Choromanski et al., 2020&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/ortho.py&#34;&gt;Orthoformer&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.05392&#34;&gt;Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers, Patrick et al., 2021&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/random.py&#34;&gt;Random&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;See BigBird, Longformers,..&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/global_tokens.py&#34;&gt;Global&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;See BigBird, Longformers,..&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/fourier_mix.py&#34;&gt;FourierMix&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.03824v1&#34;&gt;FNet: Mixing Tokens with Fourier Transforms, Lee-Thorp et al.&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/compositional.py&#34;&gt;CompositionalAttention&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/pdf/2110.09419v1.pdf&#34;&gt;Compositional Attention: Disentangling search and retrieval, S. Mittal et al.&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/pooling.py&#34;&gt;2D Pooling&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/pdf/2111.11418v1.pdf&#34;&gt;Metaformer is actually what you need for vision, Yu et al.&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/attention/visual.py&#34;&gt;Visual Attention&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/pdf/2202.09741.pdf&#34;&gt;&lt;code&gt;Visual Attention Network&lt;/code&gt;_, Guo et al&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;... add a new one &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/CONTRIBUTING.md&#34;&gt;see Contribution.md&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Feed forward mechanisms &lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/feedforward/mlp.py&#34;&gt;MLP&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/feedforward/fused_mlp.py&#34;&gt;Fused&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/feedforward/mixture_of_experts.py&#34;&gt;Mixture of Experts&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/feedforward/conv_mlp.py&#34;&gt;Conv2DFeedforward&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Positional embedding &lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/positional_embedding/sine.py&#34;&gt;Sine&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/positional_embedding/vocab.py&#34;&gt;Vocabulary&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/positional_embedding/rotary.py&#34;&gt;Rotary&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/components/simplicial_embedding.py&#34;&gt;Simplicial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Residual paths &lt;/summary&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.04745v1.pdf&#34;&gt;Pre&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.04745v1.pdf&#34;&gt;Post&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.00555v1.pdf&#34;&gt;DeepNorm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Initializations &lt;/summary&gt;&#xA; &lt;p&gt; This is completely optional, and will only occur when generating full models through xFormers, not when picking parts individually. &lt;/p&gt;&#xA; &lt;p&gt;There are basically two initialization mechanisms exposed, but the user is free to initialize weights as he/she sees fit after the fact.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Parts can expose a &lt;code&gt;init_weights()&lt;/code&gt; method, which define sane defaults&lt;/li&gt; &#xA;  &lt;li&gt;xFormers supports &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/factory/weight_init.py&#34;&gt;specific init schemes&lt;/a&gt; which &lt;em&gt;can take precedence&lt;/em&gt; over the init_weights()&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;If the second code path is being used (construct model through the model factory), we check that all the weights have been initialized, and possibly error out if it&#39;s not the case (if you set &lt;code&gt;xformers.factory.weight_init.__assert_if_not_initialized = True&lt;/code&gt;)&lt;/p&gt; &#xA; &lt;p&gt;Supported initialization schemes are:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.05895&#34;&gt;Small init&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/raw/master/timm/models/vision_transformer.py&#34;&gt;Timm defaults&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/vision_transformer&#34;&gt;ViT defaults&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/moco-v3&#34;&gt;Moco v3 defaults&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;One way to specify the init scheme is to set the &lt;code&gt;config.weight_init&lt;/code&gt; field to the matching enum value. This could easily be extended, feel free to submit a PR !&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Many attention mechanisms, interchangeables&lt;/li&gt; &#xA; &lt;li&gt;Optimized building blocks, beyond PyTorch primitives &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Memory-efficient exact attention - up to 10x faster&lt;/li&gt; &#xA;   &lt;li&gt;sparse attention&lt;/li&gt; &#xA;   &lt;li&gt;block-sparse attention&lt;/li&gt; &#xA;   &lt;li&gt;fused softmax&lt;/li&gt; &#xA;   &lt;li&gt;fused linear layer&lt;/li&gt; &#xA;   &lt;li&gt;fused layer norm&lt;/li&gt; &#xA;   &lt;li&gt;fused dropout(activation(x+bias))&lt;/li&gt; &#xA;   &lt;li&gt;fused SwiGLU&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Benchmarking and testing tools &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/BENCHMARKS.md&#34;&gt;micro benchnmarks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;transformer block benchmark&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/xformers/benchmarks/LRA/README.md&#34;&gt;LRA&lt;/a&gt;, with SLURM support&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Programatic and sweep friendly layer and model construction &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Compatible with hierarchical Transformers, like Swin or Metaformer&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Hackable &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Not using monolithic CUDA kernels, composable building blocks&lt;/li&gt; &#xA;   &lt;li&gt;Using &lt;a href=&#34;https://triton-lang.org/&#34;&gt;Triton&lt;/a&gt; for some optimized parts, explicit, pythonic and user-accessible&lt;/li&gt; &#xA;   &lt;li&gt;Native support for SquaredReLU (on top of ReLU, LeakyReLU, GeLU, ..), extensible activations&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Install troubleshooting&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NVCC and the current CUDA runtime match. Depending on your setup, you may be able to change the CUDA runtime with &lt;code&gt;module unload cuda; module load cuda/xx.x&lt;/code&gt;, possibly also &lt;code&gt;nvcc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;the version of GCC that you&#39;re using matches the current NVCC capabilities&lt;/li&gt; &#xA; &lt;li&gt;the &lt;code&gt;TORCH_CUDA_ARCH_LIST&lt;/code&gt; env variable is set to the architures that you want to support. A suggested setup (slow to build but comprehensive) is &lt;code&gt;export TORCH_CUDA_ARCH_LIST=&#34;6.0;6.1;6.2;7.0;7.2;7.5;8.0;8.6&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If the build from source OOMs, it&#39;s possible to reduce the parallelism of ninja with &lt;code&gt;MAX_JOBS&lt;/code&gt; (eg &lt;code&gt;MAX_JOBS=2&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;If you encounter &lt;a href=&#34;https://github.com/facebookresearch/xformers/issues/390#issuecomment-1315020700&#34;&gt;&lt;code&gt;UnsatisfiableError&lt;/code&gt;&lt;/a&gt; when installing with conda, make sure you have pytorch installed in your conda environment, and that your setup (pytorch version, cuda version, python version, OS) match &lt;a href=&#34;https://anaconda.org/xformers/xformers/files&#34;&gt;an existing binary for xFormers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;xFormers has a BSD-style license, as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/xformers/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Citing xFormers&lt;/h2&gt; &#xA;&lt;p&gt;If you use xFormers in your publication, please cite it by using the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{xFormers2022,&#xA;  author =       {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza},&#xA;  title =        {xFormers: A modular and hackable Transformer modelling library},&#xA;  howpublished = {\url{https://github.com/facebookresearch/xformers}},&#xA;  year =         {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;The following repositories are used in xFormers, either in close to original form or as an inspiration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/sputnik&#34;&gt;Sputnik&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hgyhungry/ge-spmm&#34;&gt;GE-SpMM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/triton&#34;&gt;Triton&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lucidrains/reformer-pytorch&#34;&gt;LucidRain Reformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/RobinBruegger/RevTorch&#34;&gt;RevTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlpen/Nystromformer&#34;&gt;Nystromformer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/fairscale/&#34;&gt;FairScale&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;Pytorch Image Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nvidia/cutlass&#34;&gt;CUTLASS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;Flash-Attention&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>