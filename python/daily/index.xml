<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-01T01:45:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AIGC-Audio/AudioGPT</title>
    <updated>2023-05-01T01:45:12Z</updated>
    <id>tag:github.com,2023-05-01:/AIGC-Audio/AudioGPT</id>
    <link href="https://github.com/AIGC-Audio/AudioGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.12995&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=AIGC-Audio.AudioGPT&#34; alt=&#34;visitors&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide our implementation and pretrained models as open source in this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/AIGC-Audio/AudioGPT/main/run.md&#34;&gt;run.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Capabilities&lt;/h2&gt; &#xA;&lt;p&gt;Here we list the capability of AudioGPT at this time. More supported models and tasks are coming soon. For prompt examples, refer to &lt;a href=&#34;https://raw.githubusercontent.com/AIGC-Audio/AudioGPT/main/assets/README.md&#34;&gt;asset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Speech&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Speech&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;FastSpeech&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;SyntaSpeech&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;VITS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Style Transfer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;GenerSpeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Recognition&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Conformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Enhancement&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;ConvTasNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Separation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;TF-GridNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Speech Translation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Multi-decoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Mono-to-Binaural&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;NeuralWarp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Sing&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Sing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;DiffSinger&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;VISinger&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Audio&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text-to-Audio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Audio Inpainting&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-to-Audio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Make-An-Audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sound Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Audio-transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Target Sound Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;TSDNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sound Extraction&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;LASSNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Talking Head&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Supported Foundation Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Talking Head Synthesis&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;GeneFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Yes (WIP)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate the open source of the following projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPNet&lt;/a&gt;   &lt;a href=&#34;https://github.com/NATSpeech/NATSpeech&#34;&gt;NATSpeech&lt;/a&gt;   &lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;Visual ChatGPT&lt;/a&gt;   &lt;a href=&#34;https://github.com/huggingface&#34;&gt;Hugging Face&lt;/a&gt;   &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;   &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;  &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>litanlitudan/skyagi</title>
    <updated>2023-05-01T01:45:12Z</updated>
    <id>tag:github.com,2023-05-01:/litanlitudan/skyagi</id>
    <link href="https://github.com/litanlitudan/skyagi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SkyAGI: Emerging human-behavior simulation capability in LLM&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/litanlitudan/skyagi/main/images/background.png&#34; height=&#34;600&#34; alt=&#34;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;SkyAGI: Emerging human-behavior simulation capability in LLM&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/skyagi/&#34;&gt; &lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/skyagi?color=gree&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;SkyAGI&lt;/code&gt; is a python package that demonstrates LLM&#39;s emerging capability in simulating believable human behaviors. Specifically, &lt;code&gt;SkyAGI&lt;/code&gt; implements the idea of &lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;Generative Agents&lt;/a&gt; and delivers a role-playing game that creates a very interesting user experience.&lt;/p&gt; &#xA;&lt;p&gt;Different from previous AI based NPC systems, &lt;code&gt;SkyAGI&lt;/code&gt;&#39;s NPC generates very believable human responses. The interesting observations in this demo show a huge potential for rethinking game development in many aspects, such as NPC script writing.&lt;/p&gt; &#xA;&lt;p&gt;To demonstrate this, &lt;code&gt;SkyAGI&lt;/code&gt; provides example characters from &lt;code&gt;The Big Bang Theory&lt;/code&gt; and &lt;code&gt;The Avengers&lt;/code&gt; as a starting point. Users could also define customized characters by creating config json files like &lt;a href=&#34;https://github.com/litanlitudan/skyagi/raw/main/examples/example_agent.json&#34;&gt;customized_character.json&lt;/a&gt; For details about the interesting observations, refer to the &lt;a href=&#34;https://github.com/litanlitudan/skyagi/#interesting-observations-in-this-demo&#34;&gt;observations section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Installation&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install --upgrade skyagi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;How to run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export OPENAI_API_KEY=&#34;...&#34;&#xA;skyagi&#xA;# or&#xA;OPENAI_API_KEY=&#34;...&#34; skyagi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example if the OpenAI key is &lt;code&gt;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&lt;/code&gt;, then the exact command would be the following&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# make sure no quote around the token&#xA;export OPENAI_API_KEY=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&#xA;skyagi&#xA;# or&#xA;OPENAI_API_KEY=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 skyagi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use example agent configs, download it from here: &lt;a href=&#34;https://github.com/litanlitudan/skyagi/tree/main/examples&#34;&gt;https://github.com/litanlitudan/skyagi/tree/main/examples&lt;/a&gt; (pip install doesn&#39;t contain the agent configuration)&lt;/p&gt; &#xA;&lt;p&gt;An example agent configuration (Sheldon) looks something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;name&#34;: &#34;Sheldon&#34;,&#xA;    &#34;age&#34;: 27,&#xA;    &#34;personality&#34;: &#34;Intelligent, rigid, socially challenged, quirky, and arrogant.&#34;,&#xA;    &#34;memories&#34;: [&#xA;        &#34;Sheldon is a theoretical physicist who works at Caltech.&#34;,&#xA;        &#34;Sheldon has an eidetic memory and is highly intelligent, but struggles with social skills and sarcasm.&#34;,&#xA;        ...&#xA;        &#34;Knock, knock, knock, Penny - This is the specific knock that Sheldon uses when he visits Penny&#39;s apartment, which he repeats three times.&#34;,&#xA;        &#34;Bazinga! - This is Sheldon&#39;s catchphrase that he uses to indicate he was joking or playing a prank on someone.&#34;&#xA;    ],&#xA;    &#34;current_status&#34;: &#34;Sheldon is at the Cheesecake Factory&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interesting observations in this demo&lt;/h2&gt; &#xA;&lt;p&gt;Here is a screenshot of a live demo using The Big Bang Theory example. &lt;img src=&#34;https://raw.githubusercontent.com/litanlitudan/skyagi/main/images/demo.png&#34; alt=&#34;demo&#34;&gt; From the conversation, we can observe three interesting points that have not been widely seen in previous systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Leonard remembered that Penny had asked him to persuade Sheldon to go for a hike, which shows the capability of some kind of memory.&lt;/li&gt; &#xA; &lt;li&gt;Leonard changed his mind after Sheldon whispered to him and even tried to convince Penny to join the scientific effort, which shows that the agents had meaningful progress in the story even without human intervention.&lt;/li&gt; &#xA; &lt;li&gt;All the responses are quite human-like. As a user, it&#39;s quite hard to tell whether it&#39;s actually an AI behind the responses.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;https://arxiv.org/abs/2304.03442&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://python.langchain.com/en/latest/use_cases/agent_simulations/characters.html#create-a-generative-character&#34;&gt;https://python.langchain.com/en/latest/use_cases/agent_simulations/characters.html#create-a-generative-character&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>lamini-ai/lamini</title>
    <updated>2023-05-01T01:45:12Z</updated>
    <id>tag:github.com,2023-05-01:/lamini-ai/lamini</id>
    <link href="https://github.com/lamini-ai/lamini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lamini: The LLM engine for rapidly customizing models 🦙&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-CC%20By%204.0-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-370/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.7+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official repo for Lamini&#39;s data generator for generating instructions to train instruction-following LLMs.&lt;/p&gt; &#xA;&lt;p&gt;All data and LLMs are under a CC-BY license that allows commercial use—all yours, you own it! 🦙🦙🦙&lt;/p&gt; &#xA;&lt;p&gt;What&#39;s here?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/#data-release&#34;&gt;71K dataset of instructions&lt;/a&gt; used for finetuning your own instruction-following LLM (like ChatGPT, which was also trained to follow instructions).&lt;/li&gt; &#xA; &lt;li&gt;The code for the &lt;a href=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/#run&#34;&gt;data generator&lt;/a&gt;, which only needs 100 datapoints to start generating 70k+ datapoints. You can customize the original 100+ datapoints to your own domain, to focus the data generator on that domain.&lt;/li&gt; &#xA; &lt;li&gt;Open-source fine-tuned LLMs that follow instructions, fine-tuned using a base Pythia model with the Lamini engine: [&lt;a href=&#34;https://huggingface.co/lamini/instruct-tuned-2.8b&#34;&gt;weights&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/lamini/instruct-playground&#34;&gt;playground&lt;/a&gt;].&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://lamini.ai/blog&#34;&gt;blogpost&lt;/a&gt; for layperson&#39;s terms of what&#39;s going on.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/assets/process.png&#34; alt=&#34;Lamini Process Step by Step&#34; width=&#34;700&#34;&gt; &#xA;&lt;h2&gt;Authentication to Lamini&lt;/h2&gt; &#xA;&lt;p&gt;Ready to configure your API key? It&#39;s easy-peasy! 🔑&lt;/p&gt; &#xA;&lt;p&gt;First, navigate to your &lt;a href=&#34;https://app.lamini.ai&#34;&gt;Lamini account page&lt;/a&gt; to retrieve your unique API key. Remember to keep this key a secret, and don&#39;t expose it in any client-side code or share it with others.&lt;/p&gt; &#xA;&lt;p&gt;Next, create a config file, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir ~/.powerml&#xA;touch ~/.powerml/configure_llama.yaml # backend system names&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, open the file with a text editor and place your key in it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;production:&#xA;    key: &#34;&amp;lt;YOUR-KEY-HERE&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The best part? The &lt;a href=&#34;https://pypi.org/project/llama-llm&#34;&gt;Lamini python package&lt;/a&gt; will automatically load your key from this config file for you, so you don&#39;t have to worry about it 🙌&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re running Lamini in a docker container, make sure to copy/mount this file inside the container 🐳&lt;/p&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://lamini-ai.github.io/auth/&#34;&gt;API docs&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:lamini-ai/lamini.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Python 🐍&lt;/h3&gt; &#xA;&lt;p&gt;In the repository, install python dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the program, to start generating data 📊📊📊&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 generate_data.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Docker 🐳&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;docker&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;Then, run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./run_generate_data_docker.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Expected Outputs &amp;amp; Autosaved Data 🦙&lt;/h2&gt; &#xA;&lt;p&gt;When you run the program, you should start seeing output of a &lt;code&gt;Seed Question&lt;/code&gt;, from the original small dataset in &lt;a href=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/seed_tasks.jsonl&#34;&gt;&lt;code&gt;seed_tasks.jsonl&lt;/code&gt;&lt;/a&gt;, and a &lt;code&gt;Novel Question&lt;/code&gt;, which is a generated question based on that &lt;code&gt;Seed Question&lt;/code&gt;.[^1] [^1]: The &lt;code&gt;Seed Questions&lt;/code&gt; in the Lamini seed dataset are instructions (combination of questions and commands), based on the &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct dataset&lt;/a&gt;. The generated questions are similar in nature to those and therefore don&#39;t &lt;em&gt;have to&lt;/em&gt; be questions. You can find the seed dataset at &lt;code&gt;data/lamini_dataset.jsonl&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;====== Seed Question =====&#xA; question=&#39;Plan a weekly lunch menu for a school. Write down a main dish, a carbohydrate side dish, a vegetable side dish, and a dessert for each day.&#39;&#xA;===== Novel Question =====&#xA; question=&#39;Write a knock knock story that has the same theme as the given one, but the characters, plots and settings are different&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These generated questions are saved to &lt;code&gt;data/questions.jsonl&lt;/code&gt;. This JSON file is a list of dictionaries with a &lt;code&gt;question&lt;/code&gt; field.&lt;/p&gt; &#xA;&lt;p&gt;Next, you&#39;ll see a &lt;code&gt;Response&lt;/code&gt; generated for each &lt;code&gt;Novel Question&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;====== Question =====&#xA; question=&#39;Which exercises are best&#39;&#xA;===== Response =====&#xA; response=&#39;It really depends what you want to achieve. If you want to lose weight, I would recommend high intensity interval training (HIIT) because it is really effective. If you want to gain muscle, I would recommend steady-state cardio like running or walking as these help maintain muscle mass. If you are interested in both losing weight and gaining muscle, I would recommend a combination of both HIIT and steady-state cardio.\n\n\nHIIT exercises are: running fast for short periods of time, followed by slow walking or jogging for longer periods of time. An example of a HIIT exercise is called the “Lunges&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These pairs are saved to &lt;code&gt;data/dataset.jsonl&lt;/code&gt;. This JSON file is a list of dictionaries with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; fields.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s poggers 💥&lt;/p&gt; &#xA;&lt;h2&gt;Modify&lt;/h2&gt; &#xA;&lt;h3&gt;I want to use my own seed data&lt;/h3&gt; &#xA;&lt;p&gt;We suggest creating your own dataset and changing the path to the &lt;a href=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/seed_tasks.jsonl&#34;&gt;&lt;code&gt;seed_tasks.jsonl&lt;/code&gt;&lt;/a&gt; in &lt;code&gt;generate_data.py&lt;/code&gt;(./generate_data.py) --- or you can replace &lt;a href=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/seed_tasks.jsonl&#34;&gt;&lt;code&gt;seed_tasks.jsonl&lt;/code&gt;&lt;/a&gt; with your own data in the same format. You can of course also modify how the data is loaded or write your own script with the &lt;code&gt;llama-llm&lt;/code&gt; library (pssst, &lt;a href=&#34;https://lamini-ai.github.io/auth/&#34;&gt;API docs&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;I only want to generate questions (to start)&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;generate_data.py&lt;/code&gt;(./generate_data.py), you can just run &lt;code&gt;generate_questions&lt;/code&gt;. This is a common use case for using human review after the question generation step to filter only the good ones for the next step of generating a response for each question.&lt;/p&gt; &#xA;&lt;h3&gt;I have my own instructions, and just want to generate responses&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;generate_data.py&lt;/code&gt;(./generate_data.py), you can just use the function &lt;code&gt;make_pairs&lt;/code&gt; to create the question-response pairs. This is a common use case step to run this stage separately, e.g. after human review of the generated questions, or if there was an error at this step last time.&lt;/p&gt; &#xA;&lt;h3&gt;I want to generate more than 100 instructions&lt;/h3&gt; &#xA;&lt;p&gt;Change the count flag &lt;code&gt;-c&lt;/code&gt; for the number question-repsonse pairs to generate in total. The default is set to 100.&lt;/p&gt; &#xA;&lt;h2&gt;Cleaning&lt;/h2&gt; &#xA;&lt;h3&gt;Using Python 🐍&lt;/h3&gt; &#xA;&lt;p&gt;In the repository, run the &lt;code&gt;remove_duplicates.py&lt;/code&gt; to remove duplicate questions from data/dataset.jsonl&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 remove_duplicates.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the program, to run a basic cleaning job on your data 🧼🧼🧼&lt;/p&gt; &#xA;&lt;p&gt;In the repository, run the &lt;code&gt;remove_duplicates_completion.py&lt;/code&gt; to remove responses where the model repeats itself from data/dataset.jsonl&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 remove_duplicates_completion.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the program, to run a more extensive cleaning job on your data 🛁🛁🛁&lt;/p&gt; &#xA;&lt;p&gt;These are examples. Consider using human filtering or writing additional post processing programs to further cleand and improve your data. Your fine-tuned models will thank you!&lt;/p&gt; &#xA;&lt;h2&gt;Data Release&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve run this script a few times and saved the results for you to freely use, at &lt;a href=&#34;https://raw.githubusercontent.com/lamini-ai/lamini/main/data/lamini_dataset.jsonl&#34;&gt;&lt;code&gt;data/lamini_dataset.jsonl&lt;/code&gt;&lt;/a&gt; 💸&lt;/p&gt; &#xA;&lt;p&gt;This file contains 72K instruction-following data for commercial use (ie. feel free to use it for your business! 💰📈). It&#39;s the same as the output, a list of dictionaries, each of which contains the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;question&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 52K instructions is unique, as generated by &lt;code&gt;lamini/open&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;response&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;lamini/instruct&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About Lamini&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lamini.ai/&#34;&gt;Lamini&lt;/a&gt; is the world&#39;s most powerful LLM engine, unlocking the power of generative AI for every company by putting their data to work. It is based on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Lamini&#34;&gt;lamini tribe&lt;/a&gt;, which includes llamas (LLMs!), alpacas, etc.&lt;/p&gt;</summary>
  </entry>
</feed>