<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-25T01:42:36Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LinkSoul-AI/Chinese-Llama-2-7b</title>
    <updated>2023-07-25T01:42:36Z</updated>
    <id>tag:github.com,2023-07-25:/LinkSoul-AI/Chinese-Llama-2-7b</id>
    <link href="https://github.com/LinkSoul-AI/Chinese-Llama-2-7b" rel="alternate"></link>
    <summary type="html">&lt;p&gt;开源社区第一个能下载、能运行的中文 LLaMA2 模型！&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chinese Llama 2 7B&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LinkSoul-AI/Chinese-Llama-2-7b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LLaMA2-Chinese-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LinkSoul-AI/Chinese-Llama-2-7b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Commercial-Support-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LinkSoul-AI/Chinese-Llama-2-7b/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_v2-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/LinkSoul/Chinese-Llama-2-7b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HuggingFace-Live_Demo-green&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/LinkSoul/instruction_merge_set&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Datasets-instruction_merge_set-blue&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;全部开源，完全可商用的&lt;strong&gt;中文版 Llama2 模型及中英文 SFT 数据集&lt;/strong&gt;，输入格式严格遵循 &lt;em&gt;llama-2-chat&lt;/em&gt; 格式，兼容适配所有针对原版 &lt;em&gt;llama-2-chat&lt;/em&gt; 模型的优化。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LinkSoul-AI/Chinese-Llama-2-7b/main/.github/preview.jpg&#34; alt=&#34;Chinese LLaMA2 7B&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;基础演示&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LinkSoul-AI/Chinese-Llama-2-7b/main/.github/demo.gif&#34; alt=&#34;Base Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;在线试玩&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Talk is cheap, Show you the Demo.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/LinkSoul/Chinese-Llama-2-7b&#34;&gt;Demo 地址 / HuggingFace Spaces&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LinkSoul-AI/Chinese-Llama-2-7b/main/#&#34;&gt;Colab 一键启动&lt;/a&gt; // 正在准备&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;资源下载&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;模型下载：&lt;a href=&#34;https://huggingface.co/LinkSoul/Chinese-Llama-2-7b&#34;&gt;Chinese Llama2 Chat Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;4bit量化：&lt;a href=&#34;https://huggingface.co/LinkSoul/Chinese-Llama-2-7b-4bit&#34;&gt;Chinese Llama2 4bit Chat Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;GGML Q4 模型（社区版）：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/rffx0/Chinese-Llama-2-7b-ggml-model-q4_0&#34;&gt;https://huggingface.co/rffx0/Chinese-Llama-2-7b-ggml-model-q4_0&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/soulteary/Chinese-Llama-2-7b-ggml-q4&#34;&gt;https://huggingface.co/soulteary/Chinese-Llama-2-7b-ggml-q4&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;我们使用了中英文 SFT 数据集，数据量 1000 万。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;数据集：&lt;a href=&#34;https://huggingface.co/datasets/LinkSoul/instruction_merge_set&#34;&gt;https://huggingface.co/datasets/LinkSoul/instruction_merge_set&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速测试&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer&#xA;&#xA;model_path = &#34;LinkSoul/Chinese-Llama-2-7b&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)&#xA;model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()&#xA;streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)&#xA;&#xA;instruction = &#34;&#34;&#34;[INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.&#xA;&#xA;            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don&#39;t know the answer to a question, please don&#39;t share false information.\n&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;\n\n{} [/INST]&#34;&#34;&#34;&#xA;&#xA;prompt = instruction.format(&#34;用中文回答，When is the best time to visit Beijing, and do you have any suggestions for me?&#34;)&#xA;generate_ids = model.generate(tokenizer(prompt, return_tensors=&#39;pt&#39;).input_ids.cuda(), max_new_tokens=4096, streamer=streamer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;你可以使用仓库中的 Dockerfile，来快速制作基于 Nvidia 最新版本的 &lt;code&gt;nvcr.io/nvidia/pytorch:23.06-py3&lt;/code&gt; 基础镜像，在任何地方使用容器来运行中文的 LLaMA2 模型应用。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t linksoul/chinese-llama2-chat .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;镜像构建完毕，使用命令运行镜像即可：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --rm -it -v `pwd`/LinkSoul:/app/LinkSoul -p 7860:7860 linksoul/chinese-llama2-chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GGML / Llama.cpp&lt;/h2&gt; &#xA;&lt;p&gt;想要在 CPU 环境运行 LLaMA2 模型么？使用下面的方法吧。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用 &lt;code&gt;ggml/convert_to_ggml.py&lt;/code&gt; 进行转换操作，详见脚本支持的 CLI 参数。&lt;/li&gt; &#xA; &lt;li&gt;或使用 &lt;code&gt;docker pull soulteary/llama2:converter&lt;/code&gt; 下载模型格式转换工具镜像，在 Docker 容器中使用下面的两条命令完成操作（教程 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/645426799&#34;&gt;构建能够使用 CPU 运行的 MetaAI LLaMA2 中文大模型 &lt;/a&gt;）：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 convert.py /app/LinkSoul/Chinese-Llama-2-7b/ --outfile /app/LinkSoul/Chinese-Llama-2-7b-ggml.bin&#xA;./quantize /app/LinkSoul/Chinese-Llama-2-7b-ggml.bin /app/LinkSoul/Chinese-Llama-2-7b-ggml-q4.bin q4_0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;如何训练&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATASET=&#34;LinkSoul/instruction_merge_set&#34;&#xA;&#xA;DATA_CACHE_PATH=&#34;hf_datasets_cache&#34;&#xA;MODEL_PATH=&#34;/PATH/TO/TRANSFORMERS/VERSION/LLAMA2&#34;&#xA;&#xA;output_dir=&#34;./checkpoints_llama2&#34;&#xA;&#xA;torchrun --nnodes=1 --node_rank=0 --nproc_per_node=8 \&#xA;    --master_port=25003 \&#xA;        train.py \&#xA;        --model_name_or_path ${MODEL_PATH} \&#xA;        --data_path ${DATASET} \&#xA;        --data_cache_path ${DATA_CACHE_PATH} \&#xA;        --bf16 True \&#xA;        --output_dir ${output_dir} \&#xA;        --num_train_epochs 1 \&#xA;        --per_device_train_batch_size 4 \&#xA;        --per_device_eval_batch_size 4 \&#xA;        --gradient_accumulation_steps 1 \&#xA;        --evaluation_strategy &#39;no&#39; \&#xA;        --save_strategy &#39;steps&#39; \&#xA;        --save_steps 1200 \&#xA;        --save_total_limit 5 \&#xA;        --learning_rate 2e-5 \&#xA;        --weight_decay 0. \&#xA;        --warmup_ratio 0.03 \&#xA;        --lr_scheduler_type cosine \&#xA;        --logging_steps 1 \&#xA;        --fsdp &#39;full_shard auto_wrap&#39; \&#xA;        --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;        --tf32 True \&#xA;        --model_max_length 4096 \&#xA;        --gradient_checkpointing True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;相关项目&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;Llama2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/soulteary/docker-llama2-chat&#34;&gt;soulteary/docker-llama2-chat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;项目协议&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/LinkSoul-AI/Chinese-Llama-2-7b/raw/main/LICENSE&#34;&gt;Apache-2.0 license&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;微信交流群&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/LinkSoul-AI/Chinese-Llama-2-7b/main/.github/QRcode.jpg&#34; alt=&#34;微信交流群&#34; width=&#34;300&#34;&gt;</summary>
  </entry>
  <entry>
    <title>Jamie-Stirling/RetNet</title>
    <updated>2023-07-25T01:42:36Z</updated>
    <id>tag:github.com,2023-07-25:/Jamie-Stirling/RetNet</id>
    <link href="https://github.com/Jamie-Stirling/RetNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An implementation of &#34;Retentive Network: A Successor to Transformer for Large Language Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RetNet&lt;/h1&gt; &#xA;&lt;p&gt;An implementation of &lt;a href=&#34;https://arxiv.org/pdf/2307.08621.pdf&#34;&gt;Retentive Network: A Successor to Transformer for Large Language Models&lt;/a&gt; in PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;About this repository&lt;/h2&gt; &#xA;&lt;p&gt;This is a minimal, pure pytorch implementation of RetNet. RetNet paper: &lt;a href=&#34;https://arxiv.org/pdf/2307.08621.pdf&#34;&gt;Retentive Network: A Successor to Transformer for Large Language Models&lt;/a&gt; in PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;The contributors(s) to this repository are not authors of the original paper. All credit for the idea and formulation of RetNet goes to the original authors.&lt;/p&gt; &#xA;&lt;p&gt;The purpose of this repository is to aid scientific and technological understanding and advancement. The code prioritizes correctness and readability over optimization.&lt;/p&gt; &#xA;&lt;h2&gt;Features implemented&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Single-scale and MultiScale retention: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;parallel paradigm&lt;/li&gt; &#xA;   &lt;li&gt;recurrent paradigm&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Multi-layer retentive network with FFN and LayerNorm &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;parallel paradigm&lt;/li&gt; &#xA;   &lt;li&gt;recurrent paradigm&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Causal language model (CLM) built on top of the the retentive network&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features coming soon:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;chunkwise recurrent paradigm (for efficient inference)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage and Examples:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See scripts prefixed with &lt;code&gt;test_&lt;/code&gt; for examples of basic usage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Positional Encodings&lt;/h2&gt; &#xA;&lt;p&gt;The main implementation in &lt;code&gt;src/&lt;/code&gt; uses &lt;a href=&#34;https://github.com/microsoft/torchscale/raw/main/torchscale/component/xpos_relative_position.py&#34;&gt;Microsoft&#39;s xPos&lt;/a&gt; for positional encoding.&lt;/p&gt; &#xA;&lt;p&gt;The implementation in &lt;code&gt;src/complex&lt;/code&gt; uses complex values to encode position, which requires parameter and data throughput types to be &lt;code&gt;torch.ComplexFloat&lt;/code&gt; (64-bit). This has some limitations due to there not yet being torch support for half-precision complex types. It also requires twice the amount of memory as real-valued data at 32-bit precision.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;All contributions are welcome. Please see &lt;a href=&#34;https://github.com/Jamie-Stirling/RetNet/issues&#34;&gt;issues&lt;/a&gt; for an idea of what needs doing.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to contribute to this project, please fork it and submit a pull request for review.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{sun2023retentive,&#xA;      title={Retentive Network: A Successor to Transformer for Large Language Models}, &#xA;      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},&#xA;      year={2023},&#xA;      eprint={2307.08621},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Lxtharia/minegrub-theme</title>
    <updated>2023-07-25T01:42:36Z</updated>
    <id>tag:github.com,2023-07-25:/Lxtharia/minegrub-theme</id>
    <link href="https://github.com/Lxtharia/minegrub-theme" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Grub Theme in the style of Minecraft!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Minegrub&lt;/h1&gt; &#xA;&lt;p&gt;A Grub Theme in the style of Minecraft!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lxtharia/minegrub-theme/main/assets/preview_minegrub.png&#34; alt=&#34;Minegrub Preview &amp;quot;Screenshot&amp;quot;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h3&gt;Note: grub vs grub2&lt;/h3&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Check if you have a &lt;code&gt;/boot/grub2&lt;/code&gt; folder instead of a &lt;code&gt;/boot/grub&lt;/code&gt; folder in which case you would just have to adjust the file paths mentioned here and in the &lt;code&gt;assets/minegrub-update.service&lt;/code&gt; file&lt;/li&gt; &#xA;  &lt;li&gt;Also if you&#39;re not sure, run &lt;code&gt;grub-mkconfig -V&lt;/code&gt; to check if you have grub version 2 (you should have)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Lxtharia/minegrub-theme.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create the theme folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo mkdir /boot/grub/themes/minegrub-theme&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copy the folder to your boot partition: (for info: &lt;code&gt;-ruv&lt;/code&gt; = recursive, update, verbose)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo cp -ruv ./minegrub-theme/* /boot/grub/themes/minegrub-theme/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change/add this line in your &lt;code&gt;/etc/default/grub&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;GRUB_THEME=/boot/grub/themes/minegrub-theme/theme.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update your live grub config by running&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo grub-mkconfig -o /boot/grub/grub.cfg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You&#39;re good to go!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Random splash texts and accurate &#34;x Packages Installed&#34; text!&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;update_theme.py&lt;/code&gt; script chooses a random line from &lt;code&gt;resources/splashes.txt&lt;/code&gt; and generates and replaces the &lt;code&gt;logo.png&lt;/code&gt; which holds the splash text, as well as updates the amount of packages currently installed&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure &lt;code&gt;neofetch&lt;/code&gt; is installed&lt;/li&gt; &#xA; &lt;li&gt;Make sure Python 3 (or an equivalent) and the Pillow python package are installed &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Install Pillow either with the python-pillow package from the AUR or with &lt;code&gt;sudo -H pip3 install pillow&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;It&#39;s important to use &lt;code&gt;sudo -H&lt;/code&gt;, because it needs to be available for the root user&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;To add new splash texts simply edit &lt;code&gt;./resources/splashes.txt&lt;/code&gt; and add them to the end of the file (if you add it at the beginning or in the middle, some splashes may never get used because the image cashing uses the line of the file the splash is on)&lt;/li&gt; &#xA; &lt;li&gt;If you want to remove splashes you should reset the cache by deleting &lt;code&gt;/boot/grub/themes/minegrub-theme/cache&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Update splash and &#34;Packages Installed&#34;...&lt;/h3&gt; &#xA;&lt;h4&gt;...without systemd&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Just run &lt;code&gt;python /boot/grub/themes/minegrub-theme/update_theme.py&lt;/code&gt; (from anywhere) after boot using whatever method works for you&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;...with systemd&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Edit &lt;code&gt;./assets/minegrub-update.service&lt;/code&gt; to use &lt;code&gt;/boot/grub2/&lt;/code&gt; on line 5 if applicable&lt;/li&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;./assets/minegrub-update.service&lt;/code&gt; to &lt;code&gt;/etc/systemd/system&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enable the service: &lt;code&gt;systemctl enable minegrub-update.service&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If it&#39;s not updating after rebooting (it won&#39;t update on the first reboot because it updates after you boot into your system), check systemctl status minegrub-update.service for any errors (for example if pillow isn&#39;t installed in the correct scope)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Adjusting for a different amount of boot options:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When you have more/less than 4 boot options, you might want to adjust the height of the bottom bar (that says &#34;Options&#34; and &#34;Console&#34;)&lt;/li&gt; &#xA; &lt;li&gt;The formula and some precalculated values (for 2,3,4,5... boot options) are in the &lt;code&gt;theme.txt&lt;/code&gt;, so you should be able to easily change it to the correct value.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the &lt;code&gt;GRUB_TIMEOUT_STYLE&lt;/code&gt; in the defaults/grub file should be set to &lt;code&gt;menu&lt;/code&gt;, so it immediately shows the menu (else you would need to press ESC and you dont want that)&lt;/li&gt; &#xA; &lt;li&gt;I&#39;m no Linux expert, that&#39;s why I explain it so thoroughly, for other newbies :&amp;gt;&lt;/li&gt; &#xA; &lt;li&gt;i use arch btw&lt;/li&gt; &#xA; &lt;li&gt;i hope u like it, cause i sure do lmao&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Thanks to&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/toboot&#34;&gt;https://github.com/toboot&lt;/a&gt; for giving me this wonderful idea!&lt;/li&gt; &#xA; &lt;li&gt;the internet for giving me wisdom lmao (Mainly &lt;a href=&#34;http://wiki.rosalab.ru/en/index.php/Grub2_theme_tutorial&#34;&gt;http://wiki.rosalab.ru/en/index.php/Grub2_theme_tutorial&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;The contributors for contributing and giving me some motivation to improve some little things here and there&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Font downloaded from &lt;a href=&#34;https://www.fontspace.com/minecraft-font-f28180&#34;&gt;https://www.fontspace.com/minecraft-font-f28180&lt;/a&gt; and used for non commercial use.&lt;/p&gt;</summary>
  </entry>
</feed>