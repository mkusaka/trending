<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-27T01:35:13Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>cvg/LightGlue</title>
    <updated>2024-11-27T01:35:13Z</updated>
    <id>tag:github.com,2024-11-27:/cvg/LightGlue</id>
    <link href="https://github.com/cvg/LightGlue" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LightGlue: Local Feature Matching at Light Speed (ICCV 2023)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;&lt;ins&gt;LightGlue&lt;/ins&gt; ⚡️&lt;br&gt;Local Feature Matching at Light Speed&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.linkedin.com/in/philipplindenberger/&#34;&gt;Philipp Lindenberger&lt;/a&gt; · &lt;a href=&#34;https://psarlin.com/&#34;&gt;Paul-Edouard&amp;nbsp;Sarlin&lt;/a&gt; · &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/mapoll/&#34;&gt;Marc&amp;nbsp;Pollefeys&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;p&gt;ICCV 2023&lt;/p&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.13643.pdf&#34; align=&#34;center&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/cvg/LightGlue/blob/main/demo.ipynb&#34; align=&#34;center&#34;&gt;Colab&lt;/a&gt; | &lt;a href=&#34;https://psarlin.com/assets/LightGlue_ICCV2023_poster_compressed.pdf&#34; align=&#34;center&#34;&gt;Poster&lt;/a&gt; | &lt;a href=&#34;https://github.com/cvg/glue-factory&#34; align=&#34;center&#34;&gt;Train your own!&lt;/a&gt; &lt;/h2&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.13643&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/assets/easy_hard.jpg&#34; alt=&#34;example&#34; width=&#34;80%&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;em&gt;LightGlue is a deep neural network that matches sparse local features across image pairs.&lt;br&gt;An adaptive mechanism makes it fast for easy pairs (top) and reduces the computational complexity for difficult ones (bottom).&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This repository hosts the inference code of LightGlue, a lightweight feature matcher with high accuracy and blazing fast inference. It takes as input a set of keypoints and descriptors for each image and returns the indices of corresponding points. The architecture is based on adaptive pruning techniques, in both network width and depth - &lt;a href=&#34;https://arxiv.org/pdf/2306.13643.pdf&#34;&gt;check out the paper for more details&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We release pretrained weights of LightGlue with &lt;a href=&#34;https://arxiv.org/abs/1712.07629&#34;&gt;SuperPoint&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2006.13566&#34;&gt;DISK&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2304.03608&#34;&gt;ALIKED&lt;/a&gt; and &lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34;&gt;SIFT&lt;/a&gt; local features. The training and evaluation code can be found in our library &lt;a href=&#34;https://github.com/cvg/glue-factory/&#34;&gt;glue-factory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation and demo &lt;a href=&#34;https://colab.research.google.com/github/cvg/LightGlue/blob/main/demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Install this repo using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/cvg/LightGlue.git &amp;amp;&amp;amp; cd LightGlue&#xA;python -m pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/demo.ipynb&#34;&gt;demo notebook&lt;/a&gt; which shows how to perform feature extraction and matching on an image pair.&lt;/p&gt; &#xA;&lt;p&gt;Here is a minimal script to match two images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lightglue import LightGlue, SuperPoint, DISK, SIFT, ALIKED, DoGHardNet&#xA;from lightglue.utils import load_image, rbd&#xA;&#xA;# SuperPoint+LightGlue&#xA;extractor = SuperPoint(max_num_keypoints=2048).eval().cuda()  # load the extractor&#xA;matcher = LightGlue(features=&#39;superpoint&#39;).eval().cuda()  # load the matcher&#xA;&#xA;# or DISK+LightGlue, ALIKED+LightGlue or SIFT+LightGlue&#xA;extractor = DISK(max_num_keypoints=2048).eval().cuda()  # load the extractor&#xA;matcher = LightGlue(features=&#39;disk&#39;).eval().cuda()  # load the matcher&#xA;&#xA;# load each image as a torch.Tensor on GPU with shape (3,H,W), normalized in [0,1]&#xA;image0 = load_image(&#39;path/to/image_0.jpg&#39;).cuda()&#xA;image1 = load_image(&#39;path/to/image_1.jpg&#39;).cuda()&#xA;&#xA;# extract local features&#xA;feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None&#xA;feats1 = extractor.extract(image1)&#xA;&#xA;# match the features&#xA;matches01 = matcher({&#39;image0&#39;: feats0, &#39;image1&#39;: feats1})&#xA;feats0, feats1, matches01 = [rbd(x) for x in [feats0, feats1, matches01]]  # remove batch dimension&#xA;matches = matches01[&#39;matches&#39;]  # indices with shape (K,2)&#xA;points0 = feats0[&#39;keypoints&#39;][matches[..., 0]]  # coordinates in image #0, shape (K,2)&#xA;points1 = feats1[&#39;keypoints&#39;][matches[..., 1]]  # coordinates in image #1, shape (K,2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a convenience method to match a pair of images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lightglue import match_pair&#xA;feats0, feats1, matches01 = match_pair(extractor, matcher, image0, image1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.13643&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/assets/teaser.svg?sanitize=true&#34; alt=&#34;Logo&#34; width=&#34;50%&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;em&gt;LightGlue can adjust its depth (number of layers) and width (number of keypoints) per image pair, with a marginal impact on accuracy.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Advanced configuration&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Detail of all parameters - click to expand]&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;n_layers&lt;/code&gt;: Number of stacked self+cross attention layers. Reduce this value for faster inference at the cost of accuracy (continuous red line in the plot above). Default: 9 (all layers).&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;flash&lt;/code&gt;: Enable FlashAttention. Significantly increases the speed and reduces the memory consumption without any impact on accuracy. Default: True (LightGlue automatically detects if FlashAttention is available).&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;mp&lt;/code&gt;: Enable mixed precision inference. Default: False (off)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;depth_confidence&lt;/code&gt;: Controls the early stopping. A lower values stops more often at earlier layers. Default: 0.95, disable with -1.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;width_confidence&lt;/code&gt;: Controls the iterative point pruning. A lower value prunes more points earlier. Default: 0.99, disable with -1.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;filter_threshold&lt;/code&gt;: Match confidence. Increase this value to obtain less, but stronger matches. Default: 0.1&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;The default values give a good trade-off between speed and accuracy. To maximize the accuracy, use all keypoints and disable the adaptive mechanisms:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;extractor = SuperPoint(max_num_keypoints=None)&#xA;matcher = LightGlue(features=&#39;superpoint&#39;, depth_confidence=-1, width_confidence=-1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To increase the speed with a small drop of accuracy, decrease the number of keypoints and lower the adaptive thresholds:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;extractor = SuperPoint(max_num_keypoints=1024)&#xA;matcher = LightGlue(features=&#39;superpoint&#39;, depth_confidence=0.9, width_confidence=0.95)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The maximum speed is obtained with a combination of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;FlashAttention&lt;/a&gt;: automatically used when &lt;code&gt;torch &amp;gt;= 2.0&lt;/code&gt; or if &lt;a href=&#34;https://github.com/HazyResearch/flash-attention#installation-and-features&#34;&gt;installed from source&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PyTorch compilation, available when &lt;code&gt;torch &amp;gt;= 2.0&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;matcher = matcher.eval().cuda()&#xA;matcher.compile(mode=&#39;reduce-overhead&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For inputs with fewer than 1536 keypoints (determined experimentally), this compiles LightGlue but disables point pruning (large overhead). For larger input sizes, it automatically falls backs to eager mode with point pruning. Adaptive depths is supported for any input size.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/assets/benchmark.png&#34; alt=&#34;Logo&#34; width=&#34;80%&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;em&gt;Benchmark results on GPU (RTX 3080). With compilation and adaptivity, LightGlue runs at 150 FPS @ 1024 keypoints and 50 FPS @ 4096 keypoints per image. This is a 4-10x speedup over SuperGlue. &lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/assets/benchmark_cpu.png&#34; alt=&#34;Logo&#34; width=&#34;80%&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;em&gt;Benchmark results on CPU (Intel i7 10700K). LightGlue runs at 20 FPS @ 512 keypoints. &lt;/em&gt; &lt;/p&gt; &#xA;&lt;p&gt;Obtain the same plots for your setup using our &lt;a href=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/benchmark.py&#34;&gt;benchmark script&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark.py [--device cuda] [--add_superglue] [--num_keypoints 512 1024 2048 4096] [--compile]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Performance tip - click to expand]&lt;/summary&gt; &#xA; &lt;p&gt;Note: &lt;strong&gt;Point pruning&lt;/strong&gt; introduces an overhead that sometimes outweighs its benefits. Point pruning is thus enabled only when the there are more than N keypoints in an image, where N is hardware-dependent. We provide defaults optimized for current hardware (RTX 30xx GPUs). We suggest running the benchmark script and adjusting the thresholds for your hardware by updating &lt;code&gt;LightGlue.pruning_keypoint_thresholds[&#39;cuda&#39;]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Training and evaluation&lt;/h2&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://github.com/cvg/glue-factory&#34;&gt;Glue Factory&lt;/a&gt;, you can train LightGlue with your own local features, on your own dataset! You can also evaluate it and other baselines on standard benchmarks like HPatches and MegaDepth.&lt;/p&gt; &#xA;&lt;h2&gt;Other links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cvg/Hierarchical-Localization/&#34;&gt;hloc - the visual localization toolbox&lt;/a&gt;: run LightGlue for Structure-from-Motion and visual localization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fabio-sim/LightGlue-ONNX&#34;&gt;LightGlue-ONNX&lt;/a&gt;: export LightGlue to the Open Neural Network Exchange (ONNX) format with support for TensorRT and OpenVINO.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vincentqyw/image-matching-webui&#34;&gt;Image Matching WebUI&lt;/a&gt;: a web GUI to easily compare different matchers, including LightGlue.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kornia.readthedocs.io&#34;&gt;kornia&lt;/a&gt; now exposes LightGlue via the interfaces &lt;a href=&#34;https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.LightGlue&#34;&gt;&lt;code&gt;LightGlue&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.LightGlueMatcher&#34;&gt;&lt;code&gt;LightGlueMatcher&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use any ideas from the paper or code from this repo, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;@inproceedings{lindenberger2023lightglue,&#xA;  author    = {Philipp Lindenberger and&#xA;               Paul-Edouard Sarlin and&#xA;               Marc Pollefeys},&#xA;  title     = {{LightGlue: Local Feature Matching at Light Speed}},&#xA;  booktitle = {ICCV},&#xA;  year      = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The pre-trained weights of LightGlue and the code provided in this repository are released under the &lt;a href=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/LICENSE&#34;&gt;Apache-2.0 license&lt;/a&gt;. &lt;a href=&#34;https://github.com/cvlab-epfl/disk&#34;&gt;DISK&lt;/a&gt; follows this license as well but SuperPoint follows &lt;a href=&#34;https://github.com/magicleap/SuperPointPretrainedNetwork/raw/master/LICENSE&#34;&gt;a different, restrictive license&lt;/a&gt; (this includes its pre-trained weights and its &lt;a href=&#34;https://raw.githubusercontent.com/cvg/LightGlue/main/lightglue/superpoint.py&#34;&gt;inference file&lt;/a&gt;). &lt;a href=&#34;https://github.com/Shiaoming/ALIKED&#34;&gt;ALIKED&lt;/a&gt; was published under a BSD-3-Clause license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>canonical/cloud-init</title>
    <updated>2024-11-27T01:35:13Z</updated>
    <id>tag:github.com,2024-11-27:/canonical/cloud-init</id>
    <link href="https://github.com/canonical/cloud-init" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official upstream for the cloud-init: cloud instance initialization&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cloud-init&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/canonical/cloud-init/actions/workflows/unit.yml/badge.svg?branch=main&#34; alt=&#34;Unit Tests&#34;&gt; &lt;img src=&#34;https://github.com/canonical/cloud-init/actions/workflows/integration.yml/badge.svg?branch=main&#34; alt=&#34;Integration Tests&#34;&gt; &lt;img src=&#34;https://github.com/canonical/cloud-init/actions/workflows/check_format.yml/badge.svg?branch=main&#34; alt=&#34;Documentation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cloud-init is the &lt;em&gt;industry standard&lt;/em&gt; multi-distribution method for cross-platform cloud instance initialization. It is supported across all major public cloud providers, provisioning systems for private cloud infrastructure, and bare-metal installations.&lt;/p&gt; &#xA;&lt;p&gt;Cloud instances are initialized from a disk image and instance data:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloud metadata&lt;/li&gt; &#xA; &lt;li&gt;User data (optional)&lt;/li&gt; &#xA; &lt;li&gt;Vendor data (optional)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cloud-init will identify the cloud it is running on during boot, read any provided metadata from the cloud and initialize the system accordingly. This may involve setting up network and storage devices to configuring SSH access key and many other aspects of a system. Later on cloud-init will also parse and process any optional user or vendor data that was passed to the instance.&lt;/p&gt; &#xA;&lt;h2&gt;Getting help&lt;/h2&gt; &#xA;&lt;p&gt;If you need support, start with the &lt;a href=&#34;https://docs.cloud-init.io/en/latest/&#34;&gt;user documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you need additional help consider reaching out with one of the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ask a question in the &lt;a href=&#34;https://kiwiirc.com/nextclient/irc.libera.chat/cloud-init&#34;&gt;&lt;code&gt;#cloud-init&lt;/code&gt; IRC channel on Libera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Search the cloud-init &lt;a href=&#34;https://lists.launchpad.net/cloud-init/&#34;&gt;mailing list archive&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow announcements or ask a question on &lt;a href=&#34;https://discourse.ubuntu.com/c/server/cloud-init/&#34;&gt;the cloud-init Discourse forum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://launchpad.net/~cloud-init&#34;&gt;cloud-init mailing list&lt;/a&gt; and participate&lt;/li&gt; &#xA; &lt;li&gt;Find a bug? &lt;a href=&#34;https://github.com/canonical/cloud-init/issues&#34;&gt;Report bugs on GitHub Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Distribution and cloud support&lt;/h2&gt; &#xA;&lt;p&gt;The majority of &lt;a href=&#34;https://docs.cloud-init.io/en/latest/reference/datasources.html#datasources_supported&#34;&gt;clouds&lt;/a&gt; and &lt;a href=&#34;https://docs.cloud-init.io/en/latest/reference/distros.html&#34;&gt;Linux / Unix OSes&lt;/a&gt; are supported by and ship with cloud-init. If your distribution or cloud is not supported, please get in contact with that distribution and send them our way!&lt;/p&gt; &#xA;&lt;h2&gt;To start developing cloud-init&lt;/h2&gt; &#xA;&lt;p&gt;Checkout the &lt;a href=&#34;https://docs.cloud-init.io/en/latest/development/index.html&#34;&gt;contributing&lt;/a&gt; document that outlines the steps necessary to develop, test, and submit code.&lt;/p&gt; &#xA;&lt;h2&gt;Daily builds&lt;/h2&gt; &#xA;&lt;p&gt;Daily builds are useful if you want to try the latest upstream code for the latest features or to verify bug fixes.&lt;/p&gt; &#xA;&lt;p&gt;For Ubuntu, see the &lt;a href=&#34;https://code.launchpad.net/~cloud-init-dev/+archive/ubuntu/daily&#34;&gt;Daily PPAs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For CentOS, see the &lt;a href=&#34;https://copr.fedorainfracloud.org/coprs/g/cloud-init/cloud-init-dev/&#34;&gt;COPR build repos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build / packaging&lt;/h2&gt; &#xA;&lt;p&gt;To see reference build/packaging implementations, refer to &lt;a href=&#34;https://raw.githubusercontent.com/canonical/cloud-init/main/packages&#34;&gt;packages&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>