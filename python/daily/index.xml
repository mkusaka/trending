<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-30T01:39:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>naver/mast3r</title>
    <updated>2024-10-30T01:39:05Z</updated>
    <id>tag:github.com,2024-10-30:/naver/mast3r</id>
    <link href="https://github.com/naver/mast3r" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Grounding Image Matching in 3D with MASt3R&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/mast3r.jpg&#34; alt=&#34;banner&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official implementation of &lt;code&gt;Grounding Image Matching in 3D with MASt3R&lt;/code&gt;&lt;br&gt; [&lt;a href=&#34;https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/&#34;&gt;Project page&lt;/a&gt;], [&lt;a href=&#34;https://arxiv.org/abs/2406.09756&#34;&gt;MASt3R arxiv&lt;/a&gt;], [&lt;a href=&#34;https://arxiv.org/abs/2312.14132&#34;&gt;DUSt3R arxiv&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/examples.jpg&#34; alt=&#34;Example of matching results obtained from MASt3R&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/mast3r_archi.jpg&#34; alt=&#34;High level overview of MASt3R&#39;s architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mast3r_arxiv24,&#xA;      title={Grounding Image Matching in 3D with MASt3R}, &#xA;      author={Vincent Leroy and Yohann Cabon and Jerome Revaud},&#xA;      year={2024},&#xA;      eprint={2406.09756},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@inproceedings{dust3r_cvpr24,&#xA;      title={DUSt3R: Geometric 3D Vision Made Easy}, &#xA;      author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},&#xA;      booktitle = {CVPR},&#xA;      year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#get-started&#34;&gt;Get Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#checkpoints&#34;&gt;Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#interactive-demo&#34;&gt;Interactive demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#interactive-demo-with-docker&#34;&gt;Interactive demo with docker&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#training&#34;&gt;Training&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#our-hyperparameters&#34;&gt;Our Hyperparameters&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#visual-localization&#34;&gt;Visual Localization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#dataset-preparation&#34;&gt;Dataset Preparation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#example-commands&#34;&gt;Example Commands&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is distributed under the CC BY-NC-SA 4.0 License. See &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Copyright (C) 2024-present Naver Corporation. All rights reserved.&#xA;# Licensed under CC BY-NC-SA 4.0 (non-commercial use only).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone MASt3R.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/naver/mast3r&#xA;cd mast3r&#xA;# if you have already cloned mast3r:&#xA;# git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create the environment, here we show an example using conda.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n mast3r python=3.11 cmake=3.14.0&#xA;conda activate mast3r &#xA;conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia  # use the correct version of cuda for your system&#xA;pip install -r requirements.txt&#xA;pip install -r dust3r/requirements.txt&#xA;# Optional: you can also install additional packages to:&#xA;# - add support for HEIC images&#xA;# - add required packages for visloc.py&#xA;pip install -r dust3r/requirements_optional.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Optional, compile the cuda kernels for RoPE (as in CroCo v2).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# DUST3R relies on RoPE positional embeddings for which you can compile some cuda kernels for faster runtime.&#xA;cd dust3r/croco/models/curope/&#xA;python setup.py build_ext --inplace&#xA;cd ../../../../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;You can obtain the checkpoints by two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use our huggingface_hub integration: the models will be downloaded automatically.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Otherwise, We provide several pre-trained models:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Modelname&lt;/th&gt; &#xA;   &lt;th&gt;Training resolutions&lt;/th&gt; &#xA;   &lt;th&gt;Head&lt;/th&gt; &#xA;   &lt;th&gt;Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Decoder&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://download.europe.naverlabs.com/ComputerVision/MASt3R/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth&#34;&gt;&lt;code&gt;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;512x384, 512x336, 512x288, 512x256, 512x160&lt;/td&gt; &#xA;   &lt;td&gt;CatMLP+DPT&lt;/td&gt; &#xA;   &lt;td&gt;ViT-L&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can check the hyperparameters we used to train these models in the &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/#our-hyperparameters&#34;&gt;section: Our Hyperparameters&lt;/a&gt; Make sure to check license of the datasets we used.&lt;/p&gt; &#xA;&lt;p&gt;To download a specific model, for example &lt;code&gt;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p checkpoints/&#xA;wget https://download.europe.naverlabs.com/ComputerVision/MASt3R/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth -P checkpoints/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For these checkpoints, make sure to agree to the license of all the training datasets we used, in addition to CC-BY-NC-SA 4.0. The mapfree dataset license in particular is very restrictive. For more information, check &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/CHECKPOINTS_NOTICE&#34;&gt;CHECKPOINTS_NOTICE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive demo&lt;/h3&gt; &#xA;&lt;p&gt;We made one huggingface space running the new sparse global alignment in a simplified demo for small scenes: &lt;a href=&#34;https://huggingface.co/spaces/naver/MASt3R&#34;&gt;naver/MASt3R&lt;/a&gt; There are two demos available to run locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;demo.py is the updated demo for MASt3R. It uses our new sparse global alignment method that allows you to reconstruct larger scenes&#xA;&#xA;python3 demo.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#xA;&#xA;# Use --weights to load a checkpoint from a local file, eg --weights checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth&#xA;# Use --local_network to make it accessible on the local network, or --server_name to specify the url manually&#xA;# Use --server_port to change the port, by default it will search for an available port starting at 7860&#xA;# Use --device to use a different device, by default it&#39;s &#34;cuda&#34;&#xA;&#xA;demo_dust3r_ga.py is the same demo as in dust3r (+ compatibility for MASt3R models)&#xA;see https://github.com/naver/dust3r?tab=readme-ov-file#interactive-demo for details&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive demo with docker&lt;/h3&gt; &#xA;&lt;p&gt;To run MASt3R using Docker, including with NVIDIA CUDA support, follow these instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Docker&lt;/strong&gt;: If not already installed, download and install &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker compose&lt;/code&gt; from the &lt;a href=&#34;https://www.docker.com/get-started&#34;&gt;Docker website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Docker Toolkit&lt;/strong&gt;: For GPU support, install the NVIDIA Docker toolkit from the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;Nvidia website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build the Docker image and run it&lt;/strong&gt;: &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;./docker&lt;/code&gt; directory and run the following commands:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;bash run.sh --with-cuda --model_name=&#34;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you want to run the demo without CUDA support, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;bash run.sh --model_name=&#34;MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;demo.py&lt;/code&gt; is launched with the option &lt;code&gt;--local_network&lt;/code&gt;.&lt;br&gt; Visit &lt;code&gt;http://localhost:7860/&lt;/code&gt; to access the web UI (or replace &lt;code&gt;localhost&lt;/code&gt; with the machine&#39;s name to access it from the network).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;run.sh&lt;/code&gt; will launch docker-compose using either the &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/docker/docker-compose-cuda.yml&#34;&gt;docker-compose-cuda.yml&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/docker/docker-compose-cpu.yml&#34;&gt;docker-compose-cpu.ym&lt;/a&gt; config file, then it starts the demo using &lt;a href=&#34;https://raw.githubusercontent.com/naver/mast3r/main/docker/files/entrypoint.sh&#34;&gt;entrypoint.sh&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/demo.jpg&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mast3r.model import AsymmetricMASt3R&#xA;from mast3r.fast_nn import fast_reciprocal_NNs&#xA;&#xA;import mast3r.utils.path_to_dust3r&#xA;from dust3r.inference import inference&#xA;from dust3r.utils.image import load_images&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    device = &#39;cuda&#39;&#xA;    schedule = &#39;cosine&#39;&#xA;    lr = 0.01&#xA;    niter = 300&#xA;&#xA;    model_name = &#34;naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;    # you can put the path to a local checkpoint in model_name if needed&#xA;    model = AsymmetricMASt3R.from_pretrained(model_name).to(device)&#xA;    images = load_images([&#39;dust3r/croco/assets/Chateau1.png&#39;, &#39;dust3r/croco/assets/Chateau2.png&#39;], size=512)&#xA;    output = inference([tuple(images)], model, device, batch_size=1, verbose=False)&#xA;&#xA;    # at this stage, you have the raw dust3r predictions&#xA;    view1, pred1 = output[&#39;view1&#39;], output[&#39;pred1&#39;]&#xA;    view2, pred2 = output[&#39;view2&#39;], output[&#39;pred2&#39;]&#xA;&#xA;    desc1, desc2 = pred1[&#39;desc&#39;].squeeze(0).detach(), pred2[&#39;desc&#39;].squeeze(0).detach()&#xA;&#xA;    # find 2D-2D matches between the two images&#xA;    matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,&#xA;                                                   device=device, dist=&#39;dot&#39;, block_size=2**13)&#xA;&#xA;    # ignore small border around the edge&#xA;    H0, W0 = view1[&#39;true_shape&#39;][0]&#xA;    valid_matches_im0 = (matches_im0[:, 0] &amp;gt;= 3) &amp;amp; (matches_im0[:, 0] &amp;lt; int(W0) - 3) &amp;amp; (&#xA;        matches_im0[:, 1] &amp;gt;= 3) &amp;amp; (matches_im0[:, 1] &amp;lt; int(H0) - 3)&#xA;&#xA;    H1, W1 = view2[&#39;true_shape&#39;][0]&#xA;    valid_matches_im1 = (matches_im1[:, 0] &amp;gt;= 3) &amp;amp; (matches_im1[:, 0] &amp;lt; int(W1) - 3) &amp;amp; (&#xA;        matches_im1[:, 1] &amp;gt;= 3) &amp;amp; (matches_im1[:, 1] &amp;lt; int(H1) - 3)&#xA;&#xA;    valid_matches = valid_matches_im0 &amp;amp; valid_matches_im1&#xA;    matches_im0, matches_im1 = matches_im0[valid_matches], matches_im1[valid_matches]&#xA;&#xA;    # visualize a few matches&#xA;    import numpy as np&#xA;    import torch&#xA;    import torchvision.transforms.functional&#xA;    from matplotlib import pyplot as pl&#xA;&#xA;    n_viz = 20&#xA;    num_matches = matches_im0.shape[0]&#xA;    match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)&#xA;    viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]&#xA;&#xA;    image_mean = torch.as_tensor([0.5, 0.5, 0.5], device=&#39;cpu&#39;).reshape(1, 3, 1, 1)&#xA;    image_std = torch.as_tensor([0.5, 0.5, 0.5], device=&#39;cpu&#39;).reshape(1, 3, 1, 1)&#xA;&#xA;    viz_imgs = []&#xA;    for i, view in enumerate([view1, view2]):&#xA;        rgb_tensor = view[&#39;img&#39;] * image_std + image_mean&#xA;        viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())&#xA;&#xA;    H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]&#xA;    img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), &#39;constant&#39;, constant_values=0)&#xA;    img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), &#39;constant&#39;, constant_values=0)&#xA;    img = np.concatenate((img0, img1), axis=1)&#xA;    pl.figure()&#xA;    pl.imshow(img)&#xA;    cmap = pl.get_cmap(&#39;jet&#39;)&#xA;    for i in range(n_viz):&#xA;        (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T&#xA;        pl.plot([x0, x1 + W0], [y0, y1], &#39;-+&#39;, color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)&#xA;    pl.show(block=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naver/mast3r/main/assets/matching.jpg&#34; alt=&#34;matching example on croco pair&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;In this section, we present a short demonstration to get started with training MASt3R.&lt;/p&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/naver/dust3r?tab=readme-ov-file#datasets&#34;&gt;Datasets section in DUSt3R&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;Like for the DUSt3R training demo, we&#39;re going to download and prepare the same subset of &lt;a href=&#34;https://github.com/facebookresearch/co3d&#34;&gt;CO3Dv2&lt;/a&gt; - &lt;a href=&#34;https://github.com/facebookresearch/co3d/raw/main/LICENSE&#34;&gt;Creative Commons Attribution-NonCommercial 4.0 International&lt;/a&gt; and launch the training code on it. It is the exact same process as DUSt3R. The demo model will be trained for a few epochs on a very small dataset. It will not be very good.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download and prepare the co3d subset&#xA;mkdir -p data/co3d_subset&#xA;cd data/co3d_subset&#xA;git clone https://github.com/facebookresearch/co3d&#xA;cd co3d&#xA;python3 ./co3d/download_dataset.py --download_folder ../ --single_sequence_subset&#xA;rm ../*.zip&#xA;cd ../../..&#xA;&#xA;python3 datasets_preprocess/preprocess_co3d.py --co3d_dir data/co3d_subset --output_dir data/co3d_subset_processed  --single_sequence_subset&#xA;&#xA;# download the pretrained dust3r checkpoint&#xA;mkdir -p checkpoints/&#xA;wget https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth -P checkpoints/&#xA;&#xA;# for this example we&#39;ll do fewer epochs, for the actual hyperparameters we used in the paper, see the next section: &#34;Our Hyperparameters&#34;&#xA;torchrun --nproc_per_node=4 train.py \&#xA;    --train_dataset &#34;1000 @ Co3d(split=&#39;train&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, aug_crop=&#39;auto&#39;, aug_monocular=0.005, aug_rot90=&#39;diff&#39;, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], n_corres=8192, nneg=0.5, transform=ColorJitter)&#34; \&#xA;    --test_dataset &#34;100 @ Co3d(split=&#39;test&#39;, ROOT=&#39;data/co3d_subset_processed&#39;, resolution=(512,384), n_corres=1024, seed=777)&#34; \&#xA;    --model &#34;AsymmetricMASt3R(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;catmlp+dpt&#39;, output_mode=&#39;pts3d+desc24&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, two_confs=True)&#34; \&#xA;    --train_criterion &#34;ConfLoss(Regr3D(L21, norm_mode=&#39;?avg_dis&#39;), alpha=0.2) + 0.075*ConfMatchingLoss(MatchingLoss(InfoNCE(mode=&#39;proper&#39;, temperature=0.05), negatives_padding=0, blocksize=8192), alpha=10.0, confmode=&#39;mean&#39;)&#34; \&#xA;    --test_criterion &#34;Regr3D_ScaleShiftInv(L21, norm_mode=&#39;?avg_dis&#39;, gt_scale=True, sky_loss_value=0) + -1.*MatchingLoss(APLoss(nq=&#39;torch&#39;, fp=torch.float16), negatives_padding=12288)&#34; \&#xA;    --pretrained &#34;checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&#34; \&#xA;    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 1 --epochs 10 --batch_size 4 --accum_iter 4 \&#xA;    --save_freq 1 --keep_freq 5 --eval_freq 1 --disable_cudnn_benchmark \&#xA;    --output_dir &#34;checkpoints/mast3r_demo&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Our Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We didn&#39;t release all the training datasets, but here are the commands we used for training our models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric - train mast3r with metric regression and matching loss&#xA;# we used cosxl to generate variations of DL3DV: &#34;foggy&#34;, &#34;night&#34;, &#34;rainy&#34;, &#34;snow&#34;, &#34;sunny&#34; but we were not convinced by it.&#xA;&#xA;torchrun --nproc_per_node=8 train.py \&#xA;    --train_dataset &#34;57_000 @ Habitat512(1_000_000, split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 68_400 @ BlendedMVS(split=&#39;train&#39;, mask_sky=True, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 68_400 @ MegaDepth(split=&#39;train&#39;, mask_sky=True, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 45_600 @ ARKitScenes(split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 22_800 @ Co3d(split=&#39;train&#39;, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 22_800 @ StaticThings3D(mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 45_600 @ ScanNetpp(split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 45_600 @ TartanAir(pairs_subset=&#39;&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 4_560 @ UnrealStereo4K(resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 1_140 @ VirtualKitti(optical_center_is_centered=True, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 22_800 @ WildRgbd(split=&#39;train&#39;, mask_bg=&#39;rand&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 145_920 @ NianticMapFree(split=&#39;train&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 57_000 @ DL3DV(split=&#39;nlight&#39;, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 57_000 @ DL3DV(split=&#39;not-nlight&#39;, cosxl_augmentations=None, resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5) + 34_200 @ InternalUnreleasedDataset(resolution=[(512, 384), (512, 336), (512, 288), (512, 256), (512, 160)], aug_crop=&#39;auto&#39;, aug_monocular=0.005, transform=ColorJitter, n_corres=8192, nneg=0.5)&#34; \&#xA;    --test_dataset &#34;Habitat512(1_000, split=&#39;val&#39;, resolution=(512,384), seed=777, n_corres=1024) + 1_000 @ BlendedMVS(split=&#39;val&#39;, resolution=(512,384), mask_sky=True, seed=777, n_corres=1024) + 1_000 @ ARKitScenes(split=&#39;test&#39;, resolution=(512,384), seed=777, n_corres=1024) + 1_000 @ MegaDepth(split=&#39;val&#39;, mask_sky=True, resolution=(512,336), seed=777, n_corres=1024) + 1_000 @ Co3d(split=&#39;test&#39;, resolution=(512,384), mask_bg=&#39;rand&#39;, seed=777, n_corres=1024)&#34; \&#xA;    --model &#34;AsymmetricMASt3R(pos_embed=&#39;RoPE100&#39;, patch_embed_cls=&#39;ManyAR_PatchEmbed&#39;, img_size=(512, 512), head_type=&#39;catmlp+dpt&#39;, output_mode=&#39;pts3d+desc24&#39;, depth_mode=(&#39;exp&#39;, -inf, inf), conf_mode=(&#39;exp&#39;, 1, inf), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, two_confs=True, desc_conf_mode=(&#39;exp&#39;, 0, inf))&#34; \&#xA;    --train_criterion &#34;ConfLoss(Regr3D(L21, norm_mode=&#39;?avg_dis&#39;), alpha=0.2, loss_in_log=False) + 0.075*ConfMatchingLoss(MatchingLoss(InfoNCE(mode=&#39;proper&#39;, temperature=0.05), negatives_padding=0, blocksize=8192), alpha=10.0, confmode=&#39;mean&#39;)&#34; \&#xA;    --test_criterion &#34;Regr3D(L21, norm_mode=&#39;?avg_dis&#39;, gt_scale=True, sky_loss_value=0) + -1.*MatchingLoss(APLoss(nq=&#39;torch&#39;, fp=torch.float16), negatives_padding=12288)&#34; \&#xA;    --pretrained &#34;checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth&#34; \&#xA;    --lr 0.0001 --min_lr 1e-06 --warmup_epochs 8 --epochs 50 --batch_size 4 --accum_iter 2 \&#xA;    --save_freq 1 --keep_freq 5 --eval_freq 1 --print_freq=10 --disable_cudnn_benchmark \&#xA;    --output_dir &#34;checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Visual Localization&lt;/h2&gt; &#xA;&lt;h3&gt;Dataset preparation&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/naver/dust3r/raw/main/dust3r_visloc/README.md#dataset-preparation&#34;&gt;Visloc section in DUSt3R&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Example Commands&lt;/h3&gt; &#xA;&lt;p&gt;With &lt;code&gt;visloc.py&lt;/code&gt; you can run our visual localization experiments on Aachen-Day-Night, InLoc, Cambridge Landmarks and 7 Scenes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Aachen-Day-Night-v1.1:&#xA;# scene in &#39;day&#39; &#39;night&#39;&#xA;# scene can also be &#39;all&#39;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocAachenDayNight(&#39;/path/to/prepared/Aachen-Day-Night-v1.1/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;fire_top50&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/Aachen-Day-Night-v1.1/${scene}/loc&#xA;&#xA;# or with coarse to fine:&#xA;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocAachenDayNight(&#39;/path/to/prepared/Aachen-Day-Night-v1.1/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;fire_top50&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/Aachen-Day-Night-v1.1/${scene}/loc --coarse_to_fine --max_batch_size 48 --c2f_crop_with_homography&#xA;&#xA;# InLoc&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocInLoc(&#39;/path/to/prepared/InLoc/&#39;, pairsfile=&#39;pairs-query-netvlad40-temporal&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/InLoc/loc&#xA;&#xA;# or with coarse to fine:&#xA;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocInLoc(&#39;/path/to/prepared/InLoc/&#39;, pairsfile=&#39;pairs-query-netvlad40-temporal&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/InLoc/loc --coarse_to_fine --max_image_size 1200 --max_batch_size 48 --c2f_crop_with_homography&#xA;&#xA;# 7-scenes:&#xA;# scene in &#39;chess&#39; &#39;fire&#39; &#39;heads&#39; &#39;office&#39; &#39;pumpkin&#39; &#39;redkitchen&#39; &#39;stairs&#39;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocSevenScenes(&#39;/path/to/prepared/7-scenes/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;APGeM-LM18_top20&#39;, topk=1)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/7-scenes/${scene}/loc&#xA;&#xA;# Cambridge Landmarks:&#xA;# scene in &#39;ShopFacade&#39; &#39;GreatCourt&#39; &#39;KingsCollege&#39; &#39;OldHospital&#39; &#39;StMarysChurch&#39;&#xA;python3 visloc.py --model_name MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric --dataset &#34;VislocCambridgeLandmarks(&#39;/path/to/prepared/Cambridge_Landmarks/&#39;, subscene=&#39;${scene}&#39;, pairsfile=&#39;APGeM-LM18_top50&#39;, topk=20)&#34; --pixel_tol 5 --pnp_mode poselib --reprojection_error_diag_ratio 0.008 --output_dir /path/to/output/Cambridge_Landmarks/${scene}/loc&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>PokeAPI/pokeapi</title>
    <updated>2024-10-30T01:39:05Z</updated>
    <id>tag:github.com,2024-10-30:/PokeAPI/pokeapi</id>
    <link href="https://github.com/PokeAPI/pokeapi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Pokémon API&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img height=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/PokeAPI/media/master/logo/pokeapi.svg?sanitize=true&#34; alt=&#34;PokeAPI&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/PokeAPI/pokeapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/circleci/project/github/PokeAPI/pokeapi/master.svg?sanitize=true&#34; alt=&#34;build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PokeAPI/api-data&#34;&gt;&lt;img src=&#34;https://img.shields.io/circleci/build/github/PokeAPI/api-data?label=data&#34; alt=&#34;data status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PokeAPI/deploy&#34;&gt;&lt;img src=&#34;https://img.shields.io/circleci/build/github/PokeAPI/deploy?label=deploy&#34; alt=&#34;deploy status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PokeAPI/pokeapi/raw/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/PokeAPI/pokeapi.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/pokeapi&#34;&gt;&lt;img src=&#34;https://opencollective.com/pokeapi/backers/badge.svg?sanitize=true&#34; alt=&#34;Backers on Open Collective&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/pokeapi&#34;&gt;&lt;img src=&#34;https://opencollective.com/pokeapi/sponsors/badge.svg?sanitize=true&#34; alt=&#34;Sponsors on Open Collective&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;A RESTful API for Pokémon - &lt;a href=&#34;https://pokeapi.co&#34;&gt;pokeapi.co&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Beta GraphQL support is rolling out! Check out the &lt;a href=&#34;https://raw.githubusercontent.com/PokeAPI/pokeapi/master/#graphql--&#34;&gt;GraphQL paragraph&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Setup &amp;nbsp; &lt;a href=&#34;https://www.python.org/download/releases/3.10/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10-blue.svg?sanitize=true&#34; alt=&#34;pyVersion310&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download this source code into a working directory, be sure to use the flag &lt;code&gt;--recurse-submodules&lt;/code&gt; to clone also our submodules.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the requirements using pip:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make install&#xA;# This will install all the required packages and libraries for using PokeAPI&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up the local development environment using the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make setup&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the server on port &lt;code&gt;8000&lt;/code&gt; using the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make serve&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Database setup&lt;/h3&gt; &#xA;&lt;p&gt;To build or rebuild the database by applying any CSV file update, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make build-db&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;http://localhost:8000/api/v2/&#34;&gt;localhost:8000/api/v2/&lt;/a&gt; to see the running API!&lt;/p&gt; &#xA;&lt;p&gt;Each time the &lt;code&gt;build-db&lt;/code&gt; script is run, it will iterate over each table in the database, wipe it, and rewrite each row using the data found in data/v2/csv.&lt;/p&gt; &#xA;&lt;p&gt;If you ever need to wipe the database use this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make wipe-sqlite-db&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the database schema has changed, generate any outstanding migrations and apply them&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make make-migrations&#xA;make migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; to see all tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Docker and Compose &amp;nbsp; &lt;a href=&#34;https://hub.docker.com/r/pokeapi/pokeapi&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/pokeapi/pokeapi?label=tag&amp;amp;sort=semver&#34; alt=&#34;docker hub&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;There is also a multi-container setup, managed by &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose V2&lt;/a&gt;. This setup allows you to deploy a production-like environment, with separate containers for each service, and is recommended if you need to simply spin up PokéAPI.&lt;/p&gt; &#xA;&lt;p&gt;Start everything by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make docker-setup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t have &lt;code&gt;make&lt;/code&gt; on your machine you can use the following commands&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker compose up -d&#xA;docker compose exec -T app python manage.py migrate --settings=config.docker-compose&#xA;docker compose exec -T app sh -c &#39;echo &#34;from data.v2.build import build_all; build_all()&#34; | python manage.py shell --settings=config.docker-compose&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Browse &lt;a href=&#34;http://localhost/api/v2/&#34;&gt;localhost/api/v2/&lt;/a&gt; or &lt;a href=&#34;http://localhost/api/v2/pokemon/bulbasaur/&#34;&gt;localhost/api/v2/pokemon/bulbasaur/&lt;/a&gt; on port &lt;code&gt;80&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To rebuild the database and apply any CSV file updates, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make docker-build-db&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the database schema has changed, generate the migrations and apply those&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make docker-make-migrations&#xA;make docker-migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GraphQL &amp;nbsp; &lt;a href=&#34;ttps://github.com/hasura/graphql-engine&#34;&gt;&lt;img height=&#34;29px&#34; src=&#34;https://graphql-engine-cdn.hasura.io/img/powered_by_hasura_blue.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;When you start PokéAPI with the above Docker Compose setup, an &lt;a href=&#34;https://github.com/hasura/graphql-engine&#34;&gt;Hasura Engine&lt;/a&gt; server is started as well. It&#39;s possible to track all the PokeAPI tables and foreign keys by simply&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# hasura cli needs to be installed and available in your $PATH: https://hasura.io/docs/latest/graphql/core/hasura-cli/install-hasura-cli.html&#xA;# hasura cli&#39;s version has to greater than v2.0.8&#xA;make hasura-apply&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When finished browse &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; and you will find the admin console. The GraphQL endpoint will be hosted at &lt;a href=&#34;http://localhost:8080/v1/graphql&#34;&gt;http://localhost:8080/v1/graphql&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A free public GraphiQL console is browsable at the address &lt;a href=&#34;https://beta.pokeapi.co/graphql/console/&#34;&gt;https://beta.pokeapi.co/graphql/console/&lt;/a&gt;. The relative GraphQL endpoint is accessible at &lt;a href=&#34;https://beta.pokeapi.co/graphql/v1beta&#34;&gt;https://beta.pokeapi.co/graphql/v1beta&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A set of examples is provided in the directory &lt;a href=&#34;https://raw.githubusercontent.com/PokeAPI/pokeapi/master/graphql/examples&#34;&gt;/graphql/examples&lt;/a&gt; of this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes &amp;nbsp; &lt;a href=&#34;https://github.com/PokeAPI/pokeapi/actions/workflows/docker-k8s.yml&#34;&gt;&lt;img src=&#34;https://github.com/PokeAPI/pokeapi/actions/workflows/docker-k8s.yml/badge.svg?sanitize=true&#34; alt=&#34;Build Docker image and create k8s with it&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/&#34;&gt;Kustomize&lt;/a&gt; files are provided in the folder &lt;a href=&#34;https://github.com/PokeAPI/pokeapi/tree/master/Resources/k8s/kustomize/base/&#34;&gt;https://github.com/PokeAPI/pokeapi/tree/master/Resources/k8s/kustomize/base/&lt;/a&gt;. Create and change your secrets:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cp Resources/k8s/kustomize/base/secrets/postgres.env.sample Resources/k8s/kustomize/base/secrets/postgres.env&#xA;cp Resources/k8s/kustomize/base/secrets/graphql.env.sample Resources/k8s/kustomize/base/secrets/graphql.env&#xA;cp Resources/k8s/kustomize/base/config/pokeapi.env.sample Resources/k8s/kustomize/base/config/pokeapi.env&#xA;# Edit the newly created files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Configure &lt;code&gt;kubectl&lt;/code&gt; to point to a cluster and then run the following commands to start a PokéAPI service.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl apply -k Resources/k8s/kustomize/base/&#xA;kubectl config set-context --current --namespace pokeapi # (Optional) Set pokeapi ns as the working ns&#xA;# Wait for the cluster to spin up&#xA;kubectl exec --namespace pokeapi deployment/pokeapi -- python manage.py migrate --settings=config.docker-compose # Migrate the DB&#xA;kubectl exec --namespace pokeapi deployment/pokeapi -- sh -c &#39;echo &#34;from data.v2.build import build_all; build_all()&#34; | python manage.py shell --settings=config.docker-compose&#39; # Build the db&#xA;kubectl wait --namespace pokeapi --timeout=120s --for=condition=complete job/load-graphql # Wait for Graphql configuration job to finish&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This k8s setup creates all k8s resources inside the &lt;em&gt;Namespace&lt;/em&gt; &lt;code&gt;pokeapi&lt;/code&gt;, run &lt;code&gt;kubectl delete namespace pokeapi&lt;/code&gt; to delete them. It also creates a &lt;em&gt;Service&lt;/em&gt; of type &lt;code&gt;LoadBalancer&lt;/code&gt; which is exposed on port &lt;code&gt;80&lt;/code&gt; and &lt;code&gt;443&lt;/code&gt;. Data is persisted on &lt;code&gt;12Gi&lt;/code&gt; of &lt;code&gt;ReadWriteOnce&lt;/code&gt; volumes.&lt;/p&gt; &#xA;&lt;h2&gt;Wrappers&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Official wrapper&lt;/th&gt; &#xA;   &lt;th&gt;Repository&lt;/th&gt; &#xA;   &lt;th&gt;Features&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Node server-side&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PokeAPI/pokedex-promise-v2&#34;&gt;PokeAPI/pokedex-promise-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;Auto caching&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Browser client-side&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PokeAPI/pokeapi-js-wrapper&#34;&gt;PokeAPI/pokeapi-js-wrapper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;Auto caching&lt;/em&gt;, &lt;em&gt;Image caching&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Java/Kotlin&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PokeAPI/pokekotlin&#34;&gt;PokeAPI/pokekotlin&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 2/3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PokeAPI/pokepy&#34;&gt;PokeAPI/pokepy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;Auto caching&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python 3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PokeAPI/pokebase&#34;&gt;PokeAPI/pokebase&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;Auto caching&lt;/em&gt;, &lt;em&gt;Image caching&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Wrapper&lt;/th&gt; &#xA;   &lt;th&gt;Repository&lt;/th&gt; &#xA;   &lt;th&gt;Features&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;.Net Standard&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mtrdp642/PokeApiNet&#34;&gt;mtrdp642/PokeApiNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dart&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/prathanbomb/pokedart&#34;&gt;prathanbomb/pokedart&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mtslzr/pokeapi-go&#34;&gt;mtslzr/pokeapi-go&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PHP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lmerotta/phpokeapi&#34;&gt;lmerotta/phpokeapi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching, lazy loading&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PowerShell&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Celerium/PokeAPI-PowerShellWrapper&#34;&gt;Celerium/PokeAPI-PowerShellWrapper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/beastmatser/aiopokeapi&#34;&gt;beastmatser/aiopokeapi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching, asynchronous&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ruby&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/rdavid1099/poke-api-v2&#34;&gt;rdavid1099/poke-api-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Rust&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gitlab.com/lunik1/pokerust&#34;&gt;lunik1/pokerust&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scala&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/juliano/pokeapi-scala&#34;&gt;juliano/pokeapi-scala&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spring Boot&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dlfigueira/spring-pokeapi&#34;&gt;dlfigueira/spring-pokeapi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swift&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kinkofer/PokemonAPI&#34;&gt;kinkofer/PokemonAPI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Typescript server-side/client-side&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Gabb-c/pokenode-ts&#34;&gt;Gabb-c/Pokenode-ts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Auto caching&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;Help to keep PokéAPI running! If you&#39;re using PokéAPI as a teaching resource or for a project, consider sending us a donation to help keep the service up. We get 1+ billion requests a month!&lt;/p&gt; &#xA;&lt;p&gt;Thank you to all our backers! &lt;a href=&#34;https://opencollective.com/pokeapi#backer&#34;&gt;Become a backer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/pokeapi#backers&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://opencollective.com/pokeapi/backers.svg?width=890&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Join Us On Slack!&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; Currently no maintainer has enough free time to support the community on Slack. Our Slack is in an unmaintained status.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Have a question or just want to discuss new ideas and improvements? Hit us up on Slack. &lt;del&gt;Consider talking with us here before creating a new issue.&lt;/del&gt; This way we can keep issues here a bit more organized and helpful in the long run. Be excellent to each other &lt;span&gt;😄&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://join.slack.com/t/pokeapi/shared_invite/zt-2ampo6her-_tHSI3uOS65WzGypt7Y96w&#34;&gt;Sign up&lt;/a&gt; easily!&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve signed up visit &lt;a href=&#34;https://pokeapi.slack.com&#34;&gt;PokéAPI on Slack&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project exists thanks to all the people who &lt;a href=&#34;https://github.com/PokeAPI/pokeapi/raw/master/CONTRIBUTING.md&#34;&gt;contribute&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PokeAPI/pokeapi/master/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/pokeapi/contributors.svg?width=890&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All contributions are welcome: bug fixes, data contributions, and recommendations.&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://github.com/PokeAPI/pokeapi/issues&#34;&gt;issues on GitHub&lt;/a&gt; before you submit a pull request or raise an issue, someone else might have beat you to it.&lt;/p&gt; &#xA;&lt;p&gt;To contribute to this repository:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://help.github.com/articles/fork-a-repo/&#34;&gt;Fork the project to your own GitHub profile&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the forked project using git clone:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone --recurse-submodules git@github.com:&amp;lt;YOUR_USERNAME&amp;gt;/pokeapi.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a new branch with a descriptive name:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git checkout -b my_new_branch&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Write some code, fix something, and add a test to prove that it works. &lt;em&gt;No pull request will be accepted without tests passing, or without new tests if new features are added.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Commit your code and push it to GitHub&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://help.github.com/articles/creating-a-pull-request/&#34;&gt;Open a new pull request&lt;/a&gt; and describe the changes you have made.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We&#39;ll accept your changes after review.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Simple!&lt;/p&gt; &#xA;&lt;h2&gt;Deprecation&lt;/h2&gt; &#xA;&lt;p&gt;As of October 2018, the v1 API has been removed from PokéAPI. For more information, see &lt;a href=&#34;https://pokeapi.co/docs/v1.html&#34;&gt;pokeapi.co/docs/v1.html&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>