<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-19T01:42:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FaceMe-SDK/FaceLivenessDetection-ServerSDK</title>
    <updated>2023-01-19T01:42:51Z</updated>
    <id>tag:github.com,2023-01-19:/FaceMe-SDK/FaceLivenessDetection-ServerSDK</id>
    <link href="https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK" rel="alternate"></link>
    <summary type="html">&lt;p&gt;iBeta (Level 2) Certified, Single-Image Based Face Liveness Detection (Face Anti Spoofing) Server SDK&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;640&#34; src=&#34;https://user-images.githubusercontent.com/122285115/211849833-8f007605-c9f8-4f60-9b95-5ad606920d61.jpg&#34;&gt; &lt;/p&gt;&#xA;&lt;h4 align=&#34;center&#34;&gt;A 100% spoofing-prevention rate for both 3D printed and resin facial masks, confirms FaceMe® as a leading facial recognition solution for preventing biometric fraud in remote applications, such as online banking, requiring identity verification before granting access to sensitive data or valuable assets.&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://www.ibeta.com/wp-content/uploads/2022/03/220304-CyberLink-PAD-Level-2-Confirmation-Letter.pdf&#34;&gt;FaceMe® iBeta Presentation Attack Detection Report&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://www.ibeta.com/wp-content/uploads/2022/03/220304-CyberLink-PAD-Level-2-Confirmation-Letter.pdf&#34;&gt; &lt;p&gt;&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;p&gt;SDK is fully on-premise, processing all happens on hosting server and no data leaves server.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &lt;h2&gt;Project Structure&lt;/h2&gt; &lt;pre&gt;&lt;code class=&#34;language-graphql&#34;&gt;# Code &amp;amp; components for pages&#xA;./FaceLivenessDetection-ServerSDK&#xA;  ├─ bin/linux_x86_64                 - # Core library files&#xA;  │  ├─ libfaceme_liveness1.so&#xA;  │  ├─ libfm_models.so&#xA;  │  └─ libimutils.so&#xA;  ├─ cpp                              - # C++ example&#xA;  │  ├─ CMakeLists.txt                - # CMake file for build example&#xA;  │  ├─ faceme_liveness.h             - # C++ header file to include library&#xA;  │  └─ main.cpp                      - # C++ example code&#xA;  ├─ flask                            - # Python flask API serving example&#xA;  │  ├─ app.py                        - # Flask example code&#xA;  │  └─ requirements.txt               - # Python requirement list&#xA;  ├─ model                            - # NN dictionary files for library&#xA;  │  ├─ data1.bin&#xA;  │  └─ data2.bin&#xA;  ├─ python                           - # Python example&#xA;  │  ├─ faceme_liveness.py            - # Python library Import Interface file&#xA;  │  ├─ main.py                       - # Python example code&#xA;  │  └─ requirements.txt              - # Python requirement list&#xA;  ├─ test_image                       - # Test Images&#xA;  │  ├─ genuine.jpg&#xA;  │  └─ spoof.png&#xA;  └─ Dockerfile                       - # Docker script for python flask API serving example&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Setup Project&lt;/h2&gt; &lt;h4&gt;- Linux&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Download repo and extract it&lt;/li&gt; &#xA;  &lt;li&gt;Install system dependencies&lt;/li&gt; &#xA; &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get update -y&#xA;sudo apt-get install -y libcurl4-openssl-dev libssl-dev libopencv-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Copy libraries into system folder&lt;/li&gt; &#xA; &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;cp ./bin/linux_x86_64/libfm_models.so /usr/lib&#xA;cp ./bin/linux_x86_64/libimutils.so /usr/lib&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;- Windows&lt;/h4&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://www.ibeta.com/wp-content/uploads/2022/03/220304-CyberLink-PAD-Level-2-Confirmation-Letter.pdf&#34;&gt;Contact us by Email &lt;/a&gt;&lt;a href=&#34;mailto:support@faceme.tw&#34;&gt;support@faceme.tw&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;- Request license&lt;/h4&gt; &#xA;&lt;p&gt;Subscribe free trial at our &lt;a href=&#34;https://sdk.faceme.tw/#contact-us&#34;&gt;Subscription Page&lt;/a&gt;&lt;br&gt; You will get email with trial license key (&#34;XXXXX-XXXXX-XXXXX-XXXXX&#34;).&lt;/p&gt; &#xA;&lt;h2&gt;C++ Example&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Replace license key in main.cpp &lt;a href=&#34;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/raw/eaf9ce81dff32b329d66853461f5d8acb38c5568/cpp/main.cpp#L1-L7&#34;&gt;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/blob/eaf9ce81dff32b329d66853461f5d8acb38c5568/cpp/main.cpp#L1-L7&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd cpp&#xA;mkdir build &amp;amp;&amp;amp; cd build&#xA;cmake ..&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;./example_liveness --image ../../test_image/spoof.png --model ../../model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python Example&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Replace license key in main.py &lt;a href=&#34;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/raw/eaf9ce81dff32b329d66853461f5d8acb38c5568/python/main.py#L11-L17&#34;&gt;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/blob/eaf9ce81dff32b329d66853461f5d8acb38c5568/python/main.py#L11-L17&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd python&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python Flask Example&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Replace license key in app.py &lt;a href=&#34;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/raw/eaf9ce81dff32b329d66853461f5d8acb38c5568/flask/app.py#L19-L25&#34;&gt;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/blob/eaf9ce81dff32b329d66853461f5d8acb38c5568/flask/app.py#L19-L25&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd flask&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;360&#34; src=&#34;https://user-images.githubusercontent.com/122285115/211873670-053fccc6-ffcf-443d-8d6d-6c3e2c161374.png&#34;&gt;   &lt;img width=&#34;360&#34; src=&#34;https://user-images.githubusercontent.com/122285115/211873784-0ba680ca-aad4-4535-bd6c-cefc328afdb3.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Docker Flask Example&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Replace license key in app.py &lt;a href=&#34;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/raw/eaf9ce81dff32b329d66853461f5d8acb38c5568/flask/app.py#L19-L25&#34;&gt;https://github.com/FaceMe-SDK/FaceLivenessDetection-ServerSDK/blob/eaf9ce81dff32b329d66853461f5d8acb38c5568/flask/app.py#L19-L25&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build docker image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build --pull --rm -f &#34;Dockerfile&#34; -t facemeliveness:latest &#34;.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --network host facemeliveness&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Request license&lt;/h2&gt; &#xA;&lt;p&gt;Subscribe free trial at our &lt;a href=&#34;https://sdk.faceme.tw/#contact-us&#34;&gt;Subscription Page&lt;/a&gt;&lt;br&gt; You will get email with trial license key (&#34;XXXXX-XXXXX-XXXXX-XXXXX&#34;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>approximatelabs/sketch</title>
    <updated>2023-01-19T01:42:51Z</updated>
    <id>tag:github.com,2023-01-19:/approximatelabs/sketch</id>
    <link href="https://github.com/approximatelabs/sketch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI code-writing assistant that understands data content&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sketch&lt;/h1&gt; &#xA;&lt;p&gt;Sketch is an AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesn&#39;t require adding a plugin to your IDE.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install sketch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Here we follow a &#34;standard&#34; (hypothetical) data-analysis workflow, showing a Natural Language interace that successfully navigates many tasks in the data stack landscape.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Catalogging: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;General tagging (eg. PII identification)&lt;/li&gt; &#xA;   &lt;li&gt;Metadata generation (names and descriptions)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data Engineering: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Data cleaning and masking (compliance)&lt;/li&gt; &#xA;   &lt;li&gt;Derived feature creation and extraction&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data Analysis: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Data questions&lt;/li&gt; &#xA;   &lt;li&gt;Data visualization&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/916073/212602281-4ebd090f-09c4-495d-b48d-0b4c37b9f665.mp4&#34;&gt;https://user-images.githubusercontent.com/916073/212602281-4ebd090f-09c4-495d-b48d-0b4c37b9f665.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try it out in colab: &lt;a href=&#34;https://colab.research.google.com/gist/bluecoconut/410a979d94613ea2aaf29987cf0233bc/sketch-demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s as simple as importing sketch, and then using the &lt;code&gt;.sketch&lt;/code&gt; extension on any pandas dataframe.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sketch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, any pandas dataframe you have will have an extension registered to it. Access this new extension with your dataframes name &lt;code&gt;.sketch&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;.sketch.ask&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Ask is a basic question-answer system on sketch, this will return an answer in text that is based off of the summary statistics and description of the data.&lt;/p&gt; &#xA;&lt;p&gt;Use ask to get an understanding of the data, get better column names, ask hypotheticals (how would I go about doing X with this data), and more.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.sketch.ask(&#34;Which columns are integer type?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.sketch.howto&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Howto is the basic &#34;code-writing&#34; prompt in sketch. This will return a code-block you should be able to copy paste and use as a starting point (or possibly ending!) for any question you have to ask of the data. Ask this how to clean the data, normalize, create new features, plot, and even build models!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.sketch.howto(&#34;Plot the sales versus time&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;.sketch.apply&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;apply is a more advanced prompt that is more useful for data generation. Use it to parse fields, generate new features, and more. This is built directly on &lt;a href=&#34;https://github.com/approximatelabs/lambdaprompt&#34;&gt;lambdaprompt&lt;/a&gt;. In order to use this, you will need to set up a free account with OpenAI, and set an environment variable with your API key. &lt;code&gt;OPENAI_API_KEY=YOUR_API_KEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;review_keywords&#39;] = df.sketch.apply(&#34;Keywords for the review [{{ review_text }}] of product [{{ product_name }}] (comma separated):&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;capitol&#39;] = pd.DataFrame({&#39;State&#39;: [&#39;Colorado&#39;, &#39;Kansas&#39;, &#39;California&#39;, &#39;New York&#39;]}).sketch.apply(&#34;What is the capitol of [{{ State }}]?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sketch currently uses &lt;code&gt;prompts.approx.dev&lt;/code&gt; to help run with minimal setup&lt;/h2&gt; &#xA;&lt;p&gt;In the future, we plan to update the prompts at this endpoint with our own custom foundation model, built to answer questions more accurately than GPT-3 can with its minimal data context.&lt;/p&gt; &#xA;&lt;p&gt;You can also directly call OpenAI directly (and not use our endpoint) by using your own API key. To do this, set 2 environment variables.&lt;/p&gt; &#xA;&lt;p&gt;(1) &lt;code&gt;SKETCH_USE_REMOTE_LAMBDAPROMPT=False&lt;/code&gt; (2) &lt;code&gt;OPENAI_API_KEY=YOUR_API_KEY&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Sketch uses efficient approximation algorithms (data sketches) to quickly summarize your data, and feed that information into language models. Right now it does this by summarizing the columns and writing these summary statistics as additional context to be used by the code-writing prompt. In the future we hope to feed these sketches directly into custom made &#34;data + language&#34; foundation models to get more accurate results.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>haha-lisa/RDM-Region-Aware-Diffusion-Model</title>
    <updated>2023-01-19T01:42:51Z</updated>
    <id>tag:github.com,2023-01-19:/haha-lisa/RDM-Region-Aware-Diffusion-Model</id>
    <link href="https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Region-Aware Diffusion for Zero-shot Text-driven Image Editing&lt;/h1&gt; &#xA;&lt;p&gt;This is the official PyTorch implementation of the paper &#39;&#39;Region-Aware Diffusion for Zero-shot Text-driven Image Editing&#39;&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model/raw/main/teaser.png&#34; alt=&#34;MAIN3_e2-min&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Image manipulation under the guidance of textual descriptions has recently received a broad range of attention. In this study, we focus on the regional editing of images with the guidance of given text prompts. Different from current mask-based image editing methods, we propose a novel region-aware diffusion model (RDM) for entity-level image editing, which could automatically locate the region of interest and replace it following given text prompts. To strike a balance between image fidelity and inference speed, we design the intensive diffusion pipeline by combing latent space diffusion and enhanced directional guidance. In addition, to preserve image content in non-edited regions, we introduce regional-aware entity editing to modify the region of interest and preserve the out-of-interest region. We validate the proposed RDM beyond the baseline methods through extensive qualitative and quantitative experiments. The results show that RDM outperforms the previous approaches in terms of visual quality, overall harmonization, non-editing region content preservation, and text-image semantic consistency.&lt;/p&gt; &#xA;&lt;h2&gt;Framework&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model/raw/main/pipeline.png&#34; alt=&#34;MAIN3_e2-min&#34;&gt; The overall framework of RDM.&lt;/p&gt; &#xA;&lt;h2&gt;Install dependencies&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model&#xA;cd RDM-Region-Aware-Diffusion-Model&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And install &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;latent diffusion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dall-3.com/models/glid-3-xl/bert.pt&#34;&gt;bert&lt;/a&gt;, &lt;a href=&#34;https://dall-3.com/models/glid-3-xl/kl-f8.pt&#34;&gt;kl-f8&lt;/a&gt;, &lt;a href=&#34;https://dall-3.com/models/glid-3-xl/inpaint.pt&#34;&gt;diffusion&lt;/a&gt; &lt;br&gt; Please download them and put them into the floder ./ &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_edit.py --edit ./input_image/flower1.jpg --region ./input_image/flower1_region.png \&#xA;-fp &#34;a flower&#34; --batch_size 6 --num_batches 2 \&#xA;--text &#34;a chrysanthemum&#34; --prefix &#34;test_flower&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The codes and the pretrained model in this repository are under the MIT license as specified by the LICENSE file.&lt;br&gt;&lt;/p&gt;</summary>
  </entry>
</feed>