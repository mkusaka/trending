<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-27T01:39:16Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jacklevin74/xenminer</title>
    <updated>2023-09-27T01:39:16Z</updated>
    <id>tag:github.com,2023-09-27:/jacklevin74/xenminer</id>
    <link href="https://github.com/jacklevin74/xenminer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Introduction:&lt;/p&gt; &#xA;&lt;p&gt;This proof of work miner is based on Argon2ID algorithm which is both GPU and ASIC resistant. It allows all participants to mine blocks fairly. Your mining speed is directly proportional to the number of miners you are running (you can run many on a single computer). The difficulty of mining is auto adjusted based on the verifier node algorithm which aproximately targets production speed of 1 block per second.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install all the required modules by executing the command below. Make sure you have at least python3 and pip3 installed in order to proceed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To start your miner just execute this command. Note you should adjust account at the top of the file to be your ethereum address if you want to claim your blocks and superblocks later&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 miner.py&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen</title>
    <updated>2023-09-27T01:39:16Z</updated>
    <id>tag:github.com,2023-09-27:/QwenLM/Qwen</id>
    <link href="https://github.com/QwenLM/Qwen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repo of Qwen (é€šä¹‰åƒé—®) chat &amp; pretrained large language model proposed by Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/README_CN.md&#34;&gt;ä¸­æ–‡&lt;/a&gt;&amp;nbsp; ï½œ &amp;amp;nbspEnglish&amp;nbsp; ï½œ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/README_JA.md&#34;&gt;æ—¥æœ¬èª&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ğŸ¤— &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤– &lt;a href=&#34;https://modelscope.cn/models/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“‘ &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf&#34;&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; ï½œ &amp;nbsp;&amp;nbsp;ğŸ–¥ï¸ &lt;a href=&#34;https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary&#34;&gt;Demo&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/wechat.png&#34;&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp;&amp;nbsp; ï½œ &amp;nbsp;&amp;nbsp; DingTalk (é’‰é’‰) &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Qwen-Chat&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Qwen-Chat (Int4)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Qwen&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary&#34;&gt;ğŸ¤–&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B-Chat&#34;&gt;ğŸ¤—&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary&#34;&gt;ğŸ¤–&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B-Chat-Int4&#34;&gt;ğŸ¤—&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-7B/summary&#34;&gt;ğŸ¤–&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B&#34;&gt;ğŸ¤—&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary&#34;&gt;ğŸ¤–&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-14B-Chat&#34;&gt;ğŸ¤—&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary&#34;&gt;ğŸ¤–&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-14B-Chat-Int4&#34;&gt;ğŸ¤—&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-14B/summary&#34;&gt;ğŸ¤–&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-14B&#34;&gt;ğŸ¤—&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We opensource our &lt;strong&gt;Qwen&lt;/strong&gt; series, now including &lt;strong&gt;Qwen&lt;/strong&gt;, the base language models, namely &lt;strong&gt;Qwen-7B&lt;/strong&gt; and &lt;strong&gt;Qwen-14B&lt;/strong&gt;, as well as &lt;strong&gt;Qwen-Chat&lt;/strong&gt;, the chat models, namely &lt;strong&gt;Qwen-7B-Chat&lt;/strong&gt; and &lt;strong&gt;Qwen-14B-Chat&lt;/strong&gt;. Links are on the above table. Click them and check the model cards. Also, we release the &lt;strong&gt;&lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf&#34;&gt;technical report&lt;/a&gt;&lt;/strong&gt;. Please click the paper link and check it out!&lt;/p&gt; &#xA;&lt;p&gt;In brief, we have strong base language models, which have been stably pretrained for up to 3 trillion tokens of multilingual data with a wide coverage of domains, languages (with a focus on Chinese and English), etc. They are able to achieve competitive performance on benchmark datasets. Additionally, we have chat models that are aligned with human preference based on SFT and RLHF (not released yet), which are able to chat, create content, extract information, summarize, translate, code, solve math problems, and so on, and are able to use tools, play as agents, or even play as code interpreters, etc.&lt;/p&gt; &#xA;&lt;p&gt;In this repo, you can figure out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quickstart with Qwen, and enjoy the simple inference.&lt;/li&gt; &#xA; &lt;li&gt;Details about the quantization models, including usage, memory, inference speed. For comparison, we also provide the statistics of the BF16 models.&lt;/li&gt; &#xA; &lt;li&gt;Tutorials on finetuning, including full-parameter tuning, LoRA, and Q-LoRA.&lt;/li&gt; &#xA; &lt;li&gt;Instructions on building demos, including WebUI, CLI demo, etc.&lt;/li&gt; &#xA; &lt;li&gt;Information about Qwen for tool use, agent, and code interpreter&lt;/li&gt; &#xA; &lt;li&gt;Statistics of long-context understanding evaluation&lt;/li&gt; &#xA; &lt;li&gt;License agreement&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, if you meet problems, turn to &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/FAQ.md&#34;&gt;FAQ&lt;/a&gt; for help first. Still feeling struggled? Feel free to shoot us issues (better in English so that more people can understand you)! If you would like to help us, send us pull requests with no hesitation! We are always excited about PR!&lt;/p&gt; &#xA;&lt;p&gt;Would like to chat with us or date us coffee time? Welcome to our Discord or WeChat! &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.9.25 ğŸ”¥ We release &lt;strong&gt;Qwen-14B&lt;/strong&gt; and &lt;strong&gt;Qwen-14B-Chat&lt;/strong&gt; on ModelScope and Hugging Face, along with &lt;a href=&#34;https://github.com/QwenLM/qwen.cpp&#34;&gt;qwen.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt;. Codes and checkpoints of &lt;strong&gt;Qwen-7B&lt;/strong&gt; and &lt;strong&gt;Qwen-7B-Chat&lt;/strong&gt; are also updated. &lt;strong&gt;PLEASE PULL THE LATEST VERSION!&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Compared to &lt;strong&gt;Qwen-7B&lt;/strong&gt; (original), &lt;strong&gt;Qwen-7B&lt;/strong&gt; uses more training tokens, increasing from 2.2T tokens to 2.4T tokens, while the context length extends from 2048 to 8192. The Chinese knowledge and coding ability of &lt;strong&gt;Qwen-7B&lt;/strong&gt; have been further improved.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2023.9.12 We now support finetuning on the Qwen-7B models, including full-parameter finetuning, LoRA and Q-LoRA.&lt;/li&gt; &#xA; &lt;li&gt;2023.8.21 We release the Int4 quantized model for Qwen-7B-Chat, &lt;strong&gt;Qwen-7B-Chat-Int4&lt;/strong&gt;, which requires low memory costs but achieves improved inference speed. Besides, there is no significant performance degradation on the benchmark evaluation.&lt;/li&gt; &#xA; &lt;li&gt;2023.8.3 We release both &lt;strong&gt;Qwen-7B&lt;/strong&gt; and &lt;strong&gt;Qwen-7B-Chat&lt;/strong&gt; on ModelScope and Hugging Face. We also provide a technical memo for more details about the model, including training details and model performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Qwen-14B and Qwen-7B (this is the new version trained with more tokens and the context length is extended from 2048 to 8192) outperform the baseline models of similar model sizes on a series of benchmark datasets, e.g., MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., which evaluate the models&#39; capabilities on natural language understanding, mathematic problem solving, coding, etc. However, even Qwen-14B still significantly fall behind GPT-3.5, let alone GPT-4. See the results below.&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/radar_14b.jpg&#34; width=&#34;600&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MATH&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MBPP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BBH&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CMMLU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-34B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGLM2-6B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;InternLM-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;InternLM-20B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qwen-7B (original)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Qwen-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Qwen-14B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;66.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;72.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;61.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;24.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;32.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;40.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;71.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For all compared models, we report the best scores between their official reported results and &lt;a href=&#34;https://opencompass.org.cn/leaderboard-llm&#34;&gt;OpenCompass&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more experimental results (detailed model performance on more benchmark datasets) and details, please refer to our technical report by clicking &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf&#34;&gt;here&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.8 and above&lt;/li&gt; &#xA; &lt;li&gt;pytorch 1.12 and above, 2.0 and above are recommended&lt;/li&gt; &#xA; &lt;li&gt;transformers 4.32 and above&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Below, we provide simple examples to show how to use Qwen-Chat with ğŸ¤– ModelScope and ğŸ¤— Transformers.&lt;/p&gt; &#xA;&lt;p&gt;Before running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your device supports fp16 or bf16, we recommend installing &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash-attention&lt;/a&gt; for higher efficiency and lower memory usage. (&lt;strong&gt;flash-attention is optional and the project can run normally without installing it&lt;/strong&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention&#xA;cd flash-attention &amp;amp;&amp;amp; pip install .&#xA;# Below are optional. Installing them might be slow.&#xA;# pip install csrc/layer_norm&#xA;# pip install csrc/rotary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can start with ModelScope or Transformers.&lt;/p&gt; &#xA;&lt;h4&gt;ğŸ¤— Transformers&lt;/h4&gt; &#xA;&lt;p&gt;To use Qwen-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. Remember to pass in the correct model names or paths, such as &#34;Qwen/Qwen-7B-Chat&#34; and &#34;Qwen/Qwen-14B-Chat&#34;. However, &lt;strong&gt;please make sure that you are using the latest code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from transformers.generation import GenerationConfig&#xA;&#xA;# Model names: &#34;Qwen/Qwen-7B-Chat&#34;, &#34;Qwen/Qwen-14B-Chat&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, trust_remote_code=True)&#xA;&#xA;# use bf16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, bf16=True).eval()&#xA;# use fp16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;# use cpu only&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;# use auto mode, automatically select precision based on the device.&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;Qwen/Qwen-7B-Chat&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;    trust_remote_code=True&#xA;).eval()&#xA;&#xA;# Specify hyperparameters for generation. But if you use transformers&amp;gt;=4.32.0, there is no need to do this.&#xA;# model.generation_config = GenerationConfig.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, trust_remote_code=True)&#xA;&#xA;# 1st dialogue turn&#xA;response, history = model.chat(tokenizer, &#34;ä½ å¥½&#34;, history=None)&#xA;print(response)&#xA;# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚&#xA;&#xA;# 2nd dialogue turn&#xA;response, history = model.chat(tokenizer, &#34;ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚&#34;, history=history)&#xA;print(response)&#xA;# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚&#xA;# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚&#xA;# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚&#xA;# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚&#xA;# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚&#xA;# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚&#xA;&#xA;# 3rd dialogue turn&#xA;response, history = model.chat(tokenizer, &#34;ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜&#34;, history=history)&#xA;print(response)&#xA;# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Running Qwen pretrained base model is also simple.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Running Qwen&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from transformers.generation import GenerationConfig&#xA;&#xA;# Model names: &#34;Qwen/Qwen-7B&#34;, &#34;Qwen/Qwen-14B&#34; &#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen-7B&#34;, trust_remote_code=True)&#xA;# use bf16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, bf16=True).eval()&#xA;# use fp16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;# use cpu only&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B&#34;, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;# use auto mode, automatically select precision based on the device.&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;Qwen/Qwen-7B&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;    trust_remote_code=True&#xA;).eval()&#xA;&#xA;# Specify hyperparameters for generation. But if you use transformers&amp;gt;=4.32.0, there is no need to do this.&#xA;# model.generation_config = GenerationConfig.from_pretrained(&#34;Qwen/Qwen-7B&#34;, trust_remote_code=True)&#xA;&#xA;inputs = tokenizer(&#39;è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯&#39;, return_tensors=&#39;pt&#39;)&#xA;inputs = inputs.to(model.device)&#xA;pred = model.generate(**inputs)&#xA;print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))&#xA;# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;ğŸ¤– ModelScope&lt;/h4&gt; &#xA;&lt;p&gt;ModelScope is an opensource platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modelscope import AutoModelForCausalLM, AutoTokenizer&#xA;from modelscope import GenerationConfig&#xA;&#xA;# Model names: &#34;qwen/Qwen-7B-Chat&#34;, &#34;qwen/Qwen-14B-Chat&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;qwen/Qwen-7B-Chat&#34;, revision=&#39;v1.0.5&#39;, trust_remote_code=True)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;qwen/Qwen-7B-Chat&#34;, revision=&#39;v1.0.5&#39;, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;model.generation_config = GenerationConfig.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, revision=&#39;v1.0.5&#39;, trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚&#xA;&#xA;response, history = model.chat(tokenizer, &#34;ä½ å¥½&#34;, history=None)&#xA;print(response)&#xA;response, history = model.chat(tokenizer, &#34;æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ&#34;, history=history) &#xA;print(response)&#xA;response, history = model.chat(tokenizer, &#34;å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹&#34;, history=history)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;We provide a solution based on &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ&#34;&gt;AutoGPTQ&lt;/a&gt;, and release an Int4 quantized model for Qwen-7B-Chat &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B-Chat-Int4&#34;&gt;Click here&lt;/a&gt; and Qwen-14B-Chat &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-14B-Chat-Int4&#34;&gt;Click here&lt;/a&gt;, which achieve nearly lossless model effects but improved performance on both memory costs and inference speed.&lt;/p&gt; &#xA;&lt;p&gt;Here we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install auto-gptq optimum&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you meet problems installing &lt;code&gt;auto-gptq&lt;/code&gt;, we advise you to check out the official &lt;a href=&#34;https://github.com/PanQiWei/AutoGPTQ&#34;&gt;repo&lt;/a&gt; to find a wheel.&lt;/p&gt; &#xA;&lt;p&gt;Then you can load the quantized model easily and run inference as same as usual:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Model names: &#34;Qwen/Qwen-7B-Chat-Int4&#34;, &#34;Qwen/Qwen-14B-Chat-Int4&#34;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;Qwen/Qwen-7B-Chat-Int4&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;    trust_remote_code=True&#xA;).eval()&#xA;response, history = model.chat(tokenizer, &#34;Hi&#34;, history=None)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;p&gt;We illustrate the model performance of both BF16 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Quantization&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CEval (val)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Humaneval&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat (BF16)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat (Int4)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat (BF16)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat (Int4)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference Speed&lt;/h3&gt; &#xA;&lt;p&gt;We measured the average inference speed (tokens/s) of generating 2048 and 8192 tokens under BF16 precision and Int4 quantization, respectively.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Quantization&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speed (2048 tokens)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speed (8192 tokens)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat (BF16)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.34&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.32&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat (Int4)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat (BF16)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.70&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat (Int4)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;In detail, the setting of profiling is generating 8192 new tokens with 1 context token. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4. The inference speed is averaged over the generated 8192 tokens.&lt;/p&gt; &#xA;&lt;h3&gt;GPU Memory Usage&lt;/h3&gt; &#xA;&lt;p&gt;We also profile the peak GPU memory usage for encoding 2048 tokens as context (and generating single token) and generating 8192 tokens (with single token as context) under BF16 or Int4 quantization level, respectively. The results are shown below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Quantization&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Peak Usage for Encoding 2048 Tokens&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Peak Usage for Generating 8192 Tokens&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat (BF16)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.66GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.58GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat (Int4)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.21GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.62GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat (BF16)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.15GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.94GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat (Int4)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.00GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.79GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The above speed and memory profiling are conducted using &lt;a href=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py&#34;&gt;this script&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quantization of KV cache&lt;/h2&gt; &#xA;&lt;p&gt;Attention KV cache can be quantized and compressed for storage, to get a higher sample throughput.&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;The parameters of &#39;use_cache_quantization&#39; and &#39;use_cache_kernel&#39; are provided to control kv-cache-quantization behavior When use_cache_quantization=True and use_cache_kernel=True, kv-cache-quantization will be enabled. The specific use method is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;Qwen/Qwen-7B-Chat&#34;,&#xA;     device_map=&#34;auto&#34;,&#xA;     trust_remote_code=True,&#xA;     use_cache_quantization=True,&#xA;     use_cache_kernel=True,&#xA;     use_flash_attn=False&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Attention: Currently, kv-cache-quantization and flash attn cannot be turned on at the same time. If you enable kv cache quantization and use_flash_attn at the same time (use_flash_attn=True, use_cache_quantization=True, use_cache_kernel=True), use_flash_attn is disabled by default(use_flash_attn=false).&lt;/p&gt; &#xA;&lt;h3&gt;Comparative Results&lt;/h3&gt; &#xA;&lt;h4&gt;Results&lt;/h4&gt; &#xA;&lt;p&gt;We have verified that the use of the quantized int8-kvcache model does not suffer from significant performance degradation.&lt;/p&gt; &#xA;&lt;h4&gt;memory usage comparison&lt;/h4&gt; &#xA;&lt;p&gt;The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4. We use BF16 models, and generate 1024 tokens (seq-length=1024) by default, and oom indicates out of memory.&lt;/p&gt; &#xA;&lt;p&gt;With kv-cache quantization turned on, we can run a larger batch size(bs).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;USE KVCache&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bs=1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bs=4&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bs=16&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bs=32&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bs=64&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bs=100&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.3GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.1GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.7GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.7GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;oom&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;oom&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.5GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.2GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.3GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.2GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.2GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.4GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;With kv-cache quantization turned on, the model can save more memory when generate longer seq-length (sl, number of tokens generated) at infer.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;USE KVCache&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;sl=512&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;sl=1024&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;sl=2048&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;sl=4096&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;sl=8192&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.2GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.3GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.6GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.5GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.2GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.5GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.8GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.6GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.6GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Difference of Storage in layer-past&lt;/h3&gt; &#xA;&lt;p&gt;The model which turn on the kv-cache quantization will convert the format of layer-past from float to int8, meanwhile the quantianted layer-past will also store quantiantion parameters of current value. Specific steps are as follows: 1ã€Quantize key/value&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    qv,scale,zero_point=quantize_cache_v(v)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;2ã€Store into layer_past&lt;/p&gt; &#xA;&lt;p&gt;Following is the format of quantized layer_past:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    layer_past=((q_key,key_scale,key_zero_point),&#xA;                (q_value,value_scale,value_zero_point))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Bascial format of layer_past:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    layer_past=(key,value)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use the attention KV which is quantized, you can use the dequantization operation to convert the int8 key/value back to the float format as following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    v=dequantize_cache_torch(qv,scale,zero_point)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;Now we provide the official training script, &lt;code&gt;finetune.py&lt;/code&gt;, for users to finetune the pretrained model for downstream applications in a simple fashion. Additionally, we provide shell scripts to launch finetuning with no worries. This script supports the training with &lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;DeepSpeed&lt;/a&gt; and &lt;a href=&#34;https://engineering.fb.com/2021/07/15/open-source/fsdp/&#34;&gt;FSDP&lt;/a&gt;. The shell scripts that we provide use DeepSpeed (Note: this may have conflicts with the latest version of pydantic) and Peft. You can install them by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install peft deepspeed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To prepare your training data, you need to put all the samples into a list and save it to a json file. Each sample is a dictionary consisting of an id and a list for conversation. Below is a simple example list with 1 sample:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;[&#xA;  {&#xA;    &#34;id&#34;: &#34;identity_0&#34;,&#xA;    &#34;conversations&#34;: [&#xA;      {&#xA;        &#34;from&#34;: &#34;user&#34;,&#xA;        &#34;value&#34;: &#34;ä½ å¥½&#34;,&#xA;      },&#xA;      {&#xA;        &#34;from&#34;: &#34;assistant&#34;,&#xA;        &#34;value&#34;: &#34;æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚&#34;&#xA;      }&#xA;    ]&#xA;  }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After data preparation, you can use the provided shell scripts to run finetuning. Remember to specify the path to the data file, &lt;code&gt;$DATA&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The finetuning scripts allow you to perform:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full-parameter finetuning&lt;/li&gt; &#xA; &lt;li&gt;LoRA&lt;/li&gt; &#xA; &lt;li&gt;Q-LoRA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Full-parameter parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.&#xA;sh finetune/finetune_ds.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument &lt;code&gt;--deepspeed&lt;/code&gt; or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use &lt;code&gt;--bf16 True&lt;/code&gt; or &lt;code&gt;--fp16 True&lt;/code&gt;. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.&lt;/p&gt; &#xA;&lt;p&gt;Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed &lt;code&gt;peft&lt;/code&gt;. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Single GPU training&#xA;sh finetune/finetune_lora_single_gpu.sh&#xA;# Distributed training&#xA;sh finetune/finetune_lora_ds.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In comparison with full-parameter finetuning, LoRA (&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;paper&lt;/a&gt;) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs. However, if you still suffer from insufficient memory, you can consider Q-LoRA (&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;paper&lt;/a&gt;), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs. To run Q-LoRA, directly run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Single GPU training&#xA;sh finetune/finetune_qlora_single_gpu.sh&#xA;# Distributed training&#xA;sh finetune/finetune_qlora_ds.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. However, different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA.&lt;/p&gt; &#xA;&lt;p&gt;Different from full-parameter finetuning, the training of both LoRA and Q-LoRA only saves the adapter parameters. Suppose your training starts from Qwen-7B, you can load the finetuned model for inference as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from peft import AutoPeftModelForCausalLM&#xA;&#xA;model = AutoPeftModelForCausalLM.from_pretrained(&#xA;    path_to_adapter, # path to the output directory&#xA;    device_map=&#34;auto&#34;,&#xA;    trust_remote_code=True&#xA;).eval()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The shell scripts uses &lt;code&gt;torchrun&lt;/code&gt; to run single-GPU or multi-GPU training. For multi-GPU training, you need to specify the proper hyperparameters for distributed training based on your machine. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;We provide code for users to build a web UI demo (thanks to @wysaid). Before you start, make sure you install the following packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements_web_demo.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the command below and click on the generated link:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/web_demo.gif&#34; width=&#34;600&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;CLI Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide a CLI demo example in &lt;code&gt;cli_demo.py&lt;/code&gt;, which supports streaming output for the generation. Users can interact with Qwen-7B-Chat by inputting prompts, and the model returns model outputs in the streaming mode. Run the command below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/cli_demo.gif&#34; width=&#34;600&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;We provide methods to deploy local API based on OpenAI API (thanks to @hanpenggit). Before you start, install the required packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install fastapi uvicorn openai &#34;pydantic&amp;gt;=2.3.0&#34; sse_starlette&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the command to deploy your API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python openai_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can change your arguments, e.g., &lt;code&gt;-c&lt;/code&gt; for checkpoint name or path, &lt;code&gt;--cpu-only&lt;/code&gt; for CPU deployment, etc. If you meet problems launching your API deployment, updating the packages to the latest version can probably solve them.&lt;/p&gt; &#xA;&lt;p&gt;Using the API is also simple. See the example below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_base = &#34;http://localhost:8000/v1&#34;&#xA;openai.api_key = &#34;none&#34;&#xA;&#xA;# create a request activating streaming response&#xA;for chunk in openai.ChatCompletion.create(&#xA;    model=&#34;Qwen&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;ä½ å¥½&#34;}&#xA;    ],&#xA;    stream=True &#xA;    # Specifying stop words in streaming output format is not yet supported and is under development.&#xA;):&#xA;    if hasattr(chunk.choices[0].delta, &#34;content&#34;):&#xA;        print(chunk.choices[0].delta.content, end=&#34;&#34;, flush=True)&#xA;&#xA;# create a request not activating streaming response&#xA;response = openai.ChatCompletion.create(&#xA;    model=&#34;Qwen&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;ä½ å¥½&#34;}&#xA;    ],&#xA;    stream=False,&#xA;    stop=[] # You can add custom stop words here, e.g., stop=[&#34;Observation:&#34;] for ReAct prompting.&#xA;)&#xA;print(response.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/openai_api.gif&#34; width=&#34;600&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Function calling is also supported (but only when &lt;code&gt;stream=False&lt;/code&gt; for the moment). See the &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/function_call_examples.py&#34;&gt;example usage&lt;/a&gt; here. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;It is simple to run the model on CPU, which requires your specification of device:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-7B-Chat&#34;, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you suffer from lack of GPU memory and you would like to run the model on more than 1 GPU, you can use our provided script &lt;code&gt;utils.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from utils import load_model_on_gpus&#xA;model = load_model_on_gpus(&#39;Qwen/Qwen-7B-Chat&#39;, num_gpus=2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the 7B chat model on 2 GPUs using the above scripts. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also provide pure C++ implementation of Qwen-LM and tiktoken, see &lt;a href=&#34;https://github.com/QwenLM/qwen.cpp&#34;&gt;qwen.cpp&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Tool Usage&lt;/h2&gt; &#xA;&lt;p&gt;Qwen-Chat has been optimized for tool usage and function calling capabilities. Users can develop agents, LangChain applications, and even agument Qwen with a Python Code Interpreter.&lt;/p&gt; &#xA;&lt;p&gt;We provide documentation on how to implement tool calls based on the principle of ReAct Prompting, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/react_prompt.md&#34;&gt;the ReAct example&lt;/a&gt;. Based on this principle, we provide support for function calling in &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/openai_api.py&#34;&gt;openai_api.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have tested the model&#39;s tool calling capabilities on our open-source Chinese evaluation benchmark and found that Qwen-Chat consistently performs well:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;4&#34; align=&#34;center&#34;&gt;Chinese Tool-Use Benchmark&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Tool Selection (Acc.â†‘)&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Tool Input (Rouge-Lâ†‘)&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;False Positive Errorâ†“&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;95%&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;0.90&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;15.0%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;85%&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;0.88&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;75.0%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;98%&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;0.91&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;7.3%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;98%&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;0.93&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;2.4%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;To assess Qwen&#39;s ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We have observed that Qwen performs well in terms of code executability and result accuracy when generating code:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;4&#34; align=&#34;center&#34;&gt;Executable Rate of Generated Code (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Mathâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Visualizationâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Generalâ†‘&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;91.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;82.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;89.2&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;65.0&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;74.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2-7B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.1 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2-13B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.3 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeLLaMA-7B-Instruct&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.7 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeLLaMA-13B-Instruct&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.1 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM-7B-Chat-v1.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.1 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM-20B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.5 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.2 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;4&#34; align=&#34;center&#34;&gt;Accuracy of Code Execution Results (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Mathâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Visualization-Hardâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Visualization-Easyâ†‘&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;82.8&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;66.7&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;60.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;47.3&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;33.3&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;55.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2-7B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.2 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2-13B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeLLaMA-7B-Instruct&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.8 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeLLaMA-13B-Instruct&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.0 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM-7B-Chat-v1.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM-20B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.6 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.4 &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/code_interpreter_showcase_001.jpg&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;In addition, we also provide experimental results demonstrating that our model is capable of acting as a HuggingFace Agent. For more information, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/examples/transformers_agent.md&#34;&gt;example documentation&lt;/a&gt;. The model&#39;s performance on the evaluation dataset provided by Hugging Face is as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;4&#34; align=&#34;center&#34;&gt;HuggingFace Agent Benchmark- Run Mode&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Tool Selectionâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Tool Usedâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Codeâ†‘&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;95.4&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;96.3&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder-Base-15B&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;86.1&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;68.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder-15B&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;88.0&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;68.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;71.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;93.5&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;94.4&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;4&#34; align=&#34;center&#34;&gt;HuggingFace Agent Benchmark - Chat Mode&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Tool Selectionâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Tool Usedâ†‘&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;Codeâ†‘&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;98.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.3&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;96.8&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;89.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder-Base-15B&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;91.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder-15B&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;89.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B-Chat&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;94.7&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;94.7&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;85.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B-Chat&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;97.9&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;95.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Long-Context Understanding&lt;/h2&gt; &#xA;&lt;p&gt;To extend the context length and break the bottleneck of training sequence length, we introduce several techniques, including NTK-aware interpolation, window attention, and LogN attention scaling, to extend the context length of Qwen-7B/14B from 2k to over 8K tokens, and Qwen-7B from 8k to 32k tokens. We conduct language modeling experiments on the arXiv dataset with the PPL evaluation and find that Qwen can reach outstanding performance in the scenario of long context. Results are demonstrated below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt;&#xA;   &lt;th colspan=&#34;6&#34; align=&#34;center&#34;&gt;Sequence Length&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1024&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;2048&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;4096&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;8192&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;16384&lt;/th&gt;&#xA;   &lt;th align=&#34;center&#34;&gt;32768&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B (original)&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;4.23&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.78&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;39.35&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;469.81&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;2645.09&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;4.23&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.78&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.59&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.66&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;5.71&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk + logn&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;4.23&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.78&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.58&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.56&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;4.62&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;4.23&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.78&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.58&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.49&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;4.32&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-7B&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.81&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.52&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.31&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;7.27&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;181.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;4.23&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.81&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.52&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.33&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.22&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.17&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-14B&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;-&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.46&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;22.79&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;334.65&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3168.35&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;+ dynamic_ntk + logn + window_attn&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;-&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.46&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.29&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;&lt;b&gt;3.18&lt;/b&gt;&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;3.42&lt;/td&gt;&#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tokenizer&lt;/h2&gt; &#xA;&lt;p&gt;Our tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/tokenization_note.md&#34;&gt;documentation&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Reproduction&lt;/h2&gt; &#xA;&lt;p&gt;For your reproduction of the model performance on benchmark datasets, we provide scripts for you to reproduce the results. Check &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/eval/EVALUATION.md&#34;&gt;eval/EVALUATION.md&lt;/a&gt; for more information. Note that the reproduction may lead to slight differences from our reported results. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;If you meet problems, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/FAQ.md&#34;&gt;FAQ&lt;/a&gt; and the issues first to search a solution before you launch a new issue. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;Researchers and developers are free to use the codes and model weights of both Qwen and Qwen-Chat. We also allow their commercial use. Check our license at &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more details. If you have requirements for commercial use, please fill out the form (&lt;a href=&#34;https://dashscope.console.aliyun.com/openModelApply/qianwen&#34;&gt;7B&lt;/a&gt;, &lt;a href=&#34;https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat&#34;&gt;14B&lt;/a&gt;) to apply. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to &lt;a href=&#34;mailto:qianwen_opensource@alibabacloud.com&#34;&gt;qianwen_opensource@alibabacloud.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>liucongg/ChatGLM-Finetuning</title>
    <updated>2023-09-27T01:39:16Z</updated>
    <id>tag:github.com,2023-09-27:/liucongg/ChatGLM-Finetuning</id>
    <link href="https://github.com/liucongg/ChatGLM-Finetuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;åŸºäºChatGLM-6Bã€ChatGLM2-6Bæ¨¡å‹ï¼Œè¿›è¡Œä¸‹æ¸¸å…·ä½“ä»»åŠ¡å¾®è°ƒï¼Œæ¶‰åŠFreezeã€Loraã€P-tuningã€å…¨å‚å¾®è°ƒç­‰&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;ChatGLMå¾®è°ƒ&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä¸»è¦é’ˆå¯¹ChatGLMå’ŒChatGLM2æ¨¡å‹è¿›è¡Œä¸åŒæ–¹å¼çš„å¾®è°ƒï¼ˆFreezeæ–¹æ³•ã€Loraæ–¹æ³•ã€P-Tuningæ–¹æ³•ã€å…¨é‡å‚æ•°ç­‰ï¼‰ï¼Œå¹¶å¯¹æ¯”å¤§æ¨¡å‹åœ¨ä¸åŒå¾®è°ƒæ–¹æ³•ä¸Šçš„æ•ˆæœï¼Œä¸»è¦é’ˆå¯¹ä¿¡æ¯æŠ½å–ä»»åŠ¡ã€ç”Ÿæˆä»»åŠ¡ã€åˆ†ç±»ä»»åŠ¡ç­‰ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æ”¯æŒå•å¡è®­ç»ƒ&amp;amp;å¤šå¡è®­ç»ƒï¼Œç”±äºé‡‡ç”¨å•æŒ‡ä»¤é›†æ–¹å¼å¾®è°ƒï¼Œæ¨¡å‹å¾®è°ƒä¹‹å&lt;strong&gt;å¹¶æ²¡æœ‰å‡ºç°ä¸¥é‡çš„ç¾éš¾æ€§é—å¿˜&lt;/strong&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ç”±äºå®˜æ–¹ä»£ç å’Œæ¨¡å‹ä¸€ç›´åœ¨æ›´æ–°ï¼Œç›®å‰ä»£ç å’Œæ¨¡å‹ä½¿ç”¨çš„æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼ˆ20230806ï¼‰ã€‚&lt;/p&gt; &#xA;&lt;p&gt;PSï¼šæ²¡æœ‰ç”¨Trainerï¼ˆè™½ç„¶Trainerä»£ç ç®€å•ï¼Œä½†ä¸æ˜“ä¿®æ”¹ï¼Œå¤§æ¨¡å‹æ—¶ä»£ç®—æ³•å·¥ç¨‹å¸ˆæœ¬å°±æˆä¸ºäº†æ•°æ®å·¥ç¨‹å¸ˆï¼Œå› æ­¤æ›´éœ€äº†è§£è®­ç»ƒæµç¨‹ï¼‰&lt;/p&gt; &#xA;&lt;h2&gt;æ›´æ–°ç®€ä»‹&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;update-2023.08.06 ä»£ç å’Œæ¨¡å‹å·²ç»æ›´æ–°åˆ°æœ€æ–°ï¼Œæ”¯æŒå•å¡&amp;amp;å¤šå¡è®­ç»ƒï¼Œæ”¯æŒChatGLM2æ¨¡å‹è®­ç»ƒã€æ”¯æŒå…¨é‡å‚æ•°è®­ç»ƒï¼Œæ‰€æœ‰ä»£ç è¿›è¡Œäº†ç»“æ„å¢åŠ å¯è¯»æ€§ã€‚&lt;/li&gt; &#xA; &lt;li&gt;update-2023.06.12 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/636488690&#34;&gt;&lt;strong&gt;å¢åŠ æµæ°´çº¿å¹¶è¡Œè®­ç»ƒæ–¹æ³•&lt;/strong&gt;&lt;/a&gt;ï¼Œè¯·çœ‹&lt;a href=&#34;https://github.com/liucongg/ChatGLM-Finetuning/tree/v0.1&#34;&gt;v0.1 Tag&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;update-2023.04.18 &lt;strong&gt;å¢åŠ æ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¯„æµ‹&lt;/strong&gt;ï¼Œè¯·çœ‹&lt;a href=&#34;https://github.com/liucongg/ChatGLM-Finetuning/tree/v0.1&#34;&gt;v0.1 Tag&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;update-2023.04.05 &lt;strong&gt;å¢åŠ ä¿¡æ¯æŠ½å–ä»»åŠ¡è¯„æµ‹&lt;/strong&gt;ï¼Œè¯·çœ‹&lt;a href=&#34;https://github.com/liucongg/ChatGLM-Finetuning/tree/v0.1&#34;&gt;v0.1 Tag&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å¾®è°ƒæ–¹æ³•&lt;/h2&gt; &#xA;&lt;p&gt;æ¨¡å‹å¾®è°ƒæ—¶ï¼Œå¦‚æœé‡åˆ°æ˜¾å­˜ä¸å¤Ÿçš„æƒ…å†µï¼Œå¯ä»¥å¼€å¯gradient_checkpointingã€zero3ã€offloadç­‰å‚æ•°æ¥èŠ‚çœæ˜¾å­˜ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¸‹é¢model_name_or_pathå‚æ•°ä¸ºæ¨¡å‹è·¯å¾„ï¼Œè¯·æ ¹æ®å¯æ ¹æ®è‡ªå·±å®é™…æ¨¡å‹ä¿å­˜åœ°å€è¿›è¡Œä¿®æ”¹ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;Freezeæ–¹æ³•&lt;/h3&gt; &#xA;&lt;p&gt;Freezeæ–¹æ³•ï¼Œå³å‚æ•°å†»ç»“ï¼Œå¯¹åŸå§‹æ¨¡å‹éƒ¨åˆ†å‚æ•°è¿›è¡Œå†»ç»“æ“ä½œï¼Œä»…è®­ç»ƒéƒ¨åˆ†å‚æ•°ï¼Œä»¥è¾¾åˆ°åœ¨å•å¡æˆ–å¤šå¡ï¼Œä¸è¿›è¡ŒTPæˆ–PPæ“ä½œå°±å¯ä»¥å¯¹å¤§æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¾®è°ƒä»£ç ï¼Œè§train.pyï¼Œæ ¸å¿ƒéƒ¨åˆ†å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python3&#34;&gt;freeze_module_name = args.freeze_module_name.split(&#34;,&#34;)&#xA;for name, param in model.named_parameters():&#xA;&#x9;if not any(nd in name for nd in freeze_module_name):&#xA;&#x9;&#x9;param.requires_grad = False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;é’ˆå¯¹æ¨¡å‹ä¸åŒå±‚è¿›è¡Œä¿®æ”¹ï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹freeze_module_nameå‚æ•°é…ç½®ï¼Œä¾‹å¦‚&#34;layers.27.,layers.26.,layers.25.,layers.24.&#34;ã€‚ è®­ç»ƒä»£ç å‡é‡‡ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒï¼Œå¯è®¾ç½®å‚æ•°åŒ…å«train_pathã€model_name_or_pathã€modeã€train_typeã€freeze_module_nameã€ds_fileã€num_train_epochsã€per_device_train_batch_sizeã€gradient_accumulation_stepsã€output_dirç­‰ï¼Œ å¯æ ¹æ®è‡ªå·±çš„ä»»åŠ¡é…ç½®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ChatGLMå•å¡è®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM-6B/ \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 1560 \&#xA;                --max_src_len 1024 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm \&#xA;                --train_type freeze \&#xA;                --freeze_module_name &#34;layers.27.,layers.26.,layers.25.,layers.24.&#34; \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --output_dir ./output-glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLMå››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM-6B/ \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 1560 \&#xA;                --max_src_len 1024 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm \&#xA;                --train_type freeze \&#xA;                --freeze_module_name &#34;layers.27.,layers.26.,layers.25.,layers.24.&#34; \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --output_dir ./output-glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLM2å•å¡è®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM2-6B/ \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 1560 \&#xA;                --max_src_len 1024 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm2 \&#xA;                --train_type freeze \&#xA;                --freeze_module_name &#34;layers.27.,layers.26.,layers.25.,layers.24.&#34; \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --output_dir ./output-glm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLM2å››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM2-6B/ \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 1560 \&#xA;                --max_src_len 1024 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm2 \&#xA;                --train_type freeze \&#xA;                --freeze_module_name &#34;layers.27.,layers.26.,layers.25.,layers.24.&#34; \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --output_dir ./output-glm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PSï¼šChatGLMå¾®è°ƒæ—¶æ‰€ç”¨æ˜¾å­˜è¦æ¯”ChatGLM2å¤šï¼Œè¯¦ç»†æ˜¾å­˜å æ¯”å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;DeepSpeed-Stage&lt;/th&gt; &#xA;   &lt;th&gt;Offload&lt;/th&gt; &#xA;   &lt;th&gt;Gradient Checkpointing&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Max Length&lt;/th&gt; &#xA;   &lt;th&gt;GPU-A40 Number&lt;/th&gt; &#xA;   &lt;th&gt;æ‰€è€—æ˜¾å­˜&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;36G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;38G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;24G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;29G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;35G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;36G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;22G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;27G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;PTæ–¹æ³•&lt;/h3&gt; &#xA;&lt;p&gt;PTæ–¹æ³•ï¼Œå³P-Tuningæ–¹æ³•ï¼Œå‚è€ƒ&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/raw/main/ptuning/README.md&#34;&gt;ChatGLMå®˜æ–¹ä»£ç &lt;/a&gt; ï¼Œæ˜¯ä¸€ç§é’ˆå¯¹äºå¤§æ¨¡å‹çš„soft-promptæ–¹æ³•ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liucongg/ChatGLM-Finetuning/master/images/PT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;P-Tuningä»…å¯¹å¤§æ¨¡å‹çš„EmbeddingåŠ å…¥æ–°çš„å‚æ•°ã€‚&lt;a href=&#34;https://arxiv.org/abs/2103.10385&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;P-Tuning-V2ï¼Œå°†å¤§æ¨¡å‹çš„Embeddingå’Œæ¯ä¸€å±‚å‰éƒ½åŠ ä¸Šæ–°çš„å‚æ•°ã€‚&lt;a href=&#34;https://arxiv.org/abs/2110.07602&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å¾®è°ƒä»£ç ï¼Œè§train.pyï¼Œæ ¸å¿ƒéƒ¨åˆ†å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python3&#34;&gt;config = MODE[args.mode][&#34;config&#34;].from_pretrained(args.model_name_or_path)&#xA;config.pre_seq_len = args.pre_seq_len&#xA;config.prefix_projection = args.prefix_projection&#xA;model = MODE[args.mode][&#34;model&#34;].from_pretrained(args.model_name_or_path, config=config)&#xA;for name, param in model.named_parameters():&#xA;&#x9;if not any(nd in name for nd in [&#34;prefix_encoder&#34;]):&#xA;&#x9;&#x9;param.requires_grad = False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å½“prefix_projectionä¸ºTrueæ—¶ï¼Œä¸ºP-Tuning-V2æ–¹æ³•ï¼Œåœ¨å¤§æ¨¡å‹çš„Embeddingå’Œæ¯ä¸€å±‚å‰éƒ½åŠ ä¸Šæ–°çš„å‚æ•°ï¼›ä¸ºFalseæ—¶ï¼Œä¸ºP-Tuningæ–¹æ³•ï¼Œä»…åœ¨å¤§æ¨¡å‹çš„Embeddingä¸Šæ–°çš„å‚æ•°ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è®­ç»ƒä»£ç å‡é‡‡ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒï¼Œå¯è®¾ç½®å‚æ•°åŒ…å«train_pathã€model_name_or_pathã€modeã€train_typeã€pre_seq_lenã€prefix_projectionã€ds_fileã€num_train_epochsã€per_device_train_batch_sizeã€gradient_accumulation_stepsã€output_dirç­‰ï¼Œ å¯æ ¹æ®è‡ªå·±çš„ä»»åŠ¡é…ç½®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ChatGLMå•å¡è®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM-6B \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 768 \&#xA;                --max_src_len 512 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm \&#xA;                --train_type ptuning \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --pre_seq_len 16 \&#xA;                --prefix_projection True \&#xA;                --output_dir ./output-glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLMå››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM-6B \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 1560 \&#xA;                --max_src_len 1024 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm \&#xA;                --train_type ptuning \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --pre_seq_len 16 \&#xA;                --prefix_projection True \&#xA;                --output_dir ./output-glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLM2å•å¡è®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM2-6B \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 1560 \&#xA;                --max_src_len 1024 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm2 \&#xA;                --train_type ptuning \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --pre_seq_len 16 \&#xA;                --prefix_projection True \&#xA;                --output_dir ./output-glm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLM2å››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;                --train_path data/spo_0.json \&#xA;                --model_name_or_path ChatGLM2-6B \&#xA;                --per_device_train_batch_size 1 \&#xA;                --max_len 1560 \&#xA;                --max_src_len 1024 \&#xA;                --learning_rate 1e-4 \&#xA;                --weight_decay 0.1 \&#xA;                --num_train_epochs 2 \&#xA;                --gradient_accumulation_steps 4 \&#xA;                --warmup_ratio 0.1 \&#xA;                --mode glm2 \&#xA;                --train_type ptuning \&#xA;                --seed 1234 \&#xA;                --ds_file ds_zero2_no_offload.json \&#xA;                --gradient_checkpointing \&#xA;                --show_loss_step 10 \&#xA;                --pre_seq_len 16 \&#xA;                --prefix_projection True \&#xA;                --output_dir ./output-glm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PSï¼šChatGLMå¾®è°ƒæ—¶æ‰€ç”¨æ˜¾å­˜è¦æ¯”ChatGLM2å¤šï¼Œè¯¦ç»†æ˜¾å­˜å æ¯”å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;DeepSpeed-Stage&lt;/th&gt; &#xA;   &lt;th&gt;Offload&lt;/th&gt; &#xA;   &lt;th&gt;Gradient Checkpointing&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Max Length&lt;/th&gt; &#xA;   &lt;th&gt;GPU-A40 Number&lt;/th&gt; &#xA;   &lt;th&gt;æ‰€è€—æ˜¾å­˜&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;43G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;44G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;37G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1360&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;44G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;20G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;40G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;19G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;39G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Loraæ–¹æ³•&lt;/h3&gt; &#xA;&lt;p&gt;Loraæ–¹æ³•ï¼Œå³åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå¯¹æŒ‡å®šå‚æ•°ï¼ˆæƒé‡çŸ©é˜µï¼‰å¹¶è¡Œå¢åŠ é¢å¤–çš„ä½ç§©çŸ©é˜µï¼Œå¹¶åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»…è®­ç»ƒé¢å¤–å¢åŠ çš„å¹¶è¡Œä½ç§©çŸ©é˜µçš„å‚æ•°ã€‚ å½“â€œç§©å€¼â€è¿œå°äºåŸå§‹å‚æ•°ç»´åº¦æ—¶ï¼Œæ–°å¢çš„ä½ç§©çŸ©é˜µå‚æ•°é‡ä¹Ÿå°±å¾ˆå°ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡tuningæ—¶ï¼Œä»…é¡»è®­ç»ƒå¾ˆå°çš„å‚æ•°ï¼Œä½†èƒ½è·å–è¾ƒå¥½çš„è¡¨ç°ç»“æœã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liucongg/ChatGLM-Finetuning/master/images/Lora.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;è®ºæ–‡ï¼š&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å®˜æ–¹ä»£ç ï¼š&lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HuggingFaceå°è£…çš„peftåº“ï¼š&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å¾®è°ƒä»£ç ï¼Œè§train.pyï¼Œæ ¸å¿ƒéƒ¨åˆ†å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python3&#34;&gt;model = MODE[args.mode][&#34;model&#34;].from_pretrained(args.model_name_or_path)&#xA;lora_module_name = args.lora_module_name.split(&#34;,&#34;)&#xA;config = LoraConfig(r=args.lora_dim,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;lora_alpha=args.lora_alpha,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;target_modules=lora_module_name,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;lora_dropout=args.lora_dropout,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;bias=&#34;none&#34;,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;task_type=&#34;CAUSAL_LM&#34;,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;inference_mode=False,&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;)&#xA;model = get_peft_model(model, config)&#xA;model.config.torch_dtype = torch.float32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è®­ç»ƒä»£ç å‡é‡‡ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒï¼Œå¯è®¾ç½®å‚æ•°åŒ…å«train_pathã€model_name_or_pathã€modeã€train_typeã€lora_dimã€lora_alphaã€lora_dropoutã€lora_module_nameã€ds_fileã€num_train_epochsã€per_device_train_batch_sizeã€gradient_accumulation_stepsã€output_dirç­‰ï¼Œ å¯æ ¹æ®è‡ªå·±çš„ä»»åŠ¡é…ç½®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ChatGLMå•å¡è®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 deepspeed --master_port 520 train.py \&#xA;              --train_path data/spo_0.json \&#xA;              --model_name_or_path ChatGLM-6B \&#xA;              --per_device_train_batch_size 1 \&#xA;              --max_len 1560 \&#xA;              --max_src_len 1024 \&#xA;              --learning_rate 1e-4 \&#xA;              --weight_decay 0.1 \&#xA;              --num_train_epochs 2 \&#xA;              --gradient_accumulation_steps 4 \&#xA;              --warmup_ratio 0.1 \&#xA;              --mode glm \&#xA;              --train_type lora \&#xA;              --lora_dim 16 \&#xA;              --lora_alpha 64 \&#xA;              --lora_dropout 0.1 \&#xA;              --lora_module_name &#34;query_key_value&#34; \&#xA;              --seed 1234 \&#xA;              --ds_file ds_zero2_no_offload.json \&#xA;              --gradient_checkpointing \&#xA;              --show_loss_step 10 \&#xA;              --output_dir ./output-glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLMå››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;              --train_path data/spo_0.json \&#xA;              --model_name_or_path ChatGLM-6B \&#xA;              --per_device_train_batch_size 1 \&#xA;              --max_len 1560 \&#xA;              --max_src_len 1024 \&#xA;              --learning_rate 1e-4 \&#xA;              --weight_decay 0.1 \&#xA;              --num_train_epochs 2 \&#xA;              --gradient_accumulation_steps 4 \&#xA;              --warmup_ratio 0.1 \&#xA;              --mode glm \&#xA;              --train_type lora \&#xA;              --lora_dim 16 \&#xA;              --lora_alpha 64 \&#xA;              --lora_dropout 0.1 \&#xA;              --lora_module_name &#34;query_key_value&#34; \&#xA;              --seed 1234 \&#xA;              --ds_file ds_zero2_no_offload.json \&#xA;              --gradient_checkpointing \&#xA;              --show_loss_step 10 \&#xA;              --output_dir ./output-glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLM2å•å¡è®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 deepspeed --master_port 520 train.py \&#xA;              --train_path data/spo_0.json \&#xA;              --model_name_or_path ChatGLM2-6B \&#xA;              --per_device_train_batch_size 1 \&#xA;              --max_len 1560 \&#xA;              --max_src_len 1024 \&#xA;              --learning_rate 1e-4 \&#xA;              --weight_decay 0.1 \&#xA;              --num_train_epochs 2 \&#xA;              --gradient_accumulation_steps 4 \&#xA;              --warmup_ratio 0.1 \&#xA;              --mode glm2 \&#xA;              --train_type lora \&#xA;              --lora_dim 16 \&#xA;              --lora_alpha 64 \&#xA;              --lora_dropout 0.1 \&#xA;              --lora_module_name &#34;query_key_value,dense_h_to_4h,dense_4h_to_h,dense&#34; \&#xA;              --seed 1234 \&#xA;              --ds_file ds_zero2_no_offload.json \&#xA;              --gradient_checkpointing \&#xA;              --show_loss_step 10 \&#xA;              --output_dir ./output-glm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLM2å››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;              --train_path data/spo_0.json \&#xA;              --model_name_or_path ChatGLM2-6B \&#xA;              --per_device_train_batch_size 1 \&#xA;              --max_len 1560 \&#xA;              --max_src_len 1024 \&#xA;              --learning_rate 1e-4 \&#xA;              --weight_decay 0.1 \&#xA;              --num_train_epochs 2 \&#xA;              --gradient_accumulation_steps 4 \&#xA;              --warmup_ratio 0.1 \&#xA;              --mode glm2 \&#xA;              --train_type lora \&#xA;              --lora_dim 16 \&#xA;              --lora_alpha 64 \&#xA;              --lora_dropout 0.1 \&#xA;              --lora_module_name &#34;query_key_value,dense_h_to_4h,dense_4h_to_h,dense&#34; \&#xA;              --seed 1234 \&#xA;              --ds_file ds_zero2_no_offload.json \&#xA;              --gradient_checkpointing \&#xA;              --show_loss_step 10 \&#xA;              --output_dir ./output-glm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PSï¼šChatGLMå¾®è°ƒæ—¶æ‰€ç”¨æ˜¾å­˜è¦æ¯”ChatGLM2å¤šï¼Œè¯¦ç»†æ˜¾å­˜å æ¯”å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;DeepSpeed-Stage&lt;/th&gt; &#xA;   &lt;th&gt;Offload&lt;/th&gt; &#xA;   &lt;th&gt;Gradient Checkpointing&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Max Length&lt;/th&gt; &#xA;   &lt;th&gt;GPU-A40 Number&lt;/th&gt; &#xA;   &lt;th&gt;æ‰€è€—æ˜¾å­˜&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;20G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;45G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;20G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;45G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;20G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;43G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;19G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero2&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;42G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;æ³¨æ„ï¼šLoraæ–¹æ³•åœ¨æ¨¡å‹ä¿å­˜æ—¶ä»…ä¿å­˜äº†Loraè®­ç»ƒå‚æ•°ï¼Œå› æ­¤åœ¨æ¨¡å‹é¢„æµ‹æ—¶éœ€è¦å°†æ¨¡å‹å‚æ•°è¿›è¡Œåˆå¹¶ï¼Œå…·ä½“å‚è€ƒmerge_lora.pyã€‚&lt;/p&gt; &#xA;&lt;h3&gt;å…¨å‚æ–¹æ³•&lt;/h3&gt; &#xA;&lt;p&gt;å…¨å‚æ–¹æ³•ï¼Œå¯¹å¤§æ¨¡å‹è¿›è¡Œå…¨é‡å‚æ•°è®­ç»ƒï¼Œä¸»è¦å€ŸåŠ©DeepSpeed-Zero3æ–¹æ³•ï¼Œå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œå¤šå¡åˆ†å‰²ï¼Œå¹¶å€ŸåŠ©Offloadæ–¹æ³•ï¼Œå°†ä¼˜åŒ–å™¨å‚æ•°å¸è½½åˆ°CPUä¸Šä»¥è§£å†³æ˜¾å¡ä¸è¶³é—®é¢˜ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¾®è°ƒä»£ç ï¼Œè§train.pyï¼Œæ ¸å¿ƒéƒ¨åˆ†å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python3&#34;&gt;model = MODE[args.mode][&#34;model&#34;].from_pretrained(args.model_name_or_path)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è®­ç»ƒä»£ç å‡é‡‡ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒï¼Œå¯è®¾ç½®å‚æ•°åŒ…å«train_pathã€model_name_or_pathã€modeã€train_typeã€ds_fileã€num_train_epochsã€per_device_train_batch_sizeã€gradient_accumulation_stepsã€output_dirç­‰ï¼Œ å¯æ ¹æ®è‡ªå·±çš„ä»»åŠ¡é…ç½®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ChatGLMå››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;              --train_path data/spo_0.json \&#xA;              --model_name_or_path ChatGLM-6B \&#xA;              --per_device_train_batch_size 1 \&#xA;              --max_len 1560 \&#xA;              --max_src_len 1024 \&#xA;              --learning_rate 1e-4 \&#xA;              --weight_decay 0.1 \&#xA;              --num_train_epochs 2 \&#xA;              --gradient_accumulation_steps 4 \&#xA;              --warmup_ratio 0.1 \&#xA;              --mode glm \&#xA;              --train_type all \&#xA;              --seed 1234 \&#xA;              --ds_file ds_zero3_offload.json \&#xA;              --gradient_checkpointing \&#xA;              --show_loss_step 10 \&#xA;              --output_dir ./output-glm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatGLM2å››å¡è®­ç»ƒï¼Œé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶å…·ä½“å“ªå‡ å—å¡è¿›è¡Œè®­ç»ƒï¼Œå¦‚æœä¸åŠ è¯¥å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨è¿è¡Œæœºå™¨ä¸Šæ‰€æœ‰å¡è¿›è¡Œè®­ç»ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 deepspeed --master_port 520 train.py \&#xA;              --train_path data/spo_0.json \&#xA;              --model_name_or_path ChatGLM2-6B \&#xA;              --per_device_train_batch_size 1 \&#xA;              --max_len 1560 \&#xA;              --max_src_len 1024 \&#xA;              --learning_rate 1e-4 \&#xA;              --weight_decay 0.1 \&#xA;              --num_train_epochs 2 \&#xA;              --gradient_accumulation_steps 4 \&#xA;              --warmup_ratio 0.1 \&#xA;              --mode glm2 \&#xA;              --train_type all \&#xA;              --seed 1234 \&#xA;              --ds_file ds_zero3_no_offload.json \&#xA;              --gradient_checkpointing \&#xA;              --show_loss_step 10 \&#xA;              --output_dir ./output-glm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PSï¼šChatGLMå¾®è°ƒæ—¶æ‰€ç”¨æ˜¾å­˜è¦æ¯”ChatGLM2å¤šï¼Œè¯¦ç»†æ˜¾å­˜å æ¯”å¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;DeepSpeed-Stage&lt;/th&gt; &#xA;   &lt;th&gt;Offload&lt;/th&gt; &#xA;   &lt;th&gt;Gradient Checkpointing&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Max Length&lt;/th&gt; &#xA;   &lt;th&gt;GPU-A40 Number&lt;/th&gt; &#xA;   &lt;th&gt;æ‰€è€—æ˜¾å­˜&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM&lt;/td&gt; &#xA;   &lt;td&gt;zero3&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;33G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero3&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;44G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChaGLM2&lt;/td&gt; &#xA;   &lt;td&gt;zero3&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;1560&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;26G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;åé¢è¡¥å……DeepSpeedçš„Zero-Stageçš„ç›¸å…³å†…å®¹è¯´æ˜ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;è¿è¡Œç¯å¢ƒ&lt;/h3&gt; &#xA;&lt;p&gt;æŸ¥çœ‹requirements.txtæ–‡ä»¶&lt;/p&gt; &#xA;&lt;h2&gt;å®éªŒç»“æœ&lt;/h2&gt; &#xA;&lt;h3&gt;ä¸‰å…ƒç»„æŠ½å–&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¸ºäº†é˜²æ­¢å¤§æ¨¡å‹çš„æ•°æ®æ³„éœ²ï¼Œé‡‡ç”¨ä¸€ä¸ªé¢†åŸŸæ¯”èµ›æ•°æ®é›†-&lt;a href=&#34;https://www.datafountain.cn/competitions/584&#34;&gt;æ±½è½¦å·¥ä¸šæ•…éšœæ¨¡å¼å…³ç³»æŠ½å–&lt;/a&gt;ï¼ŒéšæœºæŠ½å–50æ¡ä½œä¸ºæµ‹è¯•é›†&lt;/li&gt; &#xA; &lt;li&gt;è®­ç»ƒç¤ºä¾‹ï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;instruction&#34;: &#34;ä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè¯·ä½ å¸®æˆ‘æŠ½å–å‡ºå…³ç³»å†…å®¹ä¸º\&#34;æ€§èƒ½æ•…éšœ\&#34;, \&#34;éƒ¨ä»¶æ•…éšœ\&#34;, \&#34;ç»„æˆ\&#34;å’Œ \&#34;æ£€æµ‹å·¥å…·\&#34;çš„ç›¸å…³ä¸‰å…ƒç»„ï¼Œä¸‰å…ƒç»„å†…éƒ¨ç”¨\&#34;_\&#34;è¿æ¥ï¼Œä¸‰å…ƒç»„ä¹‹é—´ç”¨\\nåˆ†å‰²ã€‚æ–‡æœ¬ï¼š&#34;,&#xA;    &#34;input&#34;: &#34;æ•…éšœç°è±¡ï¼šå‘åŠ¨æœºæ°´æ¸©é«˜ï¼Œé£æ‰‡å§‹ç»ˆæ˜¯ä½é€Ÿè½¬åŠ¨ï¼Œé«˜é€Ÿæ¡£ä¸å·¥ä½œï¼Œå¼€ç©ºè°ƒå°¤å…¶å¦‚æ­¤ã€‚&#34;,&#xA;    &#34;output&#34;: &#34;å‘åŠ¨æœº_éƒ¨ä»¶æ•…éšœ_æ°´æ¸©é«˜\né£æ‰‡_éƒ¨ä»¶æ•…éšœ_ä½é€Ÿè½¬åŠ¨&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;å¾®è°ƒæ–¹æ³•&lt;/th&gt; &#xA;   &lt;th&gt;PT-Only-Embedding&lt;/th&gt; &#xA;   &lt;th&gt;PT&lt;/th&gt; &#xA;   &lt;th&gt;Freeze&lt;/th&gt; &#xA;   &lt;th&gt;Lora&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æµ‹è¯•ç»“æœF1&lt;/td&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;0.6283&lt;/td&gt; &#xA;   &lt;td&gt;0.5675&lt;/td&gt; &#xA;   &lt;td&gt;0.5359&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;ç»“æ„åˆ†æï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ•ˆæœä¸ºPT&amp;gt;Freeze&amp;gt;Lora&amp;gt;PT-Only-Embedding&lt;/li&gt; &#xA; &lt;li&gt;PT-Only-Embeddingæ•ˆæœå¾ˆä¸ç†æƒ³ï¼Œå‘ç°åœ¨è®­ç»ƒæ—¶ï¼Œæœ€åçš„lossä»…èƒ½æ”¶æ•›åˆ°2.å‡ ï¼Œè€Œå…¶ä»–æœºåˆ¶å¯ä»¥æ”¶æ•›åˆ°0.å‡ ã€‚åˆ†æåŸå› ä¸ºï¼Œè¾“å‡ºå†…å®¹å½¢å¼ä¸åŸæœ‰è¯­è¨€æ¨¡å‹ä»»åŠ¡ç›¸å·®å¾ˆå¤§ï¼Œä»…å¢åŠ é¢å¤–Embeddingå‚æ•°ï¼Œä¸è¶³ä»¥æ”¹å˜å¤æ‚çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä¸Šé¢æµ‹è¯•ä»…ä»£è¡¨ä¸ªäººæµ‹è¯•ç»“æœï¼Œå¹¶ä¸”ç”±äºç”Ÿæˆæ¨¡å‹ç”Ÿæˆé•¿åº¦å¯¹æ¨ç†è€—æ—¶å½±å“å¾ˆå¤§ï¼Œå› æ­¤å¯ä»¥å…¶ä»–æ•°æ®ä¼šæœ‰ä¸ä¸€æ ·çš„ç»“æœã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ¨¡å‹åœ¨æŒ‡å®šä»»åŠ¡ä¸Šå¾®è°ƒä¹‹åï¼Œå¹¶æ²¡æœ‰ä¸§å¤±åŸæœ‰èƒ½åŠ›ï¼Œä¾‹å¦‚ç”Ÿæˆâ€œå¸®æˆ‘å†™ä¸ªå¿«æ’ç®—æ³•â€ï¼Œä¾ç„¶å¯ä»¥ç”Ÿæˆ-å¿«æ’ä»£ç ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ç”±äºå¤§æ¨¡å‹å¾®è°ƒéƒ½é‡‡ç”¨å¤§é‡instructionè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»…é‡‡ç”¨å•ä¸€çš„æŒ‡ä»¤è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå¯¹åŸæ¥å…¶ä»–çš„æŒ‡ä»¤å½±å“ä¸å¤§ï¼Œå› æ­¤å¹¶æ²¡å¯¼è‡´åŸæ¥æ¨¡å‹çš„èƒ½åŠ›ä¸§å¤±ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å¾ˆå¤šåŒå­¦åœ¨å¾®è°ƒåå‡ºç°äº†ç¾éš¾æ€§é—å¿˜ç°è±¡ï¼Œä½†æœ¬é¡¹ç›®çš„è®­ç»ƒä»£ç å¹¶æ²¡æœ‰å‡ºç°ï¼Œå¯¹â€œç¿»è¯‘ä»»åŠ¡â€ã€â€œä»£ç ä»»åŠ¡â€ã€â€œé—®ç­”ä»»åŠ¡â€è¿›è¡Œæµ‹è¯•ï¼Œå…·ä½“æµ‹è¯•æ•ˆæœå¦‚ä¸‹ï¼š&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;ç¿»è¯‘ä»»åŠ¡&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liucongg/ChatGLM-Finetuning/master/images/ft_fanyi.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;ä»£ç ä»»åŠ¡&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liucongg/ChatGLM-Finetuning/master/images/ft_code.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;é—®ç­”ä»»åŠ¡&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liucongg/ChatGLM-Finetuning/master/images/ft_qa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;æ–‡æœ¬ç”Ÿæˆ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¸ºäº†é˜²æ­¢å¤§æ¨¡å‹çš„æ•°æ®æ³„éœ²ï¼Œé‡‡ç”¨ä¸€ä¸ªâ€œä¸‡åˆ›æ¯â€ä¸­åŒ»è¯å¤©æ± å¤§æ•°æ®ç«èµ›-&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/531826/introduction&#34;&gt;ä¸­åŒ»æ–‡çŒ®é—®é¢˜ç”ŸæˆæŒ‘æˆ˜&lt;/a&gt;ï¼ŒéšæœºæŠ½å–20æ¡ä½œä¸ºæµ‹è¯•é›†&lt;/li&gt; &#xA; &lt;li&gt;PTä¸ºå®˜æ–¹çš„P-Tuning V2è®­ç»ƒæ–¹æ³•ï¼ŒPT-Only-Embeddingè¡¨ç¤ºä»…å¯¹Embeddingè¿›è¡Œsoft-promptï¼ŒFreezeä»…è®­ç»ƒæ¨¡å‹åäº”å±‚å‚æ•°ï¼ŒLoraé‡‡ç”¨ä½ç§©çŸ©é˜µæ–¹æ³•è®­ç»ƒï¼Œç§©ä¸º8ï¼›&lt;/li&gt; &#xA; &lt;li&gt;è®­ç»ƒç¤ºä¾‹ï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;instruction&#34;: &#34;ä½ ç°åœ¨æ˜¯ä¸€ä¸ªé—®é¢˜ç”Ÿæˆæ¨¡å‹ï¼Œè¯·æ ¹æ®ä¸‹é¢æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªé—®é¢˜ï¼Œæ–‡æ¡£ï¼š&#34;,&#xA;    &#34;input&#34;: &#34;æ¸…çƒ­è§£æ¯’å£æœæ¶²ç”±ç”ŸçŸ³è†ã€çŸ¥æ¯ã€ç´«èŠ±åœ°ä¸ã€é‡‘é“¶èŠ±ã€éº¦é—¨å†¬ã€é»„èŠ©ã€ç„å‚ã€è¿ç¿˜ã€é¾™èƒ†è‰ã€ç”Ÿåœ°é»„ã€æ €å­ã€æ¿è“æ ¹ç»„æˆã€‚å…·æœ‰ç–é£è§£è¡¨ã€æ¸…çƒ­è§£æ¯’åˆ©å’½ã€ç”Ÿæ´¥æ­¢æ¸´çš„åŠŸæ•ˆï¼Œé€‚ç”¨äºæ²»ç–—å¤–æ„Ÿæ—¶é‚ªã€å†…æœ‰è•´çƒ­æ‰€è‡´çš„èº«çƒ­æ±—å‡ºã€å¤´ç—›èº«ç—›ã€å¿ƒçƒ¦å£æ¸´ã€å¾®æ¶å¯’æˆ–åæ¶çƒ­ã€èˆŒçº¢ã€è‹”é»„ã€è„‰æ•°ç­‰ç—‡ã€‚ç°ä»£ä¸´åºŠä¸»è¦ç”¨äºæ²»ç–—æµè¡Œæ€§æ„Ÿå†’ã€æµè¡Œæ€§è„‘è„Šé«“è†œç‚ã€è‚ºç‚ç­‰å„ç§å‘çƒ­æ€§ç–¾ç—…ã€‚å£æœæ¶²ï¼šæ¯æ”¯10æ¯«å‡ï¼Œæ¯æ¬¡10~20æ¯«å‡ï¼Œæ¯æ—¥3æ¬¡ã€‚ã€”æ³¨æ„äº‹é¡¹ã€•é˜³è™šä¾¿æ¾¹è€…ä¸å®œä½¿ç”¨ã€‚&#34;,&#xA;    &#34;output&#34;: &#34;æ¸…çƒ­è§£æ¯’å£æœçš„åŠŸæ•ˆæœ‰å“ªäº›ï¼Ÿ&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ç”±äºç”Ÿæˆæ¨¡å‹çš„å†…å®¹ä¸èƒ½æƒ³ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸€æ ·è¯„ä»·ï¼Œç”¨ç°æœ‰çš„BLUEæˆ–è€…Rougeæ¥è¯„ä»·ä¹Ÿæ˜¯ä¸åˆé€‚ï¼Œå› æ­¤åˆ¶å®šäº†è¯„åˆ†è§„åˆ™ã€‚ é€šè¿‡å¤šæ ·æ€§å’Œå‡†ç¡®æ€§ä¸¤ä¸ªè§’åº¦åˆ¤æ–­D2Qæ¨¡å‹å¥½åï¼Œæ¯ä¸ªæ ·æœ¬æ€»è®¡5åˆ†ï¼Œå…±20ä¸ªæ ·æœ¬ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¤šæ ·æ€§ï¼š &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;é—®é¢˜æ˜¯å¦é«˜åº¦ç›¸ä¼¼ï¼Œæ¯é‡å¤ä¸€ä¸ªé—®é¢˜æ‰£0.25åˆ†ï¼›&lt;/li&gt; &#xA;   &lt;li&gt;é—®é¢˜å¯¹åº”ç­”æ¡ˆæ˜¯å¦ç›¸åŒï¼Œæ¯æœ‰ä¸€ä¸ªé‡å¤ç­”æ¡ˆæˆ–æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œæ‰£0.25åˆ†ï¼›&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;å‡†ç¡®æ€§ï¼š &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;é—®é¢˜èƒ½å¦ä»æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œæ¯æœ‰ä¸€ä¸ªæ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œæ‰£0.25åˆ†ï¼›&lt;/li&gt; &#xA;   &lt;li&gt;é—®é¢˜å†…å®¹æ˜¯å¦æµç•…ï¼Œæ¯æœ‰ä¸€ä¸ªé—®é¢˜ä¸æµç•…ï¼Œæ‰£0.25åˆ†ï¼›&lt;/li&gt; &#xA;   &lt;li&gt;é—®é¢˜å†…å®¹æ˜¯å¦æœ‰å®³ï¼Œæ¯æœ‰ä¸€ä¸ªæœ‰å®³ï¼Œæ‰£0.25åˆ†ï¼›&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;å¾®è°ƒæ–¹æ³•&lt;/th&gt; &#xA;   &lt;th&gt;åŸå§‹æ¨¡å‹&lt;/th&gt; &#xA;   &lt;th&gt;PT-Only-Embedding&lt;/th&gt; &#xA;   &lt;th&gt;PT&lt;/th&gt; &#xA;   &lt;th&gt;Freeze&lt;/th&gt; &#xA;   &lt;th&gt;Lora&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åˆ†æ•°&lt;/td&gt; &#xA;   &lt;td&gt;51.75&lt;/td&gt; &#xA;   &lt;td&gt;73.75&lt;/td&gt; &#xA;   &lt;td&gt;87.75&lt;/td&gt; &#xA;   &lt;td&gt;79.25&lt;/td&gt; &#xA;   &lt;td&gt;86.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;æµæ°´çº¿å¹¶è¡Œè®­ç»ƒ&lt;/h2&gt; &#xA;&lt;p&gt;ä»£ç è¯´æ˜è§ï¼š&lt;a href=&#34;https://zhuanlan.zhihu.com/p/636488690&#34;&gt;å¤§æ¨¡å‹æµæ°´çº¿å¹¶è¡Œï¼ˆPipelineï¼‰å®æˆ˜&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;è¯·çœ‹&lt;a href=&#34;https://github.com/liucongg/ChatGLM-Finetuning/tree/v0.1&#34;&gt;v0.1 Tag&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=liucongg/ChatGLM-Finetuning&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>