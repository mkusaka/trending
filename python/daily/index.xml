<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-19T01:35:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>truefoundry/cognita</title>
    <updated>2024-04-19T01:35:05Z</updated>
    <id>tag:github.com,2024-04-19:/truefoundry/cognita</id>
    <link href="https://github.com/truefoundry/cognita" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cognita by TrueFoundry - Framework for building modular, open source RAG applications for production.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/(https://cognita.truefoundry.com)&#34;&gt;Cognita&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/readme-banner.png&#34; alt=&#34;RAG_TF&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why use Cognita?&lt;/h2&gt; &#xA;&lt;p&gt;Langchain/LlamaIndex provide easy to use abstractions that can be used for quick experimentation and prototyping on jupyter notebooks. But, when things move to production, there are constraints like the components should be modular, easily scalable and extendable. This is where Cognita comes in action. Cognita uses Langchain/Llamaindex under the hood and provides an organisation to your codebase, where each of the RAG component is modular, API driven and easily extendible. Cognita can be used easily in a &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#rocket-quickstart-running-cognita-locally&#34;&gt;local&lt;/a&gt; setup, at the same time, offers you a production ready environment along with no-code &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/frontend/README.md&#34;&gt;UI&lt;/a&gt; support. Cognita also supports incremental indexing by default.&lt;/p&gt; &#xA;&lt;p&gt;You can try out Cognita at: &lt;a href=&#34;https://cognita.truefoundry.com&#34;&gt;https://cognita.truefoundry.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/RAG-TF.gif&#34; alt=&#34;RAG_TF&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#cognita&#34;&gt;Cognita&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#introduction&#34;&gt;Introduction&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#advantages-of-using-cognita-are&#34;&gt;Advantages of using Cognita are:&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;‚ú® &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#sparkles-getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üêç &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#snake-installing-python-and-setting-up-a-virtual-environment&#34;&gt;Installing Python and Setting Up a Virtual Environment&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#setting-up-a-virtual-environment&#34;&gt;Setting Up a Virtual Environment&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#create-a-virtual-environment&#34;&gt;Create a Virtual Environment:&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#activate-the-virtual-environment&#34;&gt;Activate the Virtual Environment:&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#rocket-quickstart-running-cognita-locally&#34;&gt;Quickstart: Running Cognita Locally&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#install-necessary-packages&#34;&gt;Install necessary packages:&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#setting-up-env-file&#34;&gt;Setting up .env file:&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#executing-the-code&#34;&gt;Executing the Code:&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#hammer_and_pick-project-architecture&#34;&gt;Project Architecture&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#cognita-components&#34;&gt;Cognita Components:&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#data-indexing&#34;&gt;Data Indexing:&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;‚ùì&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#question-question-answering-using-api-server&#34;&gt;Question-Answering using API Server:&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;üíª &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#computer-code-structure&#34;&gt;Code Structure:&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#customizing-the-code-for-your-usecase&#34;&gt;Customizing the Code for your usecase&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#customizing-dataloaders&#34;&gt;Customizing Dataloaders:&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#customizing-embedder&#34;&gt;Customizing Embedder:&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#customizing-parsers&#34;&gt;Customizing Parsers:&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#adding-custom-vectordb&#34;&gt;Adding Custom VectorDB:&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#rerankers&#34;&gt;Rerankers:&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üí° &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#bulb-writing-your-query-controller-qna&#34;&gt;Writing your Query Controller (QnA):&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#steps-to-add-your-custom-query-controller&#34;&gt;Steps to add your custom Query Controller:&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üê≥ &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#whale-quickstart-deployment-with-truefoundry&#34;&gt;Quickstart: Deployment with Truefoundry:&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#using-the-rag-ui&#34;&gt;Using RAG UI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;üíñ &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#sparkling_heart-open-source-contribution&#34;&gt;Open Source Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîÆ &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#crystal_ball-future-developments&#34;&gt;Future developments&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Cognita is an open-source framework to organize your RAG codebase along with a frontend to play around with different RAG customizations. It provides a simple way to organize your codebase so that it becomes easy to test it locally while also being able to deploy it in a production ready environment. The key issues that arise while productionizing RAG system from a Jupyter Notebook are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chunking and Embedding Job&lt;/strong&gt;: The chunking and embedding code usually needs to be abstracted out and deployed as a job. Sometimes the job will need to run on a schedule or be trigerred via an event to keep the data updated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query Service&lt;/strong&gt;: The code that generates the answer from the query needs to be wrapped up in a api server like FastAPI and should be deployed as a service. This service should be able to handle multiple queries at the same time and also autoscale with higher traffic.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM / Embedding Model Deployment&lt;/strong&gt;: Often times, if we are using open-source models, we load the model in the Jupyter notebook. This will need to be hosted as a separate service in production and model will need to be called as an API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vector DB deployment&lt;/strong&gt;: Most testing happens on vector DBs in memory or on disk. However, in production, the DBs need to be deployed in a more scalable and reliable way.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Cognita makes it really easy to customize and experiment everything about a RAG system and still be able to deploy it in a good way. It also ships with a UI that makes it easier to try out different RAG configurations and see the results in real time. You can use it locally or with/without using any Truefoundry components. However, using Truefoundry components makes it easier to test different models and deploy the system in a scalable way. Cognita allows you to host multiple RAG systems using one app.&lt;/p&gt; &#xA;&lt;h3&gt;Advantages of using Cognita are:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A central reusable repository of parsers, loaders, embedders and retrievers.&lt;/li&gt; &#xA; &lt;li&gt;Ability for non-technical users to play with UI - Upload documents and perform QnA using modules built by the development team.&lt;/li&gt; &#xA; &lt;li&gt;Fully API driven - which allows integration with other systems. &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;If you use Cognita with Truefoundry AI Gateway, you can get logging, metrics and feedback mechanism for your user queries.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Features:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Support for multiple document retrievers that use &lt;code&gt;Similarity Search&lt;/code&gt;, &lt;code&gt;Query Decompostion&lt;/code&gt;, &lt;code&gt;Document Reranking&lt;/code&gt;, etc&lt;/li&gt; &#xA; &lt;li&gt;Support for SOTA OpenSource embeddings and reranking from &lt;code&gt;mixedbread-ai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support for using LLMs using &lt;code&gt;Ollama&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support for incremental indexing that ingests entire documents in batches (reduces compute burden), keeps track of already indexed documents and prevents re-indexing of those docs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;&lt;span&gt;‚ú®&lt;/span&gt; Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;You can play around with the code locally using the python &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/#rocket-quickstart-running-cognita-locally&#34;&gt;script&lt;/a&gt; or using the UI component that ships with the code.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üêç&lt;/span&gt; Installing Python and Setting Up a Virtual Environment&lt;/h1&gt; &#xA;&lt;p&gt;Before you can use Cognita, you&#39;ll need to ensure that &lt;code&gt;Python &amp;gt;=3.10.0&lt;/code&gt; is installed on your system and that you can create a virtual environment for a safer and cleaner project setup.&lt;/p&gt; &#xA;&lt;h2&gt;Setting Up a Virtual Environment&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s recommended to use a virtual environment to avoid conflicts with other projects or system-wide Python packages.&lt;/p&gt; &#xA;&lt;h3&gt;Create a Virtual Environment:&lt;/h3&gt; &#xA;&lt;p&gt;Navigate to your project&#39;s directory in the terminal. Run the following command to create a virtual environment named venv (you can name it anything you like):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv ./venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Activate the Virtual Environment:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On Windows, activate the virtual environment by running:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;venv\Scripts\activate.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On macOS and Linux, activate it with:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once your virtual environment is activated, you&#39;ll see its name in the terminal prompt. Now you&#39;re ready to install Cognita using the steps provided in the Quickstart sections.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Remember to deactivate the virtual environment when you&#39;re done working with Cognita by simply running deactivate in the terminal.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;&lt;span&gt;üöÄ&lt;/span&gt; Quickstart: Running Cognita Locally&lt;/h1&gt; &#xA;&lt;p&gt;Following are the instructions for running Cognita locally without any additional Truefoundry dependencies&lt;/p&gt; &#xA;&lt;h2&gt;Install necessary packages:&lt;/h2&gt; &#xA;&lt;p&gt;In the project root execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r backend/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting up .env file:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file by copying copy from &lt;code&gt;env.local.example&lt;/code&gt; set up relavant fields.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Executing the Code:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now we index the data (&lt;code&gt;sample-data/creditcards&lt;/code&gt;) by executing the following command from project root: &lt;pre&gt;&lt;code&gt;python -m local.ingest&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;To run the query execute the following command from project root: &lt;pre&gt;&lt;code&gt;python -m local.run&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;These commands make use of &lt;code&gt;local.metadata.yaml&lt;/code&gt; file where you setup qdrant collection name, different data source path, and embedder configurations.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can try out different retrievers and queries by importing them from &lt;code&gt;from backend.modules.query_controllers.example.payload&lt;/code&gt; in &lt;code&gt;run.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can also start a FastAPI server: &lt;code&gt;uvicorn --host 0.0.0.0 --port 8000 backend.server.app:app --reload&lt;/code&gt; Then, Swagger doc will be available at: &lt;code&gt;http://localhost:8000/&lt;/code&gt; For local version you need not create data sources, collection or index them using API, as it is taken care by &lt;code&gt;local.metadata.yaml&lt;/code&gt; and &lt;code&gt;ingest.py&lt;/code&gt; file. You can directly try out retrievers endpoint.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To use frontend UI for quering you can go to : &lt;code&gt;cd fronend&lt;/code&gt; and execute &lt;code&gt;yarn dev&lt;/code&gt; to start the UI and play around. Refer more at frontend &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/frontend/README.md&#34;&gt;README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;&lt;span&gt;‚öí&lt;/span&gt; Project Architecture&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/rag_arch.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Overall the architecture of Cognita is composed of several entities&lt;/p&gt; &#xA;&lt;h2&gt;Cognita Components:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Sources&lt;/strong&gt; - These are the places that contain your documents to be indexed. Usually these are S3 buckets, databases, TrueFoundry Artifacts or even local disk&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Metadata Store&lt;/strong&gt; - This store contains metadata about the collection themselves. A collection refers to a set of documents from one or more data sources combined. For each collection, the collection metadata stores&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Name of the collection&lt;/li&gt; &#xA;   &lt;li&gt;Name of the associated Vector DB collection&lt;/li&gt; &#xA;   &lt;li&gt;Linked Data Sources&lt;/li&gt; &#xA;   &lt;li&gt;Parsing Configuration for each data source&lt;/li&gt; &#xA;   &lt;li&gt;Embedding Model and Configuration to be used&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Gateway&lt;/strong&gt; - This is a central proxy that allows proxying requests to various Embedding and LLM models across many providers with a unified API format. This can be OpenAIChat, OllamaChat, or even TruefoundryChat that uses TF LLM Gateway.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector DB&lt;/strong&gt; - This stores the embeddings and metadata for parsed files for the collection. It can be queried to get similar chunks or exact matches based on filters. We are currently supporting &lt;code&gt;Qdrant&lt;/code&gt; and &lt;code&gt;SingleStore&lt;/code&gt; as our choice of vector database.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Indexing Job&lt;/strong&gt; - This is an asynchronous Job responsible for orchestrating the indexing flow. Indexing can be started manually or run regularly on a cron schedule. It will&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Scan the Data Sources to get list of documents&lt;/li&gt; &#xA;   &lt;li&gt;Check the Vector DB state to filter out unchanged documents&lt;/li&gt; &#xA;   &lt;li&gt;Downloads and parses files to create smaller chunks with associated metadata&lt;/li&gt; &#xA;   &lt;li&gt;Embeds those chunks using the AI Gateway and puts them into Vector DB &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;The source code for this is in the &lt;code&gt;backend/indexer/&lt;/code&gt;&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;API Server&lt;/strong&gt; - This component processes the user query to generate answers with references synchronously. Each application has full control over the retrieval and answer process. Broadly speaking, when a user sends a request&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The corresponsing Query Controller bootstraps retrievers or multi-step agents according to configuration.&lt;/li&gt; &#xA;   &lt;li&gt;User&#39;s question is processed and embedded using the AI Gateway.&lt;/li&gt; &#xA;   &lt;li&gt;One or more retrievers interact with the Vector DB to fetch relevant chunks and metadata.&lt;/li&gt; &#xA;   &lt;li&gt;A final answer is formed by using a LLM via the AI Gateway.&lt;/li&gt; &#xA;   &lt;li&gt;Metadata for relevant documents fetched during the process can be optionally enriched. E.g. adding presigned URLs. &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;The code for this component is in &lt;code&gt;backend/server/&lt;/code&gt;&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Data Indexing:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A Cron on some schedule will trigger the Indexing Job&lt;/li&gt; &#xA; &lt;li&gt;The data source associated with the collection are &lt;strong&gt;scanned&lt;/strong&gt; for all data points (files)&lt;/li&gt; &#xA; &lt;li&gt;The job compares the VectorDB state with data source state to figure out &lt;strong&gt;newly added files, updated files and deleted files&lt;/strong&gt;. The new and updated files are &lt;strong&gt;downloaded&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;The newly added files and updated files are &lt;strong&gt;parsed and chunked&lt;/strong&gt; into smaller pieces each with their own metadata&lt;/li&gt; &#xA; &lt;li&gt;The chunks are &lt;strong&gt;embedded&lt;/strong&gt; using embedding models like &lt;code&gt;text-ada-002&lt;/code&gt; from &lt;code&gt;openai&lt;/code&gt; or &lt;code&gt;mxbai-embed-large-v1&lt;/code&gt; from &lt;code&gt;mixedbread-ai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The embedded chunks are put into VectorDB with auto generated and provided metadata&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ùì&lt;/span&gt; Question-Answering using API Server:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Users sends a request with their query&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It is routed to one of the app&#39;s query controller&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;One or more retrievers are constructed on top of the Vector DB&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then a Question Answering chain / agent is constructed. It embeds the user query and fetches similar chunks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A single shot Question Answering chain just generates an answer given similar chunks. An agent can do multi step reasoning and use many tools before arriving at an answer. In both cases, the API server uses LLM models (like GPT 3.5, GPT 4, etc)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Before returning the answer, the metadata for relevant chunks can be updated with things like presigned urls, surrounding slides, external data source links.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The answer and relevant document chunks are returned in response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In case of agents the intermediate steps can also be streamed. It is up to the specific app to decide.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíª&lt;/span&gt; Code Structure:&lt;/h2&gt; &#xA;&lt;p&gt;Entire codebase lives in &lt;code&gt;backend/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;|-- Dockerfile&#xA;|-- README.md&#xA;|-- __init__.py&#xA;|-- backend/&#xA;|   |-- indexer/&#xA;|   |   |-- __init__.py&#xA;|   |   |-- indexer.py&#xA;|   |   |-- main.py&#xA;|   |   `-- types.py&#xA;|   |-- modules/&#xA;|   |   |-- __init__.py&#xA;|   |   |-- dataloaders/&#xA;|   |   |   |-- __init__.py&#xA;|   |   |   |-- loader.py&#xA;|   |   |   |-- localdirloader.py&#xA;|   |   |   `-- ...&#xA;|   |   |-- embedder/&#xA;|   |   |   |-- __init__.py&#xA;|   |   |   |-- embedder.py&#xA;|   |   |   -- mixbread_embedder.py&#xA;|   |   |   `-- embedding.requirements.txt&#xA;|   |   |-- metadata_store/&#xA;|   |   |   |-- base.py&#xA;|   |   |   |-- client.py&#xA;|   |   |   `-- truefoundry.py&#xA;|   |   |-- parsers/&#xA;|   |   |   |-- __init__.py&#xA;|   |   |   |-- parser.py&#xA;|   |   |   |-- pdfparser_fast.py&#xA;|   |   |   `-- ...&#xA;|   |   |-- query_controllers/&#xA;|   |   |   |-- default/&#xA;|   |   |   |   |-- controller.py&#xA;|   |   |   |   `-- types.py&#xA;|   |   |   |-- query_controller.py&#xA;|   |   |-- reranker/&#xA;|   |   |   |-- mxbai_reranker.py&#xA;|   |   |   |-- reranker.requirements.txt&#xA;|   |   |   `-- ...&#xA;|   |   `-- vector_db/&#xA;|   |       |-- __init__.py&#xA;|   |       |-- base.py&#xA;|   |       |-- qdrant.py&#xA;|   |       `-- ...&#xA;|   |-- requirements.txt&#xA;|   |-- server/&#xA;|   |   |-- __init__.py&#xA;|   |   |-- app.py&#xA;|   |   |-- decorators.py&#xA;|   |   |-- routers/&#xA;|   |   `-- services/&#xA;|   |-- settings.py&#xA;|   |-- types.py&#xA;|   `-- utils.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Customizing the Code for your usecase&lt;/h2&gt; &#xA;&lt;p&gt;Cognita goes by the tagline -&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Everything is available and Everything is customizable.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Cognita makes it really easy to switch between parsers, loaders, models and retrievers.&lt;/p&gt; &#xA;&lt;h3&gt;Customizing Dataloaders:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;You can write your own data loader by inherting the &lt;code&gt;BaseDataLoader&lt;/code&gt; class from &lt;code&gt;backend/modules/dataloaders/loader.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, register the loader in &lt;code&gt;backend/modules/dataloaders/__init__.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Testing a dataloader on localdir, in root dir, copy the following code as &lt;code&gt;test.py&lt;/code&gt; and execute it. We show how to test an existing &lt;code&gt;LocalDirLoader&lt;/code&gt; here:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from backend.modules.dataloaders import LocalDirLoader&#xA;from backend.types import DataSource&#xA;&#xA;data_source = DataSource(&#xA;type=&#34;local&#34;,&#xA;uri=&#34;sample-data/creditcards&#34;,&#xA;)&#xA;&#xA;loader = LocalDirLoader()&#xA;&#xA;&#xA;loaded_data_pts = loader.load_full_data(&#xA;    data_source=data_source,&#xA;    dest_dir=&#34;test/creditcards&#34;,&#xA;)&#xA;&#xA;&#xA;for data_pt in loaded_data_pts:&#xA;    print(data_pt)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Customizing Embedder:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The codebase currently uses &lt;code&gt;OpenAIEmbeddings&lt;/code&gt; you can registered as &lt;code&gt;default&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can register your custom embeddings in &lt;code&gt;backend/modules/embedder/__init__.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;You can also add your own embedder an example of which is given under &lt;code&gt;backend/modules/embedder/mixbread_embedder.py&lt;/code&gt;. It inherits langchain embedding class.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Customizing Parsers:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;You can write your own parser by inherting the &lt;code&gt;BaseParser&lt;/code&gt; class from &lt;code&gt;backend/modules/parsers/parser.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, register the parser in &lt;code&gt;backend/modules/parsers/__init__.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Testing a Parser on a local file, in root dir, copy the following code as &lt;code&gt;test.py&lt;/code&gt; and execute it. Here we show how we can test existing &lt;code&gt;MarkdownParser&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from backend.modules.parsers import MarkdownParser&#xA;&#xA;parser = MarkdownParser()&#xA;chunks =  asyncio.run(&#xA;    parser.get_chunks(&#xA;        filepath=&#34;sample-data/creditcards/diners-club-black.md&#34;,&#xA;    )&#xA;)&#xA;print(chunks)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Adding Custom VectorDB:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To add your own interface for a VectorDB you can inhertit &lt;code&gt;BaseVectorDB&lt;/code&gt; from &lt;code&gt;backend/modules/vector_db/base.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Register the vectordb under &lt;code&gt;backend/modules/vector_db/__init__.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Rerankers:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rerankers are used to sort relavant documents such that top k docs can be used as context effectively reducing the context and prompt in general.&lt;/li&gt; &#xA; &lt;li&gt;Sample reranker is written under &lt;code&gt;backend/modules/reranker/mxbai_reranker.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;span&gt;üí°&lt;/span&gt; Writing your Query Controller (QnA):&lt;/h1&gt; &#xA;&lt;p&gt;Code responsible for implementing the Query interface of RAG application. The methods defined in these query controllers are added routes to your FastAPI server.&lt;/p&gt; &#xA;&lt;h2&gt;Steps to add your custom Query Controller:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your Query controller class in &lt;code&gt;backend/modules/query_controllers/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add &lt;code&gt;query_controller&lt;/code&gt; decorator to your class and pass the name of your custom controller as argument&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-controller.py&#34;&gt;from backend.server.decorator import query_controller&#xA;&#xA;@query_controller(&#34;/my-controller&#34;)&#xA;class MyCustomController():&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add methods to this controller as per your needs and use our http decorators like &lt;code&gt;post, get, delete&lt;/code&gt; to make your methods an API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-controller.py&#34;&gt;from backend.server.decorator import post&#xA;&#xA;@query_controller(&#34;/my-controller&#34;)&#xA;class MyCustomController():&#xA;    ...&#xA;&#xA;    @post(&#34;/answer&#34;)&#xA;    def answer(query: str):&#xA;        # Write code to express your logic for answer&#xA;        # This API will be exposed as POST /my-controller/answer&#xA;        ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Import your custom controller class at &lt;code&gt;backend/modules/query_controllers/__init__.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-__init__.py&#34;&gt;...&#xA;from backend.modules.query_controllers.sample_controller.controller import MyCustomController&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;As an example, we have implemented sample controller in &lt;code&gt;backend/modules/query_controllers/example&lt;/code&gt;. Please refer for better understanding&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;&lt;span&gt;üê≥&lt;/span&gt; Quickstart: Deployment with Truefoundry:&lt;/h1&gt; &#xA;&lt;p&gt;To be able to &lt;strong&gt;Query&lt;/strong&gt; on your own documents, follow the steps below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Register at TrueFoundry, follow &lt;a href=&#34;https://www.truefoundry.com/register&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fill up the form and register as an organization (let&#39;s say &amp;lt;org_name&amp;gt;)&lt;/li&gt; &#xA;   &lt;li&gt;On &lt;code&gt;Submit&lt;/code&gt;, you will be redirected to your dashboard endpoint ie https://&amp;lt;org_name&amp;gt;.truefoundry.cloud&lt;/li&gt; &#xA;   &lt;li&gt;Complete your email verification&lt;/li&gt; &#xA;   &lt;li&gt;Login to the platform at your dashboard endpoint ie. https://&amp;lt;org_name&amp;gt;.truefoundry.cloud&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;&lt;code&gt;Note: Keep your dashboard endpoint handy, we will refer it as &#34;TFY_HOST&#34; and it should have structure like &#34;https://&amp;lt;org_name&amp;gt;.truefoundry.cloud&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Setup a cluster, use TrueFoundry managed for quick setup&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Give a unique name to your &lt;strong&gt;&lt;a href=&#34;https://docs.truefoundry.com/docs/workspace&#34;&gt;Cluster&lt;/a&gt;&lt;/strong&gt; and click on &lt;strong&gt;Launch Cluster&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;It will take few minutes to provision a cluster for you&lt;/li&gt; &#xA;   &lt;li&gt;On &lt;strong&gt;Configure Host Domain&lt;/strong&gt; section, click &lt;code&gt;Register&lt;/code&gt; for the pre-filled IP&lt;/li&gt; &#xA;   &lt;li&gt;Next, &lt;code&gt;Add&lt;/code&gt; a &lt;strong&gt;Docker Registry&lt;/strong&gt; to push your docker images to.&lt;/li&gt; &#xA;   &lt;li&gt;Next, &lt;strong&gt;Deploy a Model&lt;/strong&gt;, you can choose to &lt;code&gt;Skip&lt;/code&gt; this step&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add a &lt;strong&gt;Storage Integration&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;strong&gt;ML Repo&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Navigate to &lt;strong&gt;ML Repo&lt;/strong&gt; tab&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Click on &lt;code&gt;+ New ML Repo&lt;/code&gt; button on top-right&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Give a unique name to your &lt;strong&gt;ML Repo&lt;/strong&gt; (say &#39;docs-qa-llm&#39;)&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Storage Integration&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;On &lt;code&gt;Submit&lt;/code&gt;, your &lt;strong&gt;ML Repo&lt;/strong&gt; will be created&lt;/p&gt; &lt;p&gt;For more details: &lt;a href=&#34;https://docs.truefoundry.com/docs/creating-ml-repo-via-ui&#34;&gt;link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;strong&gt;Workspace&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Navigate to &lt;strong&gt;Workspace&lt;/strong&gt; tab&lt;/li&gt; &#xA;   &lt;li&gt;Click on &lt;code&gt;+ New Workspace&lt;/code&gt; button on top-right&lt;/li&gt; &#xA;   &lt;li&gt;Select your &lt;strong&gt;Cluster&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Give a name to your &lt;strong&gt;Workspace&lt;/strong&gt; (say &#39;docs-qa-llm&#39;)&lt;/li&gt; &#xA;   &lt;li&gt;Enable &lt;strong&gt;ML Repo Access&lt;/strong&gt; and &lt;code&gt;Add ML Repo Access&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Select your &lt;strong&gt;ML Repo&lt;/strong&gt; and role as &lt;strong&gt;Project Admin&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;On &lt;code&gt;Submit&lt;/code&gt;, a new &lt;strong&gt;Workspace&lt;/strong&gt; will be created. You can copy the &lt;strong&gt;Workspace FQN&lt;/strong&gt; by clicking on &lt;strong&gt;FQN&lt;/strong&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;For more details: &lt;a href=&#34;https://docs.truefoundry.com/docs/installation-and-setup#5-creating-workspaces&#34;&gt;link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy &lt;strong&gt;RAG Application&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Navigate to &lt;strong&gt;Deployments&lt;/strong&gt; tab&lt;/li&gt; &#xA;   &lt;li&gt;Click on &lt;code&gt;+ New Deployment&lt;/code&gt; buttton on top-right&lt;/li&gt; &#xA;   &lt;li&gt;Select &lt;code&gt;Application Catalogue&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Select your workspace&lt;/li&gt; &#xA;   &lt;li&gt;Select RAG Application&lt;/li&gt; &#xA;   &lt;li&gt;Fill up the deployment template &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Give your deployment a Name&lt;/li&gt; &#xA;     &lt;li&gt;Add ML Repo&lt;/li&gt; &#xA;     &lt;li&gt;You can either add an existing Qdrant DB or create a new one&lt;/li&gt; &#xA;     &lt;li&gt;By default, &lt;code&gt;main&lt;/code&gt; branch is used for deployment (You will find this option in &lt;code&gt;Show Advance fields&lt;/code&gt;). You can change the branch name and git repository if required. &#xA;      &lt;blockquote&gt; &#xA;       &lt;p&gt;Make sure to re-select the main branch, as the SHA commit, does not get updated automatically.&lt;/p&gt; &#xA;      &lt;/blockquote&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Click on &lt;code&gt;Submit&lt;/code&gt; your application will be deployed.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Using the &lt;strong&gt;RAG UI&lt;/strong&gt;:&lt;/h2&gt; &#xA;&lt;p&gt;The following steps will showcase how to use the cognita UI to query documents:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create Data Source&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Click on &lt;code&gt;Data Sources&lt;/code&gt; tab &lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/datasource.png&#34; alt=&#34;Datasource&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Click &lt;code&gt;+ New Datasource&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Data source type can be either files from local directory, web url, github url or providing Truefoundry artifact FQN. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;E.g: If &lt;code&gt;Localdir&lt;/code&gt; is selected upload files from your machine and click &lt;code&gt;Submit&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Created Data sources list will be available in the Data Sources tab. &lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/list-datasources-in-collection.png&#34; alt=&#34;DataSourceList&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create Collection&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Click on &lt;code&gt;Collections&lt;/code&gt; tab&lt;/li&gt; &#xA;   &lt;li&gt;Click &lt;code&gt;+ New Collection&lt;/code&gt; &lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/adding-collection.png&#34; alt=&#34;collection&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Enter Collection Name&lt;/li&gt; &#xA;   &lt;li&gt;Select Embedding Model&lt;/li&gt; &#xA;   &lt;li&gt;Add earlier created data source and the necessary configuration&lt;/li&gt; &#xA;   &lt;li&gt;Click &lt;code&gt;Process&lt;/code&gt; to create the collection and index the data. &lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/dataingestion-started.png&#34; alt=&#34;ingestionstarted&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;As soon as you create the collection, data ingestion begins, you can view it&#39;s status by selecting your collection in collections tab. You can also add additional data sources later on and index them in the collection. &lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/dataingestion-complete.png&#34; alt=&#34;ingestioncomplete&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Response generation &lt;img src=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/docs/images/response-generation.png&#34; alt=&#34;responsegen&#34;&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Select the collection&lt;/li&gt; &#xA;   &lt;li&gt;Select the LLM and it&#39;s configuration&lt;/li&gt; &#xA;   &lt;li&gt;Select the document retriever&lt;/li&gt; &#xA;   &lt;li&gt;Write the prompt or use the default prompt&lt;/li&gt; &#xA;   &lt;li&gt;Ask the query&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;&lt;span&gt;üíñ&lt;/span&gt; Open Source Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Your contributions are always welcome! Feel free to contribute ideas, feedback, or create issues and bug reports if you find any! Before contributing, please read the &lt;a href=&#34;https://raw.githubusercontent.com/truefoundry/cognita/main/CONTRIBUTIONGUIDE.md&#34;&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;üîÆ&lt;/span&gt; Future developments&lt;/h1&gt; &#xA;&lt;p&gt;Contributions are welcomed for the following upcoming developments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for other vector databases like &lt;code&gt;Chroma&lt;/code&gt;, &lt;code&gt;Weaviate&lt;/code&gt;, etc&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;code&gt;Scalar + Binary Quantization&lt;/code&gt; embeddings.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;code&gt;RAG Evalutaion&lt;/code&gt; of different retrievers.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;code&gt;RAG Visualization&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support for conversational chatbot with context&lt;/li&gt; &#xA; &lt;li&gt;Support for RAG optimized LLMs like &lt;code&gt;stable-lm-3b&lt;/code&gt;, &lt;code&gt;dragon-yi-6b&lt;/code&gt;, etc&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;code&gt;GraphDB&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Star History&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#truefoundry/cognita&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=truefoundry/cognita&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pytorch/torchtune</title>
    <updated>2024-04-19T01:35:05Z</updated>
    <id>tag:github.com,2024-04-19:/pytorch/torchtune</id>
    <link href="https://github.com/pytorch/torchtune" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Native-PyTorch Library for LLM Fine-tuning&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml/badge.svg?branch=main&#34; alt=&#34;Unit Test&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/pytorch/torchtune/actions/workflows/recipe_test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Recipe Integration Test&#34;&gt; &lt;a href=&#34;https://discord.gg/4Xsdn8Rr9Q&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/4Xsdn8Rr9Q?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp; &amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;torchtune now officially supports Meta Llama3! Check out our recipes for Llama3-8B with LoRA, QLoRA and Full fine-tune in the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#llama3&#34;&gt;Llama3&lt;/a&gt; section! üöÄ ü¶ô&lt;/p&gt; &#xA;&lt;h1&gt;torchtune&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#installation&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#get-started&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://pytorch.org/torchtune&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#design-principles&#34;&gt;&lt;strong&gt;Design Principles&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#community-contributions&#34;&gt;&lt;strong&gt;Community Contributions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#license&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;torchtune is a PyTorch-native library for easily authoring, fine-tuning and experimenting with LLMs. We&#39;re excited to announce our alpha release!&lt;/p&gt; &#xA;&lt;p&gt;torchtune provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Native-PyTorch implementations of popular LLMs using composable and modular building blocks&lt;/li&gt; &#xA; &lt;li&gt;Easy-to-use and hackable training recipes for popular fine-tuning techniques (LoRA, QLoRA) - no trainers, no frameworks, just PyTorch!&lt;/li&gt; &#xA; &lt;li&gt;YAML configs for easily configuring training, evaluation, quantization or inference recipes&lt;/li&gt; &#xA; &lt;li&gt;Built-in support for many popular dataset formats and prompt templates to help you quickly get started with training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;torchtune focuses on integrating with popular tools and libraries from the ecosystem. These are just a few examples, with more under development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/hub/en/index&#34;&gt;Hugging Face Hub&lt;/a&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/_cli/download.py&#34;&gt;accessing model weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;EleutherAI&#39;s LM Eval Harness&lt;/a&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/eleuther_eval.py&#34;&gt;evaluating&lt;/a&gt; trained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/datasets/en/index&#34;&gt;Hugging Face Datasets&lt;/a&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/datasets/_instruct.py&#34;&gt;access&lt;/a&gt; to training and evaluation datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/stable/fsdp.html&#34;&gt;PyTorch FSDP&lt;/a&gt; for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch-labs/ao&#34;&gt;torchao&lt;/a&gt; for lower precision dtypes and &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/quantize.py&#34;&gt;post-training quantization&lt;/a&gt; techniques&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; for &lt;a href=&#34;https://pytorch.org/torchtune/stable/deep_dives/wandb_logging.html&#34;&gt;logging&lt;/a&gt; metrics and checkpoints, and tracking training progress&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/executorch-overview&#34;&gt;ExecuTorch&lt;/a&gt; for &lt;a href=&#34;https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning&#34;&gt;on-device inference&lt;/a&gt; using fine-tuned models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/bitsandbytes/main/en/index&#34;&gt;bitsandbytes&lt;/a&gt; for low memory optimizers for our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_full_low_memory.yaml&#34;&gt;single-device recipes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;torchtune currently supports the following models.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sizes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3&#34;&gt;Llama3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama3/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama2/&#34;&gt;Llama2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 70B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama2/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/mistral/_model_builders.py&#34;&gt;model&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/mistral/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b&#34;&gt;Gemma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/gemma/_model_builders.py&#34;&gt;model&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/gemma/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We&#39;ll be adding a number of new models in the coming weeks, including support for 70B versions and MoEs.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuning recipes&lt;/h3&gt; &#xA;&lt;p&gt;torchtune provides the following fine-tuning recipes.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training&lt;/th&gt; &#xA;   &lt;th&gt;Fine-tuning Method&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distributed Training [1 to 8 GPUs]&lt;/td&gt; &#xA;   &lt;td&gt;Full [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/full_finetune_distributed.py&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_full.yaml&#34;&gt;example&lt;/a&gt;], LoRA [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_distributed.py&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_lora.yaml&#34;&gt;example&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single Device / Low Memory [1 GPU]&lt;/td&gt; &#xA;   &lt;td&gt;Full [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/full_finetune_single_device.py&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_full_single_device.yaml&#34;&gt;example&lt;/a&gt;], LoRA + QLoRA [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_single_device.py&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_lora_single_device.yaml&#34;&gt;example&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single Device [1 GPU]&lt;/td&gt; &#xA;   &lt;td&gt;DPO [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/full_finetune_distributed.py&#34;&gt;code&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_lora_dpo_single_device.yaml&#34;&gt;example&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Memory efficiency is important to us. All of our recipes are tested on a variety of setups including commodity GPUs with 24GB of VRAM as well as beefier options found in data centers.&lt;/p&gt; &#xA;&lt;p&gt;Single-GPU recipes expose a number of memory optimizations that aren&#39;t available in the distributed versions. These include support for low-precision optimizers from &lt;a href=&#34;https://huggingface.co/docs/bitsandbytes/main/en/index&#34;&gt;bitsandbytes&lt;/a&gt; and fusing optimizer step with backward to reduce memory footprint from the gradients (see example &lt;a href=&#34;https://github.com/pytorch/torchtune/raw/main/recipes/configs/llama2/7B_full_low_memory.yaml&#34;&gt;config&lt;/a&gt;). For memory-constrained setups, we recommend using the single-device configs as a starting point. For example, our default QLoRA config has a peak memory usage of &lt;code&gt;~9.3GB&lt;/code&gt;. Similarly LoRA on single device with &lt;code&gt;batch_size=2&lt;/code&gt; has a peak memory usage of &lt;code&gt;~17.1GB&lt;/code&gt;. Both of these are with &lt;code&gt;dtype=bf16&lt;/code&gt; and &lt;code&gt;AdamW&lt;/code&gt; as the optimizer.&lt;/p&gt; &#xA;&lt;p&gt;This table captures the minimum memory requirements for our different recipes using the associated configs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Example HW Resources&lt;/th&gt; &#xA;   &lt;th&gt;Finetuning Method&lt;/th&gt; &#xA;   &lt;th&gt;Config&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Peak Memory per GPU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1 x RTX 4090&lt;/td&gt; &#xA;   &lt;td&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_qlora_single_device.yaml&#34;&gt;qlora_finetune_single_device&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-7B&lt;/td&gt; &#xA;   &lt;td&gt;8.57 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2 x RTX 4090&lt;/td&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_lora.yaml&#34;&gt;lora_finetune_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-7B&lt;/td&gt; &#xA;   &lt;td&gt;20.95 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1 x RTX 4090&lt;/td&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_lora_single_device.yaml&#34;&gt;lora_finetune_single_device&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-7B&lt;/td&gt; &#xA;   &lt;td&gt;17.18 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1 x RTX 4090&lt;/td&gt; &#xA;   &lt;td&gt;Full finetune&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_full_low_memory.yaml&#34;&gt;full_finetune_single_device&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-7B&lt;/td&gt; &#xA;   &lt;td&gt;14.97 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4 x RTX 4090&lt;/td&gt; &#xA;   &lt;td&gt;Full finetune&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_full.yaml&#34;&gt;full_finetune_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-7B&lt;/td&gt; &#xA;   &lt;td&gt;22.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;these are averaged over multiple runs, but there might be some variance based on the setup. We&#39;ll update this table regularly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Llama3&lt;/h2&gt; &#xA;&lt;p&gt;torchtune supports fine-tuning for the Llama3 8B models with support for 70B on its way. We currently support LoRA, QLoRA and Full-finetune on a single GPU as well as LoRA and Full fine-tune on multiple devices. For all the details, take a look at our &lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/llama3.html&#34;&gt;tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In our initial experiments, QLoRA has a peak allocated memory of &lt;code&gt;~9GB&lt;/code&gt; while LoRA on a single GPU has a peak allocated memory of &lt;code&gt;~19GB&lt;/code&gt;. To get started, you can use our default configs to kick off training.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LoRA on a single GPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run lora_finetune_single_device --config llama3/8B_lora_single_device&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;QLoRA on a single GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run lora_finetune_single_device --config llama3/8B_qlora_single_device&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LoRA on 2 GPUs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run --nproc_per_node 4 lora_finetune_distributed --config llama3/8B_lora&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full fine-tune on 2 GPUs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run --nproc_per_node 2 full_finetune_distributed --config llama3/8B_full&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; &lt;a href=&#34;ttps://pytorch.org/get-started/locally/&#34;&gt;Install PyTorch&lt;/a&gt;. torchtune is tested with the latest stable PyTorch release (2.2.2) as well as the preview nightly version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; The latest stable version of torchtune is hosted on PyPI and can be downloaded with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torchtune&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To confirm that the package is installed correctly, you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And should see the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;usage: tune [-h] {ls,cp,download,run,validate} ...&#xA;&#xA;Welcome to the TorchTune CLI!&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with fine-tuning your first LLM with torchtune, see our tutorial on &lt;a href=&#34;https://pytorch.org/torchtune/stable/tutorials/first_finetune_tutorial.html&#34;&gt;fine-tuning Llama2 7B&lt;/a&gt;. Our &lt;a href=&#34;https://pytorch.org/torchtune/stable/tutorials/e2e_flow.html&#34;&gt;end-to-end workflow&lt;/a&gt; tutorial will show you how to evaluate, quantize and run inference with this model. The rest of this section will provide a quick overview of these steps with Llama2.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Downloading a model&lt;/h3&gt; &#xA;&lt;p&gt;Follow the instructions on the official &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b&#34;&gt;&lt;code&gt;meta-llama&lt;/code&gt;&lt;/a&gt; repository to ensure you have access to the Llama2 model weights. Once you have confirmed access, you can run the following command to download the weights to your local machine. This will also download the tokenizer model and a responsible use guide.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune download meta-llama/Llama-2-7b-hf \&#xA;--output-dir /tmp/Llama-2-7b-hf \&#xA;--hf-token &amp;lt;HF_TOKEN&amp;gt; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tip: Set your environment variable &lt;code&gt;HF_TOKEN&lt;/code&gt; or pass in &lt;code&gt;--hf-token&lt;/code&gt; to the command in order to validate your access. You can find your token at &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Running fine-tuning recipes&lt;/h3&gt; &#xA;&lt;p&gt;Llama2 7B + LoRA on single GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run lora_finetune_single_device --config llama2/7B_lora_single_device&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For distributed training, tune CLI integrates with &lt;a href=&#34;https://pytorch.org/docs/stable/elastic/run.html&#34;&gt;torchrun&lt;/a&gt;. Llama2 7B + LoRA on two GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run --nproc_per_node 2 full_finetune_distributed --config llama2/7B_full&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tip: Make sure to place any torchrun commands &lt;strong&gt;before&lt;/strong&gt; the recipe specification. Any CLI args after this will override the config and not impact distributed training.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Modify Configs&lt;/h3&gt; &#xA;&lt;p&gt;There are two ways in which you can modify configs:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Config Overrides&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can easily overwrite config properties from the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run lora_finetune_single_device \&#xA;--config llama2/7B_lora_single_device \&#xA;batch_size=8 \&#xA;enable_activation_checkpointing=True \&#xA;max_steps_per_epoch=128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update a Local Copy&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also copy the config to your local directory and modify the contents directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune cp llama2/7B_full ./my_custom_config.yaml&#xA;Copied to ./7B_full.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can run your custom recipe by directing the &lt;code&gt;tune run&lt;/code&gt; command to your local files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run full_finetune_distributed --config ./my_custom_config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Check out &lt;code&gt;tune --help&lt;/code&gt; for all possible CLI commands and options. For more information on using and updating configs, take a look at our &lt;a href=&#34;https://pytorch.org/torchtune/stable/deep_dives/configs.html&#34;&gt;config deep-dive&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Design Principles&lt;/h2&gt; &#xA;&lt;p&gt;torchtune embodies PyTorch‚Äôs design philosophy [&lt;a href=&#34;https://pytorch.org/docs/stable/community/design.html&#34;&gt;details&lt;/a&gt;], especially &#34;usability over everything else&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;Native PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;torchtune is a native-PyTorch library. While we provide integrations with the surrounding ecosystem (eg: Hugging Face Datasets, EleutherAI Eval Harness), all of the core functionality is written in PyTorch.&lt;/p&gt; &#xA;&lt;h3&gt;Simplicity and Extensibility&lt;/h3&gt; &#xA;&lt;p&gt;torchtune is designed to be easy to understand, use and extend.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Composition over implementation inheritance - layers of inheritance for code re-use makes the code hard to read and extend&lt;/li&gt; &#xA; &lt;li&gt;No training frameworks - explicitly outlining the training logic makes it easy to extend for custom use cases&lt;/li&gt; &#xA; &lt;li&gt;Code duplication is preferred over unnecessary abstractions&lt;/li&gt; &#xA; &lt;li&gt;Modular building blocks over monolithic components&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Correctness&lt;/h3&gt; &#xA;&lt;p&gt;torchtune provides well-tested components with a high-bar on correctness. The library will never be the first to provide a feature, but available features will be thoroughly tested. We provide&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extensive unit-tests to ensure component-level numerical parity with reference implementations&lt;/li&gt; &#xA; &lt;li&gt;Checkpoint-tests to ensure model-level numerical parity with reference implementations&lt;/li&gt; &#xA; &lt;li&gt;Integration tests to ensure recipe-level performance parity with reference implementations on standard benchmarks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Community Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We really value our community and the contributions made by our wonderful users. We&#39;ll use this section to call out some of these contributions! If you&#39;d like to help out as well, please see the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/solitude-alive&#34;&gt;@solitude-alive&lt;/a&gt; for adding the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/gemma/&#34;&gt;Gemma 2B model&lt;/a&gt; to torchtune, including recipe changes, numeric validations of the models and recipe correctness&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yechenzhi&#34;&gt;@yechenzhi&lt;/a&gt; for adding &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_dpo_single_device.py&#34;&gt;DPO&lt;/a&gt; to torchtune, including the recipe and config along with correctness checks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The Llama2 code in this repository is inspired by the original &lt;a href=&#34;https://github.com/meta-llama/llama/raw/main/llama/model.py&#34;&gt;Llama2 code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We want to give a huge shout-out to EleutherAI, Hugging Face and Weights &amp;amp; Biases for being wonderful collaborators and for working with us on some of these integrations within torchtune.&lt;/p&gt; &#xA;&lt;p&gt;We also want to acknowledge some awesome libraries and tools from the ecosystem:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;gpt-fast&lt;/a&gt; for performant LLM inference techniques which we&#39;ve adopted OOTB&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/meta-llama/llama-recipes&#34;&gt;llama recipes&lt;/a&gt; for spring-boarding the llama2 community&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; for bringing several memory and performance based techniques to the PyTorch ecosystem&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/winglian/&#34;&gt;@winglian&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;axolotl&lt;/a&gt; for early feedback and brainstorming on torchtune&#39;s design and feature set.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lightning-AI/litgpt&#34;&gt;lit-gpt&lt;/a&gt; for pushing the LLM fine-tuning community forward.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;HF TRL&lt;/a&gt; for making reward modeling more accessible to the PyTorch community.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;torchtune is released under the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/LICENSE&#34;&gt;BSD 3 license&lt;/a&gt;. However you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PixArt-alpha/PixArt-sigma</title>
    <updated>2024-04-19T01:35:05Z</updated>
    <id>tag:github.com,2024-04-19:/PixArt-alpha/PixArt-sigma</id>
    <link href="https://github.com/PixArt-alpha/PixArt-sigma" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PixArt-Œ£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/asset/logo-sigma.png&#34; height=&#34;120&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  üëâ PixArt-Œ£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation&#xA;  &lt;div&gt;&lt;/div&gt;&#xA; &lt;/div&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://pixart-alpha.github.io/PixArt-sigma-project/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Project%20Page&amp;amp;message=Github&amp;amp;color=blue&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://arxiv.org/abs/2403.04692&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Paper&amp;amp;message=Arxiv:Sigma&amp;amp;color=red&amp;amp;logo=arxiv&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://huggingface.co/spaces/PixArt-alpha/PixArt-Sigma&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo&amp;amp;message=HuggingFace&amp;amp;color=yellow&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://discord.gg/rde6eaE5Ta&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Discuss&amp;amp;message=Discord&amp;amp;color=purple&amp;amp;logo=discord&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This repo contains PyTorch model definitions, pre-trained weights and inference/sampling code for our paper exploring Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation. You can find more visualizations on our &lt;a href=&#34;https://pixart-alpha.github.io/PixArt-sigma-project/&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-sigma&#34;&gt;&lt;strong&gt;PixArt-Œ£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://lawrence-cj.github.io/&#34;&gt;Junsong Chen*&lt;/a&gt;, &lt;a href=&#34;https://chongjiange.github.io/&#34;&gt;Chongjian Ge*&lt;/a&gt;, &lt;a href=&#34;https://xieenze.github.io/&#34;&gt;Enze Xie*&lt;/a&gt;‚Ä†, &lt;a href=&#34;https://yuewuhkust.github.io/&#34;&gt;Yue Wu*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=hqDyTg8AAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Lewei Yao&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=3t2j87YAAAAJ&amp;amp;hl=en&#34;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&#34;https://zhongdao.github.io/&#34;&gt;Zhongdao Wang&lt;/a&gt;, &lt;a href=&#34;http://luoping.me/&#34;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=D3nE0agAAAAJ&#34;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=XboZC1AAAAAJ&#34;&gt;Zhenguo Li&lt;/a&gt; &lt;br&gt;Huawei Noah‚Äôs Ark Lab, DLUT, HKU, HKUST&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Welcome everyone to contributeüî•üî•!!&lt;/h2&gt; &#xA;&lt;p&gt;Learning from the previous &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ±&lt;/a&gt; project, we will try to keep this repo as simple as possible so that everyone in the PixArt community can use it.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Breaking News üî•üî•!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(üî• New) Apr. 16, 2024. üí• &lt;a href=&#34;https://huggingface.co/spaces/PixArt-alpha/PixArt-Sigma&#34;&gt;PixArt-Œ£ Online Demo&lt;/a&gt; is available!!&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) Apr. 16, 2024. üí• PixArt-Œ±-DMD One Step Generator &lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/asset/docs/pixart_dmd.md&#34;&gt;training code&lt;/a&gt; are all released!&lt;/li&gt; &#xA; &lt;li&gt;(‚úÖ New) Apr. 11, 2024. üí• &lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#3-pixart-demo&#34;&gt;PixArt-Œ£ Demo&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#2-integration-in-diffusers&#34;&gt;PixArt-Œ£ Pipeline&lt;/a&gt;! PixArt-Œ£ supports &lt;code&gt;üß® diffusers&lt;/code&gt; using &lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/scripts/diffusers_patches.py&#34;&gt;patches&lt;/a&gt; for fast experience!&lt;/li&gt; &#xA; &lt;li&gt;(‚úÖ New) Apr. 10, 2024. üí• PixArt-Œ±-DMD one step sampler &lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/app/app_pixart_dmd.py&#34;&gt;demo code&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Alpha-DMD-XL-2-512x512&#34;&gt;PixArt-Œ±-DMD checkpoint&lt;/a&gt; 512px are released!&lt;/li&gt; &#xA; &lt;li&gt;(‚úÖ New) Apr. 9, 2024. üí• &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma/blob/main/PixArt-Sigma-XL-2-1024-MS.pth&#34;&gt;PixArt-Œ£ checkpoint&lt;/a&gt; 1024px is released!&lt;/li&gt; &#xA; &lt;li&gt;(‚úÖ New) Apr. 6, 2024. üí• &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma/tree/main&#34;&gt;PixArt-Œ£ checkpoint&lt;/a&gt; 256px &amp;amp; 512px are released!&lt;/li&gt; &#xA; &lt;li&gt;(‚úÖ New) Mar. 29, 2024. üí• &lt;a href=&#34;https://pixart-alpha.github.io/PixArt-sigma-project/&#34;&gt;PixArt-Œ£&lt;/a&gt; training &amp;amp; inference code &amp;amp; toy data are released!!!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;p&gt;-Main&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#-compare-with-pixart-%CE%B1&#34;&gt;Weak-to-Strong&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#-how-to-train&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#-how-to-test&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#2-integration-in-diffusers&#34;&gt;Use diffusers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#3-pixart-demo&#34;&gt;Launch Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#-available-models&#34;&gt;Available Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;-Guidance&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/asset/docs/data_feature_extraction.md&#34;&gt;Feature extraction* (Optional)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/asset/docs/pixart_dmd.md&#34;&gt;One step Generation (DMD)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[LCM: coming soon]&lt;/li&gt; &#xA; &lt;li&gt;[ControlNet: coming soon]&lt;/li&gt; &#xA; &lt;li&gt;[ComfyUI: coming soon]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;-Others&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#acknowledgements&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#bibtex&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#to-do-list&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üÜö Compare with &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ±&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;T5 token length&lt;/th&gt; &#xA;   &lt;th&gt;VAE&lt;/th&gt; &#xA;   &lt;th&gt;2K/4K&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-Œ£&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/madebyollin/sdxl-vae-fp16-fix&#34;&gt;SDXL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-Œ±&lt;/td&gt; &#xA;   &lt;td&gt;120&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-ema&#34;&gt;SD1.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sample-1&lt;/th&gt; &#xA;   &lt;th&gt;Sample-2&lt;/th&gt; &#xA;   &lt;th&gt;Sample-3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-Œ£&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma-project/master/static/images/samples/compare_simga_alpha/sample1%CE%A3.webp&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma-project/master/static/images/samples/compare_simga_alpha/sample2%CE%A3.webp&#34; width=&#34;512&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma-project/master/static/images/samples/compare_simga_alpha/sample3%CE%A3.webp&#34; width=&#34;512&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-Œ±&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma-project/master/static/images/samples/compare_simga_alpha/sample1%CE%B1.webp&#34; width=&#34;256&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma-project/master/static/images/samples/compare_simga_alpha/sample2%CE%B1.webp&#34; width=&#34;512&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma-project/master/static/images/samples/compare_simga_alpha/sample3%CE%B1.webp&#34; width=&#34;512&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Prompt&lt;/td&gt; &#xA;   &lt;td&gt;Close-up, gray-haired, bearded man in 60s, observing passersby, in wool coat and &lt;strong&gt;brown beret&lt;/strong&gt;, glasses, cinematic.&lt;/td&gt; &#xA;   &lt;td&gt;Body shot, a French woman, Photography, French Streets background, backlight, rim light, Fujifilm.&lt;/td&gt; &#xA;   &lt;td&gt;Photorealistic closeup video of two pirate ships battling each other as they sail inside &lt;strong&gt;a cup of coffee&lt;/strong&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Prompt Details&lt;/summary&gt;Sample-1 full prompt: An extreme close-up of an gray-haired man with a beard in his 60s, he is deep in thought pondering the history of the universe as he sits at a cafe in Paris, his eyes focus on people offscreen as they walk as he sits mostly motionless, he is dressed in a wool coat suit coat with a button-down shirt , he wears a **brown beret** and glasses and has a very professorial appearance, and the end he offers a subtle closed-mouth smile as if he found the answer to the mystery of life, the lighting is very cinematic with the golden light and the Parisian streets and city in the background, depth of field, cinematic 35mm film.&#xA;&lt;/details&gt; &#xA;&lt;h1&gt;üîß Dependencies and Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.9 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 2.0.1+cu11.7&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n pixart python==3.9.0&#xA;conda activate pixart&#xA;conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;&#xA;git clone https://github.com/PixArt-alpha/PixArt-sigma.git&#xA;cd PixArt-sigma&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üî• How to Train&lt;/h1&gt; &#xA;&lt;h2&gt;1. PixArt Training&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;First of all.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We start a new repo to build a more user friendly and more compatible codebase. The main model structure is the same as PixArt-Œ±, you can still develop your function base on the &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;original repo&lt;/a&gt;. lso, &lt;strong&gt;This repo will support PixArt-alpha in the future&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;br&gt; &lt;strong&gt;Now you can train your model without prior feature extraction&lt;/strong&gt;. We reform the data structure in PixArt-Œ± code base, so that everyone can start to &lt;strong&gt;train &amp;amp; inference &amp;amp; visualize&lt;/strong&gt; at the very beginning without any pain.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;1.1 Downloading the toy dataset&lt;/h3&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://huggingface.co/datasets/PixArt-alpha/pixart-sigma-toy-dataset&#34;&gt;toy dataset&lt;/a&gt; first. The dataset structure for training is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ./pixart-sigma-toy-dataset&#xA;&#xA;Dataset Structure&#xA;‚îú‚îÄ‚îÄInternImgs/  (images are saved here)&#xA;‚îÇ  ‚îú‚îÄ‚îÄ000000000000.png&#xA;‚îÇ  ‚îú‚îÄ‚îÄ000000000001.png&#xA;‚îÇ  ‚îú‚îÄ‚îÄ......&#xA;‚îú‚îÄ‚îÄInternData/&#xA;‚îÇ  ‚îú‚îÄ‚îÄdata_info.json    (meta data)&#xA;Optional(üëá)&#xA;‚îÇ  ‚îú‚îÄ‚îÄimg_sdxl_vae_features_1024resolution_ms_new    (run tools/extract_caption_feature.py to generate caption T5 features, same name as images except .npz extension)&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ000000000000.npy&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ000000000001.npy&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ......&#xA;‚îÇ  ‚îú‚îÄ‚îÄcaption_features_new&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ000000000000.npz&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ000000000001.npz&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ......&#xA;‚îÇ  ‚îú‚îÄ‚îÄsharegpt4v_caption_features_new    (run tools/extract_caption_feature.py to generate caption T5 features, same name as images except .npz extension)&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ000000000000.npz&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ000000000001.npz&#xA;‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ......&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;1.2 Download pretrained checkpoint&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# SDXL-VAE, T5 checkpoints&#xA;git lfs install&#xA;git clone https://huggingface.co/PixArt-alpha/pixart_sigma_sdxlvae_T5_diffusers output/pretrained_models/pixart_sigma_sdxlvae_T5_diffusers&#xA;&#xA;# PixArt-Sigma checkpoints&#xA;python tools/download.py # environment eg. HF_ENDPOINT=https://hf-mirror.com can use for HuggingFace mirror&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;1.3 You are ready to train!&lt;/h3&gt; &#xA;&lt;p&gt;Selecting your desired config file from &lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/configs/pixart_sigma_config&#34;&gt;config files dir&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m torch.distributed.launch --nproc_per_node=1 --master_port=12345 \&#xA;          train_scripts/train.py \&#xA;          configs/pixart_sigma_config/PixArt_sigma_xl2_img512_internalms.py \&#xA;          --load-from output/pretrained_models/PixArt-Sigma-XL-2-512-MS.pth&#xA;          --work-dir output/your_first_pixart-exp \&#xA;          --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üíª How to Test&lt;/h1&gt; &#xA;&lt;h2&gt;1. Quick start with &lt;a href=&#34;https://www.gradio.app/guides/quickstart&#34;&gt;Gradio&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To get started, first install the required dependencies. Make sure you&#39;ve downloaded the checkpoint files from &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma&#34;&gt;models(coming soon)&lt;/a&gt; to the &lt;code&gt;output/pretrained_models&lt;/code&gt; folder, and then run on your local machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# SDXL-VAE, T5 checkpoints&#xA;git lfs install&#xA;git clone https://huggingface.co/PixArt-alpha/pixart_sigma_sdxlvae_T5_diffusers output/pixart_sigma_sdxlvae_T5_diffusers&#xA;&#xA;# PixArt-Sigma checkpoints&#xA;python tools/download.py&#xA;&#xA;# demo launch&#xA;python scripts/interface.py --model_path output/pretrained_models/PixArt-Sigma-XL-2-512-MS.pth --image_size 512 --port 11223&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Integration in diffusers&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; Upgrade your &lt;code&gt;diffusers&lt;/code&gt; to make the &lt;code&gt;PixArtSigmaPipeline&lt;/code&gt; available!&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/diffusers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import Transformer2DModel&#xA;from scripts.diffusers_patches import pixart_sigma_init_patched_inputs, PixArtSigmaPipeline&#xA;&#xA;assert getattr(Transformer2DModel, &#39;_init_patched_inputs&#39;, False), &#34;Need to Upgrade diffusers: pip install git+https://github.com/huggingface/diffusers&#34;&#xA;setattr(Transformer2DModel, &#39;_init_patched_inputs&#39;, pixart_sigma_init_patched_inputs)&#xA;device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;weight_dtype = torch.float16&#xA;&#xA;transformer = Transformer2DModel.from_pretrained(&#xA;    &#34;PixArt-alpha/PixArt-Sigma-XL-2-1024-MS&#34;, &#xA;    subfolder=&#39;transformer&#39;, &#xA;    torch_dtype=weight_dtype,&#xA;    use_safetensors=True,&#xA;)&#xA;pipe = PixArtSigmaPipeline.from_pretrained(&#xA;    &#34;PixArt-alpha/pixart_sigma_sdxlvae_T5_diffusers&#34;,&#xA;    transformer=transformer,&#xA;    torch_dtype=weight_dtype,&#xA;    use_safetensors=True,&#xA;)&#xA;pipe.to(device)&#xA;&#xA;# Enable memory optimizations.&#xA;# pipe.enable_model_cpu_offload()&#xA;&#xA;prompt = &#34;A small cactus with a happy face in the Sahara desert.&#34;&#xA;image = pipe(prompt).images[0]&#xA;image.save(&#34;./catcus.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. PixArt Demo&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/diffusers&#xA;&#xA;# PixArt-Sigma&#xA;DEMO_PORT=12345 python app/app_pixart_sigma.py&#xA;&#xA;# PixArt-Sigma One step Sampler(DMD)&#xA;DEMO_PORT=12345 python app/app_pixart_dmd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s have a look at a simple example using the &lt;code&gt;http://your-server-ip:12345&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Convert .pth checkpoint into diffusers version&lt;/h2&gt; &#xA;&lt;p&gt;Directly download from &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma-XL-2-1024-MS&#34;&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;or run with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/diffusers&#xA;&#xA;python tools/convert_pixart_to_diffusers.py --orig_ckpt_path output/pretrained_models/PixArt-Sigma-XL-2-1024-MS.pth --dump_path output/pretrained_models/PixArt-Sigma-XL-2-1024-MS --only_transformer=True --image_size=1024 --version sigma&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;‚è¨ Available Models&lt;/h1&gt; &#xA;&lt;p&gt;All models will be automatically downloaded &lt;a href=&#34;https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/#12-download-pretrained-checkpoint&#34;&gt;here&lt;/a&gt;. You can also choose to download manually from this &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma&#34;&gt;url&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Checkpoint path&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Download in OpenXLab&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;T5 &amp;amp; SDXL-VAE&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4.5B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Diffusers: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/pixart_sigma_sdxlvae_T5_diffusers&#34;&gt;pixart_sigma_sdxlvae_T5_diffusers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;coming soon&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PixArt-Œ£-256&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.6B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pth: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma/blob/main/PixArt-Sigma-XL-2-256x256.pth&#34;&gt;PixArt-Sigma-XL-2-256x256.pth&lt;/a&gt; &lt;br&gt; Diffusers: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma-XL-2-256x256&#34;&gt;PixArt-Sigma-XL-2-256x256&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;coming soon&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PixArt-Œ£-512&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.6B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pth: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma/blob/main/PixArt-Sigma-XL-2-512-MS.pth&#34;&gt;PixArt-Sigma-XL-2-512-MS.pth&lt;/a&gt; &lt;br&gt; Diffusers: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma-XL-2-512-MS&#34;&gt;PixArt-Sigma-XL-2-512-MS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;coming soon&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PixArt-Œ±-512-DMD&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.6B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Diffusers: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Alpha-DMD-XL-2-512x512&#34;&gt;PixArt-Alpha-DMD-XL-2-512x512&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;coming soon&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PixArt-Œ£-1024&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.6B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;pth: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma/blob/main/PixArt-Sigma-XL-2-1024-MS.pth&#34;&gt;PixArt-Sigma-XL-2-1024-MS.pth&lt;/a&gt; &lt;br&gt; Diffusers: &lt;a href=&#34;https://huggingface.co/PixArt-alpha/PixArt-Sigma-XL-2-1024-MS&#34;&gt;PixArt-Sigma-XL-2-1024-MS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;coming soon&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üí™To-Do List&lt;/h2&gt; &#xA;&lt;p&gt;We will try our best to release&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference code of One Step Sampling with &lt;a href=&#34;https://arxiv.org/abs/2311.18828&#34;&gt;DMD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Model zoo (256/512/1024)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Diffusers (for fast experience)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training code of One Step Sampling with &lt;a href=&#34;https://arxiv.org/abs/2311.18828&#34;&gt;DMD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Diffusers (stable official version)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Model zoo (2K)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;ü§óAcknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ±&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt; and &lt;a href=&#34;https://github.com/Zeqiang-Lai/OpenDMD&#34;&gt;OpenDMD&lt;/a&gt; for their wonderful work and codebase!&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;Diffusers&lt;/a&gt; for their wonderful technical support and awesome collaboration!&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/huggingface&#34;&gt;Hugging Face&lt;/a&gt; for sponsoring the nicely demo!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìñBibTeX&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{chen2024pixartsigma,&#xA;  title={PixArt-\Sigma: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation},&#xA;  author={Junsong Chen and Chongjian Ge and Enze Xie and Yue Wu and Lewei Yao and Xiaozhe Ren and Zhongdao Wang and Ping Luo and Huchuan Lu and Zhenguo Li},&#xA;  year={2024},&#xA;  eprint={2403.04692},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>