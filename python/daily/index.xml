<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-03T01:45:59Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jianzhnie/Chinese-Guanaco</title>
    <updated>2023-06-03T01:45:59Z</updated>
    <id>tag:github.com,2023-06-03:/jianzhnie/Chinese-Guanaco</id>
    <link href="https://github.com/jianzhnie/Chinese-Guanaco" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‰∏≠ÊñáGuanaco(ÂéüÈ©º)Â§ßËØ≠Ë®ÄÊ®°Âûã QLora ÈáèÂåñËÆ≠ÁªÉ +Êú¨Âú∞CPU/GPUÈÉ®ÁΩ≤ (Chinese Guanaco QLoRA: Efficient Finetuning of Quantized LLMs)&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/jianzhnie/Chinese-Guanaco/main/assets/guanaco.svg?sanitize=true&#34; width=&#34;300&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Chinese-Guanaco: Efficient Finetuning of Quantized LLMs for Chinese ‚Äî‚Äî ‰∏Ä‰∏™‰∏≠Êñá‰ΩéËµÑÊ∫êÁöÑÈáèÂåñËÆ≠ÁªÉ/ÈÉ®ÁΩ≤ÊñπÊ°à&lt;/h1&gt; &#xA;&lt;p&gt;This is the repo for the Chinese-Guanaco project, which aims to build and share instruction-following Chinese LLaMA/Pythia/GLM model tuning methods which can be trained on &lt;strong&gt;a single Nvidia RTX-2080TI&lt;/strong&gt;, multi-round chatbot which can be trained on &lt;strong&gt;a single Nvidia RTX-3090&lt;/strong&gt; with the context len 2048.&lt;/p&gt; &#xA;&lt;p&gt;Chinese-Guanaco uses &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; for quantization and is integrated with Huggingface&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; libraries.&lt;/p&gt; &#xA;&lt;p&gt;The repo contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;code for finetune the model&lt;/li&gt; &#xA; &lt;li&gt;code for generation based on trained model&lt;/li&gt; &#xA; &lt;li&gt;code for run on CPU (fp16 or int4 is support, in purely C++)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. We release all of our models and code, including CUDA kernels for 4-bit training.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To load models in 4bits with transformers and bitsandbytes, you have to install accelerate and transformers from source and make sure you have the latest version of the bitsandbytes library (0.39.0). You can achieve the above with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -q -U bitsandbytes&#xA;pip install -q -U git+https://github.com/huggingface/transformers.git&#xA;pip install -q -U git+https://github.com/huggingface/peft.git&#xA;pip install -q -U git+https://github.com/huggingface/accelerate.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h2&gt;QLora int8 Finetune&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora_int8_finetune.py \&#xA;    --model_name_or_path  decapoda-research/llama-7b-hf  \&#xA;    --data_path tatsu-lab/alpaca  \&#xA;    --output_dir work_dir_lora/ \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 500 \&#xA;    --save_total_limit 5 \&#xA;    --learning_rate 1e-4 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --model_max_length 2048 \&#xA;    --logging_steps 1 \&#xA;    --fp16 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;QLora int4 Finetune&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;qlora_int4_finetune.py&lt;/code&gt; code is a starting point for finetuning and inference on various datasets. Basic command for finetuning a baseline model on the Alpaca dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora_int4_finetune.py --model_name_or_path &amp;lt;path_or_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For models larger than 13B, we recommend adjusting the learning rate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora_int4_finetune.py ‚Äìlearning_rate 0.0001 --model_name_or_path &amp;lt;path_or_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can also tweak our hyperparameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python qlora_int4_finetune.py \&#xA;    --model_name_or_path huggyllama/llama-7b \&#xA;    --output_dir ./output/guanaco-7b \&#xA;    --logging_steps 10 \&#xA;    --save_strategy steps \&#xA;    --data_seed 42 \&#xA;    --save_steps 500 \&#xA;    --save_total_limit 40 \&#xA;    --evaluation_strategy steps \&#xA;    --eval_dataset_size 1024 \&#xA;    --max_eval_samples 1000 \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --max_new_tokens 32 \&#xA;    --dataloader_num_workers 3 \&#xA;    --group_by_length \&#xA;    --logging_strategy steps \&#xA;    --remove_unused_columns False \&#xA;    --do_train \&#xA;    --do_eval \&#xA;    --do_mmlu_eval \&#xA;    --lora_r 64 \&#xA;    --lora_alpha 16 \&#xA;    --lora_modules all \&#xA;    --double_quant \&#xA;    --quant_type nf4 \&#xA;    --bf16 \&#xA;    --bits 4 \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type constant \&#xA;    --gradient_checkpointing \&#xA;    --dataset oasst1 \&#xA;    --source_max_len 16 \&#xA;    --target_max_len 512 \&#xA;    --per_device_train_batch_size 1 \&#xA;    --gradient_accumulation_steps 16 \&#xA;    --max_steps 1875 \&#xA;    --eval_steps 187 \&#xA;    --learning_rate 0.0002 \&#xA;    --adam_beta2 0.999 \&#xA;    --max_grad_norm 0.3 \&#xA;    --lora_dropout 0.1 \&#xA;    --weight_decay 0.0 \&#xA;    --seed 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To find more scripts for finetuning and inference, please refer to the &lt;code&gt;scripts&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;p&gt;Quantization parameters are controlled from the &lt;code&gt;BitsandbytesConfig&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig&#34;&gt;see HF documenation&lt;/a&gt;) as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Loading in 4 bits is activated through &lt;code&gt;load_in_4bit&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The datatype used for the linear layer computations with &lt;code&gt;bnb_4bit_compute_dtype&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nested quantization is activated through &lt;code&gt;bnb_4bit_use_double_quant&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The datatype used for qunatization is specified with &lt;code&gt;bnb_4bit_quant_type&lt;/code&gt;. Note that there are two supported quantization datatypes &lt;code&gt;fp4&lt;/code&gt; (four bit float) and &lt;code&gt;nf4&lt;/code&gt; (normal four bit float). The latter is theoretically optimal for normally distributed weights and we recommend using &lt;code&gt;nf4&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    model = AutoModelForCausalLM.from_pretrained(&#xA;        model_name_or_path=&#39;/name/or/path/to/your/model&#39;,&#xA;        load_in_4bit=True,&#xA;        device_map=&#39;auto&#39;,&#xA;        max_memory=max_memory,&#xA;        torch_dtype=torch.bfloat16,&#xA;        quantization_config=BitsAndBytesConfig(&#xA;            load_in_4bit=True,&#xA;            bnb_4bit_compute_dtype=torch.bfloat16,&#xA;            bnb_4bit_use_double_quant=True,&#xA;            bnb_4bit_quant_type=&#39;nf4&#39;&#xA;        ),&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tutorials and Demonstrations&lt;/h2&gt; &#xA;&lt;p&gt;We provide two Google Colab notebooks to demonstrate the use of 4bit models in inference and fine-tuning. These notebooks are intended to be a starting point for further research and development.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing&#34;&gt;Basic usage Google Colab notebook&lt;/a&gt; - This notebook shows how to use 4bit models in inference with all their variants, and how to run GPT-neo-X (a 20B parameter model) on a free Google Colab instance ü§Ø&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing&#34;&gt;Fine tuning Google Colab notebook&lt;/a&gt; - This notebook shows how to fine-tune a 4bit model on a downstream task using the Hugging Face ecosystem. We show that it is possible to fine tune GPT-neo-X 20B on a Google Colab instance!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using Local Datasets&lt;/h2&gt; &#xA;&lt;p&gt;You can specify the path to your dataset using the --dataset argument. If the --dataset_format argument is not set, it will default to the Alpaca format. Here are a few examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Training with an alpaca format dataset:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python qlora_int4_finetune.py --dataset=&#34;path/to/your/dataset&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Training with a self-instruct format dataset:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python qlora_int4_finetune.py --dataset=&#34;path/to/your/dataset&#34; --dataset_format=&#34;self-instruct&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Multi GPU&lt;/h2&gt; &#xA;&lt;p&gt;Multi GPU training and inference work out-of-the-box with Hugging Face&#39;s Accelerate. Note that the per_device_train_batch_size and per_device_eval_batch_size arguments are global batch sizes unlike what their name suggest.&lt;/p&gt; &#xA;&lt;p&gt;When loading a model for training or inference on multiple GPUs you should pass something like the following to AutoModelForCausalLM.from_pretrained():&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;device_map = &#34;auto&#34;&#xA;max_memory = {i: &#39;46000MB&#39; for i in range(torch.cuda.device_count())}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;This file reads the foundation model from the Hugging Face model hub and the LoRA weights from &lt;code&gt;path/to/your/model_dir&lt;/code&gt;, and runs a Gradio interface for inference on a specified input. Users should treat this as example code for the use of the model, and modify it as needed.&lt;/p&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate_server.py \&#xA;    --model_name_or_path decapoda-research/llama-7b-hf \&#xA;    --lora_model_name_or_path  `path/to/your/model_dir`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sample Outputs&lt;/h2&gt; &#xA;&lt;p&gt;We provide generations for the models described in the paper for both OA and Vicuna queries in the &lt;code&gt;eval/generations&lt;/code&gt; folder. These are intended to foster further research on model evaluation and analysis.&lt;/p&gt; &#xA;&lt;p&gt;Can you distinguish ChatGPT from Guanaco? Give it a try! You can access &lt;a href=&#34;https://colab.research.google.com/drive/1kK6xasHiav9nhiRUJjPMZb4fAED4qRHb?usp=sharing&#34;&gt;the model response Colab here&lt;/a&gt; comparing ChatGPT and Guanaco 65B on Vicuna prompts.&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues and Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Here a list of known issues and bugs. If your issue is not reported here, please open a new issue and describe the problem.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;4-bit inference is slow. Currently, our 4-bit inference implementation is not yet integrated with the 4-bit matrix multiplication&lt;/li&gt; &#xA; &lt;li&gt;Resuming a LoRA training run with the Trainer currently runs on an error&lt;/li&gt; &#xA; &lt;li&gt;Currently, using &lt;code&gt;bnb_4bit_compute_type=&#39;fp16&#39;&lt;/code&gt; can lead to instabilities. For 7B LLaMA, only 80% of finetuning runs complete without error. We have solutions, but they are not integrated yet into bitsandbytes.&lt;/li&gt; &#xA; &lt;li&gt;Make sure that &lt;code&gt;tokenizer.bos_token_id = 1&lt;/code&gt; to avoid generation issues.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;Chinese-Guanaco&lt;/code&gt; is released under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank the Huggingface team, in particular Younes Belkada, for their support integrating QLoRA with PEFT and transformers libraries.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate the work by many open-source contributors, especially:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora/&#34;&gt;Alpaca-LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LoRA/&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/&#34;&gt;Stanford Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama/&#34;&gt;LLaMa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat/&#34;&gt;Vicuna&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{Chinese-Guanaco,&#xA;  author = {jianzhnie},&#xA;  title = {Chinese-Guanaco: Efficient Finetuning of Quantized LLMs for Chinese},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/jianzhnie/Chinese-Guanaco}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>openai/prm800k</title>
    <updated>2023-06-03T01:45:59Z</updated>
    <id>tag:github.com,2023-06-03:/openai/prm800k</id>
    <link href="https://github.com/openai/prm800k" rel="alternate"></link>
    <summary type="html">&lt;p&gt;800,000 step-level correctness labels on LLM solutions to MATH problems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PRM800K: A Process Supervision Dataset&lt;/h1&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://openai.com/research/improving-mathematical-reasoning-with-process-supervision&#34;&gt;[Blog Post]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.20050&#34;&gt;[Paper]&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This repository accompanies the paper &lt;a href=&#34;https://arxiv.org/abs/2305.20050&#34;&gt;Let&#39;s Verify Step by Step&lt;/a&gt; and presents the PRM800K dataset introduced there. PRM800K is a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the &lt;a href=&#34;https://github.com/hendrycks/math&#34;&gt;MATH&lt;/a&gt; dataset. More information on PRM800K and the project can be found in the paper.&lt;/p&gt; &#xA;&lt;p&gt;We are releasing the raw labels as well as the instructions we gave labelers during phase 1 and phase 2 of the project. Example labels can be seen in the image below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/prm800k/main/prm800k/img/interface.png&#34; height=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;data/&lt;/code&gt; folder contains our labels formatted as newline-delimited lists of &lt;code&gt;json&lt;/code&gt; data. The data has been uploaded with &lt;a href=&#34;https://git-lfs.com/&#34;&gt;Git LFS&lt;/a&gt;, which you&#39;ll need to install in order to properly clone the repository.&lt;/p&gt; &#xA;&lt;p&gt;Each line represents 1 full solution sample and can contain many step-level labels. Here is one annotated line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{&#xA;  // UUID representing a particular labeler.&#xA;  &#34;labeler&#34;: &#34;340d89bc-f5b7-45e9-b272-909ba68ee363&#34;,&#xA;&#xA;  // The timestamp this trajectory was submitted.&#xA;  &#34;timestamp&#34;: &#34;2023-01-22T04:34:27.052924&#34;,&#xA;&#xA;  // In phase 2, we split our data collection into generations, using our best&#xA;  // PRM so far to pick which solutions to score in the next generation.&#xA;  // In phase 1, this value should always be null.&#xA;  &#34;generation&#34;: 9,&#xA;&#xA;  // In each generation, we reserve some solutions for quality control. We serve&#xA;  // these solutions to every labeler, and check that they agree with our&#xA;  // gold labels.&#xA;  &#34;is_quality_control_question&#34;: false,&#xA;&#xA;  // generation -1 was reserved for a set of 30 questions we served every&#xA;  // labeler in order to screen for base task performance.&#xA;  &#34;is_initial_screening_question&#34;: false,&#xA;&#xA;  // Metadata about the question this solution is a response to.&#xA;  &#34;question&#34;: {&#xA;    // Text of the MATH problem being solved.&#xA;    &#34;problem&#34;: &#34;What is the greatest common factor of $20 !$ and $200,\\!000$?  (Reminder: If $n$ is a positive integer, then $n!$ stands for the product $1\\cdot 2\\cdot 3\\cdot \\cdots \\cdot (n-1)\\cdot n$.)&#34;,&#xA;    // Ground truth solution from the MATH dataset.&#xA;    &#34;ground_truth_solution&#34;: &#34;The prime factorization of $200,000$ is $2^6 \\cdot 5^5$. Then count the number of factors of $2$ and $5$ in $20!$. Since there are $10$ even numbers, there are more than $6$ factors of $2$. There are $4$ factors of $5$. So the greatest common factor is $2^6 \\cdot 5^4=\\boxed{40,\\!000}$.&#34;,&#xA;    // Ground truth answer.&#xA;    &#34;ground_truth_answer&#34;: &#34;40,\\!000&#34;,&#xA;&#xA;    // The full steps of the model-generated solution. This is only set for&#xA;    // phase 2 where we pre-generated all solutions that we labeled.&#xA;    &#34;pre_generated_steps&#34;: [&#xA;      &#34;I want to find the largest positive integer that divides both $20 !$ and $200,\\!000$ evenly.&#34;,&#xA;      &#34;One way to do this is to factor both numbers into prime factors and look for the common ones.&#34;,&#xA;      &#34;I know that $200,\\!000 = 2^5\\cdot 10^4 = 2^9\\cdot 5^4$.&#34;,&#xA;      &#34;To find the prime factorization of $20 !$, I can use the fact that it is the product of all the positive integers from $1$ to $20$.&#34;,&#xA;      &#34;For each prime number $p$ between $1$ and $20$, I can count how many multiples of $p$ are in that range.&#34;,&#xA;      &#34;For example, there are $10$ multiples of $2$ between $1$ and $20$, namely $2, 4, 6, \\dots, 20$.&#34;,&#xA;      &#34;But there are also $5$ multiples of $4$, which is $2^2$, and $2$ multiples of $8$, which is $2^3$, and $1$ multiple of $16$, which is $2^4$.&#34;,&#xA;      &#34;So, the total power of $2$ in $20 !$ is $10 + 5 + 2 + 1 = 18$.&#34;,&#xA;      &#34;Similarly, there are $4$ multiples of $5$, namely $5, 10, 15, 20$, so the power of $5$ in $20 !$ is $4$.&#34;,&#xA;      &#34;There are $6$ multiples of $3$, namely $3, 6, 9, \\dots, 18$, but there are also $2$ multiples of $9$, which is $3^2$, so the power of $3$ in $20 !$ is $6 + 2 = 8$.&#34;,&#xA;      &#34;There are $2$ multiples of $7$, namely $7$ and $14$, so the power of $7$ in $20 !$ is $2$.&#34;,&#xA;      &#34;There are $1$ multiple of each of the other prime numbers $11, 13, 17$, and $19$, so the powers of those primes in $20 !$ are $1$ each.&#34;,&#xA;      &#34;Therefore, the prime factorization of $20 !$ is $2^{18}\\cdot 3^8\\cdot 5^4\\cdot 7^2\\cdot 11\\cdot 13\\cdot 17\\cdot 19$.&#34;,&#xA;      &#34;To find the greatest common factor of $20 !$ and $200,\\!000$, I need to take the lowest power of each common prime factor.&#34;,&#xA;      &#34;The only common prime factors are $2$ and $5$, and the lowest powers are $9$ and $4$, respectively.&#34;,&#xA;      &#34;So, the greatest common factor is $2^9\\cdot 5^4 = 512\\cdot 625 = 320,\\!000$.\n\n# Answer\n\n320,000&#34;&#xA;    ],&#xA;    // The answer given as the end of the pre-generated solution. We can see&#xA;    // this solution is incorrect.&#xA;    &#34;pre_generated_answer&#34;: &#34;320,000&#34;,&#xA;    // The score given by our PRM to this solution. This one isn&#39;t rated very&#xA;    // highly!&#xA;    &#34;pre_generated_verifier_score&#34;: 0.010779580529581414&#xA;  },&#xA;&#xA;  // The human data we collected for this solution, containing correctness&#xA;  // labels for each step of the solution.&#xA;  &#34;label&#34;: {&#xA;    &#34;steps&#34;: [&#xA;      // Each object here represents labels for one step of the solution.&#xA;      {&#xA;        // Each step will contain one or more completions. These are candidate&#xA;        // steps the model output at this step of the trajectory. In phase 1,&#xA;        // we frequently collect labels on alternative steps, while in phase 2&#xA;        // we only collect labels on alternative steps after the first mistake,&#xA;        // so most completions lists are singletons.&#xA;        &#34;completions&#34;: [&#xA;          {&#xA;            // Text of the step.&#xA;            &#34;text&#34;: &#34;I want to find the largest positive integer that divides both $20 !$ and $200,\\!000$ evenly.&#34;,&#xA;            // The rating the labeler gave to this step. Can be -1, 0, or +1.&#xA;            // This is a 0 because it isn&#39;t incorrect, but it does not make&#xA;            // any progress.&#xA;            &#34;rating&#34;: 0,&#xA;            // The labeler can flag steps that they don&#39;t know how to label.&#xA;            // This is rarely used.&#xA;            &#34;flagged&#34;: null&#xA;          }&#xA;        ],&#xA;        // In phase 1, if all completions were rated -1, we allowed labelers to&#xA;        // write their own +1 step. This is null for all steps in phase 2.&#xA;        &#34;human_completion&#34;: null,&#xA;        // The index of the completion &#34;chosen&#34; at this step, or null if the&#xA;        // human_completion was used. You can reconstruct the solution&#xA;        // trajectory like:&#xA;        // [&#xA;        //     step[&#34;human_completion&#34;] if step[&#34;chosen_completion&#34;] is None&#xA;        //     else step[&#34;completions&#34;][step[&#34;chosen_completion&#34;]][&#34;text&#34;]&#xA;        //     for step in labeled_solution[&#34;label&#34;][&#34;steps&#34;]&#xA;        // ]&#xA;        &#34;chosen_completion&#34;: 0&#xA;      },&#xA;      {&#xA;        &#34;completions&#34;: [&#xA;          {&#xA;            &#34;text&#34;: &#34;One way to do this is to factor both numbers into prime factors and look for the common ones.&#34;,&#xA;            &#34;rating&#34;: 0,&#xA;            &#34;flagged&#34;: null&#xA;          }&#xA;        ],&#xA;        &#34;human_completion&#34;: null,&#xA;        &#34;chosen_completion&#34;: 0&#xA;      },&#xA;      {&#xA;        // Some steps contain multiple alternative completions, and each one&#xA;        // gets a rating.&#xA;        &#34;completions&#34;: [&#xA;          {&#xA;            &#34;text&#34;: &#34;I know that $200,\\!000 = 2^5\\cdot 10^4 = 2^9\\cdot 5^4$.&#34;,&#xA;            &#34;rating&#34;: -1,&#xA;            &#34;flagged&#34;: null&#xA;          },&#xA;          {&#xA;            &#34;text&#34;: &#34;To factor $20 !$, I can use the fact that every factorial is a multiple of every number less than or equal to it.&#34;,&#xA;            &#34;rating&#34;: 0,&#xA;            &#34;flagged&#34;: false&#xA;          },&#xA;          {&#xA;            &#34;text&#34;: &#34;I can use a factor tree to find the prime factors of $200,\\!000$: $200,\\!000 = 2^5\\cdot 10^4 = 2^5\\cdot 2^4\\cdot 5^4 = 2^9\\cdot 5^4$.&#34;,&#xA;            &#34;rating&#34;: -1,&#xA;            &#34;flagged&#34;: false&#xA;          },&#xA;          {&#xA;            &#34;text&#34;: &#34;I can use a factor tree to find the prime factors of $200,\\!000$.&#34;,&#xA;            &#34;rating&#34;: 0,&#xA;            &#34;flagged&#34;: false&#xA;          },&#xA;          {&#xA;            &#34;text&#34;: &#34;To factor $20 !$, I can use the fact that any factorial is divisible by all the primes less than or equal to the input.&#34;,&#xA;            &#34;rating&#34;: 0,&#xA;            &#34;flagged&#34;: false&#xA;          }&#xA;        ],&#xA;        &#34;human_completion&#34;: null,&#xA;        &#34;chosen_completion&#34;: null&#xA;      }&#xA;    ],&#xA;    // Total time in milliseconds spent on labeling this solution.&#xA;    &#34;total_time&#34;: 278270,&#xA;    // Final result of labeling this solution. Will be one of:&#xA;    //   - &#34;found_error&#34;: In phase 2 we stop labeling a solution after the&#xA;    //                    first error is found.&#xA;    //   - &#34;solution&#34;: We reached a step that concluded in the correct answer&#xA;    //                 to the problem.&#xA;    //   - &#34;bad_problem&#34;: The labeler reported the problem as broken.&#xA;    //   - &#34;give_up&#34;: The labeler was stuck (the problem was taking too long,&#xA;    //                or the instructions were unclear) and moved onto the&#xA;    //                next problem.&#xA;    &#34;finish_reason&#34;: &#34;found_error&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;instructions/&lt;/code&gt; folder contains the instructions documents we gave to labelers during each phase of the project.&lt;/p&gt; &#xA;&lt;h2&gt;Answer Grading&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;grading/&lt;/code&gt; folder contains the python grading logic we used for determining if a model-outputted answer correctly matched the ground truth answer in Hendrycks&#39; MATH dataset. We build off of Hendrycks&#39; math normalization logic in &lt;code&gt;math_normalize.py&lt;/code&gt; and use sympy to check for equality of expressions in &lt;code&gt;grader.py&lt;/code&gt;. We recommend using &lt;code&gt;grader.grade_answer(model_answer, gt_answer)&lt;/code&gt; where both answers are strings to determine if a solution is correct or not.&lt;/p&gt; &#xA;&lt;p&gt;Answer grading is difficult in general. This grading logic is designed to be conservative and will sometimes reject correct answers, though it does so less frequently than the normalization logic from MATH. Our logic might sometimes admit incorrect answers, though we&#39;ve put effort into minimizing this.&lt;/p&gt; &#xA;&lt;h2&gt;MATH Splits&lt;/h2&gt; &#xA;&lt;p&gt;As explained in Let&#39;s Verify Step by Step, we use a nonstandard MATH train/test split.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In order to avoid the risk of over-fitting on the 7,500 MATH training problems, we expanded the training set to include 4,500 MATH test split problems. We therefore evaluate our models only on the remaining 500 held-out problems. We selected these 500 test problems uniformly at random, and we believe they are representative of the test set as a whole.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The &lt;code&gt;math_splits/&lt;/code&gt; folder contains our selected splits in the &lt;code&gt;train.jsonl&lt;/code&gt; and &lt;code&gt;test.jsonl&lt;/code&gt; files. You&#39;ll need &lt;a href=&#34;https://git-lfs.com/&#34;&gt;Git LFS&lt;/a&gt; to properly clone these files.&lt;/p&gt; &#xA;&lt;h2&gt;Scored Samples&lt;/h2&gt; &#xA;&lt;p&gt;We release all large-scale model samples used to evaluate the large-scale ORM and PRM, corresponding to Figure 3 in the paper. Each test problem has to 1860 scored samples. Solutions that failed to reach an answer within 1024 tokens were discarded, resulting in less than 1860 samples on some problems. We account for this in the best-of-N evaluation logic.&lt;/p&gt; &#xA;&lt;p&gt;Evaluate the PRM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval/eval.py --method prm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Evaluate the ORM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval/eval.py --method orm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please use the below BibTeX entry to cite this dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{lightman2023lets,&#xA;      title={Let&#39;s Verify Step by Step}, &#xA;      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},&#xA;      journal={arXiv preprint arXiv:2305.20050},&#xA;      year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/detectron2</title>
    <updated>2023-06-03T01:45:59Z</updated>
    <id>tag:github.com,2023-06-03:/facebookresearch/detectron2</id>
    <link href="https://github.com/facebookresearch/detectron2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/detectron2/main/.github/Detectron2-Logo-Horz.svg?sanitize=true&#34; width=&#34;300&#34;&gt; &#xA;&lt;a href=&#34;https://opensource.facebook.com/support-ukraine&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34; alt=&#34;Support Ukraine - Help Provide Humanitarian Aid to Ukraine.&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Detectron2 is Facebook AI Research&#39;s next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of &lt;a href=&#34;https://github.com/facebookresearch/Detectron/&#34;&gt;Detectron&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/maskrcnn-benchmark/&#34;&gt;maskrcnn-benchmark&lt;/a&gt;. It supports a number of computer vision research projects and production applications in Facebook.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Learn More about Detectron2&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Explain Like I‚Äôm 5: Detectron2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Using Machine Learning with Detectron2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=1oq1Ye7dFqc&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/1oq1Ye7dFqc/0.jpg&#34; alt=&#34;Explain Like I‚Äôm 5: Detectron2&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=eUSgtfK4ivk&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/eUSgtfK4ivk/0.jpg&#34; alt=&#34;Using Machine Learning with Detectron2&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Includes new capabilities such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, ViTDet, MViTv2 etc.&lt;/li&gt; &#xA; &lt;li&gt;Used as a library to support building &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/&#34;&gt;research projects&lt;/a&gt; on top of it.&lt;/li&gt; &#xA; &lt;li&gt;Models can be exported to TorchScript format or Caffe2 format for deployment.&lt;/li&gt; &#xA; &lt;li&gt;It &lt;a href=&#34;https://detectron2.readthedocs.io/notes/benchmarks.html&#34;&gt;trains much faster&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/&#34;&gt;blog post&lt;/a&gt; to see more demos and learn about detectron2.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://detectron2.readthedocs.io/tutorials/install.html&#34;&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://detectron2.readthedocs.io/tutorials/getting_started.html&#34;&gt;Getting Started with Detectron2&lt;/a&gt;, and the &lt;a href=&#34;https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5&#34;&gt;Colab Notebook&lt;/a&gt; to learn about basic usage.&lt;/p&gt; &#xA;&lt;p&gt;Learn more at our &lt;a href=&#34;https://detectron2.readthedocs.org&#34;&gt;documentation&lt;/a&gt;. And see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/&#34;&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo and Baselines&lt;/h2&gt; &#xA;&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md&#34;&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Detectron2 is released under the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detectron2/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing Detectron2&lt;/h2&gt; &#xA;&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{wu2019detectron2,&#xA;  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and&#xA;                  Wan-Yen Lo and Ross Girshick},&#xA;  title =        {Detectron2},&#xA;  howpublished = {\url{https://github.com/facebookresearch/detectron2}},&#xA;  year =         {2019}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>