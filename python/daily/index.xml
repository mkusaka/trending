<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-09T01:42:19Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>IDEA-Research/DWPose</title>
    <updated>2023-08-09T01:42:19Z</updated>
    <id>tag:github.com,2023-08-09:/IDEA-Research/DWPose</id>
    <link href="https://github.com/IDEA-Research/DWPose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of the paper &#34;Effective Whole-body Pose Estimation with Two-stages Distillation&#34;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/logo.png&#34; width=&#34;100px&#34;&gt; &lt;/p&gt; &#xA; &lt;h2&gt;Effective Whole-body Pose Estimation with Two-stages Distillation &lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15880&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-2307.15880-red&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/2d-human-pose-estimation-on-coco-wholebody-1?p=effective-whole-body-pose-estimation-with-two&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/effective-whole-body-pose-estimation-with-two/2d-human-pose-estimation-on-coco-wholebody-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=M9qKrogAAAAJ&amp;amp;hl=en&amp;amp;oi=sra&#34;&gt;Zhendong Yang&lt;/a&gt;, &lt;a href=&#34;https://ailingzeng.site/&#34;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=fYdxi2sAAAAJ&amp;amp;hl=en&amp;amp;oi=sra&#34;&gt;Chun Yuan&lt;/a&gt;, &lt;a href=&#34;http://yu-li.github.io/&#34;&gt;Yu Li&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/lalaland.gif&#34; style=&#34;height:200px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/iron.gif&#34; style=&#34;height:200px&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ DWPose ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ ‚ÄÉ‚ÄÉ ‚ÄÉ ‚ÄÉ DWPose + ControlNet (&lt;i&gt;prompt: Ironman&lt;/i&gt;) &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;üíÉüèª DWPose üíÉüèª&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official implementation of the &lt;a href=&#34;https://arxiv.org/abs/2307.15880&#34;&gt;Effective Whole-body Pose Estimation with Two-stages Distillation&lt;/a&gt;. Our code is based on &lt;a href=&#34;https://github.com/open-mmlab/mmpose/tree/main&#34;&gt;MMPose&lt;/a&gt; and &lt;a href=&#34;https://github.com/lllyasviel/ControlNet-v1-1-nightly&#34;&gt;ControlNet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/architecture.jpg&#34; width=&#34;650px&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;‚öîÔ∏è We release a series of models named DWPose with different sizes, from tiny to large, for human whole-body pose estimation. Besides, we also replace Openpose with DWPose for ControlNet, obtaining better Generated Images.&lt;/p&gt; &#xA;&lt;h2&gt;üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/08/07&lt;/code&gt;&lt;/strong&gt;: We upload all DWPose models to &lt;a href=&#34;https://huggingface.co/yzd-v/DWPose/tree/main&#34;&gt;huggingface&lt;/a&gt;. Now, you can download them from baidu drive, google drive and huggingface.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/08/07&lt;/code&gt;&lt;/strong&gt;: We release a new DWPose with onnx. You can avoid installing mmcv through this. See branch &lt;a href=&#34;https://github.com/IDEA-Research/DWPose/tree/onnx&#34;&gt;onnx&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/08/01&lt;/code&gt;&lt;/strong&gt;: Thanks to &lt;a href=&#34;https://github.com/open-mmlab/mmpose/tree/main&#34;&gt;MMPose&lt;/a&gt;. You can try our DWPose with this &lt;a href=&#34;https://openxlab.org.cn/apps/detail/mmpose/RTMPose&#34;&gt;demo&lt;/a&gt; by choosing wholebody!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/demo.png&#34; width=&#34;800px&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;üêü Installation&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/INSTALL.md&#34;&gt;installation instructions&lt;/a&gt;. This branch uses onnx. You can try DWPose for ControlNet without mmcv.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Results and Models&lt;/h2&gt; &#xA;&lt;h3&gt;üòé DWPose on COCO. We release a series of DWPose models.&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/compare.jpg&#34; width=&#34;350px&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;Results on COCO-WholeBody v1.0 val with detector having human AP of 56.4 on COCO val2017 dataset&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Arch&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Input Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPS (G)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Body AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Foot AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Face AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hand AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Whole AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ckpt&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ckpt&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/mmpose/configs/wholebody_2d_keypoint/rtmpose/ubody/rtmpose-t_8xb64-270e_coco-ubody-wholebody-256x192.py&#34;&gt;DWPose-t&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x192&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.585&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.465&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.735&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.357&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.485&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1X2sVxv4JOZ5WFvOBiwjrNA?pwd=nmvw&#34;&gt;baidu drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Csbg56QvB0TtFamJ6pPWNil7h6WziDwl/view?usp=sharing&#34;&gt;google drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/mmpose/configs/wholebody_2d_keypoint/rtmpose/ubody/rtmpose-s_8xb64-270e_coco-ubody-wholebody-256x192.py&#34;&gt;DWPose-s&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x192&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.633&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.533&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.776&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.427&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.538&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1k2JxCtJL9dIGU-h31UBQOA?pwd=hcf2&#34;&gt;baidu drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/10TuEeLhArxfd4e6bnE7YgmBI9RFvu9DL/view?usp=sharing&#34;&gt;google drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/mmpose/configs/wholebody_2d_keypoint/rtmpose/ubody/rtmpose-m_8xb64-270e_coco-ubody-wholebody-256x192.py&#34;&gt;DWPose-m&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x192&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.685&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.636&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.828&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.527&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.606&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/183ovcYHV6I5TQ9Wu1eS-eg?pwd=rcry&#34;&gt;baidu drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13ZWnGDteGBmjALtErYS8AHhMBBNAN9en/view?usp=sharing&#34;&gt;google drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/mmpose/configs/wholebody_2d_keypoint/rtmpose/ubody/rtmpose-l_8xb64-270e_coco-ubody-wholebody-256x192.py&#34;&gt;DWPose-l&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x192&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.704&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.662&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.843&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.566&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.631&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1bWEeiFL5UGoDj9Nkazb98w?pwd=u7ek&#34;&gt;baidu drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1PHKN3p873dgCSh_YRsYqTZVj-kIbclRS/view?usp=sharing&#34;&gt;google drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/mmpose/configs/wholebody_2d_keypoint/rtmpose/ubody/rtmpose-l_8xb32-270e_coco-ubody-wholebody-384x288.py&#34;&gt;DWPose-l&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x288&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.722&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.704&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.887&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.621&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.665&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/168T2XGXQDli8j03e_dOJdg?pwd=ajcq&#34;&gt;baidu drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Oy9O18cYk8Dk776DbxpCPWmJtJCl-OCm/view?usp=sharing&#34;&gt;google drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ü¶à DWPose for ControlNet.&lt;/h3&gt; &#xA;&lt;p&gt;First, you need to download our Pose model dw-ll_ucoco_384.onnx (&lt;a href=&#34;https://pan.baidu.com/s/1nuBjw-KKSxD_BkpmwXUJiw?pwd=28d7&#34;&gt;baidu&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/12L8E2oAgZy4VACGSK9RaZBZrfgx7VTA2/view?usp=sharing&#34;&gt;google&lt;/a&gt;) and Det model yolox_l.onnx (&lt;a href=&#34;https://pan.baidu.com/s/1fpfIVpv5ypo4c1bUlzkMYQ?pwd=mjdn&#34;&gt;baidu&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1w9pXC8tT0p9ndMN-CArp1__b2GbzewWI/view?usp=sharing&#34;&gt;google&lt;/a&gt;), then put them into ControlNet-v1-1-nightly/annotator/ckpts. Then you can use DWPose to generate the images you like.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ControlNet-v1-1-nightly&#xA;python gradio_dw_open_pose.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Non-cherry-picked test with random seed 12345 (&#34;spider man&#34;):&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/jay_pose.jpg&#34; width=&#34;600px&#34;&gt; &lt;/p&gt;&#xA;&lt;h4&gt;Comparison with OpenPose&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/DWPose/onnx/resources/generation.jpg&#34; width=&#34;600px&#34;&gt; &lt;/p&gt;&#xA;&lt;h2&gt;üö¢ Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Prepare &lt;a href=&#34;https://cocodataset.org/#download&#34;&gt;COCO&lt;/a&gt; in mmpose/data/coco and &lt;a href=&#34;https://github.com/IDEA-Research/OSX&#34;&gt;UBody&lt;/a&gt; in mmpose/data/UBody.&lt;/p&gt; &#xA;&lt;p&gt;UBody needs to be tarnsferred into images. Don&#39;t forget.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd mmpose&#xA;python video2image.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to evaluate the models on UBody&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# add category into UBody&#39;s annotation&#xA;cd mmpose&#xA;python add_cat.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚≠êTrain a model&lt;/h2&gt; &#xA;&lt;h3&gt;Train DWPose with the first stage distillation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd mmpose&#xA;bash tools/dist_train.sh configs/distiller/ubody/s1_dis/rtmpose_x_dis_l__coco-ubody-256x192.py 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train DWPose with the second stage distillation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd mmpose&#xA;bash tools/dist_train.sh configs/distiller/ubody/s2_dis/dwpose_l-ll__coco-ubody-256x192.py 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tansfer the distillation models into regular models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd mmpose&#xA;# if first stage distillation&#xA;python pth_transfer.py $dis_ckpt $new_pose_ckpt&#xA;# if second stage distillation&#xA;python pth_transfer.py $dis_ckpt $new_pose_ckpt --two_dis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚≠êTest a model&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# test on UBody&#xA;bash tools/dist_test.sh configs/wholebody_2d_keypoint/rtmpose/ubody/rtmpose-l_8xb64-270e_ubody-wholebody-256x192.py $pose_ckpt 8&#xA;&#xA;# test on COCO&#xA;bash tools/dist_test.sh configs/wholebody_2d_keypoint/rtmpose/ubody/rtmpose-l_8xb64-270e_coco-ubody-wholebody-256x192.py $pose_ckpt 8&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü•≥ Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yang2023effective,&#xA;  title={Effective Whole-body Pose Estimation with Two-stages Distillation},&#xA;  author={Yang, Zhendong and Zeng, Ailing and Yuan, Chun and Li, Yu},&#xA;  journal={arXiv preprint arXiv:2307.15880},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü•Ç Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our code is based on &lt;a href=&#34;https://github.com/open-mmlab/mmpose/tree/main&#34;&gt;MMPose&lt;/a&gt; and &lt;a href=&#34;https://github.com/lllyasviel/ControlNet-v1-1-nightly&#34;&gt;ControlNet&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>junshutang/Make-It-3D</title>
    <updated>2023-08-09T01:42:19Z</updated>
    <id>tag:github.com,2023-08-09:/junshutang/Make-It-3D</id>
    <link href="https://github.com/junshutang/Make-It-3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2023] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior (ICCV 2023)&lt;/h1&gt; &#xA;&lt;!-- ![Teaser](teaser.png) --&gt; &#xA;&lt;div class=&#34;half&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/bunny-cake.png&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/bunny-cake-rgb.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/bunny-cake-normal.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/castle.png&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/castle-rgb.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/castle-normal.gif&#34; width=&#34;128&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div class=&#34;half&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/house.png&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/house-rgb.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/house-normal.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/jay.png&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/jay-rgb.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/jay-normal.gif&#34; width=&#34;128&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Project page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2303.14184&#34;&gt;Paper&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;!-- &lt;br&gt; --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://junshutang.github.io/&#34;&gt;Junshu Tang&lt;/a&gt;, &lt;a href=&#34;https://tengfei-wang.github.io/&#34;&gt;Tengfei Wang&lt;/a&gt;, &lt;a href=&#34;https://bo-zhang.me/&#34;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/tinzhan/&#34;&gt;Ting Zhang&lt;/a&gt;, &lt;a href=&#34;https://yiranran.github.io/&#34;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&#34;https://dmcv.sjtu.edu.cn/&#34;&gt;Lizhuang Ma&lt;/a&gt;, and &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/doch/&#34;&gt;Dong Chen&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- &lt;br&gt; --&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In this work, we investigate the problem of creating high-fidelity 3D content from only a single image. This is inherently challenging: it essentially involves estimating the underlying 3D geometry while simultaneously hallucinating unseen textures. To address this challenge, we leverage prior knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization pipeline: the first stage optimizes a neural radiance field by incorporating constraints from the reference image at the frontal view and diffusion prior at novel views; the second stage transforms the coarse model into textured point clouds and further elevates the realism with diffusion prior while leveraging the high-quality textures from the reference image. Extensive experiments demonstrate that our method outperforms prior works by a large margin, resulting in faithful reconstructions and impressive visual quality. Our method presents the first attempt to achieve high-quality 3D creation from a single image for general objects and enables various applications such as text-to-3D creation and texture editing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Todo (Latest update: 2023/07/01)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Release coarse stage training code&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Release all training code (coarse + &lt;a href=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/#refine-stage&#34;&gt;refine stage&lt;/a&gt;)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release the test benchmark for all results in the paper&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release more applications&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo of 360¬∞ geometry&lt;/h2&gt; &#xA;&lt;div class=&#34;half&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/teddy.png&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/teddy-rgb.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/teddy-normal.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/teddy-2.png&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/teddy-2-rgb.gif&#34; width=&#34;128&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/teddy-2-normal.gif&#34; width=&#34;128&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;SAM + Make-It-3D&lt;/h2&gt; &#xA;&lt;div class=&#34;half&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/corgi-demo.png&#34; height=&#34;170&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/corgi.png&#34; width=&#34;170&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/corgi-rgb.gif&#34; width=&#34;170&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/junshutang/Make-It-3D/master/demo/corgi-normal.gif&#34; width=&#34;170&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio===0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html&#xA;    pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&#xA;    pip install git+https://github.com/openai/CLIP.git&#xA;    pip install git+https://github.com/huggingface/diffusers.git&#xA;    pip install git+https://github.com/huggingface/huggingface_hub.git&#xA;    pip install git+https://github.com/facebookresearch/pytorch3d.git&#xA;    pip install git+https://github.com/S-aiueo32/contextual_loss_pytorch.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    pip install -r requirements.txt &#xA;    pip install ./raymarching&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training requirements&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/isl-org/DPT&#34;&gt;DPT&lt;/a&gt;. We use an off-the-shelf single-view depth estimator DPT to predict the depth for the reference image. &lt;pre&gt;&lt;code&gt;git clone https://github.com/isl-org/DPT.git&#xA;mkdir dpt_weights&#xA;&lt;/code&gt;&lt;/pre&gt; Download the pretrained model &lt;a href=&#34;https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt&#34;&gt;dpt_hybrid&lt;/a&gt;, and put it in &lt;code&gt;dpt_weights&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.12597&#34;&gt;BLIP2&lt;/a&gt;. We use BLIP2 to generate a caption. You can also modify the conditioned text using &lt;code&gt;--text &#34;{TEXT}&#34;&lt;/code&gt; which will greatly reduce time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?other=stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;. We use diffusion prior from a pretrained 2D Stable Diffusion 2.0 model. To start with, you may need a huggingface &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;token&lt;/a&gt; to access the model, or use &lt;code&gt;huggingface-cli login&lt;/code&gt; command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Coarse stage&lt;/h3&gt; &#xA;&lt;p&gt;We use progressive training strategy to generate a full 360¬∞ 3D geometry. Run the command and modify the workspace name &lt;code&gt;NAME&lt;/code&gt; and the path of the reference image &lt;code&gt;IMGPATH&lt;/code&gt;. We first optimize the scene under frontal camera views.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python main.py --workspace ${NAME} --ref_path &#34;${IMGPATH}&#34; --phi_range 135 225 --iters 2000 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then we spread the camera view samples to full 360¬∞. If you need a prompt condition &#34;back view&#34;, you can use the command &lt;code&gt;--need_back&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python main.py --workspace ${NAME} --ref_path &#34;${IMGPATH}&#34; --phi_range 0 360 --albedo_iters 3500 --iters 5000 --final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you encounter &lt;code&gt;long geometry&lt;/code&gt; issue, you can try to increase the reference fov and adjust relative setting. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python main.py --workspace ${NAME} --ref_path &#34;${IMGPATH}&#34; --phi_range 135 225 --iters 2000 --fov 60 --fovy_range 50 70 --blob_radius 0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Refine stage&lt;/h3&gt; &#xA;&lt;p&gt;After the coarse stage training, now you can easily use the command &lt;code&gt;--refine&lt;/code&gt; for refine stage training. We optimize the scene under frontal camera views.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python main.py --workspace ${NAME} --ref_path &#34;${IMGPATH}&#34; --phi_range 135 225 --refine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can modify the value of training iterations using the command &lt;code&gt;--refine_iters&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python main.py --workspace ${NAME} --ref_path &#34;${IMGPATH}&#34; --phi_range 135 225 --refine_iters 3000 --refine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We additionally use &lt;code&gt;contextual loss&lt;/code&gt; on the refine stage, we find it helps to sharpen the texture. You may need to install &lt;a href=&#34;https://github.com/S-aiueo32/contextual_loss_pytorch&#34;&gt;contextual_loss_pytorch&lt;/a&gt; before training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    pip install git+https://github.com/S-aiueo32/contextual_loss_pytorch.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Important Note&lt;/h2&gt; &#xA;&lt;p&gt;Hallucinating 3D geometry and generating novel views from a single image of general genre is a challenging task. While our method demonstrates strong capability on creating 3D from most images with a centered single object, it may still encounter difficulties in reconstructing solid geometry on complex cases. &lt;strong&gt;If you encounter any bugs, please feel free to contact us.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code helpful for your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{tang2023make,&#xA;  title={Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior},&#xA;  author={Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},&#xA;  journal={arXiv preprint arXiv:2303.14184},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This code borrows heavily from &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable-Dreamfusion&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alibaba-damo-academy/FunASR</title>
    <updated>2023-08-09T01:42:19Z</updated>
    <id>tag:github.com,2023-08-09:/alibaba-damo-academy/FunASR</id>
    <link href="https://github.com/alibaba-damo-academy/FunASR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Fundamental End-to-End Speech Recognition Toolkit&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;|English)&lt;/p&gt; &#xA;&lt;h1&gt;FunASR: A Fundamental End-to-End Speech Recognition Toolkit&lt;/h1&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OS-Linux%2C%20Win%2C%20Mac-brightgreen.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-&gt;=3.7,&lt;=3.10-aff.svg&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Pytorch-%3E%3D1.11-blue&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp;amp; finetuning of the industrial-grade speech recognition model released on &lt;a href=&#34;https://www.modelscope.cn/models?page=1&amp;amp;tasks=auto-speech-recognition&#34;&gt;ModelScope&lt;/a&gt;, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/#highlights&#34;&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/alibaba-damo-academy/FunASR#whats-new&#34;&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/#installation&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/#quick-start&#34;&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/funasr/runtime/readme.md&#34;&gt;&lt;strong&gt;Runtime&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/model_zoo/modelscope_models.md&#34;&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/#contact&#34;&gt;&lt;strong&gt;Contact&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;highlights&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.&lt;/li&gt; &#xA; &lt;li&gt;We have released a vast collection of academic and industrial pretrained models on the &lt;a href=&#34;https://www.modelscope.cn/models?page=1&amp;amp;tasks=auto-speech-recognition&#34;&gt;ModelScope&lt;/a&gt;, which can be accessed through our &lt;a href=&#34;https://github.com/alibaba-damo-academy/FunASR/raw/main/docs/model_zoo/modelscope_models.md&#34;&gt;Model Zoo&lt;/a&gt;. The representative &lt;a href=&#34;https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary&#34;&gt;Paraformer-large&lt;/a&gt;, a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/funasr/runtime/readme_cn.md&#34;&gt;service deployment document&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;whats-new&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s new:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to (&lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/funasr/runtime/docs/SDK_tutorial_online.md&#34;&gt;Deployment documentation&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to (&lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/egs/aishell/bat&#34;&gt;BAT&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;2023/07/03: The offline file transcription service (CPU) of Mandarin has been released. For more details, please refer to (&lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/funasr/runtime/docs/SDK_tutorial.md&#34;&gt;Deployment documentation&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to (&lt;a href=&#34;https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html&#34;&gt;M2MeT2.0&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;Installation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please ref to &lt;a href=&#34;https://alibaba-damo-academy.github.io/FunASR/en/installation/installation.html&#34;&gt;installation docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deployment Service&lt;/h2&gt; &#xA;&lt;p&gt;FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/funasr/runtime/docs/SDK_tutorial.md&#34;&gt;docs&lt;/a&gt;. More detailed information about service deployment can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/funasr/runtime/readme_cn.md&#34;&gt;deployment roadmap&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quick-start&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Quick start for new usersÔºà&lt;a href=&#34;https://alibaba-damo-academy.github.io/FunASR/en/funasr/quick_start_zh.html&#34;&gt;tutorial&lt;/a&gt;Ôºâ&lt;/p&gt; &#xA;&lt;p&gt;FunASR supports inference and fine-tuning of models trained on industrial datasets of tens of thousands of hours. For more details, please refer to (&lt;a href=&#34;https://alibaba-damo-academy.github.io/FunASR/en/modelscope_pipeline/quick_start.html&#34;&gt;modelscope_egs&lt;/a&gt;). It also supports training and fine-tuning of models on academic standard datasets. For more details, please refer to(&lt;a href=&#34;https://alibaba-damo-academy.github.io/FunASR/en/academic_recipe/asr_recipe.html&#34;&gt;egs&lt;/a&gt;). The models include speech recognition (ASR), speech activity detection (VAD), punctuation recovery, language model, speaker verification, speaker separation, and multi-party conversation speech recognition. For a detailed list of models, please refer to the &lt;a href=&#34;https://github.com/alibaba-damo-academy/FunASR/raw/main/docs/model_zoo/modelscope_models.md&#34;&gt;Model Zoo&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;Community Communication&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community Communication&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter problems in use, you can directly raise Issues on the github page.&lt;/p&gt; &#xA;&lt;p&gt;You can also scan the following DingTalk group or WeChat group QR code to join the community group for communication and discussion.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DingTalk group&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;WeChat group&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;div align=&#34;left&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/dingding.jpg&#34; width=&#34;250&#34;&gt;&#xA;    &lt;/div&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/wechat.png&#34; width=&#34;232&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;div align=&#34;left&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/damo.png&#34; width=&#34;180&#34;&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&#xA;    &lt;div align=&#34;left&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/nwpu.png&#34; width=&#34;260&#34;&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/China_Telecom.png&#34; width=&#34;200&#34;&gt; &lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/RapidAI.png&#34; width=&#34;200&#34;&gt; &lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/aihealthx.png&#34; width=&#34;200&#34;&gt; &lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/XVERSE.png&#34; width=&#34;250&#34;&gt; &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The contributors can be found in &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/(./Acknowledge)&#34;&gt;contributors list&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;The MIT License&lt;/a&gt;. FunASR also contains various third-party components and some code modified from other repos under other open source licenses. The use of pretraining model is subject to &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/MODEL_LICENSE&#34;&gt;model licencs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{gao2023funasr,&#xA;  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},&#xA;  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},&#xA;  year={2023},&#xA;  booktitle={INTERSPEECH},&#xA;}&#xA;@inproceedings{An2023bat,&#xA;  author={Keyu An and Xian Shi and Shiliang Zhang},&#xA;  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},&#xA;  year={2023},&#xA;  booktitle={INTERSPEECH},&#xA;}&#xA;@inproceedings{wang2023told,&#xA;  author={Jiaming Wang and Zhihao Du and Shiliang Zhang},&#xA;  title={{TOLD:} {A} Novel Two-Stage Overlap-Aware Framework for Speaker Diarization},&#xA;  year={2023},&#xA;  booktitle={ICASSP},&#xA;}&#xA;@inproceedings{gao22b_interspeech,&#xA;  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},&#xA;  title={{Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition}},&#xA;  year=2022,&#xA;  booktitle={Proc. Interspeech 2022},&#xA;  pages={2063--2067},&#xA;  doi={10.21437/Interspeech.2022-9996}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>