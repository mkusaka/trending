<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-16T01:36:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>spandanb/learndb-py</title>
    <updated>2023-08-16T01:36:14Z</updated>
    <id>tag:github.com,2023-08-16:/spandanb/learndb-py</id>
    <link href="https://github.com/spandanb/learndb-py" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn database internals by implementing it from scratch.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LearnDB&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;What I Cannot Create, I Do Not Understand -Richard Feynman&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In the spirit of Feynman&#39;s immortal words, the goal of this project is to better understand the internals of databases by implementing a relational database management system (RDBMS) (sqlite clone) from scratch.&lt;/p&gt; &#xA;&lt;p&gt;This project was motivated by a desire to: 1) understand databases more deeply and 2) work on a fun project. These dual goals led to a:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;relatively simple code base&lt;/li&gt; &#xA; &lt;li&gt;relatively complete RDBMS implementation&lt;/li&gt; &#xA; &lt;li&gt;written in pure python &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;No build step&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;zero configuration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;configuration can be overriden&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This makes the learndb codebase great for tinkering with. But the product has some key limitations that means it shouldn&#39;t be used as an actual storage solution.&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;p&gt;Learndb supports the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;it has a rich sql (learndb-sql) with support for &lt;code&gt;select, from, where, group by, having, limit, order by&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;custom lexer and parser built using &lt;a href=&#34;https://github.com/lark-parser/lark&#34;&gt;&lt;code&gt;lark&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;at a high-level, there is an engine that can accept some SQL statements. These statements expresses operations on a database (a collection of tables which contain data)&lt;/li&gt; &#xA; &lt;li&gt;allows users/agents to connect to RDBMS in multiple ways: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;REPL&lt;/li&gt; &#xA;   &lt;li&gt;importing python module&lt;/li&gt; &#xA;   &lt;li&gt;passing a file of commands to the engine&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;on-disk btree implementation as backing data structure&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Limitations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Very simplified [^1] implementation of floating point number arithmetic, e.g. compared to &lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754&#34;&gt;IEEE754&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;No support for common utility features, like wildcard column expansion, e.g. &lt;code&gt;select * ...&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;More &lt;a href=&#34;https://raw.githubusercontent.com/spandanb/learndb-py/master/docs/tutorial.md&#34;&gt;limitations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started: Tinkering and Beyond&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To get started with &lt;code&gt;learndb&lt;/code&gt; first start with &lt;a href=&#34;https://raw.githubusercontent.com/spandanb/learndb-py/master/docs/tutorial.md&#34;&gt;&lt;code&gt;tutorial.md&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Then to understand the system at a deeper technical level read &lt;a href=&#34;https://raw.githubusercontent.com/spandanb/learndb-py/master/docs/reference.md&#34;&gt;&lt;code&gt;reference.md&lt;/code&gt;&lt;/a&gt;. This is essentially a complete reference manual directed at a user of the system. This outlines the operations and capabilities of the system. It also describes what is (un)supported and undefined behavior.&lt;/li&gt; &#xA; &lt;li&gt;`Architecture.md`` - this provides a component level breakdown of the repo and the system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hacking&lt;/h2&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System requirements &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;requires a linux/macos system, since it uses &lt;code&gt;fcntl&lt;/code&gt; to get exclusive read access on database file&lt;/li&gt; &#xA;   &lt;li&gt;python &amp;gt;= 3.9&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;To install for development, i.e. src can be edited from without having to reinstall: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd &amp;lt;repo_root&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;create virtualenv: &lt;code&gt;python3 -m venv venv &lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;activate venv: &lt;code&gt;source venv/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;install requirements: &lt;code&gt;python -m pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;install &lt;code&gt;Learndb&lt;/code&gt; in edit mode: &lt;code&gt;python3 -m pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run REPL&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;source venv/bin/activate&#xA;python run_learndb.py repl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Tests&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run all tests:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;python -m pytest tests/*.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run btree tests: -&lt;code&gt;python -m pytest -s tests/btree_tests.py&lt;/code&gt; # stdout&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;python -m pytest tests/btree_tests.py&lt;/code&gt; # suppressed out&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run end-to-end tests: &lt;code&gt;python -m pytest -s tests/e2e_tests.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run end-to-end tests (employees): &lt;code&gt;python -m pytest -s tests/e2e_tests_employees.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;python -m pytest -s tests/e2e_tests_employees.py -k test_equality_select&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run serde tests: &lt;code&gt;... serde_tests.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run language parser tests: &lt;code&gt;... lang_tests.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run specific test: &lt;code&gt;python -m pytest tests.py -k test_name&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clear pytest cache &lt;code&gt;python -m pytest --cache-clear&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References consulted&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I started this project by following cstack&#39;s awesome &lt;a href=&#34;https://cstack.github.io/db_tutorial/&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Later I was primarily referencing: &lt;a href=&#34;https://books.google.com/books?id=9Z6IQQnX1JEC&amp;amp;source=gbs_similarbooks&#34;&gt;SQLite Database System: Design and Implementation (1st ed)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sqlite file format: &lt;a href=&#34;https://www.sqlite.org/fileformat2.html&#34;&gt;docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Postgres for how certain SQL statements are implemented and how their &lt;a href=&#34;https://www.postgresql.org/docs/11/index.html&#34;&gt;documentation&lt;/a&gt; is organized&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Management&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;immanent work/issues are tracked in &lt;code&gt;tasks.md&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;long-term ideas are tracked in &lt;code&gt;docs/future-work.md&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[^1]: When evaluating the difference between two floats, e.g. &lt;code&gt;3.2 &amp;gt; 4.2&lt;/code&gt;, I consider the condition True if the difference between the two is some fixed delta. The accepted epsilon should scale with the magnitude of the number&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>chatchat-space/Langchain-Chatchat</title>
    <updated>2023-08-16T01:36:14Z</updated>
    <id>tag:github.com,2023-08-16:/chatchat-space/Langchain-Chatchat</id>
    <link href="https://github.com/chatchat-space/Langchain-Chatchat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Langchain-Chatchat (formerly langchain-ChatGLM), local knowledge based LLM (like ChatGLM) QA app with langchain ｜ 基于 Langchain 与 ChatGLM 等语言模型的本地知识库问答&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/logo-long-chatchat-trans-v2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LangChain-Chatchat&lt;/strong&gt; (原 Langchain-ChatGLM): 基于 Langchain 与 ChatGLM 等大语言模型的本地知识库问答应用实现。&lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E4%BB%8B%E7%BB%8D&#34;&gt;介绍&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E5%8F%98%E6%9B%B4%E6%97%A5%E5%BF%97&#34;&gt;变更日志&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E6%A8%A1%E5%9E%8B%E6%94%AF%E6%8C%81&#34;&gt;模型支持&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#Docker-%E9%83%A8%E7%BD%B2&#34;&gt;Docker 部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E5%BC%80%E5%8F%91%E9%83%A8%E7%BD%B2&#34;&gt;开发部署&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E8%BD%AF%E4%BB%B6%E9%9C%80%E6%B1%82&#34;&gt;软件需求&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#1.-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87&#34;&gt;1. 开发环境准备&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#2.-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E8%87%B3%E6%9C%AC%E5%9C%B0&#34;&gt;2. 下载模型至本地&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#3.-%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E9%A1%B9&#34;&gt;3. 设置配置项&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#4.-%E7%9F%A5%E8%AF%86%E5%BA%93%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%BF%81%E7%A7%BB&#34;&gt;4. 知识库初始化与迁移&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#5.-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1%E6%88%96-Web-UI&#34;&gt;5. 启动 API 服务或 Web UI&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;常见问题&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E8%B7%AF%E7%BA%BF%E5%9B%BE&#34;&gt;路线图&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#%E9%A1%B9%E7%9B%AE%E4%BA%A4%E6%B5%81%E7%BE%A4&#34;&gt;项目交流群&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;介绍&lt;/h2&gt; &#xA;&lt;p&gt;🤖️ 一种利用 &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;langchain&lt;/a&gt; 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案。&lt;/p&gt; &#xA;&lt;p&gt;💡 受 &lt;a href=&#34;https://github.com/GanymedeNil&#34;&gt;GanymedeNil&lt;/a&gt; 的项目 &lt;a href=&#34;https://github.com/GanymedeNil/document.ai&#34;&gt;document.ai&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/AlexZhangji&#34;&gt;AlexZhangji&lt;/a&gt; 创建的 &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B/pull/216&#34;&gt;ChatGLM-6B Pull Request&lt;/a&gt; 启发，建立了全流程可使用开源模型实现的本地知识库问答应用。本项目的最新版本中通过使用 &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; 接入 Vicuna, Alpaca, LLaMA, Koala, RWKV 等模型，依托于 &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;langchain&lt;/a&gt; 框架支持通过基于 &lt;a href=&#34;https://github.com/tiangolo/fastapi&#34;&gt;FastAPI&lt;/a&gt; 提供的 API 调用服务，或使用基于 &lt;a href=&#34;https://github.com/streamlit/streamlit&#34;&gt;Streamlit&lt;/a&gt; 的 WebUI 进行操作。&lt;/p&gt; &#xA;&lt;p&gt;✅ 依托于本项目支持的开源 LLM 与 Embedding 模型，本项目可实现全部使用&lt;strong&gt;开源&lt;/strong&gt;模型&lt;strong&gt;离线私有部署&lt;/strong&gt;。与此同时，本项目也支持 OpenAI GPT API 的调用，并将在后续持续扩充对各类模型及模型 API 的接入。&lt;/p&gt; &#xA;&lt;p&gt;⛓️ 本项目实现原理如下图所示，过程包括加载文件 -&amp;gt; 读取文本 -&amp;gt; 文本分割 -&amp;gt; 文本向量化 -&amp;gt; 问句向量化 -&amp;gt; 在文本向量中匹配出与问句向量最相似的 &lt;code&gt;top k&lt;/code&gt;个 -&amp;gt; 匹配出的文本作为上下文和问题一起添加到 &lt;code&gt;prompt&lt;/code&gt;中 -&amp;gt; 提交给 &lt;code&gt;LLM&lt;/code&gt;生成回答。&lt;/p&gt; &#xA;&lt;p&gt;📺 &lt;a href=&#34;https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&amp;amp;vd_source=e6c5aafe684f30fbe41925d61ca6d514&#34;&gt;原理介绍视频&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm.png&#34; alt=&#34;实现原理图&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;从文档处理角度来看，实现流程如下：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm2.png&#34; alt=&#34;实现原理图2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;🚩 本项目未涉及微调、训练过程，但可利用微调或训练对本项目效果进行优化。&lt;/p&gt; &#xA;&lt;p&gt;🌐 &lt;a href=&#34;https://www.codewithgpu.com/i/imClumsyPanda/langchain-ChatGLM/Langchain-Chatchat&#34;&gt;AutoDL 镜像&lt;/a&gt; 中 &lt;code&gt;v5&lt;/code&gt; 版本所使用代码已更新至本项目 &lt;code&gt;0.2.0&lt;/code&gt; 版本。&lt;/p&gt; &#xA;&lt;p&gt;🐳 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0&#34;&gt;Docker 镜像&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;💻 一行命令运行 Docker：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;变更日志&lt;/h2&gt; &#xA;&lt;p&gt;参见 &lt;a href=&#34;https://github.com/imClumsyPanda/langchain-ChatGLM/releases&#34;&gt;版本更新日志&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;从 &lt;code&gt;0.1.x&lt;/code&gt; 升级过来的用户请注意，需要按照&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#3.-%E5%BC%80%E5%8F%91%E9%83%A8%E7%BD%B2&#34;&gt;开发部署&lt;/a&gt;过程操作，将现有知识库迁移到新格式，具体见&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/docs/INSTALL.md#%E7%9F%A5%E8%AF%86%E5%BA%93%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%BF%81%E7%A7%BB&#34;&gt;知识库初始化与迁移&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;0.2.0&lt;/code&gt; 版本与 &lt;code&gt;0.1.x&lt;/code&gt; 版本区别&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;使用 &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; 提供开源 LLM 模型的 API，以 OpenAI API 接口形式接入，提升 LLM 模型加载效果；&lt;/li&gt; &#xA; &lt;li&gt;使用 &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;langchain&lt;/a&gt; 中已有 Chain 的实现，便于后续接入不同类型 Chain，并将对 Agent 接入开展测试；&lt;/li&gt; &#xA; &lt;li&gt;使用 &lt;a href=&#34;https://github.com/tiangolo/fastapi&#34;&gt;FastAPI&lt;/a&gt; 提供 API 服务，全部接口可在 FastAPI 自动生成的 docs 中开展测试，且所有对话接口支持通过参数设置流式或非流式输出；&lt;/li&gt; &#xA; &lt;li&gt;使用 &lt;a href=&#34;https://github.com/streamlit/streamlit&#34;&gt;Streamlit&lt;/a&gt; 提供 WebUI 服务，可选是否基于 API 服务启动 WebUI，增加会话管理，可以自定义会话主题并切换，且后续可支持不同形式输出内容的显示；&lt;/li&gt; &#xA; &lt;li&gt;项目中默认 LLM 模型改为 &lt;a href=&#34;https://huggingface.co/THUDM/chatglm2-6b&#34;&gt;THUDM/chatglm2-6b&lt;/a&gt;，默认 Embedding 模型改为 &lt;a href=&#34;https://huggingface.co/moka-ai/m3e-base&#34;&gt;moka-ai/m3e-base&lt;/a&gt;，文件加载方式与文段划分方式也有调整，后续将重新实现上下文扩充，并增加可选设置；&lt;/li&gt; &#xA; &lt;li&gt;项目中扩充了对不同类型向量库的支持，除支持 &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt; 向量库外，还提供 &lt;a href=&#34;https://github.com/milvus-io/milvus&#34;&gt;Milvus&lt;/a&gt;, &lt;a href=&#34;https://github.com/pgvector/pgvector&#34;&gt;PGVector&lt;/a&gt; 向量库的接入；&lt;/li&gt; &#xA; &lt;li&gt;项目中搜索引擎对话，除 Bing 搜索外，增加 DuckDuckGo 搜索选项，DuckDuckGo 搜索无需配置 API Key，在可访问国外服务环境下可直接使用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;模型支持&lt;/h2&gt; &#xA;&lt;p&gt;本项目中默认使用的 LLM 模型为 &lt;a href=&#34;https://huggingface.co/THUDM/chatglm2-6b&#34;&gt;THUDM/chatglm2-6b&lt;/a&gt;，默认使用的 Embedding 模型为 &lt;a href=&#34;https://huggingface.co/moka-ai/m3e-base&#34;&gt;moka-ai/m3e-base&lt;/a&gt; 为例。&lt;/p&gt; &#xA;&lt;h3&gt;LLM 模型支持&lt;/h3&gt; &#xA;&lt;p&gt;本项目最新版本中基于 &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; 进行本地 LLM 模型接入，支持模型如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;meta-llama/Llama-2-7b-chat-hf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vicuna, Alpaca, LLaMA, Koala&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/BlinkDL/rwkv-4-raven&#34;&gt;BlinkDL/RWKV-4-Raven&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/camel-ai/CAMEL-13B-Combined-Data&#34;&gt;camel-ai/CAMEL-13B-Combined-Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;databricks/dolly-v2-12b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b&#34;&gt;FreedomIntelligence/phoenix-inst-chat-7b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b&#34;&gt;h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lcw99/polyglot-ko-12.8b-chang-instruct-chat&#34;&gt;lcw99/polyglot-ko-12.8b-chang-instruct-chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lmsys/fastchat-t5&#34;&gt;lmsys/fastchat-t5-3b-v1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b-chat&#34;&gt;mosaicml/mpt-7b-chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Neutralzz/BiLLa-7B-SFT&#34;&gt;Neutralzz/BiLLa-7B-SFT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/nomic-ai/gpt4all-13b-snoozy&#34;&gt;nomic-ai/gpt4all-13b-snoozy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/NousResearch/Nous-Hermes-13b&#34;&gt;NousResearch/Nous-Hermes-13b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg&#34;&gt;openaccess-ai-collective/manticore-13b-chat-pyg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5&#34;&gt;OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/project-baize/baize-v2-7b&#34;&gt;project-baize/baize-v2-7b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Salesforce/codet5p-6b&#34;&gt;Salesforce/codet5p-6b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b&#34;&gt;StabilityAI/stablelm-tuned-alpha-7b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;THUDM/chatglm-6b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/chatglm2-6b&#34;&gt;THUDM/chatglm2-6b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;tiiuae/falcon-40b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/timdettmers/guanaco-33b-merged&#34;&gt;timdettmers/guanaco-33b-merged&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat&#34;&gt;togethercomputer/RedPajama-INCITE-7B-Chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/WizardLM/WizardLM-13B-V1.0&#34;&gt;WizardLM/WizardLM-13B-V1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/WizardLM/WizardCoder-15B-V1.0&#34;&gt;WizardLM/WizardCoder-15B-V1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34;&gt;baichuan-inc/baichuan-7B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/internlm/internlm-chat-7b&#34;&gt;internlm/internlm-chat-7b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B-Chat&#34;&gt;Qwen/Qwen-7B-Chat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceH4/starchat-beta&#34;&gt;HuggingFaceH4/starchat-beta&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;任何 &lt;a href=&#34;https://huggingface.co/EleutherAI&#34;&gt;EleutherAI&lt;/a&gt; 的 pythia 模型，如 &lt;a href=&#34;https://huggingface.co/EleutherAI/pythia-6.9b&#34;&gt;pythia-6.9b&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;在以上模型基础上训练的任何 &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;Peft&lt;/a&gt; 适配器。为了激活，模型路径中必须有 &lt;code&gt;peft&lt;/code&gt; 。注意：如果加载多个peft模型，你可以通过在任何模型工作器中设置环境变量 &lt;code&gt;PEFT_SHARE_BASE_WEIGHTS=true&lt;/code&gt; 来使它们共享基础模型的权重。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;以上模型支持列表可能随 &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; 更新而持续更新，可参考 &lt;a href=&#34;https://github.com/lm-sys/FastChat/raw/main/docs/model_support.md&#34;&gt;FastChat 已支持模型列表&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;除本地模型外，本项目也支持直接接入 OpenAI API，具体设置可参考 &lt;code&gt;configs/model_configs.py.example&lt;/code&gt; 中的 &lt;code&gt;llm_model_dict&lt;/code&gt; 的 &lt;code&gt;openai-chatgpt-3.5&lt;/code&gt; 配置信息。&lt;/p&gt; &#xA;&lt;h3&gt;Embedding 模型支持&lt;/h3&gt; &#xA;&lt;p&gt;本项目支持调用 &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=sentence-similarity&#34;&gt;HuggingFace&lt;/a&gt; 中的 Embedding 模型，已支持的 Embedding 模型如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/moka-ai/m3e-small&#34;&gt;moka-ai/m3e-small&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/moka-ai/m3e-base&#34;&gt;moka-ai/m3e-base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/moka-ai/m3e-large&#34;&gt;moka-ai/m3e-large&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-small-zh&#34;&gt;BAAI/bge-small-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-base-zh&#34;&gt;BAAI/bge-base-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/BAAI/bge-large-zh&#34;&gt;BAAI/bge-large-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/shibing624/text2vec-base-chinese-sentence&#34;&gt;text2vec-base-chinese-sentence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase&#34;&gt;text2vec-base-chinese-paraphrase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/shibing624/text2vec-base-multilingual&#34;&gt;text2vec-base-multilingual&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/shibing624/text2vec-base-chinese&#34;&gt;shibing624/text2vec-base-chinese&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese&#34;&gt;GanymedeNil/text2vec-large-chinese&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/nghuyong/ernie-3.0-nano-zh&#34;&gt;nghuyong/ernie-3.0-nano-zh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/nghuyong/ernie-3.0-base-zh&#34;&gt;nghuyong/ernie-3.0-base-zh&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Docker 部署&lt;/h2&gt; &#xA;&lt;p&gt;🐳 Docker 镜像地址: &lt;code&gt;registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;该版本镜像大小 &lt;code&gt;33.9GB&lt;/code&gt;，使用 &lt;code&gt;v0.2.0&lt;/code&gt;，以 &lt;code&gt;nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04&lt;/code&gt; 为基础镜像&lt;/li&gt; &#xA; &lt;li&gt;该版本内置一个 &lt;code&gt;embedding&lt;/code&gt; 模型：&lt;code&gt;m3e-large&lt;/code&gt;，内置 &lt;code&gt;chatglm2-6b-32k&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;该版本目标为方便一键部署使用，请确保您已经在Linux发行版上安装了NVIDIA驱动程序&lt;/li&gt; &#xA; &lt;li&gt;请注意，您不需要在主机系统上安装CUDA工具包，但需要安装 &lt;code&gt;NVIDIA Driver&lt;/code&gt; 以及 &lt;code&gt;NVIDIA Container Toolkit&lt;/code&gt;，请参考&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;安装指南&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;首次拉取和启动均需要一定时间，首次启动时请参照下图使用 &lt;code&gt;docker logs -f &amp;lt;container id&amp;gt;&lt;/code&gt; 查看日志&lt;/li&gt; &#xA; &lt;li&gt;如遇到启动过程卡在 &lt;code&gt;Waiting..&lt;/code&gt; 步骤，建议使用 &lt;code&gt;docker exec -it &amp;lt;container id&amp;gt; bash&lt;/code&gt; 进入 &lt;code&gt;/logs/&lt;/code&gt; 目录查看对应阶段日志&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;开发部署&lt;/h2&gt; &#xA;&lt;h3&gt;软件需求&lt;/h3&gt; &#xA;&lt;p&gt;本项目已在 Python 3.8.1 - 3.10，CUDA 11.7 环境下完成测试。已在 Windows、ARM 架构的 macOS、Linux 系统中完成测试。&lt;/p&gt; &#xA;&lt;h3&gt;1. 开发环境准备&lt;/h3&gt; &#xA;&lt;p&gt;参见 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/docs/INSTALL.md&#34;&gt;开发环境准备&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;请注意：&lt;/strong&gt; &lt;code&gt;0.2.0&lt;/code&gt; 及更新版本的依赖包与 &lt;code&gt;0.1.x&lt;/code&gt; 版本依赖包可能发生冲突，强烈建议新建环境后重新安装依赖包。&lt;/p&gt; &#xA;&lt;h3&gt;2. 下载模型至本地&lt;/h3&gt; &#xA;&lt;p&gt;如需在本地或离线环境下运行本项目，需要首先将项目所需的模型下载至本地，通常开源 LLM 与 Embedding 模型可以从 &lt;a href=&#34;https://huggingface.co/models&#34;&gt;HuggingFace&lt;/a&gt; 下载。&lt;/p&gt; &#xA;&lt;p&gt;以本项目中默认使用的 LLM 模型 &lt;a href=&#34;https://huggingface.co/THUDM/chatglm2-6b&#34;&gt;THUDM/chatglm2-6b&lt;/a&gt; 与 Embedding 模型 &lt;a href=&#34;https://huggingface.co/moka-ai/m3e-base&#34;&gt;moka-ai/m3e-base&lt;/a&gt; 为例：&lt;/p&gt; &#xA;&lt;p&gt;下载模型需要先&lt;a href=&#34;https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage&#34;&gt;安装Git LFS&lt;/a&gt;，然后运行&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;$ git clone https://huggingface.co/THUDM/chatglm2-6b&#xA;&#xA;$ git clone https://huggingface.co/moka-ai/m3e-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. 设置配置项&lt;/h3&gt; &#xA;&lt;p&gt;复制文件 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/configs/model_config.py.example&#34;&gt;configs/model_config.py.example&lt;/a&gt; 存储至项目路径下 &lt;code&gt;./configs&lt;/code&gt; 路径下，并重命名为 &lt;code&gt;model_config.py&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;在开始执行 Web UI 或命令行交互前，请先检查 &lt;code&gt;configs/model_config.py&lt;/code&gt; 中的各项模型参数设计是否符合需求：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;请确认已下载至本地的 LLM 模型本地存储路径写在 &lt;code&gt;llm_model_dict&lt;/code&gt; 对应模型的 &lt;code&gt;local_model_path&lt;/code&gt; 属性中，如:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm_model_dict={&#xA;                &#34;chatglm2-6b&#34;: {&#xA;                        &#34;local_model_path&#34;: &#34;/Users/xxx/Downloads/chatglm2-6b&#34;,&#xA;                        &#34;api_base_url&#34;: &#34;http://localhost:8888/v1&#34;,  # &#34;name&#34;修改为 FastChat 服务中的&#34;api_base_url&#34;&#xA;                        &#34;api_key&#34;: &#34;EMPTY&#34;&#xA;                    },&#xA;                }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;请确认已下载至本地的 Embedding 模型本地存储路径写在 &lt;code&gt;embedding_model_dict&lt;/code&gt; 对应模型位置，如：&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embedding_model_dict = {&#xA;                        &#34;m3e-base&#34;: &#34;/Users/xxx/Downloads/m3e-base&#34;,&#xA;                       }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. 知识库初始化与迁移&lt;/h3&gt; &#xA;&lt;p&gt;当前项目的知识库信息存储在数据库中，在正式运行项目之前请先初始化数据库（我们强烈建议您在执行操作前备份您的知识文件）。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;如果您是从 &lt;code&gt;0.1.x&lt;/code&gt; 版本升级过来的用户，针对已建立的知识库，请确认知识库的向量库类型、Embedding 模型 &lt;code&gt;configs/model_config.py&lt;/code&gt; 中默认设置一致，如无变化只需以下命令将现有知识库信息添加到数据库即可：&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python init_database.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果您是第一次运行本项目，知识库尚未建立，或者配置文件中的知识库类型、嵌入模型发生变化，需要以下命令初始化或重建知识库：&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python init_database.py --recreate-vs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;5. 启动 API 服务或 Web UI&lt;/h3&gt; &#xA;&lt;h4&gt;5.1 启动 LLM 服务&lt;/h4&gt; &#xA;&lt;p&gt;如需使用开源模型进行本地部署，需首先启动 LLM 服务，启动方式分为三种：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#5.1.1-%E5%9F%BA%E4%BA%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B%E8%84%9A%E6%9C%AC-llm_api.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1&#34;&gt;基于多进程脚本 llm_api.py 启动 LLM 服务&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#5.1.2-%E5%9F%BA%E4%BA%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%84%9A%E6%9C%AC-llm_api_launch.py-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1&#34;&gt;基于命令行脚本 llm_api_launch.py 启动 LLM 服务&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#5.1.3-LoRA-%E5%8A%A0%E8%BD%BD&#34;&gt;LoRA 加载&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;三种方式只需选择一个即可，具体操作方式详见 5.1.1 - 5.1.3。&lt;/p&gt; &#xA;&lt;p&gt;如果启动在线的API服务（如 OPENAI 的 API 接口），则无需启动 LLM 服务，即 5.1 小节的任何命令均无需启动。&lt;/p&gt; &#xA;&lt;h5&gt;5.1.1 基于多进程脚本 llm_api.py 启动 LLM 服务&lt;/h5&gt; &#xA;&lt;p&gt;在项目根目录下，执行 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/server/llm_api.py&#34;&gt;server/llm_api.py&lt;/a&gt; 脚本启动 &lt;strong&gt;LLM 模型&lt;/strong&gt;服务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/llm_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;项目支持多卡加载，需在 llm_api.py 中修改 create_model_worker_app 函数中，修改如下三个参数:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gpus=None, &#xA;num_gpus=1, &#xA;max_gpu_memory=&#34;20GiB&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;其中，&lt;code&gt;gpus&lt;/code&gt; 控制使用的显卡的ID，如果 &#34;0,1&#34;;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;num_gpus&lt;/code&gt; 控制使用的卡数;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;max_gpu_memory&lt;/code&gt; 控制每个卡使用的显存容量。&lt;/p&gt; &#xA;&lt;h5&gt;5.1.2 基于命令行脚本 llm_api_launch.py 启动 LLM 服务&lt;/h5&gt; &#xA;&lt;p&gt;在项目根目录下，执行 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/server/llm_api.py&#34;&gt;server/llm_api_launch.py&lt;/a&gt; 脚本启动 &lt;strong&gt;LLM 模型&lt;/strong&gt;服务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/llm_api_launch.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;该方式支持启动多个worker，示例启动方式：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/llm_api_launch.py --model-path-addresss model1@host1@port1 model2@host2@port2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果要启动多卡加载，示例命令如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/llm_api_launch.py --gpus 0,1 --num-gpus 2 --max-gpu-memory 10GiB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注：以如上方式启动LLM服务会以nohup命令在后台运行 fastchat 服务，如需停止服务，可以运行如下命令,但该脚本&lt;strong&gt;仅适用于linux和mac平台&lt;/strong&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/llm_api_shutdown.py --serve all &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;亦可单独停止一个 FastChat 服务模块，可选 [&lt;code&gt;all&lt;/code&gt;, &lt;code&gt;controller&lt;/code&gt;, &lt;code&gt;model_worker&lt;/code&gt;, &lt;code&gt;openai_api_server&lt;/code&gt;]&lt;/p&gt; &#xA;&lt;h5&gt;5.1.3 LoRA 加载&lt;/h5&gt; &#xA;&lt;p&gt;本项目基于 FastChat 加载 LLM 服务，故需以 FastChat 加载 LoRA 路径，即保证路径名称里必须有 peft 这个词，配置文件的名字为 adapter_config.json，peft 路径下包含 model.bin 格式的 LoRA 权重。&lt;/p&gt; &#xA;&lt;p&gt;示例代码如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PEFT_SHARE_BASE_WEIGHTS=true python3 -m fastchat.serve.multi_model_worker \&#xA;    --model-path /data/chris/peft-llama-dummy-1 \&#xA;    --model-names peft-dummy-1 \&#xA;    --model-path /data/chris/peft-llama-dummy-2 \&#xA;    --model-names peft-dummy-2 \&#xA;    --model-path /data/chris/peft-llama-dummy-3 \&#xA;    --model-names peft-dummy-3 \&#xA;    --num-gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;详见 &lt;a href=&#34;https://github.com/lm-sys/fastchat/pull/1905#issuecomment-1627801216&#34;&gt;FastChat 相关 PR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.2 启动 API 服务&lt;/h4&gt; &#xA;&lt;p&gt;本地部署情况下，按照 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#5.1-%E5%90%AF%E5%8A%A8-LLM-%E6%9C%8D%E5%8A%A1&#34;&gt;5.1 节&lt;/a&gt;&lt;strong&gt;启动 LLM 服务后&lt;/strong&gt;，再执行 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/server/api.py&#34;&gt;server/api.py&lt;/a&gt; 脚本启动 &lt;strong&gt;API&lt;/strong&gt; 服务；&lt;/p&gt; &#xA;&lt;p&gt;在线调用API服务的情况下，直接执执行 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/server/api.py&#34;&gt;server/api.py&lt;/a&gt; 脚本启动 &lt;strong&gt;API&lt;/strong&gt; 服务；&lt;/p&gt; &#xA;&lt;p&gt;调用命令示例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动 API 服务后，可访问 &lt;code&gt;localhost:7861&lt;/code&gt; 或 &lt;code&gt;{API 所在服务器 IP}:7861&lt;/code&gt; FastAPI 自动生成的 docs 进行接口查看与测试。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;FastAPI docs 界面&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/fastapi_docs_020_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;5.3 启动 Web UI 服务&lt;/h4&gt; &#xA;&lt;p&gt;按照 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md#5.2-%E5%90%AF%E5%8A%A8-API-%E6%9C%8D%E5%8A%A1&#34;&gt;5.2 节&lt;/a&gt;&lt;strong&gt;启动 API 服务后&lt;/strong&gt;，执行 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/webui.py&#34;&gt;webui.py&lt;/a&gt; 启动 &lt;strong&gt;Web UI&lt;/strong&gt; 服务（默认使用端口 &lt;code&gt;8501&lt;/code&gt;）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ streamlit run webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;使用 Langchain-Chatchat 主题色启动 &lt;strong&gt;Web UI&lt;/strong&gt; 服务（默认使用端口 &lt;code&gt;8501&lt;/code&gt;）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ streamlit run webui.py --theme.base &#34;light&#34; --theme.primaryColor &#34;#165dff&#34; --theme.secondaryBackgroundColor &#34;#f5f5f5&#34; --theme.textColor &#34;#000000&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;或使用以下命令指定启动 &lt;strong&gt;Web UI&lt;/strong&gt; 服务并指定端口号&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ streamlit run webui.py --server.port 666&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Web UI 对话界面：&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/webui_0813_0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Web UI 知识库管理页面：&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/webui_0813_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6 一键启动：&lt;/h3&gt; &#xA;&lt;h4&gt;6.1 api服务一键启动脚本&lt;/h4&gt; &#xA;&lt;p&gt;新增api一键启动脚本，可一键开启fastchat后台服务及本项目提供的langchain api服务,调用示例：&lt;/p&gt; &#xA;&lt;p&gt;调用默认模型：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/api_allinone.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;加载多个非默认模型：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python server/api_allinone.py --model-path-address model1@host1@port1 model2@host2@port2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;多卡启动：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python server/api_allinone.py --model-path-address model@host@port --num-gpus 2 --gpus 0,1 --max-gpu-memory 10GiB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;其他参数详见各脚本及fastchat服务说明。&lt;/p&gt; &#xA;&lt;h4&gt;6.2 webui一键启动脚本&lt;/h4&gt; &#xA;&lt;p&gt;加载本地模型：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python webui_allinone.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;调用远程api服务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python webui_allinone.py --use-remote-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;后台运行webui服务：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python webui_allinone.py --nohup&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;加载多个非默认模型：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python webui_allinone.py --model-path-address model1@host1@port1 model2@host2@port2 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;多卡启动：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python webui_alline.py --model-path-address model@host@port --num-gpus 2 --gpus 0,1 --max-gpu-memory 10GiB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;其他参数详见各脚本及fastchat服务说明。&lt;/p&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;p&gt;参见 &lt;a href=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/docs/FAQ.md&#34;&gt;常见问题&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;路线图&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Langchain 应用 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 本地数据接入 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 接入非结构化文档 &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .md&lt;/li&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .txt&lt;/li&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .docx&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 结构化数据接入 &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; .csv&lt;/li&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; .xlsx&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 分词及召回 &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 接入不同类型 TextSplitter&lt;/li&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 优化依据中文标点符号设计的 ChineseTextSplitter&lt;/li&gt; &#xA;       &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 重新实现上下文拼接召回&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 本地网页接入&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SQL 接入&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 知识图谱/图数据库接入&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 搜索引擎接入 &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Bing 搜索&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; DuckDuckGo 搜索&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Agent 实现&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLM 模型接入 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持通过调用 &lt;a href=&#34;https://github.com/lm-sys/fastchat&#34;&gt;FastChat&lt;/a&gt; api 调用 llm&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 支持 ChatGLM API 等 LLM API 的接入&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Embedding 模型接入 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持调用 HuggingFace 中各开源 Emebdding 模型&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 支持 OpenAI Embedding API 等 Embedding API 的接入&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 基于 FastAPI 的 API 方式调用&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Web UI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 基于 Streamlit 的 Web UI&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;项目交流群&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/qr_code_52.jpg&#34; alt=&#34;二维码&#34; width=&#34;300&#34; height=&#34;300&#34;&gt; &#xA;&lt;p&gt;🎉 langchain-ChatGLM 项目微信交流群，如果你也对本项目感兴趣，欢迎加入群聊参与讨论交流。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>QiuChenlyOpenSource/QQFlacMusicDownloader</title>
    <updated>2023-08-16T01:36:14Z</updated>
    <id>tag:github.com,2023-08-16:/QiuChenlyOpenSource/QQFlacMusicDownloader</id>
    <link href="https://github.com/QiuChenlyOpenSource/QQFlacMusicDownloader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[秋城落叶] QQ 音乐源无损歌曲下载&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;大纲&lt;/h1&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%A4%A7%E7%BA%B2&#34;&gt;大纲&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E4%BB%8B%E7%BB%8D&#34;&gt;介绍&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%B7%B2%E7%9F%A5%E9%97%AE%E9%A2%98&#34;&gt;已知问题&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%85%B3%E4%BA%8E%E7%BD%91%E6%98%93%E4%BA%91%E7%99%BB%E5%BD%95%E5%8A%9F%E8%83%BD%E7%9A%84%E5%A3%B0%E6%98%8E&#34;&gt;关于网易云登录功能的声明:&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%89%8D%E5%90%8E%E7%AB%AF%E6%BA%90%E7%A0%81%E6%96%87%E4%BB%B6%E5%A4%B9&#34;&gt;前后端源码文件夹&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E7%89%B9%E5%88%AB%E5%8A%9F%E8%83%BD&#34;&gt;特别功能&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%B7%B2%E6%94%AF%E6%8C%81%E5%85%83%E6%95%B0%E6%8D%AE%E5%86%85%E5%B5%8C%E6%AD%8C%E8%AF%8D%E5%B0%81%E9%9D%A2%E4%BF%A1%E6%81%AF%E5%86%99%E5%85%A5&#34;&gt;已支持元数据+内嵌歌词封面信息写入&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%8D%B3%E5%B0%86%E5%A2%9E%E5%8A%A0%E6%96%B0%E5%8A%9F%E8%83%BD&#34;&gt;即将增加新功能&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95&#34;&gt;使用方法&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#0-%E4%BB%A3%E7%A0%81%E6%9B%B4%E6%96%B0%E9%A2%91%E7%B9%81%E5%88%87%E8%AE%B0%E5%8F%8A%E6%97%B6%E6%9B%B4%E6%96%B0&#34;&gt;0. 代码更新频繁,切记及时更新&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#1-%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83&#34;&gt;1. 安装环境&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#2-%E8%BF%9B%E5%85%A5%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%9B%AE%E5%BD%95%E4%B8%8B%E5%90%AF%E5%8A%A8%E8%BD%AF%E4%BB%B6&#34;&gt;2. 进入软件包目录下启动软件&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E9%AB%98%E7%BA%A7%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95&#34;&gt;高级搜索方法&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E6%96%B0%E5%A2%9E%E6%AD%8C%E6%9B%B2%E5%85%83%E6%95%B0%E6%8D%AE%E5%88%AE%E5%89%8A%E5%86%85%E5%B5%8C&#34;&gt;新增歌曲元数据刮削内嵌&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E6%96%B0%E5%A2%9E%E7%BD%91%E6%98%93%E4%BA%91%E7%9B%98%E6%AD%8C%E6%9B%B2%E4%BF%A1%E6%81%AF%E7%BA%A0%E9%94%99&#34;&gt;新增网易云盘歌曲信息纠错&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E7%A1%AE%E8%AE%A4%E5%8C%B9%E9%85%8D&#34;&gt;确认匹配&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%8F%96%E6%B6%88%E5%8C%B9%E9%85%8D&#34;&gt;取消匹配&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#docker%E9%95%9C%E5%83%8F%E9%83%A8%E7%BD%B2&#34;&gt;Docker镜像部署&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%AB%AF%E5%8F%A3%E8%AE%BE%E7%BD%AE&#34;&gt;自定义端口设置&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E&#34;&gt;免责声明&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E8%87%B4%E8%B0%A2&#34;&gt;致谢&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%85%B6%E4%BB%96%E8%B5%84%E6%96%99&#34;&gt;其他资料&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/#%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A&#34;&gt;名词解释&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;h1&gt;&lt;p align=&#34;center&#34;&gt;介绍&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Create &amp;amp; Design By QiuChenly&lt;/p&gt; &#xA;&lt;br&gt; 这是一个批量下载 QQ 音乐/酷我音乐/网易云会员无损音质歌曲的脚本,技术含量并不是很大,仅供参考。 &#xA;&lt;p&gt;参考是让你参考代码，不是让你想着法儿去白嫖。 今天你不愿意掏198的数字专辑费用，明天你就会失去赚到198万的机会。&lt;/p&gt; &#xA;&lt;p&gt;眼界决定了人生未来，格局决定了人生上限。哥们主打的就是一个三观。&lt;/p&gt; &#xA;&lt;p&gt;鲁迅曾经说过:舍小我成大我,我不入地狱谁入地狱?&lt;/p&gt; &#xA;&lt;p&gt;所以白嫖的事情我来，付钱你来。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-any&#34;&gt;前端技术:&#xA;Vue3+TS+Pinia+ElementUI Plus&#xA;&#xA;后端技术:&#xA;Python3.11.2 + Flask + Concurrency协程&#xA;&#xA;开发环境:&#xA;BenQ Laptop Core Duo 双核 1.2Ghz + 2GB DDR3&#xA;&#xA;OS : Windows 7 x64&#xA;Env: Python 3.11.2&#xA;IDE: Windows Notepad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;教程只能解决使用方法上的问题，解决不了dinner的问题。&lt;/p&gt; &#xA;&lt;h1&gt;已知问题&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows用户可能体验不是很好，因为很多适配坑。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;QQ音乐搜不到,是因为搜索频率过快。别问我为什么才搜两次就频繁了，你问qq。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;下载歌曲后前端页面没有提示，看命令行窗口就知道有没有下载成功了。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;昨天碰到有人因为MIME严格模式导致无法正常加载js文件，最终页面打开空白的bug，这里声明一下：垃圾Windows是这样的。解决办法如下： 报错信息类似于： &lt;img src=&#34;https://user-images.githubusercontent.com/24793281/235321487-6593d996-a616-4236-ae1f-4fa10211671e.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;p&gt;给后面用Windows碰到一样报错问题的同学解决方法： flak框架下运行仍然报错，这是因为加载xxx.js文件默认为text/plain格式，不能正常解析，解决方法如下图所示，修改注册表即可，将图中Content Type由原来的text/plain改为 application/javascript，然后重新打开项目即可： &lt;img src=&#34;https://user-images.githubusercontent.com/24793281/235321358-2888adb3-d571-48e0-88fc-a2836211232d.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &lt;p&gt;另外再次赞美windows的天才设计&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;关于网易云登录功能的声明:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;第三方网易云API服务器来自于互联网，开发者不承认且不担保其具有官方服务器性质，并对该服务器可能会泄露用户信息的可能性保持怀疑。&lt;/li&gt; &#xA; &lt;li&gt;此服务器不可信，请使用完后修改网易云账号密码避免账号失窃。&lt;/li&gt; &#xA; &lt;li&gt;本项目不会保存任何个人隐私信息，但是不对使用的第三方服务器安全性/隐私性做保证。&lt;/li&gt; &#xA; &lt;li&gt;使用者因使用本服务导致的数据泄露账号失窃等问题由使用者独自承担后果，开发者概不负责。&lt;/li&gt; &#xA; &lt;li&gt;使用本服务即表示你同意以上条款，对于使用者在明知或不知以上条款情况下使用本项目所造成的任何数据/泄露隐私泄漏的后果由使用者自行承担，开发者不对任何数据安全性作保证。&lt;/li&gt; &#xA; &lt;li&gt;本项目为完全公益性项目，谢绝任何形式打赏与钱财赠与，以前不会接受以后也不会接受。&lt;/li&gt; &#xA; &lt;li&gt;使用本项目所造成的一切数据泄露，账号失窃，财产损失等严重后果有用户自行承担风险，开发者已经对用户可能所受到的攻击和安全问题做到了尽可能的提示与告知。&lt;/li&gt; &#xA; &lt;li&gt;如果本项目侵犯了你的合法权益，请电子邮件到&lt;a href=&#34;mailto:qiuchenly@outlook.com&#34;&gt;qiuchenly@outlook.com&lt;/a&gt;,我将第一时间删库跑路。&lt;/li&gt; &#xA; &lt;li&gt;总结: 号没了跟我无关&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;前后端源码文件夹&lt;/h1&gt; &#xA;&lt;p&gt;前端界面源代码: &lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/WebSourceCode&#34;&gt;WebSourceCode&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;后端Flask主代码: &lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/flaskSystem&#34;&gt;flaskSystem&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;提交代码时源码顶部的版权注释尽量不要修改，这是我IDE自动生成的，每次都会自动更新。 如果你需要保留你的修改记录，可以通过Google风格的注释附加在函数注释上:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_4.png&#34; alt=&#34;img_4.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;特别功能&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;功能&lt;/th&gt; &#xA;   &lt;th&gt;状态&lt;/th&gt; &#xA;   &lt;th&gt;附加说明&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;酷我音乐&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;支持Flac和320KbpsMP3下载 根据网友梨花喵的加密算法获取解析&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FreeMyMP3&lt;/td&gt; &#xA;   &lt;td&gt;⌛️&lt;/td&gt; &#xA;   &lt;td&gt;狠狠的加密 v2算法已破解 下载接口参数有问题 暂时不可用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;咪咕音乐&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;可以下载Flac/320KbpsMP3歌曲&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QQ音乐&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;已经修复官方无损解析接口&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;网易云音乐&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;版权问题灰色歌曲没有CDN资源缓存 无法下载&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;基于web的友好界面出来啦&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/media/img_2.png&#34; alt=&#34;img_2.png&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_2.png&#34; alt=&#34;img_2.png&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_3.png&#34; alt=&#34;img_3.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;已支持元数据+内嵌歌词封面信息写入&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img.png&#34; alt=&#34;img.png&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/img_2.png&#34; alt=&#34;img_2.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;目前仅适配: 目前已完整支持全部常见元数据补全 MP3/FLAC/HiRes格式文件补全&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;咪咕音乐&lt;/li&gt; &#xA; &lt;li&gt;酷我音乐&lt;/li&gt; &#xA; &lt;li&gt;QQ音乐 (最全面支持 只要能写入的元数据信息我都写入了包括专辑/歌手/专辑高清图像/专辑简介/流派/歌曲语言...)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;即将增加新功能&lt;/h3&gt; &#xA;&lt;p&gt;功能现在还没法用，不要乱点。你点了是没有反应的。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;本地音乐元数据匹配&lt;/li&gt; &#xA; &lt;li&gt;下载音乐时自动匹配元数据过滤系统 &lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/image.png&#34; alt=&#34;匹配系统&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;使用方法&lt;/h1&gt; &#xA;&lt;h3&gt;0. 代码更新频繁,切记及时更新&lt;/h3&gt; &#xA;&lt;p&gt;有问题先下载最新的代码再看看是不是已经被解决了，而不是拿着几天前的代码问我为什么启动不了。 你怎么不问问你自己一年前追的女神怎么今天跟别人酒店里泡芙都流出来了，为什么你还没追到呢？&lt;/p&gt; &#xA;&lt;p&gt;楼主一般情况下脾气是很温和的，但是架不住小可爱太多了。提的也是可爱dinner问题，令人忍俊不禁莞尔一笑。&lt;/p&gt; &#xA;&lt;p&gt;只有你学校的老师才会耐心解决你的问题，我只对我的学生负责，我不对小可爱负责。&lt;/p&gt; &#xA;&lt;h3&gt;1. 安装环境&lt;/h3&gt; &#xA;&lt;p&gt;首先安装最新的 python3.11.2 到你的操作系统里。&lt;/p&gt; &#xA;&lt;p&gt;Windows用户强制安装此版本: &lt;a href=&#34;https://www.python.org/ftp/python/3.11.2/python-3.11.2-amd64.exe&#34;&gt;点击下载 -&amp;gt; Python3.11.2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-any&#34;&gt;Windows用户须知: 不是Windows用户不用看这一段小作文&#xA;&#xA;首先我在此明确表示我的观点: Windows就是全宇宙最优秀的操作系统。&#xA;&#xA;1.卸载电脑上所有的python版本,哪怕是3.10都不行.&#xA;如果你会玩python,那么你可以不用看这个提示.&#xA;如果你就是个纯纯的新手,我的建议是认真看一下.&#xA;&#xA;2.Windows应用商店的残疾Python版本不要安装.&#xA;装完出问题然后还要明知故问,我会为你送上优美的赞歌.&#xA;计算机技术基础不应该由我给你补.&#xA;&#xA;3.不要把项目放在文件夹路径有空格的目录下&#xA;比如你非要把下载好的代码文件放到&#xA;D:/Program Files(x86)/Test 123/AB CD/&#xA;这种存在空格的路径下面,运行报错再来问我那你确实注定要被我赞美称颂.&#xA;&#xA;4.题外话&#xA;你要实在不会用,就真别用了吧.&#xA;但是你用不了就别到处发&#34;这东西根本没吊用&#34;之类的言论&#xA;你dinner不代表别人dinner。&#xA;球球了,折磨你自己可以,别折磨我.&#xA;&#xA;windows用户如果有任何体验上的问题，请提交代码Merge。不要一句“不行没法用”来反馈。哥们看到这种可爱issues直接给你关了。&#xA;让我们共建和谐互联网大家庭。&#xA;最后，Microsoft Windows我测你的码。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;macOS用户: 系统自带的python3.9版本就够了 不需要另外安装&lt;/p&gt; &#xA;&lt;p&gt;以下所有操作&lt;strong&gt;皆默认假设CMD/Powershell/Bash/zsh的当前目录&lt;/strong&gt;在：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(Windows)&#xA;D:/Downloads/QQFlacMusicDownloader-master/&#xA;在文件夹空白处按住shift+鼠标右击,点击&#34;在此处打开PowerShell&#34;。&#xA;如果你很懂，cd命令进入上面的目录也行。&#xA;&#xA;(macOS/Unix/Linux)&#xA;~/Download/QQFlacMusicDownloader-master/&#xA;终端cd进入上面的目录.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;安装依赖包如果出现 404 错误或者太慢，可以用下面的代码切换到清华大学服务器安装。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 设置python的依赖安装镜像服务器为清华大学服务器&#xA;pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 安装软件依赖必须包&#xA;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. 进入软件包目录下启动软件&lt;/h3&gt; &#xA;&lt;p&gt;终端/控制台 进入到本文件所在的目录 执行以下指令:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 MainServer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;由于Windows的可爱设计,在Windows上可能是&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python MainServer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果你是windows10/11,那么系统很有可能会让你安装Windows应用商店的Python残疾版本，如果你不知道该不该安装的话，看看上面Windows用户须知。&lt;/p&gt; &#xA;&lt;p&gt;启动后应该能看到这些信息，红色警告不是错误，无知可以但不要怀疑哥们的实力。看到类似于下面的输出提示即表示你启动成功。没看到类似下面的信息说明没成功。&lt;/p&gt; &#xA;&lt;p&gt;没成功你也别急，截图+操作环境+操作步骤详细给出一份报告，而不是用几个字&#34;本地环境实测无法启动&#34;来概括。 恕我直言，你不是文曲星，没那本事几个字概括核心要点就不要概括，我理都不想理你。 &lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_1.png&#34; alt=&#34;img_1.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;成功后google chrome/Microsoft Edge/Safari浏览器打开&lt;a href=&#34;http://127.0.0.1:8899&#34;&gt;http://127.0.0.1:8899&lt;/a&gt;即可打开新世界。&lt;/p&gt; &#xA;&lt;p&gt;只在Edge浏览器测试UI通过，建议全部使用Microsoft Edge浏览器访问。&lt;/p&gt; &#xA;&lt;h3&gt;高级搜索方法&lt;/h3&gt; &#xA;&lt;p&gt;目前 &lt;strong&gt;仅支持&lt;/strong&gt; QQ音乐/咪咕音乐搜索。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;指令&lt;/th&gt; &#xA;   &lt;th&gt;例子&lt;/th&gt; &#xA;   &lt;th&gt;id从哪找&lt;/th&gt; &#xA;   &lt;th&gt;作用&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(album) b:+专辑ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_7.png&#34; alt=&#34;img_7.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_6.png&#34; alt=&#34;img_6.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(QQ音乐)获取一张专辑里的歌曲&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(album) b:+专辑ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/img.png&#34; alt=&#34;img.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/img_1.png&#34; alt=&#34;img_1.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(咪咕)获取一张专辑里的歌曲&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(playlist) p:+歌单ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_9.png&#34; alt=&#34;img_9.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_8.png&#34; alt=&#34;img_8.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;获取一张歌单里的所有歌曲,需要注意的是列表下面的分页是无效的 不要切换页面 因为他一次性是加载的整个歌单列表&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(id) id:+歌曲ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_11.png&#34; alt=&#34;img_11.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_10.png&#34; alt=&#34;img_10.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;有时候有些二逼非主流歌曲名称是及其之难以用输入法扣出来的，这个时候可以用id直接获取到这首歌。不想用那些二逼非主流歌曲做演示，用我最爱的杰伦演示给你们看看得了&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(toplist) t:+榜单ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_17.png&#34; alt=&#34;img_17.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_16.png&#34; alt=&#34;img_16.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;获取一张榜单里的歌曲&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;2023.4.14&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;修复歌曲关键词过滤不可定制化搜索的问题，满足搜索部分土嗨神曲的需求。现在可以在搜索界面关闭关键词过滤或者在首页自定义过滤关键词。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;新增歌曲元数据刮削内嵌&lt;/h3&gt; &#xA;&lt;p&gt;开发中，目前效果不等于最终效果。&lt;/p&gt; &#xA;&lt;p&gt;本功能用于为本地音乐文件写入元数据功能，实现内嵌歌词文件/歌手/专辑/封面图片/专辑序号/发布年代等元数据信息，避免播放时一片空白无法加载歌曲信息的问题出现。 主要使用以下接口:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;iTunes 专辑信息接口 + 歌词信息 + 苹果特有的4K超清专辑封面图&lt;/li&gt; &#xA; &lt;li&gt;QQ 音乐专辑信息接口 + 歌词信息 + 800x800低清专辑封面图&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;梗小鬼提问时间&lt;br&gt; Q: 为什么不用酷我/咪咕接口？&lt;br&gt; A: 因为老子高兴。iTunes+QQ基本上全球音乐元数据都全了，怎么，不够你用是吧？&lt;/p&gt; &#xA;&lt;p&gt;为了保证数据精确度，仅支持完整匹配以下文件夹结构:&lt;br&gt; /周杰伦/专辑列表文件夹/周杰伦 - 一路向北.flac&lt;/p&gt; &#xA;&lt;p&gt;例子： &lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_lst2.png&#34; alt=&#34;img_1.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;或已经内置歌手专辑与歌曲名称的flac文件，对于这类文件将从iTunes/QQ中搜索补全剩余专辑信息。（从本项目下载的歌曲均自动已存在此信息）&lt;/p&gt; &#xA;&lt;p&gt;效果预览： &lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_lst1.png&#34; alt=&#34;img.png&#34;&gt; 灰色无任何信息的歌曲就是没有自动匹配元数据的，写入了元数据的歌曲将会拥有精美封面图和完整的歌曲专辑元数据信息---这一切都本地存储在音乐文件中！&lt;/p&gt; &#xA;&lt;h3&gt;新增网易云盘歌曲信息纠错&lt;/h3&gt; &#xA;&lt;p&gt;可以搜索歌曲或指定歌曲ID来关联你上传到网易云盘里的歌曲。 在使用前需要打开网易云登录界面一次更新Cookie才可以访问到你的云盘数据。 加载时由于一次加载1000条数据，所以加载缓慢，请等几秒钟。&lt;/p&gt; &#xA;&lt;h4&gt;确认匹配&lt;/h4&gt; &#xA;&lt;p&gt;通过你设置的ID或者搜索到的歌曲来绑定这首歌的实际歌曲数据。 如果你想取消某首歌的匹配可以点击取消匹配。&lt;/p&gt; &#xA;&lt;h4&gt;取消匹配&lt;/h4&gt; &#xA;&lt;p&gt;取消网易云自动/手动匹配的错误歌曲信息。有时候我们上传的歌曲信息网易云会识别错误，所以我们可以取消匹配或者手动更新。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_12.png&#34; alt=&#34;img_12.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_13.png&#34; alt=&#34;img_13.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;歌曲ID如何获得？&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_14.png&#34; alt=&#34;img_14.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/img_15.png&#34; alt=&#34;img_15.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker镜像部署&lt;/h3&gt; &#xA;&lt;p&gt;可以通过自己自己打包 Docker 进行部署，也可以使用本项目打包好的容器进行部署&lt;/p&gt; &#xA;&lt;p&gt;自己打包 Docker 进行部署执行以下命令：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t dockerimage .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;使用本项目打包好的容器进行部署：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull registry.cn-hangzhou.aliyuncs.com/music_downloader/qq_flac_music_downloader&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker 镜像部署需要进行端口映射，可以采用以下命令进行端口映射：&lt;/p&gt; &#xA;&lt;p&gt;（注意：用你的本地使用目录替换下方“本地目录” 如 E:\music）&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 127.0.0.1:8899:8899 -v 本地目录:/workspace/music -it dockerimage:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;更新方式：先运行 &lt;code&gt;docker ps -a &lt;/code&gt;查看容器名称&lt;/p&gt; &#xA;&lt;p&gt;然后&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker stop 容器名称&#xA;docker rm 容器名称&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;最后，上面&lt;code&gt;docker pull&lt;/code&gt;和&lt;code&gt;docker run&lt;/code&gt;的代码重新执行一遍&lt;/p&gt; &#xA;&lt;p&gt;docker-compose部署方式&lt;/p&gt; &#xA;&lt;p&gt;本地新建txt，重命名为docker-compose.yml （不会修改后缀请百度）&lt;/p&gt; &#xA;&lt;p&gt;复制以下内容，同样注意替换“本地目录”&lt;/p&gt; &#xA;&lt;p&gt;或者你直接下载项目中的docker-compose.yml，然后自行修改本地目录&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;version: &#34;3&#34;&#xA;services:&#xA;  downloader:&#xA;    image: registry.cn-hangzhou.aliyuncs.com/music_downloader/qq_flac_music_downloader&#xA;    container_name: music&#xA;    network_mode: bridge&#xA;    volumes:&#xA;      - 本地目录:/workspace/music&#xA;    ports:&#xA;      - &#34;127.0.0.1:8899:8899&#34;&#xA;    restart: always&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后 打开cmd命令行,cd到docker-compose.yml所在目录 &lt;code&gt;docker-compose up -d&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;需要更新的时候，也是cd到docker-compose.yml所在目录&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose pull&#xA;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;相对docker，更新比较简单，所以个人比较推荐使用docker-compose的方式&lt;/p&gt; &#xA;&lt;h3&gt;自定义端口设置&lt;/h3&gt; &#xA;&lt;p&gt;部分设备会存在 8889 端口被占用的情况，部署时可自定义端口，终端/控制台 进入到本文件所在的目录 执行以下指令:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# []内为可选参数&#xA;python3 MainServer.py [--port 8999]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;docker 部署可自行切换映射本地端口，以解决端口被占用情况。&lt;/p&gt; &#xA;&lt;h1&gt;免责声明&lt;/h1&gt; &#xA;&lt;p&gt;禁止任何形式的商业用途，包括但不仅限于售卖/打赏/获利，不得使用本代码进行任何形式的牟利/贩卖/传播，再次强调仅供个人私下研究学习技术使用，有条件者请支持正版音乐！ 律师函请发给提供这些音乐资源解析服务的网站运营方，本项目仅以纯粹的技术目的去学习研究，如有侵犯到任何人的合法权利，请致信&lt;a href=&#34;mailto:qiuchenly@outlook.com&#34;&gt;qiuchenly@outlook.com&lt;/a&gt;，我将在第一时间删库跑路&lt;/p&gt; &#xA;&lt;p&gt;本项目基于 GPL V3.0 许可证发行，以下协议是对于 GPL V3.0 的补充，如有冲突，以以下协议为准。&lt;/p&gt; &#xA;&lt;p&gt;词语约定：本协议中的“本项目”指QQFlacMusicDownloader项目；“使用者”指签署本协议的使用者；“官方音乐平台”指对本项目内置的包括酷我、网易云、QQ音乐、咪咕等音乐源的官方平台统称；“版权数据”指包括但不限于图像、音频、名字等在内的他人拥有所属版权的数据。&lt;/p&gt; &#xA;&lt;p&gt;本项目的数据来源原理是从各官方音乐平台的公开服务器中拉取数据，经过对数据简单地筛选与合并后进行展示，因此本项目不对数据的准确性负责。 使用本项目的过程中可能会产生版权数据，对于这些版权数据，本项目不拥有它们的所有权，为了避免造成侵权，使用者务必在24小时内清除使用本项目的过程中所产生的版权数据。 本项目内的官方音乐平台别名为本项目内对官方音乐平台的一个称呼，不包含恶意，如果官方音乐平台觉得不妥，可联系本项目更改或移除。 本项目内使用的部分包括但不限于字体、图片等资源来源于互联网，如果出现侵权可联系本项目移除。 由于使用本项目产生的包括由于本协议或由于使用或无法使用本项目而引起的任何性质的任何直接、间接、特殊、偶然或结果性损害（包括但不限于因商誉损失、停工、计算机故障或故障引起的损害赔偿，或任何及所有其他商业损害或损失）由使用者负责。 本项目完全免费，仅供个人私下小范围研究交流学习 python 技术使用, 且开源发布于 GitHub 面向全世界人用作对技术的学习交流，本项目不对项目内的技术可能存在违反当地法律法规的行为作保证，禁止在违反当地法律法规的情况下使用本项目，对于使用者在明知或不知当地法律法规不允许的情况下使用本项目所造成的任何违法违规行为由使用者承担，本项目不承担由此造成的任何直接、间接、特殊、偶然或结果性责任。 若你使用了本项目，将代表你接受以上协议。&lt;/p&gt; &#xA;&lt;p&gt;音乐平台不易，请尊重版权，支持正版。&lt;/p&gt; &#xA;&lt;h1&gt;致谢&lt;/h1&gt; &#xA;&lt;p&gt;感谢所有项目的参与/贡献者，也感谢提出issues的各位用户。&lt;/p&gt; &#xA;&lt;h1&gt;其他资料&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/QiuChenlyOpenSource/QQFlacMusicDownloader/main/md/README.md&#34;&gt;早期接口 QMD Apk的逆向过程&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;名词解释&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;dinner: 低能&lt;/li&gt; &#xA; &lt;li&gt;我测你m: 一眼顶真 鉴定为纯纯的范剑&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>