<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-07T01:31:40Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Datalux/Osintgram</title>
    <updated>2022-06-07T01:31:40Z</updated>
    <id>tag:github.com,2022-06-07:/Datalux/Osintgram</id>
    <link href="https://github.com/Datalux/Osintgram" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Osintgram is a OSINT tool on Instagram. It offers an interactive shell to perform analysis on Instagram account of any users by its nickname&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Osintgram ğŸ”ğŸ“¸&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Datalux/Osintgram/releases/tag/1.3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-1.3-green&#34; alt=&#34;version-1.3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/badge/license-GPLv3-blue&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-GPLv3-blue&#34; alt=&#34;GPLv3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/badge/language-Python3-red&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/language-Python3-red&#34; alt=&#34;Python3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://t.me/osintgram&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Telegram-Channel-blue.svg?sanitize=true&#34; alt=&#34;Telegram&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/badge/Docker-Supported-blue&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Docker-Supported-blue&#34; alt=&#34;Docker&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Osintgram is a &lt;strong&gt;OSINT&lt;/strong&gt; tool on Instagram to collect, analyze, and run reconnaissance.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/Datalux/Osintgram/master/.img/carbon.png&#34; width=&#34;900&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Disclaimer: &lt;strong&gt;FOR EDUCATIONAL PURPOSE ONLY! The contributors do not assume any responsibility for the use of this tool.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Warning: It is advisable to &lt;strong&gt;not&lt;/strong&gt; use your own/primary account when using this tool.&lt;/p&gt; &#xA;&lt;h2&gt;Tools and Commands ğŸ§°&lt;/h2&gt; &#xA;&lt;p&gt;Osintgram offers an interactive shell to perform analysis on Instagram account of any users by its nickname. You can get:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;- addrs           Get all registered addressed by target photos&#xA;- captions        Get user&#39;s photos captions&#xA;- comments        Get total comments of target&#39;s posts&#xA;- followers       Get target followers&#xA;- followings      Get users followed by target&#xA;- fwersemail      Get email of target followers&#xA;- fwingsemail     Get email of users followed by target&#xA;- fwersnumber     Get phone number of target followers&#xA;- fwingsnumber    Get phone number of users followed by target&#xA;- hashtags        Get hashtags used by target&#xA;- info            Get target info&#xA;- likes           Get total likes of target&#39;s posts&#xA;- mediatype       Get user&#39;s posts type (photo or video)&#xA;- photodes        Get description of target&#39;s photos&#xA;- photos          Download user&#39;s photos in output folder&#xA;- propic          Download user&#39;s profile picture&#xA;- stories         Download user&#39;s stories  &#xA;- tagged          Get list of users tagged by target&#xA;- wcommented      Get a list of user who commented target&#39;s photos&#xA;- wtagged         Get a list of user who tagged target&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find detailed commands usage &lt;a href=&#34;https://raw.githubusercontent.com/Datalux/Osintgram/master/doc/COMMANDS.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Datalux/Osintgram/releases/tag/1.3&#34;&gt;&lt;strong&gt;Latest version&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Datalux/Osintgram/master/doc/COMMANDS.md&#34;&gt;Commands&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Datalux/Osintgram/master/doc/CHANGELOG.md&#34;&gt;CHANGELOG&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Can I access the contents of a private profile?&lt;/strong&gt; No, you cannot get information on private profiles. You can only get information from a public profile or a profile you follow. The tools that claim to be successful are scams!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;What is and how I can bypass the &lt;code&gt;challenge_required&lt;/code&gt; error?&lt;/strong&gt; The &lt;code&gt;challenge_required&lt;/code&gt; error means that Instagram notice a suspicious behavior on your profile, so needs to check if you are a real person or a bot. To avoid this you should follow the suggested link and complete the required operation (insert a code, confirm email, etc)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation âš™ï¸&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Fork/Clone/Download this repo&lt;/p&gt; &lt;p&gt;&lt;code&gt;git clone https://github.com/Datalux/Osintgram.git&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the directory&lt;/p&gt; &lt;p&gt;&lt;code&gt;cd Osintgram&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a virtual environment for this project&lt;/p&gt; &lt;p&gt;&lt;code&gt;python3 -m venv venv&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Load the virtual environment&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;On Windows Powershell: &lt;code&gt;.\venv\Scripts\activate.ps1&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;On Linux and Git Bash: &lt;code&gt;source venv/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the &lt;code&gt;credentials.ini&lt;/code&gt; file in the &lt;code&gt;config&lt;/code&gt; folder and write your Instagram account username and password in the corresponding fields&lt;/p&gt; &lt;p&gt;Alternatively, you can run the &lt;code&gt;make setup&lt;/code&gt; command to populate this file for you.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the main.py script in one of two ways&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;As an interactive prompt &lt;code&gt;python3 main.py &amp;lt;target username&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Or execute your command straight away &lt;code&gt;python3 main.py &amp;lt;target username&amp;gt; --command &amp;lt;command&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Docker Quick Start ğŸ³&lt;/h2&gt; &#xA;&lt;p&gt;This section will explain how you can quickly use this image with &lt;code&gt;Docker&lt;/code&gt; or &lt;code&gt;Docker-compose&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Before you can use either &lt;code&gt;Docker&lt;/code&gt; or &lt;code&gt;Docker-compose&lt;/code&gt;, please ensure you do have the following prerequisites met.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; installed - &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docker-composed&lt;/strong&gt; installed (if using Docker-compose) - &lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Credentials&lt;/strong&gt; configured - This can be done manually or by running the &lt;code&gt;make setup&lt;/code&gt; command from the root of this repo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Your container will fail if you do not do step #3 and configure your credentials&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;If docker is installed you can build an image and run this as a container.&lt;/p&gt; &#xA;&lt;p&gt;Build:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t osintgram .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it -v &#34;$PWD/output:/home/osintgram/output&#34; osintgram &amp;lt;target&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;&amp;lt;target&amp;gt;&lt;/code&gt; is the Instagram account you wish to use as your target for recon.&lt;/li&gt; &#xA; &lt;li&gt;The required &lt;code&gt;-i&lt;/code&gt; flag enables an interactive terminal to use commands within the container. &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/run/#assign-name-and-allocate-pseudo-tty---name--it&#34;&gt;docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The required &lt;code&gt;-v&lt;/code&gt; flag mounts a volume between your local filesystem and the container to save to the &lt;code&gt;./output/&lt;/code&gt; folder. &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/run/#mount-volume--v---read-only&#34;&gt;docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The optional &lt;code&gt;--rm&lt;/code&gt; flag removes the container filesystem on completion to prevent cruft build-up. &lt;a href=&#34;https://docs.docker.com/engine/reference/run/#clean-up---rm&#34;&gt;docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The optional &lt;code&gt;-t&lt;/code&gt; flag allocates a pseudo-TTY which allows colored output. &lt;a href=&#34;https://docs.docker.com/engine/reference/run/#foreground&#34;&gt;docs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file this single command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose run osintgram &amp;lt;target&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &lt;code&gt;target&lt;/code&gt; is the Instagram target for recon.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively you may run &lt;code&gt;docker-compose&lt;/code&gt; with the &lt;code&gt;Makefile&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;make run&lt;/code&gt; - Builds and Runs with compose. Prompts for a &lt;code&gt;target&lt;/code&gt; before running.&lt;/p&gt; &#xA;&lt;h3&gt;Makefile (easy mode)&lt;/h3&gt; &#xA;&lt;p&gt;For ease of use with Docker-compose, a &lt;code&gt;Makefile&lt;/code&gt; has been provided.&lt;/p&gt; &#xA;&lt;p&gt;Here is a sample work flow to spin up a container and run &lt;code&gt;osintgram&lt;/code&gt; with just two commands!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;make setup&lt;/code&gt; - Sets up your Instagram credentials&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make run&lt;/code&gt; - Builds and Runs a osintgram container and prompts for a target&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Sample workflow for development:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;make setup&lt;/code&gt; - Sets up your Instagram credentials&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make build-run-testing&lt;/code&gt; - Builds an Runs a container without invoking the &lt;code&gt;main.py&lt;/code&gt; script. Useful for an &lt;code&gt;it&lt;/code&gt; Docker session for development&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make cleanup-testing&lt;/code&gt; - Cleans up the testing container created from &lt;code&gt;build-run-testing&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Development version ğŸ’»&lt;/h2&gt; &#xA;&lt;p&gt;To use the development version with the latest feature and fixes just switch to &lt;code&gt;development&lt;/code&gt; branch using Git:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git checkout development&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;and update to last version using:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git pull origin development&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updating â¬‡ï¸&lt;/h2&gt; &#xA;&lt;p&gt;To update Osintgram with the stable release just pull the latest commit using Git.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you are in the master branch running: &lt;code&gt;git checkout master&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download the latest version: &lt;code&gt;git pull origin master&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing ğŸ’¡&lt;/h2&gt; &#xA;&lt;p&gt;You can propose a feature request opening an issue or a pull request.&lt;/p&gt; &#xA;&lt;p&gt;Here is a list of Osintgram&#39;s contributors:&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/Datalux/Osintgram/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contributors-img.web.app/image?repo=Datalux/Osintgram&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;External library ğŸ”—&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ping/instagram_private_api&#34;&gt;Instagram API&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tianqiraf/DouZero_For_HappyDouDiZhu</title>
    <updated>2022-06-07T01:31:40Z</updated>
    <id>tag:github.com,2022-06-07:/tianqiraf/DouZero_For_HappyDouDiZhu</id>
    <link href="https://github.com/tianqiraf/DouZero_For_HappyDouDiZhu" rel="alternate"></link>
    <summary type="html">&lt;p&gt;åŸºäºDouZeroå®šåˆ¶AIå®æˆ˜æ¬¢ä¹æ–—åœ°ä¸»&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DouZero_For_Happy_DouDiZhu: å°†DouZeroç”¨äºæ¬¢ä¹æ–—åœ°ä¸»å®æˆ˜&lt;/h1&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/kwai/DouZero/main/imgs/douzero_logo.jpg&#34; alt=&#34;Logo&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®åŸºäº&lt;a href=&#34;https://github.com/kwai/DouZero&#34;&gt;DouZero&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ç¯å¢ƒé…ç½®è¯·ç§»æ­¥é¡¹ç›®DouZero&lt;/li&gt; &#xA; &lt;li&gt;æ¨¡å‹é»˜è®¤ä¸ºWPï¼Œæ›´æ¢æ¨¡å‹è¯·ä¿®æ”¹start.pyä¸­çš„æ¨¡å‹è·¯å¾„&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œmain.pyå³å¯&lt;/li&gt; &#xA; &lt;li&gt;SL (&lt;code&gt;baselines/sl/&lt;/code&gt;): åŸºäºäººç±»æ•°æ®è¿›è¡Œæ·±åº¦å­¦ä¹ çš„é¢„è®­ç»ƒæ¨¡å‹&lt;/li&gt; &#xA; &lt;li&gt;DouZero-ADP (&lt;code&gt;baselines/douzero_ADP/&lt;/code&gt;): ä»¥å¹³å‡åˆ†æ•°å·®å¼‚ï¼ˆAverage Difference Points, ADPï¼‰ä¸ºç›®æ ‡è®­ç»ƒçš„Douzeroæ™ºèƒ½ä½“&lt;/li&gt; &#xA; &lt;li&gt;DouZero-WP (&lt;code&gt;baselines/douzero_WP/&lt;/code&gt;): ä»¥èƒœç‡ï¼ˆWinning Percentage, WPï¼‰ä¸ºç›®æ ‡è®­ç»ƒçš„Douzeroæ™ºèƒ½ä½“&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;è¯´æ˜&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¬¢ä¹æ–—åœ°ä¸»çª—å£æ¨¡å¼æœ€å¤§åŒ–è¿è¡Œï¼Œå±å¹•åˆ†è¾¨ç‡1920x1080ã€‚ç”±äºè®¾è®¡åƒç´ çº§æ“ä½œï¼Œè¿è¡Œå‡ºé”™è¯·æ£€æŸ¥æˆªå›¾åŒºåŸŸåæ ‡ï¼ˆä½äº&lt;code&gt;MyPyQT_Form&lt;/code&gt;ç±»ä¸­çš„&lt;code&gt;__init__&lt;/code&gt;å‡½æ•°å†…ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;çª—å£ç§»è‡³å³ä¸‹è§’ï¼Œé¿å…é®æŒ¡æ‰‹ç‰Œï¼Œå†å²ç‰Œï¼Œåº•ç‰ŒåŒºåŸŸã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ä»¥åŠæŠ€æœ¯äº¤æµï¼Œè¯·å‹¿ç”¨äºå…¶å®ƒç›®çš„ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ä½¿ç”¨æ­¥éª¤&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ç¡®è®¤ç¯å¢ƒæ­£å¸¸ï¼Œç­‰å¾…æ‰‹ç‰Œå‡ºç°ã€åº•ç‰Œå‡ºç°ã€åœ°ä¸»è§’è‰²ç¡®è®¤åï¼Œç‚¹å‡»&lt;strong&gt;å¼€å§‹&lt;/strong&gt;ï¼Œè€—æ—¶å‡ ç§’å®Œæˆè¯†åˆ«ã€‚&lt;/li&gt; &#xA; &lt;li&gt;çª—å£å†…æ˜¾ç¤ºè¯†åˆ«ç»“æœï¼Œåœ°ä¸»è§’è‰²ä½¿ç”¨æ·¡çº¢è‰²æ ‡å‡ºã€‚è¯†åˆ«å®Œæˆè‡ªåŠ¨å¼€å§‹è®°å½•å‡ºç‰Œã€‚&lt;/li&gt; &#xA; &lt;li&gt;è§‚å¯ŸAIå»ºè®®çš„å‡ºç‰Œï¼Œåœ¨æ¸¸æˆä¸­æ‰‹åŠ¨é€‰æ‹©å¹¶æ‰“å‡ºã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ¸¸æˆç»“æŸåå¼¹å‡ºå¯¹è¯æ¡†æç¤ºè¾“èµ¢ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯†åˆ«é”™è¯¯æˆ–æ— ååº”å¯é€šè¿‡&lt;strong&gt;ç»“æŸ&lt;/strong&gt;æŒ‰é’®åœæ­¢æœ¬å±€ã€‚è‡³äºæ¸¸æˆï¼Œå°±è‡ªå·±æ‰‹åŠ¨æ‰“å®Œå§ã€‚&lt;/li&gt; &#xA; &lt;li&gt;åæ ‡è‡ªè¡Œè°ƒæ•´è¯·ä½¿ç”¨pos_debug.py&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;æ½œåœ¨Bug&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç‹ç‚¸æ—¶å‡ºç‰Œç‰¹æ•ˆæ—¶é—´è¾ƒé•¿ï¼Œæœ‰ä¸€å®šå‡ ç‡å¯¼è‡´åªèƒ½è¯†åˆ«å‡ºä¸€ä¸ªç‹ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;é¸£è°¢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®åŸºäº&lt;a href=&#34;https://github.com/kwai/DouZero&#34;&gt;DouZero&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;å€Ÿé‰´äº†&lt;a href=&#34;https://github.com/ZDZX-T/cardRecorder&#34;&gt;cardRecorder&lt;/a&gt;é¡¹ç›®çš„éƒ¨åˆ†ä»£ç ä»¥åŠæ¨¡æ¿å›¾ç‰‡ï¼Œç”¨äºè¯†åˆ«æ‰‘å…‹ç‰Œ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç›¸å…³é“¾æ¥&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åšå®¢é“¾æ¥ï¼š&lt;a href=&#34;https://tqraf.cn/2021/07/DouZero-For-HappyDouDiZhu.html&#34;&gt;å¤©å¯çš„åšå®¢&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;æ–‡ç« é“¾æ¥ï¼š&lt;a href=&#34;https://zhuanlan.zhihu.com/p/389439772&#34;&gt;çŸ¥ä¹ä¸“æ &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;æ¼”ç¤ºè§†é¢‘é“¾æ¥ï¼š&lt;a href=&#34;https://b23.tv/9WFP5F&#34;&gt;è§†é¢‘&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;æ¬¢è¿åŠ å…¥QQäº¤æµç¾¤ï¼š754619468ï¼Œå…¥ç¾¤å£ä»¤ï¼šDouZero&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>kwai/DouZero</title>
    <updated>2022-06-07T01:31:40Z</updated>
    <id>tag:github.com,2022-06-07:/kwai/DouZero</id>
    <link href="https://github.com/kwai/DouZero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICML 2021] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning | æ–—åœ°ä¸»AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;[ICML 2021] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning&lt;/h1&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/kwai/DouZero/main/imgs/douzero_logo.jpg&#34; alt=&#34;Logo&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/kwai/DouZero/actions/workflows/python-package.yml&#34;&gt;&lt;img src=&#34;https://github.com/kwai/DouZero/actions/workflows/python-package.yml/badge.svg?sanitize=true&#34; alt=&#34;Building&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/douzero&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/douzero.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/douzero&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/douzero&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/douzero&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/douzero/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/daochenzha/douzero-colab/blob/main/douzero-colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwai/DouZero/main/README.zh-CN.md&#34;&gt;ä¸­æ–‡æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;DouZero is a reinforcement learning framework for &lt;a href=&#34;https://en.wikipedia.org/wiki/Dou_dizhu&#34;&gt;DouDizhu&lt;/a&gt; (&lt;a href=&#34;https://baike.baidu.com/item/%E6%96%97%E5%9C%B0%E4%B8%BB/177997&#34;&gt;æ–—åœ°ä¸»&lt;/a&gt;), the most popular card game in China. It is a shedding-type game where the playerâ€™s objective is to empty oneâ€™s hand of all cards before other players. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. DouZero is developed by AI Platform, Kwai Inc. (å¿«æ‰‹).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Online Demo: &lt;a href=&#34;https://www.douzero.org/&#34;&gt;https://www.douzero.org/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;span&gt;ğŸ“¢&lt;/span&gt; New Version with Bidï¼ˆå«ç‰Œç‰ˆæœ¬ï¼‰: &lt;a href=&#34;https://www.douzero.org/bid&#34;&gt;https://www.douzero.org/bid&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the Demo Locally: &lt;a href=&#34;https://github.com/datamllab/rlcard-showdown&#34;&gt;https://github.com/datamllab/rlcard-showdown&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2106.06135&#34;&gt;https://arxiv.org/abs/2106.06135&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Related Project: &lt;a href=&#34;https://github.com/datamllab/rlcard&#34;&gt;RLCard Project&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Related Resources: &lt;a href=&#34;https://github.com/datamllab/awesome-game-ai&#34;&gt;Awesome-Game-AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Google Colab: &lt;a href=&#34;https://github.com/daochenzha/douzero-colab/raw/main/douzero-colab.ipynb&#34;&gt;jupyter notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Unofficial improved versions of DouZero by the community: &lt;a href=&#34;https://github.com/Vincentzyx/Douzero_Resnet&#34;&gt;[DouZero ResNet]&lt;/a&gt; &lt;a href=&#34;https://github.com/Vincentzyx/DouZero_For_HLDDZ_FullAuto&#34;&gt;[DouZero FullAuto]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Community:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Slack&lt;/strong&gt;: Discuss in &lt;a href=&#34;https://join.slack.com/t/douzero/shared_invite/zt-rg3rygcw-ouxxDk5o4O0bPZ23vpdwxA&#34;&gt;DouZero&lt;/a&gt; channel.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;QQ Group&lt;/strong&gt;: Join our QQ group to discuss. Password: douzeroqqgroup&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Group 1: 819204202&lt;/li&gt; &#xA;   &lt;li&gt;Group 2: 954183174&lt;/li&gt; &#xA;   &lt;li&gt;Group 3: 834954839&lt;/li&gt; &#xA;   &lt;li&gt;Group 4: 211434658&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;News:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks for the contribution of &lt;a href=&#34;https://github.com/Vincentzyx&#34;&gt;@Vincentzyx&lt;/a&gt; for enabling CPU training. Now Windows users can train with CPUs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://douzero.org/public/demo.gif&#34; alt=&#34;Demo&#34;&gt; &#xA;&lt;h2&gt;Cite this Work&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project helpful in your research, please cite our paper:&lt;/p&gt; &#xA;&lt;p&gt;Zha, Daochen et al. â€œDouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning.â€ ICML (2021).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{pmlr-v139-zha21a,&#xA;  title = &#x9; {DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning},&#xA;  author =       {Zha, Daochen and Xie, Jingru and Ma, Wenye and Zhang, Sheng and Lian, Xiangru and Hu, Xia and Liu, Ji},&#xA;  booktitle = &#x9; {Proceedings of the 38th International Conference on Machine Learning},&#xA;  pages = &#x9; {12333--12344},&#xA;  year = &#x9; {2021},&#xA;  editor = &#x9; {Meila, Marina and Zhang, Tong},&#xA;  volume = &#x9; {139},&#xA;  series = &#x9; {Proceedings of Machine Learning Research},&#xA;  month = &#x9; {18--24 Jul},&#xA;  publisher =    {PMLR},&#xA;  pdf = &#x9; {http://proceedings.mlr.press/v139/zha21a/zha21a.pdf},&#xA;  url = &#x9; {http://proceedings.mlr.press/v139/zha21a.html},&#xA;  abstract = &#x9; {Games are abstractions of the real world, where artificial agents learn to compete and cooperate with other agents. While significant achievements have been made in various perfect- and imperfect-information games, DouDizhu (a.k.a. Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. Unfortunately, modern reinforcement learning algorithms mainly focus on simple and small action spaces, and not surprisingly, are shown not to make satisfactory progress in DouDizhu. In this work, we propose a conceptually simple yet effective DouDizhu AI system, namely DouZero, which enhances traditional Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. Starting from scratch in a single server with four GPUs, DouZero outperformed all the existing DouDizhu AI programs in days of training and was ranked the first in the Botzone leaderboard among 344 AI agents. Through building DouZero, we show that classic Monte-Carlo methods can be made to deliver strong results in a hard domain with a complex action space. The code and an online demo are released at https://github.com/kwai/DouZero with the hope that this insight could motivate future work.}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What Makes DouDizhu Challenging?&lt;/h2&gt; &#xA;&lt;p&gt;In addition to the challenge of imperfect information, DouDizhu has huge state and action spaces. In particular, the action space of DouDizhu is 10^4 (see &lt;a href=&#34;https://github.com/datamllab/rlcard#available-environments&#34;&gt;this table&lt;/a&gt;). Unfortunately, most reinforcement learning algorithms can only handle very small action spaces. Moreover, the players in DouDizhu need to both compete and cooperate with others in a partially-observable environment with limited communication, i.e., two Peasants players will play as a team to fight against the Landlord player. Modeling both competing and cooperation is an open research challenge.&lt;/p&gt; &#xA;&lt;p&gt;In this work, we propose Deep Monte Carlo (DMC) algorithm with action encoding and parallel actors. This leads to a very simple yet surprisingly effective solution for DouDizhu. Please read &lt;a href=&#34;https://arxiv.org/abs/2106.06135&#34;&gt;our paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The training code is designed for GPUs. Thus, you need to first install CUDA if you want to train models. You may refer to &lt;a href=&#34;https://docs.nvidia.com/cuda/index.html#installation-guides&#34;&gt;this guide&lt;/a&gt;. For evaluation, CUDA is optional and you can use CPU for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;First, clone the repo with (if you are in China and Github is slow, you can use the mirror in &lt;a href=&#34;https://gitee.com/daochenzha/DouZero&#34;&gt;Gitee&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/kwai/DouZero.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure you have python 3.6+ installed. Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd douzero&#xA;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend installing the stable version of DouZero with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install douzero&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are in China and the above command is too slow, you can use the mirror provided by Tsinghua University:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install douzero -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or install the up-to-date version (it could be not stable) with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that Windows users can only use CPU as actors. See &lt;a href=&#34;https://raw.githubusercontent.com/kwai/DouZero/main/README.md#issues-in-windows&#34;&gt;Issues in Windows&lt;/a&gt; about why GPUs are not supported. Nonetheless, Windows users can still &lt;a href=&#34;https://github.com/datamllab/rlcard-showdown&#34;&gt;run the demo locally&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To use GPU for training, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will train DouZero on one GPU. To train DouZero on multiple GPUs. Use the following arguments.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpu_devices&lt;/code&gt;: what gpu devices are visible&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_actor_devices&lt;/code&gt;: how many of the GPU deveices will be used for simulation, i.e., self-play&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_actors&lt;/code&gt;: how many actor processes will be used for each device&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--training_device&lt;/code&gt;: which device will be used for training DouZero&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, if we have 4 GPUs, where we want to use the first 3 GPUs to have 15 actors each for simulating and the 4th GPU for training, we can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 train.py --gpu_devices 0,1,2,3 --num_actor_devices 3 --num_actors 15 --training_device 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use CPU training or simulation (Windows can only use CPU for actors), use the following arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--training_device cpu&lt;/code&gt;: Use CPU to train the model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--actor_device_cpu&lt;/code&gt;: Use CPU as actors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, use the following command to run everything on CPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 train.py --actor_device_cpu --training_device cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following command only runs actors on CPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 train.py --actor_device_cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more customized configuration of training, see the following optional arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--xpid XPID           Experiment id (default: douzero)&#xA;--save_interval SAVE_INTERVAL&#xA;                      Time interval (in minutes) at which to save the model&#xA;--objective {adp,wp}  Use ADP or WP as reward (default: ADP)&#xA;--actor_device_cpu    Use CPU as actor device&#xA;--gpu_devices GPU_DEVICES&#xA;                      Which GPUs to be used for training&#xA;--num_actor_devices NUM_ACTOR_DEVICES&#xA;                      The number of devices used for simulation&#xA;--num_actors NUM_ACTORS&#xA;                      The number of actors for each simulation device&#xA;--training_device TRAINING_DEVICE&#xA;                      The index of the GPU used for training models. `cpu`&#xA;                &#x9;  means using cpu&#xA;--load_model          Load an existing model&#xA;--disable_checkpoint  Disable saving checkpoint&#xA;--savedir SAVEDIR     Root dir where experiment data will be saved&#xA;--total_frames TOTAL_FRAMES&#xA;                      Total environment frames to train for&#xA;--exp_epsilon EXP_EPSILON&#xA;                      The probability for exploration&#xA;--batch_size BATCH_SIZE&#xA;                      Learner batch size&#xA;--unroll_length UNROLL_LENGTH&#xA;                      The unroll length (time dimension)&#xA;--num_buffers NUM_BUFFERS&#xA;                      Number of shared-memory buffers&#xA;--num_threads NUM_THREADS&#xA;                      Number learner threads&#xA;--max_grad_norm MAX_GRAD_NORM&#xA;                      Max norm of gradients&#xA;--learning_rate LEARNING_RATE&#xA;                      Learning rate&#xA;--alpha ALPHA         RMSProp smoothing constant&#xA;--momentum MOMENTUM   RMSProp momentum&#xA;--epsilon EPSILON     RMSProp epsilon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;The evaluation can be performed with GPU or CPU (GPU will be much faster). Pretrained model is available at &lt;a href=&#34;https://drive.google.com/drive/folders/1NmM2cXnI5CIWHaLJeoDZMiwt6lOTV_UB?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/18g-JUKad6D8rmBONXUDuOQ&#34;&gt;ç™¾åº¦ç½‘ç›˜&lt;/a&gt;, æå–ç : 4624. Put pre-trained weights in &lt;code&gt;baselines/&lt;/code&gt;. The performance is evaluated through self-play. We have provided pre-trained models and some heuristics as baselines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwai/DouZero/main/douzero/evaluation/random_agent.py&#34;&gt;random&lt;/a&gt;: agents that play randomly (uniformly)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kwai/DouZero/main/douzero/evaluation/rlcard_agent.py&#34;&gt;rlcard&lt;/a&gt;: the rule-based agent in &lt;a href=&#34;https://github.com/datamllab/rlcard&#34;&gt;RLCard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SL (&lt;code&gt;baselines/sl/&lt;/code&gt;): the pre-trained deep agents on human data&lt;/li&gt; &#xA; &lt;li&gt;DouZero-ADP (&lt;code&gt;baselines/douzero_ADP/&lt;/code&gt;): the pretrained DouZero agents with Average Difference Points (ADP) as objective&lt;/li&gt; &#xA; &lt;li&gt;DouZero-WP (&lt;code&gt;baselines/douzero_WP/&lt;/code&gt;): the pretrained DouZero agents with Winning Percentage (WP) as objective&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 1: Generate evaluation data&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 generate_eval_data.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some important hyperparameters are as follows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--output&lt;/code&gt;: where the pickled data will be saved&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_games&lt;/code&gt;: how many random games will be generated, default 10000&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 2: Self-Play&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 evaluate.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some important hyperparameters are as follows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--landlord&lt;/code&gt;: which agent will play as Landlord, which can be random, rlcard, or the path of the pre-trained model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--landlord_up&lt;/code&gt;: which agent will play as LandlordUp (the one plays before the Landlord), which can be random, rlcard, or the path of the pre-trained model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--landlord_down&lt;/code&gt;: which agent will play as LandlordDown (the one plays after the Landlord), which can be random, rlcard, or the path of the pre-trained model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--eval_data&lt;/code&gt;: the pickle file that contains evaluation data&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_workers&lt;/code&gt;: how many subprocesses will be used&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpu_device&lt;/code&gt;: which GPU to use. It will use CPU by default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, the following command evaluates DouZero-ADP in Landlord position against random agents&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 evaluate.py --landlord baselines/douzero_ADP/landlord.ckpt --landlord_up random --landlord_down random&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following command evaluates DouZero-ADP in Peasants position against RLCard agents&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 evaluate.py --landlord rlcard --landlord_up baselines/douzero_ADP/landlord_up.ckpt --landlord_down baselines/douzero_ADP/landlord_down.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, our model will be saved in &lt;code&gt;douzero_checkpoints/douzero&lt;/code&gt; every half an hour. We provide a script to help you identify the most recent checkpoint. Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh get_most_recent.sh douzero_checkpoints/douzero/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The most recent model will be in &lt;code&gt;most_recent_model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Issues in Windows&lt;/h2&gt; &#xA;&lt;p&gt;You may encounter &lt;code&gt;operation not supported&lt;/code&gt; error if you use a Windows system to train with GPU as actors. This is because doing multiprocessing on CUDA tensors is not supported in Windows. However, our code extensively operates on the CUDA tensors since the code is optimized for GPUs. Please contact us if you find any solutions!&lt;/p&gt; &#xA;&lt;h2&gt;Core Team&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Algorithm: &lt;a href=&#34;https://github.com/daochenzha&#34;&gt;Daochen Zha&lt;/a&gt;, &lt;a href=&#34;https://github.com/karoka&#34;&gt;Jingru Xie&lt;/a&gt;, Wenye Ma, Sheng Zhang, &lt;a href=&#34;https://xrlian.com/&#34;&gt;Xiangru Lian&lt;/a&gt;, Xia Hu, &lt;a href=&#34;http://jiliu-ml.org/&#34;&gt;Ji Liu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GUI Demo: &lt;a href=&#34;https://github.com/hsywhu&#34;&gt;Songyi Huang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Community contributors: &lt;a href=&#34;https://github.com/Vincentzyx&#34;&gt;@Vincentzyx&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowlegements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The demo is largely based on &lt;a href=&#34;https://github.com/datamllab/rlcard-showdown&#34;&gt;RLCard-Showdown&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code implementation is inspired by &lt;a href=&#34;https://github.com/facebookresearch/torchbeast&#34;&gt;TorchBeast&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>