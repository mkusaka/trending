<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-31T01:39:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>EricGuo5513/momask-codes</title>
    <updated>2024-12-31T01:39:54Z</updated>
    <id>tag:github.com,2024-12-31:/EricGuo5513/momask-codes</id>
    <link href="https://github.com/EricGuo5513/momask-codes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;MoMask: Generative Masked Modeling of 3D Human Motions&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MoMask: Generative Masked Modeling of 3D Human Motions&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ericguo5513.github.io/momask&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.00063&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/MeYourHint/MoMask&#34;&gt;[Huggingface Demo]&lt;/a&gt; &lt;a href=&#34;https://github.com/camenduru/MoMask-colab&#34;&gt;[Colab Demo]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://ericguo5513.github.io/momask/static/images/teaser.png&#34; alt=&#34;teaser_image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find our code or paper helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{guo2023momask,&#xA;      title={MoMask: Generative Masked Modeling of 3D Human Motions}, &#xA;      author={Chuan Guo and Yuxuan Mu and Muhammad Gohar Javed and Sen Wang and Li Cheng},&#xA;      year={2023},&#xA;      eprint={2312.00063},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ“®&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;2023-12-30&lt;/strong&gt; --- For easy webUI BVH visulization, you could try this website &lt;a href=&#34;https://vrm-c.github.io/bvh2vrma/&#34;&gt;bvh2vrma&lt;/a&gt; from this &lt;a href=&#34;https://github.com/vrm-c/bvh2vrma?tab=readme-ov-file&#34;&gt;github&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;2023-12-29&lt;/strong&gt; --- Thanks to Camenduru for supporting the &lt;a href=&#34;https://github.com/camenduru/MoMask-colab&#34;&gt;ğŸ¤—Colab&lt;/a&gt; demo.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;2023-12-27&lt;/strong&gt; --- Release WebUI demo. Quickly try our work on &lt;a href=&#34;https://huggingface.co/spaces/MeYourHint/MoMask&#34;&gt;ğŸ¤—HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;2023-12-19&lt;/strong&gt; --- Release scripts for temporal inpainting.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;2023-12-15&lt;/strong&gt; --- Release codes and models for momask. Including training/eval/generation scripts.&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“¢ &lt;strong&gt;2023-11-29&lt;/strong&gt; --- Initialized the webpage and git project.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Get You Ready&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h3&gt;1. Conda Environment&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate momask&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;We test our code on Python 3.7.13 and PyTorch 1.7.1&lt;/p&gt; &#xA; &lt;h4&gt;Alternative: Pip Installation&lt;/h4&gt; &#xA; &lt;details&gt;&#xA;   We provide an alternative pip installation in case you encounter difficulties setting up the conda environment. &#xA;  &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;h3&gt;2. Models and Dependencies&lt;/h3&gt; &#xA; &lt;h4&gt;Download Pre-trained Models&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code&gt;bash prepare/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Download Evaluation Models and Gloves&lt;/h4&gt; &#xA; &lt;p&gt;For evaluation only.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;bash prepare/download_evaluator.sh&#xA;bash prepare/download_glove.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA; &lt;p&gt;To address the download error related to gdown: &#34;Cannot retrieve the public link of the file. You may need to change the permission to &#39;Anyone with the link&#39;, or have had many accesses&#34;. A potential solution is to run &lt;code&gt;pip install --upgrade --no-cache-dir gdown&lt;/code&gt;, as suggested on &lt;a href=&#34;https://github.com/wkentaro/gdown/issues/43&#34;&gt;https://github.com/wkentaro/gdown/issues/43&lt;/a&gt;. This should help resolve the issue.&lt;/p&gt; &#xA; &lt;h4&gt;(Optional) Download Manually&lt;/h4&gt; &#xA; &lt;p&gt;Visit &lt;a href=&#34;https://drive.google.com/drive/folders/1b3GnAbERH8jAoO5mdWgZhyxHB73n23sK?usp=drive_link&#34;&gt;[Google Drive]&lt;/a&gt; to download the models and evaluators mannually.&lt;/p&gt; &#xA; &lt;h3&gt;3. Get Data&lt;/h3&gt; &#xA; &lt;p&gt;You have two options here:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Skip getting data&lt;/strong&gt;, if you just want to generate motions using &lt;em&gt;own&lt;/em&gt; descriptions.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Get full data&lt;/strong&gt;, if you want to &lt;em&gt;re-train&lt;/em&gt; and &lt;em&gt;evaluate&lt;/em&gt; the model.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;strong&gt;(a). Full data (text + motion)&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Follow the instruction in &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt;, then copy the result dataset to our repository:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;-Download from &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt;, then place result in &lt;code&gt;./dataset/KIT-ML&lt;/code&gt;&lt;/p&gt; &#xA; &lt;h4&gt;&lt;/h4&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Demo&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h3&gt;(a) Generate from a single prompt&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python gen_t2m.py --gpu_id 1 --ext exp1 --text_prompt &#34;A person is running on a treadmill.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;(b) Generate from a prompt file&lt;/h3&gt; &#xA; &lt;p&gt;An example of prompt file is given in &lt;code&gt;./assets/text_prompt.txt&lt;/code&gt;. Please follow the format of &lt;code&gt;&amp;lt;text description&amp;gt;#&amp;lt;motion length&amp;gt;&lt;/code&gt; at each line. Motion length indicates the number of poses, which must be integeter and will be rounded by 4. In our work, motion is in 20 fps.&lt;/p&gt; &#xA; &lt;p&gt;If you write &lt;code&gt;&amp;lt;text description&amp;gt;#NA&lt;/code&gt;, our model will determine a length. Note once there is &lt;strong&gt;one&lt;/strong&gt; NA, all the others will be &lt;strong&gt;NA&lt;/strong&gt; automatically.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python gen_t2m.py --gpu_id 1 --ext exp2 --text_path ./assets/text_prompt.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;A few more parameters you may be interested:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--repeat_times&lt;/code&gt;: number of replications for generation, default &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--motion_length&lt;/code&gt;: specify the number of poses for generation, only applicable in (a).&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The output files are stored under folder &lt;code&gt;./generation/&amp;lt;ext&amp;gt;/&lt;/code&gt;. They are&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;numpy files&lt;/code&gt;: generated motions with shape of (nframe, 22, 3), under subfolder &lt;code&gt;./joints&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;video files&lt;/code&gt;: stick figure animation in mp4 format, under subfolder &lt;code&gt;./animation&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;bvh files&lt;/code&gt;: bvh files of the generated motion, under subfolder &lt;code&gt;./animation&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;We also apply naive foot ik to the generated motions, see files with suffix &lt;code&gt;_ik&lt;/code&gt;. It sometimes works well, but sometimes will fail.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ‘¯&lt;/span&gt; Visualization&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;All the animations are manually rendered in blender. We use the characters from &lt;a href=&#34;https://www.mixamo.com/#/&#34;&gt;mixamo&lt;/a&gt;. You need to download the characters in T-Pose with skeleton.&lt;/p&gt; &#xA; &lt;h3&gt;Retargeting&lt;/h3&gt; &#xA; &lt;p&gt;For retargeting, we found rokoko usually leads to large error on foot. On the other hand, &lt;a href=&#34;https://github.com/nkeeline/Keemap-Blender-Rig-ReTargeting-Addon/releases&#34;&gt;keemap.rig.transfer&lt;/a&gt; shows more precise retargetting. You could watch the &lt;a href=&#34;https://www.youtube.com/watch?v=EG-VCMkVpxg&#34;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt; &#xA; &lt;p&gt;Following these steps:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Download keemap.rig.transfer from the github, and install it in blender.&lt;/li&gt; &#xA;  &lt;li&gt;Import both the motion files (.bvh) and character files (.fbx) in blender.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;Shift + Select&lt;/code&gt; the both source and target skeleton. (Do not need to be Rest Position)&lt;/li&gt; &#xA;  &lt;li&gt;Switch to &lt;code&gt;Pose Mode&lt;/code&gt;, then unfold the &lt;code&gt;KeeMapRig&lt;/code&gt; tool at the top-right corner of the view window.&lt;/li&gt; &#xA;  &lt;li&gt;Load and read the bone mapping file &lt;code&gt;./assets/mapping.json&lt;/code&gt;(or &lt;code&gt;mapping6.json&lt;/code&gt; if it doesn&#39;t work). This file is manually made by us. It works for most characters in mixamo. You could make your own.&lt;/li&gt; &#xA;  &lt;li&gt;Adjust the &lt;code&gt;Number of Samples&lt;/code&gt;, &lt;code&gt;Source Rig&lt;/code&gt;, &lt;code&gt;Destination Rig Name&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Clik &lt;code&gt;Transfer Animation from Source Destination&lt;/code&gt;, wait a few seconds.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;We didn&#39;t tried other retargetting tools. Welcome to comment if you find others are more useful.&lt;/p&gt; &#xA; &lt;h3&gt;Scene&lt;/h3&gt; &#xA; &lt;p&gt;We use this &lt;a href=&#34;https://drive.google.com/file/d/1lg62nugD7RTAIz0Q_YP2iZsxpUzzOkT1/view?usp=sharing&#34;&gt;scene&lt;/a&gt; for animation.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ¬&lt;/span&gt; Temporal Inpainting&lt;/h2&gt; &#xA;&lt;details&gt;&#xA;  We conduct mask-based editing in the m-transformer stage, followed by the regeneration of residual tokens for the entire sequence. To load your own motion, provide the path through `--source_motion`. Utilize `-msec` to specify the mask section, supporting either ratio or frame index. For instance, `-msec 0.3,0.6` with `max_motion_length=196` is equivalent to `-msec 59,118`, indicating the editing of the frame section [59, 118]. &#xA; &lt;pre&gt;&lt;code&gt;python edit_t2m.py --gpu_id 1 --ext exp3 --use_res_model -msec 0.4,0.7 --text_prompt &#34;A man picks something from the ground using his right hand.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note: Presently, the source motion must adhere to the format of a HumanML3D dim-263 feature vector. An example motion vector data from the HumanML3D test set is available in &lt;code&gt;example_data/000612.npy&lt;/code&gt;. To process your own motion data, you can utilize the &lt;code&gt;process_file&lt;/code&gt; function from &lt;code&gt;utils/motion_process.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ‘¾&lt;/span&gt; Train Your Own Models&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You have to train RVQ &lt;strong&gt;BEFORE&lt;/strong&gt; training masked/residual transformers. The latter two can be trained simultaneously.&lt;/p&gt; &#xA; &lt;h3&gt;Train RVQ&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train_vq.py --name rvq_name --gpu_id 1 --dataset_name t2m --batch_size 512 --num_quantizers 6  --max_epoch 500 --quantize_drop_prob 0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Train Masked Transformer&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train_t2m_transformer.py --name mtrans_name --gpu_id 2 --dataset_name t2m --batch_size 64 --vq_name rvq_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Train Residual Transformer&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train_res_transformer.py --name rtrans_name  --gpu_id 2 --dataset_name t2m --batch_size 64 --vq_name rvq_name --cond_drop_prob 0.2 --share_weight&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--dataset_name&lt;/code&gt;: motion dataset, &lt;code&gt;t2m&lt;/code&gt; for HumanML3D and &lt;code&gt;kit&lt;/code&gt; for KIT-ML.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--name&lt;/code&gt;: name your model. This will create to model space as &lt;code&gt;./checkpoints/&amp;lt;dataset_name&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--gpu_id&lt;/code&gt;: GPU id.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--batch_size&lt;/code&gt;: we use &lt;code&gt;512&lt;/code&gt; for rvq training. For masked/residual transformer, we use &lt;code&gt;64&lt;/code&gt; on HumanML3D and &lt;code&gt;16&lt;/code&gt; for KIT-ML.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--num_quantizers&lt;/code&gt;: number of quantization layers, &lt;code&gt;6&lt;/code&gt; is used in our case.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--quantize_drop_prob&lt;/code&gt;: quantization dropout ratio, &lt;code&gt;0.2&lt;/code&gt; is used.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--vq_name&lt;/code&gt;: when training masked/residual transformer, you need to specify the name of rvq model for tokenization.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--cond_drop_prob&lt;/code&gt;: condition drop ratio, for classifier-free guidance. &lt;code&gt;0.2&lt;/code&gt; is used.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--share_weight&lt;/code&gt;: whether to share the projection/embedding weights in residual transformer.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;All the pre-trained models and intermediate results will be saved in space &lt;code&gt;./checkpoints/&amp;lt;dataset_name&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;ğŸ“–&lt;/span&gt; Evaluation&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h3&gt;Evaluate RVQ Reconstruction:&lt;/h3&gt; &#xA; &lt;p&gt;HumanML3D:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_vq.py --gpu_id 0 --name rvq_nq6_dc512_nc512_noshare_qdp0.2 --dataset_name t2m --ext rvq_nq6&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;KIT-ML:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_vq.py --gpu_id 0 --name rvq_nq6_dc512_nc512_noshare_qdp0.2_k --dataset_name kit --ext rvq_nq6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Evaluate Text2motion Generation:&lt;/h3&gt; &#xA; &lt;p&gt;HumanML3D:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_trans_res.py --res_name tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw --dataset_name t2m --name t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns --gpu_id 1 --cond_scale 4 --time_steps 10 --ext evaluation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;KIT-ML:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_trans_res.py --res_name tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw_k --dataset_name kit --name t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns_k --gpu_id 0 --cond_scale 2 --time_steps 10 --ext evaluation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--res_name&lt;/code&gt;: model name of &lt;code&gt;residual transformer&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--name&lt;/code&gt;: model name of &lt;code&gt;masked transformer&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--cond_scale&lt;/code&gt;: scale of classifer-free guidance.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--time_steps&lt;/code&gt;: number of iterations for inference.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--ext&lt;/code&gt;: filename for saving evaluation results.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The final evaluation results will be saved in &lt;code&gt;./checkpoints/&amp;lt;dataset_name&amp;gt;/&amp;lt;name&amp;gt;/eval/&amp;lt;ext&amp;gt;.log&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowlegements&lt;/h2&gt; &#xA;&lt;p&gt;We sincerely thank the open-sourcing of these works where our code is based on:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/DeepMotionEditing/deep-motion-editing&#34;&gt;deep-motion-editing&lt;/a&gt;, &lt;a href=&#34;https://github.com/lucidrains/muse-maskgit-pytorch&#34;&gt;Muse&lt;/a&gt;, &lt;a href=&#34;https://github.com/lucidrains/vector-quantize-pytorch&#34;&gt;vector-quantize-pytorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/Mael-zys/T2M-GPT&#34;&gt;T2M-GPT&lt;/a&gt;, &lt;a href=&#34;https://github.com/GuyTevet/motion-diffusion-model/tree/main&#34;&gt;MDM&lt;/a&gt; and &lt;a href=&#34;https://github.com/ChenFengYe/motion-latent-diffusion/tree/main&#34;&gt;MLD&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code is distributed under an &lt;a href=&#34;https://github.com/EricGuo5513/momask-codes/tree/main?tab=MIT-1-ov-file#readme&#34;&gt;MIT LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that our code depends on other libraries, including SMPL, SMPL-X, PyTorch3D, and uses datasets which each have their own respective licenses that must also be followed.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KwaiKEG/KwaiAgents</title>
    <updated>2024-12-31T01:39:54Z</updated>
    <id>tag:github.com,2024-12-31:/KwaiKEG/KwaiAgents</id>
    <link href="https://github.com/KwaiKEG/KwaiAgents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A generalized information-seeking agent system with Large Language Models (LLMs).&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; English ï½œ &lt;a href=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/README_ZH.md&#34;&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/blob/logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ğŸ“š &lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentInstruct&#34;&gt;Dataset&lt;/a&gt; | ğŸ“š &lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentBench&#34;&gt;Benchmark&lt;/a&gt; | ğŸ¤— &lt;a href=&#34;https://huggingface.co/collections/kwaikeg/kagentlms-6551e685b5ec9f9a077d42ef&#34;&gt;Models&lt;/a&gt; | ğŸ“‘ &lt;a href=&#34;http://arxiv.org/abs/2312.04889&#34;&gt;Paper&lt;/a&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;KwaiAgents is a series of Agent-related works open-sourced by the &lt;a href=&#34;https://github.com/KwaiKEG&#34;&gt;KwaiKEG&lt;/a&gt; from &lt;a href=&#34;https://www.kuaishou.com/en&#34;&gt;Kuaishou Technology&lt;/a&gt;. The open-sourced content includes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentSys-Lite&lt;/strong&gt;: a lite version of the KAgentSys in the paper. While retaining some of the original system&#39;s functionality, KAgentSys-Lite has certain differences and limitations when compared to its full-featured counterpart, such as: (1) a more limited set of tools; (2) a lack of memory mechanisms; (3) slightly reduced performance capabilities; and (4) a different codebase, as it evolves from open-source projects like BabyAGI and Auto-GPT. Despite these modifications, KAgentSys-Lite still delivers comparable performance among numerous open-source Agent systems available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentLMs&lt;/strong&gt;: a series of large language models with agent capabilities such as planning, reflection, and tool-use, acquired through the Meta-agent tuning proposed in the paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentInstruct&lt;/strong&gt;: over 200k Agent-related instructions finetuning data (partially human-edited) proposed in the paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentBench&lt;/strong&gt;: over 3,000 human-edited, automated evaluation data for testing Agent capabilities, with evaluation dimensions including planning, tool-use, reflection, concluding, and profiling.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt;&#xA;   &lt;th&gt;Training Data&lt;/th&gt;&#xA;   &lt;th&gt;Benchmark Data&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/kwaikeg/kagentlms_qwen_7b_mat&#34;&gt;Qwen-7B-MAT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentInstruct&#34;&gt;KAgentInstruct&lt;/a&gt;&lt;p&gt;(upcoming)&lt;/p&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentBench&#34;&gt;KAgentBench&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/kwaikeg/kagentlms_baichuan2_13b_mat&#34;&gt;Baichuan2-13B-MAT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/blob/example.gif&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/blob/overview.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.12.13 - The benchmark and evaluation code &lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentBench&#34;&gt;[link]&lt;/a&gt; released&lt;/li&gt; &#xA; &lt;li&gt;2023.12.08 - Technical report &lt;a href=&#34;https://arxiv.org/abs/2312.04889&#34;&gt;[link]&lt;/a&gt; released&lt;/li&gt; &#xA; &lt;li&gt;2023.11.17 - Initial release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Benchmark Results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Scale&lt;/th&gt; &#xA;   &lt;th&gt;Planning&lt;/th&gt; &#xA;   &lt;th&gt;Tool-use&lt;/th&gt; &#xA;   &lt;th&gt;Reflection&lt;/th&gt; &#xA;   &lt;th&gt;Concluding&lt;/th&gt; &#xA;   &lt;th&gt;Profile&lt;/th&gt; &#xA;   &lt;th&gt;Overall Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5-turbo&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;18.55&lt;/td&gt; &#xA;   &lt;td&gt;15.89&lt;/td&gt; &#xA;   &lt;td&gt;5.32&lt;/td&gt; &#xA;   &lt;td&gt;37.26&lt;/td&gt; &#xA;   &lt;td&gt;35.42&lt;/td&gt; &#xA;   &lt;td&gt;21.72&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;0.15&lt;/td&gt; &#xA;   &lt;td&gt;0.23&lt;/td&gt; &#xA;   &lt;td&gt;0.08&lt;/td&gt; &#xA;   &lt;td&gt;16.60&lt;/td&gt; &#xA;   &lt;td&gt;17.73&lt;/td&gt; &#xA;   &lt;td&gt;5.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM3&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;7.87&lt;/td&gt; &#xA;   &lt;td&gt;6.82&lt;/td&gt; &#xA;   &lt;td&gt;4.49&lt;/td&gt; &#xA;   &lt;td&gt;30.01&lt;/td&gt; &#xA;   &lt;td&gt;30.14&lt;/td&gt; &#xA;   &lt;td&gt;13.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;13.34&lt;/td&gt; &#xA;   &lt;td&gt;10.87&lt;/td&gt; &#xA;   &lt;td&gt;4.73&lt;/td&gt; &#xA;   &lt;td&gt;36.24&lt;/td&gt; &#xA;   &lt;td&gt;34.99&lt;/td&gt; &#xA;   &lt;td&gt;18.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;6.70&lt;/td&gt; &#xA;   &lt;td&gt;10.11&lt;/td&gt; &#xA;   &lt;td&gt;4.25&lt;/td&gt; &#xA;   &lt;td&gt;24.97&lt;/td&gt; &#xA;   &lt;td&gt;19.08&lt;/td&gt; &#xA;   &lt;td&gt;12.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLlama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;0.20&lt;/td&gt; &#xA;   &lt;td&gt;3.44&lt;/td&gt; &#xA;   &lt;td&gt;0.54&lt;/td&gt; &#xA;   &lt;td&gt;15.62&lt;/td&gt; &#xA;   &lt;td&gt;10.66&lt;/td&gt; &#xA;   &lt;td&gt;5.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AgentLM&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;0.17&lt;/td&gt; &#xA;   &lt;td&gt;0.09&lt;/td&gt; &#xA;   &lt;td&gt;0.05&lt;/td&gt; &#xA;   &lt;td&gt;16.30&lt;/td&gt; &#xA;   &lt;td&gt;15.22&lt;/td&gt; &#xA;   &lt;td&gt;4.86&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-MAT&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;31.64&lt;/td&gt; &#xA;   &lt;td&gt;28.26&lt;/td&gt; &#xA;   &lt;td&gt;29.50&lt;/td&gt; &#xA;   &lt;td&gt;44.85&lt;/td&gt; &#xA;   &lt;td&gt;44.78&lt;/td&gt; &#xA;   &lt;td&gt;34.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2-MAT&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;37.27&lt;/td&gt; &#xA;   &lt;td&gt;34.82&lt;/td&gt; &#xA;   &lt;td&gt;32.06&lt;/td&gt; &#xA;   &lt;td&gt;48.01&lt;/td&gt; &#xA;   &lt;td&gt;41.83&lt;/td&gt; &#xA;   &lt;td&gt;38.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Human evaluation. Each result cell shows the pass rate (%) and the average score (in parentheses)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Scale&lt;/th&gt; &#xA;   &lt;th&gt;NoAgent&lt;/th&gt; &#xA;   &lt;th&gt;ReACT&lt;/th&gt; &#xA;   &lt;th&gt;Auto-GPT&lt;/th&gt; &#xA;   &lt;th&gt;KAgentSys&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;57.21% (3.42)&lt;/td&gt; &#xA;   &lt;td&gt;68.66% (3.88)&lt;/td&gt; &#xA;   &lt;td&gt;79.60% (4.27)&lt;/td&gt; &#xA;   &lt;td&gt;83.58% (4.47)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5-turbo&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;47.26% (3.08)&lt;/td&gt; &#xA;   &lt;td&gt;54.23% (3.33)&lt;/td&gt; &#xA;   &lt;td&gt;61.74% (3.53)&lt;/td&gt; &#xA;   &lt;td&gt;64.18% (3.69)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;52.74% (3.23)&lt;/td&gt; &#xA;   &lt;td&gt;51.74% (3.20)&lt;/td&gt; &#xA;   &lt;td&gt;50.25% (3.11)&lt;/td&gt; &#xA;   &lt;td&gt;54.23% (3.27)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;54.23% (3.31)&lt;/td&gt; &#xA;   &lt;td&gt;55.72% (3.36)&lt;/td&gt; &#xA;   &lt;td&gt;57.21% (3.37)&lt;/td&gt; &#xA;   &lt;td&gt;58.71% (3.54)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-MAT&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;58.71% (3.53)&lt;/td&gt; &#xA;   &lt;td&gt;65.67% (3.77)&lt;/td&gt; &#xA;   &lt;td&gt;67.66% (3.87)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2-MAT&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;61.19% (3.60)&lt;/td&gt; &#xA;   &lt;td&gt;66.67% (3.86)&lt;/td&gt; &#xA;   &lt;td&gt;74.13% (4.11)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;User Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Using AgentLMs&lt;/h3&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; to deploy the model inference service. First, you need to install the corresponding packages (for detailed usage, please refer to the documentation of the two projects):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For Qwen-7B-MAT, install the corresponding packages with the following commands&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install vllm&#xA;pip install &#34;fschat[model_worker,webui]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;For Baichuan-13B-MAT, install the corresponding packages with the following commands&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;fschat[model_worker,webui]&#34;&#xA;pip install vllm==0.2.0&#xA;pip install transformers==4.33.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To deploy KAgentLMs, you first need to start the controller in one terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastchat.serve.controller&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Secondly, you should use the following command in another terminal for single-gpu inference service deployment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &lt;code&gt;$model_path&lt;/code&gt; is the local path of the model downloaded. If the GPU does not support Bfloat16, you can add &lt;code&gt;--dtype half&lt;/code&gt; to the command line.&lt;/p&gt; &#xA;&lt;p&gt;Thirdly, start the REST API server in the third terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastchat.serve.openai_api_server --host localhost --port 8888&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can use the curl command to invoke the model same as the OpenAI calling format. Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:8888/v1/chat/completions \&#xA;-H &#34;Content-Type: application/json&#34; \&#xA;-d &#39;{&#34;model&#34;: &#34;kagentlms_qwen_7b_mat&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who is Andy Lau&#34;}]}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, change &lt;code&gt;kagentlms_qwen_7b_mat&lt;/code&gt; to the model you deployed.&lt;/p&gt; &#xA;&lt;h3&gt;Using KAgentSys-Lite&lt;/h3&gt; &#xA;&lt;p&gt;Download and install the KwaiAgents, recommended Python&amp;gt;=3.10&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:KwaiKEG/KwaiAgents.git&#xA;cd KwaiAgents&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;ChatGPT usage&lt;/strong&gt; Declare some environment variables&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=sk-xxxxx&#xA;export WEATHER_API_KEY=xxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The WEATHER_API_KEY is not mandatory, but you need to configure it when asking weather-related questions. You can obtain the API key from &lt;a href=&#34;https://www.weatherapi.com/&#34;&gt;this website&lt;/a&gt; (Same for local model usage).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kagentsys --query=&#34;Who is Andy Lau&#39;s wife?&#34; --llm_name=&#34;gpt-3.5-turbo&#34; --lang=&#34;en&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local model usage&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To use a local model, you need to deploy the corresponding model service as described in the previous chapter&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kagentsys --query=&#34;Who is Andy Lau&#39;s wife?&#34; --llm_name=&#34;kagentlms_qwen_7b_mat&#34; \&#xA;--use_local_llm --local_llm_host=&#34;localhost&#34; --local_llm_port=8888 --lang=&#34;en&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Full command arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;options:&#xA;  -h, --help            show this help message and exit&#xA;  --id ID               ID of this conversation&#xA;  --query QUERY         User query&#xA;  --history HISTORY     History of conversation&#xA;  --llm_name LLM_NAME   the name of llm&#xA;  --use_local_llm       Whether to use local llm&#xA;  --local_llm_host LOCAL_LLM_HOST&#xA;                        The host of local llm service&#xA;  --local_llm_port LOCAL_LLM_PORT&#xA;                        The port of local llm service&#xA;  --tool_names TOOL_NAMES&#xA;                        the name of llm&#xA;  --max_iter_num MAX_ITER_NUM&#xA;                        the number of iteration of agents&#xA;  --agent_name AGENT_NAME&#xA;                        The agent name&#xA;  --agent_bio AGENT_BIO&#xA;                        The agent bio, a short description&#xA;  --agent_instructions AGENT_INSTRUCTIONS&#xA;                        The instructions of how agent thinking, acting, or talking&#xA;  --external_knowledge EXTERNAL_KNOWLEDGE&#xA;                        The link of external knowledge&#xA;  --lang {en,zh}        The language of the overall system&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you need to use the &lt;code&gt;browse_website&lt;/code&gt; tool, you need to configure the &lt;a href=&#34;https://chromedriver.chromium.org/getting-started&#34;&gt;chromedriver&lt;/a&gt; on your server.&lt;/li&gt; &#xA; &lt;li&gt;If the search fails multiple times, it may be because the network cannot access duckduckgo_search. You can solve this by setting the &lt;code&gt;http_proxy&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Using KAgentBench Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We only need two lines to evaluate the agent capabilities like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd benchmark&#xA;python infer_qwen.py qwen_benchmark_res.jsonl&#xA;python benchmark_eval.py ./benchmark_eval.jsonl ./qwen_benchmark_res.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command will give the results like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;plan : 31.64, tooluse : 28.26, reflextion : 29.50, conclusion : 44.85, profile : 44.78, overall : 34.20&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/benchmark/&#34;&gt;benchmark&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{pan2023kwaiagents,&#xA;  author    = {Haojie Pan and&#xA;               Zepeng Zhai and&#xA;               Hao Yuan and&#xA;               Yaojia Lv and&#xA;               Ruiji Fu and&#xA;               Ming Liu and&#xA;               Zhongyuan Wang and&#xA;               Bing Qin&#xA;               },&#xA;  title     = {KwaiAgents: Generalized Information-seeking Agent System with Large Language Models},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2312.04889},&#xA;  year      = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ihmily/DouyinLiveRecorder</title>
    <updated>2024-12-31T01:39:54Z</updated>
    <id>tag:github.com,2024-12-31:/ihmily/DouyinLiveRecorder</id>
    <link href="https://github.com/ihmily/DouyinLiveRecorder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;å¯å¾ªç¯å€¼å®ˆå’Œå¤šäººå½•åˆ¶çš„ç›´æ’­å½•åˆ¶è½¯ä»¶ï¼Œæ”¯æŒæŠ–éŸ³ã€Tiktokã€å¿«æ‰‹ã€è™ç‰™ã€æ–—é±¼ã€Bç«™ã€å°çº¢ä¹¦ç­‰å¹³å°ç›´æ’­å½•åˆ¶ï¼ŒæŠ“å–å¤šå¹³å°ç›´æ’­æºåœ°å€ï¼ŒæŠ–éŸ³æ— æ°´å°è§£æï¼Œå¿«æ‰‹æ— æ°´å°è§£æ&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://socialify.git.ci/ihmily/DouyinLiveRecorder/image?font=Inter&amp;amp;forks=1&amp;amp;language=1&amp;amp;owner=1&amp;amp;pattern=Circuit%20Board&amp;amp;stargazers=1&amp;amp;theme=Light&#34; alt=&#34;video_spider&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’¡ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-3116/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.11.6-blue.svg?sanitize=true&#34; alt=&#34;Python Version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/issues/ihmily/DouyinLiveRecorder.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/downloads/ihmily/DouyinLiveRecorder/total&#34; alt=&#34;Downloads&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸€æ¬¾å¯å¾ªç¯å€¼å®ˆçš„ç›´æ’­å½•åˆ¶å·¥å…·ï¼ŒåŸºäºFFmpegå®ç°å¤šå¹³å°ç›´æ’­æºå½•åˆ¶ï¼Œæ”¯æŒè‡ªå®šä¹‰é…ç½®å½•åˆ¶ä»¥åŠç›´æ’­çŠ¶æ€æ¨é€ã€‚&lt;/p&gt;  &#xA;&lt;h2&gt;ğŸ˜ºå·²æ”¯æŒå¹³å°&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æŠ–éŸ³&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; TikTok&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; å¿«æ‰‹&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; è™ç‰™&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ–—é±¼&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YY&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Bç«™&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; å°çº¢ä¹¦&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; bigo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; blued&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AfreecaTV&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ›´å¤šå¹³å°æ­£åœ¨æ›´æ–°ä¸­&lt;/li&gt; &#xA;&lt;/ul&gt;  &#xA;&lt;h2&gt;ğŸˆé¡¹ç›®ç»“æ„&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;â””â”€â”€ DouyinLiveRecorder/&#xA;    â”œâ”€â”€ /api -&amp;gt; (get live stream api )&#xA;    â”œâ”€â”€ /config -&amp;gt; (config record)&#xA;    â”œâ”€â”€ /log -&amp;gt; (save runing log file)&#xA;    â”œâ”€â”€ /backup_config -&amp;gt; (backup file)&#xA;    â”œâ”€â”€ /libs -&amp;gt; (dll file)&#xA;    â”œâ”€â”€ main.py -&amp;gt; (main file)&#xA;    â”œâ”€â”€ spider.py-&amp;gt; (get live url)&#xA;    â”œâ”€â”€ utils.py -&amp;gt; (contains utility functions)&#xA;    â”œâ”€â”€ web_rid.py -&amp;gt; (get web_rid)&#xA;    â”œâ”€â”€ msg_push.py -&amp;gt; (send live status update message)&#xA;    â”œâ”€â”€ cookies.py -&amp;gt; (get douyin cookies)&#xA;    â”œâ”€â”€ x-bogus.js -&amp;gt; (get douyin xbogus token)&#xA;    â”œâ”€â”€ ffmpeg.exe -&amp;gt; (record video)&#xA;    â”œâ”€â”€ index.html -&amp;gt; (play m3u8 and flv video)&#xA;    â”œâ”€â”€ requirements.txt -&amp;gt; (library dependencies)&#xA;&lt;/code&gt;&lt;/pre&gt;  &#xA;&lt;h2&gt;ğŸŒ±ä½¿ç”¨è¯´æ˜&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;è¿è¡Œä¸»æ–‡ä»¶main.pyå¯åŠ¨ç¨‹åº&lt;/li&gt; &#xA; &lt;li&gt;åœ¨ &lt;code&gt;config&lt;/code&gt; æ–‡ä»¶å¤¹å†…çš„é…ç½®æ–‡ä»¶ä¸­å¯¹å½•åˆ¶è¿›è¡Œé…ç½®ï¼Œå¹¶åœ¨ &lt;code&gt;URL_config.ini&lt;/code&gt; ä¸­æ·»åŠ å½•åˆ¶ç›´æ’­é—´åœ°å€ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æŠ–éŸ³å½•åˆ¶éœ€è¦ä½¿ç”¨åˆ°PCç½‘é¡µç«¯ç›´æ’­é—´é¡µé¢çš„Cookieï¼Œè¯·å…ˆåœ¨config.inié…ç½®æ–‡ä»¶ä¸­æ·»åŠ åå†è¿›è¡ŒæŠ–éŸ³å½•åˆ¶ï¼ˆæœ‰é»˜è®¤çš„cookieï¼Œä½†æœ€å¥½è¿˜æ˜¯è‡ªå·±æ·»åŠ è‡ªå·±çš„ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;å½•åˆ¶Tiktokæ—¶éœ€è¦ç§‘å­¦ä¸Šç½‘ï¼Œè¯·å…ˆåœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®å¼€å¯ä»£ç†å¹¶æ·»åŠ proxy_addré“¾æ¥ å¦‚ï¼š&lt;code&gt;http://127.0.0.1:7890&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;å¯ä»¥åœ¨URL_config.iniä¸­çš„é“¾æ¥å¼€å¤´åŠ ä¸Š#ï¼Œæ­¤æ—¶å°†ä¸ä¼šå½•åˆ¶è¯¥æ¡é“¾æ¥å¯¹åº”çš„ç›´æ’­&lt;/li&gt; &#xA; &lt;li&gt;æµ‹è¯•æ—¶æœ‰å¯èƒ½ä¼šå‡ºç°åœ¨IDEå¦‚Pycharmä¸­è¿è¡Œä»£ç è¿›è¡Œç›´æ’­å½•åˆ¶ï¼Œå½•åˆ¶å‡ºæ¥çš„è§†é¢‘å´æ— æ³•æ­£å¸¸æ’­æ”¾çš„ç°è±¡ï¼Œå¦‚æœé‡åˆ°è¿™ä¸ªé—®é¢˜ åœ¨å‘½ä»¤æ§åˆ¶å°DOSç•Œé¢è¿è¡Œä»£ç ï¼Œå½•åˆ¶å‡ºæ¥çš„è§†é¢‘å³å¯æ­£å¸¸æ’­æ”¾ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å½“åŒæ—¶åœ¨å½•åˆ¶å¤šä¸ªç›´æ’­æ—¶ï¼Œæœ€å¥½çº¿ç¨‹æ•°è®¾ç½®å¤§ä¸€äº›ï¼Œå¦åˆ™å¯èƒ½å‡ºç°å…¶ä¸­ä¸€ä¸ªç›´æ’­å½•åˆ¶å‡ºé”™ã€‚å½“ç„¶è®¾ç½®çš„è¿‡å¤§ä¹Ÿæ²¡ç”¨ï¼Œè¦åŒæ—¶è€ƒè™‘è‡ªèº«ç”µè„‘çš„é…ç½®ï¼Œå¦‚CPUå†…æ ¸æ•°ã€ç½‘ç»œå¸¦å®½ç­‰é™åˆ¶ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœæƒ³ç›´æ¥ä½¿ç”¨æ‰“åŒ…å¥½çš„å½•åˆ¶è½¯ä»¶ï¼Œè¿›å…¥&lt;a href=&#34;https://github.com/ihmily/DouyinLiveRecorder/releases&#34;&gt;Releases&lt;/a&gt; ä¸‹è½½æœ€æ–°å‘å¸ƒçš„ zipå‹ç¼©åŒ…å³å¯ï¼Œæœ‰äº›ç”µè„‘å¯èƒ½ä¼šæŠ¥æ¯’ï¼Œç›´æ¥å¿½ç•¥å³å¯ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœè¦é•¿æ—¶é—´æŒ‚ç€è½¯ä»¶å¾ªç¯ç›‘æµ‹ç›´æ’­ï¼Œæœ€å¥½å¾ªç¯æ—¶é—´è®¾ç½®é•¿ä¸€ç‚¹ï¼Œé¿å…å› è¯·æ±‚é¢‘ç¹å¯¼è‡´è¢«å®˜æ–¹å°ç¦IP ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ€åï¼Œæ¬¢è¿å¤§å®¶forkä»¥åŠprã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;â€ƒ&lt;/p&gt; &#xA;&lt;p&gt;ç›´æ’­é—´é“¾æ¥ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;æŠ–éŸ³ï¼š&#xA;https://live.douyin.com/745964462470&#xA;https://v.douyin.com/iQFeBnt/&#xA;&#xA;TikTokï¼š&#xA;https://www.tiktok.com/@pearlgaga88/live&#xA;&#xA;å¿«æ‰‹ï¼š&#xA;https://live.kuaishou.com/u/yall1102&#xA;&#xA;è™ç‰™ï¼š&#xA;https://www.huya.com/52333&#xA;&#xA;æ–—é±¼ï¼š&#xA;https://www.douyu.com/3637778?dyshid=&#xA;https://www.douyu.com/topic/wzDBLS6?rid=4921614&amp;amp;dyshid=&#xA;&#xA;YY:&#xA;https://www.yy.com/22490906/22490906&#xA;&#xA;Bç«™ï¼š&#xA;https://live.bilibili.com/320&#xA;&#xA;å°çº¢ä¹¦ï¼š&#xA;https://www.xiaohongshu.com/hina/livestream/568980065082002402?appuid=5f3f478a00000000010005b3&amp;amp;apptime=&#xA;&#xA;bigoç›´æ’­ï¼š&#xA;https://www.bigo.tv/cn/716418802&#xA;&#xA;buledç›´æ’­ï¼š&#xA;https://app.blued.cn/live?id=Mp6G2R&#xA;&#xA;AfreecaTVï¼š&#xA;https://play.afreecatv.com/sw7love/249471484&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ç›´æ’­é—´åˆ†äº«åœ°å€å’Œç½‘é¡µç«¯é•¿åœ°å€éƒ½èƒ½æ­£å¸¸è¿›è¡Œå½•åˆ¶ï¼ˆæŠ–éŸ³å°½é‡ç”¨é•¿é“¾æ¥ï¼Œé¿å…å› çŸ­é“¾æ¥è½¬æ¢å¤±æ•ˆå¯¼è‡´ä¸èƒ½æ­£å¸¸å½•åˆ¶ï¼‰ã€‚&lt;/p&gt;  &#xA;&lt;p&gt;è§£ææ¥å£ï¼š&lt;/p&gt; &#xA;&lt;p&gt;è¯¥è§£ææ¥å£ &lt;del&gt;ä»…ä¾›æ¼”ç¤º&lt;/del&gt;(æ¼”ç¤ºæ¥å£æš‚æ—¶åœæ­¢ï¼Œåç»­å†å¼€æ”¾)ï¼Œå¹¶ä¸”åªåŒ…å«æŠ–éŸ³ã€å¿«æ‰‹ã€è™ç‰™ç›´æ’­çš„è§£æï¼Œå…¶ä»–å¹³å°å¦‚æœ‰éœ€è¦è¯·è‡ªè¡Œæ·»åŠ ï¼Œæºç åœ¨è¿™é‡Œ &lt;a href=&#34;https://github.com/ihmily/DouyinLiveRecorder/tree/main/api&#34;&gt;DouyinLiveRecorder/api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-HTTP&#34;&gt;GET https://hmily.vip/api/jx/live/?url=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è¯·æ±‚ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-HTTP&#34;&gt;GET https://hmily.vip/api/jx/live/?url=https://live.douyin.com/573716250978&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è‹¥éœ€è¦å°†æŠ–éŸ³ç›´æ’­é—´çŸ­é“¾æ¥è½¬æ¢ä¸ºé•¿é“¾æ¥ï¼Œä½¿ç”¨ä»¥ä¸‹æ¥å£ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-HTTP&#34;&gt;GET https://hmily.vip/api/jx/live/convert.php?url=https://v.douyin.com/iQLgKSj/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;åœ¨çº¿æ’­æ”¾m3u8å’Œflvè§†é¢‘ç½‘ç«™ï¼š&lt;a href=&#34;https://jx.hmily.vip/play/&#34;&gt;M3U8 åœ¨çº¿è§†é¢‘æ’­æ”¾å™¨ &lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;â€ƒ&lt;/p&gt; &#xA;&lt;h2&gt;â¤ï¸è´¡çŒ®è€…&lt;/h2&gt; &#xA;&lt;p&gt;â€‚â€‚ &lt;a href=&#34;https://github.com/ihmily&#34;&gt;&lt;img src=&#34;https://github.com/ihmily.png?size=50&#34; alt=&#34;Hmily&#34;&gt;&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;h2&gt;â³æäº¤æ—¥å¿—&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;20231210&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;ä¿®å¤å½•åˆ¶åˆ†æ®µbugï¼Œä¿®å¤bigoå½•åˆ¶æ£€æµ‹bug&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;æ–°å¢è‡ªå®šä¹‰ä¿®æ”¹å½•åˆ¶ä¸»æ’­å&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;æ–°å¢AfreecaTVç›´æ’­å½•åˆ¶ï¼Œä¿®å¤æŸäº›å¯èƒ½ä¼šå‘ç”Ÿçš„bug&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231207&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ–°å¢bluedç›´æ’­å½•åˆ¶ï¼Œä¿®å¤YYç›´æ’­å½•åˆ¶ï¼Œæ–°å¢ç›´æ’­ç»“æŸæ¶ˆæ¯æ¨é€&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231206&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ–°å¢bigoç›´æ’­å½•åˆ¶&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231203&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ–°å¢å°çº¢ä¹¦ç›´æ’­å½•åˆ¶ï¼ˆå…¨ç½‘é¦–å‘ï¼‰ï¼Œç›®å‰å°çº¢ä¹¦å®˜æ–¹æ²¡æœ‰åˆ‡æ¢æ¸…æ™°åº¦åŠŸèƒ½ï¼Œå› æ­¤ç›´æ’­å½•åˆ¶ä¹Ÿåªæœ‰é»˜è®¤ç”»è´¨&lt;/li&gt; &#xA;   &lt;li&gt;å°çº¢ä¹¦å½•åˆ¶æš‚æ—¶æ— æ³•å¾ªç¯ç›‘æµ‹ï¼Œæ¯æ¬¡ä¸»æ’­å¼€å¯ç›´æ’­ï¼Œéƒ½è¦é‡æ–°è·å–ä¸€æ¬¡é“¾æ¥&lt;/li&gt; &#xA;   &lt;li&gt;è·å–é“¾æ¥çš„æ–¹å¼ä¸º å°†ç›´æ’­é—´è½¬å‘åˆ°å¾®ä¿¡ï¼Œåœ¨å¾®ä¿¡ä¸­æ‰“å¼€åï¼Œå¤åˆ¶é¡µé¢çš„é“¾æ¥ã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231030&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æœ¬æ¬¡æ›´æ–°åªæ˜¯è¿›è¡Œä¿®å¤ï¼Œæ²¡æ—¶é—´æ–°å¢åŠŸèƒ½ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;æ¬¢è¿å„ä½å¤§ä½¬æpr å¸®å¿™æ›´æ–°ç»´æŠ¤&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230930&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;æ–°å¢æŠ–éŸ³ä»æ¥å£è·å–ç›´æ’­æµï¼Œå¢å¼ºç¨³å®šæ€§&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ä¿®æ”¹å¿«æ‰‹è·å–ç›´æ’­æµçš„æ–¹å¼ï¼Œæ”¹ç”¨ä»å®˜æ–¹æ¥å£è·å–&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ç¥å¤§å®¶ä¸­ç§‹èŠ‚å¿«ä¹ï¼&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230919&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ä¿®å¤äº†å¿«æ‰‹ç‰ˆæœ¬æ›´æ–°åå½•åˆ¶å‡ºé”™çš„é—®é¢˜ï¼Œå¢åŠ äº†å…¶è‡ªåŠ¨è·å–cookie(&lt;del&gt;ç¨³å®šæ€§æœªçŸ¥&lt;/del&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;ä¿®å¤äº†TikTokæ˜¾ç¤ºæ­£åœ¨ç›´æ’­ä½†ä¸è¿›è¡Œå½•åˆ¶çš„é—®é¢˜&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230907&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;ä¿®å¤äº†å› æŠ–éŸ³å®˜æ–¹æ›´æ–°äº†ç‰ˆæœ¬å¯¼è‡´çš„å½•åˆ¶å‡ºé”™ä»¥åŠçŸ­é“¾æ¥è½¬æ¢å‡ºé”™&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ä¿®å¤Bç«™æ— æ³•å½•åˆ¶åŸç”»è§†é¢‘çš„bug&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ä¿®æ”¹äº†é…ç½®æ–‡ä»¶å­—æ®µï¼Œæ–°å¢å„å¹³å°è‡ªå®šä¹‰è®¾ç½®Cookie&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230903&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ä¿®å¤äº†TikTokå½•åˆ¶æ—¶æŠ¥644æ— æ³•å½•åˆ¶çš„é—®é¢˜&lt;/li&gt; &#xA;   &lt;li&gt;æ–°å¢ç›´æ’­çŠ¶æ€æ¨é€åˆ°é’‰é’‰å’Œå¾®ä¿¡çš„åŠŸèƒ½ï¼Œå¦‚æœ‰éœ€è¦è¯·çœ‹ &lt;a href=&#34;https://d04vqdiqwr3.feishu.cn/docx/XFPwdDDvfobbzlxhmMYcvouynDh?from=from_copylink&#34;&gt;è®¾ç½®æ¨é€æ•™ç¨‹&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;æœ€è¿‘æ¯”è¾ƒå¿™ï¼Œå…¶ä»–é—®é¢˜æœ‰æ—¶é—´å†æ›´æ–°&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230816&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ä¿®å¤æ–—é±¼ç›´æ’­ï¼ˆå®˜æ–¹æ›´æ–°äº†å­—æ®µï¼‰å’Œå¿«æ‰‹ç›´æ’­å½•åˆ¶å‡ºé”™çš„é—®é¢˜&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230814&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ–°å¢Bç«™ç›´æ’­å½•åˆ¶&lt;/li&gt; &#xA;   &lt;li&gt;å†™äº†ä¸€ä¸ªåœ¨çº¿æ’­æ”¾M3U8å’ŒFLVè§†é¢‘çš„ç½‘é¡µæºç ï¼Œæ‰“å¼€å³å¯é£Ÿç”¨&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230812&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ–°å¢YYç›´æ’­å½•åˆ¶&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230808&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ä¿®å¤ä¸»æ’­é‡æ–°å¼€æ’­æ— æ³•å†æ¬¡å½•åˆ¶çš„é—®é¢˜&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230807&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;æ–°å¢äº†æ–—é±¼ç›´æ’­å½•åˆ¶&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;ä¿®å¤æ˜¾ç¤ºå½•åˆ¶å®Œæˆä¹‹åä¼šé‡æ–°å¼€å§‹å½•åˆ¶çš„é—®é¢˜&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230805&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;æ–°å¢äº†è™ç‰™ç›´æ’­å½•åˆ¶ï¼Œå…¶æš‚æ—¶åªèƒ½ç”¨flvè§†é¢‘æµè¿›è¡Œå½•åˆ¶&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Web API æ–°å¢äº†å¿«æ‰‹å’Œè™ç‰™è¿™ä¸¤ä¸ªå¹³å°çš„ç›´æ’­æµè§£æï¼ˆTikTokè¦ä»£ç†ï¼‰&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230804&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ–°å¢äº†å¿«æ‰‹ç›´æ’­å½•åˆ¶ï¼Œä¼˜åŒ–äº†éƒ¨åˆ†ä»£ç &lt;/li&gt; &#xA;   &lt;li&gt;ä¸Šä¼ äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è·å–æŠ–éŸ³ç›´æ’­é—´é¡µé¢Cookieçš„ä»£ç ï¼Œå¯ä»¥ç”¨äºå½•åˆ¶&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230803&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;é€šå®µæ›´æ–°&lt;/li&gt; &#xA;   &lt;li&gt;æ–°å¢äº†å›½é™…ç‰ˆæŠ–éŸ³TikTokçš„ç›´æ’­å½•åˆ¶ï¼Œå»é™¤å†—ä½™ ç®€åŒ–äº†éƒ¨åˆ†ä»£ç &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230724&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ–°å¢äº†ä¸€ä¸ªé€šè¿‡æŠ–éŸ³ç›´æ’­é—´åœ°å€è·å–ç›´æ’­è§†é¢‘æµé“¾æ¥çš„APIæ¥å£ï¼Œä¸Šä¼ å³å¯ç”¨&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;â€ƒ&lt;/p&gt; &#xA;&lt;h2&gt;æœ‰é—®é¢˜å¯ä»¥æissue ï¼Œåç»­æˆ‘ä¼šåœ¨è¿™é‡Œä¸æ–­æ›´æ–°å…¶ä»–ç›´æ’­å¹³å°çš„å½•åˆ¶ æ¬¢è¿Star&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;/h4&gt;</summary>
  </entry>
</feed>