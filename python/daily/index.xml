<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-12T01:39:43Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>andyzys/jd_seckill</title>
    <updated>2022-11-12T01:39:43Z</updated>
    <id>tag:github.com,2022-11-12:/andyzys/jd_seckill</id>
    <link href="https://github.com/andyzys/jd_seckill" rel="alternate"></link>
    <summary type="html">&lt;p&gt;京东秒杀商品抢购&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jd_Seckill&lt;/h1&gt; &#xA;&lt;h5&gt;非常感谢原作者 &lt;a href=&#34;https://github.com/zhou-xiaojun/jd_mask&#34;&gt;https://github.com/zhou-xiaojun/jd_mask&lt;/a&gt; 提供的代码&lt;/h5&gt; &#xA;&lt;h5&gt;也非常感谢 &lt;a href=&#34;https://github.com/wlwwu/jd_maotai&#34;&gt;https://github.com/wlwwu/jd_maotai&lt;/a&gt; 进行的优化&lt;/h5&gt; &#xA;&lt;h2&gt;主要功能&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;登陆京东商城（&lt;a href=&#34;http://www.jd.com/&#34;&gt;www.jd.com&lt;/a&gt;） &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;cookies登录 (需要自己手动获取)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;预约茅台 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;定时自动预约&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;秒杀预约后等待抢购 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;定时开始自动抢购&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;运行环境&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python 3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;第三方库&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;需要使用到的库已经放在requirements.txt，使用pip安装的可以使用指令&lt;br&gt; &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用教程&lt;/h2&gt; &#xA;&lt;h4&gt;1. 网页扫码登录&lt;/h4&gt; &#xA;&lt;h4&gt;2. 填写config.ini配置信息&lt;/h4&gt; &#xA;&lt;p&gt;(1)eid,和fp找个普通商品随便下单,然后抓包就能看到,这两个值可以填固定的&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;不会的话参考原作者的issue &lt;a href=&#34;https://github.com/zhou-xiaojun/jd_mask/issues/22&#34;&gt;https://github.com/zhou-xiaojun/jd_mask/issues/22&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;(2)cookies_string,sku_id,DEFAULT_USER_AGENT(和cookie获取同一个地方就会看到.直接复制进去就可以了)&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;这里注意每次扫码登陆后都需要重新获取cookies_string,其他两个不用&lt;br&gt; sku_id我已经按照茅台的填好&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;(3)配置一下时间&lt;/p&gt; &#xA;&lt;p&gt;以上都是必填的.&lt;/p&gt; &#xA;&lt;h4&gt;3.运行main.py&lt;/h4&gt; &#xA;&lt;p&gt;根据提示选择相应功能即可&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mli/autocut</title>
    <updated>2022-11-12T01:39:43Z</updated>
    <id>tag:github.com,2022-11-12:/mli/autocut</id>
    <link href="https://github.com/mli/autocut" rel="alternate"></link>
    <summary type="html">&lt;p&gt;用文本编辑器剪视频&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoCut: 通过字幕来剪切视频&lt;/h1&gt; &#xA;&lt;p&gt;AutoCut对你的视频自动生成字幕。然后你选择需要保留的句子，AutoCut将对你视频中对应的片段裁切并保存。你无需使用视频编辑软件，只需要编辑文本文件即可完成剪切。&lt;/p&gt; &#xA;&lt;h2&gt;使用例子&lt;/h2&gt; &#xA;&lt;p&gt;假如你录制的视频放在 &lt;code&gt;2022-11-04/&lt;/code&gt; 这个文件夹里。那么运行&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autocut -d 2022-11-04&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;提示：如果你使用OBS录屏，可以在 &lt;code&gt;设置-&amp;gt;高级-&amp;gt;录像-&amp;gt;文件名格式&lt;/code&gt; 中将空格改成&lt;code&gt;/&lt;/code&gt;，既 &lt;code&gt;%CCYY-%MM-%DD/%hh-%mm-%ss&lt;/code&gt;。那么视频文件将放在日期命名的文件夹里。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;AutoCut将持续对这个文件夹里视频进行字幕抽取和剪切。例如，你刚完成一个视频录制，保存在 &lt;code&gt;11-28-18.mp4&lt;/code&gt;。AutoCut将生成 &lt;code&gt;11-28-18.md&lt;/code&gt;。你在里面选择需要保留的句子后，AutoCut将剪切出 &lt;code&gt;11-28-18_cut.mp4&lt;/code&gt;，并生成 &lt;code&gt;11-28-18_cut.md&lt;/code&gt; 来预览结果。&lt;/p&gt; &#xA;&lt;p&gt;你可以使用任何的Markdown编辑器。例如我常用VS Code和Typora。下图是通过Typora来对 &lt;code&gt;11-28-18.md&lt;/code&gt; 编辑。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mli/autocut/main/imgs/typora.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;全部完成后在 &lt;code&gt;autocut.md&lt;/code&gt; 里选择需要拼接的视频后，AutoCut将输出 &lt;code&gt;autocut_merged.mp4&lt;/code&gt; 和对应的字幕文件。&lt;/p&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;p&gt;首先安装 Python 包&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/mli/autocut.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;上面将安装 &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch&lt;/a&gt;。如果你需要GPU运行，且默认安装的版本不匹配的话，你可以先安装Pytorch。如果安装 Whipser 出现问题，请参考&lt;a href=&#34;https://github.com/openai/whisper#setup&#34;&gt;官方文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;另外需要安装 &lt;a href=&#34;https://ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# on Ubuntu or Debian&#xA;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&#xA;&#xA;# on Arch Linux&#xA;sudo pacman -S ffmpeg&#xA;&#xA;# on MacOS using Homebrew (https://brew.sh/)&#xA;brew install ffmpeg&#xA;&#xA;# on Windows using Scoop (https://scoop.sh/)&#xA;scoop install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;更多使用选项&lt;/h2&gt; &#xA;&lt;h3&gt;转录某个视频生成&lt;code&gt;.srt&lt;/code&gt;和&lt;code&gt;.md&lt;/code&gt;结果。&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autocut -t 22-52-00.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;如果对转录质量不满意，可以使用更大的模型，例如&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autocut -t 22-52-00.mp4 --whisper-model large&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;默认是&lt;code&gt;small&lt;/code&gt;。更好的模型是&lt;code&gt;medium&lt;/code&gt;和&lt;code&gt;large&lt;/code&gt;，但推荐使用GPU获得更好的速度。也可以使用更快的&lt;code&gt;tiny&lt;/code&gt;和&lt;code&gt;base&lt;/code&gt;，但转录质量会下降。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果你视频中有较多的长停顿，可以用&lt;code&gt;--vad&lt;/code&gt;来使用格外的VAD模型预先识别这些停顿，使得对时间戳识别更准确。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;剪切某个视频&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autocut -c 22-52-00.mp4 22-52-00.srt 22-52-00.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;默认视频比特率是 &lt;code&gt;--bitrate 10m&lt;/code&gt;，你可以根据需要调大调小。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果不习惯Markdown文件，你也可以直接在&lt;code&gt;srt&lt;/code&gt;文件里删除不要的句子，在剪切时不传入&lt;code&gt;md&lt;/code&gt;文件名即可。就是 &lt;code&gt;autocut -c 22-52-00.mp4 22-52-00.srt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如果仅有&lt;code&gt;srt&lt;/code&gt;文件，编辑不方便可以使用如下命令生成&lt;code&gt;md&lt;/code&gt;文件，然后编辑&lt;code&gt;md&lt;/code&gt;文件即可，但此时会完全对照&lt;code&gt;srt&lt;/code&gt;生成，不会出现&lt;code&gt;no speech&lt;/code&gt;等提示文本。&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autocut -m test.srt test.mp4&#xA;autocut -m test.mp4 test.srt # 支持视频和字幕乱序传入&#xA;autocut -m test.srt # 也可以只传入字幕文件&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;一些小提示&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;讲得流利的视频的转录质量会高一些，这因为是Whisper训练数据分布的缘故。对一个视频，你可以先粗选一下句子，然后在剪出来的视频上再剪一次。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;del&gt;最终视频生成的字幕通常还需要做一些小编辑。你可以直接编辑&lt;code&gt;md&lt;/code&gt;文件（比&lt;code&gt;srt&lt;/code&gt;文件更紧凑，且嵌入了视频）。然后使用 &lt;code&gt;autocut -s 22-52-00.md 22-52-00.srt&lt;/code&gt; 来生成更新的字幕 &lt;code&gt;22-52-00_edited.srt&lt;/code&gt;。注意这里会无视句子是不是被选中，而是全部转换成&lt;code&gt;srt&lt;/code&gt;。&lt;/del&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;最终视频生成的字幕通常还需要做一些小编辑。但&lt;code&gt;srt&lt;/code&gt;里面空行太多。你可以使用 &lt;code&gt;autocut -s 22-52-00.srt&lt;/code&gt; 来生成一个紧凑些的版本 &lt;code&gt;22-52-00_compact.srt&lt;/code&gt; 方便编辑（这个格式不合法，但编辑器，例如VS Code，还是会进行语法高亮）。编辑完成后，&lt;code&gt;autocut -s 22-52-00_compact.srt&lt;/code&gt; 转回正常格式。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;用Typora和VS Code编辑markdown都很方便。他们都有对应的快捷键mark一行或者多行。但VS Code视频预览似乎有点问题。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;视频是通过ffmpeg导出。在Apple M1芯片上它用不了GPU，导致导出速度不如专业视频软件。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;常见问题&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;输出的是乱码？&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;AutoCut 默认输出编码是 &lt;code&gt;utf-8&lt;/code&gt;. 确保你的编辑器也使用了&lt;code&gt;utf-8&lt;/code&gt;解码。你可以通过&lt;code&gt;--encoding&lt;/code&gt;指定其他编码格式。但是需要注意生成字幕文件和使用字幕文件剪辑时的编码格式需要一致。例如使用 &lt;code&gt;gbk&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autocut -t test.mp4 --encoding=gbk&#xA;autocut -c test.mp4 test.srt test.md --encoding=gbk&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;如果使用了其他编码格式（如gbk等）生成md文件并用Typora打开后，该文件可能会被Typora自动转码为其他编码格式，此时再通过生成时指定的编码格式进行剪辑时可能会出现编码不支持等报错。因此可以在使用Typora编辑后再通过VS Code等修改到你需要的编码格式进行保存后再使用剪辑功能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;如何使用GPU来转录？&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;当你有Nvidia GPU，而且安装了对应版本的Pytorch的时候，转录是在GPU上进行。你可以通过命令来查看当前是不是支持GPU。&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -c &#34;import torch; print(torch.cuda.is_available())&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;否则你可以在安装autocut前手动安装对应的GPU版本Pytorch。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;使用GPU是报错显存不够。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;whisper的大模型需要一定的GPU显存。如果你的显存不够，你可以用小一点的模型，例如&lt;code&gt;small&lt;/code&gt;。如果你仍然想用大模型，可以通过&lt;code&gt;--device&lt;/code&gt;来强制使用&lt;code&gt;cpu&lt;/code&gt;。例如&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;autocut -t 11-28-18.mp4 --whisper-model large --device cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;能不能直接用 &lt;code&gt;pip install autocut&lt;/code&gt; 安装?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;因为autocut的依赖whisper没有发布pypi包，所以目前只能用 &lt;code&gt;pip install git+https://github.com/mli/autocut.git&lt;/code&gt; 这种方式发布。有需求的同学可以查看whisper模型是不是能直接在 huggingface hub 下载，从而摆脱whisper包的依赖。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;如何参与贡献&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mli/autocut/issues/22&#34;&gt;这里有一些想做的feature&lt;/a&gt;，欢迎贡献&lt;/p&gt; &#xA;&lt;h3&gt;代码结构&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;autocut&#xA;│  .gitignore&#xA;│  LICENSE&#xA;│  README.md # 一般新增或修改需要让使用者知道就需要对应更新 README.md 内容&#xA;│  setup.py&#xA;│&#xA;└─autocut # 核心代码位于 autocut 文件夹中，新增功能的实现也一般在这里面进行修改或新增&#xA;   │  cut.py&#xA;   │  daemon.py&#xA;   │  main.py&#xA;   │  transcribe.py&#xA;   │  utils.py&#xA;   └─ __init__.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;安装依赖&lt;/h3&gt; &#xA;&lt;p&gt;开始安装这个项目的需要的依赖之前，建议先了解一下 Anaconda 或者 venv 的虚拟环境使用，推荐&lt;strong&gt;使用虚拟环境来搭建该项目的开发环境&lt;/strong&gt;。 具体安装方式为在你搭建搭建的虚拟环境之中按照&lt;a href=&#34;https://raw.githubusercontent.com/mli/autocut/main/README.md#%E5%AE%89%E8%A3%85&#34;&gt;上方安装步骤&lt;/a&gt;进行安装。&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;为什么推荐使用虚拟环境开发？&lt;/p&gt; &#xA; &lt;p&gt;一方面是保证各种不同的开发环境之间互相不污染。&lt;/p&gt; &#xA; &lt;p&gt;更重要的是在于这个项目实际上是一个 Python Package，所以在你安装之后 autocut 的代码实际也会变成你的环境依赖。 &lt;strong&gt;因此在你更新代码之后，你需要让将新代码重新安装到环境中，然后才能调用到新的代码。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;开发&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;代码风格目前遵循 PEP-8，可以使用相关的自动格式化软件完成。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt; 主要是全局共用的一些工具方法。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;transcribe.py&lt;/code&gt; 是调用模型生成&lt;code&gt;srt&lt;/code&gt;和&lt;code&gt;md&lt;/code&gt;的部分。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cut.py&lt;/code&gt; 提供根据标记后&lt;code&gt;md&lt;/code&gt;或&lt;code&gt;srt&lt;/code&gt;进行视频剪切合并的功能。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;daemon.py&lt;/code&gt; 提供的是监听文件夹生成字幕和剪切视频的功能。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;main.py&lt;/code&gt; 声明命令行参数，根据输入参数调用对应功能。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;开发过程中请尽量保证修改在正确的地方，以及合理地复用代码， 同时工具函数请尽可能放在&lt;code&gt;utils.py&lt;/code&gt;中。 代码格式目前是遵循 PEP-8，变量命名尽量语义化即可。&lt;/p&gt; &#xA;&lt;p&gt;在开发完成之后，最重要的一点是需要进行&lt;strong&gt;测试&lt;/strong&gt;，请保证提交之前对所有&lt;strong&gt;与你修改直接相关的部分&lt;/strong&gt;以及&lt;strong&gt;你修改会影响到的部分&lt;/strong&gt;都进行了测试，并保证功能的正常。 目前没有测试用例的CI，会在之后进行完善。&lt;/p&gt; &#xA;&lt;h3&gt;提交&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;commit 信息用英文描述清楚你做了哪些修改即可，小写字母开头。&lt;/li&gt; &#xA; &lt;li&gt;最好可以保证一次的 commit 涉及的修改比较小，可以简短地描述清楚，这样也方便之后有修改时的查找。&lt;/li&gt; &#xA; &lt;li&gt;PR 的时候 title 简述有哪些修改， contents 可以具体写下修改内容。&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>DingXiaoH/RepVGG</title>
    <updated>2022-11-12T01:39:43Z</updated>
    <id>tag:github.com,2022-11-12:/DingXiaoH/RepVGG</id>
    <link href="https://github.com/DingXiaoH/RepVGG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RepVGG: Making VGG-style ConvNets Great Again&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RepVGG: Making VGG-style ConvNets Great Again (CVPR-2021) (PyTorch)&lt;/h1&gt; &#xA;&lt;h2&gt;Highlights (Sep. 1st, 2022)&lt;/h2&gt; &#xA;&lt;p&gt;RepVGG and the methodology of re-parameterization have been used in &lt;strong&gt;YOLOv6&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/2209.02976&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/meituan/YOLOv6&#34;&gt;code&lt;/a&gt;) and &lt;strong&gt;YOLOv7&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/2207.02696&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;code&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;I have re-organized this repository and released the RepVGGplus-L2pse model with 84.06% ImageNet accuracy. Will release more RepVGGplus models in this month.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This is a super simple ConvNet architecture that achieves over &lt;strong&gt;84% top-1 accuracy on ImageNet&lt;/strong&gt; with a VGG-like architecture! This repo contains the &lt;strong&gt;pretrained models&lt;/strong&gt;, code for building the model, training, and the conversion from training-time model to inference-time, and &lt;strong&gt;an example of using RepVGG for semantic segmentation&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/megvii-model/RepVGG&#34;&gt;The MegEngine version&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/upczww/TensorRT-RepVGG&#34;&gt;TensorRT implemention with C++ API by @upczww&lt;/a&gt;. Great work!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ZJCV/ZCls&#34;&gt;Another PyTorch implementation by @zjykzj&lt;/a&gt;. He also presented detailed benchmarks &lt;a href=&#34;https://zcls.readthedocs.io/en/latest/benchmark-repvgg/&#34;&gt;here&lt;/a&gt;. Nice work!&lt;/p&gt; &#xA;&lt;p&gt;Included in a famous PyTorch model zoo &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;https://github.com/rwightman/pytorch-image-models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/benjaminjellis/Objax-RepVGG&#34;&gt;Objax implementation and models by @benjaminjellis&lt;/a&gt;. Great work!&lt;/p&gt; &#xA;&lt;p&gt;Included in the &lt;a href=&#34;https://github.com/megvii-research/basecls/tree/main/zoo/public/repvgg&#34;&gt;MegEngine Basecls model zoo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{ding2021repvgg,&#xA;title={Repvgg: Making vgg-style convnets great again},&#xA;author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},&#xA;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;pages={13733--13742},&#xA;year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;From RepVGG to RepVGGplus&lt;/h2&gt; &#xA;&lt;p&gt;We have released an improved architecture named RepVGGplus on top of the original version presented in the CVPR-2021 paper.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;RepVGGplus is deeper&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RepVGGplus has auxiliary classifiers during training, which can also be removed for inference&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) RepVGGplus uses Squeeze-and-Excitation blocks to further improve the performance.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;RepVGGplus outperformed several recent visual transformers with a top-1 accuracy of &lt;strong&gt;84.06%&lt;/strong&gt; and higher throughput. Our training script is based on &lt;a href=&#34;https://github.com/microsoft/Swin-Transformer/&#34;&gt;codebase of Swin Transformer&lt;/a&gt;. The throughput is tested with the Swin codebase as well. We would like to thank the authors of &lt;a href=&#34;https://arxiv.org/abs/2103.14030&#34;&gt;Swin&lt;/a&gt; for their clean and well-structured code.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Train image size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Test size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;ImageNet top-1&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Throughput (examples/second), 320, batchsize=128, 2080Ti)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RepVGGplus-L2pse&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;320&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;84.06%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;147&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swin Transformer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;320&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;320&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;102&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;(&#34;pse&#34; means Squeeze-and-Excitation blocks after ReLU.)&lt;/p&gt; &#xA;&lt;p&gt;Download this model: &lt;a href=&#34;https://drive.google.com/file/d/1x8VNLpfuLzg0xXDVIZv9yIIgqnSMoK-W/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/19YwKCTSPVgJu5Ueg0Q78-w?pwd=rvgg&#34;&gt;Baidu Cloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To train or finetune it, slightly change your training code like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;        #   Build model and data loader as usual&#xA;        for samples, targets in enumerate(train_data_loader):&#xA;            #   ......&#xA;            outputs = model(samples)                        #   Your original code&#xA;            if type(outputs) is dict:                       &#xA;                #   A training-time RepVGGplus outputs a dict. The items are:&#xA;                    #   &#39;main&#39;:     the output of the final layer&#xA;                    #   &#39;*aux*&#39;:    the output of auxiliary classifiers&#xA;                loss = 0&#xA;                for name, pred in outputs.items():&#xA;                    if &#39;aux&#39; in name:&#xA;                        loss += 0.1 * criterion(pred, targets)          #  Assume &#34;criterion&#34; is cross-entropy for classification&#xA;                    else:&#xA;                        loss += criterion(pred, targets)&#xA;            else:&#xA;                loss = criterion(outputs, targets)          #   Your original code&#xA;            #   Backward as usual&#xA;            #   ......&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use it for downstream tasks like semantic segmentation, just discard the aux classifiers and the final FC layer.&lt;/p&gt; &#xA;&lt;p&gt;Pleased note that the custom weight decay trick I described last year turned out to be insignificant in our recent experiments (84.16% ImageNet acc and negligible improvements on other tasks), so I decided to stop using it as a new feature of RepVGGplus. You may try it optionally on your task. Please refer to the last part of this page for details.&lt;/p&gt; &#xA;&lt;h2&gt;Use our pretrained model&lt;/h2&gt; &#xA;&lt;p&gt;You may download &lt;em&gt;all&lt;/em&gt; of the ImageNet-pretrained models reported in the paper from Google Drive (&lt;a href=&#34;https://drive.google.com/drive/folders/1Avome4KvNp0Lqh2QwhXO6L5URQjzCjUq?usp=sharing&#34;&gt;https://drive.google.com/drive/folders/1Avome4KvNp0Lqh2QwhXO6L5URQjzCjUq?usp=sharing&lt;/a&gt;) or Baidu Cloud (&lt;a href=&#34;https://pan.baidu.com/s/1nCsZlMynnJwbUBKn0ch7dQ&#34;&gt;https://pan.baidu.com/s/1nCsZlMynnJwbUBKn0ch7dQ&lt;/a&gt;, the access code is &#34;rvgg&#34;). For the ease of transfer learning on other tasks, they are all training-time models (with identity and 1x1 branches). You may test the accuracy by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node 1 --master_port 12349 main.py --arch [model name] --data-path [/path/to/imagenet] --batch-size 32 --tag test --eval --resume [/path/to/weights/file] --opts DATA.DATASET imagenet DATA.IMG_SIZE [224 or 320]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The valid model names include&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RepVGGplus-L2pse, RepVGG-A0, RepVGG-A1, RepVGG-A2, RepVGG-B0, RepVGG-B1, RepVGG-B1g2, RepVGG-B1g4, RepVGG-B2, RepVGG-B2g2, RepVGG-B2g4, RepVGG-B3, RepVGG-B3g2, RepVGG-B3g4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Convert a training-time RepVGG into the inference-time structure&lt;/h2&gt; &#xA;&lt;p&gt;For a RepVGG model or a model with RepVGG as one of its components (e.g., the backbone), you can convert the whole model by simply calling &lt;strong&gt;switch_to_deploy&lt;/strong&gt; of every RepVGG block. This is the recommended way. Examples are shown in &lt;code&gt;tools/convert.py&lt;/code&gt; and &lt;code&gt;example_pspnet.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    for module in model.modules():&#xA;        if hasattr(module, &#39;switch_to_deploy&#39;):&#xA;            module.switch_to_deploy()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have also released a script for the conversion. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python convert.py RepVGGplus-L2pse-train256-acc84.06.pth RepVGGplus-L2pse-deploy.pth -a RepVGGplus-L2pse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you may build the inference-time model with &lt;code&gt;--deploy&lt;/code&gt;, load the converted weights and test&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node 1 --master_port 12349 main.py --arch RepVGGplus-L2pse --data-path [/path/to/imagenet] --batch-size 32 --tag test --eval --resume RepVGGplus-L2pse-deploy.pth --deploy --opts DATA.DATASET imagenet DATA.IMG_SIZE [224 or 320]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Except for the final conversion after training, you may want to get the equivalent kernel and bias &lt;strong&gt;during training&lt;/strong&gt; in a &lt;strong&gt;differentiable&lt;/strong&gt; way at any time (&lt;code&gt;get_equivalent_kernel_bias&lt;/code&gt; in &lt;code&gt;repvgg.py&lt;/code&gt;). This may help training-based pruning or quantization.&lt;/p&gt; &#xA;&lt;h2&gt;Train from scratch&lt;/h2&gt; &#xA;&lt;h3&gt;Reproduce RepVGGplus-L2pse (not presented in the paper)&lt;/h3&gt; &#xA;&lt;p&gt;To train the recently released RepVGGplus-L2pse from scratch, activate mixup and use &lt;code&gt;--AUG.PRESET raug15&lt;/code&gt; for RandAug.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node 8 --master_port 12349 main.py --arch RepVGGplus-L2pse --data-path [/path/to/imagenet] --batch-size 32 --tag train_from_scratch --output-dir /path/to/save/the/log/and/checkpoints --opts TRAIN.EPOCHS 300 TRAIN.BASE_LR 0.1 TRAIN.WEIGHT_DECAY 4e-5 TRAIN.WARMUP_EPOCHS 5 MODEL.LABEL_SMOOTHING 0.1 AUG.PRESET raug15 AUG.MIXUP 0.2 DATA.DATASET imagenet DATA.IMG_SIZE 256 DATA.TEST_SIZE 320&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Reproduce original RepVGG results reported in the paper&lt;/h3&gt; &#xA;&lt;p&gt;To reproduce the models reported in the CVPR-2021 paper, use no mixup nor RandAug.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node 8 --master_port 12349 main.py --arch [model name] --data-path [/path/to/imagenet] --batch-size 32 --tag train_from_scratch --output-dir /path/to/save/the/log/and/checkpoints --opts TRAIN.EPOCHS 300 TRAIN.BASE_LR 0.1 TRAIN.WEIGHT_DECAY 1e-4 TRAIN.WARMUP_EPOCHS 5 MODEL.LABEL_SMOOTHING 0.1 AUG.PRESET weak AUG.MIXUP 0.0 DATA.DATASET imagenet DATA.IMG_SIZE 224&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The original RepVGG models were trained in 120 epochs with cosine learning rate decay from 0.1 to 0. We used 8 GPUs, global batch size of 256, weight decay of 1e-4 (no weight decay on fc.bias, bn.bias, rbr_dense.bn.weight and rbr_1x1.bn.weight) (weight decay on rbr_identity.weight makes little difference, and it is better to use it in most of the cases), and the same simple data preprocssing as the PyTorch official example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;            trans = transforms.Compose([&#xA;                transforms.RandomResizedCrop(224),&#xA;                transforms.RandomHorizontalFlip(),&#xA;                transforms.ToTensor(),&#xA;                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other released models not presented in the paper&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Apr 25, 2021&lt;/strong&gt;&lt;/em&gt; A deeper RepVGG model achieves &lt;strong&gt;83.55% top-1 accuracy on ImageNet&lt;/strong&gt; with &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html&#34;&gt;SE&lt;/a&gt; blocks and an input resolution of 320x320 (and a wider version achieves &lt;strong&gt;83.67% accuracy&lt;/strong&gt; &lt;em&gt;without SE&lt;/em&gt;). Note that it is trained with 224x224 but tested with 320x320, so that it is still trainable with a global batch size of 256 on a single machine with 8 1080Ti GPUs. If you test it with 224x224, the top-1 accuracy will be 81.82%. It has 1, 8, 14, 24, 1 layers in the 5 stages respectively. The width multipliers are a=2.5 and b=5 (the same as RepVGG-B2). The model name is &#34;RepVGG-D2se&#34;. The code for building the model (repvgg.py) and testing with 320x320 (the testing example below) has been updated and the weights have been released at Google Drive and Baidu Cloud. Please check the links below.&lt;/p&gt; &#xA;&lt;h2&gt;Example 1: use Structural Re-parameterization like this in your own code&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;from repvgg import repvgg_model_convert, create_RepVGG_A0&#xA;train_model = create_RepVGG_A0(deploy=False)&#xA;train_model.load_state_dict(torch.load(&#39;RepVGG-A0-train.pth&#39;))          # or train from scratch&#xA;# do whatever you want with train_model&#xA;deploy_model = repvgg_model_convert(train_model, save_path=&#39;RepVGG-A0-deploy.pth&#39;)&#xA;# do whatever you want with deploy_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;deploy_model = create_RepVGG_A0(deploy=True)&#xA;deploy_model.load_state_dict(torch.load(&#39;RepVGG-A0-deploy.pth&#39;))&#xA;# do whatever you want with deploy_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use RepVGG as a component of another model, the conversion is as simple as calling &lt;strong&gt;switch_to_deploy&lt;/strong&gt; of every RepVGG block.&lt;/p&gt; &#xA;&lt;h2&gt;Example 2: use RepVGG as the backbone for downstream tasks&lt;/h2&gt; &#xA;&lt;p&gt;I would suggest you use popular frameworks like MMDetection and MMSegmentation. The features from any stage or layer of RepVGG can be fed into the task-specific heads. If you are not familiar with such frameworks and just would like to see a simple example, please check &lt;code&gt;example_pspnet.py&lt;/code&gt;, which shows how to use RepVGG as the backbone of PSPNet for semantic segmentation: 1) build a PSPNet with RepVGG backbone, 2) load the ImageNet-pretrained weights, 3) convert the whole model with &lt;strong&gt;switch_to_deploy&lt;/strong&gt;, 4) save and use the converted model for inference.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;p&gt;RepVGG works fine with FP16 but the accuracy may decrease when directly quantized to INT8. If IN8 quantization is essential to your application, we suggest three practical solutions.&lt;/p&gt; &#xA;&lt;h3&gt;Solution A: RepOptimizer&lt;/h3&gt; &#xA;&lt;p&gt;I strongly recommend trying RepOptimizer if quantization is essential to your application. RepOptimizer directly trains a VGG-like model via Gradient Re-parameterization without any structural conversions. Quantizing a VGG-like model trained with RepOptimizer is as easy as quantizing a regular model. RepOptimizer has already been used in YOLOv6.&lt;/p&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2205.15242&#34;&gt;https://arxiv.org/abs/2205.15242&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code: &lt;a href=&#34;https://github.com/DingXiaoH/RepOptimizers&#34;&gt;https://github.com/DingXiaoH/RepOptimizers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tutorial provided by the authors of YOLOv6: &lt;a href=&#34;https://github.com/meituan/YOLOv6/raw/main/docs/tutorial_repopt.md&#34;&gt;https://github.com/meituan/YOLOv6/blob/main/docs/tutorial_repopt.md&lt;/a&gt;. Great work! Many thanks!&lt;/p&gt; &#xA;&lt;h3&gt;Solution B: custom quantization-aware training&lt;/h3&gt; &#xA;&lt;p&gt;Another choice is is to constrain the equivalent kernel (get_equivalent_kernel_bias() in repvgg.py) to be low-bit (e.g., make every param in {-127, -126, .., 126, 127} for int8), instead of constraining the params of every kernel separately for an ordinary model.&lt;/p&gt; &#xA;&lt;h3&gt;Solution C: use the off-the-shelf toolboxes&lt;/h3&gt; &#xA;&lt;p&gt;(TODO: check and refactor the code of this example)&lt;/p&gt; &#xA;&lt;p&gt;For the simplicity, we can also use the off-the-shelf quantization toolboxes to quantize RepVGG. We use the simple QAT (quantization-aware training) tool in torch.quantization as an example.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Given the base model converted into the inference-time structure. We insert BN after the converted 3x3 conv layers because QAT with torch.quantization requires BN. Specifically, we run the model on ImageNet training set and record the mean/std statistics and use them to initialize the BN layers, and initialize BN.gamma/beta accordingly so that the saved model has the same outputs as the inference-time model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python quantization/convert.py RepVGG-A0.pth RepVGG-A0_base.pth -a RepVGG-A0 &#xA;python quantization/insert_bn.py [imagenet-folder] RepVGG-A0_base.pth RepVGG-A0_withBN.pth -a RepVGG-A0 -b 32 -n 40000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Build the model, prepare it for QAT (torch.quantization.prepare_qat), and conduct QAT. This is only an example and the hyper-parameters may not be optimal.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python quantization/quant_qat_train.py [imagenet-folder] -j 32 --epochs 20 -b 256 --lr 1e-3 --weight-decay 4e-5 --base-weights RepVGG-A0_withBN.pth --tag quanttest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: Is the inference-time model&#39;s output the &lt;em&gt;same&lt;/em&gt; as the training-time model?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: Yes. You can verify that by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python tools/verify.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: How to use the pretrained RepVGG models for other tasks?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: It is better to finetune the training-time RepVGG models on your datasets. Then you should do the conversion after finetuning and before you deploy the models. For example, say you want to use PSPNet for semantic segmentation, you should build a PSPNet with a training-time RepVGG model as the backbone, load pre-trained weights into the backbone, and finetune the PSPNet on your segmentation dataset. Then you should convert the backbone following the code provided in this repo and keep the other task-specific structures (the PSPNet parts, in this case). The pseudo code will be like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#   train_backbone = create_RepVGG_B2(deploy=False)&#xA;#   train_backbone.load_state_dict(torch.load(&#39;RepVGG-B2-train.pth&#39;))&#xA;#   train_pspnet = build_pspnet(backbone=train_backbone)&#xA;#   segmentation_train(train_pspnet)&#xA;#   deploy_pspnet = repvgg_model_convert(train_pspnet)&#xA;#   segmentation_test(deploy_pspnet)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There is an example in &lt;strong&gt;example_pspnet.py&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finetuning with a converted RepVGG also makes sense if you insert a BN after each conv (please see the quantization example), but the performance may be slightly lower.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: I tried to finetune your model with multiple GPUs but got an error. Why are the names of params like &#34;stage1.0.rbr_dense.conv.weight&#34; in the downloaded weight file but sometimes like &#34;module.stage1.0.rbr_dense.conv.weight&#34; (shown by nn.Module.named_parameters()) in my model?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: DistributedDataParallel may prefix &#34;module.&#34; to the name of params and cause a mismatch when loading weights by name. The simplest solution is to load the weights (model.load_state_dict(...)) before DistributedDataParallel(model). Otherwise, you may insert &#34;module.&#34; before the names like this&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;checkpoint = torch.load(...)    # This is just a name-value dict&#xA;ckpt = {(&#39;module.&#39; + k) : v for k, v in checkpoint.items()}&#xA;model.load_state_dict(ckpt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Likewise, if the param names in the checkpoint file start with &#34;module.&#34; but those in your model do not, you may strip the names like line 50 in test.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ckpt = {k.replace(&#39;module.&#39;, &#39;&#39;):v for k,v in checkpoint.items()}   # strip the names&#xA;model.load_state_dict(ckpt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: So a RepVGG model derives the equivalent 3x3 kernels before each forwarding to save computations?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;: No! More precisely, we do the conversion only once right after training. Then the training-time model can be discarded, and the resultant model only has 3x3 kernels. We only save and use the resultant model.&lt;/p&gt; &#xA;&lt;h2&gt;An optional trick with a custom weight decay (deprecated)&lt;/h2&gt; &#xA;&lt;p&gt;This is deprecated. Please check &lt;code&gt;repvggplus_custom_L2.py&lt;/code&gt;. The intuition is to add regularization on the equivalent kernel. It may work in some cases.&lt;/p&gt; &#xA;&lt;p&gt;The trained model can be downloaded at &lt;a href=&#34;https://drive.google.com/file/d/14I1jWU4rS4y0wdxm03SnEVP1Tx6GGfKu/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1qFGmgJ6Ir6W3wAcCBQb9-w?pwd=rvgg&#34;&gt;Baidu Cloud&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The training code should be changed like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;        #   Build model and data loader as usual&#xA;        for samples, targets in enumerate(train_data_loader):&#xA;            #   ......&#xA;            outputs = model(samples)                        #   Your original code&#xA;            if type(outputs) is dict:                       &#xA;                #   A training-time RepVGGplus outputs a dict. The items are:&#xA;                    #   &#39;main&#39;:     the output of the final layer&#xA;                    #   &#39;*aux*&#39;:    the output of auxiliary classifiers&#xA;                    #   &#39;L2&#39;:       the custom L2 regularization term&#xA;                loss = WEIGHT_DECAY * 0.5 * outputs[&#39;L2&#39;]&#xA;                for name, pred in outputs.items():&#xA;                    if name == &#39;L2&#39;:&#xA;                        pass&#xA;                    elif &#39;aux&#39; in name:&#xA;                        loss += 0.1 * criterion(pred, targets)          #  Assume &#34;criterion&#34; is cross-entropy for classification&#xA;                    else:&#xA;                        loss += criterion(pred, targets)&#xA;            else:&#xA;                loss = criterion(outputs, targets)          #   Your original code&#xA;            #   Backward as usual&#xA;            #   ......&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;mailto:xiaohding@gmail.com&#34;&gt;xiaohding@gmail.com&lt;/a&gt;&lt;/strong&gt; (The original Tsinghua mailbox &lt;a href=&#34;mailto:dxh17@mails.tsinghua.edu.cn&#34;&gt;dxh17@mails.tsinghua.edu.cn&lt;/a&gt; will expire in several months)&lt;/p&gt; &#xA;&lt;p&gt;Google Scholar Profile: &lt;a href=&#34;https://scholar.google.com/citations?user=CIjw0KoAAAAJ&amp;amp;hl=en&#34;&gt;https://scholar.google.com/citations?user=CIjw0KoAAAAJ&amp;amp;hl=en&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Homepage: &lt;a href=&#34;https://dingxiaohan.xyz/&#34;&gt;https://dingxiaohan.xyz/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;My open-sourced papers and repos:&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Structural Re-parameterization Universe&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;RepLKNet (CVPR 2022) &lt;strong&gt;Powerful efficient architecture with very large kernels (31x31) and guidelines for using large kernels in model CNNs&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.06717&#34;&gt;Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/RepLKNet-pytorch&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;RepOptimizer&lt;/strong&gt; uses &lt;strong&gt;Gradient Re-parameterization&lt;/strong&gt; to train powerful models efficiently. The training-time model is as simple as the inference-time. It also addresses the problem of quantization. &lt;strong&gt;It has already been used in YOLOv6.&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2205.15242.pdf&#34;&gt;Re-parameterizing Your Optimizers rather than Architectures&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/RepOptimizers&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RepVGG (CVPR 2021) &lt;strong&gt;A super simple and powerful VGG-style ConvNet architecture&lt;/strong&gt;. Up to &lt;strong&gt;84.16%&lt;/strong&gt; ImageNet top-1 accuracy!&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2101.03697&#34;&gt;RepVGG: Making VGG-style ConvNets Great Again&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/RepVGG&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;RepMLP (CVPR 2022) &lt;strong&gt;MLP-style building block and Architecture&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.11081&#34;&gt;RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/RepMLP&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ResRep (ICCV 2021) &lt;strong&gt;State-of-the-art&lt;/strong&gt; channel pruning (Res50, 55% FLOPs reduction, 76.15% acc)&lt;br&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_ResRep_Lossless_CNN_Pruning_via_Decoupling_Remembering_and_Forgetting_ICCV_2021_paper.pdf&#34;&gt;ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/ResRep&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ACB (ICCV 2019) is a CNN component without any inference-time costs. The first work of our Structural Re-parameterization Universe.&lt;br&gt; &lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf&#34;&gt;ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks&lt;/a&gt;.&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/ACNet&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;DBB (CVPR 2021) is a CNN component with higher performance than ACB and still no inference-time costs. Sometimes I call it ACNet v2 because &#34;DBB&#34; is 2 bits larger than &#34;ACB&#34; in ASCII (lol).&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.13425&#34;&gt;Diverse Branch Block: Building a Convolution as an Inception-like Unit&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/DiverseBranchBlock&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Model compression and acceleration&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;(CVPR 2019) Channel pruning: &lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Centripetal_SGD_for_Pruning_Very_Deep_Convolutional_Networks_With_Complicated_CVPR_2019_paper.html&#34;&gt;Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/Centripetal-SGD&#34;&gt;code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(ICML 2019) Channel pruning: &lt;a href=&#34;http://proceedings.mlr.press/v97/ding19a.html&#34;&gt;Approximated Oracle Filter Pruning for Destructive CNN Width Optimization&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/AOFP&#34;&gt;code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(NeurIPS 2019) Unstructured pruning: &lt;a href=&#34;http://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks.pdf&#34;&gt;Global Sparse Momentum SGD for Pruning Very Deep Neural Networks&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/DingXiaoH/GSM-SGD&#34;&gt;code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>