<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-21T01:43:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Andre0512/pyhOn</title>
    <updated>2024-01-21T01:43:46Z</updated>
    <id>tag:github.com,2024-01-21:/Andre0512/pyhOn</id>
    <link href="https://github.com/Andre0512/pyhOn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Control hOn devices with python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Announcement: I have to take the project down in the next few days&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Dear User,&lt;/p&gt; &#xA; &lt;p&gt;We are writing to inform you that we have discovered two Home Assistant integration plug-ins developed by you ( &lt;a href=&#34;https://github.com/Andre0512/hon&#34;&gt;https://github.com/Andre0512/hon&lt;/a&gt; and &lt;a href=&#34;https://github.com/Andre0512/pyhOn&#34;&gt;https://github.com/Andre0512/pyhOn&lt;/a&gt; ) that are in violation of our terms of service. Specifically, the plug-ins are using our services in an unauthorized manner which is causing significant economic harm to our Company. We take the protection of our intellectual property very seriously and demand that you immediately cease and desist all illegal activities related to the development and distribution of these plug-ins. We also request that you remove the plug-ins from all stores and code hosting platforms where they are currently available. Please be advised that we will take all necessary legal action to protect our interests if you fail to comply with this notice. We reserve the right to pursue all available remedies, including but not limited to monetary damages, injunctive relief, and attorney&#39;s fees. We strongly urge you to take immediate action to rectify this situation and avoid any further legal action. If you have any questions or concerns, please do not hesitate to contact us.&lt;/p&gt; &#xA; &lt;p&gt;Haier Europe Security and Governance Department&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;This python package is unofficial and is not related in any way to Haier. It was developed by reversed engineered requests and can stop working at anytime!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;pyhOn&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pyhOn&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/pyhOn&#34; alt=&#34;PyPI - Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyhOn&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pyhOn?color=blue&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pyhOn&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Andre0512/pyhOn/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/pyhOn&#34; alt=&#34;PyPI - License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/pyhon&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/pyhOn&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt;&lt;br&gt; Control your Haier, Candy and Hoover appliances with python! The idea behind this library is, to make the use of all available commands as simple as possible.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pyhOn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick overview&lt;/h3&gt; &#xA;&lt;p&gt;To get an idea of what is possible, use the commandline-tool &lt;code&gt;pyhOn&lt;/code&gt;. This command requests all available options of connected appliances from the hOn api of your Haier Account.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;$ pyhOn --user example@mail.com --password pass123&#xA;========== WM - Waschmaschine ==========&#xA;data:&#xA;  attributes:&#xA;    parameters:&#xA;      ...&#xA;      texture: 1&#xA;      totalElectricityUsed: 28.71&#xA;      totalWashCycle: 35&#xA;      totalWaterUsed: 2494&#xA;      transMode: 0&#xA;      ...&#xA;settings:&#xA;  startProgram:&#xA;    rinseIterations:&#xA;      max: 5&#xA;      min: 3&#xA;      step: 1&#xA;    spinSpeed:&#xA;      - 0&#xA;      - 400&#xA;      - 600&#xA;      - 800&#xA;      ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python-API&lt;/h2&gt; &#xA;&lt;h3&gt;List devices&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from pyhon import Hon&#xA;&#xA;async def devices_example():&#xA;    async with Hon(USER, PASSWORD) as hon:&#xA;        for appliance in hon.appliances:&#xA;            print(appliance.nick_name)&#xA;&#xA;asyncio.run(devices_example())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Execute a command&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async with Hon(USER, PASSWORD) as hon:&#xA;    washing_machine = hon.appliances[0]&#xA;    pause_command = washing_machine.commands[&#34;pauseProgram&#34;]&#xA;    await pause_command.send()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Set command parameter&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async with Hon(USER, PASSWORD) as hon:&#xA;    washing_machine = hon.appliances[0]&#xA;    start_command = washing_machine.commands[&#34;startProgram&#34;]&#xA;    for name, setting in start_command.settings:&#xA;        print(&#34;Setting&#34;, name)&#xA;        print(&#34;Current value&#34;, setting.value)&#xA;        if setting.typology == &#34;enum&#34;:&#xA;            print(&#34;Available values&#34;, setting.values)&#xA;            setting.value = setting.values[0]&#xA;        elif setting.typology == &#34;range&#34;:&#xA;            print(&#34;Min value&#34;, setting.min)&#xA;            print(&#34;Max value&#34;, setting.max)&#xA;            print(&#34;Step value&#34;, setting.step)&#xA;            setting.value = setting.min + setting.step&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Translation&lt;/h2&gt; &#xA;&lt;p&gt;To get the translation of some keys like programs, you can use the translation command to see all of hOn&#39;s available translations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;$ pyhOn translate es&#xA;AC:&#xA;  APPLIANCE_RENAME:&#xA;    CONTENT_CHOOSE_NAME: Antes de continuar, debes elegir un nombre...&#xA;    DEFAULT_NAME: Aire acondicionado&#xA;    TITLE_CHOOSE_NAME: ¬°Elije un nombre para tu aire acondicionado!&#xA;    TITLE_SAVE_NAME: Para cambiar el nombre de tu aparato:&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This generates a huge output. It is recommended to pipe this into a file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;$ pyhOn translate fr &amp;gt; hon_fr.yaml&#xA;$ pyhOn translate en --json &amp;gt; hon_en.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage example&lt;/h2&gt; &#xA;&lt;p&gt;This library is used for the custom &lt;a href=&#34;https://github.com/Andre0512/hOn&#34;&gt;HomeAssistant Integration &#34;Haier hOn&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Any kind of contribution is welcome!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Please add your appliances data to our &lt;a href=&#34;https://github.com/Andre0512/hon-test-data&#34;&gt;hon-test-data collection&lt;/a&gt;. &lt;br&gt;This helps us to develop new features and not to break compatibility in newer versions.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>hustvl/Vim</title>
    <updated>2024-01-21T01:43:46Z</updated>
    <id>tag:github.com,2024-01-21:/hustvl/Vim</id>
    <link href="https://github.com/hustvl/Vim" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Vision Mamba &lt;/h1&gt; &#xA; &lt;h3&gt;Efficient Visual Representation Learning with Bidirectional State Space Model&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Unrealluver&#34;&gt;Lianghui Zhu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; *,&lt;a href=&#34;https://github.com/LegendBC&#34;&gt;Bencheng Liao&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; *,&lt;a href=&#34;https://scholar.google.com/citations?user=pCY-bikAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Qian Zhang&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.xloong.wang/&#34;&gt;Xinlong Wang&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;http://eic.hust.edu.cn/professor/liuwenyu/&#34;&gt;Wenyu Liu&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://xwcv.github.io/&#34;&gt;Xinggang Wang&lt;/a&gt;&lt;sup&gt;1 &lt;span&gt;üìß&lt;/span&gt;&lt;/sup&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Huazhong University of Science and Technology, &lt;sup&gt;2&lt;/sup&gt; Horizon Robotics, &lt;sup&gt;3&lt;/sup&gt; Beijing Academy of Artificial Intelligence&lt;/p&gt; &#xA; &lt;p&gt;(*) equal contribution, (&lt;sup&gt;&lt;span&gt;üìß&lt;/span&gt;&lt;/sup&gt;) corresponding author.&lt;/p&gt; &#xA; &lt;p&gt;ArXiv Preprint (&lt;a href=&#34;https://arxiv.org/abs/2401.09417&#34;&gt;arXiv 2401.09417&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;News&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt; Jan. 18th, 2024&lt;/code&gt;:&lt;/strong&gt; We released our paper on Arxiv. Code/Models are coming soon. Please stay tuned! ‚òïÔ∏è&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., Mamba, have shown great potential for long sequence modeling. Building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance of visual representation learning on self-attention is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation &amp;amp; memory efficiency. For example, Vim is 2.8√ó faster than DeiT and saves 86.8% GPU memory when performing batch inference on images with a resolution of 1248√ó1248. The results demonstrate that Vim is capable of overcoming the computation &amp;amp; memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to become the next-generation backbone for vision foundation models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hustvl/Vim/main/assets/vim_teaser_v1.7.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hustvl/Vim/main/assets/vim_pipeline_v1.9.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Envs. for Pretraining&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Python 3.10.13&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;conda create -n your_env_name python=3.10.13&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;torch 2.1.1 + cu118&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Requirements: vim_requirements.txt&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -r vim/vim_requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;causal_conv1d&lt;/code&gt; and &lt;code&gt;mamba&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -e causal_conv1d&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pip install -e mamba&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Train Your Vim&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;bash vim/scripts/pt-vim-t.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model Weights&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#param.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Top-1 Acc.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Top-5 Acc.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hugginface Repo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hustvl/Vim-tiny&#34;&gt;Vim-tiny&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hustvl/Vim-tiny&#34;&gt;https://huggingface.co/hustvl/Vim-tiny&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Evaluation on Provided Weights&lt;/h2&gt; &#xA;&lt;p&gt;To evaluate &lt;code&gt;Vim-Ti&lt;/code&gt; on ImageNet-1K, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py --eval --resume /path/to/ckpt --model vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_rope_also_residual_with_cls_token --data-path /path/to/imagenet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement &lt;span&gt;‚ù§Ô∏è&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on Mamba (&lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/state-spaces/mamba&#34;&gt;code&lt;/a&gt;), Causal-Conv1d (&lt;a href=&#34;https://github.com/Dao-AILab/causal-conv1d&#34;&gt;code&lt;/a&gt;), DeiT (&lt;a href=&#34;https://arxiv.org/abs/2012.12877&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/deit&#34;&gt;code&lt;/a&gt;). Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find Vim is useful in your research or applications, please consider giving us a star üåü and citing it by the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt; @article{vim,&#xA;  title={Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},&#xA;  author={Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},&#xA;  journal={arXiv preprint arXiv:2401.09417},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Codium-ai/AlphaCodium</title>
    <updated>2024-01-21T01:43:46Z</updated>
    <id>tag:github.com,2024-01-21:/Codium-ai/AlphaCodium</id>
    <link href="https://github.com/Codium-ai/AlphaCodium" rel="alternate"></link>
    <summary type="html">&lt;p&gt;code generation tool that surpasses most human competitors in CodeContests&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.08500&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/datasets/talrid/CodeContests_valid_and_test_AlphaCodium/blob/main/codecontests_valid_and_test_processed_alpha_codium.zip&#34;&gt;Dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official Implementation&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tal Ridnik, Dedy Kredo, Itamar Friedman &lt;br&gt; CodiumAI&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/#how-to-run&#34;&gt;How to run&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/#technical-qa&#34;&gt;Technical Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/#broader-applicability&#34;&gt;Broader Applicability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks.&lt;/p&gt; &#xA;&lt;p&gt;In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems.&lt;/p&gt; &#xA;&lt;p&gt;We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow.&lt;/p&gt; &#xA;&lt;p&gt;Many of the principles and best practices we acquired in this work, we believe, are broadly applicable to general code generation tasks.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/pics/proposed_flow.png&#34; align=&#34;center&#34; width=&#34;600&#34; &#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/pics/iterations.png&#34; align=&#34;center&#34; width=&#34;600&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;(1) setup a virtual environment and run: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;(2) Duplicate the file &lt;code&gt;alpha_codium/settings/.secrets_template.toml&lt;/code&gt;, rename it as &lt;code&gt;.secrets.toml&lt;/code&gt;, and fill your openai api key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[openai]&#xA;key = &#34;...&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(3) Download the processed CodeContest validation and test dataset from &lt;a href=&#34;https://huggingface.co/datasets/talrid/CodeContests_valid_and_test_AlphaCodium/blob/main/codecontests_valid_and_test_processed_alpha_codium.zip&#34;&gt;hugging face&lt;/a&gt;, extract the zip file, and placed the extracted folder in the root of the project.&lt;/p&gt; &#xA;&lt;h2&gt;How to run&lt;/h2&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;The file: &lt;code&gt;alpha_codium/settings/configuration.toml&lt;/code&gt; contains the configuration for the project. In the &lt;code&gt;config&lt;/code&gt; section you can choose the model you want to use (&#34;gpt-4&#34;, &#34;gpt-3.5-turbo-16k&#34;, or others).&lt;/p&gt; &#xA;&lt;h3&gt;Solving a specific problem&lt;/h3&gt; &#xA;&lt;p&gt;To solve a specific problem with AlphaCodium, from the root folder run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m alpha_codium.solve_problem \&#xA;--dataset_name /path/to/dataset \&#xA;--split_name test \&#xA;--problem_number 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;dataset_name&lt;/code&gt; is the path to the dataset folder you downloaded in the installation step.&lt;/li&gt; &#xA; &lt;li&gt;Note that the validation set contains 117 problems, and the test set contains 165 problems, so the &lt;code&gt;problem_number&lt;/code&gt; parameter should be accordingly (zero-based)&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;split_name&lt;/code&gt; can be either &lt;code&gt;valid&lt;/code&gt; or &lt;code&gt;test&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The followings sections in the configuration file: &lt;code&gt;solve&lt;/code&gt;, &lt;code&gt;self_reflection&lt;/code&gt;,&lt;code&gt;possible_solutions&lt;/code&gt;,&lt;code&gt;generate_ai_tests&lt;/code&gt;,&lt;code&gt;initial_code_generation&lt;/code&gt;,&lt;code&gt;public_tests&lt;/code&gt;, &lt;code&gt;ai_tests&lt;/code&gt;&lt;br&gt; enable to adjust possible configurations for the different stages of the flow.&lt;/li&gt; &#xA; &lt;li&gt;Each run logs the results to a file named &lt;code&gt;alpha_codium/example.log&lt;/code&gt;. Reviewing the log file is a good way to understand what is going on in each stage of the flow.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example problem (test set, problem number 12):&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/pics/example_problem.png&#34; align=&#34;center&#34; width=&#34;600&#34; &#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Solving the entire dataset&lt;/h3&gt; &#xA;&lt;p&gt;to solve the entire dataset with AlphaCodium, from the root folder run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m alpha_codium.solve_dataset \&#xA;--dataset_name /path/to/dataset \&#xA;--split_name test&#xA;--database_solution_path /path/to/output/dir/dataset_output.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;split_name&lt;/code&gt; can be either &lt;code&gt;valid&lt;/code&gt; or &lt;code&gt;test&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;database_solution_path&lt;/code&gt; is the path to the directory where the solutions will be saved.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;dataset&lt;/code&gt; section in the configuration file contains the configuration for the running and evaluation a dataset.&lt;/li&gt; &#xA; &lt;li&gt;Note that this is a long process, and it may take a few days to complete with large models (e.g. GPT-4) and several iterations per problem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dataset.num_iterations&lt;/code&gt; defines the number of iterations for each problem (pass@K). For large number of iterations, it is recommended to introduce some randomness and different options for each iteration to achieve top results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Running the evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Once you generate a solution for the entire dataset (valid or test), you can evaluate it by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m alpha_codium.evaluate_dataset\&#xA;--dataset_name /path/to/dataset\&#xA;--split_name test\&#xA;--database_solution_path /path/to/output/dir/dataset_output.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Technical Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;p&gt;Aggregating some technical questions we received about this project:&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: How much time did you spend on &#34;prompt engineering&#34; compared to &#34;flow engineering&#34;?&lt;/strong&gt;&lt;br&gt;&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; Structured output almost completely eliminates the need for simple prompt engineering. We estimate that ~95% of the time we did more high-level design, reasoning, injecting data at the correct places, ..., a.k.a. &#34;flow engineering&#34;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: How do you know that there wasn&#39;t a data leakage ?&lt;/strong&gt; &lt;br&gt;&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; The test set of CodeContests dataset comprises from problems published after September 2021, while the GPT-4 model variant we used (gpt-4-0613) has a data cutoff of September 2021. Hence, there is no data leakage for GPT4, on the test set. For other models like DeepSeek, we cannot be sure. However, note that our &lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/pics/comparison.png&#34;&gt;main result&lt;/a&gt; is a comparison of &#34;direct prompt&#34; vs. &#34;AlphaCodium flow&#34;. Data leakage would help both approaches, so the relative improvement of AlphaCodium flow is still valid.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Is this project relevant only to specific programming languages?&lt;/strong&gt;&lt;br&gt;&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; No. The proposed flow is language agnostic. We generated solutions in Python, but the flow can be applied to any language.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: How did you manage the context window?&lt;/strong&gt; &lt;br&gt;&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; We used models with a context window of 8192 tokens, and we did not encounter cases where it did not suffice. However, we clearly observed that as the context we used in practice grows larger (let&#39;s say, above 4000 tokens), the model starts to &#34;ignore&#34; some of the information in the context. Hence, there is a clear tradeoff:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Injecting the results of previous stages into the context, may help the model to generate better code.&lt;/li&gt; &#xA; &lt;li&gt;However, it may also cause the model to ignore specific details and nuances from the problem description.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Is this work &#34;realistic&#34; in terms of the number of LLM calls?&lt;/strong&gt; &lt;br&gt;&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; In comparison to AlphaCode, we do four orders of magnitude (!) fewer &lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/pics/computational_effort.png&#34;&gt;calls&lt;/a&gt; (per solution AlphaCodium does 15-20 calls). Yet we acknowledge that for some applications, this may still be too much, and more optimizations are needed. We however believe that many of the ideas and principles we acquired in this work are broadly applicable, even when the number of calls is further limited.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why do you iterate only on the generated code, and not on the AI-generated tests?&lt;/strong&gt; &lt;br&gt;&lt;br&gt; &lt;strong&gt;A:&lt;/strong&gt; For code problems in CodeContests, the tests are a list of input-output pairs. Hence, you don&#39;t really learn anything new when you &#34;fix&#34; a test - you just change its output to the prediction of the generated code. Instead of fixing tests, we preferred to always try and fix the code, while using &#34;test anchors&#34;. (see the &lt;a href=&#34;https://arxiv.org/abs/2401.08500&#34;&gt;paper&lt;/a&gt; for more details). However, for other code generation tasks, where the tests are more complex and actually contain runnable code, iterating on the tests, in addition to iterating on the generated code, may be beneficial.&lt;/p&gt; &#xA;&lt;h2&gt;Broader Applicability&lt;/h2&gt; &#xA;&lt;p&gt;While this work presents results on CodeContests dataset, we believe that it has a broader applicability.&lt;/p&gt; &#xA;&lt;p&gt;First and foremost, we feel that the proposed AlphaCodium &lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/pics/proposed_flow.png&#34;&gt;flow&lt;/a&gt;, with reasonable adjustments, can be used as a more general framework for other code generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;Secondly, many of the design concepts, principles, and tricks we acquired in this work are broadly applicable as-is to any general code generation tasks. For example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;YAML Structured output&lt;/strong&gt;: asking the model to generate an output in YAML format, equivalent to a given Pydantic class&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic reasoning via bullet points analysis&lt;/strong&gt;: bullet points analysis encourages an in-depth understanding of the problem, and force the model to divide the output into logical semantic sections, leading to improved results&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLMs do better when generating a modular code&lt;/strong&gt;: when clearly asking the model to: &lt;code&gt;divide the generated code into small sub-functions, with meaningful names and functionality&lt;/code&gt;, we observe a better-produced code, with fewer bugs, and higher success rates for the iterative fixing stages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Soft decisions with double validation&lt;/strong&gt;: with a double validation process, we add an extra step where, given the generated output, the model is asked to re-generate the same output, but correct it if needed&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Leave room for exploration&lt;/strong&gt;: since the model can be wrong, it‚Äôs better to avoid irreversible decisions, and leave room for exploration and code iterations with different possible solutions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The list above is partial. See the &lt;a href=&#34;https://arxiv.org/abs/2401.08500&#34;&gt;paper&lt;/a&gt; for more details. The code provided &lt;a href=&#34;https://raw.githubusercontent.com/Codium-ai/AlphaCodium/main/alpha_codium/settings&#34;&gt;in this repo&lt;/a&gt; can be used as a reference for better understanding the proposed concepts, and for applying them to other code generation tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Our process CodeContests dataset is based on the original &lt;a href=&#34;https://huggingface.co/datasets/deepmind/code_contests&#34;&gt;CodeContests&lt;/a&gt; dataset. We removed the train set (which is not relevant for our work), and did some post-processing and cleaning to the validation and test sets.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{ridnik2024code,&#xA;      title={Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering}, &#xA;      author={Tal Ridnik and Dedy Kredo and Itamar Friedman},&#xA;      year={2024},&#xA;      eprint={2401.08500},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>