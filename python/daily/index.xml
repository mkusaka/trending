<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-30T01:43:53Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Akegarasu/lora-scripts</title>
    <updated>2023-05-30T01:43:53Z</updated>
    <id>tag:github.com,2023-05-30:/Akegarasu/lora-scripts</id>
    <link href="https://github.com/Akegarasu/lora-scripts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LoRA training scripts use kohya-ss&#39;s trainer, for diffusion model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LoRA-scripts&lt;/h1&gt; &#xA;&lt;p&gt;LoRA training scripts for &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts.git&#34;&gt;kohya-ss/sd-scripts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ú®NEW: Train GUI&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/36563862/235594211-76b367d6-788f-4ef4-a258-6559759f371c.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow the installation guide below to install the GUI, then run &lt;code&gt;run_gui.ps1&lt;/code&gt;(windows) or &lt;code&gt;run_gui.sh&lt;/code&gt;(linux) to start the GUI.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Clone repo with submodules&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Required Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Python 3.10.8 and Git&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;Run &lt;code&gt;install.ps1&lt;/code&gt; will automaticilly create a venv for you and install necessary deps.&lt;/p&gt; &#xA;&lt;h4&gt;Train&lt;/h4&gt; &#xA;&lt;p&gt;Edit &lt;code&gt;train.ps1&lt;/code&gt;, and run it.&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;Run &lt;code&gt;install.bash&lt;/code&gt; will create a venv and install necessary deps.&lt;/p&gt; &#xA;&lt;h4&gt;Train&lt;/h4&gt; &#xA;&lt;p&gt;Training script &lt;code&gt;train.sh&lt;/code&gt; &lt;strong&gt;will not&lt;/strong&gt; activate venv for you. You should activate venv first.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Edit &lt;code&gt;train.sh&lt;/code&gt;, and run it.&lt;/p&gt; &#xA;&lt;h4&gt;TensorBoard&lt;/h4&gt; &#xA;&lt;p&gt;Run &lt;code&gt;tensorboard.ps1&lt;/code&gt; will start TensorBoard at &lt;a href=&#34;http://localhost:6006/&#34;&gt;http://localhost:6006/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Akegarasu/lora-scripts/main/assets/tensorboard-example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lucidrains/soundstorm-pytorch</title>
    <updated>2023-05-30T01:43:53Z</updated>
    <id>tag:github.com,2023-05-30:/lucidrains/soundstorm-pytorch</id>
    <link href="https://github.com/lucidrains/soundstorm-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of SoundStorm, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/soundstorm-pytorch/main/soundstorm.png&#34; width=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Soundstorm - Pytorch (wip)&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://arxiv.org/abs/2305.09636&#34;&gt;SoundStorm&lt;/a&gt;, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch.&lt;/p&gt; &#xA;&lt;p&gt;They basically applied &lt;a href=&#34;https://arxiv.org/abs/2202.04200&#34;&gt;MaskGiT&lt;/a&gt; to the residual vector quantized codes from &lt;a href=&#34;https://github.com/lucidrains/audiolm-pytorch#soundstream--encodec&#34;&gt;Soundstream&lt;/a&gt;. The transformer architecture they chose to use is one that fits well with the audio domain, named &lt;a href=&#34;https://arxiv.org/abs/2005.08100&#34;&gt;Conformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://google-research.github.io/seanet/soundstorm/examples/&#34;&gt;Project Page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://stability.ai/&#34;&gt;Stability&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/&#34;&gt;ü§ó Huggingface&lt;/a&gt; for their generous sponsorships to work on and open source cutting edge artificial intelligence research&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/docs/accelerate/index&#34;&gt;ü§ó Accelerate&lt;/a&gt; for providing a simple and powerful solution for training&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://einops.rocks/&#34;&gt;Einops&lt;/a&gt; for the indispensable abstraction that makes building neural networks fun, easy, and uplifting&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/stevenhillis&#34;&gt;Steven Hillis&lt;/a&gt; for submitting the correct masking strategy and for verifying that the repository works! üôè&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install soundstorm-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from soundstorm_pytorch import SoundStorm, ConformerWrapper&#xA;&#xA;conformer = ConformerWrapper(&#xA;    codebook_size = 1024,&#xA;    num_quantizers = 4,&#xA;    conformer = dict(&#xA;        dim = 512,&#xA;        depth = 2&#xA;    ),&#xA;)&#xA;&#xA;model = SoundStorm(&#xA;    conformer,&#xA;    steps = 18,          # 18 steps, as in original maskgit paper&#xA;    schedule = &#39;cosine&#39;  # currently the best schedule is cosine&#xA;)&#xA;&#xA;# get your pre-encoded codebook ids from the soundstream from a lot of raw audio&#xA;&#xA;codes = torch.randint(0, 1024, (2, 1024))&#xA;&#xA;# do the below in a loop for a ton of data&#xA;&#xA;loss, _ = model(codes)&#xA;loss.backward()&#xA;&#xA;# model can now generate in 18 steps. ~2 seconds sounds reasonable&#xA;&#xA;generated = model.generate(1024, batch_size = 2) # (2, 1024)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To directly train on raw audio, you need to pass in your pretrained &lt;code&gt;SoundStream&lt;/code&gt; into &lt;code&gt;SoundStorm&lt;/code&gt;. You can train your own &lt;code&gt;SoundStream&lt;/code&gt; at &lt;a href=&#34;https://github.com/lucidrains/audiolm-pytorch#soundstream--encodec&#34;&gt;audiolm-pytorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from soundstorm_pytorch import SoundStorm, ConformerWrapper, Conformer, SoundStream&#xA;&#xA;conformer = ConformerWrapper(&#xA;    codebook_size = 1024,&#xA;    num_quantizers = 4,&#xA;    conformer = dict(&#xA;        dim = 512,&#xA;        depth = 2&#xA;    ),&#xA;)&#xA;&#xA;soundstream = SoundStream(&#xA;    codebook_size = 1024,&#xA;    rq_num_quantizers = 4,&#xA;    attn_window_size = 128,&#xA;    attn_depth = 2&#xA;)&#xA;&#xA;model = SoundStorm(&#xA;    conformer,&#xA;    soundstream = soundstream   # pass in the soundstream&#xA;)&#xA;&#xA;# find as much audio you&#39;d like the model to learn&#xA;&#xA;audio = torch.randn(2, 10080)&#xA;&#xA;# course it through the model and take a gazillion tiny steps&#xA;&#xA;loss, _ = model(audio)&#xA;loss.backward()&#xA;&#xA;# and now you can generate state-of-the-art speech&#xA;&#xA;generated_audio = model.generate(seconds = 30, batch_size = 2)  # generate 30 seconds of audio (it will calculate the length in seconds based off the sampling frequency and cumulative downsamples in the soundstream passed in above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;integrate soundstream&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;when generating, and length can be defined in seconds (takes into sampling freq etc)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;make sure grouped rvq is supported. concat embeddings rather than sum across group dimension&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;just copy conformer over and redo shaw&#39;s relative positional embedding with rotary embedding. nobody uses shaw anymore.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;default flash attention to true&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;remove batchnorm, and just use layernorm, but after the swish (as in normformer paper)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;option to return list of audio files when generating&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;turn it into a command line tool&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;add cross attention and adaptive layernorm conditioning&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;trainer with accelerate&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{borsos2023soundstorm,&#xA;    title   = {SoundStorm: Efficient Parallel Audio Generation}, &#xA;    author  = {Zal√°n Borsos and Matt Sharifi and Damien Vincent and Eugene Kharitonov and Neil Zeghidour and Marco Tagliasacchi},&#xA;    year    = {2023},&#xA;    eprint  = {2305.09636},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.SD}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{dao2022flashattention,&#xA;    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},&#xA;    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\&#39;e}, Christopher},&#xA;    booktitle = {Advances in Neural Information Processing Systems},&#xA;    year    = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Chang2022MaskGITMG,&#xA;    title   = {MaskGIT: Masked Generative Image Transformer},&#xA;    author  = {Huiwen Chang and Han Zhang and Lu Jiang and Ce Liu and William T. Freeman},&#xA;    journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;    year    = {2022},&#xA;    pages   = {11305-11315}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Lezama2022ImprovedMI,&#xA;    title   = {Improved Masked Image Generation with Token-Critic},&#xA;    author  = {Jos{\&#39;e} Lezama and Huiwen Chang and Lu Jiang and Irfan Essa},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2209.04439}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Nijkamp2021SCRIPTSP,&#xA;    title   = {SCRIPT: Self-Critic PreTraining of Transformers},&#xA;    author  = {Erik Nijkamp and Bo Pang and Ying Nian Wu and Caiming Xiong},&#xA;    booktitle = {North American Chapter of the Association for Computational Linguistics},&#xA;    year    = {2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{rogozhnikov2022einops,&#xA;    title   = {Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},&#xA;    author  = {Alex Rogozhnikov},&#xA;    booktitle = {International Conference on Learning Representations},&#xA;    year    = {2022},&#xA;    url     = {https://openreview.net/forum?id=oapKSVM2bcj}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{su2021roformer,&#xA;    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},&#xA;    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},&#xA;    year    = {2021},&#xA;    eprint  = {2104.09864},&#xA;    archivePrefix = {arXiv},&#xA;    primaryClass = {cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>sjvasquez/handwriting-synthesis</title>
    <updated>2023-05-30T01:43:53Z</updated>
    <id>tag:github.com,2023-05-30:/sjvasquez/handwriting-synthesis</id>
    <link href="https://github.com/sjvasquez/handwriting-synthesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Handwriting Synthesis with RNNs ‚úèÔ∏è&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/banner.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Handwriting Synthesis&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of the handwriting synthesis experiments in the paper &lt;a href=&#34;https://arxiv.org/abs/1308.0850&#34;&gt;Generating Sequences with Recurrent Neural Networks&lt;/a&gt; by Alex Graves. The implementation closely follows the original paper, with a few slight deviations, and the generated samples are of similar quality to those presented in the paper.&lt;/p&gt; &#xA;&lt;p&gt;Web demo is available &lt;a href=&#34;https://seanvasquez.com/handwriting-generation/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lines = [&#xA;    &#34;Now this is a story all about how&#34;,&#xA;    &#34;My life got flipped turned upside down&#34;,&#xA;    &#34;And I&#39;d like to take a minute, just sit right there&#34;,&#xA;    &#34;I&#39;ll tell you how I became the prince of a town called Bel-Air&#34;,&#xA;]&#xA;biases = [.75 for i in lines]&#xA;styles = [9 for i in lines]&#xA;stroke_colors = [&#39;red&#39;, &#39;green&#39;, &#39;black&#39;, &#39;blue&#39;]&#xA;stroke_widths = [1, 2, 1, 2]&#xA;&#xA;hand = Hand()&#xA;hand.write(&#xA;    filename=&#39;img/usage_demo.svg&#39;,&#xA;    lines=lines,&#xA;    biases=biases,&#xA;    styles=styles,&#xA;    stroke_colors=stroke_colors,&#xA;    stroke_widths=stroke_widths&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/usage_demo.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Currently, the &lt;code&gt;Hand&lt;/code&gt; class must be imported from &lt;code&gt;demo.py&lt;/code&gt;. If someone would like to package this project to make it more usable, please &lt;a href=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/#contribute&#34;&gt;contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A pretrained model is included, but if you&#39;d like to train your own, read &lt;a href=&#34;https://github.com/sjvasquez/handwriting-synthesis/tree/master/data/raw&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Demonstrations&lt;/h2&gt; &#xA;&lt;p&gt;Below are a few hundred samples from the model, including some samples demonstrating the effect of priming and biasing the model. Loosely speaking, biasing controls the neatness of the samples and priming controls the style of the samples. The code for these demonstrations can be found in &lt;code&gt;demo.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Demo #1:&lt;/h3&gt; &#xA;&lt;p&gt;The following samples were generated with a fixed style and fixed bias.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Smash Mouth ‚Äì All Star (&lt;a href=&#34;https://www.azlyrics.com/lyrics/smashmouth/allstar.html&#34;&gt;lyrics&lt;/a&gt;)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/all_star.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo #2&lt;/h3&gt; &#xA;&lt;p&gt;The following samples were generated with varying style and fixed bias. Each verse is generated in a different style.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vanessa Carlton ‚Äì A Thousand Miles (&lt;a href=&#34;https://www.azlyrics.com/lyrics/vanessacarlton/athousandmiles.html&#34;&gt;lyrics&lt;/a&gt;)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/downtown.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo #3&lt;/h3&gt; &#xA;&lt;p&gt;The following samples were generated with a fixed style and varying bias. Each verse has a lower bias than the previous, with the last verse being unbiased.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leonard Cohen ‚Äì Hallelujah (&lt;a href=&#34;https://www.youtube.com/watch?v=dQw4w9WgXcQ&#34;&gt;lyrics&lt;/a&gt;)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/give_up.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;This project was intended to serve as a reference implementation for a research paper, but since the results are of decent quality, it may be worthwile to make the project more broadly usable. I plan to continue focusing on the machine learning side of things. That said, I&#39;d welcome contributors who can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Package this, and otherwise make it look more like a usable software project and less like research code.&lt;/li&gt; &#xA; &lt;li&gt;Add support for more sophisticated drawing, animations, or anything else in this direction. Currently, the project only creates some simple svg files.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>