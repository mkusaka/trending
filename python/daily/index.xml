<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-23T01:34:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>encode/starlette</title>
    <updated>2024-11-23T01:34:07Z</updated>
    <id>tag:github.com,2024-11-23:/encode/starlette</id>
    <link href="https://github.com/encode/starlette" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The little ASGI framework that shines. üåü&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.starlette.io/&#34;&gt;&lt;img width=&#34;420px&#34; src=&#34;https://raw.githubusercontent.com/encode/starlette/master/docs/img/starlette.svg?sanitize=true&#34; alt=&#34;starlette&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;em&gt;‚ú® The little ASGI framework that shines. ‚ú®&lt;/em&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/encode/starlette/actions&#34;&gt;&lt;img src=&#34;https://github.com/encode/starlette/workflows/Test%20Suite/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/starlette&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/starlette.svg?sanitize=true&#34; alt=&#34;Package version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/starlette&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/starlette.svg?color=%2334D058&#34; alt=&#34;Supported Python Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&#34;https://www.starlette.io/&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://www.starlette.io&#34;&gt;https://www.starlette.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/encode/starlette&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/encode/starlette&#34;&gt;https://github.com/encode/starlette&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Starlette&lt;/h1&gt; &#xA;&lt;p&gt;Starlette is a lightweight &lt;a href=&#34;https://asgi.readthedocs.io/en/latest/&#34;&gt;ASGI&lt;/a&gt; framework/toolkit, which is ideal for building async web services in Python.&lt;/p&gt; &#xA;&lt;p&gt;It is production-ready, and gives you the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A lightweight, low-complexity HTTP web framework.&lt;/li&gt; &#xA; &lt;li&gt;WebSocket support.&lt;/li&gt; &#xA; &lt;li&gt;In-process background tasks.&lt;/li&gt; &#xA; &lt;li&gt;Startup and shutdown events.&lt;/li&gt; &#xA; &lt;li&gt;Test client built on &lt;code&gt;httpx&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;CORS, GZip, Static Files, Streaming responses.&lt;/li&gt; &#xA; &lt;li&gt;Session and Cookie support.&lt;/li&gt; &#xA; &lt;li&gt;100% test coverage.&lt;/li&gt; &#xA; &lt;li&gt;100% type annotated codebase.&lt;/li&gt; &#xA; &lt;li&gt;Few hard dependencies.&lt;/li&gt; &#xA; &lt;li&gt;Compatible with &lt;code&gt;asyncio&lt;/code&gt; and &lt;code&gt;trio&lt;/code&gt; backends.&lt;/li&gt; &#xA; &lt;li&gt;Great overall performance &lt;a href=&#34;https://www.techempower.com/benchmarks/#hw=ph&amp;amp;test=fortune&amp;amp;l=zijzen-sf&#34;&gt;against independent benchmarks&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install starlette&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll also want to install an ASGI server, such as &lt;a href=&#34;https://www.uvicorn.org/&#34;&gt;uvicorn&lt;/a&gt;, &lt;a href=&#34;https://github.com/django/daphne/&#34;&gt;daphne&lt;/a&gt;, or &lt;a href=&#34;https://hypercorn.readthedocs.io/en/latest/&#34;&gt;hypercorn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install uvicorn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from starlette.applications import Starlette&#xA;from starlette.responses import JSONResponse&#xA;from starlette.routing import Route&#xA;&#xA;&#xA;async def homepage(request):&#xA;    return JSONResponse({&#39;hello&#39;: &#39;world&#39;})&#xA;&#xA;routes = [&#xA;    Route(&#34;/&#34;, endpoint=homepage)&#xA;]&#xA;&#xA;app = Starlette(debug=True, routes=routes)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the application using Uvicorn:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ uvicorn example:app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a more complete example, see &lt;a href=&#34;https://github.com/encode/starlette-example&#34;&gt;encode/starlette-example&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Starlette only requires &lt;code&gt;anyio&lt;/code&gt;, and the following are optional:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python-httpx.org/&#34;&gt;&lt;code&gt;httpx&lt;/code&gt;&lt;/a&gt; - Required if you want to use the &lt;code&gt;TestClient&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jinja.palletsprojects.com/&#34;&gt;&lt;code&gt;jinja2&lt;/code&gt;&lt;/a&gt; - Required if you want to use &lt;code&gt;Jinja2Templates&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://andrew-d.github.io/python-multipart/&#34;&gt;&lt;code&gt;python-multipart&lt;/code&gt;&lt;/a&gt; - Required if you want to support form parsing, with &lt;code&gt;request.form()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://itsdangerous.palletsprojects.com/&#34;&gt;&lt;code&gt;itsdangerous&lt;/code&gt;&lt;/a&gt; - Required for &lt;code&gt;SessionMiddleware&lt;/code&gt; support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pyyaml.org/wiki/PyYAMLDocumentation&#34;&gt;&lt;code&gt;pyyaml&lt;/code&gt;&lt;/a&gt; - Required for &lt;code&gt;SchemaGenerator&lt;/code&gt; support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can install all of these with &lt;code&gt;pip install starlette[full]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Framework or Toolkit&lt;/h2&gt; &#xA;&lt;p&gt;Starlette is designed to be used either as a complete framework, or as an ASGI toolkit. You can use any of its components independently.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from starlette.responses import PlainTextResponse&#xA;&#xA;&#xA;async def app(scope, receive, send):&#xA;    assert scope[&#39;type&#39;] == &#39;http&#39;&#xA;    response = PlainTextResponse(&#39;Hello, world!&#39;)&#xA;    await response(scope, receive, send)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the &lt;code&gt;app&lt;/code&gt; application in &lt;code&gt;example.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ uvicorn example:app&#xA;INFO: Started server process [11509]&#xA;INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run uvicorn with &lt;code&gt;--reload&lt;/code&gt; to enable auto-reloading on code changes.&lt;/p&gt; &#xA;&lt;h2&gt;Modularity&lt;/h2&gt; &#xA;&lt;p&gt;The modularity that Starlette is designed on promotes building re-usable components that can be shared between any ASGI framework. This should enable an ecosystem of shared middleware and mountable applications.&lt;/p&gt; &#xA;&lt;p&gt;The clean API separation also means it&#39;s easier to understand each component in isolation.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;Starlette is &lt;a href=&#34;https://github.com/encode/starlette/raw/master/LICENSE.md&#34;&gt;BSD licensed&lt;/a&gt; code.&lt;br&gt;Designed &amp;amp; crafted with care.&lt;/i&gt;&lt;br&gt;‚Äî ‚≠êÔ∏è ‚Äî&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>junyanz/pytorch-CycleGAN-and-pix2pix</title>
    <updated>2024-11-23T01:34:07Z</updated>
    <id>tag:github.com,2024-11-23:/junyanz/pytorch-CycleGAN-and-pix2pix</id>
    <link href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Image-to-Image Translation in PyTorch&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/imgs/horse2zebra.gif&#34; align=&#34;right&#34; width=&#34;384&#34;&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;CycleGAN and pix2pix in PyTorch&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;New&lt;/strong&gt;: Please check out &lt;a href=&#34;https://github.com/GaParmar/img2img-turbo&#34;&gt;img2img-turbo&lt;/a&gt; repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDiffusion-Turbo model. The inference time for 512x512 image is 0.29 sec on A6000 and 0.11 sec on A100.&lt;/p&gt; &#xA;&lt;p&gt;Please check out &lt;a href=&#34;https://github.com/taesungp/contrastive-unpaired-translation&#34;&gt;contrastive-unpaired-translation&lt;/a&gt; (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.&lt;/p&gt; &#xA;&lt;p&gt;We provide PyTorch implementations for both unpaired and paired image-to-image translation.&lt;/p&gt; &#xA;&lt;p&gt;The code was written by &lt;a href=&#34;https://github.com/junyanz&#34;&gt;Jun-Yan Zhu&lt;/a&gt; and &lt;a href=&#34;https://github.com/taesungp&#34;&gt;Taesung Park&lt;/a&gt;, and supported by &lt;a href=&#34;https://github.com/SsnL&#34;&gt;Tongzhou Wang&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original &lt;a href=&#34;https://github.com/junyanz/CycleGAN&#34;&gt;CycleGAN Torch&lt;/a&gt; and &lt;a href=&#34;https://github.com/phillipi/pix2pix&#34;&gt;pix2pix Torch&lt;/a&gt; code in Lua/Torch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The current software works well with PyTorch 1.4. Check out the older &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/pytorch0.3.1&#34;&gt;branch&lt;/a&gt; that supports PyTorch 0.1-0.3.&lt;/p&gt; &#xA;&lt;p&gt;You may find useful information in &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/tips.md&#34;&gt;training/test tips&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/qa.md&#34;&gt;frequently asked questions&lt;/a&gt;. To implement custom models and datasets, check out our &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/#custom-model-and-dataset&#34;&gt;templates&lt;/a&gt;. To help users better understand and adapt our codebase, we provide an &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/overview.md&#34;&gt;overview&lt;/a&gt; of the code structure of this repository.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CycleGAN: &lt;a href=&#34;https://junyanz.github.io/CycleGAN/&#34;&gt;Project&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/1703.10593.pdf&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/junyanz/CycleGAN&#34;&gt;Torch&lt;/a&gt; | &lt;a href=&#34;https://www.tensorflow.org/tutorials/generative/cyclegan&#34;&gt;Tensorflow Core Tutorial&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb&#34;&gt;PyTorch Colab&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2pix: &lt;a href=&#34;https://phillipi.github.io/pix2pix/&#34;&gt;Project&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/1611.07004.pdf&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/phillipi/pix2pix&#34;&gt;Torch&lt;/a&gt; | &lt;a href=&#34;https://www.tensorflow.org/tutorials/generative/pix2pix&#34;&gt;Tensorflow Core Tutorial&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb&#34;&gt;PyTorch Colab&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://phillipi.github.io/pix2pix/images/teaser_v3.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://affinelayer.com/pixsrv/&#34;&gt;EdgesCats Demo&lt;/a&gt; | &lt;a href=&#34;https://github.com/affinelayer/pix2pix-tensorflow&#34;&gt;pix2pix-tensorflow&lt;/a&gt; | by &lt;a href=&#34;https://twitter.com/christophrhesse&#34;&gt;Christopher Hesse&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/imgs/edges2cats.jpg&#34; width=&#34;400px&#34;&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite:&lt;/p&gt; &#xA;&lt;p&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.&lt;br&gt; &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;*, &lt;a href=&#34;https://taesung.me/&#34;&gt;Taesung Park&lt;/a&gt;*, &lt;a href=&#34;https://people.eecs.berkeley.edu/~isola/&#34;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~efros&#34;&gt;Alexei A. Efros&lt;/a&gt;. In ICCV 2017. (* equal contributions) &lt;a href=&#34;https://junyanz.github.io/CycleGAN/CycleGAN.txt&#34;&gt;[Bibtex]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Image-to-Image Translation with Conditional Adversarial Networks.&lt;br&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~isola&#34;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~tinghuiz&#34;&gt;Tinghui Zhou&lt;/a&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~efros&#34;&gt;Alexei A. Efros&lt;/a&gt;. In CVPR 2017. &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/projects/pix2pix/pix2pix.bib&#34;&gt;[Bibtex]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Talks and Course&lt;/h2&gt; &#xA;&lt;p&gt;pix2pix slides: &lt;a href=&#34;http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.key&#34;&gt;keynote&lt;/a&gt; | &lt;a href=&#34;http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.pdf&#34;&gt;pdf&lt;/a&gt;, CycleGAN slides: &lt;a href=&#34;http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx&#34;&gt;pptx&lt;/a&gt; | &lt;a href=&#34;http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;CycleGAN course assignment &lt;a href=&#34;http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip&#34;&gt;code&lt;/a&gt; and &lt;a href=&#34;http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-handout.pdf&#34;&gt;handout&lt;/a&gt; designed by Prof. &lt;a href=&#34;http://www.cs.toronto.edu/~rgrosse/&#34;&gt;Roger Grosse&lt;/a&gt; for &lt;a href=&#34;http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/&#34;&gt;CSC321&lt;/a&gt; &#34;Intro to Neural Networks and Machine Learning&#34; at University of Toronto. Please contact the instructor if you would like to adopt it in your course.&lt;/p&gt; &#xA;&lt;h2&gt;Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;TensorFlow Core CycleGAN Tutorial: &lt;a href=&#34;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb&#34;&gt;Google Colab&lt;/a&gt; | &lt;a href=&#34;https://github.com/tensorflow/docs/raw/master/site/en/tutorials/generative/cyclegan.ipynb&#34;&gt;Code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;TensorFlow Core pix2pix Tutorial: &lt;a href=&#34;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb&#34;&gt;Google Colab&lt;/a&gt; | &lt;a href=&#34;https://github.com/tensorflow/docs/raw/master/site/en/tutorials/generative/pix2pix.ipynb&#34;&gt;Code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PyTorch Colab notebook: &lt;a href=&#34;https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb&#34;&gt;CycleGAN&lt;/a&gt; and &lt;a href=&#34;https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb&#34;&gt;pix2pix&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ZeroCostDL4Mic Colab notebook: &lt;a href=&#34;https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/CycleGAN_ZeroCostDL4Mic.ipynb&#34;&gt;CycleGAN&lt;/a&gt; and &lt;a href=&#34;https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/pix2pix_ZeroCostDL4Mic.ipynb&#34;&gt;pix2pix&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Other implementations&lt;/h2&gt; &#xA;&lt;h3&gt;CycleGAN&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/leehomyc/cyclegan-1&#34;&gt; [Tensorflow]&lt;/a&gt; (by Harry Yang), &lt;a href=&#34;https://github.com/architrathore/CycleGAN/&#34;&gt;[Tensorflow]&lt;/a&gt; (by Archit Rathore), &lt;a href=&#34;https://github.com/vanhuyz/CycleGAN-TensorFlow&#34;&gt;[Tensorflow]&lt;/a&gt; (by Van Huy), &lt;a href=&#34;https://github.com/XHUJOY/CycleGAN-tensorflow&#34;&gt;[Tensorflow]&lt;/a&gt; (by Xiaowei Hu), &lt;a href=&#34;https://github.com/LynnHo/CycleGAN-Tensorflow-2&#34;&gt; [Tensorflow2]&lt;/a&gt; (by Zhenliang He), &lt;a href=&#34;https://github.com/luoxier/CycleGAN_Tensorlayer&#34;&gt; [TensorLayer1.0]&lt;/a&gt; (by luoxier), &lt;a href=&#34;https://github.com/tensorlayer/cyclegan&#34;&gt; [TensorLayer2.0]&lt;/a&gt; (by zsdonghao), &lt;a href=&#34;https://github.com/Aixile/chainer-cyclegan&#34;&gt;[Chainer]&lt;/a&gt; (by Yanghua Jin), &lt;a href=&#34;https://github.com/yunjey/mnist-svhn-transfer&#34;&gt;[Minimal PyTorch]&lt;/a&gt; (by yunjey), &lt;a href=&#34;https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN&#34;&gt;[Mxnet]&lt;/a&gt; (by Ldpe2G), &lt;a href=&#34;https://github.com/tjwei/GANotebooks&#34;&gt;[lasagne/Keras]&lt;/a&gt; (by tjwei), &lt;a href=&#34;https://github.com/simontomaskarlsson/CycleGAN-Keras&#34;&gt;[Keras]&lt;/a&gt; (by Simon Karlsson), &lt;a href=&#34;https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN&#34;&gt;[OneFlow]&lt;/a&gt; (by Ldpe2G) &lt;/p&gt;  &#xA;&lt;h3&gt;pix2pix&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/affinelayer/pix2pix-tensorflow&#34;&gt; [Tensorflow]&lt;/a&gt; (by Christopher Hesse), &lt;a href=&#34;https://github.com/Eyyub/tensorflow-pix2pix&#34;&gt;[Tensorflow]&lt;/a&gt; (by Eyy√ºb Sariu), &lt;a href=&#34;https://github.com/datitran/face2face-demo&#34;&gt; [Tensorflow (face2face)]&lt;/a&gt; (by Dat Tran), &lt;a href=&#34;https://github.com/awjuliani/Pix2Pix-Film&#34;&gt; [Tensorflow (film)]&lt;/a&gt; (by Arthur Juliani), &lt;a href=&#34;https://github.com/kaonashi-tyc/zi2zi&#34;&gt;[Tensorflow (zi2zi)]&lt;/a&gt; (by Yuchen Tian), &lt;a href=&#34;https://github.com/pfnet-research/chainer-pix2pix&#34;&gt;[Chainer]&lt;/a&gt; (by mattya), &lt;a href=&#34;https://github.com/tjwei/GANotebooks&#34;&gt;[tf/torch/keras/lasagne]&lt;/a&gt; (by tjwei), &lt;a href=&#34;https://github.com/taey16/pix2pixBEGAN.pytorch&#34;&gt;[Pytorch]&lt;/a&gt; (by taey16) &lt;/p&gt;  &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux or macOS&lt;/li&gt; &#xA; &lt;li&gt;Python 3&lt;/li&gt; &#xA; &lt;li&gt;CPU or NVIDIA GPU + CUDA CuDNN&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#xA;cd pytorch-CycleGAN-and-pix2pix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;http://pytorch.org&#34;&gt;PyTorch&lt;/a&gt; and 0.4+ and other dependencies (e.g., torchvision, &lt;a href=&#34;https://github.com/facebookresearch/visdom&#34;&gt;visdom&lt;/a&gt; and &lt;a href=&#34;https://github.com/Knio/dominate&#34;&gt;dominate&lt;/a&gt;). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For pip users, please type the command &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For Conda users, you can create a new Conda environment using &lt;code&gt;conda env create -f environment.yml&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/docker.md&#34;&gt;Docker&lt;/a&gt; page.&lt;/li&gt; &#xA;   &lt;li&gt;For Repl users, please click &lt;a href=&#34;https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix&#34;&gt;&lt;img src=&#34;https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix&#34; alt=&#34;Run on Repl.it&#34;&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;CycleGAN train/test&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download a CycleGAN dataset (e.g. maps):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_cyclegan_dataset.sh maps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To view training results and loss plots, run &lt;code&gt;python -m visdom.server&lt;/code&gt; and click the URL &lt;a href=&#34;http://localhost:8097&#34;&gt;http://localhost:8097&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To log training progress and test images to W&amp;amp;B dashboard, set the &lt;code&gt;--use_wandb&lt;/code&gt; flag with train and test script&lt;/li&gt; &#xA; &lt;li&gt;Train a model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!./scripts/train_cyclegan.sh&#xA;python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see more intermediate results, check out &lt;code&gt;./checkpoints/maps_cyclegan/web/index.html&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Test the model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!./scripts/test_cyclegan.sh&#xA;python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The test results will be saved to a html file here: &lt;code&gt;./results/maps_cyclegan/latest_test/index.html&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;pix2pix train/test&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download a pix2pix dataset (e.g.&lt;a href=&#34;http://cmp.felk.cvut.cz/~tylecr1/facade/&#34;&gt;facades&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_pix2pix_dataset.sh facades&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To view training results and loss plots, run &lt;code&gt;python -m visdom.server&lt;/code&gt; and click the URL &lt;a href=&#34;http://localhost:8097&#34;&gt;http://localhost:8097&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To log training progress and test images to W&amp;amp;B dashboard, set the &lt;code&gt;--use_wandb&lt;/code&gt; flag with train and test script&lt;/li&gt; &#xA; &lt;li&gt;Train a model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!./scripts/train_pix2pix.sh&#xA;python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see more intermediate results, check out &lt;code&gt;./checkpoints/facades_pix2pix/web/index.html&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Test the model (&lt;code&gt;bash ./scripts/test_pix2pix.sh&lt;/code&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!./scripts/test_pix2pix.sh&#xA;python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The test results will be saved to a html file here: &lt;code&gt;./results/facades_pix2pix/test_latest/index.html&lt;/code&gt;. You can find more scripts at &lt;code&gt;scripts&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;To train and test pix2pix-based colorization models, please add &lt;code&gt;--model colorization&lt;/code&gt; and &lt;code&gt;--dataset_mode colorization&lt;/code&gt;. See our training &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/raw/master/docs/tips.md#notes-on-colorization&#34;&gt;tips&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Apply a pre-trained model (CycleGAN)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download a pretrained model (e.g. horse2zebra) with the following script:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/download_cyclegan_model.sh horse2zebra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The pretrained model is saved at &lt;code&gt;./checkpoints/{name}_pretrained/latest_net_G.pth&lt;/code&gt;. Check &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/raw/master/scripts/download_cyclegan_model.sh#L3&#34;&gt;here&lt;/a&gt; for all the available CycleGAN models.&lt;/li&gt; &#xA; &lt;li&gt;To test the model, you also need to download the horse2zebra dataset:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_cyclegan_dataset.sh horse2zebra&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then generate the results using&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The option &lt;code&gt;--model test&lt;/code&gt; is used for generating results of CycleGAN only for one side. This option will automatically set &lt;code&gt;--dataset_mode single&lt;/code&gt;, which only loads the images from one set. On the contrary, using &lt;code&gt;--model cycle_gan&lt;/code&gt; requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at &lt;code&gt;./results/&lt;/code&gt;. Use &lt;code&gt;--results_dir {directory_path_to_save_result}&lt;/code&gt; to specify the results directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For pix2pix and your own models, you need to explicitly specify &lt;code&gt;--netG&lt;/code&gt;, &lt;code&gt;--norm&lt;/code&gt;, &lt;code&gt;--no_dropout&lt;/code&gt; to match the generator architecture of the trained model. See this &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/raw/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296&#34;&gt;FAQ&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Apply a pre-trained model (pix2pix)&lt;/h3&gt; &#xA;&lt;p&gt;Download a pre-trained model with &lt;code&gt;./scripts/download_pix2pix_model.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/raw/master/scripts/download_pix2pix_model.sh#L3&#34;&gt;here&lt;/a&gt; for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset,&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./scripts/download_pix2pix_model.sh facades_label2photo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the pix2pix facades datasets:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./datasets/download_pix2pix_dataset.sh facades&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then generate the results using&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Note that we specified &lt;code&gt;--direction BtoA&lt;/code&gt; as Facades dataset&#39;s A to B direction is photos to labels.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use &lt;code&gt;--model test&lt;/code&gt; option. See &lt;code&gt;./scripts/test_single.sh&lt;/code&gt; for how to apply a model to Facade label maps (stored in the directory &lt;code&gt;facades/testB&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;See a list of currently available models at &lt;code&gt;./scripts/download_pix2pix_model.sh&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/docker.md&#34;&gt;Docker&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We provide the pre-built Docker image and Dockerfile that can run this code repo. See &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/docker.md&#34;&gt;docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/datasets.md&#34;&gt;Datasets&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Download pix2pix/CycleGAN datasets and create your own datasets.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/tips.md&#34;&gt;Training/Test Tips&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Best practice for training and testing your models.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/qa.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Before you post a new question, please first look at the above Q &amp;amp; A and existing GitHub issues.&lt;/p&gt; &#xA;&lt;h2&gt;Custom Model and Dataset&lt;/h2&gt; &#xA;&lt;p&gt;If you plan to implement custom models and dataset for your new applications, we provide a dataset &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/data/template_dataset.py&#34;&gt;template&lt;/a&gt; and a model &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/models/template_model.py&#34;&gt;template&lt;/a&gt; as a starting point.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/overview.md&#34;&gt;Code structure&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module.&lt;/p&gt; &#xA;&lt;h2&gt;Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;You are always welcome to contribute to this repository by sending a &lt;a href=&#34;https://help.github.com/articles/about-pull-requests/&#34;&gt;pull request&lt;/a&gt;. Please run &lt;code&gt;flake8 --ignore E501 .&lt;/code&gt; and &lt;code&gt;python ./scripts/test_before_push.py&lt;/code&gt; before you commit the code. Please also update the code structure &lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/overview.md&#34;&gt;overview&lt;/a&gt; accordingly if you add or remove files.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our papers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{CycleGAN2017,&#xA;  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},&#xA;  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},&#xA;  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},&#xA;  year={2017}&#xA;}&#xA;&#xA;&#xA;@inproceedings{isola2017image,&#xA;  title={Image-to-Image Translation with Conditional Adversarial Networks},&#xA;  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},&#xA;  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},&#xA;  year={2017}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other Languages&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/README_es.md&#34;&gt;Spanish&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/taesungp/contrastive-unpaired-translation&#34;&gt;contrastive-unpaired-translation&lt;/a&gt; (CUT)&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/junyanz/CycleGAN&#34;&gt;CycleGAN-Torch&lt;/a&gt; | &lt;a href=&#34;https://github.com/phillipi/pix2pix&#34;&gt;pix2pix-Torch&lt;/a&gt; | &lt;a href=&#34;https://github.com/NVIDIA/pix2pixHD&#34;&gt;pix2pixHD&lt;/a&gt;| &lt;a href=&#34;https://github.com/junyanz/BicycleGAN&#34;&gt;BicycleGAN&lt;/a&gt; | &lt;a href=&#34;https://tcwang0509.github.io/vid2vid/&#34;&gt;vid2vid&lt;/a&gt; | &lt;a href=&#34;https://github.com/NVlabs/SPADE&#34;&gt;SPADE/GauGAN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://github.com/junyanz/iGAN&#34;&gt;iGAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/CSAILVision/GANDissect&#34;&gt;GAN Dissection&lt;/a&gt; | &lt;a href=&#34;http://ganpaint.io/&#34;&gt;GAN Paint&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cat Paper Collection&lt;/h2&gt; &#xA;&lt;p&gt;If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper &lt;a href=&#34;https://github.com/junyanz/CatPapers&#34;&gt;Collection&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Our code is inspired by &lt;a href=&#34;https://github.com/pytorch/examples/tree/master/dcgan&#34;&gt;pytorch-DCGAN&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>