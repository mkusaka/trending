<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-28T01:33:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NUS-HPC-AI-Lab/VideoSys</title>
    <updated>2024-08-28T01:33:28Z</updated>
    <id>tag:github.com,2024-08-28:/NUS-HPC-AI-Lab/VideoSys</id>
    <link href="https://github.com/NUS-HPC-AI-Lab/VideoSys" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VideoSys: An easy and efficient system for video generation&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;55%&#34; alt=&#34;VideoSys&#34; src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/assets/figures/logo.png?raw=true&#34;&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; An easy and efficient system for video generation &lt;/h3&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Latest News üî•&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/08] üî• Evole from &lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/VideoSys/tree/v1.0.0&#34;&gt;OpenDiT&lt;/a&gt; to &lt;b&gt;VideoSys: An easy and efficient system for video generation.&lt;/b&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2024/08] üî• &lt;b&gt;Release PAB paper: &lt;a href=&#34;https://arxiv.org/abs/2408.12588&#34;&gt;Real-Time Video Generation with Pyramid Attention Broadcast&lt;/a&gt;.&lt;/b&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] Propose Pyramid Attention Broadcast (PAB)[&lt;a href=&#34;https://arxiv.org/abs/2408.12588&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://oahzxl.github.io/PAB/&#34;&gt;blog&lt;/a&gt;][&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/docs/pab.md&#34;&gt;doc&lt;/a&gt;], the first approach to achieve &lt;b&gt;real-time&lt;/b&gt; DiT-based video generation, delivering &lt;b&gt;negligible quality loss&lt;/b&gt; without &lt;b&gt;requiring any training&lt;/b&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] Support &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt; and &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/03] Propose Dynamic Sequence Parallel (DSP)[&lt;a href=&#34;https://arxiv.org/abs/2403.10266&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/docs/dsp.md&#34;&gt;doc&lt;/a&gt;], achieves &lt;strong&gt;3x&lt;/strong&gt; speed for training and &lt;strong&gt;2x&lt;/strong&gt; speed for inference in Open-Sora compared with sota sequence parallelism.&lt;/li&gt; &#xA; &lt;li&gt;[2024/03] Support &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] üéâ Release &lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/VideoSys/tree/v1.0.0&#34;&gt;OpenDiT&lt;/a&gt;: An Easy, Fast and Memory-Efficent System for DiT Training and Inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;VideoSys is an open-source project that provides a user-friendly and high-performance infrastructure for video generation. This comprehensive toolkit will support the entire pipeline from training and inference to serving and compression.&lt;/p&gt; &#xA;&lt;p&gt;We are committed to continually integrating cutting-edge open-source video models and techniques. Stay tuned for exciting enhancements and new features on the horizon!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Prerequisites:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.10&lt;/li&gt; &#xA; &lt;li&gt;PyTorch &amp;gt;= 1.13 (We recommend to use a &amp;gt;2.0 version)&lt;/li&gt; &#xA; &lt;li&gt;CUDA &amp;gt;= 11.6&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We strongly recommend using Anaconda to create a new environment (Python &amp;gt;= 3.10) to run our examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n videosys python=3.10 -y&#xA;conda activate videosys&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install VideoSys:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/NUS-HPC-AI-Lab/VideoSys&#xA;cd VideoSys&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;VideoSys supports many diffusion models with our various acceleration techniques, enabling these models to run faster and consume less memory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;You can find all available models and their supported acceleration techniques in the following table. Click &lt;code&gt;Doc&lt;/code&gt; to see how to use them.&lt;/b&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Train&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Infer&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Acceleration Techniques&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Usage&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/VideoSys?tab=readme-ov-file#dyanmic-sequence-parallelism-dsp-paperdoc&#34;&gt;DSP&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/VideoSys?tab=readme-ov-file#pyramid-attention-broadcast-pab-blogdoc&#34;&gt;PAB&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Sora [&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;source&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;üü°&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/examples/open_sora/sample.py&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Sora-Plan [&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;source&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/examples/open_sora_plan/sample.py&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latte [&lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;source&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/examples/latte/sample.py&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVideo [&lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;source&lt;/a&gt;]&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/examples/cogvideo/sample.py&#34;&gt;Code&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acceleration Techniques&lt;/h2&gt; &#xA;&lt;h3&gt;Pyramid Attention Broadcast (PAB) [&lt;a href=&#34;https://arxiv.org/abs/2408.12588&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://arxiv.org/abs/2403.10266&#34;&gt;blog&lt;/a&gt;][&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/docs/pab.md&#34;&gt;doc&lt;/a&gt;]&lt;/h3&gt; &#xA;&lt;p&gt;Real-Time Video Generation with Pyramid Attention Broadcast&lt;/p&gt; &#xA;&lt;p&gt;Authors: &lt;a href=&#34;https://oahzxl.github.io/&#34;&gt;Xuanlei Zhao&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;, &lt;a href=&#34;&#34;&gt;Xiaolong Jin&lt;/a&gt;&lt;sup&gt;2*&lt;/sup&gt;, &lt;a href=&#34;https://kaiwang960112.github.io/&#34;&gt;Kai Wang&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;, and &lt;a href=&#34;https://www.comp.nus.edu.sg/~youy/&#34;&gt;Yang You&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; (* indicates equal contribution)&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;National University of Singapore, &lt;sup&gt;2&lt;/sup&gt;Purdue University&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/assets/figures/pab_method.png&#34; alt=&#34;method&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PAB is the first approach to achieve &lt;b&gt;real-time&lt;/b&gt; DiT-based video generation, delivering &lt;b&gt;lossless quality&lt;/b&gt; without &lt;b&gt;requiring any training&lt;/b&gt;. By mitigating redundant attention computation, PAB achieves up to 21.6 FPS with 10.6x acceleration, without sacrificing quality across popular DiT-based video generation models including &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;Open-Sora&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt; and &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See its details &lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/docs/pab.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Dyanmic Sequence Parallelism (DSP) [&lt;a href=&#34;https://arxiv.org/abs/2403.10266&#34;&gt;paper&lt;/a&gt;][&lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/docs/dsp.md&#34;&gt;doc&lt;/a&gt;]&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/assets/figures/dsp_overview.png&#34; alt=&#34;dsp_overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;DSP is a novel, elegant and super efficient sequence parallelism for &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;Open-Sora&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt; and other multi-dimensional transformer architecture.&lt;/p&gt; &#xA;&lt;p&gt;It achieves &lt;strong&gt;3x&lt;/strong&gt; speed for training and &lt;strong&gt;2x&lt;/strong&gt; speed for inference in Open-Sora compared with sota sequence parallelism (&lt;a href=&#34;https://arxiv.org/abs/2309.14509&#34;&gt;DeepSpeed Ulysses&lt;/a&gt;). For a 10s (80 frames) of 512x512 video, the inference latency of Open-Sora is:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;1xH800&lt;/th&gt; &#xA;   &lt;th&gt;8xH800 (DS Ulysses)&lt;/th&gt; &#xA;   &lt;th&gt;8xH800 (DSP)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latency(s)&lt;/td&gt; &#xA;   &lt;td&gt;106&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;See its details &lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/docs/dsp.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/VideoSys/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for how to get involved.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/VideoSys/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=NUS-HPC-AI-Lab/VideoSys&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#NUS-HPC-AI-Lab/VideoSys&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=NUS-HPC-AI-Lab/VideoSys&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{videosys2024,&#xA;  author={VideoSys Team},&#xA;  title={VideoSys: An Easy and Efficient System for Video Generation},&#xA;  year={2024},&#xA;  publisher={GitHub},&#xA;  url = {https://github.com/NUS-HPC-AI-Lab/VideoSys},&#xA;}&#xA;&#xA;@misc{zhao2024pab,&#xA;  title={Real-Time Video Generation with Pyramid Attention Broadcast},&#xA;  author={Xuanlei Zhao and Xiaolong Jin and Kai Wang and Yang You},&#xA;  year={2024},&#xA;  eprint={2408.12588},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV},&#xA;  url={https://arxiv.org/abs/2408.12588},&#xA;}&#xA;&#xA;@misc{zhao2024dsp,&#xA;  title={DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers},&#xA;  author={Xuanlei Zhao and Shenggan Cheng and Chang Chen and Zangwei Zheng and Ziming Liu and Zheming Yang and Yang You},&#xA;  year={2024},&#xA;  eprint={2403.10266},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.DC},&#xA;  url={https://arxiv.org/abs/2403.10266},&#xA;}&#xA;&#xA;@misc{zhao2024opendit,&#xA;  author={Xuanlei Zhao, Zhongkai Zhao, Ziming Liu, Haotian Zhou, Qianli Ma, and Yang You},&#xA;  title={OpenDiT: An Easy, Fast and Memory-Efficient System for DiT Training and Inference},&#xA;  year={2024},&#xA;  publisher={GitHub},&#xA;  url={https://github.com/NUS-HPC-AI-Lab/VideoSys/tree/v1.0.0},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>