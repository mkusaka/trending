<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-01T01:34:01Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>VinciGit00/Scrapegraph-ai</title>
    <updated>2024-05-01T01:34:01Z</updated>
    <id>tag:github.com,2024-05-01:/VinciGit00/Scrapegraph-ai</id>
    <link href="https://github.com/VinciGit00/Scrapegraph-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python scraper based on AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üï∑Ô∏è ScrapeGraphAI: You Only Scrape Once&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/scrapegraphai&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/scrapegraphai&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pylint-dev/pylint&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/linting-pylint-yellowgreen&#34; alt=&#34;linting: pylint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/pylint.yml&#34;&gt;&lt;img src=&#34;https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/pylint.yml/badge.svg?sanitize=true&#34; alt=&#34;Pylint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/codeql.yml&#34;&gt;&lt;img src=&#34;https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/codeql.yml/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/gkxQDAjfeX&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/gkxQDAjfeX&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ScrapeGraphAI is a &lt;em&gt;web scraping&lt;/em&gt; python library that uses LLM and direct graph logic to create scraping pipelines for websites, documents and XML files. Just say which information you want to extract and the library will do it for you!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/VinciGit00/Scrapegraph-ai/main/docs/assets/scrapegraphai_logo.png&#34; alt=&#34;Scrapegraph-ai Logo&#34; style=&#34;width: 50%;&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick install&lt;/h2&gt; &#xA;&lt;p&gt;The reference page for Scrapegraph-ai is available on the official page of pypy: &lt;a href=&#34;https://pypi.org/project/scrapegraphai/&#34;&gt;pypi&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install scrapegraphai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;you will also need to install Playwright for javascript-based scraping:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîç Demo&lt;/h2&gt; &#xA;&lt;p&gt;Official streamlit demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scrapegraph-ai-demo.streamlit.app/&#34;&gt;&lt;img src=&#34;https://skillicons.dev/icons?i=react&#34; alt=&#34;My Skills&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try it directly on the web using Google Colab:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1sEZBonBMGP44CtO6GQTwAlL0BGJXjtfd?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow the procedure on the following link to setup your OpenAI API key: &lt;a href=&#34;https://scrapegraph-ai.readthedocs.io/en/latest/index.html&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation for ScrapeGraphAI can be found &lt;a href=&#34;https://scrapegraph-ai.readthedocs.io/en/latest/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check out also the docusaurus &lt;a href=&#34;https://scrapegraph-doc.onrender.com/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;SmartScraper&lt;/code&gt; class to extract information from a website using a prompt.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;SmartScraper&lt;/code&gt; class is a direct graph implementation that uses the most common nodes present in a web scraping pipeline. For more information, please see the &lt;a href=&#34;https://scrapegraph-ai.readthedocs.io/en/latest/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Case 1: Extracting information using Ollama&lt;/h3&gt; &#xA;&lt;p&gt;Remember to download the model on Ollama separately!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scrapegraphai.graphs import SmartScraperGraph&#xA;&#xA;graph_config = {&#xA;    &#34;llm&#34;: {&#xA;        &#34;model&#34;: &#34;ollama/mistral&#34;,&#xA;        &#34;temperature&#34;: 0,&#xA;        &#34;format&#34;: &#34;json&#34;,  # Ollama needs the format to be specified explicitly&#xA;        &#34;base_url&#34;: &#34;http://localhost:11434&#34;,  # set Ollama URL&#xA;    },&#xA;    &#34;embeddings&#34;: {&#xA;        &#34;model&#34;: &#34;ollama/nomic-embed-text&#34;,&#xA;        &#34;base_url&#34;: &#34;http://localhost:11434&#34;,  # set Ollama URL&#xA;    }&#xA;}&#xA;&#xA;smart_scraper_graph = SmartScraperGraph(&#xA;    prompt=&#34;List me all the articles&#34;,&#xA;    # also accepts a string with the already downloaded HTML code&#xA;    source=&#34;https://perinim.github.io/projects&#34;,&#xA;    config=graph_config&#xA;)&#xA;&#xA;result = smart_scraper_graph.run()&#xA;print(result)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Case 2: Extracting information using Docker&lt;/h3&gt; &#xA;&lt;p&gt;Note: before using the local model remember to create the docker container!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;    docker-compose up -d&#xA;    docker exec -it ollama ollama pull stablelm-zephyr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use which models avaiable on Ollama or your own model instead of stablelm-zephyr&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scrapegraphai.graphs import SmartScraperGraph&#xA;&#xA;graph_config = {&#xA;    &#34;llm&#34;: {&#xA;        &#34;model&#34;: &#34;ollama/mistral&#34;,&#xA;        &#34;temperature&#34;: 0,&#xA;        &#34;format&#34;: &#34;json&#34;,  # Ollama needs the format to be specified explicitly&#xA;        # &#34;model_tokens&#34;: 2000, # set context length arbitrarily&#xA;    },&#xA;}&#xA;&#xA;smart_scraper_graph = SmartScraperGraph(&#xA;    prompt=&#34;List me all the articles&#34;,&#xA;    # also accepts a string with the already downloaded HTML code&#xA;    source=&#34;https://perinim.github.io/projects&#34;,  &#xA;    config=graph_config&#xA;)&#xA;&#xA;result = smart_scraper_graph.run()&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Case 3: Extracting information using Openai model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scrapegraphai.graphs import SmartScraperGraph&#xA;OPENAI_API_KEY = &#34;YOUR_API_KEY&#34;&#xA;&#xA;graph_config = {&#xA;    &#34;llm&#34;: {&#xA;        &#34;api_key&#34;: OPENAI_API_KEY,&#xA;        &#34;model&#34;: &#34;gpt-3.5-turbo&#34;,&#xA;    },&#xA;}&#xA;&#xA;smart_scraper_graph = SmartScraperGraph(&#xA;    prompt=&#34;List me all the articles&#34;,&#xA;    # also accepts a string with the already downloaded HTML code&#xA;    source=&#34;https://perinim.github.io/projects&#34;,&#xA;    config=graph_config&#xA;)&#xA;&#xA;result = smart_scraper_graph.run()&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Case 4: Extracting information using Groq&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scrapegraphai.graphs import SmartScraperGraph&#xA;from scrapegraphai.utils import prettify_exec_info&#xA;&#xA;groq_key = os.getenv(&#34;GROQ_APIKEY&#34;)&#xA;&#xA;graph_config = {&#xA;    &#34;llm&#34;: {&#xA;        &#34;model&#34;: &#34;groq/gemma-7b-it&#34;,&#xA;        &#34;api_key&#34;: groq_key,&#xA;        &#34;temperature&#34;: 0&#xA;    },&#xA;    &#34;embeddings&#34;: {&#xA;        &#34;model&#34;: &#34;ollama/nomic-embed-text&#34;,&#xA;        &#34;temperature&#34;: 0,&#xA;        &#34;base_url&#34;: &#34;http://localhost:11434&#34;, &#xA;    },&#xA;    &#34;headless&#34;: False&#xA;}&#xA;&#xA;smart_scraper_graph = SmartScraperGraph(&#xA;    prompt=&#34;List me all the projects with their description and the author.&#34;,&#xA;    source=&#34;https://perinim.github.io/projects&#34;,&#xA;    config=graph_config&#xA;)&#xA;&#xA;result = smart_scraper_graph.run()&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Case 5: Extracting information using Gemini&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scrapegraphai.graphs import SmartScraperGraph&#xA;GOOGLE_APIKEY = &#34;YOUR_API_KEY&#34;&#xA;&#xA;# Define the configuration for the graph&#xA;graph_config = {&#xA;    &#34;llm&#34;: {&#xA;        &#34;api_key&#34;: GOOGLE_APIKEY,&#xA;        &#34;model&#34;: &#34;gemini-pro&#34;,&#xA;    },&#xA;}&#xA;&#xA;# Create the SmartScraperGraph instance&#xA;smart_scraper_graph = SmartScraperGraph(&#xA;    prompt=&#34;List me all the articles&#34;,&#xA;    source=&#34;https://perinim.github.io/projects&#34;,&#xA;    config=graph_config&#xA;)&#xA;&#xA;result = smart_scraper_graph.run()&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output for all 3 the cases will be a dictionary with the extracted information, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;{&#xA;    &#39;titles&#39;: [&#xA;        &#39;Rotary Pendulum RL&#39;&#xA;        ],&#xA;    &#39;descriptions&#39;: [&#xA;        &#39;Open Source project aimed at controlling a real life rotary pendulum using RL algorithms&#39;&#xA;        ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to contribute and join our Discord server to discuss with us improvements and give us suggestions!&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://github.com/VinciGit00/Scrapegraph-ai/raw/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/gkxQDAjfeX&#34;&gt;&lt;img src=&#34;https://skillicons.dev/icons?i=discord&#34; alt=&#34;My Skills&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/company/scrapegraphai/&#34;&gt;&lt;img src=&#34;https://skillicons.dev/icons?i=linkedin&#34; alt=&#34;My Skills&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/scrapegraphai&#34;&gt;&lt;img src=&#34;https://skillicons.dev/icons?i=twitter&#34; alt=&#34;My Skills&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ù§Ô∏è Contributors&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/VinciGit00/Scrapegraph-ai/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=VinciGit00/Scrapegraph-ai&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üéì Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you have used our library for research purposes please quote us with the following reference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;  @misc{scrapegraph-ai,&#xA;    author = {Marco Perini, Lorenzo Padoan, Marco Vinciguerra},&#xA;    title = {Scrapegraph-ai},&#xA;    year = {2024},&#xA;    url = {https://github.com/VinciGit00/Scrapegraph-ai},&#xA;    note = {A Python library for scraping leveraging large language models}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/VinciGit00/Scrapegraph-ai/main/docs/assets/logo_authors.png&#34; alt=&#34;Authors Logos&#34;&gt; &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Contact Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Marco Vinciguerra&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/marco-vinciguerra-7ba365242/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Linkedin-blue?style=flat&amp;amp;logo=Linkedin&amp;amp;logoColor=white&#34; alt=&#34;Linkedin Badge&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Marco Perini&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/perinim/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Linkedin-blue?style=flat&amp;amp;logo=Linkedin&amp;amp;logoColor=white&#34; alt=&#34;Linkedin Badge&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lorenzo Padoan&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/lorenzo-padoan-4521a2154/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Linkedin-blue?style=flat&amp;amp;logo=Linkedin&amp;amp;logoColor=white&#34; alt=&#34;Linkedin Badge&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;ScrapeGraphAI is licensed under the MIT License. See the &lt;a href=&#34;https://github.com/VinciGit00/Scrapegraph-ai/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We would like to thank all the contributors to the project and the open-source community for their support.&lt;/li&gt; &#xA; &lt;li&gt;ScrapeGraphAI is meant to be used for data exploration and research purposes only. We are not responsible for any misuse of the library.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>JackAILab/ConsistentID</title>
    <updated>2024-05-01T01:34:01Z</updated>
    <id>tag:github.com,2024-05-01:/JackAILab/ConsistentID</id>
    <link href="https://github.com/JackAILab/ConsistentID" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Customized ID Consistent for human&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/JackAILab/ConsistentID/assets/135965025/c0594480-d73d-4268-95ca-5494ca2a61e4&#34; height=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- ## &lt;div align=&#34;center&#34;&gt;&lt;b&gt;ConsistentID&lt;/b&gt;&lt;/div&gt; --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;ConsistentID : Portrait Generation with Multimodal Fine-Grained Identity Preserving &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-md-dark.svg?sanitize=true&#34; alt=&#34;Paper page&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;p&gt;[üìÑ&lt;a href=&#34;https://arxiv.org/abs/2404.16771&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [üö©&lt;a href=&#34;https://ssugarwh.github.io/consistentid.github.io/&#34;&gt;Project Page&lt;/a&gt;] ‚ÄÉ [üñº&lt;a href=&#34;http://consistentid.natapp1.cc/&#34;&gt;Gradio Demo&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA; &lt;p&gt;[ü§ó&lt;a href=&#34;https://huggingface.co/spaces/JackAILab/ConsistentID&#34;&gt;Faster Demo&lt;/a&gt;] ‚ÄÉ &lt;br&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;üå† &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Portrait generation with extremely high &lt;strong&gt;ID fidelity&lt;/strong&gt;, without sacrificing diversity, text controllability.&lt;/li&gt; &#xA; &lt;li&gt;Introducing &lt;strong&gt;FaceParsing&lt;/strong&gt; and &lt;strong&gt;FaceID&lt;/strong&gt; information into the Diffusion model.&lt;/li&gt; &#xA; &lt;li&gt;Rapid customization &lt;strong&gt;within seconds&lt;/strong&gt;, with no additional LoRA training.&lt;/li&gt; &#xA; &lt;li&gt;Can serve as an &lt;strong&gt;Adapter&lt;/strong&gt; to collaborate with other Base Models alongside LoRA modules in community.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üî• &lt;strong&gt;Examples&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/JackAILab/ConsistentID/assets/135965025/f949a03d-bed2-4839-a995-7b451d8c981b&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üö© To-Do List&lt;/h2&gt; &#xA;&lt;p&gt;Your star will help facilitate the process.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release training, evaluation code, and demo!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Retrain with more data and the SDXL base model to enhance aesthetics and generalization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release a multi-ID input version to guide the improvement of ID diversity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimize training and inference structures to further improve text following and ID decoupling capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üè∑Ô∏è Abstract&lt;/h2&gt; &#xA;&lt;p&gt;This is a work in the field of AIGC that introduces FaceParsing information and FaceID information into the Diffusion model. Previous work mainly focused on overall ID preservation, even though fine-grained ID preservation models such as InstantID have recently been proposed, the injection of facial ID features will be fixed. In order to achieve more flexible consistency maintenance of fine-grained IDs for facial features, a batch of 50000 multimodal fine-grained ID datasets was reconstructed for training the proposed FacialEncoder model, which can support common functions such as personalized photos, gender/age changes, and identity confusion.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, we have defined a unified measurement benchmark FGIS for Fine-Grained Identity Preservice, covering several common facial personalized character scenes and characters, and constructed a fine-grained ID preservation model baseline.&lt;/p&gt; &#xA;&lt;p&gt;Finally, a large number of experiments were conducted in this article, and ConsistentID achieved the effect of SOTA in facial personalization task processing. It was verified that ConsistentID can improve ID consistency and even modify facial features by selecting finer-grained prompts, which opens up a direction for future research on Fine-Grained facial personalization.&lt;/p&gt; &#xA;&lt;h2&gt;üîß Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 2.0.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cuda==11.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name ConsistentID python=3.8.10&#xA;conda activate ConsistentID&#xA;pip install -U pip&#xA;&#xA;# Install requirements&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üì¶Ô∏è Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;Prepare Data in the following format&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ data&#xA;|   ‚îú‚îÄ‚îÄ JSON_all.json &#xA;|   ‚îú‚îÄ‚îÄ resize_IMG # Imgaes &#xA;|   ‚îú‚îÄ‚îÄ all_faceID  # FaceID&#xA;|   ‚îî‚îÄ‚îÄ parsing_mask_IMG # Parsing Mask &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The .json file should be like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;    {&#xA;        &#34;resize_IMG&#34;: &#34;Path to resized image...&#34;,&#xA;        &#34;parsing_color_IMG&#34;: &#34;...&#34;,&#xA;        &#34;parsing_mask_IMG&#34;: &#34;...&#34;,&#xA;        &#34;vqa_llva&#34;: &#34;...&#34;,&#xA;        &#34;id_embed_file_resize&#34;: &#34;...&#34;,&#xA;        &#34;vqa_llva_more_face_detail&#34;: &#34;...&#34;&#xA;    },&#xA;    ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Train&lt;/h2&gt; &#xA;&lt;p&gt;Ensure that the workspace is the root directory of the project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-setup&#34;&gt;bash train_bash.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üß™ Infer&lt;/h2&gt; &#xA;&lt;p&gt;Ensure that the workspace is the root directory of the project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-setup&#34;&gt;python infer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚è¨ Model weights&lt;/h2&gt; &#xA;&lt;p&gt;We are hosting the model weights on &lt;strong&gt;huggingface&lt;/strong&gt; to achieve a faster and more stable demo experience, so stay tuned ~&lt;/p&gt; &#xA;&lt;p&gt;The pre-trained model parameters of the model can now be downloaded on &lt;a href=&#34;https://drive.google.com/file/d/1jCHICryESmNkzGi8J_FlY3PjJz9gqoSI/view?usp=drive_link&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1NAVmH8S7Ls5rZc-snDk1Ng?pwd=nsh6&#34;&gt;Baidu Netdisk&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspired from many excellent demos and repos, including &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IPAdapter&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/fastcomposer&#34;&gt;FastComposer&lt;/a&gt;, &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker&#34;&gt;PhotoMaker&lt;/a&gt;. Thanks for their great works!&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the open source contributions of the following work: &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parsing.PyTorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;insightface&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;FFHQ&lt;/a&gt;, &lt;a href=&#34;https://github.com/switchablenorms/CelebAMask-HQ&#34;&gt;CelebA&lt;/a&gt;, &lt;a href=&#34;https://github.com/SelfishGene/SFHQ-dataset&#34;&gt;SFHQ&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ü§ó Thanks to the huggingface gradio team &lt;a href=&#34;https://github.com/huggingface&#34;&gt;ZeroGPUs&lt;/a&gt; for their free GPU support!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project strives to impact the domain of AI-driven image generation positively. Users are granted the freedom to create images using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{huang2024consistentid,&#xA;  title={ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving},&#xA;  author={Huang, Jiehui and Dong, Xiao and Song, Wenhui and Li, Hanhui and Zhou, Jun and Cheng, Yuhao and Liao, Shutao and Chen, Long and Yan, Yiqiang and Liao, Shengcai and others},&#xA;  journal={arXiv preprint arXiv:2404.16771},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mbzuai-oryx/LLaVA-pp</title>
    <updated>2024-05-01T01:34:01Z</updated>
    <id>tag:github.com,2024-05-01:/mbzuai-oryx/LLaVA-pp</id>
    <link href="https://github.com/mbzuai-oryx/LLaVA-pp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üî•üî• LLaVA++: Extending LLaVA with Phi-3 and LLaMA-3 (LLaVA LLaMA-3, LLaVA Phi-3)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaVA++: Extending Visual Capabilities with LLaMA-3 and Phi-3&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;Oryx Models&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://www.hanoonarasheed.com/&#34;&gt;Hanoona Rasheed&lt;/a&gt;*, &lt;a href=&#34;https://www.muhammadmaaz.com&#34;&gt;Muhammad Maaz&lt;/a&gt;*, &lt;a href=&#34;https://salman-h-khan.github.io/&#34;&gt;Salman Khan&lt;/a&gt;, and &lt;a href=&#34;https://sites.google.com/view/fahadkhans/home&#34;&gt;Fahad Khan&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;* Equal contributions&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;Mohamed bin Zayed University of AI (MBZUAI)&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/10Z2HaY5zvy2GZZ4v245PtiDPukm0NbF6?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Google-Colab-red&#34; alt=&#34;Google&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bengal-eminent-wasp.ngrok-free.app&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Online-Demo-F9D371&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/MBZUAI/LLaMA-3-V&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HF_Demo-LLaMA_3-0FFFFF.svg?sanitize=true&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/MBZUAI/Phi-3-V&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HF_Demo-Phi_3-0FFFFF.svg?sanitize=true&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üì¢ Latest Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Apr-30-24&lt;/strong&gt;- LLaMA-3-V and Phi-3-V demos are now available via Hugging Face Spaces. Check them out at &lt;a href=&#34;https://huggingface.co/spaces/MBZUAI/LLaMA-3-V&#34;&gt;LLaMA-3-V&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/spaces/MBZUAI/Phi-3-V&#34;&gt;Phi-3-V&lt;/a&gt; üî•üî•üî•&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Apr-28-24&lt;/strong&gt;- Online demo of Phi-3-V and LLaMA-3-V are released, check them out at &lt;a href=&#34;https://bengal-eminent-wasp.ngrok-free.app&#34;&gt;Online Demo&lt;/a&gt; üî•üî•üî•&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Apr-28-24&lt;/strong&gt;- LoRA, fully fine-tuned and &lt;a href=&#34;https://github.com/bfshi/scaling_on_scales.git&#34;&gt;S&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; fine-tuned models and results are added! üî•üî•üî•&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Apr-27-24&lt;/strong&gt;- Google Colab is released to chat with Phi-3-V-3.8B model, check it out at &lt;a href=&#34;https://colab.research.google.com/drive/10Z2HaY5zvy2GZZ4v245PtiDPukm0NbF6?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; üî•üî•üî•&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Apr-26-24&lt;/strong&gt;- Phi-3-V and LLaVA-3-V released: Excited to release the new integration of LLaVA with Phi-3 Mini Instruct and LLaMA-3 Instruct models! &lt;a href=&#34;https://huggingface.co/collections/MBZUAI/llava-662b38b972e3e3e4d8f821bb&#34;&gt;Hugging Face&lt;/a&gt; üî•üî•üî•&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/face.png&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üí¨ Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This repository enhances the capabilities of the LLaVA 1.5 model, incorporating latest LLMs released this weaküî•, &lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct&#34;&gt;Phi-3 Mini Instruct 3.8B&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;LLaMA-3 Instruct 8B&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üèÜ Results: Phi-3-V and LLaVA-3-V&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/lava++_radar_plot.png&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Comparison on Benchmarks for Instruction-following LMMS &amp;amp; academic-task-oriented datasets:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/LLaVA-pp-results.png&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Average computed excluding MME, and second-best are underlined.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ñ Model-Zoo&lt;/h2&gt; &#xA;&lt;p&gt;The following table provides an overview of the available models in our zoo. For each model, you can find links to its Hugging Face page.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hugging Face Link&lt;/th&gt; &#xA;   &lt;th&gt;Summary&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Phi-3-mini-4k-instruct-pretrain&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Phi-3-mini-4k-instruct-pretrain&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pretrained on &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain&#34;&gt;LCS-558K&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Phi-3-mini-4k-instruct-lora&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Phi-3-mini-4k-instruct-lora&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LoRA weights fine-tuned on &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;LLaVA-Instruct-665K&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Phi-3-mini-4k-instruct&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Phi-3-mini-4k-instruct&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Merged LoRA weights in HuggingFace format.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Phi-3-mini-4k-instruct-FT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Phi-3-mini-4k-instruct-FT&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fully fine-tuned model weights in HuggingFace format.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hugging Face Link&lt;/th&gt; &#xA;   &lt;th&gt;Summary&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Meta-Llama-3-8B-Instruct-pretrain&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Meta-Llama-3-8B-Instruct-pretrain&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pretrained on &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain&#34;&gt;LCS-558K&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Meta-Llama-3-8B-Instruct-lora&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Meta-Llama-3-8B-Instruct-lora&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LoRA weights fine-tuned on &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;LLaVA-Instruct-665K&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Meta-Llama-3-8B-Instruct&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Meta-Llama-3-8B-Instruct&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Merged weights in HuggingFace format.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Meta-Llama-3-8B-Instruct-FT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Meta-Llama-3-8B-Instruct-FT&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fully fine-tuned model weights in HuggingFace format.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Meta-Llama-3-8B-Instruct-FT-S2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/LLaVA-Meta-Llama-3-8B-Instruct-FT-S2&#34;&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fully fine-tuned S2 model weights in HuggingFace format.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/mbzuai-oryx/LLaVA-pp.git&#xA;cd LLaVA-pp&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Packages you need to update from LLAVA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/transformers@a98c41798cf6ed99e1ff17e3792d6e06a2ff2ff3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Phi-3-V&lt;/h2&gt; &#xA;&lt;p&gt;To integrate Phi-3-V with LLaVA, follow these steps to update the codebase:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Copy necessary files&#xA;cp Phi-3-V/train.py LLaVA/llava/train/train.py&#xA;cp Phi-3-V/llava_phi3.py LLaVA/llava/model/language_model/llava_phi3.py&#xA;cp Phi-3-V/builder.py LLaVA/llava/model/builder.py&#xA;cp Phi-3-V/model__init__.py LLaVA/llava/model/__init__.py&#xA;cp Phi-3-V/main__init__.py LLaVA/llava/__init__.py&#xA;cp Phi-3-V/conversation.py LLaVA/llava/conversation.py&#xA;&#xA;# Training commands&#xA;cp scripts/Phi3-V_pretrain.sh LLaVA/Vi-phi3_pretrain.sh&#xA;cp scripts/Phi3-V_finetune_lora.sh LLaVA/Vi-phi3_finetune_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train Phi-3-V&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pre-train&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd LLaVA&#xA;bash Phi3-V_pretrain.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetune&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd LLaVA&#xA;bash Phi3-V_finetune_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ LLaMA-3-V&lt;/h2&gt; &#xA;&lt;p&gt;To integrate LLaMA-3-V with LLaVA, follow these steps to update the codebase:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Copy necessary files&#xA;cp LLaMA-3-V/train.py LLaVA/llava/train/train.py&#xA;cp LLaMA-3-V/conversation.py LLaVA/llava/conversation.py&#xA;cp LLaMA-3-V/builder.py LLaVA/llava/model/builder.py&#xA;cp LLaMA-3-V/llava_llama.py LLaVA/llava/model/language_model/llava_llama.py&#xA;&#xA;# Training commands&#xA;cp scripts/LLaMA3-V_pretrain.sh LLaVA/LLaMA3-V_pretrain.sh&#xA;cp scripts/LLaMA3-V_finetune_lora.sh LLaVA/LLaMA3-V_finetune_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train LLaMA-3-V&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pre-train&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd LLaVA&#xA;bash LLaMA3-V_pretrain.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetune&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd LLaVA&#xA;bash LLaMA3-V_finetune_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üôè Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We are thankful to &lt;a href=&#34;https://github.com/haotian-liu/LLaVA.git&#34;&gt;LLaVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval.git&#34;&gt;lmms-eval&lt;/a&gt; and &lt;a href=&#34;https://github.com/bfshi/scaling_on_scales.git&#34;&gt;S&lt;sup&gt;2&lt;/sup&gt;-Wrapper&lt;/a&gt; for releasing their models and code as open-source contributions.&lt;/p&gt; &#xA;&lt;p&gt;In case if you face any issues or have any questions, please feel free to create an issue or reach out at &lt;a href=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/hanoona.bangalath@mbzuai.ac.ae&#34;&gt;hanoona.bangalath@mbzuai.ac.ae&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/muhammad.maaz@mbzuai.ac.ae&#34;&gt;muhammad.maaz@mbzuai.ac.ae&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìú Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;  @misc{hanoona2024LLaVA++,&#xA;          title={LLaVA++: Extending Visual Capabilities with LLaMA-3 and Phi-3},&#xA;          author={Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad S.},&#xA;          url={https://github.com/mbzuai-oryx/LLaVA-pp},&#xA;          year={2024}&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ival-mbzuai.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/IVAL_logo.png&#34; width=&#34;200&#34; height=&#34;100&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mbzuai-oryx&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/Oryx_logo.png&#34; width=&#34;100&#34; height=&#34;100&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mbzuai.ac.ae&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/MBZUAI_logo.png&#34; width=&#34;360&#34; height=&#34;85&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>