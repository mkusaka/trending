<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-29T01:35:39Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yzhao062/pyod</title>
    <updated>2024-06-29T01:35:39Z</updated>
    <id>tag:github.com,2024-06-29:/yzhao062/pyod</id>
    <link href="https://github.com/yzhao062/pyod" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Comprehensive and Scalable Python Library for Outlier Detection (Anomaly Detection)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Outlier Detection (PyOD)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deployment &amp;amp; Documentation &amp;amp; Stats &amp;amp; License&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;|badge_pypi| |badge_anaconda| |badge_docs| |badge_stars| |badge_forks| |badge_downloads| |badge_testing| |badge_coverage| |badge_maintainability| |badge_license| |badge_benchmark|&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_pypi| image:: &lt;a href=&#34;https://img.shields.io/pypi/v/pyod.svg?color=brightgreen&#34;&gt;https://img.shields.io/pypi/v/pyod.svg?color=brightgreen&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/pyod/&#34;&gt;https://pypi.org/project/pyod/&lt;/a&gt; :alt: PyPI version&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_anaconda| image:: &lt;a href=&#34;https://anaconda.org/conda-forge/pyod/badges/version.svg&#34;&gt;https://anaconda.org/conda-forge/pyod/badges/version.svg&lt;/a&gt; :target: &lt;a href=&#34;https://anaconda.org/conda-forge/pyod&#34;&gt;https://anaconda.org/conda-forge/pyod&lt;/a&gt; :alt: Anaconda version&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_docs| image:: &lt;a href=&#34;https://readthedocs.org/projects/pyod/badge/?version=latest&#34;&gt;https://readthedocs.org/projects/pyod/badge/?version=latest&lt;/a&gt; :target: &lt;a href=&#34;https://pyod.readthedocs.io/en/latest/?badge=latest&#34;&gt;https://pyod.readthedocs.io/en/latest/?badge=latest&lt;/a&gt; :alt: Documentation status&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_stars| image:: &lt;a href=&#34;https://img.shields.io/github/stars/yzhao062/pyod.svg&#34;&gt;https://img.shields.io/github/stars/yzhao062/pyod.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/yzhao062/pyod/stargazers&#34;&gt;https://github.com/yzhao062/pyod/stargazers&lt;/a&gt; :alt: GitHub stars&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_forks| image:: &lt;a href=&#34;https://img.shields.io/github/forks/yzhao062/pyod.svg?color=blue&#34;&gt;https://img.shields.io/github/forks/yzhao062/pyod.svg?color=blue&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/yzhao062/pyod/network&#34;&gt;https://github.com/yzhao062/pyod/network&lt;/a&gt; :alt: GitHub forks&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_downloads| image:: &lt;a href=&#34;https://pepy.tech/badge/pyod&#34;&gt;https://pepy.tech/badge/pyod&lt;/a&gt; :target: &lt;a href=&#34;https://pepy.tech/project/pyod&#34;&gt;https://pepy.tech/project/pyod&lt;/a&gt; :alt: Downloads&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_testing| image:: &lt;a href=&#34;https://github.com/yzhao062/pyod/actions/workflows/testing.yml/badge.svg&#34;&gt;https://github.com/yzhao062/pyod/actions/workflows/testing.yml/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/yzhao062/pyod/actions/workflows/testing.yml&#34;&gt;https://github.com/yzhao062/pyod/actions/workflows/testing.yml&lt;/a&gt; :alt: testing&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_coverage| image:: &lt;a href=&#34;https://coveralls.io/repos/github/yzhao062/pyod/badge.svg&#34;&gt;https://coveralls.io/repos/github/yzhao062/pyod/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://coveralls.io/github/yzhao062/pyod&#34;&gt;https://coveralls.io/github/yzhao062/pyod&lt;/a&gt; :alt: Coverage Status&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_maintainability| image:: &lt;a href=&#34;https://api.codeclimate.com/v1/badges/bdc3d8d0454274c753c4/maintainability&#34;&gt;https://api.codeclimate.com/v1/badges/bdc3d8d0454274c753c4/maintainability&lt;/a&gt; :target: &lt;a href=&#34;https://codeclimate.com/github/yzhao062/Pyod/maintainability&#34;&gt;https://codeclimate.com/github/yzhao062/Pyod/maintainability&lt;/a&gt; :alt: Maintainability&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_license| image:: &lt;a href=&#34;https://img.shields.io/github/license/yzhao062/pyod.svg&#34;&gt;https://img.shields.io/github/license/yzhao062/pyod.svg&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/yzhao062/pyod/raw/master/LICENSE&#34;&gt;https://github.com/yzhao062/pyod/blob/master/LICENSE&lt;/a&gt; :alt: License&lt;/p&gt; &#xA;&lt;p&gt;.. |badge_benchmark| image:: &lt;a href=&#34;https://img.shields.io/badge/ADBench-benchmark_results-pink&#34;&gt;https://img.shields.io/badge/ADBench-benchmark_results-pink&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/Minqi824/ADBench&#34;&gt;https://github.com/Minqi824/ADBench&lt;/a&gt; :alt: Benchmark&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Read Me First ^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;Welcome to PyOD, a versatile Python library for detecting anomalies in multivariate data. Whether you&#39;re tackling a small-scale project or large datasets, PyOD offers a range of algorithms to suit your needs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;For time-series outlier detection&lt;/strong&gt;, please use &lt;code&gt;TODS &amp;lt;https://github.com/datamllab/tods&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;For graph outlier detection&lt;/strong&gt;, please use &lt;code&gt;PyGOD &amp;lt;https://pygod.org/&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance Comparison &amp;amp; Datasets&lt;/strong&gt;: We have a 45-page, the most comprehensive &lt;code&gt;anomaly detection benchmark paper &amp;lt;https://www.andrew.cmu.edu/user/yuezhao2/papers/22-neurips-adbench.pdf&amp;gt;&lt;/code&gt;&lt;em&gt;. The fully &lt;code&gt;open-sourced ADBench &amp;lt;https://github.com/Minqi824/ADBench&amp;gt;&lt;/code&gt;&lt;/em&gt; compares 30 anomaly detection algorithms on 57 benchmark datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learn more about anomaly detection&lt;/strong&gt; @ &lt;code&gt;Anomaly Detection Resources &amp;lt;https://github.com/yzhao062/anomaly-detection-resources&amp;gt;&lt;/code&gt;_&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PyOD on Distributed Systems&lt;/strong&gt;: you could also run &lt;code&gt;PyOD on databricks &amp;lt;https://www.databricks.com/blog/2023/03/13/unsupervised-outlier-detection-databricks.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;About PyOD ^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;PyOD, established in 2017, has become a go-to &lt;strong&gt;Python library&lt;/strong&gt; for &lt;strong&gt;detecting anomalous/outlying objects&lt;/strong&gt; in multivariate data. This exciting yet challenging field is commonly referred as &lt;code&gt;Outlier Detection &amp;lt;https://en.wikipedia.org/wiki/Anomaly_detection&amp;gt;&lt;/code&gt;_ or &lt;code&gt;Anomaly Detection &amp;lt;https://en.wikipedia.org/wiki/Anomaly_detection&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;PyOD includes more than 50 detection algorithms, from classical LOF (SIGMOD 2000) to the cutting-edge ECOD and DIF (TKDE 2022 and 2023). Since 2017, PyOD has been successfully used in numerous academic researches and commercial products with more than &lt;code&gt;17 million downloads &amp;lt;https://pepy.tech/project/pyod&amp;gt;&lt;/code&gt;&lt;em&gt;. It is also well acknowledged by the machine learning community with various dedicated posts/tutorials, including &lt;code&gt;Analytics Vidhya &amp;lt;https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/&amp;gt;&lt;/code&gt;&lt;/em&gt;, &lt;code&gt;KDnuggets &amp;lt;https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html&amp;gt;&lt;/code&gt;&lt;em&gt;, and &lt;code&gt;Towards Data Science &amp;lt;https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1&amp;gt;&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyOD is featured for&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unified, User-Friendly Interface&lt;/strong&gt; across various algorithms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wide Range of Models&lt;/strong&gt;, from classic techniques to the latest deep learning methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Performance &amp;amp; Efficiency&lt;/strong&gt;, leveraging &lt;code&gt;numba &amp;lt;https://github.com/numba/numba&amp;gt;&lt;/code&gt;_ and &lt;code&gt;joblib &amp;lt;https://github.com/joblib/joblib&amp;gt;&lt;/code&gt;_ for JIT compilation and parallel processing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast Training &amp;amp; Prediction&lt;/strong&gt;, achieved through the SUOD framework [#Zhao2021SUOD]_.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Outlier Detection with 5 Lines of Code&lt;/strong&gt;\ :&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Example: Training an ECOD detector&#xA;from pyod.models.ecod import ECOD&#xA;clf = ECOD()&#xA;clf.fit(X_train)&#xA;y_train_scores = clf.decision_scores_  # Outlier scores for training data&#xA;y_test_scores = clf.decision_function(X_test)  # Outlier scores for test data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Selecting the Right Algorithm:&lt;/strong&gt;. Unsure where to start? Consider these robust and interpretable options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ECOD &amp;lt;https://github.com/yzhao062/pyod/blob/master/examples/ecod_example.py&amp;gt;&lt;/code&gt;_: Example of using ECOD for outlier detection&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Isolation Forest &amp;lt;https://github.com/yzhao062/pyod/blob/master/examples/iforest_example.py&amp;gt;&lt;/code&gt;_: Example of using Isolation Forest for outlier detection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Alternatively, explore &lt;code&gt;MetaOD &amp;lt;https://github.com/yzhao062/MetaOD&amp;gt;&lt;/code&gt;_ for a data-driven approach.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Citing PyOD&lt;/strong&gt;\ :&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;PyOD paper &amp;lt;http://www.jmlr.org/papers/volume20/19-011/19-011.pdf&amp;gt;&lt;/code&gt;_ is published in &lt;code&gt;Journal of Machine Learning Research (JMLR) &amp;lt;http://www.jmlr.org/&amp;gt;&lt;/code&gt;_ (MLOSS track). If you use PyOD in a scientific publication, we would appreciate citations to the following paper::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhao2019pyod,&#xA;    author  = {Zhao, Yue and Nasrullah, Zain and Li, Zheng},&#xA;    title   = {PyOD: A Python Toolbox for Scalable Outlier Detection},&#xA;    journal = {Journal of Machine Learning Research},&#xA;    year    = {2019},&#xA;    volume  = {20},&#xA;    number  = {96},&#xA;    pages   = {1-7},&#xA;    url     = {http://jmlr.org/papers/v20/19-011.html}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Zhao, Y., Nasrullah, Z. and Li, Z., 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of machine learning research (JMLR), 20(96), pp.1-7.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a broader perspective on anomaly detection, see our NeurIPS papers &lt;code&gt;ADBench: Anomaly Detection Benchmark Paper &amp;lt;https://viterbi-web.usc.edu/~yzhao010/papers/22-neurips-adbench.pdf&amp;gt;&lt;/code&gt;_ &amp;amp; &lt;code&gt;ADGym: Design Choices for Deep Anomaly Detection &amp;lt;https://viterbi-web.usc.edu/~yzhao010/papers/23-neurips-adgym.pdf&amp;gt;&lt;/code&gt;_::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{han2022adbench,&#xA;    title={Adbench: Anomaly detection benchmark},&#xA;    author={Han, Songqiao and Hu, Xiyang and Huang, Hailiang and Jiang, Minqi and Zhao, Yue},&#xA;    journal={Advances in Neural Information Processing Systems},&#xA;    volume={35},&#xA;    pages={32142--32159},&#xA;    year={2022}&#xA;}&#xA;&#xA;@article{jiang2023adgym,&#xA;    title={ADGym: Design Choices for Deep Anomaly Detection},&#xA;    author={Jiang, Minqi and Hou, Chaochuan and Zheng, Ao and Han, Songqiao and Huang, Hailiang and Wen, Qingsong and Hu, Xiyang and Zhao, Yue},&#xA;    journal={Advances in Neural Information Processing Systems},&#xA;    volume={36},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;\ :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Installation &amp;lt;#installation&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;API Cheatsheet &amp;amp; Reference &amp;lt;#api-cheatsheet--reference&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ADBench Benchmark and Datasets &amp;lt;#adbench-benchmark-and-datasets&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Model Save &amp;amp; Load &amp;lt;#model-save--load&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Fast Train with SUOD &amp;lt;#fast-train-with-suod&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Thresholding Outlier Scores &amp;lt;#thresholding-outlier-scores&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Implemented Algorithms &amp;lt;#implemented-algorithms&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Quick Start for Outlier Detection &amp;lt;#quick-start-for-outlier-detection&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;How to Contribute &amp;lt;#how-to-contribute&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Inclusion Criteria &amp;lt;#inclusion-criteria&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Installation ^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;PyOD is designed for easy installation using either &lt;strong&gt;pip&lt;/strong&gt; or &lt;strong&gt;conda&lt;/strong&gt;. We recommend using the latest version of PyOD due to frequent updates and enhancements:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;p&gt;pip install pyod # normal install pip install --upgrade pyod # or update if needed&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;p&gt;conda install -c conda-forge pyod&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you could clone and run setup.py file:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: bash&lt;/p&gt; &#xA;&lt;p&gt;git clone &lt;a href=&#34;https://github.com/yzhao062/pyod.git&#34;&gt;https://github.com/yzhao062/pyod.git&lt;/a&gt; cd pyod pip install .&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Required Dependencies&lt;/strong&gt;\ :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8 or higher&lt;/li&gt; &#xA; &lt;li&gt;joblib&lt;/li&gt; &#xA; &lt;li&gt;matplotlib&lt;/li&gt; &#xA; &lt;li&gt;numpy&amp;gt;=1.19&lt;/li&gt; &#xA; &lt;li&gt;numba&amp;gt;=0.51&lt;/li&gt; &#xA; &lt;li&gt;scipy&amp;gt;=1.5.1&lt;/li&gt; &#xA; &lt;li&gt;scikit_learn&amp;gt;=0.22.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional Dependencies (see details below)&lt;/strong&gt;\ :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;combo (optional, required for models/combination.py and FeatureBagging)&lt;/li&gt; &#xA; &lt;li&gt;keras/tensorflow (optional, required for AutoEncoder, and other deep learning models)&lt;/li&gt; &#xA; &lt;li&gt;suod (optional, required for running SUOD model)&lt;/li&gt; &#xA; &lt;li&gt;xgboost (optional, required for XGBOD)&lt;/li&gt; &#xA; &lt;li&gt;pythresh (optional, required for thresholding)optional&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;API Cheatsheet &amp;amp; Reference ^^^^^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;The full API Reference is available at &lt;code&gt;PyOD Documentation &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.html&amp;gt;&lt;/code&gt;_. Below is a quick cheatsheet for all detectors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;fit(X)&lt;/strong&gt;\ : Fit the detector. The parameter y is ignored in unsupervised methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;decision_function(X)&lt;/strong&gt;\ : Predict raw anomaly scores for X using the fitted detector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;predict(X)&lt;/strong&gt;\ : Determine whether a sample is an outlier or not as binary labels using the fitted detector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;predict_proba(X)&lt;/strong&gt;\ : Estimate the probability of a sample being an outlier using the fitted detector.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;predict_confidence(X)&lt;/strong&gt;\ : Assess the model&#39;s confidence on a per-sample basis (applicable in predict and predict_proba) [#Perini2020Quantifying]_.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key Attributes of a fitted model&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;decision_scores_&lt;/strong&gt;\ : Outlier scores of the training data. Higher scores typically indicate more abnormal behavior. Outliers usually have higher scores.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;labels_&lt;/strong&gt;\ : Binary labels of the training data, where 0 indicates inliers and 1 indicates outliers/anomalies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ADBench Benchmark and Datasets ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;We just released a 45-page, the most comprehensive &lt;code&gt;ADBench: Anomaly Detection Benchmark &amp;lt;https://arxiv.org/abs/2206.09426&amp;gt;&lt;/code&gt;_ [#Han2022ADBench]&lt;em&gt;. The fully &lt;code&gt;open-sourced ADBench &amp;lt;https://github.com/Minqi824/ADBench&amp;gt;&lt;/code&gt;&lt;/em&gt; compares 30 anomaly detection algorithms on 57 benchmark datasets.&lt;/p&gt; &#xA;&lt;p&gt;The organization of &lt;strong&gt;ADBench&lt;/strong&gt; is provided below:&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/Minqi824/ADBench/raw/main/figs/ADBench.png?raw=true&#34;&gt;https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/Minqi824/ADBench/raw/main/figs/ADBench.png?raw=true&#34;&gt;https://github.com/Minqi824/ADBench/blob/main/figs/ADBench.png?raw=true&lt;/a&gt; :alt: benchmark-fig&lt;/p&gt; &#xA;&lt;p&gt;For a simpler visualization, we make &lt;strong&gt;the comparison of selected models&lt;/strong&gt; via &lt;code&gt;compare_all_models.py &amp;lt;https://github.com/yzhao062/pyod/blob/master/examples/compare_all_models.py&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/yzhao062/pyod/raw/development/examples/ALL.png?raw=true&#34;&gt;https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/yzhao062/pyod/raw/development/examples/ALL.png?raw=true&#34;&gt;https://github.com/yzhao062/pyod/blob/development/examples/ALL.png?raw=true&lt;/a&gt; :alt: Comparison_of_All&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Model Save &amp;amp; Load ^^^^^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;PyOD takes a similar approach of sklearn regarding model persistence. See &lt;code&gt;model persistence &amp;lt;https://scikit-learn.org/stable/modules/model_persistence.html&amp;gt;&lt;/code&gt;_ for clarification.&lt;/p&gt; &#xA;&lt;p&gt;In short, we recommend to use joblib or pickle for saving and loading PyOD models. See &lt;code&gt;&#34;examples/save_load_model_example.py&#34; &amp;lt;https://github.com/yzhao062/pyod/blob/master/examples/save_load_model_example.py&amp;gt;&lt;/code&gt;_ for an example. In short, it is simple as below:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from joblib import dump, load&#xA;&#xA;# save the model&#xA;dump(clf, &#39;clf.joblib&#39;)&#xA;# load the model&#xA;clf = load(&#39;clf.joblib&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is known that there are challenges in saving neural network models. Check &lt;code&gt;#328 &amp;lt;https://github.com/yzhao062/pyod/issues/328#issuecomment-917192704&amp;gt;&lt;/code&gt;_ and &lt;code&gt;#88 &amp;lt;https://github.com/yzhao062/pyod/issues/88#issuecomment-615343139&amp;gt;&lt;/code&gt;_ for temporary workaround.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Fast Train with SUOD ^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast training and prediction&lt;/strong&gt;: it is possible to train and predict with a large number of detection models in PyOD by leveraging SUOD framework [#Zhao2021SUOD]&lt;em&gt;. See &lt;code&gt;SUOD Paper &amp;lt;https://www.andrew.cmu.edu/user/yuezhao2/papers/21-mlsys-suod.pdf&amp;gt;&lt;/code&gt;&lt;/em&gt; and &lt;code&gt;SUOD example &amp;lt;https://github.com/yzhao062/pyod/blob/master/examples/suod_example.py&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from pyod.models.suod import SUOD&#xA;&#xA;# initialized a group of outlier detectors for acceleration&#xA;detector_list = [LOF(n_neighbors=15), LOF(n_neighbors=20),&#xA;                 LOF(n_neighbors=25), LOF(n_neighbors=35),&#xA;                 COPOD(), IForest(n_estimators=100),&#xA;                 IForest(n_estimators=200)]&#xA;&#xA;# decide the number of parallel process, and the combination method&#xA;# then clf can be used as any outlier detection model&#xA;clf = SUOD(base_estimators=detector_list, n_jobs=2, combination=&#39;average&#39;,&#xA;           verbose=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Thresholding Outlier Scores ^^^^^^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;A more data based approach can be taken when setting the contamination level. By using a thresholding method, guessing an abritrary value can be replaced with tested techniques for seperating inliers and outliers. Refer to &lt;code&gt;PyThresh &amp;lt;https://github.com/KulikDM/pythresh&amp;gt;&lt;/code&gt;_ for a more in depth look at thresholding.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from pyod.models.knn import KNN&#xA;from pyod.models.thresholds import FILTER&#xA;&#xA;# Set the outlier detection and thresholding methods&#xA;clf = KNN(contamination=FILTER())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Implemented Algorithms ^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;PyOD toolkit consists of four major functional groups:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(i) Individual Detection Algorithms&lt;/strong&gt; :&lt;/p&gt; &#xA;&lt;p&gt;=================== ================== ====================================================================================================== ===== ======================================== Type Abbr Algorithm Year Ref =================== ================== ====================================================================================================== ===== ======================================== Probabilistic ECOD Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions 2022 [#Li2021ECOD]_ Probabilistic ABOD Angle-Based Outlier Detection 2008 [#Kriegel2008Angle]_ Probabilistic FastABOD Fast Angle-Based Outlier Detection using approximation 2008 [#Kriegel2008Angle]_ Probabilistic COPOD COPOD: Copula-Based Outlier Detection 2020 [#Li2020COPOD]_ Probabilistic MAD Median Absolute Deviation (MAD) 1993 [#Iglewicz1993How]_ Probabilistic SOS Stochastic Outlier Selection 2012 [#Janssens2012Stochastic]_ Probabilistic QMCD Quasi-Monte Carlo Discrepancy outlier detection 2001 [#Fang2001Wrap]_ Probabilistic KDE Outlier Detection with Kernel Density Functions 2007 [#Latecki2007Outlier]_ Probabilistic Sampling Rapid distance-based outlier detection via sampling 2013 [#Sugiyama2013Rapid]_ Probabilistic GMM Probabilistic Mixture Modeling for Outlier Analysis [#Aggarwal2015Outlier]_ [Ch.2] Linear Model PCA Principal Component Analysis (the sum of weighted projected distances to the eigenvector hyperplanes) 2003 [#Shyu2003A]_ Linear Model KPCA Kernel Principal Component Analysis 2007 [#Hoffmann2007Kernel]_ Linear Model MCD Minimum Covariance Determinant (use the mahalanobis distances as the outlier scores) 1999 [#Hardin2004Outlier]_ [#Rousseeuw1999A]_ Linear Model CD Use Cook&#39;s distance for outlier detection 1977 [#Cook1977Detection]_ Linear Model OCSVM One-Class Support Vector Machines 2001 [#Scholkopf2001Estimating]_ Linear Model LMDD Deviation-based Outlier Detection (LMDD) 1996 [#Arning1996A]_ Proximity-Based LOF Local Outlier Factor 2000 [#Breunig2000LOF]_ Proximity-Based COF Connectivity-Based Outlier Factor 2002 [#Tang2002Enhancing]_ Proximity-Based (Incremental) COF Memory Efficient Connectivity-Based Outlier Factor (slower but reduce storage complexity) 2002 [#Tang2002Enhancing]_ Proximity-Based CBLOF Clustering-Based Local Outlier Factor 2003 [#He2003Discovering]_ Proximity-Based LOCI LOCI: Fast outlier detection using the local correlation integral 2003 [#Papadimitriou2003LOCI]_ Proximity-Based HBOS Histogram-based Outlier Score 2012 [#Goldstein2012Histogram]_ Proximity-Based kNN k Nearest Neighbors (use the distance to the kth nearest neighbor as the outlier score) 2000 [#Ramaswamy2000Efficient]_ Proximity-Based AvgKNN Average kNN (use the average distance to k nearest neighbors as the outlier score) 2002 [#Angiulli2002Fast]_ Proximity-Based MedKNN Median kNN (use the median distance to k nearest neighbors as the outlier score) 2002 [#Angiulli2002Fast]_ Proximity-Based SOD Subspace Outlier Detection 2009 [#Kriegel2009Outlier]_ Proximity-Based ROD Rotation-based Outlier Detection 2020 [#Almardeny2020A]_ Outlier Ensembles IForest Isolation Forest 2008 [#Liu2008Isolation]_ Outlier Ensembles INNE Isolation-based Anomaly Detection Using Nearest-Neighbor Ensembles 2018 [#Bandaragoda2018Isolation]_ Outlier Ensembles DIF Deep Isolation Forest for Anomaly Detection 2023 [#Xu2023Deep]_ Outlier Ensembles FB Feature Bagging 2005 [#Lazarevic2005Feature]_ Outlier Ensembles LSCP LSCP: Locally Selective Combination of Parallel Outlier Ensembles 2019 [#Zhao2019LSCP]_ Outlier Ensembles XGBOD Extreme Boosting Based Outlier Detection &lt;strong&gt;(Supervised)&lt;/strong&gt; 2018 [#Zhao2018XGBOD]_ Outlier Ensembles LODA Lightweight On-line Detector of Anomalies 2016 [#Pevny2016Loda]_ Outlier Ensembles SUOD SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection &lt;strong&gt;(Acceleration)&lt;/strong&gt; 2021 [#Zhao2021SUOD]_ Neural Networks AutoEncoder Fully connected AutoEncoder (use reconstruction error as the outlier score) [#Aggarwal2015Outlier]_ [Ch.3] Neural Networks VAE Variational AutoEncoder (use reconstruction error as the outlier score) 2013 [#Kingma2013Auto]_ Neural Networks Beta-VAE Variational AutoEncoder (all customized loss term by varying gamma and capacity) 2018 [#Burgess2018Understanding]_ Neural Networks SO_GAAL Single-Objective Generative Adversarial Active Learning 2019 [#Liu2019Generative]_ Neural Networks MO_GAAL Multiple-Objective Generative Adversarial Active Learning 2019 [#Liu2019Generative]_ Neural Networks DeepSVDD Deep One-Class Classification 2018 [#Ruff2018Deep]_ Neural Networks AnoGAN Anomaly Detection with Generative Adversarial Networks 2017 [#Schlegl2017Unsupervised]_ Neural Networks ALAD Adversarially learned anomaly detection 2018 [#Zenati2018Adversarially]_ Graph-based R-Graph Outlier detection by R-graph 2017 [#You2017Provable]_ Graph-based LUNAR LUNAR: Unifying Local Outlier Detection Methods via Graph Neural Networks 2022 [#Goodge2022Lunar]_ =================== ================== ====================================================================================================== ===== ========================================&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(ii) Outlier Ensembles &amp;amp; Outlier Detector Combination Frameworks&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;=================== ================ ===================================================================================================== ===== ======================================== Type Abbr Algorithm Year Ref =================== ================ ===================================================================================================== ===== ======================================== Outlier Ensembles FB Feature Bagging 2005 [#Lazarevic2005Feature]_ Outlier Ensembles LSCP LSCP: Locally Selective Combination of Parallel Outlier Ensembles 2019 [#Zhao2019LSCP]_ Outlier Ensembles XGBOD Extreme Boosting Based Outlier Detection &lt;strong&gt;(Supervised)&lt;/strong&gt; 2018 [#Zhao2018XGBOD]_ Outlier Ensembles LODA Lightweight On-line Detector of Anomalies 2016 [#Pevny2016Loda]_ Outlier Ensembles SUOD SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection &lt;strong&gt;(Acceleration)&lt;/strong&gt; 2021 [#Zhao2021SUOD]_ Outlier Ensembles INNE Isolation-based Anomaly Detection Using Nearest-Neighbor Ensembles 2018 [#Bandaragoda2018Isolation]_ Combination Average Simple combination by averaging the scores 2015 [#Aggarwal2015Theoretical]_ Combination Weighted Average Simple combination by averaging the scores with detector weights 2015 [#Aggarwal2015Theoretical]_ Combination Maximization Simple combination by taking the maximum scores 2015 [#Aggarwal2015Theoretical]_ Combination AOM Average of Maximum 2015 [#Aggarwal2015Theoretical]_ Combination MOA Maximization of Average 2015 [#Aggarwal2015Theoretical]_ Combination Median Simple combination by taking the median of the scores 2015 [#Aggarwal2015Theoretical]_ Combination majority Vote Simple combination by taking the majority vote of the labels (weights can be used) 2015 [#Aggarwal2015Theoretical]_ =================== ================ ===================================================================================================== ===== ========================================&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(iii) Outlier Detection Score Thresholding Methods&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;================================== ================ ================================================================ ==================================================================================================================== Type Abbr Algorithm Documentation&lt;br&gt; ================================== ================ ================================================================ ==================================================================================================================== Kernel-Based AUCP Area Under Curve Percentage &lt;code&gt;AUCP &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.AUCP&amp;gt;&lt;/code&gt;_ Statistical Moment-Based BOOT Bootstrapping &lt;code&gt;BOOT &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.BOOT&amp;gt;&lt;/code&gt;_ Normality-Based CHAU Chauvenet&#39;s Criterion &lt;code&gt;CHAU &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.CHAU&amp;gt;&lt;/code&gt;_ Linear Model CLF Trained Linear Classifier &lt;code&gt;CLF &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.CLF&amp;gt;&lt;/code&gt;_ cluster-Based CLUST Clustering Based &lt;code&gt;CLUST &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.CLUST&amp;gt;&lt;/code&gt;_ Kernel-Based CPD Change Point Detection &lt;code&gt;CPD &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.CPD&amp;gt;&lt;/code&gt;_ Transformation-Based DECOMP Decomposition &lt;code&gt;DECOMP &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.DECOMP&amp;gt;&lt;/code&gt;_ Normality-Based DSN Distance Shift from Normal &lt;code&gt;DSN &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.DSN&amp;gt;&lt;/code&gt;_ Curve-Based EB Elliptical Boundary &lt;code&gt;EB &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.EB&amp;gt;&lt;/code&gt;_ Kernel-Based FGD Fixed Gradient Descent &lt;code&gt;FGD &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.FGD&amp;gt;&lt;/code&gt;_ Filter-Based FILTER Filtering Based &lt;code&gt;FILTER &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.FILTER&amp;gt;&lt;/code&gt;_ Curve-Based FWFM Full Width at Full Minimum &lt;code&gt;FWFM &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.FWFM&amp;gt;&lt;/code&gt;_ Statistical Test-Based GESD Generalized Extreme Studentized Deviate &lt;code&gt;GESD &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.GESD&amp;gt;&lt;/code&gt;_ Filter-Based HIST Histogram Based &lt;code&gt;HIST &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.HIST&amp;gt;&lt;/code&gt;_ Quantile-Based IQR Inter-Quartile Region &lt;code&gt;IQR &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.IQR&amp;gt;&lt;/code&gt;_ Statistical Moment-Based KARCH Karcher mean (Riemannian Center of Mass) &lt;code&gt;KARCH &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.KARCH&amp;gt;&lt;/code&gt;_ Statistical Moment-Based MAD Median Absolute Deviation &lt;code&gt;MAD &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.MAD&amp;gt;&lt;/code&gt;_ Statistical Test-Based MCST Monte Carlo Shapiro Tests &lt;code&gt;MCST &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.MCST&amp;gt;&lt;/code&gt;_ Ensembles-Based META Meta-model Trained Classifier &lt;code&gt;META &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.META&amp;gt;&lt;/code&gt;_ Transformation-Based MOLL Friedrichs&#39; Mollifier &lt;code&gt;MOLL &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.MOLL&amp;gt;&lt;/code&gt;_ Statistical Test-Based MTT Modified Thompson Tau Test &lt;code&gt;MTT &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.MTT&amp;gt;&lt;/code&gt;_ Linear Model OCSVM One-Class Support Vector Machine &lt;code&gt;OCSVM &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.OCSVM&amp;gt;&lt;/code&gt;_ Quantile-Based QMCD Quasi-Monte Carlo Discrepancy &lt;code&gt;QMCD &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.QMCD&amp;gt;&lt;/code&gt;_ Linear Model REGR Regression Based &lt;code&gt;REGR &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.REGR&amp;gt;&lt;/code&gt;_ Neural Networks VAE Variational Autoencoder &lt;code&gt;VAE &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.VAE&amp;gt;&lt;/code&gt;_ Curve-Based WIND Topological Winding Number &lt;code&gt;WIND &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.WIND&amp;gt;&lt;/code&gt;_ Transformation-Based YJ Yeo-Johnson Transformation &lt;code&gt;YJ &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.YJ&amp;gt;&lt;/code&gt;_ Normality-Based ZSCORE Z-score &lt;code&gt;ZSCORE &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.thresholds.ZSCORE&amp;gt;&lt;/code&gt;_ ================================== ================ ================================================================ ====================================================================================================================&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(iV) Utility Functions&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;=================== ====================== ===================================================================================================================================================== ====================================================================================================================================== Type Name Function Documentation =================== ====================== ===================================================================================================================================================== ====================================================================================================================================== Data generate_data Synthesized data generation; normal data is generated by a multivariate Gaussian and outliers are generated by a uniform distribution &lt;code&gt;generate_data &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.data.generate_data&amp;gt;&lt;/code&gt;_ Data generate_data_clusters Synthesized data generation in clusters; more complex data patterns can be created with multiple clusters &lt;code&gt;generate_data_clusters &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.utils.html#pyod.utils.data.generate_data_clusters&amp;gt;&lt;/code&gt;_ Stat wpearsonr Calculate the weighted Pearson correlation of two samples &lt;code&gt;wpearsonr &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.stat_models.wpearsonr&amp;gt;&lt;/code&gt;_ Utility get_label_n Turn raw outlier scores into binary labels by assign 1 to top n outlier scores &lt;code&gt;get_label_n &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.utility.get_label_n&amp;gt;&lt;/code&gt;_ Utility precision_n_scores calculate precision @ rank n &lt;code&gt;precision_n_scores &amp;lt;https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.utility.precision_n_scores&amp;gt;&lt;/code&gt;_ =================== ====================== ===================================================================================================================================================== ======================================================================================================================================&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Quick Start for Outlier Detection ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;PyOD has been well acknowledged by the machine learning community with a few featured posts and tutorials.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analytics Vidhya&lt;/strong&gt;: &lt;code&gt;An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library &amp;lt;https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;KDnuggets&lt;/strong&gt;: &lt;code&gt;Intuitive Visualization of Outlier Detection Methods &amp;lt;https://www.kdnuggets.com/2019/02/outlier-detection-methods-cheat-sheet.html&amp;gt;&lt;/code&gt;&lt;em&gt;, &lt;code&gt;An Overview of Outlier Detection Methods from PyOD &amp;lt;https://www.kdnuggets.com/2019/06/overview-outlier-detection-methods-pyod.html&amp;gt;&lt;/code&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Data Science&lt;/strong&gt;: &lt;code&gt;Anomaly Detection for Dummies &amp;lt;https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Computer Vision News (March 2019)&lt;/strong&gt;: &lt;code&gt;Python Open Source Toolbox for Outlier Detection &amp;lt;https://rsipvision.com/ComputerVisionNews-2019March/18/&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;&#34;examples/knn_example.py&#34; &amp;lt;https://github.com/yzhao062/pyod/blob/master/examples/knn_example.py&amp;gt;&lt;/code&gt;_ demonstrates the basic API of using kNN detector. &lt;strong&gt;It is noted that the API across all other algorithms are consistent/similar&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More detailed instructions for running examples can be found in &lt;code&gt;examples directory &amp;lt;https://github.com/yzhao062/pyod/blob/master/examples&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;#. Initialize a kNN detector, fit the model, and make the prediction.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   from pyod.models.knn import KNN   # kNN detector&#xA;&#xA;   # train kNN detector&#xA;   clf_name = &#39;KNN&#39;&#xA;   clf = KNN()&#xA;   clf.fit(X_train)&#xA;&#xA;   # get the prediction label and outlier scores of the training data&#xA;   y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)&#xA;   y_train_scores = clf.decision_scores_  # raw outlier scores&#xA;&#xA;   # get the prediction on the test data&#xA;   y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)&#xA;   y_test_scores = clf.decision_function(X_test)  # outlier scores&#xA;&#xA;   # it is possible to get the prediction confidence as well&#xA;   y_test_pred, y_test_pred_confidence = clf.predict(X_test, return_confidence=True)  # outlier labels (0 or 1) and confidence in the range of [0,1]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;#. Evaluate the prediction by ROC and Precision @ Rank n (p@n).&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   from pyod.utils.data import evaluate_print&#xA;   &#xA;   # evaluate and print the results&#xA;   print(&#34;\nOn Training Data:&#34;)&#xA;   evaluate_print(clf_name, y_train, y_train_scores)&#xA;   print(&#34;\nOn Test Data:&#34;)&#xA;   evaluate_print(clf_name, y_test, y_test_scores)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;#. See a sample output &amp;amp; visualization.&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   On Training Data:&#xA;   KNN ROC:1.0, precision @ rank n:1.0&#xA;&#xA;   On Test Data:&#xA;   KNN ROC:0.9989, precision @ rank n:0.9&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;.. code-block:: python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,&#xA;       y_test_pred, show_figure=True, save_figure=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visualization (\ &lt;code&gt;knn_figure &amp;lt;https://raw.githubusercontent.com/yzhao062/pyod/master/examples/KNN.png&amp;gt;&lt;/code&gt;_\ ):&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://raw.githubusercontent.com/yzhao062/pyod/master/examples/KNN.png&#34;&gt;https://raw.githubusercontent.com/yzhao062/pyod/master/examples/KNN.png&lt;/a&gt; :target: &lt;a href=&#34;https://raw.githubusercontent.com/yzhao062/pyod/master/examples/KNN.png&#34;&gt;https://raw.githubusercontent.com/yzhao062/pyod/master/examples/KNN.png&lt;/a&gt; :alt: kNN example figure&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Reference ^^^^^^^^^&lt;/p&gt; &#xA;&lt;p&gt;.. [#Aggarwal2015Outlier] Aggarwal, C.C., 2015. Outlier analysis. In Data mining (pp. 237-263). Springer, Cham.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Aggarwal2015Theoretical] Aggarwal, C.C. and Sathe, S., 2015. Theoretical foundations and algorithms for outlier ensembles.\ &lt;em&gt;ACM SIGKDD Explorations Newsletter&lt;/em&gt;\ , 17(1), pp.24-47.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Aggarwal2017Outlier] Aggarwal, C.C. and Sathe, S., 2017. Outlier ensembles: An introduction. Springer.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Almardeny2020A] Almardeny, Y., Boujnah, N. and Cleary, F., 2020. A Novel Outlier Detection Method for Multivariate Data. &lt;em&gt;IEEE Transactions on Knowledge and Data Engineering&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Angiulli2002Fast] Angiulli, F. and Pizzuti, C., 2002, August. Fast outlier detection in high dimensional spaces. In &lt;em&gt;European Conference on Principles of Data Mining and Knowledge Discovery&lt;/em&gt; pp. 15-27.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Arning1996A] Arning, A., Agrawal, R. and Raghavan, P., 1996, August. A Linear Method for Deviation Detection in Large Databases. In &lt;em&gt;KDD&lt;/em&gt; (Vol. 1141, No. 50, pp. 972-981).&lt;/p&gt; &#xA;&lt;p&gt;.. [#Bandaragoda2018Isolation] Bandaragoda, T. R., Ting, K. M., Albrecht, D., Liu, F. T., Zhu, Y., and Wells, J. R., 2018, Isolation-based anomaly detection using nearest-neighbor ensembles. &lt;em&gt;Computational Intelligence&lt;/em&gt;\ , 34(4), pp. 968-998.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Breunig2000LOF] Breunig, M.M., Kriegel, H.P., Ng, R.T. and Sander, J., 2000, May. LOF: identifying density-based local outliers. &lt;em&gt;ACM Sigmod Record&lt;/em&gt;\ , 29(2), pp. 93-104.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Burgess2018Understanding] Burgess, Christopher P., et al. &#34;Understanding disentangling in beta-VAE.&#34; arXiv preprint arXiv:1804.03599 (2018).&lt;/p&gt; &#xA;&lt;p&gt;.. [#Cook1977Detection] Cook, R.D., 1977. Detection of influential observation in linear regression. Technometrics, 19(1), pp.15-18.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Fang2001Wrap] Fang, K.T. and Ma, C.X., 2001. Wrap-around L2-discrepancy of random sampling, Latin hypercube and uniform designs. Journal of complexity, 17(4), pp.608-624.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Goldstein2012Histogram] Goldstein, M. and Dengel, A., 2012. Histogram-based outlier score (hbos): A fast unsupervised anomaly detection algorithm. In &lt;em&gt;KI-2012: Poster and Demo Track&lt;/em&gt;\ , pp.59-63.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Goodge2022Lunar] Goodge, A., Hooi, B., Ng, S.K. and Ng, W.S., 2022, June. Lunar: Unifying local outlier detection methods via graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Gopalan2019PIDForest] Gopalan, P., Sharan, V. and Wieder, U., 2019. PIDForest: Anomaly Detection via Partial Identification. In Advances in Neural Information Processing Systems, pp. 15783-15793.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Han2022ADBench] Han, S., Hu, X., Huang, H., Jiang, M. and Zhao, Y., 2022. ADBench: Anomaly Detection Benchmark. arXiv preprint arXiv:2206.09426.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Hardin2004Outlier] Hardin, J. and Rocke, D.M., 2004. Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator. &lt;em&gt;Computational Statistics &amp;amp; Data Analysis&lt;/em&gt;\ , 44(4), pp.625-638.&lt;/p&gt; &#xA;&lt;p&gt;.. [#He2003Discovering] He, Z., Xu, X. and Deng, S., 2003. Discovering cluster-based local outliers. &lt;em&gt;Pattern Recognition Letters&lt;/em&gt;\ , 24(9-10), pp.1641-1650.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Hoffmann2007Kernel] Hoffmann, H., 2007. Kernel PCA for novelty detection. Pattern recognition, 40(3), pp.863-874.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Iglewicz1993How] Iglewicz, B. and Hoaglin, D.C., 1993. How to detect and handle outliers (Vol. 16). Asq Press.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Janssens2012Stochastic] Janssens, J.H.M., Huszár, F., Postma, E.O. and van den Herik, H.J., 2012. Stochastic outlier selection. Technical report TiCC TR 2012-001, Tilburg University, Tilburg Center for Cognition and Communication, Tilburg, The Netherlands.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Kingma2013Auto] Kingma, D.P. and Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Kriegel2008Angle] Kriegel, H.P. and Zimek, A., 2008, August. Angle-based outlier detection in high-dimensional data. In &lt;em&gt;KDD &#39;08&lt;/em&gt;\ , pp. 444-452. ACM.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Kriegel2009Outlier] Kriegel, H.P., Kröger, P., Schubert, E. and Zimek, A., 2009, April. Outlier detection in axis-parallel subspaces of high dimensional data. In &lt;em&gt;Pacific-Asia Conference on Knowledge Discovery and Data Mining&lt;/em&gt;\ , pp. 831-838. Springer, Berlin, Heidelberg.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Latecki2007Outlier] Latecki, L.J., Lazarevic, A. and Pokrajac, D., 2007, July. Outlier detection with kernel density functions. In International Workshop on Machine Learning and Data Mining in Pattern Recognition (pp. 61-75). Springer, Berlin, Heidelberg.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Lazarevic2005Feature] Lazarevic, A. and Kumar, V., 2005, August. Feature bagging for outlier detection. In &lt;em&gt;KDD &#39;05&lt;/em&gt;. 2005.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Li2019MADGAN] Li, D., Chen, D., Jin, B., Shi, L., Goh, J. and Ng, S.K., 2019, September. MAD-GAN: Multivariate anomaly detection for time series data with generative adversarial networks. In &lt;em&gt;International Conference on Artificial Neural Networks&lt;/em&gt; (pp. 703-716). Springer, Cham.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Li2020COPOD] Li, Z., Zhao, Y., Botta, N., Ionescu, C. and Hu, X. COPOD: Copula-Based Outlier Detection. &lt;em&gt;IEEE International Conference on Data Mining (ICDM)&lt;/em&gt;, 2020.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Li2021ECOD] Li, Z., Zhao, Y., Hu, X., Botta, N., Ionescu, C. and Chen, H. G. ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions. &lt;em&gt;IEEE Transactions on Knowledge and Data Engineering (TKDE)&lt;/em&gt;, 2022.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Liu2008Isolation] Liu, F.T., Ting, K.M. and Zhou, Z.H., 2008, December. Isolation forest. In &lt;em&gt;International Conference on Data Mining&lt;/em&gt;\ , pp. 413-422. IEEE.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Liu2019Generative] Liu, Y., Li, Z., Zhou, C., Jiang, Y., Sun, J., Wang, M. and He, X., 2019. Generative adversarial active learning for unsupervised outlier detection. &lt;em&gt;IEEE Transactions on Knowledge and Data Engineering&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Papadimitriou2003LOCI] Papadimitriou, S., Kitagawa, H., Gibbons, P.B. and Faloutsos, C., 2003, March. LOCI: Fast outlier detection using the local correlation integral. In &lt;em&gt;ICDE &#39;03&lt;/em&gt;, pp. 315-326. IEEE.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Pevny2016Loda] Pevný, T., 2016. Loda: Lightweight on-line detector of anomalies. &lt;em&gt;Machine Learning&lt;/em&gt;, 102(2), pp.275-304.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Perini2020Quantifying] Perini, L., Vercruyssen, V., Davis, J. Quantifying the confidence of anomaly detectors in their example-wise predictions. In &lt;em&gt;Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)&lt;/em&gt;, 2020.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Ramaswamy2000Efficient] Ramaswamy, S., Rastogi, R. and Shim, K., 2000, May. Efficient algorithms for mining outliers from large data sets. &lt;em&gt;ACM Sigmod Record&lt;/em&gt;\ , 29(2), pp. 427-438.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Rousseeuw1999A] Rousseeuw, P.J. and Driessen, K.V., 1999. A fast algorithm for the minimum covariance determinant estimator. &lt;em&gt;Technometrics&lt;/em&gt;\ , 41(3), pp.212-223.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Ruff2018Deep] Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S.A., Binder, A., Müller, E. and Kloft, M., 2018, July. Deep one-class classification. In &lt;em&gt;International conference on machine learning&lt;/em&gt; (pp. 4393-4402). PMLR.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Schlegl2017Unsupervised] Schlegl, T., Seeböck, P., Waldstein, S.M., Schmidt-Erfurth, U. and Langs, G., 2017, June. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging (pp. 146-157). Springer, Cham.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Scholkopf2001Estimating] Scholkopf, B., Platt, J.C., Shawe-Taylor, J., Smola, A.J. and Williamson, R.C., 2001. Estimating the support of a high-dimensional distribution. &lt;em&gt;Neural Computation&lt;/em&gt;, 13(7), pp.1443-1471.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Shyu2003A] Shyu, M.L., Chen, S.C., Sarinnapakorn, K. and Chang, L., 2003. A novel anomaly detection scheme based on principal component classifier. &lt;em&gt;MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Sugiyama2013Rapid] Sugiyama, M. and Borgwardt, K., 2013. Rapid distance-based outlier detection via sampling. Advances in neural information processing systems, 26.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Tang2002Enhancing] Tang, J., Chen, Z., Fu, A.W.C. and Cheung, D.W., 2002, May. Enhancing effectiveness of outlier detections for low density patterns. In &lt;em&gt;Pacific-Asia Conference on Knowledge Discovery and Data Mining&lt;/em&gt;, pp. 535-548. Springer, Berlin, Heidelberg.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Wang2020adVAE] Wang, X., Du, Y., Lin, S., Cui, P., Shen, Y. and Yang, Y., 2019. adVAE: A self-adversarial variational autoencoder with Gaussian anomaly prior knowledge for anomaly detection. &lt;em&gt;Knowledge-Based Systems&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Xu2023Deep] Xu, H., Pang, G., Wang, Y., Wang, Y., 2023. Deep isolation forest for anomaly detection. &lt;em&gt;IEEE Transactions on Knowledge and Data Engineering&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. [#You2017Provable] You, C., Robinson, D.P. and Vidal, R., 2017. Provable self-representation based outlier detection in a union of subspaces. In Proceedings of the IEEE conference on computer vision and pattern recognition.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Zenati2018Adversarially] Zenati, H., Romain, M., Foo, C.S., Lecouat, B. and Chandrasekhar, V., 2018, November. Adversarially learned anomaly detection. In 2018 IEEE International conference on data mining (ICDM) (pp. 727-736). IEEE.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Zhao2018XGBOD] Zhao, Y. and Hryniewicki, M.K. XGBOD: Improving Supervised Outlier Detection with Unsupervised Representation Learning. &lt;em&gt;IEEE International Joint Conference on Neural Networks&lt;/em&gt;\ , 2018.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Zhao2019LSCP] Zhao, Y., Nasrullah, Z., Hryniewicki, M.K. and Li, Z., 2019, May. LSCP: Locally selective combination in parallel outlier ensembles. In &lt;em&gt;Proceedings of the 2019 SIAM International Conference on Data Mining (SDM)&lt;/em&gt;, pp. 585-593. Society for Industrial and Applied Mathematics.&lt;/p&gt; &#xA;&lt;p&gt;.. [#Zhao2021SUOD] Zhao, Y., Hu, X., Cheng, C., Wang, C., Wan, C., Wang, W., Yang, J., Bai, H., Li, Z., Xiao, C., Wang, Y., Qiao, Z., Sun, J. and Akoglu, L. (2021). SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection. &lt;em&gt;Conference on Machine Learning and Systems (MLSys)&lt;/em&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>