<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-17T01:43:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google-deepmind/graphcast</title>
    <updated>2023-11-17T01:43:42Z</updated>
    <id>tag:github.com,2023-11-17:/google-deepmind/graphcast</id>
    <link href="https://github.com/google-deepmind/graphcast" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GraphCast: Learning skillful medium-range global weather forecasting&lt;/h1&gt; &#xA;&lt;p&gt;This package contains example code to run and train &lt;a href=&#34;https://arxiv.org/abs/2212.12794&#34;&gt;GraphCast&lt;/a&gt;. It also provides three pretrained models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GraphCast&lt;/code&gt;, the high-resolution model used in the GraphCast paper (0.25 degree resolution, 37 pressure levels), trained on ERA5 data from 1979 to 2017,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GraphCast_small&lt;/code&gt;, a smaller, low-resolution version of GraphCast (1 degree resolution, 13 pressure levels, and a smaller mesh), trained on ERA5 data from 1979 to 2015, useful to run a model with lower memory and compute constraints,&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;GraphCast_operational&lt;/code&gt;, a high-resolution model (0.25 degree resolution, 13 pressure levels) pre-trained on ERA5 data from 1979 to 2017 and fine-tuned on HRES data from 2016 to 2021. This model can be initialized from HRES data (does not require precipitation inputs).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The model weights, normalization statistics, and example inputs are available on &lt;a href=&#34;https://console.cloud.google.com/storage/browser/dm_graphcast&#34;&gt;Google Cloud Bucket&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Full model training requires downloading the &lt;a href=&#34;https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5&#34;&gt;ERA5&lt;/a&gt; dataset, available from &lt;a href=&#34;https://www.ecmwf.int/&#34;&gt;ECMWF&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Overview of files&lt;/h2&gt; &#xA;&lt;p&gt;The best starting point is to open &lt;code&gt;graphcast_demo.ipynb&lt;/code&gt; in &lt;a href=&#34;https://colab.research.google.com/github/deepmind/graphcast/blob/master/graphcast_demo.ipynb&#34;&gt;Colaboratory&lt;/a&gt;, which gives an example of loading data, generating random weights or load a pre-trained snapshot, generating predictions, computing the loss and computing gradients. The one-step implementation of GraphCast architecture, is provided in &lt;code&gt;graphcast.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Brief description of library files:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;autoregressive.py&lt;/code&gt;: Wrapper used to run (and train) the one-step GraphCast to produce a sequence of predictions by auto-regressively feeding the outputs back as inputs at each step, in JAX a differentiable way.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;casting.py&lt;/code&gt;: Wrapper used around GraphCast to make it work using BFloat16 precision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;checkpoint.py&lt;/code&gt;: Utils to serialize and deserialize trees.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data_utils.py&lt;/code&gt;: Utils for data preprocessing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;deep_typed_graph_net.py&lt;/code&gt;: General purpose deep graph neural network (GNN) that operates on &lt;code&gt;TypedGraph&lt;/code&gt;&#39;s where both inputs and outputs are flat vectors of features for each of the nodes and edges. &lt;code&gt;graphcast.py&lt;/code&gt; uses three of these for the Grid2Mesh GNN, the Multi-mesh GNN and the Mesh2Grid GNN, respectively.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;graphcast.py&lt;/code&gt;: The main GraphCast model architecture for one-step of predictions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;grid_mesh_connectivity.py&lt;/code&gt;: Tools for converting between regular grids on a sphere and triangular meshes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;icosahedral_mesh.py&lt;/code&gt;: Definition of an icosahedral multi-mesh.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;losses.py&lt;/code&gt;: Loss computations, including latitude-weighting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_utils.py&lt;/code&gt;: Utilities to produce flat node and edge vector features from input grid data, and to manipulate the node output vectors back into a multilevel grid data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;normalization.py&lt;/code&gt;: Wrapper for the one-step GraphCast used to normalize inputs according to historical values, and targets according to historical time differences.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;predictor_base.py&lt;/code&gt;: Defines the interface of the predictor, which GraphCast and all of the wrappers implement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rollout.py&lt;/code&gt;: Similar to &lt;code&gt;autoregressive.py&lt;/code&gt; but used only at inference time using a python loop to produce longer, but non-differentiable trajectories.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;typed_graph.py&lt;/code&gt;: Definition of &lt;code&gt;TypedGraph&lt;/code&gt;&#39;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;typed_graph_net.py&lt;/code&gt;: Implementation of simple graph neural network building blocks defined over &lt;code&gt;TypedGraph&lt;/code&gt;&#39;s that can be combined to build deeper models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;xarray_jax.py&lt;/code&gt;: A wrapper to let JAX work with &lt;code&gt;xarray&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;xarray_tree.py&lt;/code&gt;: An implementation of tree.map_structure that works with &lt;code&gt;xarray&lt;/code&gt;s.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Dependencies.&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/deepmind/chex&#34;&gt;Chex&lt;/a&gt;, &lt;a href=&#34;https://github.com/dask/dask&#34;&gt;Dask&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/dm-haiku&#34;&gt;Haiku&lt;/a&gt;, &lt;a href=&#34;https://github.com/google/jax&#34;&gt;JAX&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/jaxline&#34;&gt;JAXline&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/jraph&#34;&gt;Jraph&lt;/a&gt;, &lt;a href=&#34;https://numpy.org/&#34;&gt;Numpy&lt;/a&gt;, &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;Pandas&lt;/a&gt;, &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt;, &lt;a href=&#34;https://scipy.org/&#34;&gt;SciPy&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/tree&#34;&gt;Tree&lt;/a&gt;, &lt;a href=&#34;https://github.com/mikedh/trimesh&#34;&gt;Trimesh&lt;/a&gt; and &lt;a href=&#34;https://github.com/pydata/xarray&#34;&gt;XArray&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;License and attribution&lt;/h3&gt; &#xA;&lt;p&gt;The Colab notebook and the associated code are licensed under the Apache License, Version 2.0. You may obtain a copy of the License at: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The model weights are made available for use under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0). You may obtain a copy of the License at: &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;https://creativecommons.org/licenses/by-nc-sa/4.0/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The weights were trained on ECMWF&#39;s ERA5 and HRES data. The colab includes a few examples of ERA5 and HRES data that can be used as inputs to the models. ECMWF data product are subject to the following terms:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copyright statement: Copyright &#34;Â© 2023 European Centre for Medium-Range Weather Forecasts (ECMWF)&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Source &lt;a href=&#34;http://www.ecmwf.int&#34;&gt;www.ecmwf.int&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Licence Statement: ECMWF data is published under a Creative Commons Attribution 4.0 International (CC BY 4.0). &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;https://creativecommons.org/licenses/by/4.0/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Disclaimer: ECMWF does not accept any liability whatsoever for any error or omission in the data, their availability, or for any loss or damage arising from their use.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;p&gt;Copyright 2023 DeepMind Technologies Limited.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;If you use this work, consider citing our &lt;a href=&#34;https://arxiv.org/abs/2212.12794&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{lam2022graphcast,&#xA;      title={GraphCast: Learning skillful medium-range global weather forecasting},&#xA;      author={Remi Lam and Alvaro Sanchez-Gonzalez and Matthew Willson and Peter Wirnsberger and Meire Fortunato and Alexander Pritzel and Suman Ravuri and Timo Ewalds and Ferran Alet and Zach Eaton-Rosen and Weihua Hu and Alexander Merose and Stephan Hoyer and George Holland and Jacklynn Stott and Oriol Vinyals and Shakir Mohamed and Peter Battaglia},&#xA;      year={2022},&#xA;      eprint={2212.12794},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>cubiq/ComfyUI_IPAdapter_plus</title>
    <updated>2023-11-17T01:43:42Z</updated>
    <id>tag:github.com,2023-11-17:/cubiq/ComfyUI_IPAdapter_plus</id>
    <link href="https://github.com/cubiq/ComfyUI_IPAdapter_plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI IPAdapter plus&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt; reference implementation for &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter/&#34;&gt;IPAdapter&lt;/a&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;IPAdapter implementation that follows the ComfyUI way of doing things. The code is memory efficient, fast, and shouldn&#39;t break with Comfy updates.&lt;/p&gt; &#xA;&lt;h2&gt;Important updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/08&lt;/strong&gt;: Added &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#attention-masking&#34;&gt;attention masking&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/07&lt;/strong&gt;: Added three ways to apply the weight. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#weight-types&#34;&gt;See below&lt;/a&gt; for more info. &lt;strong&gt;This might break things!&lt;/strong&gt; Please let me know if you are having issues. When loading an old workflow try to reload the page a couple of times or delete the &lt;code&gt;IPAdapter Apply&lt;/code&gt; node and insert a new one.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/02&lt;/strong&gt;: Added compatibility with the new models in safetensors format (available on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/10/12&lt;/strong&gt;: Added image weighting in the &lt;code&gt;IPAdapterEncoder&lt;/code&gt; node. This update is somewhat breaking; if you use &lt;code&gt;IPAdapterEncoder&lt;/code&gt; and &lt;code&gt;PrepImageForClipVision&lt;/code&gt; nodes you need to remove them from your workflow, refresh and recreate them. In the examples you&#39;ll find a &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weighted.json&#34;&gt;workflow&lt;/a&gt; for weighted images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/9/29&lt;/strong&gt;: Added save/load of encoded images. Fix minor bugs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(previous updates removed for better readability)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;p&gt;The IPAdapter are very powerful models for image-to-image conditioning. Given a reference image you can do variations augmented by text prompt, controlnets and masks. Think of it as a 1-image lora.&lt;/p&gt; &#xA;&lt;h2&gt;Example workflow&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/ipadapter_workflow.png&#34; alt=&#34;IPAdapter Example workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Video Introduction&lt;/h2&gt; &#xA;&lt;a href=&#34;https://youtu.be/7m9ZZFU3HWo&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/7m9ZZFU3HWo/hqdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ð¤&lt;/span&gt; &lt;a href=&#34;https://youtu.be/7m9ZZFU3HWo&#34;&gt;Basic usage video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ð&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=mJQ62ly7jrg&#34;&gt;Advanced features video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðº&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=vqG1VXKteQg&#34;&gt;Attention Masking&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download or git clone this repository inside &lt;code&gt;ComfyUI/custom_nodes/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The pre-trained models are available on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;, download and place them in the &lt;code&gt;ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/models&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;For SD1.5 you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.bin&#34;&gt;ip-adapter_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/blob/main/models/ip-adapter_sd15_light.bin&#34;&gt;ip-adapter_sd15_light.bin&lt;/a&gt;, use this when text prompt is more important than reference images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus_sd15.bin&#34;&gt;ip-adapter-plus_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus-face_sd15.bin&#34;&gt;ip-adapter-plus-face_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-full-face_sd15.bin&#34;&gt;ip-adapter-full-face_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For SDXL you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl.bin&#34;&gt;ip-adapter_sdxl.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.bin&#34;&gt;ip-adapter_sdxl_vit-h.bin&lt;/a&gt; &lt;strong&gt;This model requires the use of the SD1.5 encoder despite being for SDXL checkpoints&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus_sdxl_vit-h.bin&#34;&gt;ip-adapter-plus_sdxl_vit-h.bin&lt;/a&gt; Same as above, use the SD1.5 encoder&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus-face_sdxl_vit-h.bin&#34;&gt;ip-adapter-plus-face_sdxl_vit-h.bin&lt;/a&gt; As always, use the SD1.5 encoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that now the models are also available in safetensors format, you can find them on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additionally you need the image encoders to be placed in the &lt;code&gt;ComfyUI/models/clip_vision/&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors&#34;&gt;SD 1.5 model&lt;/a&gt; (use this also for all models ending with &lt;strong&gt;_vit-h&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/image_encoder/model.safetensors&#34;&gt;SDXL model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can rename them to something easier to remember or put them into a sub-directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the image encoders are actually &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&#34;&gt;ViT-H&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&#34;&gt;ViT-bigG&lt;/a&gt; (used only for one SDXL model). You probably already have them.&lt;/p&gt; &#xA;&lt;h2&gt;How to&lt;/h2&gt; &#xA;&lt;p&gt;There&#39;s a basic workflow included in this repo and a few examples in the &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/&#34;&gt;examples&lt;/a&gt; directory. Usually it&#39;s a good idea to lower the &lt;code&gt;weight&lt;/code&gt; to at least &lt;code&gt;0.8&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;noise&lt;/code&gt; paramenter is an experimental exploitation of the IPAdapter models. You can set it as low as &lt;code&gt;0.01&lt;/code&gt; for an arguably better result.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;More info about the noise option&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/noise_example.jpg&#34; width=&#34;100%&#34; alt=&#34;canny controlnet&#34;&gt; &#xA; &lt;p&gt;Basically the IPAdapter sends two pictures for the conditioning, one is the reference the other --that you don&#39;t see-- is an empty image that could be considered like a negative conditioning.&lt;/p&gt; &#xA; &lt;p&gt;What I&#39;m doing is to send a very noisy image instead of an empty one. The &lt;code&gt;noise&lt;/code&gt; parameter determines the amount of noise that is added. A value of &lt;code&gt;0.01&lt;/code&gt; adds a lot of noise (more noise == less impact becaue the model doesn&#39;t get it); a value of &lt;code&gt;1.0&lt;/code&gt; removes most of noise so the generated image gets conditioned more.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Preparing the reference image&lt;/h3&gt; &#xA;&lt;p&gt;The reference image needs to be encoded by the CLIP vision model. The encoder resizes the image to 224Ã224 &lt;strong&gt;and crops it to the center!&lt;/strong&gt;. It&#39;s not an IPAdapter thing, it&#39;s how the clip vision works. This means that if you use a portrait or landscape image and the main attention (eg: the face of a character) is not in the middle you&#39;ll likely get undesired results. Use square pictures as reference for more predictable results.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve added a &lt;code&gt;PrepImageForClipVision&lt;/code&gt; node that does all the required operations for you. You just have to select the crop position (top/left/center/etc...) and a sharpening amount if you want.&lt;/p&gt; &#xA;&lt;p&gt;In the image below you can see the difference between prepped and not prepped images.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/prep_images.jpg&#34; width=&#34;100%&#34; alt=&#34;prepped images&#34;&gt; &#xA;&lt;h3&gt;KSampler configuration suggestions&lt;/h3&gt; &#xA;&lt;p&gt;The IPAdapter generally requires a few more &lt;code&gt;steps&lt;/code&gt; than usual, if the result is underwhelming try to add 10+ steps. The model tends to burn the images a little. If needed lower the CFG scale.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;noise&lt;/code&gt; option generally grants better results, experiment with it.&lt;/p&gt; &#xA;&lt;h3&gt;IPAdapter + ControlNet&lt;/h3&gt; &#xA;&lt;p&gt;The model is very effective when paired with a ControlNet. In the example below I experimented with Canny. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_Canny.json&#34;&gt;The workflow&lt;/a&gt; is in the examples directory.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/canny_controlnet.jpg&#34; width=&#34;100%&#34; alt=&#34;canny controlnet&#34;&gt; &#xA;&lt;h3&gt;IPAdapter Face&lt;/h3&gt; &#xA;&lt;p&gt;IPAdapter offers an interesting model for a kind of &#34;face swap&#34; effect. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_face.json&#34;&gt;The workflow is provided&lt;/a&gt;. Set a close up face as reference image and then input your text prompt as always. The generated character should have the face of the reference. It also works with img2img given a high denoise.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/face_swap.jpg&#34; width=&#34;50%&#34; alt=&#34;face swap&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; there&#39;s a new &lt;code&gt;full-face&lt;/code&gt; model available that&#39;s arguably better.&lt;/p&gt; &#xA;&lt;h3&gt;Masking&lt;/h3&gt; &#xA;&lt;p&gt;The most effective way to apply the IPAdapter to a region is by an &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_inpaint.json&#34;&gt;inpainting workflow&lt;/a&gt;. Remeber to use a specific checkpoint for inpainting otherwise it won&#39;t work. Even if you are inpainting a face I find that the &lt;em&gt;IPAdapter-Plus&lt;/em&gt; (not the &lt;em&gt;face&lt;/em&gt; one), works best.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/inpainting.jpg&#34; width=&#34;100%&#34; alt=&#34;inpainting&#34;&gt; &#xA;&lt;h3&gt;Image Batches&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to pass multiple images for the conditioning with the &lt;code&gt;Batch Images&lt;/code&gt; node. An &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_batch_images.json&#34;&gt;example workflow&lt;/a&gt; is provided; in the picture below you can see the result of one and two images conditioning.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/batch_images.jpg&#34; width=&#34;100%&#34; alt=&#34;batcg images&#34;&gt; &#xA;&lt;p&gt;It seems to be effective with 2-3 images, beyond that it tends to &lt;em&gt;blur&lt;/em&gt; the information too much.&lt;/p&gt; &#xA;&lt;h3&gt;Image Weighting&lt;/h3&gt; &#xA;&lt;p&gt;When sending multiple images you can increase/decrease the weight of each image by using the &lt;code&gt;IPAdapterEncoder&lt;/code&gt; node. The workflow (&lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weighted.json&#34;&gt;included in the examples&lt;/a&gt;) looks like this:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/image_weighting.jpg&#34; width=&#34;100%&#34; alt=&#34;image weighting&#34;&gt; &#xA;&lt;p&gt;The node accepts 4 images, but remember that you can send batches of images to each slot.&lt;/p&gt; &#xA;&lt;h3&gt;Weight types&lt;/h3&gt; &#xA;&lt;p&gt;You can choose how the IPAdapter weight is applied to the image embeds. Options are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;original&lt;/strong&gt;: The weight is applied to the aggregated tensors. The weight works predictably for values greater and lower than 1.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;linear&lt;/strong&gt;: The weight is applied to the individual tensors before aggretating them. Compared to &lt;code&gt;original&lt;/code&gt; the influence is weaker when weight is &amp;lt;1 and stronger when &amp;gt;1. &lt;strong&gt;Note:&lt;/strong&gt; at weight &lt;code&gt;1&lt;/code&gt; the two methods are equivalent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;channel penalty&lt;/strong&gt;: This method is a modified version of Lvmin Zhang&#39;s (Fooocus). Results are sometimes sharper. It works very well also when weight is &amp;gt;1. Still experimental, may change in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The image below shows the difference (zoom in).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/weight_types.jpg&#34; width=&#34;100%&#34; alt=&#34;weight types&#34;&gt; &#xA;&lt;p&gt;In the examples directory you can find &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weight_types.json&#34;&gt;a workflow&lt;/a&gt; that lets you easily compare the three methods.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&#39;m not still sure whether all methods will stay. &lt;code&gt;Linear&lt;/code&gt; seems the most sensible but I wanted to keep the &lt;code&gt;original&lt;/code&gt; for backward compatibility. &lt;code&gt;channel penalty&lt;/code&gt; has a weird non-commercial clause but it&#39;s still part of a GNU GPLv3 software (ie: there&#39;s a licensing clash) so I&#39;m trying to understand how to deal with that.&lt;/p&gt; &#xA;&lt;h3&gt;Attention masking&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s possible to add a mask to define the area where the IPAdapter will be applied to. Everything outside the mask will ignore the reference images and will only listen to the text prompt.&lt;/p&gt; &#xA;&lt;p&gt;It is suggested to use a mask of the same size of the final generated image.&lt;/p&gt; &#xA;&lt;p&gt;In the picture below I use two reference images masked one on the left and the other on the right. The image is generated only with IPAdapter and one ksampler (without in/outpainting or area conditioning).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/masking.jpg&#34; width=&#34;512&#34; alt=&#34;masking&#34;&gt; &#xA;&lt;p&gt;In the examples directory you&#39;ll find a couple of masking workflows: &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_mask.json&#34;&gt;simple&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_2_masks.json&#34;&gt;two masks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Error: &#39;CLIPVisionModelOutput&#39; object has no attribute &#39;penultimate_hidden_states&#39;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You are using an old version of ComfyUI. Update and you&#39;ll be fine. &lt;strong&gt;Please note&lt;/strong&gt; that on Windows for a full update you might need to re-download the latest standalone version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;size mismatch for proj_in.weight: copying a param with shape torch.Size([..., ...]) from checkpoint, the shape in current model is torch.Size([..., ...])&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You are using the wrong image encoder+IPAdapter Model+Checkpoint combo. Remember that you need to select the CLIP encoder v1.5 for all v1.5 IPAdapter models AND for all models ending with &lt;code&gt;vit-h&lt;/code&gt; (even if they are for SDXL).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is it true that the input reference image must have the same size of the output image?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;No, that&#39;s a metropolitan legend. Your input and output images can be of any size. Remember that all input images are scaled and cropped to 224x224 anyway.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusers version&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested I&#39;ve also implemented the same features for &lt;a href=&#34;https://github.com/cubiq/Diffusers_IPAdapter&#34;&gt;Huggingface Diffusers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter/&#34;&gt;IPAdapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/laksjdjf/IPAdapter-ComfyUI/&#34;&gt;laksjdjf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/Fooocus/raw/main/fooocus_extras/ip_adapter.py&#34;&gt;fooocus&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;IPAdapter in the wild&lt;/h2&gt; &#xA;&lt;p&gt;Let me know if you spot the IPAdapter in the wild!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For German speakers you can find interesting YouTube tutorials on &lt;a href=&#34;https://www.youtube.com/watch?v=rAWn_0YOBU0&#34;&gt;A Latent Place&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xzGdynQDzsM&#34;&gt;Scott Detweiler&lt;/a&gt; covered this extension.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>luosiallen/latent-consistency-model</title>
    <updated>2023-11-17T01:43:42Z</updated>
    <id>tag:github.com,2023-11-17:/luosiallen/latent-consistency-model</id>
    <link href="https://github.com/luosiallen/latent-consistency-model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Latent Consistency Models&lt;/h1&gt; &#xA;&lt;p&gt;Official Repository of the paper: &lt;a href=&#34;https://arxiv.org/abs/2310.04378&#34;&gt;Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Official Repository of the paper: &lt;a href=&#34;https://arxiv.org/abs/2311.05556&#34;&gt;LCM-LoRA: A Universal Stable-Diffusion Acceleration Module&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Project Page: &lt;a href=&#34;https://latent-consistency-models.github.io&#34;&gt;https://latent-consistency-models.github.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Try our Demos:&lt;/h3&gt; &#xA;&lt;p&gt;ð¤ &lt;strong&gt;Hugging Face Demo&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; ð¥ð¥ð¥&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Replicate Demo&lt;/strong&gt;: &lt;a href=&#34;https://replicate.com/cjwbw/latent-consistency-model&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/latent-consistency-model/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OpenXLab Demo&lt;/strong&gt;: &lt;a href=&#34;https://openxlab.org.cn/apps/detail/Latent-Consistency-Model/Latent-Consistency-Model&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luosiallen/latent-consistency-model/main/lcm_logo.png&#34; width=&#34;4%&#34; alt=&#34;&#34;&gt; &lt;strong&gt;LCM Community&lt;/strong&gt;: Join our LCM discord channels &lt;a href=&#34;https://discord.gg/KM6aeW6CgD&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; for discussions. Coders are welcome to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;Breaking News ð¥ð¥!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(â¤ï¸New) 2023/11/10 &lt;strong&gt;Training Scripts&lt;/strong&gt; are released!! Check &lt;a href=&#34;https://github.com/luosiallen/latent-consistency-model/tree/main/LCM_Training_Script/consistency_distillation&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¤¯New) 2023/11/10 &lt;strong&gt;Training-free acceleration LCM-LoRA&lt;/strong&gt; is born! See our technical report &lt;a href=&#34;https://arxiv.org/abs/2311.05556&#34;&gt;here&lt;/a&gt; and Hugging Face blog &lt;a href=&#34;https://huggingface.co/blog/lcm_lora&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(â¡ï¸New) 2023/11/10 LCM has a major update! We release &lt;strong&gt;3 LCM-LoRA (SD-XL, SSD-1B, SD-V1.5)&lt;/strong&gt;, see &lt;a href=&#34;https://huggingface.co/latent-consistency/lcm-lora-sdxl&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ðNew) 2023/11/10 LCM has a major update! We release &lt;strong&gt;2 Full Param-tuned LCM (SD-XL, SSD-1B)&lt;/strong&gt;, see &lt;a href=&#34;https://huggingface.co/latent-consistency/lcm-sdxl&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(ð¥New) 2023/11/10 We support LCM Inference with C# and ONNX Runtime now! Thanks to &lt;a href=&#34;https://github.com/saddam213&#34;&gt;@saddam213&lt;/a&gt;! Check the link &lt;a href=&#34;https://github.com/saddam213/OnnxStack&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/11/01 &lt;strong&gt;Real-Time Latent Consistency Models&lt;/strong&gt; is out!! Github link &lt;a href=&#34;https://github.com/radames/Real-Time-Latent-Consistency-Model&#34;&gt;here&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/radames&#34;&gt;@radames&lt;/a&gt; for the really cool Huggingfaceð¤ demo &lt;a href=&#34;https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model&#34;&gt;Real-Time Image-to-Image&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model-Text-To-Image&#34;&gt;Real-Time Text-to-Image&lt;/a&gt;. Twitter/X &lt;a href=&#34;https://x.com/radamar/status/1718783886413709542?s=20&#34;&gt;Link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/28 We support &lt;strong&gt;Img2Img&lt;/strong&gt; for LCM! Please refer to &#34;ð¥ Image2Image Demos&#34;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/25 We have official &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/latent_consistency_models&#34;&gt;&lt;strong&gt;LCM Pipeline&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/src/diffusers/schedulers/scheduling_lcm.py&#34;&gt;&lt;strong&gt;LCM Scheduler&lt;/strong&gt;&lt;/a&gt; in ð§¨ Diffusers library now! Check the new &#34;Usage&#34;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/24 Simple &lt;strong&gt;Streamlit UI&lt;/strong&gt; for local use: See the &lt;a href=&#34;https://github.com/akx/lcm_test&#34;&gt;link&lt;/a&gt; Thanks for &lt;a href=&#34;https://github.com/akx&#34;&gt;@akx&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/24 We support &lt;strong&gt;SD-Webui&lt;/strong&gt; and &lt;strong&gt;ComfyUI&lt;/strong&gt; now!! Thanks for &lt;a href=&#34;https://github.com/0xbitches&#34;&gt;@0xbitches&lt;/a&gt;. See the link: &lt;a href=&#34;https://github.com/0xbitches/sd-webui-lcm&#34;&gt;SD-Webui&lt;/a&gt; and &lt;a href=&#34;https://github.com/0xbitches/ComfyUI-LCM&#34;&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/23 Running on &lt;strong&gt;Windows/Linux CPU&lt;/strong&gt; is also supported! Thanks for &lt;a href=&#34;https://github.com/rupeshs&#34;&gt;@rupeshs&lt;/a&gt; See the &lt;a href=&#34;https://github.com/rupeshs/fastsdcpu&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/22 &lt;strong&gt;Google Colab&lt;/strong&gt; is supported now. Thanks for &lt;a href=&#34;https://github.com/camenduru&#34;&gt;@camenduru&lt;/a&gt; See the link: &lt;a href=&#34;https://github.com/camenduru/latent-consistency-model-colab&#34;&gt;Colab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/21 We support &lt;strong&gt;local gradio demo&lt;/strong&gt; now. LCM can run locally!! Please refer to the &#34;&lt;strong&gt;Local gradio Demos&lt;/strong&gt;&#34;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/19 We provide a demo of LCM in ð¤ Hugging Face Space. Try it &lt;a href=&#34;https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/19 We provide the LCM model (Dreamshaper_v7) in ð¤ Hugging Face. Download &lt;a href=&#34;https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(ð¥New) 2023/10/19 LCM is integrated in ð§¨ Diffusers library. Please refer to the &#34;Usage&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ð¥ Image2Image Demos (Image-to-Image):&lt;/h2&gt; &#xA;&lt;p&gt;We support &lt;strong&gt;Img2Img&lt;/strong&gt; now! Try the impressive img2img demos here: &lt;a href=&#34;https://replicate.com/fofr/latent-consistency-model&#34;&gt;Replicate&lt;/a&gt;, &lt;a href=&#34;https://github.com/0xbitches/sd-webui-lcm&#34;&gt;SD-webui&lt;/a&gt;, &lt;a href=&#34;https://github.com/0xbitches/ComfyUI-LCM&#34;&gt;ComfyUI&lt;/a&gt;, &lt;a href=&#34;https://github.com/camenduru/latent-consistency-model-colab/&#34;&gt;Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Local gradio for img2img is on the way!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/luosiallen/latent-consistency-model/main/img2img_demo/taylor.png&#34; , width=&#34;50%&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/luosiallen/latent-consistency-model/main/img2img_demo/elon.png&#34; , width=&#34;49%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ð¥ Local gradio Demos (Text-to-Image):&lt;/h2&gt; &#xA;&lt;p&gt;To run the model locally, you can download the &#34;local_gradio&#34; folder:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Pytorch (CUDA). MacOS system can download the &#34;MPS&#34; version of Pytorch. Please refer to: &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt;. Install &lt;a href=&#34;https://intel.github.io/intel-extension-for-pytorch/xpu/latest/&#34;&gt;Intel Extension for Pytorch&lt;/a&gt; as well if you&#39;re using Intel GPUs.&lt;/li&gt; &#xA; &lt;li&gt;Install the main library:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install diffusers transformers accelerate gradio==3.48.0 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Launch the gradio: (For MacOS users, need to set the device=&#34;mps&#34; in app.py; For Intel GPU users, set &lt;code&gt;device=&#34;xpu&#34;&lt;/code&gt; in app.py)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demos &amp;amp; Models Released&lt;/h2&gt; &#xA;&lt;p&gt;Ours Hugging Face Demo and Model are released ! Latent Consistency Models are supported in ð§¨ &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LCM Model Download&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7&#34;&gt;LCM_Dreamshaper_v7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For Chinese users, download LCM here: (ä¸­æç¨æ·å¯ä»¥å¨æ­¤ä¸è½½LCMæ¨¡å) &lt;a href=&#34;https://openxlab.org.cn/models/detail/Latent-Consistency-Model/LCM_Dreamshaper_v7_4k.safetensors&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/header/openxlab_models.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hugging Face Demo: &lt;a href=&#34;https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Replicate Demo: &lt;a href=&#34;https://replicate.com/cjwbw/latent-consistency-model&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/latent-consistency-model/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenXLab Demo: &lt;a href=&#34;https://openxlab.org.cn/apps/detail/Latent-Consistency-Model/Latent-Consistency-Model&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34; alt=&#34;Open in OpenXLab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tungsten Demo: &lt;a href=&#34;https://tungsten.run/mjpyeon/lcm&#34;&gt;&lt;img src=&#34;https://tungsten.run/mjpyeon/lcm/_badge&#34; alt=&#34;Tungsten&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Novita.AI Demo: &lt;a href=&#34;https://novita.ai/product/lcm-txt2img&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20Novita.AI%20-Demo%20&amp;amp;%20API-blue&#34; alt=&#34;Novita.AI Latent Consistency Playground&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/luosiallen/latent-consistency-model/main/teaser.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;By distilling classifier-free guidance into the model&#39;s input, LCM can generate high-quality images in very short inference time. We compare the inference time at the setting of 768 x 768 resolution, CFG scale w=8, batchsize=4, using a A800 GPU.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/luosiallen/latent-consistency-model/main/speed_fid.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We have official &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/latent_consistency_models&#34;&gt;&lt;strong&gt;LCM Pipeline&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/src/diffusers/schedulers/scheduling_lcm.py&#34;&gt;&lt;strong&gt;LCM Scheduler&lt;/strong&gt;&lt;/a&gt; in ð§¨ Diffusers library now! The older usages will be deprecated.&lt;/p&gt; &#xA;&lt;p&gt;You can try out Latency Consistency Models directly on: &lt;a href=&#34;https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the model yourself, you can leverage the ð§¨ Diffusers library:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the library:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade diffusers  # make sure to use at least diffusers &amp;gt;= 0.22&#xA;pip install transformers accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from diffusers import DiffusionPipeline&#xA;import torch&#xA;&#xA;pipe = DiffusionPipeline.from_pretrained(&#34;SimianLuo/LCM_Dreamshaper_v7&#34;)&#xA;&#xA;# To save GPU memory, torch.float16 can be used, but it may compromise image quality.&#xA;pipe.to(torch_device=&#34;cuda&#34;, torch_dtype=torch.float32)&#xA;&#xA;prompt = &#34;Self-portrait oil painting, a beautiful cyborg with golden hair, 8k&#34;&#xA;&#xA;# Can be set to 1~50 steps. LCM support fast inference even &amp;lt;= 4 steps. Recommend: 1~8 steps.&#xA;num_inference_steps = 4 &#xA;&#xA;images = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=&#34;pil&#34;).images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, please have a look at the official docs: ð &lt;a href=&#34;https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models#latent-consistency-models&#34;&gt;https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models#latent-consistency-models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage (Deprecated)&lt;/h2&gt; &#xA;&lt;p&gt;We have official &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/latent_consistency_models&#34;&gt;&lt;strong&gt;LCM Pipeline&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/src/diffusers/schedulers/scheduling_lcm.py&#34;&gt;&lt;strong&gt;LCM Scheduler&lt;/strong&gt;&lt;/a&gt; in ð§¨ Diffusers library now! The older usages will be deprecated. But you can still use the older usages by adding &lt;code&gt;revision=&#34;fb9c5d1&#34;&lt;/code&gt; from &lt;code&gt;from_pretrained(...)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the model yourself, you can leverage the ð§¨ Diffusers library:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the library:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install diffusers transformers accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the model:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from diffusers import DiffusionPipeline&#xA;import torch&#xA;&#xA;pipe = DiffusionPipeline.from_pretrained(&#34;SimianLuo/LCM_Dreamshaper_v7&#34;, custom_pipeline=&#34;latent_consistency_txt2img&#34;, custom_revision=&#34;main&#34;, revision=&#34;fb9c5d&#34;)&#xA;&#xA;# To save GPU memory, torch.float16 can be used, but it may compromise image quality.&#xA;pipe.to(torch_device=&#34;cuda&#34;, torch_dtype=torch.float32)&#xA;&#xA;prompt = &#34;Self-portrait oil painting, a beautiful cyborg with golden hair, 8k&#34;&#xA;&#xA;# Can be set to 1~50 steps. LCM support fast inference even &amp;lt;= 4 steps. Recommend: 1~8 steps.&#xA;num_inference_steps = 4 &#xA;&#xA;images = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=&#34;pil&#34;).images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Our Contributors :&lt;/h3&gt; &#xA;&lt;a href=&#34;https://github.com/luosiallen/latent-consistency-model/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=luosiallen/latent-consistency-model&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;LCM:&#xA;@misc{luo2023latent,&#xA;      title={Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference}, &#xA;      author={Simian Luo and Yiqin Tan and Longbo Huang and Jian Li and Hang Zhao},&#xA;      year={2023},&#xA;      eprint={2310.04378},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;LCM-LoRA:&#xA;@article{luo2023lcm,&#xA;  title={LCM-LoRA: A Universal Stable-Diffusion Acceleration Module},&#xA;  author={Luo, Simian and Tan, Yiqin and Patil, Suraj and Gu, Daniel and von Platen, Patrick and Passos, Apolin{\&#39;a}rio and Huang, Longbo and Li, Jian and Zhao, Hang},&#xA;  journal={arXiv preprint arXiv:2311.05556},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>