<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-06T01:34:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sunnypilot/sunnypilot</title>
    <updated>2025-01-06T01:34:24Z</updated>
    <id>tag:github.com,2025-01-06:/sunnypilot/sunnypilot</id>
    <link href="https://github.com/sunnypilot/sunnypilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;sunnypilot is an open source driver assistance system. sunnypilot offers the user a unique driving experience for over 290 supported car makes and models with modified behaviors of driving assist engagements. sunnypilot complies with comma.ai&#39;s safety rules as accurately as possible.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; style=&#34;text-align: center;&#34;&gt; &#xA; &lt;h1&gt;openpilot&lt;/h1&gt; &#xA; &lt;p&gt; &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt; &lt;br&gt; Currently, it upgrades the driver assistance system in 275+ supported cars. &lt;/p&gt; &#xA; &lt;h3&gt; &lt;a href=&#34;https://docs.comma.ai&#34;&gt;Docs&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href=&#34;https://docs.comma.ai/contributing/roadmap/&#34;&gt;Roadmap&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href=&#34;https://github.com/commaai/openpilot/raw/master/docs/CONTRIBUTING.md&#34;&gt;Contribute&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href=&#34;https://discord.comma.ai&#34;&gt;Community&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href=&#34;https://comma.ai/shop&#34;&gt;Try it on a comma 3X&lt;/a&gt; &lt;/h3&gt; &#xA; &lt;p&gt;Quick start: &lt;code&gt;bash &amp;lt;(curl -fsSL openpilot.comma.ai)&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg?sanitize=true&#34; alt=&#34;openpilot tests&#34;&gt; &lt;a href=&#34;https://codecov.io/gh/commaai/openpilot&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/commaai/openpilot/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/sunnypilot/sunnypilot/master-new/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/comma_ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/comma_ai&#34; alt=&#34;X Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.comma.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/469524606043160576&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/NmBfgOanCyk&#34; title=&#34;Video By Greer Viau&#34;&gt;&lt;img src=&#34;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/VHKyqZ7t8Gw&#34; title=&#34;Video By Logan LeGrand&#34;&gt;&lt;img src=&#34;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://youtu.be/SUIZYzxtMQs&#34; title=&#34;A drive to Taco Bell&#34;&gt;&lt;img src=&#34;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Using openpilot in a car&lt;/h2&gt; &#xA;&lt;p&gt;To use openpilot in a car, you need four things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supported Device:&lt;/strong&gt; a comma 3/3X, available at &lt;a href=&#34;https://comma.ai/shop/comma-3x&#34;&gt;comma.ai/shop&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL &lt;code&gt;openpilot.comma.ai&lt;/code&gt; to install the release version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supported Car:&lt;/strong&gt; Ensure that you have one of &lt;a href=&#34;https://raw.githubusercontent.com/sunnypilot/sunnypilot/master-new/docs/CARS.md&#34;&gt;the 275+ supported cars&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Car Harness:&lt;/strong&gt; You will also need a &lt;a href=&#34;https://comma.ai/shop/car-harness&#34;&gt;car harness&lt;/a&gt; to connect your comma 3/3X to your car.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We have detailed instructions for &lt;a href=&#34;https://comma.ai/setup&#34;&gt;how to install the harness and device in a car&lt;/a&gt;. Note that it&#39;s possible to run openpilot on &lt;a href=&#34;https://blog.comma.ai/self-driving-car-for-free/&#34;&gt;other hardware&lt;/a&gt;, although it&#39;s not plug-and-play.&lt;/p&gt; &#xA;&lt;h3&gt;Branches&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;branch&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;release3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;openpilot.comma.ai&lt;/td&gt; &#xA;   &lt;td&gt;This is openpilot&#39;s release branch.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;release3-staging&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;openpilot-test.comma.ai&lt;/td&gt; &#xA;   &lt;td&gt;This is the staging branch for releases. Use it to get new releases slightly early.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;openpilot-nightly.comma.ai&lt;/td&gt; &#xA;   &lt;td&gt;This is the bleeding edge development branch. Do not expect this to be stable.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;nightly-dev&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;installer.comma.ai/commaai/nightly-dev&lt;/td&gt; &#xA;   &lt;td&gt;Same as nightly, but includes experimental development features for some cars.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;To start developing openpilot&lt;/h2&gt; &#xA;&lt;p&gt;openpilot is developed by &lt;a href=&#34;https://comma.ai/&#34;&gt;comma&lt;/a&gt; and by users like you. We welcome both pull requests and issues on &lt;a href=&#34;http://github.com/commaai/openpilot&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://discord.comma.ai&#34;&gt;community Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/sunnypilot/sunnypilot/master-new/docs/CONTRIBUTING.md&#34;&gt;the contributing docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/sunnypilot/sunnypilot/master-new/tools/&#34;&gt;openpilot tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read about the &lt;a href=&#34;https://raw.githubusercontent.com/sunnypilot/sunnypilot/master-new/docs/WORKFLOW.md&#34;&gt;development workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code documentation lives at &lt;a href=&#34;https://docs.comma.ai&#34;&gt;https://docs.comma.ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Information about running openpilot lives on the &lt;a href=&#34;https://github.com/commaai/openpilot/wiki&#34;&gt;community wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want to get paid to work on openpilot? &lt;a href=&#34;https://comma.ai/jobs#open-positions&#34;&gt;comma is hiring&lt;/a&gt; and offers lots of &lt;a href=&#34;https://comma.ai/bounties&#34;&gt;bounties&lt;/a&gt; for external contributors.&lt;/p&gt; &#xA;&lt;h2&gt;Safety and Testing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;openpilot observes &lt;a href=&#34;https://en.wikipedia.org/wiki/ISO_26262&#34;&gt;ISO26262&lt;/a&gt; guidelines, see &lt;a href=&#34;https://raw.githubusercontent.com/sunnypilot/sunnypilot/master-new/docs/SAFETY.md&#34;&gt;SAFETY.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;openpilot has software-in-the-loop &lt;a href=&#34;https://raw.githubusercontent.com/sunnypilot/sunnypilot/master-new/.github/workflows/selfdrive_tests.yaml&#34;&gt;tests&lt;/a&gt; that run on every commit.&lt;/li&gt; &#xA; &lt;li&gt;The code enforcing the safety model lives in panda and is written in C, see &lt;a href=&#34;https://github.com/commaai/panda#code-rigor&#34;&gt;code rigor&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;panda has software-in-the-loop &lt;a href=&#34;https://github.com/commaai/panda/tree/master/tests/safety&#34;&gt;safety tests&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.&lt;/li&gt; &#xA; &lt;li&gt;panda has additional hardware-in-the-loop &lt;a href=&#34;https://github.com/commaai/panda/raw/master/Jenkinsfile&#34;&gt;tests&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.&lt;/p&gt; &#xA;&lt;p&gt;Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT. YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS. NO WARRANTY EXPRESSED OR IMPLIED.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;User Data and comma Account&lt;/h2&gt; &#xA;&lt;p&gt;By default, openpilot uploads the driving data to our servers. You can also access your data through &lt;a href=&#34;https://connect.comma.ai/&#34;&gt;comma connect&lt;/a&gt;. We use your data to train better models and improve openpilot for everyone.&lt;/p&gt; &#xA;&lt;p&gt;openpilot is open source software: the user is free to disable data collection if they wish to do so.&lt;/p&gt; &#xA;&lt;p&gt;openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs. The driver-facing camera is only logged if you explicitly opt-in in settings. The microphone is not recorded.&lt;/p&gt; &#xA;&lt;p&gt;By using openpilot, you agree to &lt;a href=&#34;https://comma.ai/privacy&#34;&gt;our Privacy Policy&lt;/a&gt;. You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/nv-ingest</title>
    <updated>2025-01-06T01:34:24Z</updated>
    <id>tag:github.com,2025-01-06:/NVIDIA/nv-ingest</id>
    <link href="https://github.com/NVIDIA/nv-ingest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA Ingest is an early access set of microservices for parsing hundreds of thousands of complex, messy unstructured PDFs and other enterprise documents into metadata and text to embed into retrieval systems.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;NVIDIA-Ingest: Multi-modal data extraction&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA-Ingest is a scalable, performance-oriented document content and metadata extraction microservice. Including support for parsing PDFs, Word and PowerPoint documents, it uses specialized NVIDIA NIM microservices to find, contextualize, and extract text, tables, charts and images for use in downstream generative applications.&lt;/p&gt; &#xA;&lt;p&gt;NVIDIA Ingest enables parallelization of the process of splitting documents into pages where contents are classified (as tables, charts, images, text), extracted into discrete content, and further contextualized via optical character recognition (OCR) into a well defined JSON schema. From there, NVIDIA Ingest can optionally manage computation of embeddings for the extracted content, and also optionally manage storing into a vector database &lt;a href=&#34;https://milvus.io/&#34;&gt;Milvus&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#repo-structure&#34;&gt;Repo Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#notices&#34;&gt;Notices&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;h3&gt;What NVIDIA-Ingest is ‚úîÔ∏è&lt;/h3&gt; &#xA;&lt;p&gt;A microservice that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accepts a JSON Job description, containing a document payload, and a set of ingestion tasks to perform on that payload.&lt;/li&gt; &#xA; &lt;li&gt;Allows the results of a Job to be retrieved; the result is a JSON dictionary containing a list of Metadata describing objects extracted from the base document, as well as processing annotations and timing/trace data.&lt;/li&gt; &#xA; &lt;li&gt;Supports PDF, Docx, pptx, and images.&lt;/li&gt; &#xA; &lt;li&gt;Supports multiple methods of extraction for each document type in order to balance trade-offs between throughput and accuracy. For example, for PDF documents we support extraction via pdfium, Unstructured.io, and Adobe Content Extraction Services.&lt;/li&gt; &#xA; &lt;li&gt;Supports various types of pre and post processing operations, including text splitting and chunking; transform, and filtering; embedding generation, and image offloading to storage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What NVIDIA-Ingest is not ‚úñÔ∏è&lt;/h3&gt; &#xA;&lt;p&gt;A service that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Runs a static pipeline or fixed set of operations on every submitted document.&lt;/li&gt; &#xA; &lt;li&gt;Acts as a wrapper for any specific document parsing library.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;h3&gt;Hardware&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Family&lt;/th&gt; &#xA;   &lt;th&gt;Memory&lt;/th&gt; &#xA;   &lt;th&gt;# of GPUs (min.)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;H100&lt;/td&gt; &#xA;   &lt;td&gt;SXM or PCIe&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A100&lt;/td&gt; &#xA;   &lt;td&gt;SXM or PCIe&lt;/td&gt; &#xA;   &lt;td&gt;80GB&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Software&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux operating systems (Ubuntu 22.04 or later recommended)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA Toolkit&lt;/a&gt; (NVIDIA Driver &amp;gt;= &lt;code&gt;535&lt;/code&gt;, CUDA &amp;gt;= &lt;code&gt;12.2&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To get started using NVIDIA Ingest, you need to do a few things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#step-1-starting-containers&#34;&gt;Start supporting NIM microservices&lt;/a&gt; üèóÔ∏è&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#step-2-installing-python-dependencies&#34;&gt;Install the NVIDIA Ingest client dependencies in a Python environment&lt;/a&gt; üêç&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#step-3-ingesting-documents&#34;&gt;Submit ingestion job(s)&lt;/a&gt; üìì&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/#step-4-inspecting-and-consuming-results&#34;&gt;Inspect and consume results&lt;/a&gt; üîç&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Optional:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docs/deployment.md&#34;&gt;Direct Library Deployment&lt;/a&gt; üì¶&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 1: Starting containers&lt;/h3&gt; &#xA;&lt;p&gt;This example demonstrates how to use the provided &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docker-compose.yaml&#34;&gt;docker-compose.yaml&lt;/a&gt; to start all needed services with a few commands.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] NIM containers on their first startup can take 10-15 minutes to pull and fully load models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If preferred, you can also &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docs/deployment.md&#34;&gt;start services one by one&lt;/a&gt;, or run on Kubernetes via &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/helm/README.md&#34;&gt;our Helm chart&lt;/a&gt;. Also of note are &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docs/environment-config.md&#34;&gt;additional environment variables&lt;/a&gt; you may wish to configure.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Git clone the repo: &lt;code&gt;git clone https://github.com/nvidia/nv-ingest&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Change directory to the cloned repo &lt;code&gt;cd nv-ingest&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docs/ngc-api-key.md&#34;&gt;Generate API keys&lt;/a&gt; and authenticate with NGC with the &lt;code&gt;docker login&lt;/code&gt; command:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# This is required to access pre-built containers and NIM microservices&#xA;$ docker login nvcr.io&#xA;Username: $oauthtoken&#xA;Password: &amp;lt;Your Key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] during the early access (EA) phase, your API key must be created as a member of &lt;code&gt;nemo-microservice / ea-participants&lt;/code&gt; which you may join by applying for early access here: &lt;a href=&#34;https://developer.nvidia.com/nemo-microservices-early-access/join&#34;&gt;https://developer.nvidia.com/nemo-microservices-early-access/join&lt;/a&gt;. When approved, switch your profile to this org / team, then the key you generate will have access to the resources outlined below.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Create a .env file containing your NGC API key, and the following paths:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Container images must access resources from NGC.&#xA;NGC_API_KEY=... # Optional, set this if you are deploying NIMs locally from NGC&#xA;NVIDIA_BUILD_API_KEY=... # Optional, set this is you are using build.nvidia.com NIMs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] As configured by default in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docker-compose.yaml#L52&#34;&gt;docker-compose.yaml&lt;/a&gt;, the DePlot NIM is on a dedicated GPU. All other NIMs and the nv-ingest container itself share a second. This is to avoid DePlot and other NIMs competing for VRAM on the same device.&lt;/p&gt; &#xA; &lt;p&gt;Change the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; pinnings as desired for your system within docker-compose.yaml.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Make sure NVIDIA is set as your default container runtime before running the docker compose command with the command: &lt;code&gt;sudo nvidia-ctk runtime configure --runtime=docker --set-as-default&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Start all services: &lt;code&gt;docker compose up&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] By default we have &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docker-compose.yaml#L27&#34;&gt;configured log levels to be verbose&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;It&#39;s possible to observe service startup proceeding: you will notice &lt;em&gt;many&lt;/em&gt; log messages. Disable verbose logging by configuring &lt;code&gt;NIM_TRITON_LOG_VERBOSE=0&lt;/code&gt; for each NIM in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docker-compose.yaml&#34;&gt;docker-compose.yaml&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;If you want to build from source, use &lt;code&gt;docker compose up --build&lt;/code&gt; instead. This will build from your repo&#39;s code rather than from an already published container.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;When all services have fully started, &lt;code&gt;nvidia-smi&lt;/code&gt; should show processes like the following:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# If it&#39;s taking &amp;gt; 1m for `nvidia-smi` to return, it&#39;s likely the bus is still busy setting up the models.&#xA;+---------------------------------------------------------------------------------------+&#xA;| Processes:                                                                            |&#xA;|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |&#xA;|        ID   ID                                                             Usage      |&#xA;|=======================================================================================|&#xA;|    0   N/A  N/A   1352957      C   tritonserver                                762MiB |&#xA;|    1   N/A  N/A   1322081      C   /opt/nim/llm/.venv/bin/python3            63916MiB |&#xA;|    2   N/A  N/A   1355175      C   tritonserver                                478MiB |&#xA;|    2   N/A  N/A   1367569      C   ...s/python/triton_python_backend_stub       12MiB |&#xA;|    3   N/A  N/A   1321841      C   python                                      414MiB |&#xA;|    3   N/A  N/A   1352331      C   tritonserver                                478MiB |&#xA;|    3   N/A  N/A   1355929      C   ...s/python/triton_python_backend_stub      424MiB |&#xA;|    3   N/A  N/A   1373202      C   tritonserver                                414MiB |&#xA;+---------------------------------------------------------------------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Observe the started containers with &lt;code&gt;docker ps&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CONTAINER ID   IMAGE                                                                      COMMAND                  CREATED          STATUS                    PORTS                                                                                                                                                                                                                                                                                NAMES&#xA;0f2f86615ea5   nvcr.io/ohlfw0olaadg/ea-participants/nv-ingest:24.10                       &#34;/opt/conda/bin/tini‚Ä¶&#34;   35 seconds ago   Up 33 seconds             0.0.0.0:7670-&amp;gt;7670/tcp, :::7670-&amp;gt;7670/tcp                                                                                                                                                                                                                                            nv-ingest-nv-ingest-ms-runtime-1&#xA;de44122c6ddc   otel/opentelemetry-collector-contrib:0.91.0                                &#34;/otelcol-contrib --‚Ä¶&#34;   14 hours ago     Up 24 seconds             0.0.0.0:4317-4318-&amp;gt;4317-4318/tcp, :::4317-4318-&amp;gt;4317-4318/tcp, 0.0.0.0:8888-8889-&amp;gt;8888-8889/tcp, :::8888-8889-&amp;gt;8888-8889/tcp, 0.0.0.0:13133-&amp;gt;13133/tcp, :::13133-&amp;gt;13133/tcp, 55678/tcp, 0.0.0.0:32849-&amp;gt;9411/tcp, :::32848-&amp;gt;9411/tcp, 0.0.0.0:55680-&amp;gt;55679/tcp, :::55680-&amp;gt;55679/tcp   nv-ingest-otel-collector-1&#xA;02c9ab8c6901   nvcr.io/ohlfw0olaadg/ea-participants/cached:0.2.0                          &#34;/opt/nvidia/nvidia_‚Ä¶&#34;   14 hours ago     Up 24 seconds             0.0.0.0:8006-&amp;gt;8000/tcp, :::8006-&amp;gt;8000/tcp, 0.0.0.0:8007-&amp;gt;8001/tcp, :::8007-&amp;gt;8001/tcp, 0.0.0.0:8008-&amp;gt;8002/tcp, :::8008-&amp;gt;8002/tcp                                                                                                                                                      nv-ingest-cached-1&#xA;d49369334398   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.1.0                                  &#34;/opt/nvidia/nvidia_‚Ä¶&#34;   14 hours ago     Up 33 seconds             0.0.0.0:8012-&amp;gt;8000/tcp, :::8012-&amp;gt;8000/tcp, 0.0.0.0:8013-&amp;gt;8001/tcp, :::8013-&amp;gt;8001/tcp, 0.0.0.0:8014-&amp;gt;8002/tcp, :::8014-&amp;gt;8002/tcp                                                                                                                                                      nv-ingest-embedding-1&#xA;508715a24998   nvcr.io/ohlfw0olaadg/ea-participants/nv-yolox-structured-images-v1:0.2.0   &#34;/opt/nvidia/nvidia_‚Ä¶&#34;   14 hours ago     Up 33 seconds             0.0.0.0:8000-8002-&amp;gt;8000-8002/tcp, :::8000-8002-&amp;gt;8000-8002/tcp                                                                                                                                                                                                                        nv-ingest-yolox-1&#xA;5b7a174a0a85   nvcr.io/ohlfw0olaadg/ea-participants/deplot:1.0.0                          &#34;/opt/nvidia/nvidia_‚Ä¶&#34;   14 hours ago     Up 33 seconds             0.0.0.0:8003-&amp;gt;8000/tcp, :::8003-&amp;gt;8000/tcp, 0.0.0.0:8004-&amp;gt;8001/tcp, :::8004-&amp;gt;8001/tcp, 0.0.0.0:8005-&amp;gt;8002/tcp, :::8005-&amp;gt;8002/tcp                                                                                                                                                      nv-ingest-deplot-1&#xA;430045f98c02   nvcr.io/ohlfw0olaadg/ea-participants/paddleocr:0.2.0                       &#34;/opt/nvidia/nvidia_‚Ä¶&#34;   14 hours ago     Up 24 seconds             0.0.0.0:8009-&amp;gt;8000/tcp, :::8009-&amp;gt;8000/tcp, 0.0.0.0:8010-&amp;gt;8001/tcp, :::8010-&amp;gt;8001/tcp, 0.0.0.0:8011-&amp;gt;8002/tcp, :::8011-&amp;gt;8002/tcp                                                                                                                                                      nv-ingest-paddle-1&#xA;8e587b45821b   grafana/grafana                                                            &#34;/run.sh&#34;                14 hours ago     Up 33 seconds             0.0.0.0:3000-&amp;gt;3000/tcp, :::3000-&amp;gt;3000/tcp                                                                                                                                                                                                                                            grafana-service&#xA;aa2c0ec387e2   redis/redis-stack                                                          &#34;/entrypoint.sh&#34;         14 hours ago     Up 33 seconds             0.0.0.0:6379-&amp;gt;6379/tcp, :::6379-&amp;gt;6379/tcp, 8001/tcp                                                                                                                                                                                                                                  nv-ingest-redis-1&#xA;bda9a2a9c8b5   openzipkin/zipkin                                                          &#34;start-zipkin&#34;           14 hours ago     Up 33 seconds (healthy)   9410/tcp, 0.0.0.0:9411-&amp;gt;9411/tcp, :::9411-&amp;gt;9411/tcp                                                                                                                                                                                                                                  nv-ingest-zipkin-1&#xA;ac27e5297d57   prom/prometheus:latest                                                     &#34;/bin/prometheus --w‚Ä¶&#34;   14 hours ago     Up 33 seconds             0.0.0.0:9090-&amp;gt;9090/tcp, :::9090-&amp;gt;9090/tcp                                                                                                                                                                                                                                            nv-ingest-prometheus-1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] nv-ingest is in Early Access mode, meaning the codebase gets frequent updates. To build an updated nv-ingest service container with the latest changes you can:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;After the image is built, run &lt;code&gt;docker compose up&lt;/code&gt; per item 5 above.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Step 2: Installing Python dependencies&lt;/h3&gt; &#xA;&lt;p&gt;To interact with the nv-ingest service, you can do so from the host, or by &lt;code&gt;docker exec&lt;/code&gt;-ing into the nv-ingest container.&lt;/p&gt; &#xA;&lt;p&gt;To interact from the host, you&#39;ll need a Python environment and install the client dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# conda not required, but makes it easy to create a fresh python environment&#xA;conda create --name nv-ingest-dev --file ./conda/environments/nv_ingest_environment.yml&#xA;conda activate nv-ingest-dev&#xA;&#xA;cd client&#xA;pip install .&#xA;&#xA;# When not using Conda, pip dependencies for the client can be installed directly via pip. Pip based installation of&#xA;# the ingest service is not supported.&#xA;cd client&#xA;pip install -r requirements.txt&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Interacting from the host depends on the appropriate port being exposed from the nv-ingest container to the host as defined in &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docker-compose.yaml#L141&#34;&gt;docker-compose.yaml&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;If you prefer, you can disable exposing that port, and interact with the nv-ingest service directly from within its container.&lt;/p&gt; &#xA; &lt;p&gt;To interact within the container:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You&#39;ll be in the &lt;code&gt;/workspace&lt;/code&gt; directory, which has &lt;code&gt;DATASET_ROOT&lt;/code&gt; from the .env file mounted at &lt;code&gt;./data&lt;/code&gt;. The pre-activated &lt;code&gt;morpheus&lt;/code&gt; conda environment has all the python client libraries pre-installed:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;(morpheus) root@aba77e2a4bde:/workspace#&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;From the bash prompt above, you can run nv-ingest-cli and Python examples described below.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Step 3: Ingesting Documents&lt;/h3&gt; &#xA;&lt;p&gt;You can submit jobs programmatically in Python or via the nv-ingest-cli tool.&lt;/p&gt; &#xA;&lt;p&gt;In the below examples, we are doing text, chart, table, and image extraction:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;extract_text&lt;/code&gt;, - uses &lt;a href=&#34;https://github.com/pypdfium2-team/pypdfium2/&#34;&gt;PDFium&lt;/a&gt; to find and extract text from pages&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;extract_images&lt;/code&gt; - uses &lt;a href=&#34;https://github.com/pypdfium2-team/pypdfium2/&#34;&gt;PDFium&lt;/a&gt; to extract images&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;extract_tables&lt;/code&gt; - uses &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34;&gt;YOLOX&lt;/a&gt; to find tables and charts. Uses &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddleOCR&lt;/a&gt; for table extraction, and &lt;a href=&#34;https://huggingface.co/google/deplot&#34;&gt;Deplot&lt;/a&gt; and CACHED for chart extraction&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;extract_charts&lt;/code&gt; - (optional) enables or disables the use of Deplot and CACHED for chart extraction.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;code&gt;extract_tables&lt;/code&gt; controls extraction for both tables and charts. You can optionally disable chart extraction by setting &lt;code&gt;extract_charts&lt;/code&gt; to false.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;In Python (you can find more documentation and examples &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/client/client_examples/examples/python_client_usage.ipynb&#34;&gt;here&lt;/a&gt;):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging, time&#xA;&#xA;from nv_ingest_client.client import NvIngestClient&#xA;from nv_ingest_client.primitives import JobSpec&#xA;from nv_ingest_client.primitives.tasks import ExtractTask&#xA;from nv_ingest_client.util.file_processing.extract import extract_file_content&#xA;&#xA;logger = logging.getLogger(&#34;nv_ingest_client&#34;)&#xA;&#xA;file_name = &#34;data/multimodal_test.pdf&#34;&#xA;file_content, file_type = extract_file_content(file_name)&#xA;&#xA;# A JobSpec is an object that defines a document and how it should&#xA;# be processed by the nv-ingest service.&#xA;job_spec = JobSpec(&#xA;  document_type=file_type,&#xA;  payload=file_content,&#xA;  source_id=file_name,&#xA;  source_name=file_name,&#xA;  extended_options=&#xA;    {&#xA;      &#34;tracing_options&#34;:&#xA;      {&#xA;        &#34;trace&#34;: True,&#xA;        &#34;ts_send&#34;: time.time_ns()&#xA;      }&#xA;    }&#xA;)&#xA;&#xA;# configure desired extraction modes here. Multiple extraction&#xA;# methods can be defined for a single JobSpec&#xA;extract_task = ExtractTask(&#xA;  document_type=file_type,&#xA;  extract_text=True,&#xA;  extract_images=True,&#xA;  extract_tables=True&#xA;)&#xA;&#xA;job_spec.add_task(extract_task)&#xA;&#xA;# Create the client and inform it about the JobSpec we want to process.&#xA;client = NvIngestClient(&#xA;  message_client_hostname=&#34;localhost&#34;, # Host where nv-ingest-ms-runtime is running&#xA;  message_client_port=7670 # REST port, defaults to 7670&#xA;)&#xA;job_id = client.add_job(job_spec)&#xA;client.submit_job(job_id, &#34;morpheus_task_queue&#34;)&#xA;result = client.fetch_job_result(job_id, timeout=60)&#xA;print(f&#34;Got {len(result)} results&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Using the the &lt;code&gt;nv-ingest-cli&lt;/code&gt; (you can find more nv-ingest-cli examples &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/client/client_examples/examples/cli_client_usage.ipynb&#34;&gt;here&lt;/a&gt;):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;nv-ingest-cli \&#xA;  --doc ./data/multimodal_test.pdf \&#xA;  --output_directory ./processed_docs \&#xA;  --task=&#39;extract:{&#34;document_type&#34;: &#34;pdf&#34;, &#34;extract_method&#34;: &#34;pdfium&#34;, &#34;extract_tables&#34;: &#34;true&#34;, &#34;extract_images&#34;: &#34;true&#34;}&#39; \&#xA;  --client_host=localhost \&#xA;  --client_port=7670&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should notice output indicating document processing status, followed by a breakdown of time spent during job execution:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;INFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.&#xA;INFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs&#xA;Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:10&amp;lt;00:00, 10.47s/file, pages_per_sec=0.29]&#xA;INFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 1.02 ms, Median: 1.02 ms, Total Time: 1.02 ms, Total % of Trace Computation: 0.01%&#xA;INFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 1.44 ms, Median: 1.44 ms, Total Time: 1.44 ms, Total % of Trace Computation: 0.01%&#xA;INFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.66 ms, Median: 0.66 ms, Total Time: 0.66 ms, Total % of Trace Computation: 0.01%&#xA;INFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 1.09 ms, Median: 1.09 ms, Total Time: 1.09 ms, Total % of Trace Computation: 0.01%&#xA;INFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.84 ms, Median: 0.84 ms, Total Time: 0.84 ms, Total % of Trace Computation: 0.01%&#xA;INFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 7.75 ms, Median: 7.75 ms, Total Time: 7.75 ms, Total % of Trace Computation: 0.07%&#xA;INFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 2.13 ms, Median: 2.13 ms, Total Time: 2.13 ms, Total % of Trace Computation: 0.02%&#xA;INFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 2.05 ms, Median: 2.05 ms, Total Time: 2.05 ms, Total % of Trace Computation: 0.02%&#xA;INFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 14.48 ms, Median: 14.48 ms, Total Time: 14.48 ms, Total % of Trace Computation: 0.14%&#xA;INFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.22 ms, Median: 0.22 ms, Total Time: 0.22 ms, Total % of Trace Computation: 0.00%&#xA;INFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 10332.97 ms, Median: 10332.97 ms, Total Time: 10332.97 ms, Total % of Trace Computation: 99.45%&#xA;INFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.44 ms, Median: 0.44 ms, Total Time: 0.44 ms, Total % of Trace Computation: 0.00%&#xA;INFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 1.19 ms, Median: 1.19 ms, Total Time: 1.19 ms, Total % of Trace Computation: 0.01%&#xA;INFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.98 ms, Median: 0.98 ms, Total Time: 0.98 ms, Total % of Trace Computation: 0.01%&#xA;INFO:nv_ingest_client.cli.util.processing:redis_source_network_in: Avg: 12.27 ms, Median: 12.27 ms, Total Time: 12.27 ms, Total % of Trace Computation: 0.12%&#xA;INFO:nv_ingest_client.cli.util.processing:redis_task_sink_channel_in: Avg: 2.16 ms, Median: 2.16 ms, Total Time: 2.16 ms, Total % of Trace Computation: 0.02%&#xA;INFO:nv_ingest_client.cli.util.processing:redis_task_source: Avg: 8.00 ms, Median: 8.00 ms, Total Time: 8.00 ms, Total % of Trace Computation: 0.08%&#xA;INFO:nv_ingest_client.cli.util.processing:Unresolved time: 82.82 ms, Percent of Total Elapsed: 0.79%&#xA;INFO:nv_ingest_client.cli.util.processing:Processed 1 files in 10.47 seconds.&#xA;INFO:nv_ingest_client.cli.util.processing:Total pages processed: 3&#xA;INFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 0.29&#xA;INFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 4: Inspecting and Consuming Results&lt;/h3&gt; &#xA;&lt;p&gt;After the ingestion steps above have completed, you should be able to find &lt;code&gt;text&lt;/code&gt; and &lt;code&gt;image&lt;/code&gt; subfolders inside your processed docs folder. Each will contain JSON formatted extracted content and metadata.&lt;/p&gt; &#xA;&lt;h4&gt;When processing has completed, you&#39;ll have separate result files for text and image data:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ls -R processed_docs/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;processed_docs/:&#xA;image  structured  text&#xA;&#xA;processed_docs/image:&#xA;multimodal_test.pdf.metadata.json&#xA;&#xA;processed_docs/structured:&#xA;multimodal_test.pdf.metadata.json&#xA;&#xA;processed_docs/text:&#xA;multimodal_test.pdf.metadata.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can view the full JSON extracts and the metadata definitions &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docs/content-metadata.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;We also provide a script for inspecting &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/src/util/image_viewer.py&#34;&gt;extracted images&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;First, install &lt;code&gt;tkinter&lt;/code&gt; by running the following commands depending on your OS.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For Ubuntu/Debian Linux:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get update&#xA;sudo apt-get install python3-tk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For Fedora/RHEL Linux:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo dnf install python3-tkinter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For macOS using Homebrew:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install python-tk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following command to execute the script for inspecting the extracted image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Beyond inspecting the results, you can read them into things like &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/examples/llama_index_multimodal_rag.ipynb&#34;&gt;llama-index&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/examples/langchain_multimodal_rag.ipynb&#34;&gt;langchain&lt;/a&gt; retrieval pipelines.&lt;/p&gt; &#xA; &lt;p&gt;Please also checkout our &lt;a href=&#34;https://build.nvidia.com/nvidia/multimodal-pdf-data-extraction-for-enterprise-rag&#34;&gt;demo using a retrieval pipeline on build.nvidia.com&lt;/a&gt; to query over document content pre-extracted w/ NVIDIA Ingest.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Repo Structure&lt;/h2&gt; &#xA;&lt;p&gt;Beyond the relevant documentation, examples, and other links above, below is a description of contents in this repo&#39;s folders:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/.github&#34;&gt;.github&lt;/a&gt;: GitHub repo configuration files&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/ci&#34;&gt;ci&lt;/a&gt;: scripts used to build the nv-ingest container and other packages&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/client&#34;&gt;client&lt;/a&gt;: docs and source code for the nv-ingest-cli utility&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/config&#34;&gt;config&lt;/a&gt;: various yaml files defining configuration for OTEL, Prometheus&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/data&#34;&gt;data&lt;/a&gt;: Sample PDFs provided for testing convenience&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docker&#34;&gt;docker&lt;/a&gt;: houses scripts used by the nv-ingest docker container&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/docs&#34;&gt;docs&lt;/a&gt;: Various READMEs describing deployment, metadata schemas, auth and telemetry setup&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/examples&#34;&gt;examples&lt;/a&gt;: Example notebooks, scripts, and longer form tutorial content&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/helm&#34;&gt;helm&lt;/a&gt;: Documentation for deploying nv-ingest to a Kubernetes cluster via Helm chart&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/skaffold&#34;&gt;skaffold&lt;/a&gt;: Skaffold configuration&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/src&#34;&gt;src&lt;/a&gt;: source code for the nv-ingest pipelines and service&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/nv-ingest/main/tests&#34;&gt;tests&lt;/a&gt;: unit tests for nv-ingest&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Notices&lt;/h2&gt; &#xA;&lt;h3&gt;Third Party License Notice:&lt;/h3&gt; &#xA;&lt;p&gt;If configured to do so, this project will download and install additional third-party open source software projects. Review the license terms of these open source projects before use:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pdfservices-sdk/&#34;&gt;https://pypi.org/project/pdfservices-sdk/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;INSTALL_ADOBE_SDK&lt;/code&gt;&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: If set to &lt;code&gt;true&lt;/code&gt;, the Adobe SDK will be installed in the container at launch time. This is required if you want to use the Adobe extraction service for PDF decomposition. Please review the &lt;a href=&#34;https://github.com/adobe/pdfservices-python-sdk?tab=License-1-ov-file&#34;&gt;license agreement&lt;/a&gt; for the pdfservices-sdk before enabling this option.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;We require that all contributors &#34;sign-off&#34; on their commits. This certifies that the contribution is your original work, or you have rights to submit it under the same license, or a compatible license.&lt;/p&gt; &#xA;&lt;p&gt;Any contribution which contains commits that are not Signed-Off will not be accepted.&lt;/p&gt; &#xA;&lt;p&gt;To sign off on a commit you simply use the --signoff (or -s) option when committing your changes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git commit -s -m &#34;Add cool feature.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will append the following to your commit message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Signed-off-by: Your Name &amp;lt;your@email.com&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Full text of the DCO:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;  Developer Certificate of Origin&#xA;  Version 1.1&#xA;&#xA;  Copyright (C) 2004, 2006 The Linux Foundation and its contributors.&#xA;  1 Letterman Drive&#xA;  Suite D4700&#xA;  San Francisco, CA, 94129&#xA;&#xA;  Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;  Developer&#39;s Certificate of Origin 1.1&#xA;&#xA;  By making a contribution to this project, I certify that:&#xA;&#xA;  (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or&#xA;&#xA;  (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or&#xA;&#xA;  (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.&#xA;&#xA;  (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>