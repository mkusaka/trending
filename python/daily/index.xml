<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-25T01:35:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google/google-ctf</title>
    <updated>2024-06-25T01:35:58Z</updated>
    <id>tag:github.com,2024-06-25:/google/google-ctf</id>
    <link href="https://github.com/google/google-ctf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Google CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google CTF&lt;/h1&gt; &#xA;&lt;p&gt;This repository lists most of the challenges used in the Google CTF since 2017, as well as most of the infrastructure that can be used to run them. &lt;strong&gt;IMPORTANT&lt;/strong&gt; - The code in the 201x and 202x folders have unfixed security vulnerabilities. These are there on purpose, and running these on real production infrastructure is not safe.&lt;/p&gt; &#xA;&lt;p&gt;Read more about the Google CTF here: &lt;a href=&#34;https://security.googleblog.com/2017/06/announcing-google-capture-flag-2017.html&#34;&gt;https://security.googleblog.com/2017/06/announcing-google-capture-flag-2017.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note this is not an official Google product.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/DiffSynth-Studio</title>
    <updated>2024-06-25T01:35:58Z</updated>
    <id>tag:github.com,2024-06-25:/modelscope/DiffSynth-Studio</id>
    <link href="https://github.com/modelscope/DiffSynth-Studio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth Studio&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;DiffSynth Studio is a Diffusion engine. We have restructured architectures including Text Encoder, UNet, VAE, among others, maintaining compatibility with models from the open-source community while enhancing computational performance. We provide many interesting features. Enjoy the magic of Diffusion models!&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Aug 29, 2023. We propose DiffSynth, a video synthesis framework. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffSynth.github.io/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth&#34;&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (ECML PKDD 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2308.03463&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Oct 1, 2023. We release an early version of this project, namely FastSDXL. A try for building a diffusion engine. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The source codes are released on &lt;a href=&#34;https://github.com/Artiprocher/FastSDXL&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;FastSDXL includes a trainable OLSS scheduler for efficiency improvement. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The original repo of OLSS is &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;The technical report (CIKM 2023) is released on &lt;a href=&#34;https://arxiv.org/abs/2305.14677&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;A demo video is shown on &lt;a href=&#34;https://www.bilibili.com/video/BV1w8411y7uj&#34;&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Since OLSS requires additional training, we don&#39;t implement it in this project.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Nov 15, 2023. We propose FastBlend, a powerful video deflickering algorithm. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The sd-webui extension is released on &lt;a href=&#34;https://github.com/Artiprocher/sd-webui-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Demo videos are shown on Bilibili, including three tasks. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1d94y1W7PE&#34;&gt;Video deflickering&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Lw411m71p&#34;&gt;Video interpolation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1RB4y1Z7LF&#34;&gt;Image-driven video rendering&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2311.09265&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;An unofficial ComfyUI extension developed by other users is released on &lt;a href=&#34;https://github.com/AInseven/ComfyUI-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Dec 8, 2023. We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.&lt;/li&gt; &#xA; &lt;li&gt;Jan 29, 2024. We propose Diffutoon, a fantastic solution for toon shading. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffutoonProjectPage/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in this project.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (IJCAI 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2401.16224&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;June 13, 2024. DiffSynth Studio is transfered to ModelScope. The developers have transitioned from &#34;I&#34; to &#34;we&#34;. Of course, I will still participate in development and maintenance.&lt;/li&gt; &#xA; &lt;li&gt;June 21, 2024. We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/ExVideoProjectPage/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Source code is released in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Models are released on &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;HuggingFace&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2406.14130&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Until now, DiffSynth Studio has supported the following models: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/guoyww/animatediff/&#34;&gt;AnimateDiff&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;Ip-Adapter&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/hzwer/ECCV2022-RIFE&#34;&gt;RIFE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanDiT&#34;&gt;Hunyuan-DiT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt&#34;&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ExVideo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git&#xA;cd DiffSynth-Studio&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage (in Python code)&lt;/h2&gt; &#xA;&lt;p&gt;The Python examples are in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt;. We provide an overview here.&lt;/p&gt; &#xA;&lt;h3&gt;Long Video Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;We trained an extended video synthesis model, which can generate 128 frames. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;Generate high-resolution images, by breaking the limitation of diffusion models! &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/&#34;&gt;&lt;code&gt;examples/image_synthesis&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;512*512&lt;/th&gt; &#xA;   &lt;th&gt;1024*1024&lt;/th&gt; &#xA;   &lt;th&gt;2048*2048&lt;/th&gt; &#xA;   &lt;th&gt;4096*4096&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/55f679e9-7445-4605-9315-302e93d11370&#34; alt=&#34;512&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/9087a73c-9164-4c58-b2a0-effc694143fb&#34; alt=&#34;2048&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/edee9e71-fc39-4d1c-9ca9-fa52002c67ac&#34; alt=&#34;4096&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1024*1024&lt;/th&gt; &#xA;   &lt;th&gt;2048*2048&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/584186bc-9855-4140-878e-99541f9a757f&#34; alt=&#34;2048&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Toon Shading&lt;/h3&gt; &#xA;&lt;p&gt;Render realistic videos in a flatten style and enable video editing features. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/Diffutoon/&#34;&gt;&lt;code&gt;examples/Diffutoon&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video Stylization&lt;/h3&gt; &#xA;&lt;p&gt;Video stylization without video models. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/diffsynth/&#34;&gt;&lt;code&gt;examples/diffsynth&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chinese Models&lt;/h3&gt; &#xA;&lt;p&gt;Use Hunyuan-DiT to generate images with Chinese prompts. We also support LoRA fine-tuning of this model. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/hunyuan_dit/&#34;&gt;&lt;code&gt;examples/hunyuan_dit&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: 少女手捧鲜花，坐在公园的长椅上，夕阳的余晖洒在少女的脸庞，整个画面充满诗意的美感&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1024x1024&lt;/th&gt; &#xA;   &lt;th&gt;2048x2048 (highres-fix)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/2b6528cf-a229-46e9-b7dd-4a9475b07308&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/11d264ec-966b-45c9-9804-74b60428b866&#34; alt=&#34;image_2048&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Prompt: 一只小狗蹦蹦跳跳，周围是姹紫嫣红的鲜花，远处是山脉&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Without LoRA&lt;/th&gt; &#xA;   &lt;th&gt;With LoRA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/1aa21de5-a992-4b66-b14f-caa44e08876e&#34; alt=&#34;image_without_lora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/83a0a41a-691f-4610-8e7b-d8e17c50a282&#34; alt=&#34;image_with_lora&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage (in WebUI)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m streamlit run DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>