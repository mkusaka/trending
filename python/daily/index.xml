<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-22T01:37:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ProjectUnifree/unifree</title>
    <updated>2023-09-22T01:37:28Z</updated>
    <id>tag:github.com,2023-09-22:/ProjectUnifree/unifree</id>
    <link href="https://github.com/ProjectUnifree/unifree" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Migrate Unity projects to other engines!&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;span&gt;‚ö†&lt;/span&gt; This is an early prototype! &lt;span&gt;‚ö†&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This project is being actively developed and it is not stable yet! It is currently recommended you either wait, or migrate manually.&lt;/p&gt; &#xA;&lt;p&gt;You can use this tool to help aid migration by providing you with a base, but you &lt;em&gt;will&lt;/em&gt; have to modify a lot of code yourself. Please do not under any circumstance expect help regarding this tool from any Godot or Unreal Engine Discord server &lt;a href=&#34;https://discord.gg/Ee5wJ4JWBQ&#34;&gt;&lt;em&gt;(join our server instead!)&lt;/em&gt;&lt;/a&gt; And please don&#39;t correlate any bad experience you have while using this tool to &lt;em&gt;&#34;Godot/Unreal sucking and being broken&#34;&lt;/em&gt;, both engines are very stable and game-ready; This is not an official project for either of those engines, this is a community effort.&lt;/p&gt; &#xA;&lt;h4&gt;Godot Engine&lt;/h4&gt; &#xA;&lt;p&gt;If you wish to migrate manually to Godot: &lt;a href=&#34;https://www.youtube.com/results?search_query=Migrate+from+Unity+to+Godot&#34;&gt;YouTube migration guides&lt;/a&gt;, &lt;a href=&#34;https://docs.godotengine.org/en/stable/&#34;&gt;Godot docs&lt;/a&gt; &lt;em&gt;(very helpful!)&lt;/em&gt;, &lt;a href=&#34;https://discord.gg/4JBkykG&#34;&gt;Discord server&lt;/a&gt; &lt;em&gt;(for any burning questions you can&#39;t find answers to online)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;These other tools would also be very helpful!:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KhronosGroup/UnityGLTF&#34;&gt;UnityGLTF &lt;em&gt;(Unity 3D models to Godot models)&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/V-Sekai/unidot_importer&#34;&gt;Unidot Importer&lt;/a&gt; and &lt;a href=&#34;https://github.com/barcoderdev/unitypackage_godot&#34;&gt;UnityPackage Godot&lt;/a&gt; &lt;em&gt;(Unity assets to Godot assets)&lt;/em&gt; (some of these projects may be merged into Unifree, but are not currently)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Unreal Engine&lt;/h4&gt; &#xA;&lt;p&gt;If you wish to migrate manually to Unreal Engine: &lt;a href=&#34;https://www.youtube.com/results?search_query=Unreal+Engine+basics+%22(for+Unity+developers)%22&#34;&gt;helpful UE tutorials&lt;/a&gt;, &lt;a href=&#34;https://docs.unrealengine.com/5.3/en-US/unreal-engine-for-unity-developers/&#34;&gt;Unreal Engine docs&lt;/a&gt; &lt;em&gt;(very helpful!)&lt;/em&gt;, &lt;a href=&#34;https://discord.com/invite/unreal-slackers&#34;&gt;Semi-official Discord server&lt;/a&gt; &lt;em&gt;(for any burning questions you can&#39;t find answers to online)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Other engines (Cocos, Stride, etc)&lt;/h4&gt; &#xA;&lt;p&gt;Search up a tutorial on YouTube, view the docs, and join your engine&#39;s Discord server or subreddit for any questions! These engines are not currently supported by this tool, as we have a lot more to work on. But support may be added sometime in the future. We are currently focusing on Godot, as well as Unreal.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scan source project and detect files eligible for migration&lt;/li&gt; &#xA; &lt;li&gt;Use ChatGPT to translate &lt;code&gt;.cs&lt;/code&gt; files into Godot scripts and Unreal 3D classes. Here is how sample translation looks:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;  Unity                                                  | Godot                                   &#xA;  ------------------------------------------------------ | ------------------------------------------------------&#xA;  public class Player : MonoBehaviour                    | class_name Player:&#xA;  {                                                      | &#xA;    private SpriteRenderer spriteRenderer;               | var sprite_renderer: SpriteRenderer&#xA;    public Sprite[] sprites;                             | var sprites: Array&#xA;    private int spriteIndex;                             | var sprite_index: int&#xA;                                                         | &#xA;    public float strength = 5f;                          | var strength: float = 5.0&#xA;                                                         | &#xA;    private Vector3 direction;                           | var direction: Vector3&#xA;                                                         | &#xA;    private void Awake()                                 | func _ready():&#xA;    {                                                    |   sprite_renderer = get_node(&#34;SpriteRenderer&#34;)&#xA;      spriteRenderer = GetComponent&amp;lt;SpriteRenderer&amp;gt;();   | &#xA;    }                                                    | func _on_start():&#xA;                                                         |   call_deferred(&#34;animate_sprite&#34;)&#xA;    private void Start()                                 |   set_process(true)&#xA;    {                                                    | &#xA;      InvokeRepeating(                                   | func _on_enable():&#xA;          nameof(AnimateSprite),                         |   var position = transform.position&#xA;          0.15f,                                         |   position.y = 0.0&#xA;          0.15f                                          |   transform.position = position&#xA;      );                                                 |   direction = Vector3.ZERO&#xA;    }                                                    | class_name Player:&#xA;                                                         | &#xA;    private void OnEnable()                              | var sprite_renderer: SpriteRenderer&#xA;    {                                                    | var sprites: Array&#xA;      Vector3 position = transform.position;             | var sprite_index: int&#xA;      position.y = 0f;                                   | &#xA;      transform.position = position;                     | var strength: float = 5.0&#xA;      direction = Vector3.zero;                          | &#xA;    }                                                    | var direction: Vector3&#xA;  }                                                      |&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation and Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;launch.sh&lt;/code&gt; (or &lt;code&gt;launch.bat&lt;/code&gt; on windows) is the main script that downloads Unifree code, installs a python virtual environment, installs dependencies and launches the main program (&lt;code&gt;unifree/free.py&lt;/code&gt;). It accepts following parameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Key&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;your_openai_api_key&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key&#34;&gt;How to get an OpenAI API Key&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;config_name&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the migration. Currently supported: &lt;code&gt;godot&lt;/code&gt;, &lt;code&gt;unreal&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;source_project_dir&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Absolute path of the project to migrate&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;lt;destination_project_dir&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Absolute path of where results should be written&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install git python3&#xA;curl -0 https://raw.githubusercontent.com/ProjectUnifree/unifree/main/launch.sh | bash&#xA;OPENAI_API_KEY=&amp;lt;your_openai_api_key&amp;gt; ./launch.sh &amp;lt;config_name&amp;gt; &amp;lt;source_project_dir&amp;gt; &amp;lt;destination_project_dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Install Git for Windows &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;https://git-scm.com/download/win&lt;/a&gt;, then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/ProjectUnifree/unifree&#xA;cd unifree&#xA;launch.bat &amp;lt;your_openai_api_key&amp;gt; &amp;lt;config_name&amp;gt; &amp;lt;source_project_dir&amp;gt; &amp;lt;destination_project_dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Ubuntu/Debian&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install git python3 python3-venv&#xA;curl -0 https://raw.githubusercontent.com/ProjectUnifree/unifree/main/launch.sh | bash&#xA;OPENAI_API_KEY=&amp;lt;your_openai_api_key&amp;gt; ./launch.sh &amp;lt;config_name&amp;gt; &amp;lt;source_project_dir&amp;gt; &amp;lt;destination_project_dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Call To Action&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;üëã&lt;/span&gt; Join our &lt;a href=&#34;https://discord.gg/Ee5wJ4JWBQ&#34;&gt;Discord server&lt;/a&gt; for a live discussion!&lt;/p&gt; &#xA;&lt;p&gt;We are actively seeking contributors. If you are familiar with Unity, Godot, Cocos Creator or any other engines, help us!&lt;/p&gt; &#xA;&lt;p&gt;Here is what we need help with:&lt;/p&gt; &#xA;&lt;h4&gt;Asset Migration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;‚ùó&lt;/span&gt; &lt;span&gt;‚ùó&lt;/span&gt; Migrate Unity assets to Godot&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚ùó&lt;/span&gt; &lt;span&gt;‚ùó&lt;/span&gt; Migrate Unity assets to Cocos Creator&lt;/li&gt; &#xA; &lt;li&gt;What other engines should be implemented?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Source Migration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiment with ChatGPT prompts for better code translation&lt;/li&gt; &#xA; &lt;li&gt;Add prompts/configs to migrate to Cocos Creator&lt;/li&gt; &#xA; &lt;li&gt;What other engines should be implemented?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Framework&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;‚ùó&lt;/span&gt; Step-by-step console based migration wizard&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚ùó&lt;/span&gt; More edge case handling&lt;/li&gt; &#xA; &lt;li&gt;Architecture review and improvements&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>haoheliu/versatile_audio_super_resolution</title>
    <updated>2023-09-22T01:37:28Z</updated>
    <id>tag:github.com,2023-09-22:/haoheliu/versatile_audio_super_resolution</id>
    <link href="https://github.com/haoheliu/versatile_audio_super_resolution" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Versatile audio super resolution (any -&gt; 48kHz) with AudioSR.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioSR: Versatile Audio Super-resolution at Scale&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.07314&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2309.07314-brightgreen.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audioldm.github.io/audiosr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub.io-Audio_Samples-blue?logo=Github&amp;amp;style=flat-square&#34; alt=&#34;githubio&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pass your audio in, AudioSR will make it high fidelity!&lt;/p&gt; &#xA;&lt;p&gt;Work on all types of audio (e.g., music, speech, dog, raining, ...) &amp;amp; all sampling rates.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/haoheliu/versatile_audio_super_resolution/raw/main/visualization.png?raw=true&#34; alt=&#34;Image Description&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Change Log&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023-09-16: Fix DC shift issue. Fix duration padding bug. Update default DDIM steps to 50.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Commandline Usage&lt;/h2&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Optional&#xA;conda create -n audiosr python=3.9; conda activate audiosr&#xA;# Install AudioLDM&#xA;pip3 install audiosr==0.0.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Process a list of files. The result will be saved at ./output by default.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audiosr -il batch.lst&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Process a single audio file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audiosr -i example/music.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Full usage instruction&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&amp;gt; audiosr -h&#xA;&#xA;&amp;gt; usage: audiosr [-h] -i INPUT_AUDIO_FILE [-il INPUT_FILE_LIST] [-s SAVE_PATH] [--model_name {basic,speech}] [-d DEVICE] [--ddim_steps DDIM_STEPS] [-gs GUIDANCE_SCALE] [--seed SEED]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  -i INPUT_AUDIO_FILE, --input_audio_file INPUT_AUDIO_FILE&#xA;                        Input audio file for audio super resolution&#xA;  -il INPUT_FILE_LIST, --input_file_list INPUT_FILE_LIST&#xA;                        A file that contains all audio files that need to perform audio super resolution&#xA;  -s SAVE_PATH, --save_path SAVE_PATH&#xA;                        The path to save model output&#xA;  --model_name {basic,speech}&#xA;                        The checkpoint you gonna use&#xA;  -d DEVICE, --device DEVICE&#xA;                        The device for computation. If not specified, the script will automatically choose the device based on your environment.&#xA;  --ddim_steps DDIM_STEPS&#xA;                        The sampling step for DDIM&#xA;  -gs GUIDANCE_SCALE, --guidance_scale GUIDANCE_SCALE&#xA;                        Guidance scale (Large =&amp;gt; better quality and relavancy to text; Small =&amp;gt; better diversity)&#xA;  --seed SEED           Change this value (any integer number) will lead to a different generation result.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add gradio demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimize the inference speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite our work&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023audiosr,&#xA;  title={{AudioSR}: Versatile Audio Super-resolution at Scale},&#xA;  author={Liu, Haohe and Chen, Ke and Tian, Qiao and Wang, Wenwu and Plumbley, Mark D},&#xA;  journal={arXiv preprint arXiv:2309.07314},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>aharley/pips2</title>
    <updated>2023-09-22T01:37:28Z</updated>
    <id>tag:github.com,2023-09-22:/aharley/pips2</id>
    <link href="https://github.com/aharley/pips2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PIPs++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Long-Term Point Tracking with PIPs++&lt;/h1&gt; &#xA;&lt;p&gt;This is the official code release for the PIPs++ model presented in our ICCV 2023 paper, &#34;PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[&lt;a href=&#34;https://arxiv.org/abs/2307.15055&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pointodyssey.com/&#34;&gt;Project Page&lt;/a&gt;]&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://pointodyssey.com/assets/point_tracks.jpg&#34;&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The lines below should set up a fresh environment with everything you need:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n pips2 python=3.8&#xA;conda activate pips2&#xA;conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;To download our reference model, run this line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh get_reference_model.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, use the dropbox link inside that file.&lt;/p&gt; &#xA;&lt;p&gt;To try this model on a sample video, run this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run the model on the video included in &lt;code&gt;stock_videos/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For each 8-frame subsequence, the model will return &lt;code&gt;trajs_e&lt;/code&gt;. This is estimated trajectory data for a set of points, shaped &lt;code&gt;B,S,N,2&lt;/code&gt;, where &lt;code&gt;S&lt;/code&gt; is the sequence length and &lt;code&gt;N&lt;/code&gt; is the number of particles, and 2 is the x and y coordinates. The script will also produce tensorboard logs with visualizations, which go into &lt;code&gt;logs_demo/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the tensorboard for &lt;code&gt;logs_demo/&lt;/code&gt; you should be able to find visualizations like this:&lt;/p&gt; &#xA;&lt;img src=&#34;https://pointodyssey.com/assets/camel_compressed2.gif&#34;&gt; &#xA;&lt;h2&gt;PointOdyssey&lt;/h2&gt; &#xA;&lt;p&gt;We train our model on the &lt;a href=&#34;https://huggingface.co/datasets/aharley/pointodyssey&#34;&gt;PointOdyssey&lt;/a&gt; dataset.&lt;/p&gt; &#xA;&lt;p&gt;With a standard dataloader (e.g., &lt;code&gt;datasets/pointodysseydataset.py&lt;/code&gt;), loading PointOdyssey&#39;s high-resolution images can be a bottleneck at training time. To speed things up, we export mp4 clips from the dataset, at the resolution we want to train at, with augmentations baked in. To do this, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python export_mp4_dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will produce a dataset of clips, in &lt;code&gt;pod_export/$VERSION_$SEQLEN&lt;/code&gt;. The script will also produce some temporary folders to help write the data; these can be safely deleted afterwards. The script should also be safe to run in multiple threads in parallel. Depending on disk speeds, writing the full dataset with 4 threads should take about 24h.&lt;/p&gt; &#xA;&lt;p&gt;The script output should look something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.36_A_em00_aa_110247; step 000676/153264; this_step 018524; itime 3.77&#xA;.36_A_em00_aa_110247; step 000677/153264; this_step 017116; itime 2.74&#xA;.36_A_em00_aa_110247; step 000678/153264; this_step 095616; itime 6.11&#xA;sum(~mot_ok) 276&#xA;xN=0&#xA;sum(~mot_ok) 2000&#xA;N=0&#xA;:::sum(~mot_ok) 14&#xA;.36_A_em00_aa_110247; step 000685/153264; this_step 002960; itime 6.51&#xA;.36_A_em00_aa_110247; step 000686/153264; this_step 034423; itime 6.91&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the clips are produced in random order. The script is fairly friendly to multiple parallel runs, and avoids re-writing mp4s that have already been produced. Sometimes sampling from PointOdyssey will fail, and the script will report the reason for the failure (e.g., no valid tracks after applying augmentations).&lt;/p&gt; &#xA;&lt;p&gt;As soon as you have a few exported clips, you can start playing with the trainer. The trainer will load the exported data using &lt;code&gt;dataset/exportdataset.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train a model, simply run &lt;code&gt;train.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It should first print some diagnostic information about the model and dataset. Then it should print a message for each training step, indicating the model name, progress, read time, iteration time, and loss.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;model_name 4_36_128_i6_5e-4s_A_aa03_113745&#xA;loading export...&#xA;found 57867 folders in pod_export/aa_36&#xA;+--------------------------------------------------------+------------+&#xA;|                        Modules                         | Parameters |&#xA;+--------------------------------------------------------+------------+&#xA;|           module.fnet.layer3.0.conv1.weight            |   110592   |&#xA;|           module.fnet.layer3.0.conv2.weight            |   147456   |&#xA;|           module.fnet.layer3.1.conv1.weight            |   147456   |&#xA;|           module.fnet.layer3.1.conv2.weight            |   147456   |&#xA;|           module.fnet.layer4.0.conv1.weight            |   147456   |&#xA;|           module.fnet.layer4.0.conv2.weight            |   147456   |&#xA;|           module.fnet.layer4.1.conv1.weight            |   147456   |&#xA;|           module.fnet.layer4.1.conv2.weight            |   147456   |&#xA;|                module.fnet.conv2.weight                |   958464   |&#xA;|    module.delta_block.first_block_conv.conv.weight     |   275712   |&#xA;| module.delta_block.basicblock_list.2.conv2.conv.weight |   196608   |&#xA;| module.delta_block.basicblock_list.3.conv1.conv.weight |   196608   |&#xA;| module.delta_block.basicblock_list.3.conv2.conv.weight |   196608   |&#xA;| module.delta_block.basicblock_list.4.conv1.conv.weight |   393216   |&#xA;| module.delta_block.basicblock_list.4.conv2.conv.weight |   786432   |&#xA;| module.delta_block.basicblock_list.5.conv1.conv.weight |   786432   |&#xA;| module.delta_block.basicblock_list.5.conv2.conv.weight |   786432   |&#xA;| module.delta_block.basicblock_list.6.conv1.conv.weight |  1572864   |&#xA;| module.delta_block.basicblock_list.6.conv2.conv.weight |  3145728   |&#xA;| module.delta_block.basicblock_list.7.conv1.conv.weight |  3145728   |&#xA;| module.delta_block.basicblock_list.7.conv2.conv.weight | 3145728  |&#xA;+--------------------------------------------------------+------------+&#xA;total params: 17.57 M&#xA;4_36_128_i6_5e-4s_A_aa03_113745; step 000001/200000; rtime 3.69; itime 5.63; loss 35.030; loss_t 35.03; d_t 1.8; d_v nan&#xA;4_36_128_i6_5e-4s_A_aa03_113745; step 000002/200000; rtime 0.00; itime 1.45; loss 31.024; loss_t 33.03; d_t 2.5; d_v nan&#xA;4_36_128_i6_5e-4s_A_aa03_113745; step 000003/200000; rtime 0.00; itime 1.45; loss 30.908; loss_t 32.32; d_t 2.7; d_v nan&#xA;4_36_128_i6_5e-4s_A_aa03_113745; step 000004/200000; rtime 0.00; itime 1.45; loss 31.327; loss_t 32.07; d_t 2.8; d_v nan&#xA;4_36_128_i6_5e-4s_A_aa03_113745; step 000005/200000; rtime 0.00; itime 1.45; loss 29.762; loss_t 31.61; d_t 2.9; d_v nan&#xA;[...]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The final items in each line, &lt;code&gt;d_t&lt;/code&gt; and d_v&lt;code&gt;, show the result of the &lt;/code&gt;d_avg&lt;code&gt;metric on the training set and the validation set. Note that&lt;/code&gt;d_v&lt;code&gt;will show&lt;/code&gt;nan` until the first validation step.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce the reference model, you should train for 200k iterations (using the fully-exported dataset), with &lt;code&gt;B=4, S=36, crop_size=(256,384)&lt;/code&gt;. Then, fine-tune for 10k iterations using higher resolution and longer clips: &lt;code&gt;B=1, S=64, crop_size=(512,896)&lt;/code&gt;. If you can afford a higher batch size, you should use it. For this high-resolution finetuning, you can either export new mp4s, or use &lt;code&gt;pointodysseydataset.py&lt;/code&gt; directly.&lt;/p&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;We provide evaluation scripts for all of the datasets reported in the paper. The values in this repo are slightly different than those in the PDF, largely because we fixed some bugs in the dataset and re-trained the model for this release.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TAP-VID-DAVIS&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For each point with a valid annotation in frame0, we track it to the end of the video (&amp;lt;200 frames). The data comes from the &lt;a href=&#34;https://github.com/google-deepmind/tapnet#tap-vid-benchmark&#34;&gt;DeepMind TAP-NET repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With the reference model, &lt;code&gt;test_on_tap.py&lt;/code&gt; should yield &lt;code&gt;d_avg 70.6; survival_16 89.3; median_l2 6.9&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CroHD&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We chop the videos in to 1000-frame clips, and track heads from the beginning to the end. The data comes from the &#34;Get all data&#34; link on the &lt;a href=&#34;https://motchallenge.net/data/Head_Tracking_21/&#34;&gt;Head Tracking 21 MOT Challenge&lt;/a&gt; page. Downloading and unzipping that should give you the folders HT21 and HT21Labels, which our dataloader relies on.&lt;/p&gt; &#xA;&lt;p&gt;With the reference model, &lt;code&gt;test_on_cro.py&lt;/code&gt; should yield &lt;code&gt;d_avg 50.6.5; survival_16 50.6; median_l2 19.7&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PointOdyssey test set&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For each point with a valid annotation in frame0, we track it to the end of the video (~2k frames). Note that here we use the &lt;code&gt;pointodysseydataset_fullseq.py&lt;/code&gt; dataloader, and we load &lt;code&gt;S=128&lt;/code&gt; frames at a time, because 2k frames will not fit in memory.&lt;/p&gt; &#xA;&lt;p&gt;With the reference model, &lt;code&gt;test_on_pod.py&lt;/code&gt; should yield &lt;code&gt;d_avg 31.3; survival_16 32.7; median_l2 33.0&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking.&lt;/strong&gt; Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, Leonidas J. Guibas. In ICCV 2023.&lt;/p&gt; &#xA;&lt;p&gt;Bibtex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{zheng2023point,&#xA;‚ÄÉauthor = {Yang Zheng and Adam W. Harley and Bokui Shen and Gordon Wetzstein and Leonidas J. Guibas},&#xA;‚ÄÉtitle = {PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking},&#xA;‚ÄÉbooktitle = {ICCV},&#xA;‚ÄÉyear = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>