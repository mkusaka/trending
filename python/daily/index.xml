<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-30T01:29:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xlang-ai/OSWorld</title>
    <updated>2024-04-30T01:29:47Z</updated>
    <id>tag:github.com,2024-04-30:/xlang-ai/OSWorld</id>
    <link href="https://github.com/xlang-ai/OSWorld" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OSWorld: A real computer environment for multimodal agents to evaluate open-ended computer tasks&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/xlangai/assets/resolve/main/github_banner_v2.png&#34; alt=&#34;Banner&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://os-world.github.io/&#34;&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;Paper&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples&#34;&gt;Data&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://os-world.github.io/explorer.html&#34;&gt;Data Viewer&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://discord.gg/4Gnw7eTEZR&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://img.shields.io/badge/PRs-Welcome-red&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-Welcome-red&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://img.shields.io/github/last-commit/xlang-ai/OSWorld?color=green&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/xlang-ai/OSWorld?color=green&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/desktop-env&#34;&gt; &lt;img src=&#34;https://badge.fury.io/py/desktop-env.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/desktop-env&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/badge/desktop-env&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üì¢ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024-04-11: We released our &lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/xlang-ai/OSWorld&#34;&gt;environment and benchmark&lt;/a&gt;, and &lt;a href=&#34;https://os-world.github.io/&#34;&gt;project page&lt;/a&gt;. Check it out!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíæ Installation&lt;/h2&gt; &#xA;&lt;h3&gt;On Your Desktop or Server (Non-Virtualized Platform)&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you are operating on a system that has not been virtualized, meaning you are not utilizing a virtualized environment like AWS, Azure, or k8s. If this is the case, proceed with the instructions below. However, if you are on a virtualized platform, please refer to the &lt;a href=&#34;https://github.com/xlang-ai/OSWorld?tab=readme-ov-file#virtualized-platform&#34;&gt;virtualized platform&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, clone this repository and &lt;code&gt;cd&lt;/code&gt; into it. Then, install the dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. It is recommended that you use the latest version of Conda to manage the environment, but you can also choose to manually install the dependencies. Please ensure that the version of Python is &amp;gt;= 3.9.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the OSWorld repository&#xA;git clone https://github.com/xlang-ai/OSWorld&#xA;&#xA;# Change directory into the cloned repository&#xA;cd OSWorld&#xA;&#xA;# Optional: Create a Conda environment for OSWorld&#xA;# conda create -n osworld python=3.9&#xA;# conda activate osworld&#xA;&#xA;# Install required dependencies&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can install the environment without any benchmark tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install desktop-env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html&#34;&gt;VMware Workstation Pro&lt;/a&gt; (for systems with Apple Chips, you should install &lt;a href=&#34;https://www.vmware.com/go/getfusion&#34;&gt;VMware Fusion&lt;/a&gt;) and configure the &lt;code&gt;vmrun&lt;/code&gt; command. The installation process can refer to &lt;a href=&#34;https://raw.githubusercontent.com/xlang-ai/OSWorld/main/INSTALL_VMWARE.md&#34;&gt;How to install VMware Worksation Pro&lt;/a&gt;. Verify the successful installation by running the following:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vmrun -T ws list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the installation along with the environment variable set is successful, you will see the message showing the current running virtual machines.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We will also support using &lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;VirtualBox&lt;/a&gt; in the near future if you have issues with VMware Pro. However, features such as parallelism and macOS on Apple chips are not supported.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All set! Our setup script will automatically download the necessary virtual machines and configure the environment for you.&lt;/p&gt; &#xA;&lt;h3&gt;On AWS or Azure (Virtualized platform)&lt;/h3&gt; &#xA;&lt;p&gt;We are working on supporting it üë∑. Please hold tight!&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Run the following minimal example to interact with the environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from desktop_env.envs.desktop_env import DesktopEnv&#xA;&#xA;example = {&#xA;    &#34;id&#34;: &#34;94d95f96-9699-4208-98ba-3c3119edf9c2&#34;,&#xA;    &#34;instruction&#34;: &#34;I want to install Spotify on my current system. Could you please help me?&#34;,&#xA;    &#34;config&#34;: [&#xA;        {&#xA;            &#34;type&#34;: &#34;execute&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;command&#34;: [&#xA;                    &#34;python&#34;,&#xA;                    &#34;-c&#34;,&#xA;                    &#34;import pyautogui; import time; pyautogui.click(960, 540); time.sleep(0.5);&#34;&#xA;                ]&#xA;            }&#xA;        }&#xA;    ],&#xA;    &#34;evaluator&#34;: {&#xA;        &#34;func&#34;: &#34;check_include_exclude&#34;,&#xA;        &#34;result&#34;: {&#xA;            &#34;type&#34;: &#34;vm_command_line&#34;,&#xA;            &#34;command&#34;: &#34;which spotify&#34;&#xA;        },&#xA;        &#34;expected&#34;: {&#xA;            &#34;type&#34;: &#34;rule&#34;,&#xA;            &#34;rules&#34;: {&#xA;                &#34;include&#34;: [&#34;spotify&#34;],&#xA;                &#34;exclude&#34;: [&#34;not found&#34;]&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;env = DesktopEnv(action_space=&#34;pyautogui&#34;)&#xA;&#xA;obs = env.reset(task_config=example)&#xA;obs, reward, done, info = env.step(&#34;pyautogui.rightClick()&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will see all the logs of the system running normally, including the successful creation of the environment, completion of setup, and successful execution of actions. In the end, you will observe a successful right-click on the screen, which means you are ready to go.&lt;/p&gt; &#xA;&lt;h2&gt;üß™ Experiments&lt;/h2&gt; &#xA;&lt;h3&gt;Agent Baselines&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to run the baseline agent used in our paper, you can execute the following command as an example under the GPT-4V pure-screenshot setting:&lt;/p&gt; &#xA;&lt;p&gt;Set &lt;strong&gt;OPENAI_API_KEY&lt;/strong&gt; environment variable with your API key&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#39;changme&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --path_to_vm Ubuntu/Ubuntu.vmx --headless --observation_type screenshot --model gpt-4-vision-preview --result_dir ./results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results, which include screenshots, actions, and video recordings of the agent&#39;s task completion, will be saved in the &lt;code&gt;./results&lt;/code&gt; directory in this case. You can then run the following command to obtain the result:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python show_result.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Please start by reading through the &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/raw/main/mm_agents/README.md&#34;&gt;agent interface&lt;/a&gt; and the &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/raw/main/desktop_env/README.md&#34;&gt;environment interface&lt;/a&gt;. Correctly implement the agent interface and import your customized version in the &lt;code&gt;run.py&lt;/code&gt; file. Afterward, you can execute a command similar to the one in the previous section to run the benchmark on your agent.&lt;/p&gt; &#xA;&lt;h2&gt;‚ùì FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;What is the username and password for the virtual machines?&lt;/h3&gt; &#xA;&lt;p&gt;The username and password for the virtual machines are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ubuntu:&lt;/strong&gt; &lt;code&gt;user&lt;/code&gt; / &lt;code&gt;password&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How can I configure a proxy for the VM if I&#39;m behind a GFW?&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/xlang-ai/OSWorld/main/PROXY_GUIDELINE.md&#34;&gt;Proxy Guideline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What are the running times and costs under different settings?&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Setting&lt;/th&gt; &#xA;   &lt;th&gt;Expected Time*&lt;/th&gt; &#xA;   &lt;th&gt;Budget Cost (Full Test Set/Small Test Set)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4V (screenshot)&lt;/td&gt; &#xA;   &lt;td&gt;10h&lt;/td&gt; &#xA;   &lt;td&gt;$100 ($10)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemini-ProV (screenshot)&lt;/td&gt; &#xA;   &lt;td&gt;15h&lt;/td&gt; &#xA;   &lt;td&gt;$0 ($0)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Claude-3 Opus (screenshot)&lt;/td&gt; &#xA;   &lt;td&gt;15h&lt;/td&gt; &#xA;   &lt;td&gt;$150 ($15)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4V (a11y tree, SoM, etc.)&lt;/td&gt; &#xA;   &lt;td&gt;30h&lt;/td&gt; &#xA;   &lt;td&gt;$500 ($50)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*No environment parallelism. Calculated in April 2024.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this environment useful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{OSWorld,&#xA;      title={OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments}, &#xA;      author={Tianbao Xie and Danyang Zhang and Jixuan Chen and Xiaochuan Li and Siheng Zhao and Ruisheng Cao and Toh Jing Hua and Zhoujun Cheng and Dongchan Shin and Fangyu Lei and Yitao Liu and Yiheng Xu and Shuyan Zhou and Silvio Savarese and Caiming Xiong and Victor Zhong and Tao Yu},&#xA;      year={2024},&#xA;      eprint={2404.07972},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>janeczku/calibre-web</title>
    <updated>2024-04-30T01:29:47Z</updated>
    <id>tag:github.com,2024-04-30:/janeczku/calibre-web</id>
    <link href="https://github.com/janeczku/calibre-web" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üìö Web app for browsing, reading and downloading eBooks stored in a Calibre database&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Short Notice from the maintainer&lt;/h1&gt; &#xA;&lt;p&gt;After 6 years of more or less intensive programming on Calibre-Web, I need a break. The last few months, maintaining Calibre-Web has felt more like work than a hobby. I felt pressured and teased by people to solve &#34;their&#34; problems and merge PRs for &#34;their&#34; Calibre-Web. I have turned off all notifications from Github/Discord and will now concentrate undisturbed on the development of ‚Äúmy‚Äù Calibre-Web over the next few weeks/months.&lt;br&gt; I will look into the issues and maybe also the PRs from time to time, but don&#39;t expect a quick response from me.&lt;/p&gt; &#xA;&lt;h1&gt;Calibre-Web&lt;/h1&gt; &#xA;&lt;p&gt;Calibre-Web is a web app that offers a clean and intuitive interface for browsing, reading, and downloading eBooks using a valid &lt;a href=&#34;https://calibre-ebook.com&#34;&gt;Calibre&lt;/a&gt; database.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/janeczku/calibre-web/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/janeczku/calibre-web?style=flat-square&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/commit-activity/w/janeczku/calibre-web?logo=github&amp;amp;style=flat-square&amp;amp;label=commits&#34; alt=&#34;Commit Activity&#34;&gt; &lt;a href=&#34;https://github.com/janeczku/calibre-web/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/janeczku/calibre-web/total?logo=github&amp;amp;style=flat-square&#34; alt=&#34;All Releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/calibreweb/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/calibreweb?logo=pypi&amp;amp;logoColor=fff&amp;amp;style=flat-square&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/calibreweb/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/calibreweb?logo=pypi&amp;amp;logoColor=fff&amp;amp;style=flat-square&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/h2VsJ2NEfB&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/838810113564344381?label=Discord&amp;amp;logo=discord&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt; (click to expand)&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#calibre-web&#34;&gt;About&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#installation-via-pip-recommended&#34;&gt;Installation via pip (recommended)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#quick-start&#34;&gt;Quick start&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#docker-images&#34;&gt;Docker Images&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#contributor-recognition&#34;&gt;Contributor Recognition&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#contact&#34;&gt;Contact&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/janeczku/calibre-web/master/#contributing-to-calibre-web&#34;&gt;Contributing to Calibre-Web&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;em&gt;This software is a fork of &lt;a href=&#34;https://github.com/mutschler/calibreserver&#34;&gt;library&lt;/a&gt; and licensed under the GPL v3 License.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/janeczku/calibre-web/wiki/images/main_screen.png&#34; alt=&#34;Main screen&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modern and responsive Bootstrap 3 HTML5 interface&lt;/li&gt; &#xA; &lt;li&gt;Full graphical setup&lt;/li&gt; &#xA; &lt;li&gt;Comprehensive user management with fine-grained per-user permissions&lt;/li&gt; &#xA; &lt;li&gt;Admin interface&lt;/li&gt; &#xA; &lt;li&gt;Multilingual user interface supporting 20+ languages (&lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/Translation-Status&#34;&gt;supported languages&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;OPDS feed for eBook reader apps&lt;/li&gt; &#xA; &lt;li&gt;Advanced search and filtering options&lt;/li&gt; &#xA; &lt;li&gt;Custom book collection (shelves) creation&lt;/li&gt; &#xA; &lt;li&gt;eBook metadata editing and deletion support&lt;/li&gt; &#xA; &lt;li&gt;Metadata download from various sources (extensible via plugins)&lt;/li&gt; &#xA; &lt;li&gt;eBook conversion through Calibre binaries&lt;/li&gt; &#xA; &lt;li&gt;eBook download restriction to logged-in users&lt;/li&gt; &#xA; &lt;li&gt;Public user registration support&lt;/li&gt; &#xA; &lt;li&gt;Send eBooks to E-Readers with a single click&lt;/li&gt; &#xA; &lt;li&gt;Sync Kobo devices with your Calibre library&lt;/li&gt; &#xA; &lt;li&gt;In-browser eBook reading support for multiple formats&lt;/li&gt; &#xA; &lt;li&gt;Upload new books in various formats, including audio formats&lt;/li&gt; &#xA; &lt;li&gt;Calibre Custom Columns support&lt;/li&gt; &#xA; &lt;li&gt;Content hiding based on categories and Custom Column content per user&lt;/li&gt; &#xA; &lt;li&gt;Self-update capability&lt;/li&gt; &#xA; &lt;li&gt;&#34;Magic Link&#34; login for easy access on eReaders&lt;/li&gt; &#xA; &lt;li&gt;LDAP, Google/GitHub OAuth, and proxy authentication support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h4&gt;Installation via pip (recommended)&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a virtual environment for Calibre-Web to avoid conflicts with existing Python dependencies&lt;/li&gt; &#xA; &lt;li&gt;Install Calibre-Web via pip: &lt;code&gt;pip install calibreweb&lt;/code&gt; (or &lt;code&gt;pip3&lt;/code&gt; depending on your OS/distro)&lt;/li&gt; &#xA; &lt;li&gt;Install optional features via pip as needed, see &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/Dependencies-in-Calibre-Web-Linux-and-Windows&#34;&gt;this page&lt;/a&gt; for details&lt;/li&gt; &#xA; &lt;li&gt;Start Calibre-Web by typing &lt;code&gt;cps&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Raspberry Pi OS users may encounter issues during installation. If so, please update pip (&lt;code&gt;./venv/bin/python3 -m pip install --upgrade pip&lt;/code&gt;) and/or install cargo (&lt;code&gt;sudo apt install cargo&lt;/code&gt;) before retrying the installation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Refer to the Wiki for additional installation examples: &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/Manual-installation&#34;&gt;manual installation&lt;/a&gt;, &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/How-To:-Install-Calibre-Web-in-Linux-Mint-19-or-20&#34;&gt;Linux Mint&lt;/a&gt;, &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/How-To:-Install-Calibre-Web-on-a-Cloud-Provider&#34;&gt;Cloud Provider&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open your browser and navigate to &lt;code&gt;http://localhost:8083&lt;/code&gt; or &lt;code&gt;http://localhost:8083/opds&lt;/code&gt; for the OPDS catalog&lt;/li&gt; &#xA; &lt;li&gt;Log in with the default admin credentials&lt;/li&gt; &#xA; &lt;li&gt;If you don&#39;t have a Calibre database, you can use &lt;a href=&#34;https://github.com/janeczku/calibre-web/raw/master/library/metadata.db&#34;&gt;this database&lt;/a&gt; (move it out of the Calibre-Web folder to prevent overwriting during updates)&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;Location of Calibre database&lt;/code&gt; to the path of the folder containing your Calibre library (metadata.db) and click &#34;Save&#34;&lt;/li&gt; &#xA; &lt;li&gt;Optionally, use Google Drive to host your Calibre library by following the &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/G-Drive-Setup#using-google-drive-integration&#34;&gt;Google Drive integration guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Configure your Calibre-Web instance via the admin page, referring to the &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/Configuration#basic-configuration&#34;&gt;Basic Configuration&lt;/a&gt; and &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki/Configuration#ui-configuration&#34;&gt;UI Configuration&lt;/a&gt; guides&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Default Admin Login:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Username:&lt;/strong&gt; admin&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Password:&lt;/strong&gt; admin123&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.5+&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/script/download.php&#34;&gt;Imagemagick&lt;/a&gt; for cover extraction from EPUBs (Windows users may need to install &lt;a href=&#34;https://ghostscript.com/releases/gsdnld.html&#34;&gt;Ghostscript&lt;/a&gt; for PDF cover extraction)&lt;/li&gt; &#xA; &lt;li&gt;Optional: &lt;a href=&#34;https://calibre-ebook.com/download&#34;&gt;Calibre desktop program&lt;/a&gt; for on-the-fly conversion and metadata editing (set &#34;calibre&#39;s converter tool&#34; path on the setup page)&lt;/li&gt; &#xA; &lt;li&gt;Optional: &lt;a href=&#34;https://github.com/pgaskin/kepubify/releases/latest&#34;&gt;Kepubify tool&lt;/a&gt; for Kobo device support (place the binary in &lt;code&gt;/opt/kepubify&lt;/code&gt; on Linux or &lt;code&gt;C:\Program Files\kepubify&lt;/code&gt; on Windows)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker Images&lt;/h2&gt; &#xA;&lt;p&gt;Pre-built Docker images are available in the following Docker Hub repositories (maintained by the LinuxServer team):&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;LinuxServer - x64, aarch64&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/linuxserver/calibre-web&#34;&gt;Docker Hub&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/linuxserver/docker-calibre-web&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/linuxserver/docker-mods/tree/universal-calibre&#34;&gt;GitHub - Optional Calibre layer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Include the environment variable &lt;code&gt;DOCKER_MODS=linuxserver/mods:universal-calibre&lt;/code&gt; in your Docker run/compose file to add the Calibre &lt;code&gt;ebook-convert&lt;/code&gt; binary (x64 only). Omit this variable for a lightweight image.&lt;/p&gt; &lt;p&gt;Both the Calibre-Web and Calibre-Mod images are automatically rebuilt on new releases and updates.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set &#34;path to convertertool&#34; to &lt;code&gt;/usr/bin/ebook-convert&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Set &#34;path to unrar&#34; to &lt;code&gt;/usr/bin/unrar&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributor Recognition&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank all the &lt;a href=&#34;https://github.com/janeczku/calibre-web/graphs/contributors&#34;&gt;contributors&lt;/a&gt; and maintainers of Calibre-Web for their valuable input and dedication to the project. Your contributions are greatly appreciated.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Join us on &lt;a href=&#34;https://discord.gg/h2VsJ2NEfB&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more information, How To&#39;s, and FAQs, please visit the &lt;a href=&#34;https://github.com/janeczku/calibre-web/wiki&#34;&gt;Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to Calibre-Web&lt;/h2&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://github.com/janeczku/calibre-web/raw/master/CONTRIBUTING.md&#34;&gt;Contributing Guidelines&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>autonomousvision/gaussian-opacity-fields</title>
    <updated>2024-04-30T01:29:47Z</updated>
    <id>tag:github.com,2024-04-30:/autonomousvision/gaussian-opacity-fields</id>
    <link href="https://github.com/autonomousvision/gaussian-opacity-fields" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gaussian Opacity Fields for Efficient and Compact Surface Reconstruction in Unbounded Scenes&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://niujinshuchong.github.io/&#34;&gt;Zehao Yu&lt;/a&gt; ¬∑ &lt;a href=&#34;https://tsattler.github.io/&#34;&gt;Torsten Sattler&lt;/a&gt; ¬∑ &lt;a href=&#34;http://www.cvlibs.net/&#34;&gt;Andreas Geiger&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13i3HeVBiqN8JXnwAzTvQrPz2rShxIhMv/view?usp=sharing&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2404.10772.pdf&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://niujinshuchong.github.io/gaussian-opacity-fields/&#34;&gt;Project Page&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/autonomousvision/gaussian-opacity-fields/main/media/teaser_gof.png&#34; alt=&#34;Logo&#34; width=&#34;95%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Gaussian Opacity Fields (GOF) enables geometry extraction with 3D Gaussians directly by indentifying its level set. Our regularization improves surface reconstruction and we utilize Marching Tetrahedra for compact and adaptive mesh extraction.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repository and create an anaconda environment using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:autonomousvision/gaussian-opacity-fields.git&#xA;cd gaussian-opacity-fields&#xA;&#xA;conda create -y -n gof python=3.8&#xA;conda activate gof&#xA;&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html&#xA;conda install cudatoolkit-dev=11.3 -c conda-forge&#xA;&#xA;pip install -r requirements.txt&#xA;&#xA;pip install submodules/diff-gaussian-rasterization&#xA;pip install submodules/simple-knn/&#xA;&#xA;# tetra-nerf for triangulation&#xA;cd submodules/tetra-triangulation&#xA;conda install cmake&#xA;conda install conda-forge::gmp&#xA;conda install conda-forge::cgal&#xA;cmake .&#xA;make &#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Dataset&lt;/h1&gt; &#xA;&lt;p&gt;Please download the Mip-NeRF 360 dataset from the &lt;a href=&#34;https://jonbarron.info/mipnerf360/&#34;&gt;official webiste&lt;/a&gt;, the NeRF-Synthetic dataset from the &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;NeRF&#39;s official Google Drive&lt;/a&gt;, the preprocessed DTU dataset from &lt;a href=&#34;https://surfsplatting.github.io/&#34;&gt;2DGS&lt;/a&gt;, the proprocessed Tanks and Temples dataset from &lt;a href=&#34;https://huggingface.co/datasets/ZehaoYu/gaussian-opacity-fields/tree/main&#34;&gt;here&lt;/a&gt;. You need to download the ground truth point clouds from the &lt;a href=&#34;https://roboimagedata.compute.dtu.dk/?page_id=36&#34;&gt;DTU dataset&lt;/a&gt; and save to &lt;code&gt;dtu_eval/Offical_DTU_Dataset&lt;/code&gt; to evaluate the geometry reconstruction. For the &lt;a href=&#34;https://www.tanksandtemples.org/download/&#34;&gt;Tanks and Temples&lt;/a&gt; dataset, you need to download the ground truth point clouds, alignments and cropfiles and save to &lt;code&gt;eval_tnt/TrainingSet&lt;/code&gt;, such as &lt;code&gt;eval_tnt/TrainingSet/Caterpillar/Caterpillar.ply&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Training and Evaluation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;# you might need to update the data path in the script accordingly&#xA;&#xA;# NeRF-synthetic dataset&#xA;python scripts/run_nerf_synthetic.py&#xA;&#xA;# Mip-NeRF 360 dataset&#xA;python scripts/run_mipnerf360.py&#xA;&#xA;# Tanks and Temples dataset&#xA;python scripts/run_tnt.py&#xA;&#xA;# DTU dataset&#xA;python scripts/run_dtu.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Custom Dataset&lt;/h1&gt; &#xA;&lt;p&gt;We use the same data format from 3DGS, please follow &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting?tab=readme-ov-file#processing-your-own-scenes&#34;&gt;here&lt;/a&gt; to prepare the your dataset. Then you can train your model and extract a mesh (we use the Tanks and Temples dataset for example)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# training&#xA;# -r 2 for using downsampled images with factor 2&#xA;# --use_decoupled_appearance to enable decoupled appearance modeling if your images has changing lighting conditions&#xA;python train.py -s TNT_GOF/TrainingSet/Caterpillar -m exp_TNT/Caterpillar -r 2 --use_decoupled_appearance&#xA;&#xA;# extract the mesh after training&#xA;python extract_mesh.py -m exp_TNT/Caterpillar --iteration 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;This project is built upon &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3DGS&lt;/a&gt; and &lt;a href=&#34;https://github.com/autonomousvision/mip-splatting&#34;&gt;Mip-Splatting&lt;/a&gt;. Regularizations and some visualizations are taken from &lt;a href=&#34;https://surfsplatting.github.io/&#34;&gt;2DGS&lt;/a&gt;. Tetrahedra triangulation is taken from &lt;a href=&#34;https://github.com/jkulhanek/tetra-nerf&#34;&gt;Tetra-NeRF&lt;/a&gt;. Marching Tetrahdedra is adapted from &lt;a href=&#34;https://github.com/NVIDIAGameWorks/kaolin/raw/master/kaolin/ops/conversions/tetmesh.py&#34;&gt;Kaolin&lt;/a&gt; Library. Evaluation scripts for DTU and Tanks and Temples dataset are taken from &lt;a href=&#34;https://github.com/jzhangbs/DTUeval-python&#34;&gt;DTUeval-python&lt;/a&gt; and &lt;a href=&#34;https://github.com/isl-org/TanksAndTemples/tree/master/python_toolbox/evaluation&#34;&gt;TanksAndTemples&lt;/a&gt; respectively. We thank all the authors for their great work and repos.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find our code or paper useful, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Yu2024GOF,&#xA;  author    = {Yu, Zehao and Sattler, Torsten and Geiger, Andreas},&#xA;  title     = {Gaussian Opacity Fields: Efficient High-quality Compact Surface Reconstruction in Unbounded Scenes},&#xA;  journal   = {arXiv:2404.10772},&#xA;  year      = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you find the regularizations useful, please kindly cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{Huang2DGS2024,&#xA;  title={2D Gaussian Splatting for Geometrically Accurate Radiance Fields},&#xA;  author={Huang, Binbin and Yu, Zehao and Chen, Anpei and Geiger, Andreas and Gao, Shenghua},&#xA;  journal={arXiv preprint arXiv:2403.17888},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>