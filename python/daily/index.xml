<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-11T01:55:30Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ml-explore/mlx-examples</title>
    <updated>2023-12-11T01:55:30Z</updated>
    <id>tag:github.com,2023-12-11:/ml-explore/mlx-examples</id>
    <link href="https://github.com/ml-explore/mlx-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples in the MLX framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX Examples&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains a variety of standalone examples using the &lt;a href=&#34;https://github.com/ml-explore/mlx&#34;&gt;MLX framework&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/mnist&#34;&gt;MNIST&lt;/a&gt; example is a good starting point to learn how to use MLX.&lt;/p&gt; &#xA;&lt;p&gt;Some more useful examples include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/transformer_lm&#34;&gt;Transformer language model&lt;/a&gt; training.&lt;/li&gt; &#xA; &lt;li&gt;Large scale text generation with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/llama&#34;&gt;LLaMA&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/mistral&#34;&gt;Mistral&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Parameter efficient fine-tuning with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/lora&#34;&gt;LoRA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Generating images with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/stable_diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Speech recognition with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/whisper&#34;&gt;OpenAI&#39;s Whisper&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt; for more information on contributing to this repo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shariqfarooq123/LooseControl</title>
    <updated>2023-12-11T01:55:30Z</updated>
    <id>tag:github.com,2023-12-11:/shariqfarooq123/LooseControl</id>
    <link href="https://github.com/shariqfarooq123/LooseControl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lifting ControlNet for Generalized Depth Conditioning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LooseControl: Lifting ControlNet for Generalized Depth Conditioning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/shariqfarooq/LooseControl&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/PyTorch_v1.10.1-EE4C2C?&amp;amp;logo=pytorch&amp;amp;logoColor=white&#34; alt=&#34;PyTorch&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official repository for our paper:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/shariqfarooq123/LooseControl/main/#&#34;&gt;LooseControl: Lifting ControlNet for Generalized Depth Conditioning&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;h5&gt;&lt;a href=&#34;https://shariqfarooq123.github.io&#34;&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/n.mitra/&#34;&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href=&#34;http://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;&lt;/h5&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://shariqfarooq123.github.io/loose-control/&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.03079&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/shariqfarooq/LooseControl&#34;&gt;[Demo ðŸ¤—]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/shariqfarooq/loose-control-3dbox&#34;&gt;[Weights (3D Box Control)]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shariqfarooq123/LooseControl/main/assets/looseControl_teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/shariqfarooq123/LooseControl &amp;amp;&amp;amp; cd LooseControl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start the UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gradio app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or use via python API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from loosecontrol import LooseControlNet&#xA;&#xA;lcn = LooseControlNet(&#34;shariqfarooq/loose-control-3dbox&#34;)&#xA;&#xA;boxy_depth = ...&#xA;prompt = &#34;A photo of a snowman in a desert&#34;&#xA;negative_prompt = &#34;blurry, text, caption, lowquality,lowresolution, low res, grainy, ugly&#34;&#xA;&#xA;&#xA;gen_image_1 = lcn(prompt, negative_prompt=negative_prompt, control_image=boxy_depth)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Style preserving edits:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fix the &#39;style&#39; and edit&#xA;# Edit &#39;boxy_depth&#39; -&amp;gt; &#39;boxy_depth_edited&#39;&#xA;&#xA;lcn.set_cf_attention()&#xA;&#xA;gen_image_edited = lcn.edit(boxy_depth, boxy_depth_edited, prompt, negative_prompt=negative_prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Credits&lt;/h1&gt; &#xA;&lt;p&gt;The Cross Frame attention is adapted from &lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;Text2Video-Zero&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{bhat2023loosecontrol,&#xA;      title={LooseControl: Lifting ControlNet for Generalized Depth Conditioning}, &#xA;      author={Shariq Farooq Bhat and Niloy J. Mitra and Peter Wonka},&#xA;      year={2023},&#xA;      eprint={2312.03079},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>kyegomez/Gemini</title>
    <updated>2023-12-11T01:55:30Z</updated>
    <id>tag:github.com,2023-12-11:/kyegomez/Gemini</id>
    <link href="https://github.com/kyegomez/Gemini" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The open source implementation of Gemini, the model that will &#34;eclipse ChatGPT&#34; by Google&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/Gemini/main/agorabanner.png&#34; alt=&#34;Multi-Modality&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Gemini&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/Gemini/main/gemini.png&#34; alt=&#34;gemini&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The open source implementation of Gemini, the model that will &#34;eclipse ChatGPT&#34;, it seems to work by directly taking in all modalities all at once into a transformer with special decoders for text or img generation!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/CMDpRxCV8g&#34;&gt;Join the Agora discord channel to help with the implementation!&lt;/a&gt; and &lt;a href=&#34;https://github.com/users/kyegomez/projects/11/views/1&#34;&gt;Here is the project board:&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The input sequences for Gemini consist of texts, audio, images, and videos. These inputs are transformed into tokens, which are then processed by a transformer. Subsequently, conditional decoding takes place to generate image outputs.&lt;/p&gt; &#xA;&lt;p&gt;Interestingly, the architecture of Gemini bears resemblance to Fuyu&#39;s architecture but is expanded to encompass multiple modalities. Instead of utilizing a visual transformer (vit) encoder, Gemini simply feeds image embeddings directly into the transformer.&lt;/p&gt; &#xA;&lt;p&gt;For Gemini, the token inputs will likely be indicated by special modality tokens such as [IMG], &lt;img&gt;, [AUDIO], or &#xA; &lt;audio&gt;&#xA;  . Codi, a component of Gemini, also employs conditional generation and makes use of the tokenized outputs.&#xA; &lt;/audio&gt;&lt;/p&gt; &#xA;&lt;p&gt;To implement this model effectively, I intend to initially focus on the image embeddings to ensure their smooth integration. Subsequently, I will proceed with incorporating audio embeddings and then video embeddings.&lt;/p&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;pip3 install gemini-torch&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Gemini Transformer Usage&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Base transformer&lt;/li&gt; &#xA; &lt;li&gt;Multi Grouped Query Attn / flash attn&lt;/li&gt; &#xA; &lt;li&gt;rope&lt;/li&gt; &#xA; &lt;li&gt;alibi&lt;/li&gt; &#xA; &lt;li&gt;xpos&lt;/li&gt; &#xA; &lt;li&gt;qk norm&lt;/li&gt; &#xA; &lt;li&gt;no pos embeds&lt;/li&gt; &#xA; &lt;li&gt;kv cache&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch &#xA;from gemini_torch import Gemini&#xA;&#xA;# Initialize the model&#xA;model = Gemini(&#xA;    num_tokens=50432,&#xA;    max_seq_len=8192,&#xA;    dim=2560,&#xA;    depth=32,&#xA;    dim_head=128,&#xA;    heads=24,&#xA;    use_abs_pos_emb=False,&#xA;    alibi_pos_bias=True,&#xA;    alibi_num_heads=12,&#xA;    rotary_xpos=True,&#xA;    attn_flash=True,&#xA;    attn_kv_heads=2,&#xA;    qk_norm=True,&#xA;    attn_qk_norm=True,&#xA;    attn_qk_norm_dim_scale=True,&#xA;)&#xA;&#xA;# Initialize the text random tokens&#xA;x = torch.randint(0, 50432, (1, 8192))&#xA;&#xA;# Apply model to x&#xA;y = model(x)&#xA;&#xA;# Print logits&#xA;print(y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Multi-Modal with Imgs + Audio&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Img processing through a specially crafted module that takes in img -&amp;gt; patches it -&amp;gt; then reshapes to the shape of the text tensors, [B, seqlen, dim] -&amp;gt; align with text tokens&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from gemini_torch.model import Gemini&#xA;&#xA;# Initialize model&#xA;model = Gemini(&#xA;    num_tokens=50432,&#xA;    max_seq_len=8192,&#xA;    dim=2560,&#xA;    depth=32,&#xA;    dim_head=128,&#xA;    heads=24,&#xA;    use_abs_pos_emb=False,&#xA;    alibi_pos_bias=True,&#xA;    alibi_num_heads=12,&#xA;    rotary_xpos=True,&#xA;    attn_flash=True,&#xA;    attn_kv_heads=2,&#xA;    qk_norm=True,&#xA;    attn_qk_norm=True,&#xA;    attn_qk_norm_dim_scale=True,&#xA;)&#xA;&#xA;# Text shape: [batch, seq_len, dim]&#xA;text = torch.randint(0, 50432, (1, 8192))&#xA;&#xA;# Img shape: [batch, channels, height, width]&#xA;img = torch.randn(1, 3, 256, 256)&#xA;&#xA;# Audio shape: [batch, audio_seq_len, dim]&#xA;audio = torch.randn(1, 128)&#xA;&#xA;# Apply model to text and img&#xA;y = model(text, img, audio)&#xA;&#xA;# Output shape: [batch, seq_len, dim]&#xA;print(y.shape)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Tokenizer&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sentencepiece, tokenizer&lt;/li&gt; &#xA; &lt;li&gt;We&#39;re using the same tokenizer as LLAMA with special tokens denoting the beginning and end of the multi modality tokens.&lt;/li&gt; &#xA; &lt;li&gt;Does not fully process img, audio, or videos now we need help on that&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gemini_torch.tokenizer import MultimodalSentencePieceTokenizer&#xA;&#xA;# Example usage&#xA;tokenizer_name = &#34;hf-internal-testing/llama-tokenizer&#34;&#xA;tokenizer = MultimodalSentencePieceTokenizer(tokenizer_name=tokenizer_name)&#xA;&#xA;# Encoding and decoding examples&#xA;encoded_audio = tokenizer.encode(&#34;Audio description&#34;, modality=&#34;audio&#34;)&#xA;decoded_audio = tokenizer.decode(encoded_audio)&#xA;&#xA;print(&#34;Encoded audio:&#34;, encoded_audio)&#xA;print(&#34;Decoded audio:&#34;, decoded_audio)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;ImgToTransformer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;takes in img -&amp;gt; patches -&amp;gt; reshapes to [B, SEQLEN, Dim] to align with transformer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from gemini_torch.utils import ImgToTransformer&#xA;&#xA;# Example usage&#xA;num_patches = 16&#xA;patch_size = 16&#xA;transformer_dim = 512&#xA;img_channels = 3&#xA;seq_len = 50000&#xA;reduced_dim = 256  # Reduced dimension after dimensionality reduction&#xA;&#xA;model = ImgToTransformer(&#xA;    num_patches, patch_size, transformer_dim, img_channels, seq_len, reduced_dim&#xA;)&#xA;&#xA;# Dummy image input [BATCH, CHANNELS, HEIGHT, WIDTH]&#xA;dummy_img = torch.randn(1, 3, 64, 64)  # Batch size of 1, 64x64 RGB image&#xA;&#xA;# Forward pass&#xA;seq_space_output = model(dummy_img)&#xA;print(seq_space_output.shape)  # Expected shape: [1, 50000, 256]&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;AudioToLangEmbedding&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transforms audio into the same shape as text tensors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch &#xA;from gemini_torch.utils import AudioToLangEmbedding&#xA;&#xA;# Example usage&#xA;audio_seq_len = 32000  # Input audio sequence length&#xA;seqlen = 512  # Sequence length to align with the language transformer&#xA;dim = 512  # Embedding dimension&#xA;&#xA;model = AudioToLangEmbedding(audio_seq_len, seqlen, dim)&#xA;audio_input = torch.randn(1, audio_seq_len)  # Example input tensor&#xA;output = model(audio_input)&#xA;&#xA;print(&#34;Output shape:&#34;, output.shape)  # Should be [1, 512, 512]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Combine Reinforcment learning with modular pretrained transformer, multi-modal capabilities, image, audio,&lt;/li&gt; &#xA; &lt;li&gt;self improving mechanisms like robocat&lt;/li&gt; &#xA; &lt;li&gt;PPO? or MPO&lt;/li&gt; &#xA; &lt;li&gt;get good at backtracking and exploring alternative paths&lt;/li&gt; &#xA; &lt;li&gt;speculative decoding&lt;/li&gt; &#xA; &lt;li&gt;Algorithm of Thoughts&lt;/li&gt; &#xA; &lt;li&gt;RLHF&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf&#34;&gt;Gemini Report&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deepmind.google/technologies/gemini/#introduction&#34;&gt;Gemini Landing Page&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Todo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/users/kyegomez/projects/11/views/1&#34;&gt;Check out the project board for more todos&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Implement the img feature embedder and align imgs with text and pass into transformer: &lt;code&gt;Gemini models are trained to accommodate textual input interleaved with a wide variety of audio and visual inputs, such as natural images, charts, screenshots, PDFs, and videos, and they can produce text and image outputs (see Figure 2). The visual encoding of Gemini models is inspired by our own foundational work on Flamingo (Alayrac et al., 2022), CoCa (Yu et al., 2022a), and PaLI (Chen et al., 2022), with the important distinction that the models are multimodal from the beginning and can natively output images using discrete image tokens (Ramesh et al., 2021; Yu et al., 2022b).&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Implement the audio processing using USM by Google:&lt;code&gt;In addition, Gemini can directly ingest audio signals at 16kHz from Universal Speech Model (USM) (Zhang et al., 2023) features. This enables the model to capture nuances that are typically lost when the audio is naively mapped to a text input (for example, see audio understanding demo on the website).&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Video Processing Technique: &#34; Video understanding is accomplished by encoding the video as a sequence of frames in the large context window. Video frames or images can be interleaved naturally with text or audio as part of the model input&#34;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Prompting Technique: &lt;code&gt; We find Gemini Ultra achieves highest accuracy when used in combination with a chain-of-thought prompting approach (Wei et al., 2022) that accounts for model uncertainty. The model produces a chain of thought with k samples, for example 8 or 32. If there is a consensus above a preset threshold (selected based on the validation split), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihood choice without chain of thought. We refer the reader to appendix for a detailed breakdown of how this approach compares with only chain-of-thought prompting or only greedy sampling.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Train a 1.8B + 3.25 Model: &lt;code&gt;Nano-1 and Nano-2 model sizes are only 1.8B and 3.25B parameters respectively. Despite their size, they show exceptionally strong performance on factuality, i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>