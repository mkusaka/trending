<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-25T01:38:09Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>VikParuchuri/texify</title>
    <updated>2023-12-25T01:38:09Z</updated>
    <id>tag:github.com,2023-12-25:/VikParuchuri/texify</id>
    <link href="https://github.com/VikParuchuri/texify" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OCR model for math that outputs LaTeX and markdown&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Texify&lt;/h1&gt; &#xA;&lt;p&gt;Texify is an OCR model that converts images or pdfs containing math into markdown and LaTeX that can be rendered by MathJax ($$ and $ are delimiters). It can run on CPU, GPU, or MPS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/VikParuchuri/texify/assets/913340/882022a6-020d-4796-af02-67cb77bc084c&#34;&gt;https://github.com/VikParuchuri/texify/assets/913340/882022a6-020d-4796-af02-67cb77bc084c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Texify can work with block equations, or equations mixed with text (inline). It will convert both the equations and the text.&lt;/p&gt; &#xA;&lt;p&gt;The closest open source comparisons to texify are &lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR&#34;&gt;pix2tex&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/nougat&#34;&gt;nougat&lt;/a&gt;, although they&#39;re designed for different purposes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pix2tex is designed only for block LaTeX equations, and hallucinates more on text.&lt;/li&gt; &#xA; &lt;li&gt;Nougat is designed to OCR entire pages, and hallucinates more on small images only containing math.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Pix2tex is trained on im2latex, and nougat is trained on arxiv. Texify is trained on a more diverse set of web data, and works on a range of images.&lt;/p&gt; &#xA;&lt;p&gt;See more details in the &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/#benchmarks&#34;&gt;benchmarks&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg//KuZwXNGnfH&#34;&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; I added spaces after _ symbols because &lt;a href=&#34;https://github.com/github/markup/issues/1575&#34;&gt;Github math formatting is broken&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/examples/0.png&#34; alt=&#34;Example 0&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detected Text&lt;/strong&gt; The potential $V_{i}$ of cell $\mathcal{C}_ {j}$ centred at position $\mathbf{r}_ {i}$ is related to the surface charge densities $\sigma_ {j}$ of cells $\mathcal{E}_ {j}$ $j\in[1,N]$ through the superposition principle as:&lt;/p&gt; &#xA;&lt;p&gt;$$V_ {i},=,\sum_ {j=0}^{N},\frac{\sigma_ {j}}{4\pi\varepsilon_ {0}},\int_{\mathcal{E}_ {j}}\frac{1}{\left|\mathbf{r}_ {i}-\mathbf{r}^{\prime}\right|},\mathrm{d}^{2}\mathbf{r}^{\prime},=,\sum_{j=0}^{N},Q_ {ij},\sigma_{j},$$&lt;/p&gt; &#xA;&lt;p&gt;where the integral over the surface of cell $\mathcal{C}_ {j}$ only depends on $\mathcal{C}{j}$ shape and on the relative position of the target point $\mathbf{r}_ {i}$ with respect to $\mathcal{C}_ {j}$ location, as $\sigma_ {j}$ is assumed constant over the whole surface of cell $\mathcal{C}_ {j}$.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Image&lt;/th&gt; &#xA;   &lt;th&gt;OCR Markdown&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/examples/100.png&#34;&gt;1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/examples/100.md&#34;&gt;1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/examples/300.png&#34;&gt;2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/examples/300.md&#34;&gt;2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/examples/400.png&#34;&gt;3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/examples/400.md&#34;&gt;3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ll need python 3.10+ and PyTorch. You may need to install the CPU version of torch first if you&#39;re not using a Mac or a GPU machine. See &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;`pip install texify`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weights will automatically download the first time you run it.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspect the settings in &lt;code&gt;texify/settings.py&lt;/code&gt;. You can override any settings with environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt; or &lt;code&gt;TORCH_DEVICE=mps&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Don&#39;t make your boxes too small or too large. See the examples and the video above for good crops.&lt;/li&gt; &#xA; &lt;li&gt;Texify is sensitive to how you draw the box around the text you want to OCR. If you get bad results, try selecting a slightly different box, or splitting the box into 2+. You can also try changing the &lt;code&gt;TEMPERATURE&lt;/code&gt; setting.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes, KaTeX won&#39;t be able to render an equation (red error), but it will still be valid LaTeX. You can copy the LaTeX and render it elsewhere.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;App for interactive conversion&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;ve included a streamlit app that lets you interactively select and convert equations from images or PDF files. Run it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;texify_gui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The app will allow you to select the specific equations you want to convert on each page, then render the results with KaTeX and enable easy copying.&lt;/p&gt; &#xA;&lt;h2&gt;Convert images&lt;/h2&gt; &#xA;&lt;p&gt;You can OCR a single image or a folder of images with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;texify /path/to/folder_or_file --max 8 --json_path results.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; is how many images in the folder to convert at most. Omit this to convert all images in the folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--json_path&lt;/code&gt; is an optional path to a json file where the results will be saved. If you omit this, the results will be saved to &lt;code&gt;data/results.json&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Import and run&lt;/h2&gt; &#xA;&lt;p&gt;You can import texify and run it in python code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from texify.inference import batch_inference&#xA;from texify.model.model import load_model&#xA;from texify.model.processor import load_processor&#xA;from PIL import Image&#xA;&#xA;model = load_model()&#xA;processor = load_processor()&#xA;img = Image.open(&#34;test.png&#34;) # Your image name here&#xA;results = batch_inference([img], model, processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Manual install&lt;/h1&gt; &#xA;&lt;p&gt;If you want to develop texify, you can install it manually:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/VikParuchuri/texify.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd texify&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install&lt;/code&gt; # Installs main and dev dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;p&gt;OCR is complicated, and texify is not perfect. Here are some known limitations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The OCR is dependent on how you crop the image. If you get bad results, try a different selection/crop. Or try changing the &lt;code&gt;TEMPERATURE&lt;/code&gt; setting.&lt;/li&gt; &#xA; &lt;li&gt;Texify will OCR equations and surrounding text, but is not good for general purpose OCR. Think sections of a page instead of a whole page.&lt;/li&gt; &#xA; &lt;li&gt;Texify was mostly trained with 96 DPI images, and only at a max 420x420 resolution. Very wide or very tall images may not work well.&lt;/li&gt; &#xA; &lt;li&gt;It works best with English, although it should support other languages with similar character sets.&lt;/li&gt; &#xA; &lt;li&gt;The output format will be markdown with embedded LaTeX for equations (close to Github flavored markdown). It will not be pure LaTeX.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;p&gt;Benchmarking OCR quality is hard - you ideally need a parallel corpus that models haven&#39;t been trained on. I sampled from arxiv and im2latex to create the benchmark set.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/texify/master/data/images/texify_bench.png&#34; alt=&#34;Benchmark results&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Each model is trained on one of the benchmark tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nougat was trained on arxiv, possibly the images in the benchmark.&lt;/li&gt; &#xA; &lt;li&gt;Pix2tex was trained on im2latex.&lt;/li&gt; &#xA; &lt;li&gt;Texify was trained on im2latex. It was trained on arxiv, but not the images in the benchmark.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Although this makes the benchmark results biased, it does seem like a good compromise, since nougat and pix2tex don&#39;t work as well out of domain. Note that neither pix2tex or nougat is really designed for this task (OCR inline equations and text), so this is not a perfect comparison.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;BLEU ⬆&lt;/th&gt; &#xA;   &lt;th&gt;METEOR ⬆&lt;/th&gt; &#xA;   &lt;th&gt;Edit Distance ⬇&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pix2tex&lt;/td&gt; &#xA;   &lt;td&gt;0.382659&lt;/td&gt; &#xA;   &lt;td&gt;0.543363&lt;/td&gt; &#xA;   &lt;td&gt;0.352533&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nougat&lt;/td&gt; &#xA;   &lt;td&gt;0.697667&lt;/td&gt; &#xA;   &lt;td&gt;0.668331&lt;/td&gt; &#xA;   &lt;td&gt;0.288159&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;texify&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.842349&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.885731&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.0651534&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can benchmark the performance of texify on your machine.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the manual install instructions above.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use pix2tex, run &lt;code&gt;pip install pix2tex&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you want to use nougat, run &lt;code&gt;pip install nougat-ocr&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download the benchmark data &lt;a href=&#34;https://drive.google.com/file/d/1dbY0kBq2SUa885gmbLPUWSRzy5K7O5XJ/view?usp=sharing&#34;&gt;here&lt;/a&gt; and put it in the &lt;code&gt;data&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;benchmark.py&lt;/code&gt; like this:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark.py --max 100 --pix2tex --nougat --data_path data/bench_data.json --result_path data/bench_results.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will benchmark marker against pix2tex and nougat. It will do batch inference with texify and nougat, but not with pix2tex, since I couldn&#39;t find an option for batching.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; is how many benchmark images to convert at most.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path&lt;/code&gt; is the path to the benchmark data. If you omit this, it will use the default path.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--result_path&lt;/code&gt; is the path to the benchmark results. If you omit this, it will use the default path.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pix2tex&lt;/code&gt; specifies whether to run pix2tex (Latex-OCR) or not.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--nougat&lt;/code&gt; specifies whether to run nougat or not.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;p&gt;Texify was trained on latex images and paired equations from across the web. It includes the &lt;a href=&#34;https://github.com/guillaumegenthial/im2latex&#34;&gt;im2latex&lt;/a&gt; dataset. Training happened on 4x A6000s for 2 days (~6 epochs).&lt;/p&gt; &#xA;&lt;h1&gt;Commercial usage&lt;/h1&gt; &#xA;&lt;p&gt;This model is trained on top of the openly licensed &lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base&#34;&gt;Donut&lt;/a&gt; model, and thus can be used for commercial purposes. Model weights are licensed under the &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;CC BY-SA 4.0&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;This work would not have been possible without lots of amazing open source work. I particularly want to acknowledge &lt;a href=&#34;https://github.com/lukas-blecher&#34;&gt;Lukas Blecher&lt;/a&gt;, whose work on Nougat and pix2tex was key for this project. I learned a lot from his code, and used parts of it for texify.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guillaumegenthial/im2latex&#34;&gt;im2latex&lt;/a&gt; - one of the datasets used for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/naver-clova-ix/donut-base&#34;&gt;Donut&lt;/a&gt; from Naver, the base model for texify&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/nougat&#34;&gt;Nougat&lt;/a&gt; - I used the tokenizer from Nougat&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR&#34;&gt;Latex-OCR&lt;/a&gt; - The original open source Latex OCR project&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mnotgod96/AppAgent</title>
    <updated>2023-12-25T01:38:09Z</updated>
    <id>tag:github.com,2023-12-25:/mnotgod96/AppAgent</id>
    <link href="https://github.com/mnotgod96/AppAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A novel LLM-based multimodal agent framework designed to operate smartphone applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AppAgent&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.13771&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2312.13771-b31b1b.svg?sanitize=true&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://appagent-official.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://github.com/buaacyw/GaussianEditor/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA; &lt;!-- [![Model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue)](https://huggingface.co/listen2you002/ChartLlama-13b) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/listen2you002/ChartLlama-Dataset) --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://icoz69.github.io/&#34;&gt;&lt;strong&gt;Chi Zhang&lt;/strong&gt;*&lt;/a&gt;, &lt;a href=&#34;https://github.com/yz93&#34;&gt;&lt;strong&gt;Zhao Yang&lt;/strong&gt;*&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jiaxuan-liu-9051b7105/&#34;&gt;&lt;strong&gt;Jiaxuan Liu&lt;/strong&gt;*&lt;/a&gt;, &lt;a href=&#34;http://tingxueronghua.github.io&#34;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&#34;https://chenxin.tech/&#34;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Zebiao Huang&lt;/a&gt;, &lt;br&gt; &lt;a href=&#34;https://openreview.net/profile?id=~BIN_FU2&#34;&gt;Bin Fu&lt;/a&gt;, &lt;a href=&#34;https://www.skicyyu.org/&#34;&gt;Gang Yu (Corresponding Author)&lt;/a&gt; &lt;br&gt; (* equal contributions)&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mnotgod96/AppAgent/main/assets/teaser.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔆 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We introduce a novel LLM-based multimodal agent framework designed to operate smartphone applications.&lt;/p&gt; &#xA;&lt;p&gt;Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps.&lt;/p&gt; &#xA;&lt;p&gt;Central to our agent&#39;s functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications.&lt;/p&gt; &#xA;&lt;h2&gt;📝 Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12.21]&lt;/strong&gt;: 🔥🔥 Open-source the git repository, including the detailed configuration steps to implement our AppAgent!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✨ Demo&lt;/h2&gt; &#xA;&lt;p&gt;The demo video shows the process of using AppAgent to follow a user on X (Twitter) in the deployment phase.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mnotgod96/AppAgent/assets/40715314/db99d650-dec1-4531-b4b2-e085bfcadfb7&#34;&gt;https://github.com/mnotgod96/AppAgent/assets/40715314/db99d650-dec1-4531-b4b2-e085bfcadfb7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An interesting experiment showing AppAgent&#39;s ability to pass CAPTCHA.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mnotgod96/AppAgent/assets/27103154/5cc7ba50-dbab-42a0-a411-a9a862482548&#34;&gt;https://github.com/mnotgod96/AppAgent/assets/27103154/5cc7ba50-dbab-42a0-a411-a9a862482548&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;This section will guide you on how to quickly use gpt-4-vision-preview as an agent to complete specific tasks for you on your Android app.&lt;/p&gt; &#xA;&lt;h3&gt;⚙️ Step 1. Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Get an Android device and enable the USB debugging that can be found in Developer Options in Settings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On your PC, download and install &lt;a href=&#34;https://developer.android.com/tools/adb&#34;&gt;Android Debug Bridge&lt;/a&gt; (adb) which is a command-line tool that lets you communicate with your Android device from the PC.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Connect your device to your PC using a USB cable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repo and install the dependencies. All scripts in this project are written in Python 3 so make sure you have installed it.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd AppAgent&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🤖 Step 2. Configure the Agent&lt;/h3&gt; &#xA;&lt;p&gt;AppAgent needs to be powered by a multi-modal model which can receive both text and visual inputs. During our experiment , we used &lt;code&gt;gpt-4-vision-preview&lt;/code&gt; as the model to make decisions on how to take actions to complete a task on the smartphone.&lt;/p&gt; &#xA;&lt;p&gt;To configure your requests to GPT-4V, you should modify &lt;code&gt;config.yaml&lt;/code&gt; in the root directory. There are two key parameters that must be configured to try AppAgent:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;OpenAI API key: you must purchase an eligible API key from OpenAI so that you can have access to GPT-4V.&lt;/li&gt; &#xA; &lt;li&gt;Request interval: this is the time interval in seconds between consecutive GPT-4V requests to control the frequency of your requests to GPT-4V. Adjust this value according to the status of your account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Other parameters in &lt;code&gt;config.yaml&lt;/code&gt; are well commented. Modify them as you need.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Be aware that GPT-4V is not free. Each request/response pair involved in this project costs around $0.03. Use it wisely.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you want to test AppAgent using your own models, you should modify the &lt;code&gt;ask_gpt_4v&lt;/code&gt; function in &lt;code&gt;scripts/model.py&lt;/code&gt; accordingly.&lt;/p&gt; &#xA;&lt;h3&gt;🔍 Step 3. Exploration Phase&lt;/h3&gt; &#xA;&lt;p&gt;Our paper proposed a novel solution that involves two phases, exploration and deployment, to turn GPT-4V into a capable agent that can help users operate their Android phones when a task is given. The exploration phase starts with a task given by you, and you can choose to let the agent either explore the app on its own or learn from your demonstration. In both cases, the agent generates documentations for elements interacted during the exploration/demonstration and saves them for use in the deployment phase.&lt;/p&gt; &#xA;&lt;h4&gt;Option 1: Autonomous Exploration&lt;/h4&gt; &#xA;&lt;p&gt;This solution features a fully autonomous exploration which allows the agent to explore the use of the app by attempting the given task without any intervention from humans.&lt;/p&gt; &#xA;&lt;p&gt;To start, run &lt;code&gt;learn.py&lt;/code&gt; in the root directory. Follow prompted instructions to select &lt;code&gt;autonomous exploration&lt;/code&gt; as the operating mode and provide the app name and task description. Then, your agent will do the job for you. Under this mode, AppAgent will reflect on its previous action making sure its action adheres to the given task and generate documentations for the elements explored.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python learn.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Option 2: Learning from Human Demonstrations&lt;/h4&gt; &#xA;&lt;p&gt;This solution requires users to demonstrate a similar task first. AppAgent will learn from the demo and generate documentations for UI elements seen during the demo.&lt;/p&gt; &#xA;&lt;p&gt;To start human demonstration, you should run &lt;code&gt;learn.py&lt;/code&gt; in the root directory. Follow prompted instructions to select &lt;code&gt;human demonstration&lt;/code&gt; as the operating mode and provide the app name and task description. A screenshot of your phone will be captured and all interactive elements shown on the screen will be labeled with numeric tags. You need to follow the prompts to determine your next action and the target of the action. When you believe the demonstration is finished, type &lt;code&gt;stop&lt;/code&gt; to end the demo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python learn.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mnotgod96/AppAgent/main/assets/demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;📱 Step 4. Deployment Phase&lt;/h3&gt; &#xA;&lt;p&gt;After the exploration phase finishes, you can run &lt;code&gt;run.py&lt;/code&gt; in the root directory. Follow prompted instructions to enter the name of the app, select the appropriate documentation base you want the agent to use, and provide the task description. Then, your agent will do the job for you. The agent will automatically detect if there is documentation base generated before for the app; if there is no documentation found, you can also choose to run the agent without any documentation (success rate not guaranteed).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📖 TO-DO LIST&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Open source the Benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Open source the configuration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;😉 Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@misc{yang2023appagent,&#xA;      title={AppAgent: Multimodal Agents as Smartphone Users}, &#xA;      author={Chi Zhang and Zhao Yang and Jiaxuan Liu and Yucheng Han and Xin Chen and Zebiao Huang and Bin Fu and Gang Yu},&#xA;      year={2023},&#xA;      eprint={2312.13771},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/LMOps</title>
    <updated>2023-12-25T01:38:09Z</updated>
    <id>tag:github.com,2023-12-25:/microsoft/LMOps</id>
    <link href="https://github.com/microsoft/LMOps" rel="alternate"></link>
    <summary type="html">&lt;p&gt;General technology for enabling AI capabilities w/ LLMs and MLLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LMOps&lt;/h1&gt; &#xA;&lt;p&gt;LMOps is a research initiative on fundamental research and technology for building AI products w/ foundation models, especially on the general technology for enabling AI capabilities w/ LLMs and Generative AI models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Better Prompts: &lt;a href=&#34;https://arxiv.org/abs/2305.03495&#34;&gt;Automatic Prompt Optimization&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2212.09611&#34;&gt;Promptist&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2212.00616&#34;&gt;Extensible prompts&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2303.08518&#34;&gt;Universal prompt retrieval&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2307.07164&#34;&gt;LLM Retriever&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2305.14726&#34;&gt;In-Context Demonstration Selection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Longer Context: &lt;a href=&#34;https://arxiv.org/abs/2212.06713&#34;&gt;Structured prompting&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2212.10554&#34;&gt;Length-Extrapolatable Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLM Alignment: &lt;a href=&#34;&#34;&gt;Alignment via LLM feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLM Accelerator (Faster Inference): &lt;a href=&#34;https://arxiv.org/abs/2304.04487&#34;&gt;Lossless Acceleration of LLMs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLM Customization: &lt;a href=&#34;https://arxiv.org/pdf/2309.09530.pdf&#34;&gt;Adapt LLM to domains&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fundamentals: &lt;a href=&#34;https://arxiv.org/abs/2212.10559&#34;&gt;Understanding In-Context Learning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;microsoft/unilm&lt;/a&gt;: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/torchscale&#34;&gt;microsoft/torchscale&lt;/a&gt;: Transformers at (any) Scale&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Paper Release] Nov, 2023: &lt;a href=&#34;https://arxiv.org/abs/2305.14726&#34;&gt;In-Context Demonstration Selection with Cross Entropy Difference&lt;/a&gt; (EMNLP 2023)&lt;/li&gt; &#xA; &lt;li&gt;[Paper Release] Oct, 2023: &lt;a href=&#34;https://arxiv.org/pdf/2310.13385.pdf&#34;&gt;Tuna: Instruction Tuning using Feedback from Large Language Models&lt;/a&gt; (EMNLP 2023)&lt;/li&gt; &#xA; &lt;li&gt;[Paper Release] Oct, 2023: &lt;a href=&#34;https://arxiv.org/abs/2305.03495&#34;&gt;Automatic Prompt Optimization with &#34;Gradient Descent&#34; and Beam Search&lt;/a&gt; (EMNLP 2023)&lt;/li&gt; &#xA; &lt;li&gt;[Paper Release] Oct, 2023: &lt;a href=&#34;https://arxiv.org/abs/2303.08518&#34;&gt;UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation&lt;/a&gt; (EMNLP 2023)&lt;/li&gt; &#xA; &lt;li&gt;[Paper Release] July, 2023: &lt;a href=&#34;https://arxiv.org/abs/2307.07164&#34;&gt;Learning to Retrieve In-Context Examples for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Paper Release] April, 2023: &lt;a href=&#34;https://arxiv.org/abs/2304.04487&#34;&gt;Inference with Reference: Lossless Acceleration of Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Paper Release] Dec, 2022: &lt;a href=&#34;https://arxiv.org/abs/2212.10559&#34;&gt;Why Can GPT Learn In-Context? Language Models Secretly Perform Finetuning as Meta Optimizers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Paper &amp;amp; Model &amp;amp; Demo Release] Dec, 2022: &lt;a href=&#34;https://aka.ms/promptist&#34;&gt;Optimizing Prompts for Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Paper &amp;amp; Code Release] Dec, 2022: &lt;a href=&#34;https://arxiv.org/abs/2212.06713&#34;&gt;Structured Prompting: Scaling In-Context Learning to 1,000 Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[Paper Release] Nov, 2022: &lt;a href=&#34;https://arxiv.org/abs/2212.00616&#34;&gt;Extensible Prompts for Language Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prompt Intelligence&lt;/h2&gt; &#xA;&lt;p&gt;Advanced technologies facilitating prompting language models.&lt;/p&gt; &#xA;&lt;h3&gt;Promptist: reinforcement learning for automatic prompt optimization&lt;/h3&gt; &#xA;&lt;p&gt;[Paper] &lt;a href=&#34;https://arxiv.org/abs/2212.09611&#34;&gt;Optimizing Prompts for Text-to-Image Generation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Language models serve as a prompt interface that optimizes user input into model-preferred prompts.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Learn a language model for automatic prompt optimization via reinforcement learning.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1070872/207856962-02f08d92-f2bf-441a-b1c3-efff1a4b6187.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Structured Prompting: consume long-sequence prompts in an efficient way&lt;/h3&gt; &#xA;&lt;p&gt;[Paper] &lt;a href=&#34;https://arxiv.org/abs/2212.06713&#34;&gt;Structured Prompting: Scaling In-Context Learning to 1,000 Examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example use cases:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Prepend (many) retrieved (long) documents as context in GPT.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Scale in-context learning to many demonstration examples.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1070872/207856629-2bb0c933-c27b-4177-9e10-e397622ae79b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;X-Prompt: extensible prompts beyond NL for descriptive instructions&lt;/h3&gt; &#xA;&lt;p&gt;[Paper] &lt;a href=&#34;https://arxiv.org/abs/2212.00616&#34;&gt;Extensible Prompts for Language Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Extensible interface allowing prompting LLMs beyond natural language for fine-grain specifications&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Context-guided imaginary word learning for general usability&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1070872/207856788-5409d04d-c406-4b29-ae7b-2732e727d4cc.png&#34; alt=&#34;Extensible Prompts for Language Models&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;LLMA: LLM Accelerators&lt;/h2&gt; &#xA;&lt;h3&gt;Accelerate LLM Inference with References&lt;/h3&gt; &#xA;&lt;p&gt;[Paper] &lt;a href=&#34;https://arxiv.org/abs/2304.04487&#34;&gt;Inference with Reference: Lossless Acceleration of Large Language Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Outputs of LLMs often have significant overlaps with some references (e.g., retrieved documents).&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;LLMA losslessly accelerate the inference of LLMs by copying and verifying text spans from references into the LLM inputs.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Applicable to important LLM scenarios such as retrieval-augmented generation and multi-turn conversations.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Achieves 2~3 times speed-up without additional models.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6700539/231664563-aec35679-b4ab-4b6b-b6b4-b2b4ea1aab53.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fundamental Understanding of LLMs&lt;/h2&gt; &#xA;&lt;h3&gt;Understanding In-Context Learning&lt;/h3&gt; &#xA;&lt;p&gt;[Paper] &lt;a href=&#34;https://arxiv.org/abs/2212.10559&#34;&gt;Why Can GPT Learn In-Context? Language Models Secretly Perform Finetuning as Meta Optimizers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;According to the demonstration examples, GPT produces meta gradients for In-Context Learning (ICL) through forward computation. ICL works by applying these meta gradients to the model through attention.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;The meta optimization process of ICL shares a dual view with finetuning that explicitly updates the model parameters with back-propagated gradients.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;We can translate optimization algorithms (such as SGD with Momentum) to their corresponding Transformer architectures.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1070872/208835096-54407f5f-d136-4747-9629-3219988df5d4.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Hiring: &lt;a href=&#34;https://aka.ms/GeneralAI&#34;&gt;aka.ms/GeneralAI&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and AGI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to &lt;a href=&#34;mailto:fuwei@microsoft.com&#34; class=&#34;x-hidden-focus&#34;&gt;&lt;/a&gt;&lt;a href=&#34;mailto:fuwei@microsoft.com&#34;&gt;fuwei@microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the license found in the LICENSE file in the root directory of this source tree.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.microsoft.com/codeofconduct&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Contact Information&lt;/h3&gt; &#xA;&lt;p&gt;For help or issues using the pre-trained models, please submit a GitHub issue. For other communications, please contact &lt;a href=&#34;http://gitnlp.org/&#34;&gt;Furu Wei&lt;/a&gt; (&lt;code&gt;fuwei@microsoft.com&lt;/code&gt;).&lt;/p&gt;</summary>
  </entry>
</feed>