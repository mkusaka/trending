<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-26T01:33:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/Video-Pre-Training</title>
    <updated>2022-06-26T01:33:24Z</updated>
    <id>tag:github.com,2022-06-26:/openai/Video-Pre-Training</id>
    <link href="https://github.com/openai/Video-Pre-Training" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Video-Pre-Training&lt;/h1&gt; &#xA;&lt;p&gt;Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üìÑ&lt;/span&gt; &lt;a href=&#34;https://cdn.openai.com/vpt/Paper.pdf&#34;&gt;Read Paper&lt;/a&gt; &lt;br&gt; &lt;span&gt;üì£&lt;/span&gt; &lt;a href=&#34;https://openai.com/blog/vpt&#34;&gt;Blog Post&lt;/a&gt; &lt;br&gt; &lt;span&gt;üëæ&lt;/span&gt; &lt;a href=&#34;https://github.com/minerllabs/minerl&#34;&gt;MineRL Environment&lt;/a&gt; (note version 1.0+ required) &lt;br&gt; &lt;span&gt;üèÅ&lt;/span&gt; &lt;a href=&#34;https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition&#34;&gt;MineRL BASALT Competition&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Running models&lt;/h2&gt; &#xA;&lt;p&gt;Install requirements with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/minerllabs/minerl@v1.0.0&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the code, call&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_agent.py --model [path to .model file] --weights [path to .weight file]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After loading up, you should see a window of the agent playing Minecraft.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;Below are the model files and weights files for various pre-trained Minecraft models. The 1x, 2x and 3x model files correspond to their respective model weights width.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.model&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 1x Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/2x.model&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 2x Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-3x.model&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 3x Model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Demonstration Only - Behavioral Cloning&lt;/h3&gt; &#xA;&lt;p&gt;These models are trained on video demonstrations of humans playing Minecraft using behavioral cloning (BC) and are more general than later models which use reinforcement learning (RL) to further optimize the policy. Foundational models are trained across all videos in a single training run while house and early game models refine their respective size foundational model further using either the housebuilding contractor data or early game video sub-set. See the paper linked above for more details.&lt;/p&gt; &#xA;&lt;h4&gt;Foundational Model &lt;span&gt;üìà&lt;/span&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 1x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-2x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 2x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-3x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 3x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Fine-Tuned from House &lt;span&gt;üìà&lt;/span&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-house-3x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 3x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Fine-Tuned from Early Game &lt;span&gt;üìà&lt;/span&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-2x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 2x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-3x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 3x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Models With Environment Interactions&lt;/h3&gt; &#xA;&lt;p&gt;These models further refine the above demonstration based models with a reward function targeted at obtaining diamond pickaxes. While less general then the behavioral cloning models, these models have the benefit of interacting with the environment using a reward function and excel at progressing through the tech tree quickly. See the paper for more information on how they were trained and the exact reward schedule.&lt;/p&gt; &#xA;&lt;h4&gt;RL from Foundation &lt;span&gt;üìà&lt;/span&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-foundation-2x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 2x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;RL from House &lt;span&gt;üìà&lt;/span&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-house-2x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 2x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;RL from Early Game &lt;span&gt;üìà&lt;/span&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-early-game-2x.weights&#34;&gt;&lt;span&gt;‚¨á&lt;/span&gt; 2x Width Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contractor Demonstrations Dataset&lt;/h2&gt; &#xA;&lt;p&gt;We are currently working on to release contractor data collected over the course of the project. Links to index files with more information will be linked here as the data is released.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;This was a large effort by a dedicated team at OpenAI: &lt;a href=&#34;https://github.com/bowenbaker&#34;&gt;Bowen Baker&lt;/a&gt;, &lt;a href=&#34;https://github.com/ilge&#34;&gt;Ilge Akkaya&lt;/a&gt;, &lt;a href=&#34;https://github.com/pzhokhov&#34;&gt;Peter Zhokhov&lt;/a&gt;, &lt;a href=&#34;https://github.com/JoostHuizinga&#34;&gt;Joost Huizinga&lt;/a&gt;, &lt;a href=&#34;https://github.com/jietang&#34;&gt;Jie Tang&lt;/a&gt;, &lt;a href=&#34;https://github.com/AdrienLE&#34;&gt;Adrien Ecoffet&lt;/a&gt;, &lt;a href=&#34;https://github.com/brandonhoughton&#34;&gt;Brandon Houghton&lt;/a&gt;, &lt;a href=&#34;https://github.com/raul-openai&#34;&gt;Raul Sampedro&lt;/a&gt;, Jeff Clune The code here represents a minimal version of our model code which was prepared by &lt;a href=&#34;https://github.com/miffyli&#34;&gt;Anssi Kanervisto&lt;/a&gt; and others so that these models could be used as part of the MineRL BASALT competition.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CFandR-github/PHP-binary-bugs</title>
    <updated>2022-06-26T01:33:24Z</updated>
    <id>tag:github.com,2022-06-26:/CFandR-github/PHP-binary-bugs</id>
    <link href="https://github.com/CFandR-github/PHP-binary-bugs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PHP binary bugs advisory&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Advisory of Exploits AI POP Builder&lt;/h1&gt; &#xA;&lt;p&gt;Collection of PHP binary bugs advisory&lt;/p&gt; &#xA;&lt;h3&gt;Unfixed GMP Type confusion in unserialize&lt;/h3&gt; &#xA;&lt;p&gt;Idea: bypass delayed __wakeup and exploit unfixed GMP type confusion bug in PHP &amp;lt;= 5.6.40&lt;/p&gt; &#xA;&lt;p&gt;POC source: &lt;a href=&#34;https://raw.githubusercontent.com/CFandR-github/PHP-binary-bugs/main/GMP_type_conf_unserialize/GMP_type_conf_POC.php&#34;&gt;GMP_type_conf_POC.php&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CFandR-github/PHP-binary-bugs/main/GMP_type_conf_unserialize/GMP_type_conf_advisory.md&#34;&gt;Advisory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;CVE-2022-31626 analysis&lt;/h3&gt; &#xA;&lt;p&gt;Idea: heap buffer overflow in mysqlnd, PHP &amp;lt;= 7.4.29&lt;/p&gt; &#xA;&lt;p&gt;POC source: &lt;a href=&#34;https://raw.githubusercontent.com/CFandR-github/PHP-binary-bugs/main/cve_2022_31626_remote_exploit/exploit_poc.py&#34;&gt;./cve_2022_31626_remote_exploit/exploit_poc.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CFandR-github/PHP-binary-bugs/main/cve_2022_31626_remote_exploit/cve_writeup.md&#34;&gt;Advisory&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contacts&lt;/h1&gt; &#xA;&lt;p&gt;Project channel in Telegram:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://t.me/CFandR_project&#34;&gt;https://t.me/CFandR_project&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset</title>
    <updated>2022-06-26T01:33:24Z</updated>
    <id>tag:github.com,2022-06-26:/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset</id>
    <link href="https://github.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository provides motion datasets collected by Bandai Namco Research Inc&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bandai-Namco-Research-Motiondataset&lt;/h1&gt; &#xA;&lt;p&gt;This repository provides motion datasets collected by Bandai Namco Research Inc.&lt;/p&gt; &#xA;&lt;p&gt;Find &lt;a href=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/README_Japanese.md&#34;&gt;here&lt;/a&gt; for a READEME in Japanese.&lt;/p&gt; &#xA;&lt;p&gt;There is a long-standing interest in making diverse stylized motions for games and movies that pursue realistic and expressive character animation; however, creating new movements that include all the various styles of expression using existing methods is difficult. Due to this, Motion Style Transfer (MST) has been drawing attention recently, which aims to convert the motion in a clip with a given content into another motion in a different style, while keeping the same content. A motion is composed of a content and style, where content is the base of the motion and style comprises of the attributes such as mood and personality of the character tied to the motion.&lt;/p&gt; &#xA;&lt;p&gt;The datasets contain a diverse range of contents such as daily activities, fighting, and dancing; with styles such as active, tired, and happy. These can be used as training data for MST models. The animation below shows examples of visualized motions.&lt;/p&gt; &#xA;&lt;p&gt;Currently, two datasets are available in this repository and are both located under the &lt;code&gt;dataset&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bandai-Namco-Research-Motiondataset-1&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/dataset/Bandai-Namco-Research-Motiondataset-1/README.md&#34;&gt;Details&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;17 types of wide-range contents including daily activities, fighting, and dancing.&lt;/li&gt; &#xA;   &lt;li&gt;15 styles that include expression variety.&lt;/li&gt; &#xA;   &lt;li&gt;A total of 36,673 frames.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/src/Bandai-Namco-Research-Motiondataset-1/movie_walk.gif&#34; width=&#34;100%&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bandai-Namco-Research-Motiondataset-2&lt;/strong&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/dataset/Bandai-Namco-Research-Motiondataset-2/README.md&#34;&gt;Details&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;10 types of content mainly focusing on locomotion and hand actions.&lt;/li&gt; &#xA;   &lt;li&gt;7 styles that use a single, uniform expression.&lt;/li&gt; &#xA;   &lt;li&gt;A total of 384,931 frames.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/src/Bandai-Namco-Research-Motiondataset-2/movie_walk.gif&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p&gt;You can find a sample script for visualization (using Blender) including step-by-step instructions under the &lt;code&gt;utils&lt;/code&gt; directory. Please see &lt;a href=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/utils/blender/README.md&#34;&gt;here&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Data Collection&lt;/h2&gt; &#xA;&lt;p&gt;Each dataset is based on the motion of three professional actors, collected at the motion capture studio of Bandai Namco. We applied post-processing such as noise removal, proportion alignment, and clipping; and saved in BVH format.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/src/images/motion_capture_studio.png&#34; width=&#34;50%&#34;&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The datasets and scripts are available in the following lincenses.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bandai-Namco-Research-Motiondataset-1: &lt;a href=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/dataset/Bandai-Namco-Research-Motiondataset-1/LICENSE&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bandai-Namco-Research-Motiondataset-2: &lt;a href=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/dataset/Bandai-Namco-Research-Motiondataset-2/LICENSE&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Motion visualization on Blender: &lt;a href=&#34;https://raw.githubusercontent.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/master/utils/blender/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related works&lt;/h2&gt; &#xA;&lt;p&gt;As mentioned previously, the field of MST has been drawing attention as of late. If you are interested in this topic, please refer to the following non-exhaustive list of research and datasets. The models introduced in these papers show a high-level of performance and the datasets related to MST provide a wide variety of motion data.&lt;/p&gt; &#xA;&lt;h3&gt;Papers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Jurnal of Sensors 2021) A Cyclic Consistency Motion Style Transfer Method Combined with Kinematic Constraints &lt;a href=&#34;https://www.hindawi.com/journals/js/2021/5548614/&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(CVPR 2021) Autoregressive Stylized Motion Synthesis with Generative Flow &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Wen_Autoregressive_Stylized_Motion_Synthesis_With_Generative_Flow_CVPR_2021_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(CVPR 2021) Understanding Object Dynamics for Interactive Image-to-Video Synthesis &lt;a href=&#34;https://ieeexplore.ieee.org/document/9577842&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(ACM Trans. Graph 2020) Unpaired Motion Style Transfer from Video to Animation &lt;a href=&#34;https://deepmotionediting.github.io/papers/Motion_Style_Transfer-camera-ready.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deep-motion-editing &lt;a href=&#34;https://github.com/DeepMotionEditing/deep-motion-editing&#34;&gt;[GitHub]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ubisoft La Forge Animation Dataset &lt;a href=&#34;https://github.com/ubisoft/ubisoft-laforge-animation-dataset&#34;&gt;[GitHub]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;¬© [2022] Bandai Namco Research Inc. All Rights Reserved&lt;/p&gt;</summary>
  </entry>
</feed>