<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-09T01:39:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sd-webui/stable-diffusion-webui</title>
    <updated>2022-09-09T01:39:12Z</updated>
    <id>tag:github.com,2022-09-09:/sd-webui/stable-diffusion-webui</id>
    <link href="https://github.com/sd-webui/stable-diffusion-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion web UI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/altryne/sd-webui-colab/blob/main/Stable_Diffusion_WebUi_Altryne.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Installation&#34;&gt;Installation&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;Have an &lt;strong&gt;issue&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If the issue involves &lt;em&gt;a bug&lt;/em&gt; in &lt;strong&gt;textual-inversion&lt;/strong&gt; create the issue on &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui&#34;&gt;sd-webui/stable-diffusion-webui&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you want to know how to &lt;strong&gt;activate&lt;/strong&gt; or &lt;strong&gt;use&lt;/strong&gt; textual-inversion see &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/hlky/sd-enable-textual-inversion&#34;&gt;hlky/sd-enable-textual-inversion&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;. Activation not working? create the issue on &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui&#34;&gt;sd-webui/stable-diffusion-webui&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More documentation about features, troubleshooting, common issues very soon&lt;/h2&gt; &#xA;&lt;h3&gt;Want to help with documentation? Documented something? Use &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Important&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;🔥 NEW! webui.cmd updates with any changes in environment.yaml file so the environment will always be up to date as long as you get the new environment.yaml file 🔥&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;🔥&lt;/span&gt; no need to remove environment, delete src folder and create again, MUCH simpler! 🔥&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Upscalers&#34;&gt;Upscalers&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Optimized-mode&#34;&gt;Optimized mode&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;h3&gt;Questions about &lt;strong&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/wiki/Command-line-options&#34;&gt;Command line options&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio GUI: Idiot-proof, fully featured frontend for both txt2img and img2img generation&lt;/li&gt; &#xA; &lt;li&gt;No more manually typing parameters, now all you have to do is write your prompt and adjust sliders&lt;/li&gt; &#xA; &lt;li&gt;GFPGAN Face Correction 🔥: &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui#gfpgan&#34;&gt;Download the model&lt;/a&gt;Automatically correct distorted faces with a built-in GFPGAN option, fixes them in less than half a second&lt;/li&gt; &#xA; &lt;li&gt;RealESRGAN Upscaling 🔥: &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui#realesrgan&#34;&gt;Download the models&lt;/a&gt; Boosts the resolution of images with a built-in RealESRGAN option&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;💻&lt;/span&gt; esrgan/gfpgan on cpu support &lt;span&gt;💻&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Textual inversion 🔥: &lt;a href=&#34;https://textual-inversion.github.io/&#34;&gt;info&lt;/a&gt; - requires enabling, see &lt;a href=&#34;https://github.com/hlky/sd-enable-textual-inversion&#34;&gt;here&lt;/a&gt;, script works as usual without it enabled&lt;/li&gt; &#xA; &lt;li&gt;Advanced img2img editor &lt;span&gt;🎨&lt;/span&gt; &lt;span&gt;🔥&lt;/span&gt; &lt;span&gt;🎨&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt; Mask and crop &lt;span&gt;🔥&lt;/span&gt;&lt;span&gt;🔥&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mask painting (NEW) 🖌️: Powerful tool for re-generating only specific parts of an image you want to change&lt;/li&gt; &#xA; &lt;li&gt;More k_diffusion samplers 🔥🔥 : Far greater quality outputs than the default sampler, less distortion and more accurate&lt;/li&gt; &#xA; &lt;li&gt;txt2img samplers: &#34;DDIM&#34;, &#34;PLMS&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/li&gt; &#xA; &lt;li&gt;img2img samplers: &#34;DDIM&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/li&gt; &#xA; &lt;li&gt;Loopback (NEW) ➿: Automatically feed the last generated sample back into img2img&lt;/li&gt; &#xA; &lt;li&gt;Prompt Weighting (NEW) 🏋️: Adjust the strength of different terms in your prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔥&lt;/span&gt; gpu device selectable with --gpu &#xA;  &lt;id&gt; &#xA;   &lt;span&gt;🔥&lt;/span&gt;&#xA;  &lt;/id&gt;&lt;/li&gt; &#xA; &lt;li&gt;Memory Monitoring 🔥: Shows Vram usage and generation time after outputting.&lt;/li&gt; &#xA; &lt;li&gt;Word Seeds 🔥: Use words instead of seed numbers&lt;/li&gt; &#xA; &lt;li&gt;CFG: Classifier free guidance scale, a feature for fine-tuning your output&lt;/li&gt; &#xA; &lt;li&gt;Launcher Automatic 👑🔥 shortcut to load the model, no more typing in Conda&lt;/li&gt; &#xA; &lt;li&gt;Lighter on Vram: 512x512 img2img &amp;amp; txt2img tested working on 6gb&lt;/li&gt; &#xA; &lt;li&gt;and ????&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Stable Diffusion web UI&lt;/h1&gt; &#xA;&lt;p&gt;A browser interface based on Gradio library for Stable Diffusion.&lt;/p&gt; &#xA;&lt;p&gt;Original script with Gradio UI was written by a kind anonymous user. This is a modification.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/txt2img.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/img2img.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/gfpgan.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/sd-webui/stable-diffusion-webui/raw/master/images/esrgan.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GFPGAN&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use GFPGAN to improve generated faces, you need to install it separately. Download &lt;a href=&#34;https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth&#34;&gt;GFPGANv1.3.pth&lt;/a&gt; and put it into the &lt;code&gt;/stable-diffusion-webui/src/gfpgan/experiments/pretrained_models&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;RealESRGAN&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth&#34;&gt;RealESRGAN_x4plus.pth&lt;/a&gt; and &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth&#34;&gt;RealESRGAN_x4plus_anime_6B.pth&lt;/a&gt;. Put them into the &lt;code&gt;stable-diffusion-webui/src/realesrgan/experiments/pretrained_models&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;LDSR&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;strong&gt;LDSR&lt;/strong&gt; &lt;a href=&#34;https://heibox.uni-heidelberg.de/f/31a76b13ea27482981b4/?dl=1&#34;&gt;project.yaml&lt;/a&gt; and &lt;a href=&#34;https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1&#34;&gt; model last.cpkt&lt;/a&gt;. Rename last.ckpt to model.ckpt and place both under stable-diffusion-webui/src/latent-diffusion/experiments/pretrained_models/&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;When launching, you may get a very long warning message related to some weights not being used. You may freely ignore it. After a while, you will get a message like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Running on local URL:  http://127.0.0.1:7860/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open the URL in browser, and you are good to go.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The script creates a web UI for Stable Diffusion&#39;s txt2img and img2img scripts. Following are features added that are not in original script.&lt;/p&gt; &#xA;&lt;h3&gt;GFPGAN&lt;/h3&gt; &#xA;&lt;p&gt;Lets you improve faces in pictures using the GFPGAN model. There is a checkbox in every tab to use GFPGAN at 100%, and also a separate tab that just allows you to use GFPGAN on any picture, with a slider that controls how strongthe effect is.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/GFPGAN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;RealESRGAN&lt;/h3&gt; &#xA;&lt;p&gt;Lets you double the resolution of generated images. There is a checkbox in every tab to use RealESRGAN, and you can choose between the regular upscaler and the anime version. There is also a separate tab for using RealESRGAN on any picture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/RealESRGAN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Sampling method selection&lt;/h3&gt; &#xA;&lt;p&gt;txt2img samplers: &#34;DDIM&#34;, &#34;PLMS&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39; img2img samplers: &#34;DDIM&#34;, &#39;k_dpm_2_a&#39;, &#39;k_dpm_2&#39;, &#39;k_euler_a&#39;, &#39;k_euler&#39;, &#39;k_heun&#39;, &#39;k_lms&#39;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/sampling.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prompt matrix&lt;/h3&gt; &#xA;&lt;p&gt;Separate multiple prompts using the &lt;code&gt;|&lt;/code&gt; character, and the system will produce an image for every combination of them. For example, if you use &lt;code&gt;a busy city street in a modern city|illustration|cinematic lighting&lt;/code&gt; prompt, there are four combinations possible (first part of prompt is always kept):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, illustration&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, cinematic lighting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;a busy city street in a modern city, illustration, cinematic lighting&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Four images will be produced, in this order, all with same seed and each with corresponding prompt: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/prompt-matrix.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Another example, this time with 5 prompts and 16 variations: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/prompt_matrix.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use this feature, batch count will be ignored, because the number of pictures to produce depends on your prompts, but batch size will still work (generating multiple pictures at the same time for a small speed boost).&lt;/p&gt; &#xA;&lt;h3&gt;Flagging (Broken after UI changed to gradio.Blocks() see &lt;a href=&#34;https://github.com/sd-webui/stable-diffusion-webui/issues/50&#34;&gt;Flag button missing from new UI&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;Click the Flag button under the output section, and generated images will be saved to &lt;code&gt;log/images&lt;/code&gt; directory, and generation parameters will be appended to a csv file &lt;code&gt;log/log.csv&lt;/code&gt; in the &lt;code&gt;/sd&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;but every image is saved, why would I need this?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you&#39;re like me, you experiment a lot with prompts and settings, and only few images are worth saving. You can just save them using right click in browser, but then you won&#39;t be able to reproduce them later because you will not know what exact prompt created the image. If you use the flag button, generation paramerters will be written to csv file, and you can easily find parameters for an image by searching for its filename.&lt;/p&gt; &#xA;&lt;h3&gt;Copy-paste generation parameters&lt;/h3&gt; &#xA;&lt;p&gt;A text output provides generation parameters in an easy to copy-paste form for easy sharing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/kopipe.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you generate multiple pictures, the displayed seed will be the seed of the first one.&lt;/p&gt; &#xA;&lt;h3&gt;Correct seeds for batches&lt;/h3&gt; &#xA;&lt;p&gt;If you use a seed of 1000 to generate two batches of two images each, four generated images will have seeds: &lt;code&gt;1000, 1001, 1002, 1003&lt;/code&gt;. Previous versions of the UI would produce &lt;code&gt;1000, x, 1001, x&lt;/code&gt;, where x is an iamge that can&#39;t be generated by any seed.&lt;/p&gt; &#xA;&lt;h3&gt;Resizing&lt;/h3&gt; &#xA;&lt;p&gt;There are three options for resizing input images in img2img mode:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Just resize - simply resizes source image to target resolution, resulting in incorrect aspect ratio&lt;/li&gt; &#xA; &lt;li&gt;Crop and resize - resize source image preserving aspect ratio so that entirety of target resolution is occupied by it, and crop parts that stick out&lt;/li&gt; &#xA; &lt;li&gt;Resize and fill - resize source image preserving aspect ratio so that it entirely fits target resolution, and fill empty space by rows/columns from source image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example: &lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/resizing.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Loading&lt;/h3&gt; &#xA;&lt;p&gt;Gradio&#39;s loading graphic has a very negative effect on the processing speed of the neural network. My RTX 3090 makes images about 10% faster when the tab with gradio is not active. By default, the UI now hides loading progress animation and replaces it with static &#34;Loading...&#34; text, which achieves the same effect. Use the --no-progressbar-hiding commandline option to revert this and show loading animations.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt validation&lt;/h3&gt; &#xA;&lt;p&gt;Stable Diffusion has a limit for input text length. If your prompt is too long, you will get a warning in the text output field, showing which parts of your text were truncated and ignored by the model.&lt;/p&gt; &#xA;&lt;h3&gt;Loopback&lt;/h3&gt; &#xA;&lt;p&gt;A checkbox for img2img allowing to automatically feed output image as input for the next batch. Equivalent to saving output image, and replacing input image with it. Batch count setting controls how many iterations of this you get.&lt;/p&gt; &#xA;&lt;p&gt;Usually, when doing this, you would choose one of many images for the next iteration yourself, so the usefulness of this feature may be questionable, but I&#39;ve managed to get some very nice outputs with it that I wasn&#39;t abble to get otherwise.&lt;/p&gt; &#xA;&lt;p&gt;Example: (cherrypicked result; original picture by anon)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/images/loopback.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;--help&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --outdir [OUTDIR]     dir to write results to&#xA;  --outdir_txt2img [OUTDIR_TXT2IMG]&#xA;                        dir to write txt2img results to (overrides --outdir)&#xA;  --outdir_img2img [OUTDIR_IMG2IMG]&#xA;                        dir to write img2img results to (overrides --outdir)&#xA;  --save-metadata       Whether to embed the generation parameters in the sample images&#xA;  --skip-grid           do not save a grid, only individual samples. Helpful when evaluating lots of samples&#xA;  --skip-save           do not save indiviual samples. For speed measurements.&#xA;  --n_rows N_ROWS       rows in the grid; use -1 for autodetect and 0 for n_rows to be same as batch_size (default:&#xA;                        -1)&#xA;  --config CONFIG       path to config which constructs model&#xA;  --ckpt CKPT           path to checkpoint of model&#xA;  --precision {full,autocast}&#xA;                        evaluate at this precision&#xA;  --gfpgan-dir GFPGAN_DIR&#xA;                        GFPGAN directory&#xA;  --realesrgan-dir REALESRGAN_DIR&#xA;                        RealESRGAN directory&#xA;  --realesrgan-model REALESRGAN_MODEL&#xA;                        Upscaling model for RealESRGAN&#xA;  --no-verify-input     do not verify input to check if it&#39;s too long&#xA;  --no-half             do not switch the model to 16-bit floats&#xA;  --no-progressbar-hiding&#xA;                        do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware&#xA;                        accleration in browser)&#xA;  --defaults DEFAULTS   path to configuration file providing UI defaults, uses same format as cli parameter&#xA;  --gpu GPU             choose which GPU to use if you have multiple&#xA;  --extra-models-cpu    run extra models (GFGPAN/ESRGAN) on cpu&#xA;  --esrgan-cpu          run ESRGAN on cpu&#xA;  --gfpgan-cpu          run GFPGAN on cpu&#xA;  --cli CLI             don&#39;t launch web server, take Python function kwargs from this file.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Diffusion was made possible thanks to a collaboration with &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and &lt;a href=&#34;https://runwayml.com/&#34;&gt;Runway&lt;/a&gt; and builds upon our previous work:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Björn Ommer&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CVPR &#39;22 Oral&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;which is available on &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;GitHub&lt;/a&gt;. PDF at &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt;. Please also visit our &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/assets/stable-samples/txt2img/merged-0006.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/#stable-diffusion-v1&#34;&gt;Stable Diffusion&lt;/a&gt; is a latent text-to-image diffusion model. Thanks to a generous compute donation from &lt;a href=&#34;https://stability.ai/&#34;&gt;Stability AI&lt;/a&gt; and support from &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;, we were able to train a Latent Diffusion Model on 512x512 images from a subset of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; database. Similar to Google&#39;s &lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Imagen&lt;/a&gt;, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See &lt;a href=&#34;https://raw.githubusercontent.com/sd-webui/stable-diffusion-webui/master/#stable-diffusion-v1&#34;&gt;this section&lt;/a&gt; below and the &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion v1&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images.&lt;/p&gt; &#xA;&lt;p&gt;*Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Comments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The implementation of the transformer encoder is from &lt;a href=&#34;https://github.com/lucidrains/x-transformers&#34;&gt;x-transformers&lt;/a&gt; by &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34;&gt;lucidrains&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>d2l-ai/d2l-zh</title>
    <updated>2022-09-09T01:39:12Z</updated>
    <id>tag:github.com,2022-09-09:/d2l-ai/d2l-zh</id>
    <link href="https://github.com/d2l-ai/d2l-zh" rel="alternate"></link>
    <summary type="html">&lt;p&gt;《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被60个国家的400所大学用于教学。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;动手学深度学习（Dive into Deep Learning，D2L.ai）&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://ci.d2l.ai/job/d2l-zh/job/master/&#34;&gt;&lt;img src=&#34;http://ci.d2l.ai/job/d2l-zh/job/master/badge/icon&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zh-v1.d2l.ai/&#34;&gt;第一版：zh-v1.D2L.ai&lt;/a&gt; | &lt;a href=&#34;https://zh.d2l.ai&#34;&gt;第二版预览版：zh.D2L.ai&lt;/a&gt; | 安装和使用书中源代码：&lt;a href=&#34;https://zh-v1.d2l.ai/chapter_prerequisite/install.html&#34;&gt;第一版&lt;/a&gt; &lt;a href=&#34;https://zh.d2l.ai/chapter_installation/index.html&#34;&gt;第二版&lt;/a&gt; | 当前版本: v2.0.0-beta1&lt;/p&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt;&lt;i&gt;理解深度学习的最佳方法是学以致用。&lt;/i&gt;&lt;/h5&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/eq.jpg&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/figure.jpg&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/code.jpg&#34;&gt; &lt;img width=&#34;200&#34; src=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/notebook.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;本开源项目代表了我们的一种尝试：我们将教给读者概念、背景知识和代码；我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。&lt;/p&gt; &#xA;&lt;p&gt;我们的目标是创建一个为实现以下目标的统一资源：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;所有人均可在网上免费获取；&lt;/li&gt; &#xA; &lt;li&gt;提供足够的技术深度，从而帮助读者实际成为深度学习应用科学家：既理解数学原理，又能够实现并不断改进方法；&lt;/li&gt; &#xA; &lt;li&gt;包含可运行的代码，为读者展示如何在实际中解决问题。这样不仅直接将数学公式对应成实际代码，而且可以修改代码、观察结果并及时获取经验；&lt;/li&gt; &#xA; &lt;li&gt;允许我们和整个社区不断快速迭代内容，从而紧跟仍在高速发展的深度学习领域；&lt;/li&gt; &#xA; &lt;li&gt;由包含有关技术细节问答的论坛作为补充，使大家可以相互答疑并交换经验。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt;将本书（中英文版）用作教材或参考书的大学&lt;/h5&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;400&#34; src=&#34;http://en.d2l.ai.s3-website-us-west-2.amazonaws.com/_images/map.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;如果本书对你有帮助，请Star (★) 本仓库或引用本书的英文版：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2021dive,&#xA;    title={Dive into Deep Learning},&#xA;    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},&#xA;    journal={arXiv preprint arXiv:2106.11342},&#xA;    year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;本书的第二版&lt;/h2&gt; &#xA;&lt;p&gt;虽然纸质书第一版已经出版，但深度学习领域依然在迅速发展。为了得到来自更广泛的英文开源社区的帮助，从而提升本书质量，本书的第二版正在用英文写。英文版正不断被搬回中文版中。&lt;/p&gt; &#xA;&lt;p&gt;目前，英文版已超过160节（中文版共96节），例如增加了理论背景（如优化收敛分析）、硬件设计（如参数服务器）、全新篇章（如注意力机制、推荐系统、深度学习的数学、生成对抗网络）、应用种类（如自然语言推断）、模型种类（如Transformer、BERT）等，并优化重组了大量章节（如将自然语言处理篇章按从预训练表征、到模型设计、再到下游应用重构）。&lt;/p&gt; &#xA;&lt;p&gt;欢迎关注本书&lt;a href=&#34;https://github.com/d2l-ai/d2l-en&#34;&gt;第二版的英文开源项目&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;中英文教学资源&lt;/h2&gt; &#xA;&lt;p&gt;加州大学伯克利分校 2019 年春学期 &lt;a href=&#34;http://courses.d2l.ai/berkeley-stat-157/index.html&#34;&gt;&lt;em&gt;Introduction to Deep Learning&lt;/em&gt; 课程&lt;/a&gt;教材（同时提供含教学视频地址的&lt;a href=&#34;https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh&#34;&gt;中文版课件&lt;/a&gt;）。&lt;/p&gt; &#xA;&lt;h2&gt;学术界推荐&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;Dive into this book if you want to dive into deep learning!&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 韩家炜，ACM 院士、IEEE 院士，美国伊利诺伊大学香槟分校计算机系 Michael Aiken Chair 教授&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;This is a highly welcome addition to the machine learning literature.&#34;&lt;/p&gt; &#xA; &lt;b&gt;— Bernhard Schölkopf，ACM 院士、德国国家科学院院士，德国马克斯•普朗克研究所智能系统院院长&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;书中代码可谓‘所学即所用’。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 周志华，ACM 院士、IEEE 院士、AAAS 院士，南京大学计算机科学与技术系主任&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;这本书可以帮助深度学习实践者快速提升自己的能力。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 张潼，ASA 院士、IMS 院士，香港科技大学计算机系和数学系教授&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;工业界推荐&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;一本优秀的深度学习教材，值得任何想了解深度学习何以引爆人工智能革命的人关注。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 黄仁勋，NVIDIA创始人 &amp;amp; CEO&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;《动手学深度学习》是最适合工业界研发工程师学习的。我毫无保留地向广大的读者们强烈推荐。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 余凯，地平线公司创始人 &amp;amp; CEO&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;强烈推荐这本书！我特别赞赏这种手脑一体的学习方式。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 漆远，蚂蚁金服副总裁、首席AI科学家&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;《动手学深度学习》是一本很容易让学习者上瘾的书。&#34;&lt;/p&gt; &#xA; &lt;b&gt;— 沈强，将门创投创始合伙人&lt;/b&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;贡献&lt;/h2&gt; &#xA;&lt;p&gt;感谢&lt;a href=&#34;https://github.com/d2l-ai/d2l-zh/graphs/contributors&#34;&gt;社区贡献者们&lt;/a&gt;为每一位读者改进这本开源书。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zh-v2.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html&#34;&gt;如何贡献&lt;/a&gt; | &lt;a href=&#34;https://zh-v2.d2l.ai/chapter_preface/index.html&#34;&gt;致谢&lt;/a&gt; | &lt;a href=&#34;https://discuss.d2l.ai/c/chinese-version/16&#34;&gt;讨论或报告问题&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/INFO.md&#34;&gt;其他&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tensorflow/models</title>
    <updated>2022-09-09T01:39:12Z</updated>
    <id>tag:github.com,2022-09-09:/tensorflow/models</id>
    <link href="https://github.com/tensorflow/models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Models and examples built with TensorFlow&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://storage.googleapis.com/tf_model_garden/tf_model_garden_logo.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/tensorflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/tf-models-official&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/tf-models-official.svg?sanitize=true&#34; alt=&#34;tf-models-official PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Welcome to the Model Garden for TensorFlow&lt;/h1&gt; &#xA;&lt;p&gt;The TensorFlow Model Garden is a repository with a number of different implementations of state-of-the-art (SOTA) models and modeling solutions for TensorFlow users. We aim to demonstrate the best practices for modeling so that TensorFlow users can take full advantage of TensorFlow for their research and product development.&lt;/p&gt; &#xA;&lt;p&gt;To improve the transparency and reproducibility of our models, training logs on &lt;a href=&#34;https://tensorboard.dev&#34;&gt;TensorBoard.dev&lt;/a&gt; are also provided for models to the extent possible though not all models are suitable.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Directory&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/official&#34;&gt;official&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A collection of example implementations for SOTA models using the latest TensorFlow 2&#39;s high-level APIs&lt;br&gt;• Officially maintained, supported, and kept up to date with the latest TensorFlow 2 APIs by TensorFlow&lt;br&gt;• Reasonably optimized for fast performance while still being easy to read&lt;br&gt; For more details on the capabilities, check the guide on the &lt;a href=&#34;https://www.tensorflow.org/tfmodels&#34;&gt;Model-garden&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/research&#34;&gt;research&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A collection of research model implementations in TensorFlow 1 or 2 by researchers&lt;br&gt;• Maintained and supported by researchers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/community&#34;&gt;community&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A curated list of the GitHub repositories with machine learning models and implementations powered by TensorFlow 2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/orbit&#34;&gt;orbit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;• A flexible and lightweight library that users can easily use or fork when writing customized training loop code in TensorFlow 2.x. It seamlessly integrates with &lt;code&gt;tf.distribute&lt;/code&gt; and supports running on different device types (CPU, GPU, and TPU).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install the current release of tensorflow-models, please follow any one of the methods described below.&lt;/p&gt; &#xA;&lt;h4&gt;Method 1: Install the TensorFlow Model Garden pip package&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;&lt;strong&gt;tf-models-official&lt;/strong&gt; is the stable Model Garden package. Please check out the &lt;a href=&#34;https://github.com/tensorflow/models/releases&#34;&gt;releases&lt;/a&gt; to see what are available modules.&lt;/p&gt; &#xA; &lt;p&gt;pip3 will install all models and dependencies automatically.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install tf-models-official&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Please check out our examples:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/raw/master/tensorflow_models/tensorflow_models_pypi.ipynb&#34;&gt;basic library import&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/models/raw/master/docs/nlp/index.ipynb&#34;&gt;nlp model building&lt;/a&gt; to learn how to use a PIP package.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Note that &lt;strong&gt;tf-models-official&lt;/strong&gt; may not include the latest changes in the master branch of this github repo. To include latest changes, you may install &lt;strong&gt;tf-models-nightly&lt;/strong&gt;, which is the nightly Model Garden package created daily automatically.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install tf-models-nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Method 2: Clone the source&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Clone the GitHub repository:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/tensorflow/models.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Add the top-level &lt;em&gt;&lt;strong&gt;/models&lt;/strong&gt;&lt;/em&gt; folder to the Python path.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export PYTHONPATH=$PYTHONPATH:/path/to/models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you are using in a Windows environment, you may need to use the following command with PowerShell:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$env:PYTHONPATH += &#34;:\path\to\models&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you are using a Colab notebook, please set the Python path with os.environ.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#39;PYTHONPATH&#39;] += &#34;:/path/to/models&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Install other dependencies&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install --user -r models/official/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Finally, if you are using nlp packages, please also install &lt;strong&gt;tensorflow-text-nightly&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install tensorflow-text-nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Announcements&lt;/h2&gt; &#xA;&lt;p&gt;Please check &lt;a href=&#34;https://github.com/tensorflow/models/wiki/Announcements&#34;&gt;this page&lt;/a&gt; for recent announcements.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/models/labels/help%20wanted%3Apaper%20implementation&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/tensorflow/models/help%20wanted%3Apaper%20implementation&#34; alt=&#34;help wanted:paper implementation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you want to contribute, please review the &lt;a href=&#34;https://github.com/tensorflow/models/wiki/How-to-contribute&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tensorflow/models/master/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citing TensorFlow Model Garden&lt;/h2&gt; &#xA;&lt;p&gt;If you use TensorFlow Model Garden in your research, please cite this repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{tensorflowmodelgarden2020,&#xA;  author = {Hongkun Yu, Chen Chen, Xianzhi Du, Yeqing Li, Abdullah Rashwan, Le Hou, Pengchong Jin, Fan Yang,&#xA;            Frederick Liu, Jaeyoun Kim, and Jing Li},&#xA;  title = {{TensorFlow Model Garden}},&#xA;  howpublished = {\url{https://github.com/tensorflow/models}},&#xA;  year = {2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>