<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-26T01:32:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ollama/ollama-python</title>
    <updated>2024-07-26T01:32:54Z</updated>
    <id>tag:github.com,2024-07-26:/ollama/ollama-python</id>
    <link href="https://github.com/ollama/ollama-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ollama Python library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ollama Python Library&lt;/h1&gt; &#xA;&lt;p&gt;The Ollama Python library provides the easiest way to integrate Python 3.8+ projects with &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ollama&#xA;response = ollama.chat(model=&#39;llama3&#39;, messages=[&#xA;  {&#xA;    &#39;role&#39;: &#39;user&#39;,&#xA;    &#39;content&#39;: &#39;Why is the sky blue?&#39;,&#xA;  },&#xA;])&#xA;print(response[&#39;message&#39;][&#39;content&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Streaming responses&lt;/h2&gt; &#xA;&lt;p&gt;Response streaming can be enabled by setting &lt;code&gt;stream=True&lt;/code&gt;, modifying function calls to return a Python generator where each part is an object in the stream.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ollama&#xA;&#xA;stream = ollama.chat(&#xA;    model=&#39;llama3&#39;,&#xA;    messages=[{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Why is the sky blue?&#39;}],&#xA;    stream=True,&#xA;)&#xA;&#xA;for chunk in stream:&#xA;  print(chunk[&#39;message&#39;][&#39;content&#39;], end=&#39;&#39;, flush=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;The Ollama Python library&#39;s API is designed around the &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/docs/api.md&#34;&gt;Ollama REST API&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.chat(model=&#39;llama3&#39;, messages=[{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Why is the sky blue?&#39;}])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.generate(model=&#39;llama3&#39;, prompt=&#39;Why is the sky blue?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;List&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.list()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Show&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.show(&#39;llama3&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;modelfile=&#39;&#39;&#39;&#xA;FROM llama3&#xA;SYSTEM You are mario from super mario bros.&#xA;&#39;&#39;&#39;&#xA;&#xA;ollama.create(model=&#39;example&#39;, modelfile=modelfile)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Copy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.copy(&#39;llama3&#39;, &#39;user/llama3&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Delete&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.delete(&#39;llama3&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.pull(&#39;llama3&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Push&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.push(&#39;user/llama3&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Embeddings&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.embeddings(model=&#39;llama3&#39;, prompt=&#39;The sky is blue because of rayleigh scattering&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Ps&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ollama.ps()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Custom client&lt;/h2&gt; &#xA;&lt;p&gt;A custom client can be created with the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;host&lt;/code&gt;: The Ollama host to connect to&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;timeout&lt;/code&gt;: The timeout for requests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ollama import Client&#xA;client = Client(host=&#39;http://localhost:11434&#39;)&#xA;response = client.chat(model=&#39;llama3&#39;, messages=[&#xA;  {&#xA;    &#39;role&#39;: &#39;user&#39;,&#xA;    &#39;content&#39;: &#39;Why is the sky blue?&#39;,&#xA;  },&#xA;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Async client&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from ollama import AsyncClient&#xA;&#xA;async def chat():&#xA;  message = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Why is the sky blue?&#39;}&#xA;  response = await AsyncClient().chat(model=&#39;llama3&#39;, messages=[message])&#xA;&#xA;asyncio.run(chat())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Setting &lt;code&gt;stream=True&lt;/code&gt; modifies functions to return a Python asynchronous generator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from ollama import AsyncClient&#xA;&#xA;async def chat():&#xA;  message = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;Why is the sky blue?&#39;}&#xA;  async for part in await AsyncClient().chat(model=&#39;llama3&#39;, messages=[message], stream=True):&#xA;    print(part[&#39;message&#39;][&#39;content&#39;], end=&#39;&#39;, flush=True)&#xA;&#xA;asyncio.run(chat())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Errors&lt;/h2&gt; &#xA;&lt;p&gt;Errors are raised if requests return an error status or if an error is detected while streaming.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = &#39;does-not-yet-exist&#39;&#xA;&#xA;try:&#xA;  ollama.chat(model)&#xA;except ollama.ResponseError as e:&#xA;  print(&#39;Error:&#39;, e.error)&#xA;  if e.status_code == 404:&#xA;    ollama.pull(model)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>google-research/neuralgcm</title>
    <updated>2024-07-26T01:32:54Z</updated>
    <id>tag:github.com,2024-07-26:/google-research/neuralgcm</id>
    <link href="https://github.com/google-research/neuralgcm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hybrid ML + physics model of the Earth&#39;s atmosphere&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/google-research/neuralgcm/main/docs/_static/neuralgcm-logo-light.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Neural General Circulation Models for Weather and Climate&lt;/h1&gt; &#xA;&lt;p&gt;NeuralGCM is a Python library for building hybrid ML/physics atmospheric models for weather and climate simulation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.07222&#34;&gt;Paper&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://neuralgcm.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Code: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Trained model weights: &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>