<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-08T01:42:04Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>melih-unsal/DemoGPT</title>
    <updated>2023-08-08T01:42:04Z</updated>
    <id>tag:github.com,2023-08-08:/melih-unsal/DemoGPT</id>
    <link href="https://github.com/melih-unsal/DemoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create ü¶úÔ∏èüîó LangChain apps by just using prompts with the power of Llama 2 üåü Star to support our work! | Âè™ÈúÄ‰ΩøÁî®Âè•Â≠êÂç≥ÂèØÂàõÂª∫ LangChain Â∫îÁî®Á®ãÂ∫è„ÄÇ Áªô‰∏™starÊîØÊåÅÊàë‰ª¨ÁöÑÂ∑•‰ΩúÂêßÔºÅ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/assets/puzzle.png&#34; alt=&#34;favicon&#34;&gt; DemoGPT: Auto Gen-AI App Generator with the Power of Llama 2&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/assets/banner_small.png&#34; alt=&#34;DemoGPT logo: Generate automatic LangChain pipelines&#34; width=&#34;450px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;‚ö° With just a prompt, you can create interactive Streamlit apps via ü¶úÔ∏èüîó LangChain&#39;s transformative capabilities &amp;amp; Llama 2.‚ö°&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pepy.tech/project/demogpt&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/demogpt?period=total&amp;amp;units=international_system&amp;amp;left_color=blue&amp;amp;right_color=red&amp;amp;left_text=Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/melih-unsal/DemoGPT/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/melih-unsal/DemoGPT&#34; alt=&#34;Releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://demogpt.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Official%20Website-demogpt.io-blue?style=flat&amp;amp;logo=world&amp;amp;logoColor=white&#34; alt=&#34;Official Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.demogpt.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-üìò-blueviolet&#34; alt=&#34;DemoGPT Documentation&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/docs/README_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ÊñáÊ°£-‰∏≠ÊñáÁâà-blue.svg&#34; alt=&#34;CN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/document-English-blue.svg?sanitize=true&#34; alt=&#34;EN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/docs/ROADMAP_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ROADMAP-Ë∑ØÁ∫øÂõæ-blue&#34; alt=&#34;roadmap&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/docs/ROADMAP.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ROADMAP-english-red&#34; alt=&#34;roadmap&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/melih-unsal/DemoGPT/issues?q=is%3Aopen+is%3Aissue&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/melih-unsal/DemoGPT.svg?maxAge=2592000000000000&#34; alt=&#34;Open an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/melih-unsal/DemoGPT/issues?q=is%3Aissue+is%3Aclosed&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed-raw/melih-unsal/DemoGPT.svg?maxAge=25920000000000000000&#34; alt=&#34;Closed issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#melih-unsal/DemoGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/melih-unsal/DemoGPT?style=social&#34; alt=&#34;DemoGPT  Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/melih-unsal/DemoGPT&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://twitter.com/demo_gpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/demo_gpt?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://demogpt.medium.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?style=for-the-badge&amp;amp;message=Medium&amp;amp;color=000000&amp;amp;logo=Medium&amp;amp;logoColor=FFFFFF&amp;amp;label=&#34; alt=&#34;DemoGPT Medium&#34; height=&#34;20&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.producthunt.com/posts/demogpt?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-demogpt&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=406106&amp;amp;theme=light&#34; alt=&#34;DemoGPT - Auto generative AI app generator with the power of Llama 2 | Product Hunt&#34; height=&#34;20&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://demogpt.streamlit.app&#34;&gt;&lt;img src=&#34;https://static.streamlit.io/badges/streamlit_badge_black_white.svg?sanitize=true&#34; alt=&#34;Streamlit application&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/melihunsal/demogpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Spaces-yellow&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#melih-unsal/DemoGPT&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=melih-unsal/DemoGPT&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Consider starring us if you&#39;re using DemoGPT so more people hear about us!&lt;/p&gt; &#xA;&lt;h2&gt;üî• Demo&lt;/h2&gt; &#xA;&lt;p&gt;For quick demo, you can visit &lt;a href=&#34;https://demogpt.io&#34;&gt;our website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/assets/demogpt_new_version.gif&#34; alt=&#34;Tweet Generator&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìö Documentation&lt;/h2&gt; &#xA;&lt;p&gt;See our documentation site &lt;a href=&#34;https://docs.demogpt.io/&#34;&gt;here&lt;/a&gt; for full how-to docs and guidelines&lt;/p&gt; &#xA;&lt;p&gt;‚ö° The new release with the power of &lt;strong&gt;Llama 2&lt;/strong&gt; is within a week. ‚ö°&lt;/p&gt; &#xA;&lt;h2&gt;üì¶ Using DemoGPT Package&lt;/h2&gt; &#xA;&lt;p&gt;The DemoGPT package is now available and can be installed using pip. Run the following command to install the package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install demogpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the DemoGPT application, simply type &#34;demogpt&#34; into your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;demogpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/#-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/#%EF%B8%8F-architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/#-installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/#-usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/#to-do-&#34;&gt;To-Do&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/#-contribute&#34;&gt;Contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/#-license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìå Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to DemoGPT, a revolutionary open-source initiative that is reshaping the landscape of Large Language Model (LLM) based application development.&lt;/p&gt; &#xA;&lt;p&gt;At the core of DemoGPT lies the synergy of GPT-3.5-turbo &amp;amp; Llama 2, which powers the auto-generation of LangChain code. This process is enriched with a sophisticated architecture that translates user instructions into interactive Streamlit applications.&lt;/p&gt; &#xA;&lt;h3&gt;How DemoGPT Works&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Planning:&lt;/strong&gt; DemoGPT starts by generating a plan from the user&#39;s instruction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task Creation:&lt;/strong&gt; It then creates specific tasks from the plan and instruction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code Snippet Generation:&lt;/strong&gt; These tasks are transferred into code snippets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Final Code Assembly:&lt;/strong&gt; The code snippets are combined into a final code, resulting in an interactive Streamlit app.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The LangChain code, once generated, is not a mere endpoint but a transformative stage. It evolves into a user-friendly Streamlit application, adding an interactive dimension to the logic crafted. This metamorphosis embodies DemoGPT&#39;s commitment to user engagement and experience.&lt;/p&gt; &#xA;&lt;h3&gt;Future Enhancements&lt;/h3&gt; &#xA;&lt;p&gt;We are planning to add a publicly available database that will accelerate the generation process by retrieving similar examples during the refining process. This innovation will further streamline the development workflow, making it more efficient and responsive.&lt;/p&gt; &#xA;&lt;h3&gt;Model Flexibility&lt;/h3&gt; &#xA;&lt;p&gt;DemoGPT is designed to be adaptable, capable of using any LLM model that meets specific performance criteria in terms of code generation. This flexibility ensures that DemoGPT remains at the forefront of technology, embracing new advancements in LLM.&lt;/p&gt; &#xA;&lt;p&gt;DemoGPT&#39;s iterative development process remains a cornerstone of its innovation. Each code segment undergoes individual testing, and the self-refining strategy ensures an efficient and error-minimized workflow. This fusion of meticulous testing and refinement is a testament to DemoGPT&#39;s pursuit of excellence.&lt;/p&gt; &#xA;&lt;p&gt;By transcending traditional coding paradigms, DemoGPT is pioneering a new era in LLM-based applications. It&#39;s not just about code generation; it&#39;s about crafting intelligent, interactive, and inclusive solutions.&lt;/p&gt; &#xA;&lt;p&gt;In summary, DemoGPT is more than a project; it&#39;s a visionary approach, pushing the boundaries of what&#39;s possible in LLM-based application development.&lt;/p&gt; &#xA;&lt;p&gt;In the next release, we are gonna add &lt;strong&gt;Llama 2&lt;/strong&gt; inside of DemoGPT to make the whole system runnable completely locally. The future is bright, and the journey has just begun. Join us in this exciting adventure!&lt;/p&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Architecture&lt;/h2&gt; &#xA;&lt;h3&gt;DemoGPT Architecture&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/assets/plan_based_pipeline.png?raw=true&#34; alt=&#34;DemoGPT Architecture&#34; title=&#34;DemoGPT Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üîß Installation&lt;/h2&gt; &#xA;&lt;h3&gt;For the Package Version&lt;/h3&gt; &#xA;&lt;p&gt;You can install the DemoGPT package by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install demogpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For the Source Code Version&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/melih-unsal/DemoGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Navigate into the project directory: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd DemoGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install the necessary dependencies: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üéÆ Usage&lt;/h2&gt; &#xA;&lt;h3&gt;For the Package Version&lt;/h3&gt; &#xA;&lt;p&gt;Once the DemoGPT package is installed, you can use it by running the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;demogpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run the previous version, you can do so by using the &lt;strong&gt;--basic&lt;/strong&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;demogpt --basic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For the Source Code Version&lt;/h3&gt; &#xA;&lt;p&gt;If you have cloned the repository and wish to run the source code version, you can use DemoGPT by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;streamlit run src/plan/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run the previous version of DemoGPT, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;streamlit run src/prompt_based/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;To-Do üìù&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement new DemoGPT pipeline including plan generation, task creation, code snippet generation, and final code assembly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add feature to allow users to select models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Define useful LangChain tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Publish release with the new pipeline without refinement&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement remaining LangChain tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement self-refining strategy for model response refinement.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integrate ü¶ç Gorilla model for API calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add Rapid API for expanding available API calls.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement publicly available database to accelerate the generation process by retrieving similar examples during the refining process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add all successfully generated steps to a DB to eliminate redundant refinement.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Contributions to the DemoGPT project are welcomed! Whether you&#39;re fixing bugs, improving the documentation, or proposing new features, your efforts are highly appreciated. Please check the open issues before starting any work.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Please read &lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING&lt;/code&gt;&lt;/a&gt; for details on our &lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;code&gt;CODE OF CONDUCT&lt;/code&gt;&lt;/a&gt;, and the process for submitting pull requests to us.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;DemoGPT is an open-source project licensed under &lt;a href=&#34;https://raw.githubusercontent.com/melih-unsal/DemoGPT/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For any issues, questions, or comments, please feel free to contact us or open an issue. We appreciate your feedback to make DemoGPT better.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MeetKai/functionary</title>
    <updated>2023-08-08T01:42:04Z</updated>
    <id>tag:github.com,2023-08-08:/MeetKai/functionary</id>
    <link href="https://github.com/MeetKai/functionary" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat language model that can interpret and execute functions/plugins&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Functionary&lt;/h1&gt; &#xA;&lt;img align=&#34;right&#34; width=&#34;256&#34; height=&#34;256&#34; src=&#34;https://github.com/musabgultekin/functionary/assets/3749407/c7a1972d-6ad7-40dc-8000-dceabe6baabd&#34;&gt; &#xA;&lt;p&gt;Functionary is a language model that can interpret and execute functions/plugins.&lt;/p&gt; &#xA;&lt;p&gt;The model determines when to execute a function and can understand its output. It only triggers functions as needed. Function definitions are given as JSON Schema Objects, similar to OpenAI GPT function calls.&lt;/p&gt; &#xA;&lt;p&gt;Based on &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI compatible server&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt; installed. Then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;python3 server.py --model &#34;musabgultekin/functionary-7b-v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or start blazing fast &lt;a href=&#34;https://vllm.readthedocs.io/en/latest/getting_started/installation.html&#34;&gt;vLLM&lt;/a&gt; server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 server_vllm.py --model &#34;musabgultekin/functionary-7b-v1&#34; --host 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Server Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;&#xA;openai.api_base = &#34;http://localhost:8000/v1&#34;&#xA;openai.api_key = &#34;functionary&#34; # We just need to set this something other than None, so it works with openai package. No API key is required.&#xA;&#xA;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is the weather for Istanbul?&#34;}],&#xA;    functions=[{&#xA;        &#34;name&#34;: &#34;get_current_weather&#34;,&#xA;        &#34;description&#34;: &#34;Get the current weather&#34;,&#xA;        &#34;parameters&#34;: {&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: {&#xA;                &#34;location&#34;: {&#xA;                    &#34;type&#34;: &#34;string&#34;,&#xA;                    &#34;description&#34;: &#34;The city and state, e.g. San Francisco, CA&#34;&#xA;                },&#xA;            },&#xA;            &#34;required&#34;: [&#34;location&#34;],&#xA;        },&#xA;    }]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re having trouble with dependencies, and you have &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#setting-up-nvidia-container-toolkit&#34;&gt;nvidia-container-toolkit&lt;/a&gt;, you can start your environment like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo docker run --gpus all -it --shm-size=8g --name functionary -v ${PWD}/functionary_workspace:/workspace -p 8000:8000 nvcr.io/nvidia/pytorch:22.12-py3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Use Cases&lt;/h1&gt; &#xA;&lt;p&gt;Here are a few examples of how you can use this function calling system:&lt;/p&gt; &#xA;&lt;h3&gt;Travel and Hospitality - Trip Planning&lt;/h3&gt; &#xA;&lt;p&gt;The function &lt;code&gt;plan_trip(destination: string, duration: int, interests: list)&lt;/code&gt; can take user input such as &#34;I want to plan a 7-day trip to Paris with a focus on art and culture&#34; and generate an itinerary accordingly.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Details (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#39;I want to plan a 7-day trip to Paris with a focus on art and culture&#39;},&#xA;    ], &#xA;    functions=[&#xA;        {&#xA;            &#34;name&#34;: &#34;plan_trip&#34;,&#xA;            &#34;description&#34;: &#34;Plan a trip based on user&#39;s interests&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;type&#34;: &#34;object&#34;,&#xA;                &#34;properties&#34;: {&#xA;                    &#34;destination&#34;: {&#xA;                        &#34;type&#34;: &#34;string&#34;,&#xA;                        &#34;description&#34;: &#34;The destination of the trip&#34;,&#xA;                    },&#xA;                    &#34;duration&#34;: {&#xA;                        &#34;type&#34;: &#34;integer&#34;,&#xA;                        &#34;description&#34;: &#34;The duration of the trip in days&#34;,&#xA;                    },&#xA;                    &#34;interests&#34;: {&#xA;                        &#34;type&#34;: &#34;array&#34;,&#xA;                        &#34;items&#34;: {&#34;type&#34;: &#34;string&#34;},&#xA;                        &#34;description&#34;: &#34;The interests based on which the trip will be planned&#34;,&#xA;                    },&#xA;                },&#xA;                &#34;required&#34;: [&#34;destination&#34;, &#34;duration&#34;, &#34;interests&#34;],&#xA;            },&#xA;        },&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Response will have:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;plan_trip&#34;, &#34;arguments&#34;: &#39;{\n  &#34;destination&#34;: &#34;Paris&#34;,\n  &#34;duration&#34;: 7,\n  &#34;interests&#34;: [&#34;art&#34;, &#34;culture&#34;]\n}&#39;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then you need to call &lt;code&gt;plan_trip&lt;/code&gt; function with provided arguments. If you would like a commentary from the model, then you&#39;ll call the model again with the response from the function, the model will write necessary commentary.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Real Estate - Property Valuation&lt;/h3&gt; &#xA;&lt;p&gt;A function like estimate_property_value(property_details: dict) could allow users to input details about a property (such as location, size, number of rooms, etc.) and receive an estimated market value.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Details (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#39;What is the estimated value of a 3-bedroom house in San Francisco with 2000 sq ft area?&#39;},&#xA;        {&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;estimate_property_value&#34;, &#34;arguments&#34;: &#39;{\n  &#34;property_details&#34;: {&#34;location&#34;: &#34;San Francisco&#34;, &#34;size&#34;: 2000, &#34;rooms&#34;: 3}\n}&#39;}},&#xA;    ], &#xA;    functions=[&#xA;        {&#xA;            &#34;name&#34;: &#34;estimate_property_value&#34;,&#xA;            &#34;description&#34;: &#34;Estimate the market value of a property&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;type&#34;: &#34;object&#34;,&#xA;                &#34;properties&#34;: {&#xA;                    &#34;property_details&#34;: {&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: {&#xA;                            &#34;location&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The location of the property&#34;,&#xA;                            },&#xA;                            &#34;size&#34;: {&#xA;                                &#34;type&#34;: &#34;integer&#34;,&#xA;                                &#34;description&#34;: &#34;The size of the property in square feet&#34;,&#xA;                            },&#xA;                            &#34;rooms&#34;: {&#xA;                                &#34;type&#34;: &#34;integer&#34;,&#xA;                                &#34;description&#34;: &#34;The number of rooms in the property&#34;,&#xA;                            },&#xA;                        },&#xA;                        &#34;required&#34;: [&#34;location&#34;, &#34;size&#34;, &#34;rooms&#34;],&#xA;                    },&#xA;                },&#xA;                &#34;required&#34;: [&#34;property_details&#34;],&#xA;            },&#xA;        },&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Response will have:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;plan_trip&#34;, &#34;arguments&#34;: &#39;{\n  &#34;destination&#34;: &#34;Paris&#34;,\n  &#34;duration&#34;: 7,\n  &#34;interests&#34;: [&#34;art&#34;, &#34;culture&#34;]\n}&#39;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then you need to call &lt;code&gt;plan_trip&lt;/code&gt; function with provided arguments. If you would like a commentary from the model, then you&#39;ll call the model again with the response from the function, the model will write necessary commentary.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Telecommunications - Customer Support&lt;/h3&gt; &#xA;&lt;p&gt;A function &lt;code&gt;parse_customer_complaint(complaint: {issue: string, frequency: string, duration: string})&lt;/code&gt; could help in extracting structured information from a complex, narrative customer complaint, identifying the core issue and potential solutions. The &lt;code&gt;complaint&lt;/code&gt; object could include properties such as &lt;code&gt;issue&lt;/code&gt; (the main problem), &lt;code&gt;frequency&lt;/code&gt; (how often the issue occurs), and &lt;code&gt;duration&lt;/code&gt; (how long the issue has been occurring).&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Details (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;openai.ChatCompletion.create(&#xA;    model=&#34;musabgultekin/functionary-7b-v1&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#39;My internet has been disconnecting frequently for the past week&#39;},&#xA;    ], &#xA;    functions=[&#xA;        {&#xA;            &#34;name&#34;: &#34;parse_customer_complaint&#34;,&#xA;            &#34;description&#34;: &#34;Parse a customer complaint and identify the core issue&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;type&#34;: &#34;object&#34;,&#xA;                &#34;properties&#34;: {&#xA;                    &#34;complaint&#34;: {&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: {&#xA;                            &#34;issue&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The main problem&#34;,&#xA;                            },&#xA;                            &#34;frequency&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;How often the issue occurs&#34;,&#xA;                            },&#xA;                            &#34;duration&#34;: {&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;How long the issue has been occurring&#34;,&#xA;                            },&#xA;                        },&#xA;                        &#34;required&#34;: [&#34;issue&#34;, &#34;frequency&#34;, &#34;duration&#34;],&#xA;                    },&#xA;                },&#xA;                &#34;required&#34;: [&#34;complaint&#34;],&#xA;            },&#xA;        },&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Response will have:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;role&#34;: &#34;assistant&#34;, &#34;function_call&#34;: {&#34;name&#34;: &#34;parse_customer_complaint&#34;, &#34;arguments&#34;: &#39;{\n  &#34;complaint&#34;: {&#34;issue&#34;: &#34;internet disconnecting&#34;, &#34;frequency&#34;: &#34;frequently&#34;, &#34;duration&#34;: &#34;past week&#34;}\n}&#39;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then you need to call parse_customer_complaint function with provided arguments. If you would like a commentary from the model, then you&#39;ll call the model again with the response from the function, the model will write necessary commentary.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We use standard HuggingFace Trainer. When calculating the loss, we only calculate the loss on assistant outputs and assistant function calls. Not on function responses and function definitions&lt;/p&gt; &#xA;&lt;p&gt;We use the similar hyperparameters as its used in LLama 2 &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;paper&lt;/a&gt;. Except we use bigger weight decay (0.3 instead of 0.1) and warmup of 0.03, to reduce overfitting as we sample 2x of the function calling example conversations. But ablation study is required.&lt;/p&gt; &#xA;&lt;p&gt;We use transformers after this &lt;a href=&#34;https://github.com/huggingface/transformers/commit/f4eb459ef25c62c4cc9edde38052da1980977872&#34;&gt;commit&lt;/a&gt;. As it fixes OOM for FSDP training on Llama 2.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hyperparameters&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch size: 64&lt;/li&gt; &#xA; &lt;li&gt;Learning rate: 2e-5&lt;/li&gt; &#xA; &lt;li&gt;Epochs: 2&lt;/li&gt; &#xA; &lt;li&gt;Max length: 4096&lt;/li&gt; &#xA; &lt;li&gt;Weight decay: 0.3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More on training: &lt;a href=&#34;https://raw.githubusercontent.com/MeetKai/functionary/main/train/README.md&#34;&gt;README.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it Works?&lt;/h2&gt; &#xA;&lt;p&gt;We convert function definitions to a similar text like TypeScript definitions. Then we inject these definitions as system prompts. After that, we inject the default system prompt. Then we start the conversation messages.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example prompt that will be provided to the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;system:&#xA;namespace weather {&#xA;&#xA;// Get the current weather&#xA;type get_current_weather  = (_: {&#xA;// The city and state, e.g. San Francisco, CA&#xA;location: string,&#xA;// The temperature unit to use. Infer this from the users location.&#xA;format: &#34;celsius&#34; | &#34;fahrenheit&#34;,&#xA;}) =&amp;gt; any;&#xA;&#xA;} // namespace weather&#xA;system:&#xA;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#39;s questions. The assistant calls functions with appropriate input when necessary&#xA;user:&#xA;&amp;lt;/s&amp;gt;What is the weather in Istanbul?&amp;lt;/s&amp;gt;&#xA;assistant&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The model will output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt; to=weather.get_current_weather:&#xA;{&#34;location&#34;: &#34;Istanbul&#34;, &#34;format&#34;: &#34;celsius&#34;}&amp;lt;/s&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then it will stop.&lt;/p&gt; &#xA;&lt;p&gt;We don&#39;t change the logit probabilities to conform a certain schema, but the model itself knows how to conform. This allows us to use existing tools and caching systems with ease.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;--- Work In Progress ---&lt;/p&gt; &#xA;&lt;p&gt;Due to the unique nature, it requires custom evaluation suite. But we can probably evaluate with gpt-4-0613, likely with a similar approach like &lt;a href=&#34;https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge&#34;&gt;LLM Judge&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;--- Work In Progress ---&lt;/p&gt; &#xA;&lt;p&gt;Dataset preparation process consists of several steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Definitions Conversion:&lt;/strong&gt; We begin by selecting multiple function definitions and converting them into TypeScript definitions. This approach benefits from the model&#39;s prior exposure to TypeScript tokens during the pretraining phase. &lt;a href=&#34;https://github.com/musabgultekin/functionary/raw/17a86de9b06acaedd0afab212717205c0484a218/schema.py#L54&#34;&gt;See how we do it&lt;/a&gt; Also see &lt;a href=&#34;https://github.com/microsoft/TypeChat/raw/d2f2de9ca37ef9adeb108d5fc60703b72fec0a22/site/src/blog/introducing-typechat.md#just-add-types&#34;&gt;Microsoft TypeChat&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Human Prompts Generation:&lt;/strong&gt; We then create human prompts that incorporate the converted TypeScript function definitions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Calls Generation:&lt;/strong&gt; Following the generation of human prompts, we proceed to generate corresponding function calls.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Answers Generation:&lt;/strong&gt; Once function calls have been generated, we derive the outputs of these function calls would produce.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Function Answers Interpretation:&lt;/strong&gt; After procuring function answers, we generate language model answers for the function response. So the model knows how to interpret the function response.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Merging and Training:&lt;/strong&gt; We combine all the generated elements (prompts, function calls, function answers, and their interpretations) using a custom formatting. This consolidated dataset is then used for the model&#39;s training.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Llama 2 70b is capable of doing all synthetic data generation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;More information about this process will be provided soon as possible.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;v0.1&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Sources:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/ehartford/wizard_vicuna_70k_unfiltered/blob/cfe3f5810110d4d763665c070b4a966fda43e5c5/wizard_vicuna_dataset_unfiltered.json&#34;&gt;ShareGPT 34K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Synthetic function calling dataset (2.7k examples)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt; This version showed limitations in handling multi-prompt conversations, likely due to the absence of multiple instructions in the function calling dataset. Also hallucinations are common, we likely need more conversation data.&lt;/p&gt; &#xA;&lt;h3&gt;v0.2&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Sources:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/bcd32a724d8460ebe14e1d05b0195e30e9a46cb1/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json&#34;&gt;ShareGPT 53K&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Synthetic function calling dataset (3.5k examples). Sampled 2 times.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;v1&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Sources:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Same as v0.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt; Compared to v0.2, because the model supports 4k context sizes, its much more resilient to the longer conversations and longer function definitions. Also we switched to Llama 2.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; If I can save more money, I&#39;ll train &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2&lt;/a&gt; 13B model too, with 2x more data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; OpenAPI specification based plugin support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fast inference server &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;text-generation-inference&lt;/a&gt; ? See: &lt;a href=&#34;https://github.com/huggingface/text-generation-inference/issues/726&#34;&gt;License Issue&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Streaming Support&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; function_call parameter to server&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Python function calling support (Automatic detection of type annotations and calling them automatically)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Real world usage examples, such as creating agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Please consider opening a PR for future requests&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>daprofiler/DaProfiler</title>
    <updated>2023-08-08T01:42:04Z</updated>
    <id>tag:github.com,2023-08-08:/daprofiler/DaProfiler</id>
    <link href="https://github.com/daprofiler/DaProfiler" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DaProfiler is an OSINT tool allowing you to collect certain information about yourself in order to rectify by rgpd requests the traces you may have left on the net. DaProfiler is indeed able to recover: Addresses, Social media accounts, e-mail addresses, mobile / landline number, jobs. On a specified subject in a limited time. DaProfiler is desi‚Ä¶&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/daprofiler/DaProfiler/raw/main/files/DaProfiler_Logo.png?raw=true&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=TheRealDalunacrobate.daprofiler&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;DaProfiler is an OSINT tool capable of tracing the digital identity of a target via social networks, emails, public information such as directories, business listings, etc. DaProfiler is also able to check if a face appears on the profile photos of returned accounts. DaProfiler can bring you a lot in the context of a search for email addresses on a French target using various techniques of mail guessing, mail swapping using the first and last name of your target. The developers inform you that you are responsible for the uses and actions you make of DaProfiler, this tool is above all a big data awareness tool, created to search for a person with his agreement and allow him to remove malicious traces of his digital identity to protect themselves.&lt;/p&gt; &#xA;&lt;p&gt;Input : Name , Last Name &lt;br&gt; Output : email addresses, landline numbers, physical addresses, social networks, work history, photos, etc.&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;Python 3.8 &amp;amp; Mozilla Firefox required.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/TheRealDalunacrobate/DaProfiler.git&#xA;cd DaProfiler&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Connect to LinkedIN API&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://github.com/daprofiler/DaProfiler/raw/main/modules/social_medias/linkedin_search.py&#34;&gt;modules\linkedin_search&lt;/a&gt; then add your creditentials.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/XSzG90S/Capture-censored.jpg&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;All suggestions are welcome.&lt;/p&gt; &#xA;&lt;h2&gt;Code parts used under license and authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/megadose/toutatis&#34;&gt;Palenath - Instagram Advanced Lookup Function&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>