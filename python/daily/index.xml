<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-19T01:44:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yizhongw/self-instruct</title>
    <updated>2023-03-19T01:44:52Z</updated>
    <id>tag:github.com,2023-03-19:/yizhongw/self-instruct</id>
    <link href="https://github.com/yizhongw/self-instruct" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Aligning pretrained language models with instruction data generated by themselves.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Self-Instruct: Aligning LM with Self Generated Instructions&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code and data for the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Self-Instruct paper&lt;/a&gt;, a method for aligning pretrained language models with instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Self-Instruct is a framework that helps language models improve their ability to follow natural language instructions. It does this by using the model&#39;s own generations to create a large collection of instructional data. With Self-Instruct, it is possible to improve the instruction-following capabilities of language models without relying on extensive manual annotation.&lt;/p&gt; &#xA;&lt;h3&gt;Background&lt;/h3&gt; &#xA;&lt;p&gt;In recent years, there has been a growing interest in building models that can follow natural language instructions to perform a wide range of tasks. These models, known as &#34;instruction-tuned&#34; language models, have demonstrated the ability to generalize to new tasks. However, their performance is heavily dependent on the quality and quantity of the human-written instruction data used to train them, which can be limited in diversity and creativity. To overcome these limitations, it is important to develop alternative approaches for supervising instruction-tuned models and improving their instruction-following capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;How Self-Instruct works?&lt;/h3&gt; &#xA;&lt;p&gt;The Self-Instruct process is an iterative bootstrapping algorithm that starts with a seed set of manually-written instructions and uses them to prompt the language model to generate new instructions and corresponding input-output instances. These generations are then filtered to remove low-quality or similar ones, and the resulting data is added back to the task pool. This process can be repeated multiple times, resulting in a large collection of instructional data that can be used to fine-tune the language model to follow instructions more effectively.&lt;/p&gt; &#xA;&lt;p&gt;Here is an overview of Self-Instruct:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yizhongw/self-instruct/main/docs/pipeline.JPG&#34; alt=&#34;The pipeline for generating instruction data from a language model itself.&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;* &lt;strong&gt;This work is still in progress. We may update the code and data as we make progress. Please be cautious about the version control.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Instruction-tuning using our Self-Instruct data&lt;/h3&gt; &#xA;&lt;p&gt;We release a dataset that contains 52k instructions, paired with 82K instance inputs and outputs. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better. The entire model-generated data can be accessed in &lt;code&gt;data/gpt3-generations/batch_221203/all_instances_82K.jsonl&lt;/code&gt;. This data (+ the 175 seed tasks) reformatted in clean GPT3-finetuning format (prompt + completion) is put in &lt;code&gt;data/finetuning/self_instruct_221203&lt;/code&gt;. You can use the script in &lt;a href=&#34;https://raw.githubusercontent.com/yizhongw/self-instruct/main/scripts/finetune_gpt3.sh&#34;&gt;&lt;code&gt;./scripts/finetune_gpt3.sh&lt;/code&gt;&lt;/a&gt; to finetune GPT3 on this data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This data is generated by a language model (GPT-3) and inevitably contains some errors or biases. We analyzed the data quality on 200 random instructions in our paper, and found that 46% of the data points may have problems. We encourage users to use this data with caution and propose new methods to filter or improve the imperfections.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluating instruction-following capabilities&lt;/h3&gt; &#xA;&lt;p&gt;We also release a new set of 252 expert-written tasks and their instructions motivated by user-oriented applications (rather than well-studied NLP tasks). This data is used in the human evaluation section of &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;the self-instruct paper&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/yizhongw/self-instruct/main/human_eval/README.md&#34;&gt;the human evaluation README&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Generating Self-Instruct data from scratch&lt;/h3&gt; &#xA;&lt;p&gt;To generate Self-Instruct data using your own seed tasks or other models, we open-source our scripts for the entire pipeline here. Our current code is only tested on the GPT3 model accessible via the &lt;a href=&#34;https://beta.openai.com/docs/models/gpt-3&#34;&gt;OpenAI API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here are the scripts for generating the data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. Generate instructions from the seed tasks&#xA;./scripts/generate_instructions.sh&#xA;&#xA;# 2. Identify whether the instruction represents a classification task or not&#xA;./scripts/is_clf_or_not.sh&#xA;&#xA;# 3. Generate instances for each instruction&#xA;./scripts/generate_instances.sh&#xA;&#xA;# 4. Filtering, processing, and reformatting&#xA;./scripts/prepare_for_finetuning.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use the Self-Instruct framework or data, feel free to cite us.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{selfinstruct,&#xA;  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},&#xA;  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},&#xA;  journal={arXiv preprint arXiv:2212.10560},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/GLM</title>
    <updated>2023-03-19T01:44:52Z</updated>
    <id>tag:github.com,2023-03-19:/THUDM/GLM</id>
    <link href="https://github.com/THUDM/GLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GLM (General Language Model)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GLM&lt;/h1&gt; &#xA;&lt;p&gt;GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on various natural language understanding and generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to our paper for a detailed description of GLM:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.10360&#34;&gt;GLM: General Language Model Pretraining with Autoregressive Blank Infilling&lt;/a&gt; (ACL 2022)&lt;/p&gt; &#xA;&lt;p&gt;Zhengxiao Du*, Yujie Qian*, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang (*: equal contribution)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;News: We release &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;, an open pre-trained language model with 6 billion parameters optimized for Chinese QA and dialogue based on the GLM framework.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;You can download the pretrained models used in the paper from &lt;a href=&#34;https://mailstsinghuaeducn-my.sharepoint.com/:f:/g/personal/zx-du20_mails_tsinghua_edu_cn/En6zA7_utRxHptKWZoDMO14Bkfj3uGRpslYkNvMPdGOmow?e=G0lGSc&#34;&gt;OneDrive&lt;/a&gt; or &lt;a href=&#34;https://cloud.tsinghua.edu.cn/d/13f5b03da9594e5490c4&#34;&gt;Tsinghua-Cloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Params&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Corpus&lt;/th&gt; &#xA;   &lt;th&gt;Objective&lt;/th&gt; &#xA;   &lt;th&gt;File&lt;/th&gt; &#xA;   &lt;th&gt;Config&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-Base&lt;/td&gt; &#xA;   &lt;td&gt;110M&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;Wiki+Book&lt;/td&gt; &#xA;   &lt;td&gt;Token&lt;/td&gt; &#xA;   &lt;td&gt;glm-base-blank.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_base.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-Large&lt;/td&gt; &#xA;   &lt;td&gt;335M&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;Wiki+Book&lt;/td&gt; &#xA;   &lt;td&gt;Token&lt;/td&gt; &#xA;   &lt;td&gt;glm-large-blank.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_large.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-Large-Chinese&lt;/td&gt; &#xA;   &lt;td&gt;335M&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2666651021000152&#34;&gt;WuDaoCorpora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Token+Sent+Doc&lt;/td&gt; &#xA;   &lt;td&gt;glm-large-chinese.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_large_chinese.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-Doc&lt;/td&gt; &#xA;   &lt;td&gt;335M&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;Wiki+Book&lt;/td&gt; &#xA;   &lt;td&gt;Token+Doc&lt;/td&gt; &#xA;   &lt;td&gt;glm-large-generation.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_large_generation.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-410M&lt;/td&gt; &#xA;   &lt;td&gt;410M&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;Wiki+Book&lt;/td&gt; &#xA;   &lt;td&gt;Token+Doc&lt;/td&gt; &#xA;   &lt;td&gt;glm-1.25-generation.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_1.25_generation.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-515M&lt;/td&gt; &#xA;   &lt;td&gt;515M&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;Wiki+Book&lt;/td&gt; &#xA;   &lt;td&gt;Token+Doc&lt;/td&gt; &#xA;   &lt;td&gt;glm-1.5-generation.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_1.5_generation.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;335M&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;Token&lt;/td&gt; &#xA;   &lt;td&gt;glm-roberta-large-blank.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_roberta_large.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-2B&lt;/td&gt; &#xA;   &lt;td&gt;2B&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.00027&#34;&gt;Pile&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Token+Sent+Doc&lt;/td&gt; &#xA;   &lt;td&gt;glm-2b.tar.bz2&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_2B.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-10B&lt;/td&gt; &#xA;   &lt;td&gt;10B&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.00027&#34;&gt;Pile&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Token+Sent+Doc&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lfs.aminer.cn/misc/cogview/glm-10b-1024.zip&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_10B.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-10B-Chinese&lt;/td&gt; &#xA;   &lt;td&gt;10B&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2666651021000152&#34;&gt;WuDaoCorpora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Token+Sent+Doc&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lfs.aminer.cn/misc/cogview/glm-10b-chinese.zip&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;model_blocklm_10B_chinese.sh&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Unzip the downloaded file into a local folder and set &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; in the corresponding scripts to the folder path.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://super.gluebenchmark.com&#34;&gt;SuperGLUE&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;dev set, single model, single-task finetuning&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;COPA&lt;/th&gt; &#xA;   &lt;th&gt;WSC&lt;/th&gt; &#xA;   &lt;th&gt;RTE&lt;/th&gt; &#xA;   &lt;th&gt;WiC&lt;/th&gt; &#xA;   &lt;th&gt;CB&lt;/th&gt; &#xA;   &lt;th&gt;MultiRC&lt;/th&gt; &#xA;   &lt;th&gt;BoolQ&lt;/th&gt; &#xA;   &lt;th&gt;ReCoRD&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-10B&lt;/td&gt; &#xA;   &lt;td&gt;98.0&lt;/td&gt; &#xA;   &lt;td&gt;95.2&lt;/td&gt; &#xA;   &lt;td&gt;93.1&lt;/td&gt; &#xA;   &lt;td&gt;75.7&lt;/td&gt; &#xA;   &lt;td&gt;98.7/98.2&lt;/td&gt; &#xA;   &lt;td&gt;88.1/63.3&lt;/td&gt; &#xA;   &lt;td&gt;88.7&lt;/td&gt; &#xA;   &lt;td&gt;94.4/94.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/DeBERTa/tree/master/experiments/superglue&#34;&gt;DeBERTa-XXLarge-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;97.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;93.5&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;87.8/63.6&lt;/td&gt; &#xA;   &lt;td&gt;88.3&lt;/td&gt; &#xA;   &lt;td&gt;94.1/93.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Seq2Seq&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/abisee/cnn-dailymail&#34;&gt;CNN/Daily Mail&lt;/a&gt; (test set, no additional data used)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;ROUGE-1&lt;/th&gt; &#xA;   &lt;th&gt;ROUGE-2&lt;/th&gt; &#xA;   &lt;th&gt;ROUGE-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-10B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;44.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;21.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;41.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5-11B&lt;/td&gt; &#xA;   &lt;td&gt;43.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;21.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;40.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PEGASUS-Large&lt;/td&gt; &#xA;   &lt;td&gt;44.2&lt;/td&gt; &#xA;   &lt;td&gt;21.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;41.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BART-Large&lt;/td&gt; &#xA;   &lt;td&gt;44.2&lt;/td&gt; &#xA;   &lt;td&gt;21.3&lt;/td&gt; &#xA;   &lt;td&gt;40.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/EdinburghNLP/XSum&#34;&gt;XSum&lt;/a&gt; (test set, no additional data used)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;ROUGE-1&lt;/th&gt; &#xA;   &lt;th&gt;ROUGE-2&lt;/th&gt; &#xA;   &lt;th&gt;ROUGE-L&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-10B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;48.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;25.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;40.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PEGASUS-Large&lt;/td&gt; &#xA;   &lt;td&gt;47.2&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;   &lt;td&gt;39.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BART-Large&lt;/td&gt; &#xA;   &lt;td&gt;45.1&lt;/td&gt; &#xA;   &lt;td&gt;22.3&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Language Modeling&lt;/h3&gt; &#xA;&lt;p&gt;test set, zero-shot&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LAMBADA (accuracy)&lt;/th&gt; &#xA;   &lt;th&gt;Wikitext103 (perplexity)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-10B (bi)&lt;/td&gt; &#xA;   &lt;td&gt;72.35&lt;/td&gt; &#xA;   &lt;td&gt;11.33&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-10B (uni)&lt;/td&gt; &#xA;   &lt;td&gt;67.18&lt;/td&gt; &#xA;   &lt;td&gt;12.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;52.66&lt;/td&gt; &#xA;   &lt;td&gt;17.48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Megatron-LM (8.3B)&lt;/td&gt; &#xA;   &lt;td&gt;66.51&lt;/td&gt; &#xA;   &lt;td&gt;10.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Turing-NLG&lt;/td&gt; &#xA;   &lt;td&gt;67.98&lt;/td&gt; &#xA;   &lt;td&gt;10.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Hugging Face Hub&lt;/h3&gt; &#xA;&lt;p&gt;You can access GLM models via HuggingFace Hub. Please install &lt;code&gt;transformers&amp;gt;=4.23.1&lt;/code&gt; and find all the available models &lt;a href=&#34;https://huggingface.co/models?filter=glm,thudm&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Generation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForSeq2SeqLM&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/glm-10b&#34;, trust_remote_code=True)&#xA;model = AutoModelForSeq2SeqLM.from_pretrained(&#34;THUDM/glm-10b&#34;, trust_remote_code=True)&#xA;model = model.half().cuda()&#xA;model.eval()&#xA;&#xA;# Inference&#xA;inputs = tokenizer(&#34;Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.&#34;, return_tensors=&#34;pt&#34;)&#xA;inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=512)&#xA;inputs = inputs.to(&#39;cuda&#39;)&#xA;outputs = model.generate(**inputs, max_length=512, eos_token_id=tokenizer.eop_token_id)&#xA;print(tokenizer.decode(outputs[0].tolist()))&#xA;&#xA;# Training&#xA;inputs = tokenizer(&#xA;    [&#34;Tsinghua University is located in [MASK].&#34;, &#34;One minus one equals zero, is it correct? Answer: [MASK]&#34;],&#xA;    return_tensors=&#34;pt&#34;, padding=True)&#xA;inputs = tokenizer.build_inputs_for_generation(inputs, targets=[&#34;Beijing&#34;, &#34;No&#34;], max_gen_length=8, padding=False)&#xA;inputs = inputs.to(&#39;cuda&#39;)&#xA;outputs = model(**inputs)&#xA;loss = outputs.loss&#xA;logits = outputs.logits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Classification&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForMultipleChoice&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/glm-10b&#34;, trust_remote_code=True)&#xA;model = AutoModelForMultipleChoice.from_pretrained(&#34;THUDM/glm-10b&#34;, trust_remote_code=True)&#xA;model = model.half().cuda()&#xA;model.eval()&#xA;&#xA;inputs = tokenizer([&#34;Tsinghua University is located in [MASK].&#34;,&#xA;                    &#34;One minus one equals zero, is it correct? Answer: [MASK]&#34;], return_tensors=&#34;pt&#34;, padding=True)&#xA;choices = [[&#34;Beijing&#34;, &#34;Shanghai&#34;], [&#34;Yes&#34;, &#34;No&#34;]]&#xA;inputs = tokenizer.build_inputs_for_multiple_choice(inputs, choices)&#xA;inputs = inputs.to(&#39;cuda&#39;)&#xA;outputs = model(**inputs)&#xA;logits = outputs.logits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also convert the finetuned checkpoints with &lt;code&gt;scripts/convert_glm_checkpoint_to_transformers.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;p&gt;We prepare two docker images based on CUDA 10.2 and CUDA 11.2. You can pull the pre-built images from Docker Hub and run with docker v19.03+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --gpus all --rm -it --ipc=host zxdu20/glm-cuda102&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or replace &lt;code&gt;glm-cuda102&lt;/code&gt; with &lt;code&gt;glm-cuda112&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also modify the image according to your requirements in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/docker/cuda102.dockerfile&#34;&gt;docker/cuda102.dockerfile&lt;/a&gt; and build the image yourself&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  docker build -f cuda102.dockerfile . -t glm-cuda102&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Manual Installation&lt;/h3&gt; &#xA;&lt;p&gt;Please first install PyTorch (we use 1.7.0) and &lt;a href=&#34;https://github.com/NVIDIA/apex&#34;&gt;apex&lt;/a&gt;, and then install other dependencies by &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Clone this repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/THUDM/GLM&#xA;cd GLM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model Parallelism&lt;/h3&gt; &#xA;&lt;p&gt;If your encounter the &lt;code&gt;CUDA out of memory&lt;/code&gt; error, which means you GPU memory is limited, you can try the model parallelism to divide the parameters into multiple GPUs. Take the two-way model parallelism as an example. First run &lt;code&gt;change_mp.py&lt;/code&gt; to divide the checkpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python change_mp.py path_to_the_checkpoint 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then update the checkpoint path in the model config file (such as &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/config_tasks/model_blocklm_10B.sh&#34;&gt;config_tasks/model_blocklm_10B.sh&lt;/a&gt;) and change &lt;code&gt;MP_SIZE&lt;/code&gt; in the script (such as &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/scripts/ds_finetune_superglue.sh&#34;&gt;scripts/ds_finetune_superglue.sh&lt;/a&gt;) to &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We provide scripts for finetuning GLM on some downstream tasks.&lt;/p&gt; &#xA;&lt;h3&gt;Left-to-Right Generation / Blank Filling (Interactive)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; to your local path. Run the following script&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/generate_block.sh \&#xA;     config_tasks/model_blocklm_10B_chinese.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some models (GLM-2B, GLM-10B, and GLM-10B-Chinese) use three different mask tokens: &lt;code&gt;[MASK]&lt;/code&gt; for short blank filling, &lt;code&gt;[sMASK]&lt;/code&gt; for sentence filling, and &lt;code&gt;[gMASK]&lt;/code&gt; for left-to-right generation.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Examples&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;h4&gt;Usage of &lt;code&gt;[MASK]&lt;/code&gt; (Entity Prediction):&lt;/h4&gt; &#xA; &lt;h5&gt;Example1&lt;/h5&gt; &#xA; &lt;p&gt;Context: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.&lt;/p&gt; &#xA; &lt;p&gt;GLM: the stanford university&lt;/p&gt; &#xA; &lt;h5&gt;Example2 (Chinese)&lt;/h5&gt; &#xA; &lt;p&gt;Context: 凯旋门位于意大利米兰市古城堡旁。1807年为纪念[MASK]而建，门高25米，顶上矗立两武士青铜古兵车铸像。&lt;/p&gt; &#xA; &lt;p&gt;GLM:拿破仑军队攻克米兰城&lt;/p&gt; &#xA; &lt;h4&gt;Usage of &lt;code&gt;[sMASK]&lt;/code&gt; (Sentence Prediction)&lt;/h4&gt; &#xA; &lt;h5&gt;Example3&lt;/h5&gt; &#xA; &lt;p&gt;Context: There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). [sMASK] We propose a General Language Model ( GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large, demonstrating its generalizability to different downstream tasks.&lt;/p&gt; &#xA; &lt;p&gt;GLM: However, there is a growing need to develop a single pretraining model that is not only good at natural language understanding (NLU) or dialog generation/generation (dialog), but is also able to predict other tasks such as sentiment analysis, conditional generation, or machine translation (MT).&lt;/p&gt; &#xA; &lt;h5&gt;Example4 (Chinese)&lt;/h5&gt; &#xA; &lt;p&gt;Context: 工业互联网（Industrial Internet）是新一代信息通信技术与工业经济深度融合的新型基础设施、应用模式和工业生态，通过对人、机、物、系统等的全面连接，构建起覆盖全产业链、全价值链的全新制造和服务体系，为工业乃至产业数字化、网络化、智能化发展提供了实现途径，是第四次工业革命的重要基石。[sMASK] 它以网络为基础、平台为中枢、数据为要素、安全为保障，既是工业数字化、网络化、智能化转型的基础设施，也是互联网、大数据、人工智能与实体经济深度融合的应用模式，同时也是一种新业态、新产业，将重塑企业形态、供应链和产业链。当前，工业互联网融合应用向国民经济重点行业广泛拓展，形成平台化设计、智能化制造、网络化协同、个性化定制、服务化延伸、数字化管理六大新模式，赋能、赋智、赋值作用不断显现，有力的促进了实体经济提质、增效、降本、绿色、安全发展。&lt;/p&gt; &#xA; &lt;p&gt;GLM: 工业互联网是制造业技术、管理、模式的重大变革,是推动互联网、大数据、人工智能和实体经济深度融合的重要载体,是建设制造强国和网络强国的重要基础。&lt;/p&gt; &#xA; &lt;h4&gt;Usage of &lt;code&gt;[gMASK]&lt;/code&gt; (Long Text Generation)&lt;/h4&gt; &#xA; &lt;h5&gt;Example5 (Chinese)&lt;/h5&gt; &#xA; &lt;p&gt;Context: 问题：冬天，中国哪座城市最适合避寒？问题描述：能推荐一些国内适合冬天避寒的城市吗？回答用户：旅游爱好者 回答： [gMASK]&lt;/p&gt; &#xA; &lt;p&gt;GLM: 海南三亚,三亚的冬天是三亚的旺季,因为冬天国内大部分地方都有雾霾,而三亚却是没有雾霾的地方,所以三亚是冬天最惬意不过的地方了。在东北长大的我觉得三亚简直就是一个天堂,冬天去海口三亚旅游,享受冬天,享受阳光沙滩。但是海口却是很干燥,需要多喝水。 三亚冬天最热门的玩法就是晒太阳,在海边晒晒太阳,在沙滩上晒晒太阳,感觉整个人都得到了解放。三亚还有一个特色项目,就是海上冲浪,在三亚的沙滩上冲浪也是一件非常刺激的事情。 海口,海口冬季的阳光十分温暖,海南的冬季也是属于冬季旅游的旺季。冬季的海口最棒的是去海南的热带野生动植物园,那里有数之不尽的热带小动物,在这里可以近距离的和它们接触,海南的热带野生动植物园也是海南的天然氧吧。还可以在海口观澜湖公园里感受海口美丽的海景。 贵阳,贵州的冬天也是十分温暖的,贵阳也是冬季避寒很好的城市之一。冬季去贵阳玩一定要去黔灵山,黔灵山是贵州香火很旺盛的一个寺庙,寺庙的冬季香火鼎盛,在冬季去寺庙游玩也是一个很好的体验。除了黔灵山,贵阳在冬季还有花溪公园可以去玩,花溪公园也是去当地公园玩最好的选择。 青岛,青岛的冬天是青岛最舒服的时候,青岛有很多海滨浴场,冬天去海边泡一泡温泉,然后晒晒太阳是一件十分惬意的事情。青岛也有沙滩,冬天在沙滩上晒晒太阳,看看海,再玩玩沙滩游戏,感觉十分快乐的事。&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You can also add multiple &lt;code&gt;[MASK]&lt;/code&gt; and &lt;code&gt;[sMASK]&lt;/code&gt; in a single example. The model will fill the blanks one by one from left to right. The answer to each blank always begins with a special &lt;code&gt;&amp;lt;|startofpiece|&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Examples&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;h5&gt;Example1&lt;/h5&gt; &#xA; &lt;p&gt;Context: There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and [MASK] (e.g., T5). [sMASK] We propose a General Language Model ( GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over [MASK] on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and [MASK], GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks.&lt;/p&gt; &#xA; &lt;p&gt;GLM: &amp;lt;|startofpiece|&amp;gt; blank filling models&amp;lt;|startofpiece|&amp;gt; However, most of them cannot easily transfer to other downstream tasks due to the different characteristics of these tasks.&amp;lt;|startofpiece|&amp;gt; other pretrained models&amp;lt;|startofpiece|&amp;gt; unconditional reading, and semantic role labeling tasks&lt;/p&gt; &#xA; &lt;h5&gt;Example2 (Chinese)&lt;/h5&gt; &#xA; &lt;p&gt;Context: 工业互联网（Industrial Internet）是新一代[MASK]与[MASK]深度融合的新型基础设施、应用模式和工业生态，通过对人、机、物、系统等的全面连接，构建起覆盖全产业链、全价值链的全新制造和服务体系，为工业乃至产业数字化、网络化、智能化发展提供了实现途径，是第四次工业革命的重要基石。[sMASK] 它以网络为基础、平台为中枢、数据为要素、安全为保障，既是工业数字化、网络化、智能化转型的基础设施，也是互联网、大数据、人工智能与实体经济深度融合的应用模式，同时也是一种新业态、新产业，将重塑企业形态、供应链和产业链。当前，工业互联网融合应用向国民经济重点行业广泛拓展，形成[MASK]、智能化制造、[MASK]、个性化定制、服务化延伸、数字化管理六大新模式，赋能、赋智、赋值作用不断显现，有力的促进了实体经济提质、增效、降本、绿色、安全发展。&lt;/p&gt; &#xA; &lt;p&gt;GLM: &amp;lt;|startofpiece|&amp;gt;信息技术(ICT)&amp;lt;|startofpiece|&amp;gt;工业经济(II2O)&amp;lt;|startofpiece|&amp;gt;我国工业互联网是面向工业全领域、全流程、全体系的互联网,具有多产业、多领域融合的特点。&amp;lt;|startofpiece|&amp;gt;网络化协同&amp;lt;|startofpiece|&amp;gt;平台企业&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;SuperGLUE&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the &lt;a href=&#34;https://super.gluebenchmark.com/tasks&#34;&gt;SuperGlue&lt;/a&gt; data and check the experiment setup in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/scripts/ds_finetune_superglue.sh&#34;&gt;scripts/ds_finetune_superglue.sh&lt;/a&gt;. Note that &lt;code&gt;DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH&lt;/code&gt; need to be changed to your local path. You may also change the &lt;code&gt;batch-size&lt;/code&gt; and &lt;code&gt;nproc_per_node&lt;/code&gt; according to your available hardware.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following script (use the COPA dataset as an example)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/ds_finetune_superglue.sh \&#xA;     config_tasks/model_blocklm_10B.sh \&#xA;     config_tasks/task_copa.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We also implement &lt;a href=&#34;https://arxiv.org/abs/2103.10385&#34;&gt;P-Tuning&lt;/a&gt; in our code. Run the following script to integrate p-tuning:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/ds_finetune_superglue_prompt.sh \&#xA;     config_tasks/model_blocklm_10B.sh \&#xA;     config_tasks/task_copa.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To apply GLM to a new NLU dataset with cloze-filling finetuning, implement a &lt;code&gt;DataProcessor&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/tasks/superglue/dataset.py&#34;&gt;tasks/superglue/dataset.py&lt;/a&gt; for data loading and add a &lt;code&gt;PVP&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/tasks/superglue/pvp.py&#34;&gt;tasks/superglue/pvp.py&lt;/a&gt; for the cloze question. More details can be found &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/tasks/superglue/README.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Seq2Seq&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the &lt;a href=&#34;https://github.com/harvardnlp/sent-summary&#34;&gt;Gigaword&lt;/a&gt; , &lt;a href=&#34;https://github.com/artmatsak/cnn-dailymail&#34;&gt;CNN/Daily Mail&lt;/a&gt; or &lt;a href=&#34;https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset&#34;&gt;XSum&lt;/a&gt; dataset and check the experiment setup in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/scripts/ds_finetune_seq2seq.sh&#34;&gt;scripts/ds_finetune_seq2seq.sh&lt;/a&gt;. Change &lt;code&gt;DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH&lt;/code&gt; to your local path.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following script (use the CNN/Daily Mail dataset as an example)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/ds_finetune_seq2seq.sh \ &#xA;   config_tasks/model_blocklm_10B.sh \ &#xA;   config_tasks/seq_cnndm_org.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The summaries are written into &lt;code&gt;./runs/experiment_name/test.jsonl.hyps&lt;/code&gt;. The references are written into &lt;code&gt;test.jsonl.refs&lt;/code&gt; in the same directory. For calculating rouge, install &lt;a href=&#34;https://github.com/pltrdy/files2rouge&#34;&gt;file2rouge&lt;/a&gt; and download Stanford CoreNLP from &lt;a href=&#34;http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip&#34;&gt;here&lt;/a&gt;. Run the following script&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/evaluate_seq2seq.sh \&#xA; ./runs/experiment_name/test.jsonl.hyps ./runs/experiment_name/test.jsonl.refs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Train with your own data&lt;/h4&gt; &#xA;&lt;p&gt;Process your seq2seq data into &lt;code&gt;{split}.source&lt;/code&gt; and &lt;code&gt;{split}.target&lt;/code&gt;, with each line being the context or the target of a sample, and &lt;code&gt;split&lt;/code&gt; being &lt;code&gt;train&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;, and &lt;code&gt;test&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run the following script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/ds_finetune_seq2seq.sh \ &#xA;   config_tasks/model_blocklm_10B.sh \ &#xA;   config_tasks/seq_customization.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify the hyperparameters in &lt;code&gt;config_tasks/seq_customization.sh&lt;/code&gt; and &lt;code&gt;config_tasks/config_blocklm_10B_cnndm.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multiple Choice (Zero-shot)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/evaluate_multichoice.sh config_tasks/model_blocklm_10B.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; and &lt;code&gt;DATA_PATH&lt;/code&gt; need to be changed to your local path.&lt;/p&gt; &#xA;&lt;p&gt;The format of each line of the data file should be&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#34;inputs_pretokenized&#34;: &#34;Context and question here&#34;, &#34;choices_pretokenized&#34;: [&#34;Choice 1&#34;, &#34;Choice 2&#34;, &#34;Choice 3&#34;], &#34;label&#34;: int}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Language Modeling&lt;/h3&gt; &#xA;&lt;h4&gt;LAMBADA Cloze Accuracy&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the &lt;a href=&#34;https://github.com/cybertronai/bflm/raw/master/lambada_test.jsonl&#34;&gt;LAMBADA&lt;/a&gt; data and change &lt;code&gt;DATA_ROOT, CHECKPOINT_PATH&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/scripts/evaluate_lm.sh&#34;&gt;scripts/evaluate_lm.sh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the following script&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/evaluate_lm.sh \ &#xA;     config_tasks/model_blocklm_large_generation.sh \&#xA;     config_tasks/zero_lambada.sh &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;LM Perplexity&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download our &lt;a href=&#34;https://mailstsinghuaeducn-my.sharepoint.com/:t:/g/personal/duzx16_mails_tsinghua_edu_cn/EQa_B6KY_q1FjtUeG-T52iMBFtNrfhfHcZbzMxfkJKXKRQ?e=inTdHh&#34;&gt;test set of wikibook&lt;/a&gt; or &lt;a href=&#34;https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip&#34;&gt;Wikitext103&lt;/a&gt; dataset and change &lt;code&gt;DATA_ROOT, CHECKPOINT_PATH&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/scripts/evaluate_lm.sh&#34;&gt;scripts/evaluate_lm.sh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the following script &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/evaluate_lm.sh \ &#xA;   config_tasks/model_blocklm_large_generation.sh \&#xA;   config_tasks/zero_wikitext.sh &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Text Infilling&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the &lt;a href=&#34;https://github.com/Varal7/blank_language_model&#34;&gt;Yahoo&lt;/a&gt; dataset and check the experiment setup in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/scripts/finetune_blank.sh&#34;&gt;scripts/finetune_blank.sh&lt;/a&gt;. Change &lt;code&gt;DATA_ROOT, CHECKPOINT_PATH, SAVE_PATH&lt;/code&gt; to your local path.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following script&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/finetune_blank.sh \ &#xA;     config_tasks/model_blocklm_large.sh \ &#xA;     config_tasks/seq_blank.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretrain&lt;/h2&gt; &#xA;&lt;p&gt;Run the following script to pre-train the GLM-Large model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash scripts/ds_pretrain_nvidia.sh config/ds_block_large.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/scripts/ds_pretrain_nvidia.sh&#34;&gt;scripts/ds_pretrain_nvidia.sh&lt;/a&gt; launches the training program with DeepSpeed. You should change &lt;code&gt;NUM_WORKERS&lt;/code&gt; and &lt;code&gt;NUM_GPUS_PER_WORKER&lt;/code&gt; to the number of workers and the number of gpus per worker. Also change &lt;code&gt;HOST_FILE_PATH&lt;/code&gt; to the path to an OpenMPI-style hostfile. More details about DeepSpeed launcher can be found &lt;a href=&#34;https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The file &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/config/ds_block_large.sh&#34;&gt;config/ds_block_large.sh&lt;/a&gt; defines the hyperparameters for pretraining. Most of the arguments are fairly self-explanatory. Specifically, &lt;code&gt;--train-data&lt;/code&gt; can be multiple keywords defined in &lt;code&gt;NAMED_CORPORA&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM/main/data_utils/corpora.py&#34;&gt;data_utils/corpora.py&lt;/a&gt;. The hyperparameters of the optimizer are defined in the corresponding json file under &lt;code&gt;config&lt;/code&gt;. The semantics of the json file can be found &lt;a href=&#34;https://www.deepspeed.ai/docs/config-json&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Part of the code is based on &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt; and &lt;a href=&#34;https://github.com/timoschick/pet&#34;&gt;PET&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please cite our paper if you find this code useful for your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{DBLP:conf/acl/DuQLDQY022,&#xA;  author    = {Zhengxiao Du and&#xA;               Yujie Qian and&#xA;               Xiao Liu and&#xA;               Ming Ding and&#xA;               Jiezhong Qiu and&#xA;               Zhilin Yang and&#xA;               Jie Tang},&#xA;  title     = {{GLM:} General Language Model Pretraining with Autoregressive Blank Infilling},&#xA;  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational&#xA;               Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,&#xA;               May 22-27, 2022},&#xA;  pages     = {320--335},&#xA;  publisher = {Association for Computational Linguistics},&#xA;  year      = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Akegarasu/ChatGLM-webui</title>
    <updated>2023-03-19T01:44:52Z</updated>
    <id>tag:github.com,2023-03-19:/Akegarasu/ChatGLM-webui</id>
    <link href="https://github.com/Akegarasu/ChatGLM-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A WebUI for ChatGLM-6B&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM-webui&lt;/h1&gt; &#xA;&lt;p&gt;A webui for ChatGLM made by THUDM. &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;chatglm-6b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;requirements&lt;/h3&gt; &#xA;&lt;p&gt;python3.10&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117&#xA;pip install --upgrade -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Args&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;--model-path&lt;/code&gt;: specify model path. If this parameter is not specified manually, the default value is &lt;code&gt;THUDM/chatglm-6b&lt;/code&gt;. Transformers will automatically download model from huggingface.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--listen&lt;/code&gt;: launch gradio with 0.0.0.0 as server name, allowing to respond to network requests&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--port&lt;/code&gt;: webui port&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--share&lt;/code&gt;: use gradio to share&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--precision&lt;/code&gt;: fp16, int4, int8&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--cpu&lt;/code&gt;: use cpu&lt;/p&gt;</summary>
  </entry>
</feed>