<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-21T01:38:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ShihuaHuang95/DEIM</title>
    <updated>2025-03-21T01:38:50Z</updated>
    <id>tag:github.com,2025-03-21:/ShihuaHuang95/DEIM</id>
    <link href="https://github.com/ShihuaHuang95/DEIM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2025] DEIM: DETR with Improved Matching for Fast Convergence&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt; DEIM: DETR with Improved Matching for Fast Convergence &lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM/raw/master/LICENSE&#34;&gt; &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/badge/LICENSE-Apache%202.0-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2412.04234&#34;&gt; &lt;img alt=&#34;arXiv&#34; src=&#34;https://img.shields.io/badge/arXiv-2412.04234-red&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.shihuahuang.cn/DEIM/&#34;&gt; &lt;img alt=&#34;project webpage&#34; src=&#34;https://img.shields.io/badge/Webpage-DEIM-purple&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM/pulls&#34;&gt; &lt;img alt=&#34;prs&#34; src=&#34;https://img.shields.io/github/issues-pr/ShihuaHuang95/DEIM&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM/issues&#34;&gt; &lt;img alt=&#34;issues&#34; src=&#34;https://img.shields.io/github/issues/ShihuaHuang95/DEIM?color=olive&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM&#34;&gt; &lt;img alt=&#34;stars&#34; src=&#34;https://img.shields.io/github/stars/ShihuaHuang95/DEIM&#34;&gt; &lt;/a&gt; &lt;a href=&#34;mailto:shihuahuang95@gmail.com&#34;&gt; &lt;img alt=&#34;Contact Us&#34; src=&#34;https://img.shields.io/badge/Contact-Email-yellow&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; DEIM is an advanced training framework designed to enhance the matching mechanism in DETRs, enabling faster convergence and improved accuracy. It serves as a robust foundation for future research and applications in the field of real-time object detection. &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;http://www.shihuahuang.cn&#34;&gt;Shihua Huang&lt;/a&gt;&#xA; &lt;sup&gt;1&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=tIFWBcQAAAAJ&amp;amp;hl=en&#34;&gt;Zhichao Lu&lt;/a&gt;&#xA; &lt;sup&gt;2&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://vinthony.github.io/academic/&#34;&gt;Xiaodong Cun&lt;/a&gt;&#xA; &lt;sup&gt;3&lt;/sup&gt;, Yongjun Yu&#xA; &lt;sup&gt;1&lt;/sup&gt;, Xiao Zhou&#xA; &lt;sup&gt;4&lt;/sup&gt;, &#xA; &lt;a href=&#34;https://xishen0220.github.io&#34;&gt;Xi Shen&lt;/a&gt;&#xA; &lt;sup&gt;1*&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt; 1. Intellindust AI Lab &amp;nbsp; 2. City University of Hong Kong &amp;nbsp; 3. Great Bay University &amp;nbsp; 4. Hefei Normal University &lt;/i&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; **üìß Corresponding author:** &lt;a href=&#34;mailto:shenxiluc@gmail.com&#34;&gt;shenxiluc@gmail.com&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=deim-detr-with-improved-matching-for-fast&#34;&gt; &lt;img alt=&#34;sota&#34; src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/deim-detr-with-improved-matching-for-fast/real-time-object-detection-on-coco&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;If you like our work, please give us a ‚≠ê!&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/figures/teaser_a.png&#34; alt=&#34;Image 1&#34; width=&#34;49%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/figures/teaser_b.png&#34; alt=&#34;Image 2&#34; width=&#34;49%&#34;&gt; &lt;/p&gt;  &#xA;&lt;h2&gt;üöÄ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2025.03.12]&lt;/strong&gt; The Object365 Pretrained &lt;a href=&#34;https://drive.google.com/file/d/1RMNrHh3bYN0FfT5ZlWhXtQxkG23xb2xj/view?usp=drive_link&#34;&gt;DEIM-D-FINE-X&lt;/a&gt; model is released, which achieves 59.5% AP after fine-tuning 24 COCO epochs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2025.03.05]&lt;/strong&gt; The Nano DEIM model is released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2025.02.27]&lt;/strong&gt; The DEIM paper is accepted to CVPR 2025. Thanks to all co-authors.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.12.26]&lt;/strong&gt; A more efficient implementation of Dense O2O, achieving nearly a 30% improvement in loading speed (See &lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM/pull/13&#34;&gt;the pull request&lt;/a&gt; for more details). Huge thanks to my colleague &lt;a href=&#34;https://github.com/capsule2077&#34;&gt;Longfei Liu&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.12.03]&lt;/strong&gt; Release DEIM series. Besides, this repo also supports the re-implmentations of &lt;a href=&#34;https://arxiv.org/abs/2410.13842&#34;&gt;D-FINE&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2407.17140&#34;&gt;RT-DETR&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Content&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#1-model-zoo&#34;&gt;1. Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#2-quick-start&#34;&gt;2. Quick start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#3-usage&#34;&gt;3. Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#4-tools&#34;&gt;4. Tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#5-citation&#34;&gt;5. Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#6-acknowledgement&#34;&gt;6. Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. Model Zoo&lt;/h2&gt; &#xA;&lt;h3&gt;DEIM-D-FINE&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;D-FINE&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;DEIM&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;N&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;42.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;43.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.12ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_dfine/deim_hgnetv2_n_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ZPEhiU9nhW4M5jLnYOFwTSLQC1Ugf62e/view?usp=sharing&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;48.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.49ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_dfine/deim_hgnetv2_s_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1tB8gVJNrfb6dhFvoHJECKOF5VpkthhfC/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;52.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;52.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.62ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_dfine/deim_hgnetv2_m_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/18Lj2a6UN6k_n_UzqnJyiaiLGpDzQQit8/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;L&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_dfine/deim_hgnetv2_l_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1PIRf02XkrA2xAD3wEiKE2FaamZgSGTAr/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;X&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.89ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;202&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_dfine/deim_hgnetv2_x_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1dPtbgtGgq1Oa7k_LgH1GXPelg1IVeu0j/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;DEIM-RT-DETRv2&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;RT-DETRv2&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;DEIM&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;47.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.59ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_rtdetrv2/deim_r18vd_120e_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/153_JKff6EpFgiLKaqkJsoDcLal_0ux_F/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.40ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_rtdetrv2/deim_r34vd_120e_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1O9RjZF6kdFWGv1Etn1Toml4r-YfdMDMM/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;M&lt;/strong&gt;*&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.90ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_rtdetrv2/deim_r50vd_m_60e_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/10dLuqdBZ6H5ip9BbBiE6S7ZcmHkRbD0E/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;L&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.15ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;136&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_rtdetrv2/deim_r50vd_60e_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1mWknAXD5JYknUQ94WCEvPfXz13jcNOTI/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;X&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.66ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;259&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_rtdetrv2/deim_r101vd_60e_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1BIevZijOcBO17llTyDX32F_pYppBfnzu/view?usp=drive_link&#34;&gt;ckpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;2. Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n deim python=3.11.9&#xA;conda activate deim&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; COCO2017 Dataset &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download COCO2017 from &lt;a href=&#34;https://opendatalab.com/OpenDataLab/COCO_2017&#34;&gt;OpenDataLab&lt;/a&gt; or &lt;a href=&#34;https://cocodataset.org/#download&#34;&gt;COCO&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Modify paths in &lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/dataset/coco_detection.yml&#34;&gt;coco_detection.yml&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    img_folder: /data/COCO2017/train2017/&#xA;    ann_file: /data/COCO2017/annotations/instances_train2017.json&#xA;val_dataloader:&#xA;    img_folder: /data/COCO2017/val2017/&#xA;    ann_file: /data/COCO2017/annotations/instances_val2017.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Custom Dataset&lt;/summary&gt; &#xA; &lt;p&gt;To train on your custom dataset, you need to organize it in the COCO format. Follow the steps below to prepare your dataset:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set &lt;code&gt;remap_mscoco_category&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This prevents the automatic remapping of category IDs to match the MSCOCO categories.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;remap_mscoco_category: False&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize Images:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Structure your dataset directories as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset/&#xA;‚îú‚îÄ‚îÄ images/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ train/&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image2.jpg&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ val/&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image2.jpg&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îî‚îÄ‚îÄ annotations/&#xA;    ‚îú‚îÄ‚îÄ instances_train.json&#xA;    ‚îú‚îÄ‚îÄ instances_val.json&#xA;    ‚îî‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;images/train/&lt;/code&gt;&lt;/strong&gt;: Contains all training images.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;images/val/&lt;/code&gt;&lt;/strong&gt;: Contains all validation images.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;annotations/&lt;/code&gt;&lt;/strong&gt;: Contains COCO-formatted annotation files.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Convert Annotations to COCO Format:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If your annotations are not already in COCO format, you&#39;ll need to convert them. You can use the following Python script as a reference or utilize existing tools:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;&#xA;def convert_to_coco(input_annotations, output_annotations):&#xA;    # Implement conversion logic here&#xA;    pass&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    convert_to_coco(&#39;path/to/your_annotations.json&#39;, &#39;dataset/annotations/instances_train.json&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update Configuration Files:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/dataset/custom_detection.yml&#34;&gt;custom_detection.yml&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;task: detection&#xA;&#xA;evaluator:&#xA;  type: CocoEvaluator&#xA;  iou_types: [&#39;bbox&#39;, ]&#xA;&#xA;num_classes: 777 # your dataset classes&#xA;remap_mscoco_category: False&#xA;&#xA;train_dataloader:&#xA;  type: DataLoader&#xA;  dataset:&#xA;    type: CocoDetection&#xA;    img_folder: /data/yourdataset/train&#xA;    ann_file: /data/yourdataset/train/train.json&#xA;    return_masks: False&#xA;    transforms:&#xA;      type: Compose&#xA;      ops: ~&#xA;  shuffle: True&#xA;  num_workers: 4&#xA;  drop_last: True&#xA;  collate_fn:&#xA;    type: BatchImageCollateFunction&#xA;&#xA;val_dataloader:&#xA;  type: DataLoader&#xA;  dataset:&#xA;    type: CocoDetection&#xA;    img_folder: /data/yourdataset/val&#xA;    ann_file: /data/yourdataset/val/ann.json&#xA;    return_masks: False&#xA;    transforms:&#xA;      type: Compose&#xA;      ops: ~&#xA;  shuffle: False&#xA;  num_workers: 4&#xA;  drop_last: False&#xA;  collate_fn:&#xA;    type: BatchImageCollateFunction&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;3. Usage&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; COCO2017 &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Training&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml --use-amp --seed=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Testing&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml --test-only -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;3. Tuning &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Tuning&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Customizing Batch Size &lt;/summary&gt; &#xA; &lt;p&gt;For example, if you want to double the total batch size when training D-FINE-L on COCO2017, here are the steps you should follow:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/base/dataloader.yml&#34;&gt;dataloader.yml&lt;/a&gt;&lt;/strong&gt; to increase the &lt;code&gt;total_batch_size&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    total_batch_size: 64  # Previously it was 32, now doubled&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/deim_dfine/deim_hgnetv2_l_coco.yml&#34;&gt;deim_hgnetv2_l_coco.yml&lt;/a&gt;&lt;/strong&gt;. Here‚Äôs how the key parameters should be adjusted:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;optimizer:&#xA;type: AdamW&#xA;params:&#xA;    -&#xA;    params: &#39;^(?=.*backbone)(?!.*norm|bn).*$&#39;&#xA;    lr: 0.000025  # doubled, linear scaling law&#xA;    -&#xA;    params: &#39;^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$&#39;&#xA;    weight_decay: 0.&#xA;&#xA;lr: 0.0005  # doubled, linear scaling law&#xA;betas: [0.9, 0.999]&#xA;weight_decay: 0.0001  # need a grid search&#xA;&#xA;ema:  # added EMA settings&#xA;    decay: 0.9998  # adjusted by 1 - (1 - decay) * 2&#xA;    warmups: 500  # halved&#xA;&#xA;lr_warmup_scheduler:&#xA;    warmup_duration: 250  # halved&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Customizing Input Size &lt;/summary&gt; &#xA; &lt;p&gt;If you&#39;d like to train &lt;strong&gt;DEIM&lt;/strong&gt; on COCO2017 with an input size of 320x320, follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/base/dataloader.yml&#34;&gt;dataloader.yml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;&#xA;train_dataloader:&#xA;dataset:&#xA;    transforms:&#xA;        ops:&#xA;            - {type: Resize, size: [320, 320], }&#xA;collate_fn:&#xA;    base_size: 320&#xA;dataset:&#xA;    transforms:&#xA;        ops:&#xA;            - {type: Resize, size: [320, 320], }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/ShihuaHuang95/DEIM/main/configs/base/dfine_hgnetv2.yml&#34;&gt;dfine_hgnetv2.yml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;eval_spatial_size: [320, 320]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;4. Tools&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Deployment &lt;/summary&gt; &#xA; &lt;!-- &lt;summary&gt;4. Export onnx &lt;/summary&gt; --&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install onnx onnxsim&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Export onnx&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/deployment/export_onnx.py --check -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Export &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html&#34;&gt;tensorrt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;trtexec --onnx=&#34;model.onnx&#34; --saveEngine=&#34;model.engine&#34; --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Inference (Visualization) &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r tools/inference/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;5. Inference &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Inference (onnxruntime / tensorrt / torch)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Inference on images and videos is now supported.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg  # video.mp4&#xA;python tools/inference/trt_inf.py --trt model.engine --input image.jpg&#xA;python tools/inference/torch_inf.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Benchmark &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r tools/benchmark/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;6. Benchmark &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Model FLOPs, MACs, and Params&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/benchmark/get_info.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;TensorRT Latency&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Fiftyone Visualization &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install fiftyone&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Voxel51 Fiftyone Visualization (&lt;a href=&#34;https://github.com/voxel51/fiftyone&#34;&gt;fiftyone&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/visualization/fiftyone_vis.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Others &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Auto Resume Training&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash reference/safe_training.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Converting Model Weights&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python reference/convert_weight.py model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;5. Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;DEIM&lt;/code&gt; or its methods in your work, please cite the following BibTeX entries:&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; bibtex &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{huang2024deim,&#xA;      title={DEIM: DETR with Improved Matching for Fast Convergence},&#xA;      author={Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, and Xi Shen},&#xA;      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},&#xA;      year={2025},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;6. Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our work is built upon &lt;a href=&#34;https://github.com/Peterande/D-FINE&#34;&gt;D-FINE&lt;/a&gt; and &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt;RT-DETR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚ú® Feel free to contribute and reach out if you have any questions! ‚ú®&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/markitdown</title>
    <updated>2025-03-21T01:38:50Z</updated>
    <id>tag:github.com,2025-03-21:/microsoft/markitdown</id>
    <link href="https://github.com/microsoft/markitdown" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/markitdown/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/markitdown.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/dd/markitdown&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue&#34; alt=&#34;Built by AutoGen Team&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install &#39;markitdown[all]~=0.1.0a1&#39;&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; &#xA;  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href=&#34;https://github.com/deanmalmgren/textract&#34;&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; &#xA;&lt;p&gt;At present, MarkItDown supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PDF&lt;/li&gt; &#xA; &lt;li&gt;PowerPoint&lt;/li&gt; &#xA; &lt;li&gt;Word&lt;/li&gt; &#xA; &lt;li&gt;Excel&lt;/li&gt; &#xA; &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; &#xA; &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; &#xA; &lt;li&gt;HTML&lt;/li&gt; &#xA; &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; &#xA; &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; &#xA; &lt;li&gt;Youtube URLs&lt;/li&gt; &#xA; &lt;li&gt;EPubs&lt;/li&gt; &#xA; &lt;li&gt;... and more!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why Markdown?&lt;/h2&gt; &#xA;&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI&#39;s GPT-4o, natively &#34;&lt;em&gt;speak&lt;/em&gt;&#34; Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install &#39;markitdown[all]~=0.1.0a1&#39;&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:microsoft/markitdown.git&#xA;cd markitdown&#xA;pip install -e packages/markitdown[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Command-Line&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown path-to-file.pdf &amp;gt; document.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown path-to-file.pdf -o document.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pipe content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat path-to-file.pdf | markitdown&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Optional Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install markitdown[pdf, docx, pptx]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; &#xA;&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Plugins&lt;/h3&gt; &#xA;&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown --list-plugins&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable plugins use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown --use-plugins path-to-file.pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; &#xA;&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;markitdown path-to-file.pdf -o document.md -d -e &#34;&amp;lt;document_intelligence_endpoint&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python API&lt;/h3&gt; &#xA;&lt;p&gt;Basic usage in Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from markitdown import MarkItDown&#xA;&#xA;md = MarkItDown(enable_plugins=False) # Set to True to enable plugins&#xA;result = md.convert(&#34;test.xlsx&#34;)&#xA;print(result.text_content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from markitdown import MarkItDown&#xA;&#xA;md = MarkItDown(docintel_endpoint=&#34;&amp;lt;document_intelligence_endpoint&amp;gt;&#34;)&#xA;result = md.convert(&#34;test.pdf&#34;)&#xA;print(result.text_content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use Large Language Models for image descriptions, provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from markitdown import MarkItDown&#xA;from openai import OpenAI&#xA;&#xA;client = OpenAI()&#xA;md = MarkItDown(llm_client=client, llm_model=&#34;gpt-4o&#34;)&#xA;result = md.convert(&#34;example.jpg&#34;)&#xA;print(result.text_content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker build -t markitdown:latest .&#xA;docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h3&gt;How to Contribute&lt;/h3&gt; &#xA;&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#39;open for contribution&#39; and &#39;open for reviewing&#39; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;/th&gt; &#xA;    &lt;th&gt;All&lt;/th&gt; &#xA;    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/issues&#34;&gt;All Issues&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22&#34;&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/pulls&#34;&gt;All PRs&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22&#34;&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd packages/markitdown&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/&#xA;hatch shell&#xA;hatch test&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Reopen the project in Devcontainer and run:&#xA;hatch test&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; &#xA;&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/physicsnemo</title>
    <updated>2025-03-21T01:38:50Z</updated>
    <id>tag:github.com,2025-03-21:/NVIDIA/physicsnemo</id>
    <link href="https://github.com/NVIDIA/physicsnemo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA PhysicsNeMo&lt;/h1&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;p&gt;üìù NVIDIA Modulus has been renamed to NVIDIA PhysicsNeMo&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.repostatus.org/#active&#34;&gt;&lt;img src=&#34;https://www.repostatus.org/badges/latest/active.svg?sanitize=true&#34; alt=&#34;Project Status: Active - The project has reached a stable, usable state and is being actively developed.&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/physicsnemo/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/NVIDIA/physicsnemo&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- markdownlint-enable --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#getting-started&#34;&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#installation&#34;&gt;&lt;strong&gt;Install guide&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#contributing-to-physicsnemo&#34;&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#resources&#34;&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#physicsnemo-migration-guide&#34;&gt;&lt;strong&gt;PhysicsNeMo Migration Guide&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#communication&#34;&gt;&lt;strong&gt;Communication&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#license&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is PhysicsNeMo?&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA PhysicsNeMo is an open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art SciML methods for AI4science and engineering.&lt;/p&gt; &#xA;&lt;p&gt;PhysicsNeMo provides utilities and optimized pipelines to develop AI models that combine physics knowledge with data, enabling real-time predictions.&lt;/p&gt; &#xA;&lt;p&gt;Whether you are exploring the use of Neural operators, GNNs, or transformers or are interested in Physics-informed Neural Networks or a hybrid approach in between, PhysicsNeMo provides you with an optimized stack that will enable you to train your models at scale.&lt;/p&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/Knowledge_guided_models.gif&#34; alt=&#34;PhysicsNeMo&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- markdownlint-enable --&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#more-about-physicsnemo&#34;&gt;More About PhysicsNeMo&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#scalable-gpu-optimized-training-library&#34;&gt;Scalable GPU-optimized training Library&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#a-suite-of-physics-informed-ml-models&#34;&gt;A suite of Physics-Informed ML Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#seamless-pytorch-integration&#34;&gt;Seamless PyTorch Integration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#easy-customization-and-extension&#34;&gt;Easy Customization and Extension&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#ai4science-library&#34;&gt;AI4Science Library&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#domain-specific-packages&#34;&gt;Domain Specific Packages&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#who-is-using-and-contributing-to-physicsnemo&#34;&gt;Who is contributing to PhysicsNeMo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#why-are-they-using-physicsnemo&#34;&gt;Why use PhysicsNeMo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#contributing-to-physicsnemo&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#communication&#34;&gt;Communication&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;More About PhysicsNeMo&lt;/h2&gt; &#xA;&lt;p&gt;At a granular level, PhysicsNeMo provides a library of a few key components:&lt;/p&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Component&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html&#34;&gt;&lt;strong&gt;physicsnemo.models&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A collection of optimized, customizable, and easy-to-use models such as Fourier Neural Operators, Graph Neural Networks, and many more&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html&#34;&gt;&lt;strong&gt;physicsnemo.datapipes&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A data pipeline and data loader library, including benchmark datapipes, weather daptapipes, and graph datapipes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html&#34;&gt;&lt;strong&gt;physicsnemo.distributed&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A distributed computing library build on top of &lt;code&gt;torch.distributed&lt;/code&gt; to enable parallel training with just a few steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/csg_and_tessellated_module.html&#34;&gt;&lt;strong&gt;physicsnemo.sym.geometry&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A library to handle geometry for DL training using the Constructive Solid Geometry modeling and CAD files in STL format.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/nodes.html&#34;&gt;&lt;strong&gt;physicsnemo.sym.eq&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A library to use PDEs in your DL training with several implementations of commonly observed equations and easy ways for customization.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- markdownlint-enable --&gt; &#xA;&lt;p&gt;For a complete list, refer to the PhysicsNeMo API documentation for &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html&#34;&gt;PhysicsNeMo Core&lt;/a&gt; and &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/api/api_index.html&#34;&gt;PhysicsNeMo Sym&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Usually, PhysicsNeMo is used either as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A complementary tool to Pytorch when exploring AI for SciML and AI4Science applications.&lt;/li&gt; &#xA; &lt;li&gt;A deep learning research platform that provides scale and optimal performance on NVIDIA GPUs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Elaborating Further:&lt;/p&gt; &#xA;&lt;h3&gt;Scalable GPU-optimized training Library&lt;/h3&gt; &#xA;&lt;p&gt;PhysicsNeMo provides a highly optimized and scalable training library for maximizing the power of NVIDIA GPUs. &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html&#34;&gt;Distributed computing&lt;/a&gt; utilities allow for efficient scaling from a single GPU to multi-node GPU clusters with a few lines of code, ensuring that large-scale. physics-informed machine learning (ML) models can be trained quickly and effectively. The framework includes support for advanced. &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.utils.html#module-physicsnemo.utils.capture&#34;&gt;optimization utilities&lt;/a&gt;, &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html&#34;&gt;tailor made datapipes&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/physicsnemo-sym/tree/main/physicsnemo/sym/eq&#34;&gt;validation utilities&lt;/a&gt; to enhance the end to end training speed.&lt;/p&gt; &#xA;&lt;h3&gt;A suite of Physics Informed ML Models&lt;/h3&gt; &#xA;&lt;p&gt;PhysicsNeMo offers a comprehensive library of state-of-the-art models specifically designed for physics-ML applications. The &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#model-zoo&#34;&gt;Model Zoo&lt;/a&gt; includes generalizable model architectures such as &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/fno&#34;&gt;Fourier Neural Operators (FNOs)&lt;/a&gt;, &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/neural_operators/deeponet.html&#34;&gt;DeepONet&lt;/a&gt;, &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/foundational/1d_wave_equation.html&#34;&gt;Physics-Informed Neural Networks (PINNs)&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/gnn_layers&#34;&gt;Graph Neural Networks (GNNs)&lt;/a&gt;, and generative AI models like &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/diffusion&#34;&gt;Diffusion Models&lt;/a&gt; as well as domain-specific models such as &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/dlwp&#34;&gt;Deep Learning Weather Prediction (DLWP)&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/physicsnemo/models/srrn&#34;&gt;Super Resolution Network (SrNN)&lt;/a&gt; among others. These models are optimized for various physics domains, such as computational fluid dynamics, structural mechanics, and electromagnetics. Users can download, customize, and build upon these models to suit their specific needs, significantly reducing the time required to develop high-fidelity simulations.&lt;/p&gt; &#xA;&lt;h3&gt;Seamless PyTorch Integration&lt;/h3&gt; &#xA;&lt;p&gt;PhysicsNeMo is built on top of PyTorch, providing a familiar and user-friendly experience for those already proficient with PyTorch. This includes a simple Python interface and modular design, making it easy to use PhysicsNeMo with existing PyTorch workflows. Users can leverage the extensive PyTorch ecosystem, including its libraries and tools while benefiting from PhysicsNeMo&#39;s specialized capabilities for physics-ML. This seamless integration ensures users can quickly adopt PhysicsNeMo without a steep learning curve.&lt;/p&gt; &#xA;&lt;p&gt;For more information, refer &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#converting-pytorch-models-to-physicsnemo-models&#34;&gt;Converting PyTorch Models to PhysicsNeMo Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Easy Customization and Extension&lt;/h3&gt; &#xA;&lt;p&gt;PhysicsNeMo is designed to be highly extensible, allowing users to add new functionality with minimal effort. The framework provides Pythonic APIs for defining new physics models, geometries, and constraints, making it easy to extend its capabilities to new use cases. The adaptability of PhysicsNeMo is further enhanced by key features such as &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.deploy.html&#34;&gt;ONNX support&lt;/a&gt; for flexible model deployment, robust &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.logging.html&#34;&gt;logging utilities&lt;/a&gt; for streamlined error handling, and efficient &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.utils.html#module-physicsnemo.launch.utils.checkpoint&#34;&gt;checkpointing&lt;/a&gt; to simplify model loading and saving.&lt;/p&gt; &#xA;&lt;p&gt;This extensibility ensures that PhysicsNeMo can adapt to the evolving needs of researchers and engineers, facilitating the development of innovative solutions in the field of physics-ML.&lt;/p&gt; &#xA;&lt;p&gt;Detailed information on features and capabilities can be found in the &lt;a href=&#34;https://docs.nvidia.com/physicsnemo/index.html#core&#34;&gt;PhysicsNeMo documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/examples/README.md&#34;&gt;Reference samples&lt;/a&gt; cover a broad spectrum of physics-constrained and data-driven workflows to suit the diversity of use cases in the science and engineering disciplines.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Have questions about how PhysicsNeMo can assist you? Try our [Experimental] chatbot, &lt;a href=&#34;https://chatgpt.com/g/g-PXrBv20SC-modulus-guide&#34;&gt;PhysicsNeMo Guide&lt;/a&gt;, for answers.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Hello world&lt;/h3&gt; &#xA;&lt;p&gt;You can start using PhysicsNeMo in your PyTorch code as simple as shown here:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; from physicsnemo.models.mlp.fully_connected import FullyConnected&#xA;&amp;gt;&amp;gt;&amp;gt; model = FullyConnected(in_features=32, out_features=64)&#xA;&amp;gt;&amp;gt;&amp;gt; input = torch.randn(128, 32)&#xA;&amp;gt;&amp;gt;&amp;gt; output = model(input)&#xA;&amp;gt;&amp;gt;&amp;gt; output.shape&#xA;torch.Size([128, 64])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;AI4Science Library&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/physicsnemo-sym&#34;&gt;PhysicsNeMo Symbolic&lt;/a&gt;: This repository of algorithms and utilities allows SciML researchers and developers to physics inform model training and model validation. It also provides a higher level abstraction for domain experts that is native to science and engineering.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Domain Specific Packages&lt;/h4&gt; &#xA;&lt;p&gt;The following are packages dedicated for domain experts of specific communities catering to their unique exploration needs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/earth2studio&#34;&gt;Earth-2 Studio&lt;/a&gt;: Open source project to enable climate researchers and scientists to explore and experiment with AI models for weather and climate.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Research packages&lt;/h4&gt; &#xA;&lt;p&gt;The following are research packages that get packaged into PhysicsNeMo once they are stable.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/modulus-makani&#34;&gt;PhysicsNeMo Makani&lt;/a&gt;: Experimental library designed to enable the research and development of machine-learning based weather and climate models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVlabs/earth2grid&#34;&gt;Earth2 Grid&lt;/a&gt;: Experimental library with utilities for working geographic data defined on various grids.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/earth2mip&#34;&gt;Earth-2 MIP&lt;/a&gt;: Experimental library with utilities for model intercomparison for weather and climate models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Who is using and contributing to PhysicsNeMo&lt;/h2&gt; &#xA;&lt;p&gt;PhysicsNeMo is an open source project and gets contributions from researchers in the SciML and AI4science fields. While PhysicsNeMo team works on optimizing the underlying SW stack, the community collaborates and contributes model architectures, datasets, and reference applications so we can innovate in the pursuit of developing generalizable model architectures and algorithms.&lt;/p&gt; &#xA;&lt;p&gt;Some latest examples of community contributors are &lt;a href=&#34;https://developer.nvidia.com/blog/spotlight-hp-3d-printing-and-nvidia-physicsnemo-collaborate-on-open-source-manufacturing-digital-twin/&#34;&gt;HP Labs 3D Printing team&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/&#34;&gt;Stanford Cardiovascular research team&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/physicsnemo/tree/main/examples/cfd/mhd_pino&#34;&gt;UIUC team&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/physicsnemo/tree/main/examples/generative/diffusion&#34;&gt;CMU team&lt;/a&gt; etc.&lt;/p&gt; &#xA;&lt;p&gt;Latest examples of research teams using PhysicsNeMo are &lt;a href=&#34;https://arxiv.org/abs/2404.05768&#34;&gt;ORNL team&lt;/a&gt;, &lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtc24-s62237/&#34;&gt;TU Munich CFD team&lt;/a&gt; etc.&lt;/p&gt; &#xA;&lt;p&gt;Please navigate to this page for a complete list of research work leveraging PhysicsNeMo. For a list of enterprises using PhysicsNeMo refer &lt;a href=&#34;https://developer.nvidia.com/physicsnemo&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Using PhysicsNeMo and interested in showcasing your work on &lt;a href=&#34;https://developer.nvidia.com/blog/category/simulation-modeling-design/&#34;&gt;NVIDIA Blogs&lt;/a&gt;? Fill out this &lt;a href=&#34;https://forms.gle/XsBdWp3ji67yZAUF7&#34;&gt;proposal form&lt;/a&gt; and we will get back to you!&lt;/p&gt; &#xA;&lt;h2&gt;Why are they using PhysicsNeMo&lt;/h2&gt; &#xA;&lt;p&gt;Here are some of the key benefits of PhysicsNeMo for SciML model development:&lt;/p&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/benchmarking.svg?sanitize=true&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/recipe.svg?sanitize=true&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/performance.svg?sanitize=true&#34; width=&#34;100&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SciML Benchmarking and validation&lt;/td&gt; &#xA;   &lt;td&gt;Ease of using generalized SciML recipes with heterogenous datasets&lt;/td&gt; &#xA;   &lt;td&gt;Out of the box performance and scalability&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PhysicsNeMo enables researchers to benchmark their AI model against proven architectures for standard benchmark problems with detailed domain-specific validation criteria.&lt;/td&gt; &#xA;   &lt;td&gt;PhysicsNeMo enables researchers to pick from SOTA SciML architectures and use built-in data pipelines for their use case.&lt;/td&gt; &#xA;   &lt;td&gt;PhysicsNeMo provides out-of-the-box performant training pipelines including optimized ETL pipelines for heterogrneous engineering and scientific datasets and out of the box scaling across multi-GPU and multi-node GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- markdownlint-enable --&gt; &#xA;&lt;p&gt;See what your peer SciML researchers are saying about PhysicsNeMo (Coming soon).&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;The following resources will help you in learning how to use PhysicsNeMo. The best way is to start with a reference sample and then update it for your own use case.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-custom-models-in-physicsnemo&#34;&gt;Using PhysicsNeMo with your PyTorch model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-built-in-models&#34;&gt;Using PhysicsNeMo built-in models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/getting-started/index.html&#34;&gt;Getting started Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/physicsnemo/raw/main/examples/README.md&#34;&gt;Reference Samples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html&#34;&gt;User guide Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtc24-dlit61460/?playlistId=playList-bd07f4dc-1397-4783-a959-65cec79aa985&#34;&gt;Getting started Webinar&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openhackathons-org/End-to-End-AI-for-Science&#34;&gt;AI4Science PhysicsNeMo Bootcamp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://catalog.ngc.nvidia.com/models?filters=&amp;amp;orderBy=scoreDESC&amp;amp;query=Modulus&amp;amp;page=&amp;amp;pageSize=&#34;&gt;PhysicsNeMo Pretrained models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://catalog.ngc.nvidia.com/resources?filters=&amp;amp;orderBy=scoreDESC&amp;amp;query=Modulus&amp;amp;page=&amp;amp;pageSize=&#34;&gt;PhysicsNeMo Datasets and Supplementary materials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-OV-04+V1&#34;&gt;Self-paced PhysicsNeMo DLI training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/deep-learning-for-science-and-engineering/&#34;&gt;Deep Learning for Science and Engineering Lecture Series with PhysicsNeMo&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/dliteachingkit-setk5002/&#34;&gt;PhysicsNeMo: purpose and usage&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/search/?facet.mimetype%5B%5D=event%20session&amp;amp;layout=list&amp;amp;page=1&amp;amp;q=modulus&amp;amp;sort=relevance&amp;amp;sortDir=desc&#34;&gt;Video Tutorials&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;PyPi&lt;/h3&gt; &#xA;&lt;p&gt;The recommended method for installing the latest version of PhysicsNeMo is using PyPi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;pip install nvidia-physicsnemo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The installation can be verified by running the hello world example as demonstrated &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/#hello-world&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Optional dependencies&lt;/h4&gt; &#xA;&lt;p&gt;PhysicsNeMo has many optional dependencies that are used in specific components. When using pip, all dependencies used in PhysicsNeMo can be installed with &lt;code&gt;pip install nvidia-physicsnemo[all]&lt;/code&gt;. If you are developing PhysicsNeMo, developer dependencies can be installed using &lt;code&gt;pip install nvidia-physicsnemo[dev]&lt;/code&gt;. Otherwise, additional dependencies can be installed on a case by case basis. Detailed information on installing the optional dependencies can be found in the &lt;a href=&#34;https://docs.nvidia.com/deeplearning/physicsnemo/getting-started/index.html&#34;&gt;Getting Started Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;NVCR Container&lt;/h3&gt; &#xA;&lt;p&gt;The recommended PhysicsNeMo docker image can be pulled from the &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/teams/physicsnemo/containers/physicsnemo&#34;&gt;NVIDIA Container Registry&lt;/a&gt; (refer to the NGC registry for the latest tag):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;docker pull nvcr.io/nvidia/physicsnemo/physicsnemo:25.03&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inside the container, you can clone the PhysicsNeMo git repositories and get started with the examples. The below command shows the instructions to launch the physicsnemo container and run examples from this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --runtime nvidia \&#xA;--rm -it nvcr.io/nvidia/physicsnemo/physicsnemo:25.03 bash&#xA;git clone https://github.com/NVIDIA/physicsnemo.git&#xA;cd physicsnemo/examples/cfd/darcy_fno/&#xA;pip install warp-lang # install NVIDIA Warp to run the darcy example&#xA;python train_fno_darcy.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For enterprise supported NVAIE container, refer &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/teams/modulus/containers/modulus-sfb&#34;&gt;PhysicsNeMo Secured Feature Branch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;From Source&lt;/h2&gt; &#xA;&lt;h3&gt;Package&lt;/h3&gt; &#xA;&lt;p&gt;For a local build of the PhysicsNeMo Python package from source use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Bash&#34;&gt;git clone git@github.com:NVIDIA/physicsnemo.git &amp;amp;&amp;amp; cd physicsnemo&#xA;&#xA;pip install --upgrade pip&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Source Container&lt;/h3&gt; &#xA;&lt;p&gt;To build PhysicsNeMo docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t physicsnemo:deploy \&#xA;    --build-arg TARGETPLATFORM=linux/amd64 --target deploy -f Dockerfile .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can run &lt;code&gt;make container-deploy&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To build CI image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t physicsnemo:ci \&#xA;    --build-arg TARGETPLATFORM=linux/amd64 --target ci -f Dockerfile .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can run &lt;code&gt;make container-ci&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently, only &lt;code&gt;linux/amd64&lt;/code&gt; and &lt;code&gt;linux/arm64&lt;/code&gt; platforms are supported. If using &lt;code&gt;linux/arm64&lt;/code&gt;, some dependencies like &lt;code&gt;warp-lang&lt;/code&gt; might not install correctly.&lt;/p&gt; &#xA;&lt;h2&gt;PhysicsNeMo Migration Guide&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA Modulus has been renamed to NVIDIA PhysicsNeMo. For migration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;pip install nvidia-physicsnemo&lt;/code&gt; rather than &lt;code&gt;pip install nvidia-modulus&lt;/code&gt; for PyPi wheels.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;nvcr.io/nvidia/physicsnemo/physicsnemo:&amp;lt;tag&amp;gt;&lt;/code&gt; rather than &lt;code&gt;nvcr.io/nvidia/modulus/modulus:&amp;lt;tag&amp;gt;&lt;/code&gt; for Docker containers.&lt;/li&gt; &#xA; &lt;li&gt;Replace &lt;code&gt;nvidia-modulus&lt;/code&gt; by &lt;code&gt;nvidia-physicsnemo&lt;/code&gt; in your pip requirements files (&lt;code&gt;requirements.txt&lt;/code&gt;, &lt;code&gt;setup.py&lt;/code&gt;, &lt;code&gt;setup.cfg&lt;/code&gt;, &lt;code&gt;pyproject.toml&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;In your code, change the import statements from &lt;code&gt;import modulus&lt;/code&gt; to &lt;code&gt;import physicsnemo&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The old PyPi registry and the NGC container registry will be deprecated soon and will not receive any bug fixes/updates. The old checkpoints will remain compatible with these updates.&lt;/p&gt; &#xA;&lt;p&gt;More details to follow soon.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing to PhysicsNeMo&lt;/h2&gt; &#xA;&lt;p&gt;PhysicsNeMo is an open source collaboration and its success is rooted in community contribution to further the field of Physics-ML. Thank you for contributing to the project so others can build on top of your contribution.&lt;/p&gt; &#xA;&lt;p&gt;For guidance on contributing to PhysicsNeMo, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Cite PhysicsNeMo&lt;/h2&gt; &#xA;&lt;p&gt;If PhysicsNeMo helped your research and you would like to cite it, please refer to the &lt;a href=&#34;https://github.com/NVIDIA/physicsnemo/raw/main/CITATION.cff&#34;&gt;guidelines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Github Discussions: Discuss new architectures, implementations, Physics-ML research, etc.&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: Bug reports, feature requests, install issues, etc.&lt;/li&gt; &#xA; &lt;li&gt;PhysicsNeMo Forum: The &lt;a href=&#34;https://forums.developer.nvidia.com/t/welcome-to-the-physicsnemo-ml-model-framework-forum/178556&#34;&gt;PhysicsNeMo Forum&lt;/a&gt; hosts an audience of new to moderate-level users and developers for general chat, online discussions, collaboration, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Feedback&lt;/h2&gt; &#xA;&lt;p&gt;Want to suggest some improvements to PhysicsNeMo? Use our feedback form &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSfX4zZ0Lp7MMxzi3xqvzX4IQDdWbkNh5H_a_clzIhclE2oSBQ/viewform?usp=sf_link&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PhysicsNeMo is provided under the Apache License 2.0, please see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/LICENSE.txt&#34;&gt;LICENSE.txt&lt;/a&gt; for full license text. Enterprise SLA, support and preview access are available under NVAIE.&lt;/p&gt;</summary>
  </entry>
</feed>