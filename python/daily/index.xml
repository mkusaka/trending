<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-27T01:33:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MatthewKuKanich/FindMyFlipper</title>
    <updated>2024-03-27T01:33:56Z</updated>
    <id>tag:github.com,2024-03-27:/MatthewKuKanich/FindMyFlipper</id>
    <link href="https://github.com/MatthewKuKanich/FindMyFlipper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The FindMy Flipper app turns your FlipperZero into an AirTag or other tracking device, compatible with both Apple AirTag and Samsung SmartTag. It uses the BLE beacon to broadcast, allowing users to clone existing tags, generate OpenHaystack key pairs for Apple&#39;s FindMy network, and customize beacon intervals and transmit power.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FindMy Flipper - FindMy SmartTag Emulator&lt;/h1&gt; &#xA;&lt;p&gt;This app extends the functionality of the FlipperZero&#39;s bluetooth capabilities, enabling it to act as an Apple AirTag or Samsung SmartTag, or even both simultaneously. It utilizes the FlipperZero&#39;s BLE beacon to broadcast a SmartTag signal to be picked up by the FindMy Network. I made this to serve as a versatile tool for tracking purposes, offering the ability to clone existing tags, generate OpenHaystack key pairs for integration with Apple&#39;s FindMy network, and tune the device&#39;s beacon broadcast settings.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Tag Emulation: Clone your existing Apple AirTag or Samsung SmartTag to the FlipperZero, or generate a key pair for use with the FindMy network without owning an actual AirTag.&lt;/li&gt; &#xA; &lt;li&gt;Customization: Users can adjust the interval between beacon broadcasts and modify the transmit power to suit their needs, optimizing for both visibility and battery life.&lt;/li&gt; &#xA; &lt;li&gt;Efficient Background Operation: The app is optimized to run in the background, ensuring that your FlipperZero can still be tracked with minimal battery usage and without stopping normal use.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1: Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Option A:&lt;/strong&gt; Use the released/precompiled firmware appropriate (FAP) for your device.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Option B:&lt;/strong&gt; Build the firmware yourself using &lt;code&gt;fbt/ufbt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Both Installation options require you to be running a dev build of firmware. When release gets access to the extra BLE beacon this will change, thank you!&lt;/li&gt; &#xA; &lt;li&gt;All firmware should now work with main branch, including icons&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 2: Obtaining SmartTag Data&lt;/h3&gt; &#xA;&lt;h6&gt;There are 2 methods to get SmartTag data depending on the type of tag you wish to emulate. Option A allows you to use Apple, Samsung, and Tile tags through the use of cloning the MAC Address and Payload of an actual tag. This also allows you to use the native app for tracking (Apple FindMy, Samsung SmartThing, Tile App). Option B allows you to emulate an Apple AirTag without needing to own an Apple device or airtag. This is done through key generation and requires a computer to download the location data.&lt;/h6&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Option A: Cloning Existing Tag (Preferred and allows you to track without additional setup)&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Pair a Tag:&lt;/strong&gt; First, pair an AirTag or Samsung SmartTag with your device.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Enter &#39;Lost&#39; Mode:&lt;/strong&gt; Keep the tag away from the device it&#39;s registered to for approximately 15 minutes.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Download nrfConnect or use an ESP32&lt;/strong&gt; Install nrfConnect from the Google Play Store. (Apple version doesn&#39;t reveal the needed Raw data, looking for a workaround)&lt;/li&gt; &#xA;  &lt;li&gt;OR &lt;strong&gt;Use an ESP32-WROOM / ESP32-S3&lt;/strong&gt; Don&#39;t have an android? No problem! You can get all the data you need from an ESP32: &lt;a href=&#34;https://github.com/MatthewKuKanich/ESP32-AirTag-Scanner&#34;&gt;https://github.com/MatthewKuKanich/ESP32-AirTag-Scanner&lt;/a&gt; (Skip to step 7 if using an ESP32)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Filter and Scan:&lt;/strong&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Open the app, click on filters, and exclude all except for the brand of your tag (Apple/Samsung).&lt;/li&gt; &#xA;    &lt;li&gt;Adjust the RSSI to the lowest setting (-40 dBm).&lt;/li&gt; &#xA;    &lt;li&gt;Initiate a scan. Wait for your SmartTag to appear as a &#34;FindMy&#34; device.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Capture Data:&lt;/strong&gt; Click &lt;strong&gt;Raw&lt;/strong&gt; or &lt;strong&gt;View Raw&lt;/strong&gt; to capture your &lt;strong&gt;payload&lt;/strong&gt; and note your tag&#39;s &lt;strong&gt;MAC Address&lt;/strong&gt;. Immediately remove the tag&#39;s battery to prevent key/MAC rotation.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Enter Data in FlipperZero App:&lt;/strong&gt; Input the captured &lt;strong&gt;payload&lt;/strong&gt; and &lt;strong&gt;MAC Address&lt;/strong&gt; into the FlipperZero app.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Option B: AirTag Key Generation&lt;/summary&gt; &#xA; &lt;p&gt;Video Tutorial: &lt;a href=&#34;https://youtu.be/XGwHmwvQoqo?si=CAsKWEqGP5VFi9p9&#34;&gt;https://youtu.be/XGwHmwvQoqo?si=CAsKWEqGP5VFi9p9&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA; &lt;p&gt;Before you begin, ensure you have the following installed on your system:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Docker Desktop&lt;/li&gt; &#xA;  &lt;li&gt;Python (likely already installed)&lt;/li&gt; &#xA;  &lt;li&gt;Git&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;Step-by-Step Instructions&lt;/h2&gt; &#xA; &lt;h3&gt;1. Clone the Repository&lt;/h3&gt; &#xA; &lt;p&gt;Navigate to Matthew KuKanich&#39;s GitHub repository, copy the repository URL, and clone it to your desired location using the terminal.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;git clone https://github.com/MatthewKuKanich/FindMyFlipper.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;2. Set Up the AirTag Generation Folder&lt;/h3&gt; &#xA; &lt;p&gt;Inside the cloned repository, locate the &#39;air tag generation&#39; folder which contains all necessary files for creating AirTags.&lt;/p&gt; &#xA; &lt;h3&gt;3. Start Docker Desktop&lt;/h3&gt; &#xA; &lt;p&gt;Ensure Docker Desktop is running on your computer, as it is required for the server setup.&lt;/p&gt; &#xA; &lt;h3&gt;4. Set Up a Server Using Docker&lt;/h3&gt; &#xA; &lt;p&gt;Run the following Docker command to set up the server. This server emulates an environment that tricks Apple&#39;s authentication servers.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;docker run -d --restart always --name anisette-v3 -p 6969:6969 dadoum/anisette-v3-server:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;5. Create a Python Virtual Environment&lt;/h3&gt; &#xA; &lt;p&gt;Navigate to the AirTag generation directory, then create and activate a Python virtual environment.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;cd AirTagGeneration&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code&gt;python3 -m venv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;(or &lt;code&gt;python -m venv venv&lt;/code&gt;)&lt;/p&gt; &#xA; &lt;p&gt;Activate the environment:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Windows:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;.\venv\Scripts\activate.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Mac/Linux:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;6. Install the Required Python Packages&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;7. Generate Keys for AirTags&lt;/h3&gt; &#xA; &lt;p&gt;Run the &lt;code&gt;generate_keys.py&lt;/code&gt; script to generate the keys needed for AirTags, which will be saved in a new folder called &#39;keys&#39;.&lt;/p&gt; &#xA; &lt;h3&gt;8. Transfer the Generated Keys to Flipper Zero&lt;/h3&gt; &#xA; &lt;p&gt;Move the &#39;.Keys&#39; file to your Flipper device by connecting it to your computer and using the Flipper&#39;s file management system.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;For ease of use, drag your &lt;code&gt;.keys&lt;/code&gt; file onto your FlipperZero&#39;s SD card in the apps_data-&amp;gt;findmy folder. You can import it directly from the app! &#xA;   &lt;ol&gt; &#xA;    &lt;li&gt;Open the app and navigate to the config menu.&lt;/li&gt; &#xA;    &lt;li&gt;Choose &#34;register tag&#34; and select the tag type.&lt;/li&gt; &#xA;    &lt;li&gt;Either click import &lt;code&gt;.keys&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, or enter Manually.&lt;/li&gt; &#xA;    &lt;li&gt;If entering manually then a MAC and payload dialog will appear next. Enter your &lt;strong&gt;MAC&lt;/strong&gt; then &lt;strong&gt;Payload&lt;/strong&gt; here.&lt;/li&gt; &#xA;    &lt;li&gt;Click save.&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;9. Request Location Reports&lt;/h3&gt; &#xA; &lt;p&gt;Use the &lt;code&gt;request_reports.py&lt;/code&gt; script to request real-time location data, requiring your Apple ID and password for authentication. This will save your Apple login information to a auth file so you won&#39;t need to re-enter your Apple credentials.&lt;/p&gt; &#xA; &lt;h3&gt;10. Generate an Advanced Location Map&lt;/h3&gt; &#xA; &lt;p&gt;Finally, run the &lt;code&gt;RequestReport&amp;amp;Map.py&lt;/code&gt; script to generate an interactive map of all location data in the past 24 hours. This script automates the process by requesting the location report using the hashed adv key in your &lt;code&gt;keys&lt;/code&gt; folder, then decrypting that data from your private key located in the same &lt;code&gt;.keys&lt;/code&gt; file. After the data is decrypted it will be displayed in the terminal. It then launches a mapping script that maps all the coordinates, connects them to show movement, displays a plethora of location metadata, and saves to an html file named by the date of the report.&lt;/p&gt; &#xA; &lt;p&gt;You&#39;re done!&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;If you want to use OpenHaystack or Macless instead, then you can follow the steps below. I don&#39;t recommend these methods due to reliability issues and setup complexity. To use OpenHayStack for tracking, you must use MacOS lower than version 14 (Mail Plug-in Incompetiablity of MacOS 14+ seemoo-lab/openhaystack#224). If you do own a device, I believe a convertor script can be provided without much of effort. If you do not own a Mac device or the system has been upgraded to 14 and beyond. The alternative solutions includes,&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/dchristl/macless-haystack&#34;&gt;https://github.com/dchristl/macless-haystack&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;If using this solution, be sure to only use the &lt;code&gt;generate_keys.py&lt;/code&gt; script from this repo in the AirTagGeneration folder. Not the ones included in that repo as the formatting of the key file changes. (Mine includes data that the FlipperZero needs for proper importing)&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;On The Flipper: Configuration on the FlipperZero (if not completed yet)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Upon launching the app, open the config menu and either click &lt;code&gt;Import Tag From File&lt;/code&gt; or &lt;code&gt;Register Tag Manually&lt;/code&gt;. Put your generated .keys file onto the FlipperZero SD card inside the AppsData/FindMyFlipper folder to import from file. Or you can manually enter the tag information. When using the cloning method, you can export a .txt file from nrfConnect (click save button) amd place that in the same folder in order to import.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Customization&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Beacon Interval: Adjust how frequently your FlipperZero broadcasts its presence.&lt;/li&gt; &#xA; &lt;li&gt;Transmit Power: Increase or decrease the signal strength to balance between tracking range and battery life.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Background Use&lt;/p&gt; &#xA;&lt;p&gt;The app is designed to have a negligible impact on battery life, even when running in the background. This allows for continuous tracking without the need for frequent recharging.&lt;/p&gt; &#xA;&lt;p&gt;Compatibility&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple devices for AirTag tracking via the FindMy network.&lt;/li&gt; &#xA; &lt;li&gt;Any device that supports Samsung SmartTag tracking, including web browsers (previously FindMyMobile).&lt;/li&gt; &#xA; &lt;li&gt;Tile Trackers via the Tile App&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Huge thanks to all the people that contributed to the OpenHaystack project, supporting projects, and guides on the subject. This wouldn&#39;t be a thing without any of you! Special thanks to WillyJL for helping get the app input working and overall overhaul of the apps functions!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Legal and Privacy&lt;/p&gt; &#xA;&lt;p&gt;This app is intended for personal and educational use. Users are responsible for complying with local privacy laws and regulations regarding tracking devices. The cloning and emulation of tracking tags should be done responsibly and with respect to the ownership of the original devices.&lt;/p&gt; &#xA;&lt;p&gt;Disclaimer&lt;/p&gt; &#xA;&lt;p&gt;This project is not affiliated with Apple Inc. or Samsung. All product names, logos, and brands are property of their respective owners. Use this app responsibly and ethically.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mistralai-sf24/hackathon</title>
    <updated>2024-03-27T01:33:56Z</updated>
    <id>tag:github.com,2024-03-27:/mistralai-sf24/hackathon</id>
    <link href="https://github.com/mistralai-sf24/hackathon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mistral Transformer&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains minimal code to run our 7B model and to finetune it.&lt;br&gt; Blog: &lt;a href=&#34;https://mistral.ai/news/announcing-mistral-7b/&#34;&gt;https://mistral.ai/news/announcing-mistral-7b/&lt;/a&gt;&lt;br&gt; Discord: &lt;a href=&#34;https://discord.com/invite/mistralai&#34;&gt;https://discord.com/invite/mistralai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Download the model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://models.mistralcdn.com/mistral-7b-v0-1/mistral-7B-v0.1.tar&#xA;tar -xf mistral-7B-v0.1.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The unzipped folder can be used as &lt;code&gt;initial_model_path:&lt;/code&gt; in the training config.&lt;/p&gt; &#xA;&lt;h3&gt;Installation Hackathon&lt;/h3&gt; &#xA;&lt;p&gt;Upon running the &lt;a href=&#34;http://ghcr.io/coreweave/ml-containers/torch-extras:a5a99e8-nccl-cuda12.2.2-ubuntu22.04-nccl2.19.3-1-torch2.2.0-vision0.17.0-audio2.2.0&#34;&gt;Docker container&lt;/a&gt;, all necessary dependencies can be installed with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_hackathon.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using the trained model&lt;/h2&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;deploy&lt;/code&gt; folder contains code to build a &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; image with the required dependencies to serve the Mistral AI model. In the image, the &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;transformers&lt;/a&gt; library is used instead of the reference implementation. To build it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build deploy --build-arg MAX_JOBS=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions to run the image can be found in the &lt;a href=&#34;https://docs.mistral.ai/quickstart&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run the model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m main demo /path/to/mistral-7B-v0.1/&#xA;# To give your own prompts&#xA;python -m main interactive /path/to/mistral-7B-v0.1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change &lt;code&gt;temperature&lt;/code&gt; or &lt;code&gt;max_tokens&lt;/code&gt; using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m main interactive /path/to/mistral-7B-v0.1/ --max_tokens 256 --temperature 1.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want a self-contained implementation, look at &lt;code&gt;one_file_ref.py&lt;/code&gt;, or run it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m one_file_ref /path/to/mistral-7B-v0.1/&#xA;&#xA;This is a test of the emergency broadcast system. This is only a test.&#xA;&#xA;If this were a real emergency, you would be told what to do.&#xA;&#xA;This is a test&#xA;=====================&#xA;This is another test of the new blogging software. I’m not sure if I’m going to keep it or not. I’m not sure if I’m going to keep&#xA;=====================&#xA;This is a third test, mistral AI is very good at testing. 🙂&#xA;&#xA;This is a third test, mistral AI is very good at testing. 🙂&#xA;&#xA;This&#xA;=====================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run logits equivalence through chunking and sliding window, launch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m test_generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-tune the model&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;Data must be stored in jsonl format files.&lt;/p&gt; &#xA;&lt;p&gt;You can build two types of data files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Pretrain&lt;/em&gt;: plain text data stored in the &lt;code&gt;&#34;text&#34;&lt;/code&gt; key. E.g:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsonl&#34;&gt;{&#34;text&#34;: &#34;Text contained in document n°1&#34;}&#xA;{&#34;text&#34;: &#34;Text contained in document n°2&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Instruct&lt;/em&gt;: conversational data stored in the &lt;code&gt;&#34;interactions&#34;&lt;/code&gt; key in the form of a list. Each list item is a dictionary containing the &lt;code&gt;&#34;text&#34;&lt;/code&gt; and &lt;code&gt;&#34;is_user&#34;&lt;/code&gt; keys. &lt;code&gt;is_user&lt;/code&gt; is a boolean, if it is equal to True the loss will not be calculated on these tokens. E.g.:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsonl&#34;&gt;{&#34;interactions&#34;: [{&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n°1 contained in document n°1&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n°1 contained in document n°1&#34;}, {&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n°2 contained in document n°1&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n°2 contained in document n°1&#34;}]}&#xA;{&#34;interactions&#34;: [{&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n°1 contained in document n°2&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n°1 contained in document n°2&#34;}, {&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n°2 contained in document n°2&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n°2 contained in document n°2&#34;}, {&#34;is_user&#34;: true, &#34;text&#34;: &#34;User interaction n°3 contained in document n°2&#34;}, {&#34;is_user&#34;: false, &#34;text&#34;: &#34;Bot interaction n°3 contained in document n°2&#34;}]}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LoRA Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;To benefit from a memory-efficient and performant finetuning, we recommend to use &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;. The idea is to freeze weights and to only learn 1-2% additional weights in the form of low-rank matrix perturbations.&lt;/p&gt; &#xA;&lt;p&gt;With proper tuning (carefully calibrated learning rate, rank, LoRA dropout, learning the LoRA weights as well as the normalization layers), LoRA finetuning effectively recovers the performance of full finetuning. We support DDP on top of that, meaning that training speed can be increased on multiple GPUs.&lt;/p&gt; &#xA;&lt;p&gt;After the training, we merge the LoRA weights: hence, the saved checkpoint is exacly in the same format as one would get with full finetuning. To run a LoRA finetuning on a single GPU, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc-per-node 1 --master_port $RANDOM -m train reference/7B_lora.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>FoundationVision/GLEE</title>
    <updated>2024-03-27T01:33:56Z</updated>
    <id>tag:github.com,2024-03-27:/FoundationVision/GLEE</id>
    <link href="https://github.com/FoundationVision/GLEE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;【CVPR2024】GLEE: General Object Foundation Model for Images and Videos at Scale&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GLEE: General Object Foundation Model for Images and Videos at Scale&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Junfeng Wu*, Yi Jiang*, Qihao Liu, Zehuan Yuan, Xiang Bai&lt;sup&gt;†&lt;/sup&gt;,and Song Bai&lt;sup&gt;†&lt;/sup&gt;&lt;/h4&gt; &#xA; &lt;p&gt;* Equal Contribution, &lt;sup&gt;†&lt;/sup&gt;Correspondence&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://glee-vision.github.io/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2312.09158&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/Junfeng5/GLEE_demo&#34;&gt;HuggingFace Demo&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/PSVhfTPx0GQ&#34;&gt;Video Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/long-tail-video-object-segmentation-on-burst-1?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/long-tail-video-object-segmentation-on-burst-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/video-instance-segmentation-on-ovis-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-video-object-segmentation-on-refer?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-video-object-segmentation-on-refer&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-refer-1?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-expression-segmentation-on-refer-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-tao?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/multi-object-tracking-on-tao&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/open-world-instance-segmentation-on-uvo?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/open-world-instance-segmentation-on-uvo&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-expression-segmentation-on-refcoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-refcocog?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-expression-segmentation-on-refcocog&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/video-instance-segmentation-on-youtube-vis-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/object-detection-on-lvis-v1-0-val?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/object-detection-on-lvis-v1-0-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/instance-segmentation-on-lvis-v1-0-val?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/instance-segmentation-on-lvis-v1-0-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-expression-comprehension-on-refcoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco-3?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-expression-segmentation-on-refcoco-3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/instance-segmentation-on-coco-minival?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/instance-segmentation-on-coco-minival&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-comprehension-on?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-expression-comprehension-on&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/instance-segmentation-on-coco?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/instance-segmentation-on-coco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1?p=general-object-foundation-model-for-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/general-object-foundation-model-for-images/referring-expression-comprehension-on-refcoco-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/images/glee_func.gif&#34; alt=&#34;data_demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlight:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GLEE is accepted by &lt;strong&gt;CVPR2024&lt;/strong&gt; !&lt;/li&gt; &#xA; &lt;li&gt;GLEE is a general object foundation model jointly trained on over &lt;strong&gt;ten million images&lt;/strong&gt; from various benchmarks with diverse levels of supervision.&lt;/li&gt; &#xA; &lt;li&gt;GLEE is capable of addressing &lt;strong&gt;a wide range of object-centric tasks&lt;/strong&gt; simultaneously while maintaining &lt;strong&gt;SOTA&lt;/strong&gt; performance.&lt;/li&gt; &#xA; &lt;li&gt;GLEE demonstrates remarkable versatility and robust &lt;strong&gt;zero-shot transferability&lt;/strong&gt; across a spectrum of object-level image and video tasks, and able to &lt;strong&gt;serve as a foundational component&lt;/strong&gt; for enhancing other architectures or models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We will release the following contents for &lt;strong&gt;GLEE&lt;/strong&gt;&lt;span&gt;❗&lt;/span&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Demo Code&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Model Zoo&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Comprehensive User Guide&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Training Code and Scripts&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Detailed Evaluation Code and Scripts&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Tutorial for Zero-shot Testing or Fine-tuning GLEE on New Datasets&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Installation: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Data preparation: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/DATA.md&#34;&gt;DATA.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Training: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/TRAIN.md&#34;&gt;TRAIN.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Testing: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/TEST.md&#34;&gt;TEST.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Model zoo: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/MODEL_ZOO.md&#34;&gt;MODEL_ZOO.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Run the demo APP&lt;/h2&gt; &#xA;&lt;p&gt;Try our online demo app on [&lt;a href=&#34;https://huggingface.co/spaces/Junfeng5/GLEE_demo&#34;&gt;HuggingFace Demo&lt;/a&gt;] or use it locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/FoundationVision/GLEE&#xA;# support CPU and GPU running&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;GLEE has been trained on over ten million images from 16 datasets, fully harnessing both existing annotated data and cost-effective automatically labeled data to construct a diverse training set. This extensive training regime endows GLEE with formidable generalization capabilities.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/images/data_demo.png&#34; alt=&#34;data_demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;GLEE consists of an image encoder, a text encoder, a visual prompter, and an object decoder, as illustrated in Figure. The text encoder processes arbitrary descriptions related to the task, including &lt;strong&gt;1) object category list 2）object names in any form 3）captions about objects 4）referring expressions&lt;/strong&gt;. The visual prompter encodes user inputs such as &lt;strong&gt;1) points 2) bounding boxes 3) scribbles&lt;/strong&gt; during interactive segmentation into corresponding visual representations of target objects. Then they are integrated into a detector for extracting objects from images according to textual and visual input.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/images/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Based on the above designs, GLEE can be used to seamlessly unify a wide range of object perception tasks in images and videos, including object detection, instance segmentation, grounding, multi-target tracking (MOT), video instance segmentation (VIS), video object segmentation (VOS), interactive segmentation and tracking, and supports &lt;strong&gt;open-world/large-vocabulary image and video detection and segmentation&lt;/strong&gt; tasks.&lt;/p&gt; &#xA;&lt;h1&gt;Results&lt;/h1&gt; &#xA;&lt;h2&gt;Image-level tasks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/images/imagetask.png&#34; alt=&#34;imagetask&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/images/odinw13zero.png&#34; alt=&#34;odinw&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Video-level tasks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/images/videotask.png&#34; alt=&#34;videotask&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FoundationVision/GLEE/main/assets/images/visvosrvos.png&#34; alt=&#34;visvosrvos&#34;&gt;`&lt;/p&gt; &#xA;&lt;h1&gt;Citing GLEE&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wu2023GLEE,&#xA;  author= {Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai},&#xA;  title = {General Object Foundation Model for Images and Videos at Scale},&#xA;  year={2023},&#xA;  eprint={2312.09158},&#xA;  archivePrefix={arXiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/MasterBin-IIAU/UNINEXT&#34;&gt;UNINEXT&lt;/a&gt; for the implementation of multi-dataset training and data processing.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/wjf5203/VNext&#34;&gt;VNext&lt;/a&gt; for providing experience of Video Instance Segmentation (VIS).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM&lt;/a&gt; for providing the implementation of the visual prompter.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks &lt;a href=&#34;https://github.com/IDEA-Research/MaskDINO&#34;&gt;MaskDINO&lt;/a&gt; for providing a powerful detector and segmenter.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>