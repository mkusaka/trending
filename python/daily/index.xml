<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-18T01:37:49Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hustvl/4DGaussians</title>
    <updated>2023-10-18T01:37:49Z</updated>
    <id>tag:github.com,2023-10-18:/hustvl/4DGaussians</id>
    <link href="https://github.com/hustvl/4DGaussians" rel="alternate"></link>
    <summary type="html">&lt;p&gt;4D Gaussian Splatting for Real-Time Dynamic Scene Rendering&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;4D Gaussian Splatting for Real-Time Dynamic Scene Rendering&lt;/h1&gt; &#xA;&lt;h2&gt;arXiv Preprint&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://guanjunwu.github.io/4dgs/index.html&#34;&gt;Project Page&lt;/a&gt;| &lt;a href=&#34;https://arxiv.org/abs/2310.08528&#34;&gt;arXiv Paper&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://guanjunwu.github.io/&#34;&gt;Guanjun Wu&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;, &lt;a href=&#34;https://github.com/taoranyi&#34;&gt;Taoran Yi&lt;/a&gt;&lt;sup&gt;2*&lt;/sup&gt;, &lt;a href=&#34;https://jaminfong.cn/&#34;&gt;Jiemin Fang&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;http://lingxixie.com/&#34;&gt;Lingxi Xie&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;br&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=Ud6aBAcAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Xiaopeng Zhang&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www.eric-weiwei.com/&#34;&gt;Wei Wei&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,&lt;a href=&#34;http://eic.hust.edu.cn/professor/liuwenyu/&#34;&gt;Wenyu Liu&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.qitian1987.com/&#34;&gt;Qi Tian&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt; , &lt;a href=&#34;https://xwcv.github.io&#34;&gt;Xinggang Wang&lt;/a&gt;&lt;sup&gt;2‚úâ&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;School of CS, HUST ‚ÄÉ &lt;sup&gt;2&lt;/sup&gt;School of EIC, HUST ‚ÄÉ &lt;sup&gt;3&lt;/sup&gt;Huawei Inc. ‚ÄÉ&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hustvl/4DGaussians/master/assets/teaserfig.png&#34; alt=&#34;block&#34;&gt;&lt;br&gt; Our method converges very quickly and achieves real-time rendering speed.&lt;/p&gt; &#xA;&lt;p&gt;Colab demo:&lt;a href=&#34;https://colab.research.google.com/github/camenduru/4DGaussians-colab/blob/main/4DGaussians_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; (Thanks &lt;a href=&#34;https://github.com/camenduru/4DGaussians-colab&#34;&gt;camenduru&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt; &#xA; &lt;sourc src=&#34;assets/teaservideo.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA; &lt;/sourc&gt;&#xA;&lt;/video&gt; &#xA;&lt;video width=&#34;320&#34; height=&#34;240&#34; controls&gt; &#xA; &lt;source src=&#34;assets/cut_roasted_beef_time.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA;&lt;/video&gt; &#xA;&lt;h2&gt;Environmental Setups&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3D-GS&lt;/a&gt; to install the relative packages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hustvl/4DGaussians&#xA;cd 4DGaussians&#xA;git submodule update --init --recursive&#xA;conda create -n Gaussians4D python=3.7 &#xA;conda activate Gaussians4D&#xA;&#xA;pip install -r requirements.txt&#xA;pip install -e submodules/depth-diff-gaussian-rasterization&#xA;pip install -e submodules/simple-knn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In our environment, we use pytorch=1.13.1+cu116.&lt;/p&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;For synthetic scenes:&lt;/strong&gt;&lt;br&gt; The dataset provided in &lt;a href=&#34;https://github.com/albertpumarola/D-NeRF&#34;&gt;D-NeRF&lt;/a&gt; is used. You can download the dataset from &lt;a href=&#34;https://www.dropbox.com/s/0bf6fl0ye2vz3vr/data.zip?dl=0&#34;&gt;dropbox&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For real dynamic scenes:&lt;/strong&gt;&lt;br&gt; The dataset provided in &lt;a href=&#34;https://github.com/google/hypernerf&#34;&gt;HyperNeRF&lt;/a&gt; is used. You can download scenes from &lt;a href=&#34;https://github.com/google/hypernerf/releases/tag/v0.1&#34;&gt;Hypernerf Dataset&lt;/a&gt; and organize them as &lt;a href=&#34;https://github.com/google/nerfies#datasets&#34;&gt;Nerfies&lt;/a&gt;. Meanwhile, &lt;a href=&#34;https://github.com/facebookresearch/Neural_3D_Video&#34;&gt;Plenoptic Dataset&lt;/a&gt; could be downloaded from their official websites. To save the memory, you should extract the frames of each video and then organize your dataset as follows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ data&#xA;‚îÇ   | dnerf &#xA;‚îÇ     ‚îú‚îÄ‚îÄ mutant&#xA;‚îÇ     ‚îú‚îÄ‚îÄ standup &#xA;‚îÇ     ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ   | hypernerf&#xA;‚îÇ     ‚îú‚îÄ‚îÄ interp&#xA;‚îÇ     ‚îú‚îÄ‚îÄ misc&#xA;‚îÇ     ‚îú‚îÄ‚îÄ virg&#xA;‚îÇ   | dynerf&#xA;‚îÇ     ‚îú‚îÄ‚îÄ cook_spinach&#xA;‚îÇ       ‚îú‚îÄ‚îÄ cam00&#xA;‚îÇ           ‚îú‚îÄ‚îÄ images&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0000.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0001.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0002.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ       ‚îú‚îÄ‚îÄ cam01&#xA;‚îÇ           ‚îú‚îÄ‚îÄ images&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0000.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ 0001.png&#xA;‚îÇ               ‚îú‚îÄ‚îÄ ...&#xA;‚îÇ     ‚îú‚îÄ‚îÄ cut_roasted_beef&#xA;|     ‚îú‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;For training synthetic scenes such as &lt;code&gt;lego&lt;/code&gt;, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train.py -s data/dnerf/bouncingballs --port 6017 --expname &#34;dnerf/bouncingballs&#34; --configs arguments/dnerf/bouncingballs.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can customize your training config through the config files.&lt;/p&gt; &#xA;&lt;h2&gt;Rendering&lt;/h2&gt; &#xA;&lt;p&gt;Run the following script to render the images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python render.py --model_path &#34;output/dnerf/bouncingballs/&#34;  --skip_train --configs arguments/dnerf/bouncingballs.py  &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;You can just run the following script to evaluate the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python metrics.py --model_path &#34;output/dnerf/bouncingballs/&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Scripts&lt;/h2&gt; &#xA;&lt;p&gt;There are some helpful scripts in &lt;code&gt;scripts/&lt;/code&gt;, please feel free to use them.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Some source code of ours is borrowed from&amp;nbsp;&lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3DGS&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://github.com/Giodiro/kplanes_nerfstudio&#34;&gt;k-planes&lt;/a&gt;,&lt;a href=&#34;https://github.com/Caoang327/HexPlane&#34;&gt;HexPlane&lt;/a&gt;,&amp;nbsp;&lt;a href=&#34;https://github.com/hustvl/TiNeuVox&#34;&gt;TiNeuVox&lt;/a&gt;. We sincerely appreciate the excellent works of these authors.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our sincere gratitude to @zhouzhenghong-gt for his revisions to our code and discussions on the content of our paper.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository/work helpful in your research, welcome to cite the paper and give a ‚≠ê.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wu20234dgaussians,&#xA;  title={4D Gaussian Splatting for Real-Time Dynamic Scene Rendering},&#xA;  author={Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei Wei and Liu, Wenyu and Tian, Qi and Wang Xinggang},&#xA;  journal={arXiv preprint arXiv:2310.08528},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>AI-Citizen/SolidGPT</title>
    <updated>2023-10-18T01:37:49Z</updated>
    <id>tag:github.com,2023-10-18:/AI-Citizen/SolidGPT</id>
    <link href="https://github.com/AI-Citizen/SolidGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with your code repository to get Generate Product Requirement Document and Code Plan base on private project. üß± üß±&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/AI-Citizen/SolidGPT/assets/39673228/347a6be2-93d6-42e9-99e2-f8b7b1ea96de&#34; alt=&#34;IMG_4502&#34;&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;üß± SolidGPT-Technology Business Boosting Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI-Citizen/SolidGPT/main/docs/Introduction_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ÊñáÊ°£-‰∏≠ÊñáÁâà-blue.svg&#34; alt=&#34;CN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/SolidGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/SolidGPT?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üöÄ&amp;nbsp;What‚Äôs this&lt;/h1&gt; &#xA;&lt;p&gt;Help you generate Product Requirement Document and Code Solution base on your private code repository.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;V0.2.5:&lt;/strong&gt; Enable users to host the whole features of SolidGPT service locally while ensuring data privacy.&lt;/p&gt; &#xA;&lt;h3&gt;üì∫ Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AI-Citizen/SolidGPT/assets/39673228/8ef57ba1-093e-4cc5-a07d-45b5c2dea850&#34; alt=&#34;copy_FD8819CE-0A56-4E9C-A018-FA90700E7605&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üî•üî• &lt;a href=&#34;https://calm-flower-017281610.3.azurestaticapps.net/&#34;&gt;Click to try official host SolidGPT&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you like our work, please give us a üåü star. Your support serves as a great encouragement for us. Thank you! üòä&lt;/p&gt; &#xA;&lt;h1&gt;üèÅ&amp;nbsp;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;üß± &lt;strong&gt;Prerequisite&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python3.8 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/openai-api&#34;&gt;OpenAI api key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.npmjs.com/downloading-and-installing-node-js-and-npm&#34;&gt;Install npm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîß &lt;strong&gt;Setup&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/AI-Citizen/SolidGPT.git&#xA;cd SolidGPT &#xA;pip3 install -r requirements.txt #installing the env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set project root folder as python path &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Linux/Mac&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export PYTHONPATH=$PYTHONPATH:$(git rev-parse --show-toplevel)/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Windows&lt;/p&gt; &lt;p&gt;Replace path\to\directory with the path of the project root directory&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;set PYTHONPATH=path\to\directory&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Start Server&lt;/h3&gt; cd to the project root folder(SolidGPT) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Linux/Mac/WSL2&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh StartServer.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Windows&lt;/p&gt; &lt;p&gt;Install the &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install&#34;&gt;WSL2&lt;/a&gt; and start the server from WSL2&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wsl --install&#xA;wsl2&#xA;sh StartServer.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or install directly on Windows [Not Recommended]&lt;/p&gt; &lt;p&gt;Note: redis server needs to be installed before running below commands: &lt;a href=&#34;https://github.com/microsoftarchive/redis/releases&#34;&gt;https://github.com/microsoftarchive/redis/releases&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uvicorn solidgpt.src.api.api:app --reload&#xA;&#xA;celery -A solidgpt.src.api.celery_tasks worker --loglevel=info -P eventlet&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Docker&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker buildx build -t solidgptlocalhost .&#xA;docker run -p 8000:8000 solidgptlocalhost&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Start UI portal&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;You&#39;ll need to install npm, and we recommend using version 9.8.1 or higher.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# From the project root folder&#xA;cd solidportal/panel  &#xA;npm i &amp;amp;&amp;amp; npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üè† Introduction&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;SolidGPT first learns from your repository in the &lt;code&gt;Onboard Project&lt;/code&gt; phase.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After this, choose Generate PRD or Get Tech Solution for customized solutions based on the onboarded project.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìñ Onborading your project&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Choose &lt;code&gt;Onboard Project&lt;/code&gt; from the top left dropdown.&lt;/li&gt; &#xA; &lt;li&gt;Enter your OpenAI API key.&lt;/li&gt; &#xA; &lt;li&gt;Upload your project folder.ÔºàAll files will be save in your localstorage &lt;code&gt;SolidGPT/localstorage/...&lt;/code&gt;Ôºâ&lt;/li&gt; &#xA; &lt;li&gt;‚ùóÔ∏èNote: After completing the Onboard Project, an Onboard ID will be generated. If you remain in the same browser session, it will be automatically applied to subsequent actions. Alternatively, you can save it and manually input it in the future to bypass onboarding.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üßÆ Get Technical Solution&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Choose &lt;code&gt;Get Tech Solution&lt;/code&gt; from the top left dropdown.&lt;/li&gt; &#xA; &lt;li&gt;Enter your OpenAI API key.&lt;/li&gt; &#xA; &lt;li&gt;Input your problem/Requirement.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: We currently support Python, JavaScript, and TypeScript projects. Support for more languages is on the way.&lt;/p&gt; &#xA;&lt;h2&gt;üìÅ Generate Product Requirement Document&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Choose &lt;code&gt;Generate RPD&lt;/code&gt; from the top left dropdown.&lt;/li&gt; &#xA; &lt;li&gt;input your requirement (suggest short and clear)&lt;/li&gt; &#xA; &lt;li&gt;input additional info or your project, SolidGPT will use both summary from repository and additional info you provided (optinoal)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üñáÔ∏è Document&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/READMEv1.md&#34;&gt;Explore SolidGPT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/solidportal.md&#34;&gt;Solid Portal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/infrastructure.md&#34;&gt;Solid GPT Infrastructure&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/customagentskill.md&#34;&gt;Deeply Customize Agent Skill&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/embeddingprivatedata.md&#34;&gt;Embedding with private data&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/gptfinetuning.md&#34;&gt;Fine-tuning with GPT3.5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/llama2finetuning.md&#34;&gt;Fine-tuning with Llama2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or feedback about our project, please don&#39;t hesitate to reach out to us. We greatly appreciate your suggestions!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Email: &lt;a href=&#34;mailto:aict@ai-citi.com&#34;&gt;aict@ai-citi.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: For more technical inquiries, you can also create a new issue in our &lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/issues&#34;&gt;GitHub repository&lt;/a&gt;. We will respond to all questions within 2-3 business days.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/stable-audio-tools</title>
    <updated>2023-10-18T01:37:49Z</updated>
    <id>tag:github.com,2023-10-18:/Stability-AI/stable-audio-tools</id>
    <link href="https://github.com/Stability-AI/stable-audio-tools" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative models for conditional audio generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;stable-audio-tools&lt;/h1&gt; &#xA;&lt;p&gt;Training and inference code for audio generation models&lt;/p&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;p&gt;The library can be installed from PyPI with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install stable-audio-tools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the training scripts or inference code, you&#39;ll want to clone this repository, navigate to the root, and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;p&gt;Requires PyTorch 2.0 or later for Flash Attention support&lt;/p&gt; &#xA;&lt;p&gt;Development for the repo is done in Python 3.8.10&lt;/p&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before starting your training run, you&#39;ll need a model config file, as well as a dataset config file. For more information about those, refer to the Configurations section below&lt;/p&gt; &#xA;&lt;p&gt;The training code also requires a Weights &amp;amp; Biases account to log the training outputs and demos. Create an account and log in with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Start training&lt;/h2&gt; &#xA;&lt;p&gt;To start a training run, run the &lt;code&gt;train.py&lt;/code&gt; script in the repo root with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python3 ./train.py --dataset-config /path/to/dataset/config --model-config /path/to/model/config --name harmonai_train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;--name&lt;/code&gt; parameter will set the project name for your Weights and Biases run.&lt;/p&gt; &#xA;&lt;h2&gt;Training wrappers and model unwrapping&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;stable-audio-tools&lt;/code&gt; uses PyTorch Lightning to facilitate multi-GPU and multi-node training.&lt;/p&gt; &#xA;&lt;p&gt;When a model is being trained, it is wrapped in a &#34;training wrapper&#34;, which is a &lt;code&gt;pl.LightningModule&lt;/code&gt; that contains all of the relevant objects needed only for training. That includes things like discriminators for autoencoders, EMA copies of models, and all of the optimizer states.&lt;/p&gt; &#xA;&lt;p&gt;The checkpoint files created during training include this training wrapper, which greatly increases the size of the checkpoint file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;unwrap_model.py&lt;/code&gt; in the repo root will take in a wrapped model checkpoint and save a new checkpoint file including only the model itself.&lt;/p&gt; &#xA;&lt;p&gt;That can be run with from the repo root with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python3 ./unwrap_model.py --model-config /path/to/model/config --ckpt-path /path/to/wrapped/ckpt --name model_unwrap&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unwrapped model checkpoints are required for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference scripts&lt;/li&gt; &#xA; &lt;li&gt;Using a model as a pretransform for another model (e.g. using an autoencoder model for latent diffusion)&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuning a pre-trained model with a modified configuration (i.e. partial initialization)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;Fine-tuning a model involves continuning a training run from a pre-trained checkpoint.&lt;/p&gt; &#xA;&lt;p&gt;To continue a training run from a wrapped model checkpoint, you can pass in the checkpoint path to &lt;code&gt;train.py&lt;/code&gt; with the &lt;code&gt;--ckpt-path&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;p&gt;To start a fresh training run using a pre-trained unwrapped model, you can pass in the unwrapped checkpoint to &lt;code&gt;train.py&lt;/code&gt; with the &lt;code&gt;--pretrained-ckpt-path&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;h2&gt;Additional training flags&lt;/h2&gt; &#xA;&lt;p&gt;Additional optional flags for &lt;code&gt;train.py&lt;/code&gt; include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--config-file&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The path to the defaults.ini file in the repo root, required if running &lt;code&gt;train.py&lt;/code&gt; from a directory other than the repo root&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pretransform-ckpt-path&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Used in various model types such as latent diffusion models to load a pre-trained autoencoder. Requires an unwrapped model checkpoint.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--save-dir&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The directory in which to save the model checkpoints&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--checkpoint-every&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The number of steps between saved checkpoints.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Default&lt;/em&gt;: 10000&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch-size&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Number of samples per-GPU during training. Should be set as large as your GPU VRAM will allow.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Default&lt;/em&gt;: 8&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num-gpus&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Number of GPUs per-node to use for training&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Default&lt;/em&gt;: 1&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num-nodes&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Number of GPU nodes being used for training&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Default&lt;/em&gt;: 1&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--accum-batches&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enables and sets the number of batches for gradient batch accumulation. Useful for increasing effective batch size when training on smaller GPUs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--strategy&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi-GPU strategy for distributed training. Setting to &lt;code&gt;deepspeed&lt;/code&gt; will enable DeepSpeed ZeRO Stage 2.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Default&lt;/em&gt;: &lt;code&gt;ddp&lt;/code&gt; if &lt;code&gt;--num_gpus&lt;/code&gt; &amp;gt; 1, else None&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--precision&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;floating-point precision to use during training&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Default&lt;/em&gt;: 16&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num-workers&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Number of CPU workers used by the data loader&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--seed&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RNG seed for PyTorch, helps with deterministic training&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Configurations&lt;/h1&gt; &#xA;&lt;p&gt;Training and inference code for &lt;code&gt;stable-audio-tools&lt;/code&gt; is based around JSON configuration files that define model hyperparameters, training settings, and information about your training dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Model config&lt;/h2&gt; &#xA;&lt;p&gt;The model config file defines all of the information needed to load a model for training or inference. It also contains the training configuration needed to fine-tune a model or train from scratch.&lt;/p&gt; &#xA;&lt;p&gt;The following properties are defined in the top level of the model configuration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_type&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The type of model being defined, currently limited to one of &lt;code&gt;&#34;autoencoder&#34;, &#34;diffusion_uncond&#34;, &#34;diffusion_cond&#34;, &#34;diffusion_cond_inpaint&#34;, &#34;diffusion_autoencoder&#34;, &#34;musicgen&#34;&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample_size&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The length of the audio provided to the model during training, in samples. For diffusion models, this is also the raw audio sample length used for inference.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample_rate&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The sample rate of the audio provided to the model during training, and generated during inference, in Hz.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;audio_channels&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The number of channels of audio provided to the model during training, and generated during inference. Defaults to 2. Set to 1 for mono.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The specific configuration for the model being defined, varies based on &lt;code&gt;model_type&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;training&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The training configuration for the model, varies based on &lt;code&gt;model_type&lt;/code&gt;. Provides parameters for training as well as demos.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dataset config&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;stable-audio-tools&lt;/code&gt; currently supports two kinds of data sources: local directories of audio files, and WebDataset datasets stored in Amazon S3. More information can be found in &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stable-audio-tools/main/docs/datasets.md&#34;&gt;the dataset config documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Todo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add documentation for different model types&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add documentation for Gradio interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add troubleshooting section&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add contribution guidelines&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>