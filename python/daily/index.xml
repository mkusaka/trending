<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-28T01:34:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MDK8888/GPTFast</title>
    <updated>2024-02-28T01:34:35Z</updated>
    <id>tag:github.com,2024-02-28:/MDK8888/GPTFast</id>
    <link href="https://github.com/MDK8888/GPTFast" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Accelerate your Hugging Face Transformers 6-7x. Native to Hugging Face and PyTorch.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPTFast&lt;/h1&gt; &#xA;&lt;p&gt;Accelerate your Hugging Face Transformers 6-7x with GPTFast!&lt;/p&gt; &#xA;&lt;h1&gt;Background&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;GPTFast&lt;/a&gt; was originally a set of techniques developed by the PyTorch Team to accelerate the inference speed of Llama-2-7b. This pip package generalizes those techniques to all Hugging Face models.&lt;/p&gt; &#xA;&lt;h1&gt;Demo&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;GPTFast Inference Time&lt;/th&gt; &#xA;   &lt;th&gt;Eager Inference Time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/MDK8888/GPTFast/assets/79173446/4d7ed04e-ba3d-49c7-aeca-8f2b96ac45a8&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/MDK8888/GPTFast/assets/79173446/1a4f2236-d2f4-42c7-a689-553482871905&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure that your python version &amp;gt;= 3.10, and you are on a cuda enabled device.&lt;/li&gt; &#xA; &lt;li&gt;Make a virtual environment on your machine and activate it. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$python3 -m venv VENV_NAME&#xA;source VENV_NAME/bin/activate #./VENV_NAME/scripts/activate if you are on Windows&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Call the following: &lt;code&gt;pip install gptfast&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy the following code into a python file: &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import torch&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from GPTFast.Core import gpt_fast&#xA;from GPTFast.Helpers import timed&#xA;&#xA;torch._dynamo.reset()&#xA;os.environ[&#34;TOKENIZERS_PARALLELISM&#34;] = &#34;false&#34;&#xA;&#xA;device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;def argmax(self, probabilities):&#xA;    # Use argmax to get the token with the maximum probability&#xA;    max_prob_index = torch.argmax(probabilities, dim=-1)&#xA;    return max_prob_index.unsqueeze(0)&#xA;&#xA;model_name = &#34;gpt2-xl&#34;&#xA;draft_model_name = &#34;gpt2&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;initial_string = &#34;Write me a short story.&#34;&#xA;input_tokens = tokenizer.encode(initial_string, return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;N_ITERS=10&#xA;MAX_TOKENS=50&#xA;&#xA;gpt_fast_model = gpt_fast(model_name, draft_model_name=draft_model_name, sample_function=argmax)&#xA;gpt_fast_model.to(device)&#xA;&#xA;fast_compile_times = []&#xA;for i in range(N_ITERS):&#xA;    with torch.no_grad():&#xA;        res, compile_time = timed(lambda: gpt_fast_model.generate(cur_tokens=input_tokens, max_tokens=MAX_TOKENS, speculate_k=6))&#xA;    fast_compile_times.append(compile_time)&#xA;    print(f&#34;gpt fast eval time {i}: {compile_time}&#34;)&#xA;print(&#34;~&#34; * 10)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run it and watch the magic ü™Ñ!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;At its core, this library provides a simple interface to LLM Inference acceleration techniques. All of the following functions can be imported from &lt;code&gt;GPTFast.Core&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;gpt_fast(model_name:str, draft_model_name:str, sample_function:Callable) -&amp;gt; torch.nn.Module&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt;: This is the name of the Hugging face model that you want to optimize.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;draft_model_name&lt;/code&gt;: This is the name of the Hugging face draft model which is needed for &lt;a href=&#34;https://arxiv.org/abs/2211.17192&#34;&gt;speculative decoding&lt;/a&gt;. Note that the model and the draft model must both use the same tokenizer, and the draft model must be &lt;strong&gt;significantly&lt;/strong&gt; smaller to achieve inference acceleration.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;sample function(distribution, **kwargs)&lt;/code&gt;: This is a function which is used to sample from the distribution generated by the main model. This function has a mandatory parameter which is a tensor of dimension &lt;code&gt;(seq_len, vocab_size)&lt;/code&gt; and returns a tensor of shape &lt;code&gt;(1, 1)&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Returns&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;An accelerated model with one method: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;code&gt;generate(self, cur_tokens:torch.Tensor, max_tokens:int, speculate_k:int, **sampling_kwargs) -&amp;gt; torch.Tensor&lt;/code&gt; &#xA;        &lt;ul&gt; &#xA;         &lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt;: &#xA;          &lt;ul&gt; &#xA;           &lt;li&gt;&lt;code&gt;cur_tokens&lt;/code&gt;: A PyTorch Tensor of size (1, seq_len).&lt;/li&gt; &#xA;           &lt;li&gt;&lt;code&gt;max_tokens&lt;/code&gt;: An int representing how many tokens you want to generate.&lt;/li&gt; &#xA;           &lt;li&gt;&lt;code&gt;speculate_k&lt;/code&gt;: An int specifying how far you want the draft model to speculate in speculative decoding.&lt;/li&gt; &#xA;           &lt;li&gt;&lt;code&gt;**sampling_kwargs&lt;/code&gt;: Additional parameters that are necessary for sampling from the distribution. Should match the &lt;code&gt;**kwargs&lt;/code&gt; of the &lt;code&gt;sample&lt;/code&gt; function above.&lt;/li&gt; &#xA;          &lt;/ul&gt; &lt;/li&gt; &#xA;         &lt;li&gt;&lt;strong&gt;Returns&lt;/strong&gt;: &#xA;          &lt;ul&gt; &#xA;           &lt;li&gt;The generated tokens to your prompt, a tensor with dimensions &lt;code&gt;(1, max_tokens)&lt;/code&gt;.&lt;/li&gt; &#xA;          &lt;/ul&gt; &lt;/li&gt; &#xA;        &lt;/ul&gt; &lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;load_int8(model_name:str) -&amp;gt; torch.nn.Module&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt;: This is a string specifying the model that you are using.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Returns&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;An &lt;code&gt;int8&lt;/code&gt; quantized version of your model.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;add_kv_cache(model_name:str) -&amp;gt; KVCacheModel&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt;: This is a string specifying the model that you are using.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Returns&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;An instance of the &lt;code&gt;KVCacheModel&lt;/code&gt; class which is essentially just your model but with a key-value cache attached for accelerated inference.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;add_speculative_decode_kv_cache(model:KVCacheModel, draft_model:KVCacheModel, sample_function:Callable) -&amp;gt; torch.nn.Module&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: This is the KVCached version of your model.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;draft_model&lt;/code&gt;: This is the KVCached version of your draft model.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;sample function(distribution, **kwargs)&lt;/code&gt;: same as the documentation for &lt;code&gt;gpt_fast&lt;/code&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Returns&lt;/strong&gt;: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;An accelerated model with the &lt;code&gt;generate&lt;/code&gt; method described above under the &lt;code&gt;gpt_fast&lt;/code&gt; section.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>jasonyzhang/RayDiffusion</title>
    <updated>2024-02-28T01:34:35Z</updated>
    <id>tag:github.com,2024-02-28:/jasonyzhang/RayDiffusion</id>
    <link href="https://github.com/jasonyzhang/RayDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for &#34;Cameras as Rays&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cameras as Rays&lt;/h1&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2402.14817&#34;&gt;&lt;code&gt;arXiv&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://jasonyzhang.com/RayDiffusion/&#34;&gt;&lt;code&gt;Project Page&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/jasonyzhang/RayDiffusion/main/#citing-cameras-as-rays&#34;&gt;&lt;code&gt;Bibtex&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;This repository contains code for &#34;Cameras as Rays: Pose Estimation via Ray Diffusion&#34; (ICLR 2024).&lt;/p&gt; &#xA;&lt;h2&gt;Setting up Environment&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using a conda environment to manage dependencies. Install a version of Pytorch compatible with your CUDA version from the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;Pytorch website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n raydiffusion python=3.10&#xA;conda activate raydiffusion&#xA;conda install pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=11.8 -c pytorch -c nvidia&#xA;conda install xformers -c xformers&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, follow the directions to install Pytorch3D &lt;a href=&#34;https://github.com/facebookresearch/pytorch3d/raw/main/INSTALL.md&#34;&gt;here&lt;/a&gt;. We recommend installing Pytorch3D using the pre-built wheel with the corresponding Python/Pytorch/CUDA version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt211/download.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are having trouble installing using the pre-built wheel, you can also try building from source, but this will take a lot longer.&lt;/p&gt; &#xA;&lt;h2&gt;Run Demo&lt;/h2&gt; &#xA;&lt;p&gt;Download the model weights from &lt;a href=&#34;https://drive.google.com/file/d/1anIKsm66zmDiFuo8Nmm1HupcitM6NY7e/view?usp=drive_link&#34;&gt;Google Drive&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run ray diffusion with known bounding boxes (provided as a json):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py  --model_dir models/co3d_diffusion --image_dir examples/robot/images \&#xA;    --bbox_path examples/robot/bboxes.json --output_path robot.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run ray diffusion with bounding boxes extracted automatically from masks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py  --model_dir models/co3d_diffusion --image_dir examples/robot/images \&#xA;    --mask_dir examples/robot/masks --output_path robot.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run ray regression:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py  --model_dir models/co3d_regression --image_dir examples/robot/images \&#xA;    --bbox_path examples/robot/bboxes.json --output_path robot.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code release status&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Demo Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluation Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training Code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citing Cameras as Rays&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code helpful, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{zhang2024raydiffusion,&#xA;    title={Cameras as Rays: Pose Estimation via Ray Diffusion},&#xA;    author={Zhang, Jason Y and Lin, Amy and Kumar, Moneish and Yang, Tzu-Hsuan and Ramanan, Deva and Tulsiani, Shubham},&#xA;    booktitle={International Conference on Learning Representations (ICLR)},&#xA;    year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>openvinotoolkit/anomalib</title>
    <updated>2024-02-28T01:34:35Z</updated>
    <id>tag:github.com,2024-02-28:/openvinotoolkit/anomalib</id>
    <link href="https://github.com/openvinotoolkit/anomalib" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/main/docs/source/_static/images/logos/anomalib-wide-blue.png&#34; width=&#34;600px&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;A library for benchmarking, developing and deploying deep learning anomaly detection algorithms&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/main/#key-features&#34;&gt;Key Features&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://anomalib.readthedocs.io/en/latest/&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/main/notebooks&#34;&gt;Notebooks&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/raw/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7%2B-green&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pytorch-1.8.1%2B-orange&#34; alt=&#34;pytorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/openvino-2022.3.0-purple&#34; alt=&#34;openvino&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/pre_merge.yml&#34;&gt;&lt;img src=&#34;https://github.com/openvinotoolkit/anomalib/actions/workflows/pre_merge.yml/badge.svg?sanitize=true&#34; alt=&#34;Pre-Merge Checks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anomalib.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/anomalib/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/openvinotoolkit/anomalib&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/openvinotoolkit/anomalib/branch/main/graph/badge.svg?token=Z6A07N1BZK&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/anomalib&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/anomalib?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=green&amp;amp;left_text=PyPI%20Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üëã Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib is a deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets. Anomalib provides several ready-to-use implementations of anomaly detection algorithms described in the recent literature, as well as a set of tools that facilitate the development and implementation of custom models. The library has a strong focus on visual anomaly detection, where the goal of the algorithm is to detect and/or localize anomalies within images or videos in a dataset. Anomalib is constantly updated with new algorithms and training/inference extensions, so keep checking!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/main/docs/source/_static/images/readme.png&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Key features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple and modular API and CLI for training, inference, benchmarking, and hyperparameter optimization.&lt;/li&gt; &#xA; &lt;li&gt;The largest public collection of ready-to-use deep learning anomaly detection algorithms and benchmark datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lightning.ai/&#34;&gt;&lt;strong&gt;Lightning&lt;/strong&gt;&lt;/a&gt; based model implementations to reduce boilerplate code and limit the implementation efforts to the bare essentials.&lt;/li&gt; &#xA; &lt;li&gt;All models can be exported to &lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html&#34;&gt;&lt;strong&gt;OpenVINO&lt;/strong&gt;&lt;/a&gt; Intermediate Representation (IR) for accelerated inference on intel hardware.&lt;/li&gt; &#xA; &lt;li&gt;A set of &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/main/tools&#34;&gt;inference tools&lt;/a&gt; for quick and easy deployment of the standard or custom anomaly detection models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üì¶ Installation&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib provides two ways to install the library. The first is through PyPI, and the second is through a local installation. PyPI installation is recommended if you want to use the library without making any changes to the source code. If you want to make changes to the library, then a local installation is recommended.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Install from PyPI&lt;/summary&gt; Installing the library with pip is the easiest way to get started with `anomalib`. &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install anomalib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Install from source&lt;/summary&gt; To install from source, you need to clone the repository and install the library using pip via editable mode. &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Use of virtual environment is highy recommended&#xA;# Using conda&#xA;yes | conda create -n anomalib_env python=3.10&#xA;conda activate anomalib_env&#xA;&#xA;# Or using your favorite virtual environment&#xA;# ...&#xA;&#xA;# Clone the repository and install in editable mode&#xA;git clone https://github.com/openvinotoolkit/anomalib.git&#xA;cd anomalib&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;üß† Training&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib supports both API and CLI-based training. The API is more flexible and allows for more customization, while the CLI training utilizes command line interfaces, and might be easier for those who would like to use anomalib off-the-shelf.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Training via API&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the required modules&#xA;from anomalib.data import MVTec&#xA;from anomalib.models import Patchcore&#xA;from anomalib.engine import Engine&#xA;&#xA;# Initialize the datamodule, model and engine&#xA;datamodule = MVTec()&#xA;model = Patchcore()&#xA;engine = Engine()&#xA;&#xA;# Train the model&#xA;engine.fit(datamodule=datamodule, model=model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Training via CLI&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Get help about the training arguments, run:&#xA;anomalib train -h&#xA;&#xA;# Train by using the default values.&#xA;anomalib train --model Patchcore --data anomalib.data.MVTec&#xA;&#xA;# Train by overriding arguments.&#xA;anomalib train --model Patchcore --data anomalib.data.MVTec --data.category transistor&#xA;&#xA;#&amp;nbsp;Train by using a config file.&#xA;anomalib train --config &amp;lt;path/to/config&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;ü§ñ Inference&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib includes multiple inferencing scripts, including Torch, Lightning, Gradio, and OpenVINO inferencers to perform inference using the trained/exported model. Here we show an inference example using the Lightning inferencer. For other inferencers, please refer to the &lt;a href=&#34;https://anomalib.readthedocs.io&#34;&gt;Inference Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Inference via API&lt;/summary&gt; &#xA; &lt;p&gt;The following example demonstrates how to perform Lightning inference by loading a model from a checkpoint file.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Assuming the datamodule, model and engine is initialized from the previous step,&#xA;# a prediction via a checkpoint file can be performed as follows:&#xA;predictions = engine.predict(&#xA;    datamodule=datamodule,&#xA;    model=model,&#xA;    ckpt_path=&#34;path/to/checkpoint.ckpt&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Inference via CLI&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# To get help about the arguments, run:&#xA;anomalib predict -h&#xA;&#xA;# Predict by using the default values.&#xA;anomalib predict --model anomalib.models.Patchcore \&#xA;                 --data anomalib.data.MVTec \&#xA;                 --ckpt_path &amp;lt;path/to/model.ckpt&amp;gt;&#xA;&#xA;# Predict by overriding arguments.&#xA;anomalib predict --model anomalib.models.Patchcore \&#xA;                 --data anomalib.data.MVTec \&#xA;                 --ckpt_path &amp;lt;path/to/model.ckpt&amp;gt;&#xA;                 --return_predictions&#xA;&#xA;# Predict by using a config file.&#xA;anomalib predict --config &amp;lt;path/to/config&amp;gt; --return_predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;‚öôÔ∏è Hyperparameter Optimization&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib supports hyperparameter optimization (HPO) using &lt;a href=&#34;https://wandb.ai/&#34;&gt;wandb&lt;/a&gt; and &lt;a href=&#34;https://www.comet.com/&#34;&gt;comet.ml&lt;/a&gt;. For more details refer the &lt;a href=&#34;https://openvinotoolkit.github.io/anomalib/tutorials/hyperparameter_optimization.html&#34;&gt;HPO Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;HPO via API&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# To be enabled in v1.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;HPO via CLI&lt;/summary&gt; &#xA; &lt;p&gt;The following example demonstrates how to perform HPO for the Patchcore model.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;anomalib hpo --backend WANDB  --sweep_config tools/hpo/configs/wandb.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;üß™ Experiment Management&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib is integrated with various libraries for experiment tracking such as Comet, tensorboard, and wandb through &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html&#34;&gt;pytorch lighting loggers&lt;/a&gt;. For more information, refer to the &lt;a href=&#34;https://openvinotoolkit.github.io/anomalib/tutorials/logging.html&#34;&gt;Logging Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Experiment Management via API&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# To be enabled in v1.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Experiment Management via CLI&lt;/summary&gt; &#xA; &lt;p&gt;Below is an example of how to enable logging for hyper-parameters, metrics, model graphs, and predictions on images in the test data-set.&lt;/p&gt; &#xA; &lt;p&gt;You first need to modify the &lt;code&gt;config.yaml&lt;/code&gt; file to enable logging. The following example shows how to enable logging:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Place the experiment management config here.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Place the Experiment Management CLI command here.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;üìä Benchmarking&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib provides a benchmarking tool to evaluate the performance of the anomaly detection models on a given dataset. The benchmarking tool can be used to evaluate the performance of the models on a given dataset, or to compare the performance of multiple models on a given dataset.&lt;/p&gt; &#xA;&lt;p&gt;Each model in anomalib is benchmarked on a set of datasets, and the results are available in &lt;code&gt;src/anomalib/models/&amp;lt;model_name&amp;gt;README.md&lt;/code&gt;. For example, the MVTec AD results for the Patchcore model are available in the corresponding &lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/tree/main/src/anomalib/models/patchcore#mvtec-ad-dataset&#34;&gt;README.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Benchmarking via API&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# To be enabled in v1.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Benchmarking via CLI&lt;/summary&gt; &#xA; &lt;p&gt;To run the benchmarking tool, run the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;anomalib benchmark --config tools/benchmarking/benchmark_params.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;‚úçÔ∏è Reference&lt;/h1&gt; &#xA;&lt;p&gt;If you use this library and love it, use this to cite it ü§ó&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{akcay2022anomalib,&#xA;  title={Anomalib: A deep learning library for anomaly detection},&#xA;  author={Akcay, Samet and Ameln, Dick and Vaidya, Ashwin and Lakshmanan, Barath and Ahuja, Nilesh and Genc, Utku},&#xA;  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},&#xA;  pages={1706--1710},&#xA;  year={2022},&#xA;  organization={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üë• Contributing&lt;/h1&gt; &#xA;&lt;p&gt;For those who would like to contribute to the library, see &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/anomalib/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;p&gt;Thank you to all of the people who have already made a contribution - we appreciate your support!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/openvinotoolkit/anomalib/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=openvinotoolkit/anomalib&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>