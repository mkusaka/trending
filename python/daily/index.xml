<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-19T01:53:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>UX-Decoder/Semantic-SAM</title>
    <updated>2023-07-19T01:53:33Z</updated>
    <id>tag:github.com,2023-07-19:/UX-Decoder/Semantic-SAM</id>
    <link href="https://github.com/UX-Decoder/Semantic-SAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of the paper &#34;Semantic-SAM: Segment and Recognize Anything at Any Granularity&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Semantic-SAM: Segment and Recognize Anything at Any Granularity&lt;/h1&gt; &#xA;&lt;p&gt;In this work, we introduce &lt;strong&gt;Semantic-SAM&lt;/strong&gt;, a universal image segmentation model to enable segment and recognize anything at any desired granularity. We have trained on the whole &lt;strong&gt;SA-1B&lt;/strong&gt; dataset and our model can &lt;strong&gt;reproduce SAM and beyond it&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üçá&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/pdf/2307.04767.pdf&#34;&gt;Read our arXiv Paper&lt;/a&gt;] &amp;nbsp; &lt;span&gt;üçé&lt;/span&gt; [&lt;a href=&#34;http://semantic-sam.xyzou.net:6081/&#34;&gt;Try Gradio Demo1&lt;/a&gt;] &amp;nbsp; &lt;span&gt;üçé&lt;/span&gt; [&lt;a href=&#34;http://semantic-sam.xyzou.net:6520/&#34;&gt;Try Auto Generation Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;üöÄ&lt;/span&gt; Features&lt;/h3&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;strong&gt;Reproduce SAM&lt;/strong&gt;. SAM training is a sub-task of ours. We have released the training code to reproduce SAM training.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;strong&gt;Beyond SAM&lt;/strong&gt;. Our newly proposed model offers the following attributes from instance to part level:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Granularity Abundance&lt;/strong&gt;. Our model can produce all possible segmentation granularities for a user click with high quality, which enables more &lt;strong&gt;controllable&lt;/strong&gt; and &lt;strong&gt;user-friendly&lt;/strong&gt; interactive segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic Awareness&lt;/strong&gt;. We jointly train SA-1B with semantically labeled datasets to learn the semantics at both object-level and part-level.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Quality&lt;/strong&gt;. We base on the DETR-based model to implement both generic and interactive segmentation, and validate that SA-1B helps generic and part segmentation. The mask quality of multi-granularity is high.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;span&gt;üöÄ&lt;/span&gt; &lt;strong&gt;News&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We release the &lt;strong&gt;demo code for mask auto-generation!&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;We release the &lt;strong&gt;demo code for interactive segmentation!&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;We release the &lt;strong&gt;training and inference code and checkpoints (SwinT, SwinL) trained on SA-1B!&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;We release the &lt;strong&gt;training code to reproduce SAM!&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/UX-Decoder/Semantic-SAM/assets/11957155/769a0c28-bcdf-42ac-b418-17961c1f2430&#34; alt=&#34;teaser_xyz&#34;&gt; &lt;span&gt;üî•&lt;/span&gt; One-click to output up to 6 granularity masks. Try it in our demo! &lt;img src=&#34;https://github.com/UX-Decoder/Semantic-SAM/assets/34880758/10554e8c-e7cf-463b-875e-0792e629315e&#34; alt=&#34;character&#34;&gt; &lt;span&gt;üî•&lt;/span&gt; Segment everything for one image. We output more masks with more granularity. &lt;img src=&#34;https://github.com/UX-Decoder/Semantic-SAM/assets/34880758/c8672450-0d1f-48b3-8227-ec669263d3b5&#34; alt=&#34;auto&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our model supports a wide range of segmentation tasks and their related applications, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generic Segmentation&lt;/li&gt; &#xA; &lt;li&gt;Part Segmentation&lt;/li&gt; &#xA; &lt;li&gt;Interactive Multi-Granularity Segmentation with Semantics&lt;/li&gt; &#xA; &lt;li&gt;Multi-Granularity Image Editing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;strong&gt;Related projects:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/MaskDINO&#34;&gt;Mask DINO&lt;/a&gt;: We build upon Mask DINO which is a unified detection and segmentation model to implement our model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/OpenSeeD&#34;&gt;OpenSeeD&lt;/a&gt;: Strong open-set segmentation methods based on Mask DINO. We base on it to implement our open-vocabulary segmentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM&lt;/a&gt;: Segment using a wide range of user prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/VLPart&#34;&gt;VLPart&lt;/a&gt;: Going denser with open-vocabulary part segmentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install torch==1.13.1 torchvision==0.14.1 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;python -m pip install &#39;git+https://github.com/MaureenZOU/detectron2-xyz.git&#39;&#xA;pip install git+https://github.com/cocodataset/panopticapi.git&#xA;git clone https://github.com/UX-Decoder/Semantic-SAM&#xA;cd Semantic-SAM&#xA;python -m pip install -r requirements.txt&#xA;&#xA;export DATASET=/pth/to/dataset  # path to your coco data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data preparation&lt;/h3&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/UX-Decoder/Semantic-SAM/main/DATASET.md&#34;&gt;prepare SA-1B data&lt;/a&gt;. Let us know if you need more instructions about it.&lt;/p&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;p&gt;The currently released checkpoints are only trained with SA-1B data.&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE HEADER --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th valign=&#34;bottom&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Training Dataset&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Backbone&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;1-IoU@Multi-Granularity&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;1-IoU@COCO(Max|Oracle)&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Semantic-SAM | &lt;a href=&#34;https://raw.githubusercontent.com/UX-Decoder/Semantic-SAM/main/configs/semantic_sam_only_sa-1b_swinT.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SA-1B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.5|73.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Semantic-SAM/releases/download/checkpoint/swint_only_sam_many2many.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Semantic-SAM | &lt;a href=&#34;https://raw.githubusercontent.com/UX-Decoder/Semantic-SAM/main/configs/semantic_sam_only_sa-1b_swinL.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SA-1B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SwinL&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.1|74.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Semantic-SAM/releases/download/checkpoint/swinl_only_sam_many2many.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;For interactive segmentation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For mask auto-generation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python demo_auto_generation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We do zero-shot evaluation on COCO val2017. &lt;code&gt;$n&lt;/code&gt; is the number of gpus you use&lt;/p&gt; &#xA;&lt;p&gt;For SwinL backbone&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_net.py --eval_only --resume --num-gpus $n --config-file configs/semantic_sam_only_sa-1b_swinL.yaml COCO.TEST.BATCH_SIZE_TOTAL=$n  MODEL.WEIGHTS=/path/to/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For SwinT backbone&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_net.py --eval_only --resume --num-gpus $n --config-file configs/semantic_sam_only_sa-1b_swinT.yaml COCO.TEST.BATCH_SIZE_TOTAL=$n  MODEL.WEIGHTS=/path/to/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;We currently release the code of training on SA-1B only. Complete training with semantics will be released later. &lt;code&gt;$n&lt;/code&gt; is the number of gpus you use before running the training code, you need to specify your training data of SA-1B.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export SAM_DATASET=/pth/to/dataset&#xA;export SAM_DATASET_START=$start&#xA;export SAM_DATASET_END=$end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We convert SA-1B data into 100 tsv files. &lt;code&gt;start&lt;/code&gt;(int, 0-99) is the start of your SA-1B data index and &lt;code&gt;end&lt;/code&gt;(int, 0-99) is the end of your data index. If you are not using the tsv data formats, you can refer to this &lt;a href=&#34;https://raw.githubusercontent.com/UX-Decoder/Semantic-SAM/main/datasets/registration/register_sam_json.py&#34;&gt;json registration for SAM&lt;/a&gt; for a reference.&lt;/p&gt; &#xA;&lt;p&gt;For SwinL backbone&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_net.py --resume --num-gpus $n  --config-file configs/semantic_sam_only_sa-1b_swinL.yaml COCO.TEST.BATCH_SIZE_TOTAL=$n  SAM.TEST.BATCH_SIZE_TOTAL=$n  SAM.TRAIN.BATCH_SIZE_TOTAL=$n MODEL.WEIGHTS=/path/to/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For SwinT backbone&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_net.py --resume --num-gpus $n  --config-file configs/semantic_sam_only_sa-1b_swinT.yaml COCO.TEST.BATCH_SIZE_TOTAL=$n  SAM.TEST.BATCH_SIZE_TOTAL=$n  SAM.TRAIN.BATCH_SIZE_TOTAL=$n MODEL.WEIGHTS=/path/to/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;We also support training to reproduce SAM&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train_net.py --resume --num-gpus $n  --config-file configs/semantic_sam_reproduce_sam_swinL.yaml COCO.TEST.BATCH_SIZE_TOTAL=$n  SAM.TEST.BATCH_SIZE_TOTAL=$n  SAM.TRAIN.BATCH_SIZE_TOTAL=$n MODEL.WEIGHTS=/path/to/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is a swinL backbone. The only difference of this script is to use many-to-one matching and 3 prompts as in SAM.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with SAM and SA-1B Ground-truth&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/UX-Decoder/Semantic-SAM/assets/34880758/6c7b50eb-6fe4-4a4f-b3cb-71920e30193e&#34; alt=&#34;compare_sam_v3&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(a)(b) are the output masks of our model and SAM, respectively. The red points on the left-most image of each row are the user clicks. (c) shows the GT masks that contain the user clicks. The outputs of our model have been processed to remove duplicates.&lt;/p&gt; &#xA;&lt;h2&gt;Learned prompt semantics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/UX-Decoder/Semantic-SAM/assets/34880758/d4c3df78-ba07-4f09-9d4f-e5d4f2fc7f45&#34; alt=&#34;levels&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We visualize the prediction of each content prompt embedding of points with a fixed order for our model. We find all the output masks are from small to large. This indicates each prompt embedding represents a semantic level. The red point in the first column is the click.&lt;/p&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/UX-Decoder/Semantic-SAM/assets/11957155/8e8150a4-a1de-49a6-a817-3c43cf55871b&#34; alt=&#34;method_xyz&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Experiments&lt;/h2&gt; &#xA;&lt;p&gt;We also show that jointly training SA-1B interactive segmentation and generic segmentation can improve the generic segmentation performance. &lt;img src=&#34;https://github.com/UX-Decoder/Semantic-SAM/assets/34880758/b4963761-ef36-47bb-b960-9884b86dce5b&#34; alt=&#34;coco&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also outperform SAM on both mask quality and granularity completeness, please refer to our paper for more experimental details.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; &lt;font size=&#34;8&#34;&gt;&lt;strong&gt;Todo list&lt;/strong&gt;&lt;/font&gt; &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Release demo&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Release code and checkpoints trained on SA-1B&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Release demo with semantics&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;Release code and checkpoints trained on SA-1B and semantically-labeled datasets&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>RayVentura/ShortGPT</title>
    <updated>2023-07-19T01:53:33Z</updated>
    <id>tag:github.com,2023-07-19:/RayVentura/ShortGPT</id>
    <link href="https://github.com/RayVentura/ShortGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üöÄüé¨ ShortGPT - An experimental AI framework for automated short/video content creation. Enables creators to rapidly produce, manage, and deliver content using AI and automation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄüé¨ ShortGPT&lt;/h1&gt; &#xA;&lt;!-- [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/uERx39ru3R&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/uERx39ru3R?compact=true&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/RayVenturaHQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/rayventurahq.svg?style=social&amp;amp;label=Follow%20%40RayVentura&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://star-history.com/#rayventura/shortgpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rayventura/shortgpt?style=social&#34; alt=&#34;GitHub star chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/RayVentura/ShortGPT/assets/121462835/083c8dc3-bac5-42c1-a08d-3ff9686d18c5&#34; alt=&#34;ShortGPT-logo&#34; style=&#34;border-radius: 20px;&#34; width=&#34;22%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!--![android-chrome-192x192](https://github.com/RayVentura/ShortGPT/assets/121462835/083c8dc3-bac5-42c1-a08d-3ff9686d18c5) --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/uERx39ru3R&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/discord-join%20chat-blue.svg?sanitize=true&#34; alt=&#34;Join our Discord&#34; height=&#34;34&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;‚ö° Automating video and short content creation with AI ‚ö°&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è How it works&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/RayVentura/ShortGPT/assets/121462835/fcee74d4-f856-4481-949f-244558bf3bfa&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìù Introduction to ShortGPT&lt;/h2&gt; &#xA;&lt;p&gt;ShortGPT is a powerful framework for automating content creation. It simplifies video creation, footage sourcing, voiceover synthesis, and editing tasks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üéûÔ∏è &lt;strong&gt;Automated editing framework&lt;/strong&gt;: Streamlines the video creation process with an LLM oriented video editing language.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üìÉ &lt;strong&gt;Scripts and Prompts&lt;/strong&gt;: Provides ready-to-use scripts and prompts for various LLM automated editing processes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üó£Ô∏è &lt;strong&gt;Voiceover / Content Creation&lt;/strong&gt;: Supports multiple languages including English üá∫üá∏, Spanish üá™üá∏, Arabic üá¶üá™, French üá´üá∑, Polish üáµüá±, German üá©üá™, Italian üáÆüáπ, and Portuguese üáµüáπ.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîó &lt;strong&gt;Caption Generation&lt;/strong&gt;: Automates the generation of video captions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üåêüé• &lt;strong&gt;Asset Sourcing&lt;/strong&gt;: Sources images and video footage from the internet, connecting with the web and Pexels API as necessary.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Memory and persistency&lt;/strong&gt;: Ensures long-term persistency of automated editing variables with TinyDB.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üé• Showcase (full-video on &lt;a href=&#34;https://www.youtube.com/watch?v=hpoSHq-ER8U&#34;&gt;https://www.youtube.com/watch?v=hpoSHq-ER8U&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/RayVentura/ShortGPT/assets/121462835/a802faad-0fd7-4fcb-aa82-6365c27ea5fe&#34;&gt;https://github.com/RayVentura/ShortGPT/assets/121462835/a802faad-0fd7-4fcb-aa82-6365c27ea5fe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Start: Run ShortGPT on Google Colab (&lt;a href=&#34;https://colab.research.google.com/drive/1_2UKdpF6lqxCqWaAcZb3rwMVQqtbisdE?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1_2UKdpF6lqxCqWaAcZb3rwMVQqtbisdE?usp=sharing&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;If you prefer not to install the prerequisites on your local system, you can use the Google Colab notebook. This option is free and requires no installation setup.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Click on the link to the Google Colab notebook: &lt;a href=&#34;https://colab.research.google.com/drive/1_2UKdpF6lqxCqWaAcZb3rwMVQqtbisdE?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1_2UKdpF6lqxCqWaAcZb3rwMVQqtbisdE?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you&#39;re in the notebook, simply run the cells in order from top to bottom. You can do this by clicking on each cell and pressing the &#39;Play&#39; button, or by using the keyboard . Enjoy using ShortGPT!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üåü Show Your Support&lt;/h2&gt; &#xA;&lt;p&gt;We hope you find ShortGPT helpful! If you do, let us know by giving us a star ‚≠ê on the repo. It&#39;s easy, just click on the &#39;Star&#39; button at the top right of the page. Your support means a lot to us and keeps us motivated to improve and expand ShortGPT. Thank you and happy content creating! üéâ&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/RayVentura/ShortGPT/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rayventura/shortgpt?style=social&#34; alt=&#34;GitHub star chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Instructions for running shortGPT&lt;/h1&gt; &#xA;&lt;p&gt;This guide provides step-by-step instructions for installing ImageMagick and FFmpeg on your system, which are both required to do automated editing. Once installed, you can proceed to run &lt;code&gt;runShortGPT.py&lt;/code&gt; successfully.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before you begin, ensure that you have the following prerequisites installed on your system:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.x&lt;/li&gt; &#xA; &lt;li&gt;Pip (Python package installer)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Steps&lt;/h2&gt; &#xA;&lt;p&gt;Follow the instructions below to install ImageMagick, FFmpeg, and clone the shortGPT repository:&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Install ImageMagick&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;For &lt;code&gt;Windows&lt;/code&gt; download the installer from the official ImageMagick website and follow the installation instructions.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://imagemagick.org/script/download.php&#34;&gt;https://imagemagick.org/script/download.php&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For Ubuntu/Debian-based systems, use the command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get install imagemagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then run the following command to fix a moviepy Imagemagick policy.xml incompatibility problem:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;!sed -i &#39;/&amp;lt;policy domain=&#34;path&#34; rights=&#34;none&#34; pattern=&#34;@\*&#34;/d&#39; /etc/ImageMagick-6/policy.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For macOS using Homebrew, use the command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew install imagemagick&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Verify the installation by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;convert --version&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see the ImageMagick version information if the installation was successful.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 2: Install FFmpeg (REQUIRED FOR SHORTGPT TO WORK)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;For &lt;code&gt;Windows&lt;/code&gt;Download the FFmpeg binaries from this Windows Installer (It will download ffmpeg, ffprobe and add it to your path).&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/icedterminal/ffmpeg-installer/releases/tag/6.0.0.20230306&#34;&gt;https://github.com/icedterminal/ffmpeg-installer/releases/tag/6.0.0.20230306&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For macOS using Homebrew, use the command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For Ubuntu/Debian-based systems, use the command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Verify the installation by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ffmpeg -version&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see the FFmpeg version information if the installation was successful.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 3: Clone the shortGPT Repository&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a terminal or command prompt.&lt;/li&gt; &#xA; &lt;li&gt;Execute the following command to clone the shortGPT repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/rayventura/shortgpt.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 4: Install Python Dependencies&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Open a terminal or command prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the directory where &lt;code&gt;shortgpt.py&lt;/code&gt; is located (the cloned repo).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Execute the following command to install the required Python dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will install the necessary packages specified in the &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running runShortGPT.py Web Interface&lt;/h2&gt; &#xA;&lt;p&gt;Once you have successfully installed ImageMagick, FFmpeg, and the Python dependencies, you can run &lt;code&gt;shortgpt.py&lt;/code&gt; by following these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a terminal or command prompt.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the directory where &lt;code&gt;runShortGPT.py&lt;/code&gt; is located (the cloned repo).&lt;/li&gt; &#xA; &lt;li&gt;Execute the following command to run the script: &lt;pre&gt;&lt;code&gt;python runShortGPT.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;After running the script, a Gradio interface should open at your local host on port 31415 (&lt;a href=&#34;http://localhost:31415&#34;&gt;http://localhost:31415&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Putting API Keys&lt;/h2&gt; &#xA;&lt;p&gt;The ShortGPT UI needs you to input at least OpenAI and ElevenLabs api keys for running short automations. For video automations, you will also need to add a Pexels API.&lt;/p&gt; &#xA;&lt;p&gt;Follow these steps to add your OpenAI and ElevenLabs API keys:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open &lt;a href=&#34;http://localhost:31415/?__theme=light&#34;&gt;http://localhost:31415/?__theme=light&lt;/a&gt; from a web browser.&lt;/li&gt; &#xA; &lt;li&gt;Click on the &lt;code&gt;config&lt;/code&gt; tab located at the left side bar of the user interface.&lt;/li&gt; &#xA; &lt;li&gt;Add your &lt;code&gt;OPENAI API KEY&lt;/code&gt; and &lt;code&gt;ELEVENLABS API KEY&lt;/code&gt; in the corresponding input fields.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Save&lt;/code&gt; to save your API keys.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;That&#39;s it! You have successfully set up your API keys and can now utilize the functionality of ShortGPT in the Gradio interface.&lt;/p&gt; &#xA;&lt;h2&gt;üíÅ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Framework overview&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üé¨ The &lt;code&gt;ContentShortEngine&lt;/code&gt; is designed for creating shorts, handling tasks from script generation to final rendering, including adding YouTube metadata.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üé• The &lt;code&gt;ContentVideoEngine&lt;/code&gt; is ideal for longer videos, taking care of tasks like generating audio, automatically sourcing background video footage, timing captions, and preparing background assets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üéûÔ∏è The automated &lt;code&gt;EditingEngine&lt;/code&gt;, using Editing Markup Language and JSON, breaks down the editing process into manageable and customizable blocks, comprehensible to Large Language Models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üí° ShortGPT offers customization options to suit your needs, from language selection to watermark addition.&lt;/p&gt; &#xA;&lt;p&gt;üîß As a framework, ShortGPT is adaptable and flexible, offering the potential for efficient, creative content creation.&lt;/p&gt; &#xA;&lt;p&gt;More documentation incomming, please be patient.&lt;/p&gt; &#xA;&lt;h2&gt;Technologies Used&lt;/h2&gt; &#xA;&lt;p&gt;ShortGPT utilizes the following technologies to power its functionality:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Moviepy&lt;/strong&gt;: Moviepy is used for video editing, allowing ShortGPT to make video editing and rendering&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Openai&lt;/strong&gt;: Openai is used for automating the entire process, including generating scripts and prompts for LLM automated editing processes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ElevenLabs&lt;/strong&gt;: ElevenLabs is used for voice synthesis, supporting multiple languages for voiceover creation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pexels&lt;/strong&gt;: Pexels is used for sourcing background footage, allowing ShortGPT to connect with the web and access a wide range of images and videos.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bing Image&lt;/strong&gt;: Bing Image is used for sourcing images, providing a comprehensive database for ShortGPT to retrieve relevant visuals.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These technologies work together to provide a seamless and efficient experience in automating video and short content creation with AI.&lt;/p&gt; &#xA;&lt;h2&gt;üîó Get in touch on Twitter üê¶&lt;/h2&gt; &#xA;&lt;p&gt;Keep up with the latest happenings, announcements, and insights about Short-GPT by checking out our Twitter accounts. Spark a conversation with our developer and the AI&#39;s own account for fascinating dialogues, latest news about the project, and more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Developer&lt;/strong&gt;: Stay updated &lt;a href=&#34;https://twitter.com/RayVenturaHQ&#34;&gt;@RayVentura&lt;/a&gt;. Deep-dive into behind-the-scenes, project news, and related topics from the person behind ShortGPT.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We&#39;re eager to interact with you and listen to your feedback, concepts, and experiences with Short-GPT. Come on board on Twitter and let&#39;s navigate the future of AI as a team! üí°ü§ñ&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#RayVentura/ShortGPT&amp;amp;Date&#34;&gt; &lt;img src=&#34;https://api.star-history.com/svg?repos=RayVentura/ShortGPT&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>databricks-academy/large-language-models</title>
    <updated>2023-07-19T01:53:33Z</updated>
    <id>tag:github.com,2023-07-19:/databricks-academy/large-language-models</id>
    <link href="https://github.com/databricks-academy/large-language-models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks for Large Language Models (LLMs) Specialization&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Large Language Models&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains the notebooks and slides for the &lt;a href=&#34;https://www.edx.org/course/large-language-models-application-through-production&#34;&gt;Large Language Models: Application through Production&lt;/a&gt; course on &lt;a href=&#34;https://www.edx.org/professional-certificate/databricks-large-language-models&#34;&gt;edX&lt;/a&gt; &amp;amp; Databricks Academy.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Notebooks&lt;/summary&gt; &#xA; &lt;h2&gt;How to Import the Repo into Databricks?&lt;/h2&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;You first need to add Git credentials to Databricks. Refer to &lt;a href=&#34;https://docs.databricks.com/repos/repos-setup.html#add-git-credentials-to-databricks&#34;&gt;documentation here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Click &lt;code&gt;Repos&lt;/code&gt; in the sidebar. Click &lt;code&gt;Add Repo&lt;/code&gt; on the top right.&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;repo_1&#34; src=&#34;https://files.training.databricks.com/images/llm/repo_1.png&#34;&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Clone the &#34;HTTPS&#34; URL from GitHub, or copy &lt;code&gt;https://github.com/databricks-academy/large-language-models.git&lt;/code&gt; and paste into the box &lt;code&gt;Git repository URL&lt;/code&gt;. The rest of the fields, i.e. &lt;code&gt;Git provider&lt;/code&gt; and &lt;code&gt;Repository name&lt;/code&gt;, will be automatically populated. Click &lt;code&gt;Create Repo&lt;/code&gt; on the bottom right.&lt;/p&gt; &lt;img width=&#34;700&#34; alt=&#34;add_repo&#34; src=&#34;https://files.training.databricks.com/images/llm/add_repo.png&#34;&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;h2&gt;How to Import the files from &lt;code&gt;.dbc&lt;/code&gt; releases on GitHub&lt;/h2&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;You can download the notebooks from a release by navigating to the releases section on the GitHub page:&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;dbc_release1&#34; src=&#34;https://files.training.databricks.com/images/llm/dbc_release1.png&#34;&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;From the releases page, download the &lt;code&gt;.dbc&lt;/code&gt; file. This contains all of the course notebooks, with the structure and meta data.&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;dbc_release2&#34; src=&#34;https://files.training.databricks.com/images/llm/dbc_release2.png&#34;&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;In your Databricks workspace, navigate to the Workspace menu, click on Home and select &lt;code&gt;Import&lt;/code&gt;:&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;dbc_release3&#34; src=&#34;https://files.training.databricks.com/images/llm/dbc_release3.png&#34;&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Using the import tool, navigate to the location on your computer where the &lt;code&gt;.dbc&lt;/code&gt; file was dowloaded from Step 1. Once you select the file, click &lt;code&gt;Import&lt;/code&gt;, and the files will be loaded and extracted to your workspace:&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;dbc_release4&#34; src=&#34;https://files.training.databricks.com/images/llm/dbc_release4.png&#34;&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Cluster settings &lt;/summary&gt; &#xA; &lt;h2&gt;Which Databricks cluster should I use?&lt;/h2&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;First, select &lt;code&gt;Single Node&lt;/code&gt;&lt;/p&gt; &lt;img width=&#34;500&#34; alt=&#34;single_node&#34; src=&#34;https://files.training.databricks.com/images/llm/single_node.png&#34;&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;This courseware has been tested on &lt;a href=&#34;https://raw.githubusercontent.com/databricks-academy/large-language-models/published/%5Burl%5D(https://docs.databricks.com/release-notes/runtime/13.1ml.html)&#34;&gt;Databricks Runtime 13.1 for Machine Learning&lt;/a&gt;. If you do not have access to a 13.1 ML Runtime cluster, you will need to install many additional libraries (as the ML Runtime pre-installs many commonly used machine learning packages), and this courseware is not guaranteed to run.&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;cluster&#34; src=&#34;https://files.training.databricks.com/images/llm/cluster.png&#34;&gt; &lt;p&gt;For all of the notebooks except &lt;code&gt;LLM 04a - Fine-tuning LLMs&lt;/code&gt; and &lt;code&gt;LLM04L - Fine-tuning LLMs Lab&lt;/code&gt;, you can run them on a CPU just fine. We recommend either &lt;code&gt;i3.xlarge&lt;/code&gt; or &lt;code&gt;i3.2xlarge&lt;/code&gt; (i3.2xlarge will have slightly faster performance).&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;cpu_settings&#34; src=&#34;https://files.training.databricks.com/images/llm/cpu_settings.png&#34;&gt; &lt;p&gt;For these notebooks: &lt;code&gt;LLM 04a - Fine-tuning LLMs&lt;/code&gt; and &lt;code&gt;LLM04L - Fine-tuning LLMs Lab&lt;/code&gt;, you will need the Databricks Runtime 13.1 for Machine Learning &lt;strong&gt;with GPU&lt;/strong&gt;.&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;gpu&#34; src=&#34;https://files.training.databricks.com/images/llm/gpu.png&#34;&gt; &lt;p&gt;Select GPU instance type of &lt;code&gt;g5.2xlarge&lt;/code&gt;.&lt;/p&gt; &lt;img width=&#34;400&#34; alt=&#34;gpu_settings&#34; src=&#34;https://files.training.databricks.com/images/llm/gpu_settings.png&#34;&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Install datasets and models &lt;/summary&gt; &#xA; &lt;h2&gt;How do I install the datasets and models locally?&lt;/h2&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;To improve performance of the code, we highly recommend pre-installing the datasets and models by running the &lt;code&gt;LLM 00a - Install Datasets&lt;/code&gt; notebook. &lt;br&gt; &lt;img width=&#34;400&#34; alt=&#34;install_datasets_file&#34; src=&#34;https://files.training.databricks.com/images/llm/installdatasets1.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;You should run this script before running any of the other notebooks. This can take up to 25mins to complete. &lt;img width=&#34;1000&#34; alt=&#34;install_datasets_notebook&#34; src=&#34;https://files.training.databricks.com/images/llm/installdatasets2.png&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Slides &lt;/summary&gt; &#xA; &lt;h2&gt;Where do I download course slides?&lt;/h2&gt; &#xA; &lt;p&gt;Please click the latest version under the &lt;code&gt;Releases&lt;/code&gt; section. You will be able to download the slides in PDF.&lt;/p&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
</feed>