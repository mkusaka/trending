<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-25T01:32:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xmu-xiaoma666/External-Attention-pytorch</title>
    <updated>2022-06-25T01:32:05Z</updated>
    <id>tag:github.com,2022-06-25:/xmu-xiaoma666/External-Attention-pytorch</id>
    <link href="https://github.com/xmu-xiaoma666/External-Attention-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🍀 Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.⭐⭐⭐&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/FightingCVimg/LOGO.gif&#34; height=&#34;200&#34; width=&#34;400&#34;&gt; &#xA;&lt;h1&gt;FightingCV Codebase For &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#attention-series&#34;&gt;&lt;em&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;,&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#backbone-series&#34;&gt;&lt;em&gt;&lt;strong&gt;Backbone&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#mlp-series&#34;&gt;&lt;em&gt;&lt;strong&gt;MLP&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#re-parameter-series&#34;&gt;&lt;em&gt;&lt;strong&gt;Re-parameter&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#convolution-series&#34;&gt;&lt;strong&gt;Convolution&lt;/strong&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-%3E=v3.0-blue&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/pytorch-%3E=v1.4-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;If this project is helpful to you, welcome to give a &lt;em&gt;&lt;strong&gt;star&lt;/strong&gt;&lt;/em&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Don&#39;t forget to &lt;em&gt;&lt;strong&gt;follow&lt;/strong&gt;&lt;/em&gt; me to learn about project updates.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- - **😄 😄 😄 I am looking for jobs for 2023, my interest is multi-modal pretraining, video-text retrieval, computer vision or other feilds about multi modality!!! Welcome to chat with me by wechat(id:mayiwei1998)**&#xA;- **😄 😄 😄 我正在寻找2023年的工作，我的兴趣是多模态预训练、视频文本检索、计算机视觉或其他关于多模态的领域！！！欢迎大家通过微信与我聊天（id:mayiwei1998）** --&gt; &#xA;&lt;p&gt;Hello，大家好，我是小马🚀🚀🚀&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For 小白（Like Me）：&lt;/strong&gt;&lt;/em&gt; 最近在读论文的时候会发现一个问题，有时候论文核心思想非常简单，核心代码可能也就十几行。但是打开作者release的源码时，却发现提出的模块嵌入到分类、检测、分割等任务框架中，导致代码比较冗余，对于特定任务框架不熟悉的我，&lt;strong&gt;很难找到核心代码&lt;/strong&gt;，导致在论文和网络思想的理解上会有一定困难。&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For 进阶者（Like You）：&lt;/strong&gt;&lt;/em&gt; 如果把Conv、FC、RNN这些基本单元看做小的Lego积木，把Transformer、ResNet这些结构看成已经搭好的Lego城堡。那么本项目提供的模块就是一个个具有完整语义信息的Lego组件。&lt;strong&gt;让科研工作者们避免反复造轮子&lt;/strong&gt;，只需思考如何利用这些“Lego组件”，搭建出更多绚烂多彩的作品。&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For 大神（May Be Like You）：&lt;/strong&gt;&lt;/em&gt; 能力有限，&lt;strong&gt;不喜轻喷&lt;/strong&gt;！！！&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For All：&lt;/strong&gt;&lt;/em&gt; 本项目就是要实现一个既能&lt;strong&gt;让深度学习小白也能搞懂&lt;/strong&gt;，又能&lt;strong&gt;服务科研和工业社区&lt;/strong&gt;的代码库。作为&lt;a href=&#34;https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading&#34;&gt;【论文解析项目】&lt;/a&gt;的补充，本项目的宗旨是从代码角度，实现🚀&lt;strong&gt;让世界上没有难读的论文&lt;/strong&gt;🚀。&lt;/p&gt; &#xA;&lt;p&gt;（同时也非常欢迎各位科研工作者将自己的工作的核心代码整理到本项目中，推动科研社区的发展，会在readme中注明代码的作者~）&lt;/p&gt; &#xA;&lt;h2&gt;公众号 &amp;amp; 微信交流群&lt;/h2&gt; &#xA;&lt;p&gt;欢迎大家关注公众号：&lt;strong&gt;FightingCV&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;公众号&lt;strong&gt;每天&lt;/strong&gt;都会进行&lt;strong&gt;论文、算法和代码的干货分享&lt;/strong&gt;哦~&lt;/p&gt; &#xA;&lt;!-- 已建立**机器学习/深度学习算法/计算机视觉/多模态交流群**微信交流群！&#xA;&#xA;（加不进去可以加微信：**775629340**，记得备注【**公司/学校+方向+ID**】） --&gt; &#xA;&lt;p&gt;&lt;strong&gt;每天在群里分享一些近期的论文和解析&lt;/strong&gt;，欢迎大家一起&lt;strong&gt;学习交流&lt;/strong&gt;哈~~~&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/FightingCVimg/wechat.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;强烈推荐大家关注&lt;a href=&#34;https://www.zhihu.com/people/jason-14-58-38/posts&#34;&gt;&lt;strong&gt;知乎&lt;/strong&gt;&lt;/a&gt;账号和&lt;a href=&#34;https://mp.weixin.qq.com/s/sgNw6XFBPcD20Ef3ddfE1w&#34;&gt;&lt;strong&gt;FightingCV公众号&lt;/strong&gt;&lt;/a&gt;，可以快速了解到最新优质的干货资源。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#attention-series&#34;&gt;Attention Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-external-attention-usage&#34;&gt;1. External Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-self-attention-usage&#34;&gt;2. Self Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-simplified-self-attention-usage&#34;&gt;3. Simplified Self Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-squeeze-and-excitation-attention-usage&#34;&gt;4. Squeeze-and-Excitation Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#5-sk-attention-usage&#34;&gt;5. SK Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#6-cbam-attention-usage&#34;&gt;6. CBAM Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#7-bam-attention-usage&#34;&gt;7. BAM Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#8-eca-attention-usage&#34;&gt;8. ECA Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#9-danet-attention-usage&#34;&gt;9. DANet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#10-Pyramid-Split-Attention-Usage&#34;&gt;10. Pyramid Split Attention (PSA) Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#11-Efficient-Multi-Head-Self-Attention-Usage&#34;&gt;11. Efficient Multi-Head Self-Attention(EMSA) Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#12-Shuffle-Attention-Usage&#34;&gt;12. Shuffle Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#13-MUSE-Attention-Usage&#34;&gt;13. MUSE Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#14-SGE-Attention-Usage&#34;&gt;14. SGE Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#15-A2-Attention-Usage&#34;&gt;15. A2 Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#16-AFT-Attention-Usage&#34;&gt;16. AFT Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#17-Outlook-Attention-Usage&#34;&gt;17. Outlook Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#18-ViP-Attention-Usage&#34;&gt;18. ViP Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#19-CoAtNet-Attention-Usage&#34;&gt;19. CoAtNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#20-HaloNet-Attention-Usage&#34;&gt;20. HaloNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#21-Polarized-Self-Attention-Usage&#34;&gt;21. Polarized Self-Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#22-CoTAttention-Usage&#34;&gt;22. CoTAttention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#23-Residual-Attention-Usage&#34;&gt;23. Residual Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#24-S2-Attention-Usage&#34;&gt;24. S2 Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#25-GFNet-Attention-Usage&#34;&gt;25. GFNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#26-TripletAttention-Usage&#34;&gt;26. Triplet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#27-Coordinate-Attention-Usage&#34;&gt;27. Coordinate Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#28-MobileViT-Attention-Usage&#34;&gt;28. MobileViT Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#29-ParNet-Attention-Usage&#34;&gt;29. ParNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#30-UFO-Attention-Usage&#34;&gt;30. UFO Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#31-MobileViTv2-Attention-Usage&#34;&gt;31. MobileViTv2 Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#Backbone-series&#34;&gt;Backbone Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-ResNet-Usage&#34;&gt;1. ResNet Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-ResNeXt-Usage&#34;&gt;2. ResNeXt Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-MobileViT-Usage&#34;&gt;3. MobileViT Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-ConvMixer-Usage&#34;&gt;4. ConvMixer Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#mlp-series&#34;&gt;MLP Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-RepMLP-Usage&#34;&gt;1. RepMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-MLP-Mixer-Usage&#34;&gt;2. MLP-Mixer Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-ResMLP-Usage&#34;&gt;3. ResMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-gMLP-Usage&#34;&gt;4. gMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#5-sMLP-Usage&#34;&gt;5. sMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#Re-Parameter-series&#34;&gt;Re-Parameter(ReP) Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-RepVGG-Usage&#34;&gt;1. RepVGG Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-ACNet-Usage&#34;&gt;2. ACNet Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-Diverse-Branch-Block-Usage&#34;&gt;3. Diverse Branch Block(DDB) Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#Convolution-series&#34;&gt;Convolution Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-Depthwise-Separable-Convolution-Usage&#34;&gt;1. Depthwise Separable Convolution Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-MBConv-Usage&#34;&gt;2. MBConv Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-Involution-Usage&#34;&gt;3. Involution Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-DynamicConv-Usage&#34;&gt;4. DynamicConv Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#5-CondConv-Usage&#34;&gt;5. CondConv Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Attention Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2105.02358&#34;&gt;&#34;Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;&#34;Attention Is All You Need---NIPS2017&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1709.01507&#34;&gt;&#34;Squeeze-and-Excitation Networks---CVPR2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1903.06586.pdf&#34;&gt;&#34;Selective Kernel Networks---CVPR2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf&#34;&gt;&#34;CBAM: Convolutional Block Attention Module---ECCV2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1807.06514.pdf&#34;&gt;&#34;BAM: Bottleneck Attention Module---BMCV2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1910.03151.pdf&#34;&gt;&#34;ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1809.02983.pdf&#34;&gt;&#34;Dual Attention Network for Scene Segmentation---CVPR2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.14447.pdf&#34;&gt;&#34;EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2105.13677&#34;&gt;&#34;ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2102.00240.pdf&#34;&gt;&#34;SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1911.09483&#34;&gt;&#34;MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1905.09646.pdf&#34;&gt;&#34;Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1810.11579.pdf&#34;&gt;&#34;A2-Nets: Double Attention Networks---NIPS2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.14103v1.pdf&#34;&gt;&#34;An Attention Free Transformer---ICLR2021 (Apple New Work)&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2106.13112&#34;&gt;VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24&#34;&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/385561050&#34;&gt;【论文解析】&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2106.12368&#34;&gt;Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23&lt;/a&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/5gonUQgBho_m2O54jyXF_Q&#34;&gt;【论文解析】&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2106.04803&#34;&gt;CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/385578588&#34;&gt;【论文解析】&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2103.12731.pdf&#34;&gt;Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/388598744&#34;&gt;【论文解析】&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2107.00782&#34;&gt;Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/389770482&#34;&gt;【论文解析】&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2107.12292&#34;&gt;Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/394795481&#34;&gt;【论文解析】&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2108.02456&#34;&gt;Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2108.01072&#34;&gt;S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/397003638&#34;&gt;【论文解析】&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2107.00645&#34;&gt;Global Filter Networks for Image Classification---arXiv 2021.07.01&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2010.03045&#34;&gt;Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2110.07641&#34;&gt;Non-deep Networks---ArXiv 2021.10.20&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2109.14382&#34;&gt;UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2206.02680&#34;&gt;Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1. External Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.02358&#34;&gt;&#34;Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/External_Attention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.ExternalAttention import ExternalAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,49,512)&#xA;ea = ExternalAttention(d_model=512,S=8)&#xA;output=ea(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Self Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;&#34;Attention Is All You Need&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SelfAttention import ScaledDotProductAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,49,512)&#xA;sa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;output=sa(input,input,input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Simplified Self Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;&#34;&gt;None&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SSA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,49,512)&#xA;ssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)&#xA;output=ssa(input,input,input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. Squeeze-and-Excitation Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.01507&#34;&gt;&#34;Squeeze-and-Excitation Networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SEAttention import SEAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;se = SEAttention(channel=512,reduction=8)&#xA;output=se(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. SK Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;5.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1903.06586.pdf&#34;&gt;&#34;Selective Kernel Networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SK.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SKAttention import SKAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;se = SKAttention(channel=512,reduction=8)&#xA;output=se(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6. CBAM Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;6.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf&#34;&gt;&#34;CBAM: Convolutional Block Attention Module&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;6.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CBAM1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CBAM2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;6.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.CBAM import CBAMBlock&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;kernel_size=input.shape[2]&#xA;cbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)&#xA;output=cbam(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;7. BAM Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;7.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1807.06514.pdf&#34;&gt;&#34;BAM: Bottleneck Attention Module&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/BAM.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.BAM import BAMBlock&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;bam = BAMBlock(channel=512,reduction=16,dia_val=2)&#xA;output=bam(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;8. ECA Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;8.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.03151.pdf&#34;&gt;&#34;ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;8.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ECA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;8.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.ECAAttention import ECAAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;eca = ECAAttention(kernel_size=3)&#xA;output=eca(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;9. DANet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;9.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.02983.pdf&#34;&gt;&#34;Dual Attention Network for Scene Segmentation&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;9.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/danet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;9.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.DANet import DAModule&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;danet=DAModule(d_model=512,kernel_size=3,H=7,W=7)&#xA;print(danet(input).shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;10. Pyramid Split Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;10.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.14447.pdf&#34;&gt;&#34;EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;10.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/psa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;10.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.PSA import PSA&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;psa = PSA(channel=512,reduction=8)&#xA;output=psa(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;11. Efficient Multi-Head Self-Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;11.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.13677&#34;&gt;&#34;ResT: An Efficient Transformer for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;11.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/EMSA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;11.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.EMSA import EMSA&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,64,512)&#xA;emsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)&#xA;output=emsa(input,input,input)&#xA;print(output.shape)&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;12. Shuffle Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;12.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.00240.pdf&#34;&gt;&#34;SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;12.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ShuffleAttention.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;12.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.ShuffleAttention import ShuffleAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;se = ShuffleAttention(channel=512,G=8)&#xA;output=se(input)&#xA;print(output.shape)&#xA;&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;13. MUSE Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;13.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.09483&#34;&gt;&#34;MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MUSE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.MUSEAttention import MUSEAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;&#xA;input=torch.randn(50,49,512)&#xA;sa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;output=sa(input,input,input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;14. SGE Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;14.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.09646.pdf&#34;&gt;Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;14.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SGE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;14.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SGE import SpatialGroupEnhance&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;sge = SpatialGroupEnhance(groups=8)&#xA;output=sge(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;15. A2 Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;15.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1810.11579.pdf&#34;&gt;A2-Nets: Double Attention Networks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;15.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/A2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;15.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.A2Atttention import DoubleAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;a2 = DoubleAttention(512,128,128,True)&#xA;output=a2(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;16. AFT Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;16.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.14103v1.pdf&#34;&gt;An Attention Free Transformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;16.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/AFT.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;16.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.AFT import AFT_FULL&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,49,512)&#xA;aft_full = AFT_FULL(d_model=512, n=49)&#xA;output=aft_full(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;17. Outlook Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;17.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13112&#34;&gt;VOLO: Vision Outlooker for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;17.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/OutlookAttention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;17.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.OutlookAttention import OutlookAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,28,28,512)&#xA;outlook = OutlookAttention(dim=512)&#xA;output=outlook(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;18. ViP Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;18.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.12368&#34;&gt;Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;18.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ViP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;18.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.ViP import WeightedPermuteMLP&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(64,8,8,512)&#xA;seg_dim=8&#xA;vip=WeightedPermuteMLP(512,seg_dim)&#xA;out=vip(input)&#xA;print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;19. CoAtNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;19.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.04803&#34;&gt;CoAtNet: Marrying Convolution and Attention for All Data Sizes&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;19.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;19.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.CoAtNet import CoAtNet&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,3,224,224)&#xA;mbconv=CoAtNet(in_ch=3,image_size=224)&#xA;out=mbconv(input)&#xA;print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;20. HaloNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;20.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.12731.pdf&#34;&gt;Scaling Local Self-Attention for Parameter Efficient Visual Backbones&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;20.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/HaloNet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;20.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.HaloAttention import HaloAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,512,8,8)&#xA;halo = HaloAttention(dim=512,&#xA;    block_size=2,&#xA;    halo_size=1,)&#xA;output=halo(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;21. Polarized Self-Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;21.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.00782&#34;&gt;Polarized Self-Attention: Towards High-quality Pixel-wise Regression&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;21.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/PoSA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;21.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,512,7,7)&#xA;psa = SequentialPolarizedSelfAttention(channel=512)&#xA;output=psa(input)&#xA;print(output.shape)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;22. CoTAttention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;22.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.12292&#34;&gt;Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;22.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CoT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;22.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.CoTAttention import CoTAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;cot = CoTAttention(dim=512,kernel_size=3)&#xA;output=cot(input)&#xA;print(output.shape)&#xA;&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;23. Residual Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;23.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.02456&#34;&gt;Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;23.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ResAtt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;23.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.ResidualAttention import ResidualAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;resatt = ResidualAttention(channel=512,num_class=1000,la=0.2)&#xA;output=resatt(input)&#xA;print(output.shape)&#xA;&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;24. S2 Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;24.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.01072&#34;&gt;S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;24.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/S2Attention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;24.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.S2Attention import S2Attention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;s2att = S2Attention(channels=512)&#xA;output=s2att(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;25. GFNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;25.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.00645&#34;&gt;Global Filter Networks for Image Classification---arXiv 2021.07.01&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;25.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/GFNet.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;25.3. Usage Code - Implemented by &lt;a href=&#34;https://scholar.google.com/citations?user=lyPWvuEAAAAJ&amp;amp;hl=en&#34;&gt;Wenliang Zhao (Author)&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.gfnet import GFNet&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;x = torch.randn(1, 3, 224, 224)&#xA;gfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)&#xA;out = gfnet(x)&#xA;print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;26. TripletAttention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;26.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.03045&#34;&gt;Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;26.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/triplet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;26.3. Usage Code - Implemented by &lt;a href=&#34;https://github.com/digantamisra98&#34;&gt;digantamisra98&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.TripletAttention import TripletAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;input=torch.randn(50,512,7,7)&#xA;triplet = TripletAttention()&#xA;output=triplet(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;27. Coordinate Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;27.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;Coordinate Attention for Efficient Mobile Network Design---CVPR 2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;27.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CoordAttention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;27.3. Usage Code - Implemented by &lt;a href=&#34;https://github.com/Andrew-Qibin&#34;&gt;Andrew-Qibin&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.CoordAttention import CoordAtt&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;inp=torch.rand([2, 96, 56, 56])&#xA;inp_dim, oup_dim = 96, 96&#xA;reduction=32&#xA;&#xA;coord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)&#xA;output=coord_attention(inp)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;28. MobileViT Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;28.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;28.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MobileViTAttention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;28.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.MobileViTAttention import MobileViTAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    m=MobileViTAttention()&#xA;    input=torch.randn(1,3,49,49)&#xA;    output=m(input)&#xA;    print(output.shape)  #output:(1,3,49,49)&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;29. ParNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;29.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.07641&#34;&gt;Non-deep Networks---ArXiv 2021.10.20&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;29.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ParNet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;29.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.ParNetAttention import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,512,7,7)&#xA;    pna = ParNetAttention(channel=512)&#xA;    output=pna(input)&#xA;    print(output.shape) #50,512,7,7&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;30. UFO Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;30.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.07641&#34;&gt;UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/UFO.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.UFOAttention import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,49,512)&#xA;    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;    output=ufo(input,input,input)&#xA;    print(output.shape) #[50, 49, 512]&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;31. MobileViTv2 Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;31.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02680&#34;&gt;Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;31.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MobileViTv2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;31.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.UFOAttention import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,49,512)&#xA;    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;    output=ufo(input,input,input)&#xA;    print(output.shape) #[50, 49, 512]&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Backbone Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;&#34;Deep Residual Learning for Image Recognition---CVPR2016 Best Paper&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1611.05431v2&#34;&gt;&#34;Aggregated Residual Transformations for Deep Neural Networks---CVPR2017&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://openreview.net/forum?id=TVHS5Y4dNvM&#34;&gt;Patches Are All You Need?---ICLR2022 (Under Review)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. ResNet Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;&#34;Deep Residual Learning for Image Recognition---CVPR2016 Best Paper&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resnet.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resnet2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.resnet import ResNet50,ResNet101,ResNet152&#xA;import torch&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,3,224,224)&#xA;    resnet50=ResNet50(1000)&#xA;    # resnet101=ResNet101(1000)&#xA;    # resnet152=ResNet152(1000)&#xA;    out=resnet50(input)&#xA;    print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. ResNeXt Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.05431v2&#34;&gt;&#34;Aggregated Residual Transformations for Deep Neural Networks---CVPR2017&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resnext.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152&#xA;import torch&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,3,224,224)&#xA;    resnext50=ResNeXt50(1000)&#xA;    # resnext101=ResNeXt101(1000)&#xA;    # resnext152=ResNeXt152(1000)&#xA;    out=resnext50(input)&#xA;    print(out.shape)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. MobileViT Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/mobileViT.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.MobileViT import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(1,3,224,224)&#xA;&#xA;    ### mobilevit_xxs&#xA;    mvit_xxs=mobilevit_xxs()&#xA;    out=mvit_xxs(input)&#xA;    print(out.shape)&#xA;&#xA;    ### mobilevit_xs&#xA;    mvit_xs=mobilevit_xs()&#xA;    out=mvit_xs(input)&#xA;    print(out.shape)&#xA;&#xA;&#xA;    ### mobilevit_s&#xA;    mvit_s=mobilevit_s()&#xA;    out=mvit_s(input)&#xA;    print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. ConvMixer Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openreview.net/forum?id=TVHS5Y4dNvM&#34;&gt;Patches Are All You Need?---ICLR2022 (Under Review)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ConvMixer.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.ConvMixer import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    x=torch.randn(1,3,224,224)&#xA;    convmixer=ConvMixer(dim=512,depth=12)&#xA;    out=convmixer(x)&#xA;    print(out.shape)  #[1, 1000]&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;MLP Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.01883v1.pdf&#34;&gt;&#34;RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.01601.pdf&#34;&gt;&#34;MLP-Mixer: An all-MLP Architecture for Vision---arXiv 2021.05.17&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.03404.pdf&#34;&gt;&#34;ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2105.08050&#34;&gt;&#34;Pay Attention to MLPs---arXiv 2021.05.17&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2109.05422&#34;&gt;&#34;Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. RepMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.01883v1.pdf&#34;&gt;&#34;RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/repmlp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.repmlp import RepMLP&#xA;import torch&#xA;from torch import nn&#xA;&#xA;N=4 #batch size&#xA;C=512 #input dim&#xA;O=1024 #output dim&#xA;H=14 #image height&#xA;W=14 #image width&#xA;h=7 #patch height&#xA;w=7 #patch width&#xA;fc1_fc2_reduction=1 #reduction ratio&#xA;fc3_groups=8 # groups&#xA;repconv_kernels=[1,3,5,7] #kernel list&#xA;repmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)&#xA;x=torch.randn(N,C,H,W)&#xA;repmlp.eval()&#xA;for module in repmlp.modules():&#xA;    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):&#xA;        nn.init.uniform_(module.running_mean, 0, 0.1)&#xA;        nn.init.uniform_(module.running_var, 0, 0.1)&#xA;        nn.init.uniform_(module.weight, 0, 0.1)&#xA;        nn.init.uniform_(module.bias, 0, 0.1)&#xA;&#xA;#training result&#xA;out=repmlp(x)&#xA;#inference result&#xA;repmlp.switch_to_deploy()&#xA;deployout = repmlp(x)&#xA;&#xA;print(((deployout-out)**2).sum())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. MLP-Mixer Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.01601.pdf&#34;&gt;&#34;MLP-Mixer: An all-MLP Architecture for Vision&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/mlpmixer.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.mlp_mixer import MlpMixer&#xA;import torch&#xA;mlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)&#xA;input=torch.randn(50,3,40,40)&#xA;output=mlp_mixer(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. ResMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.03404.pdf&#34;&gt;&#34;ResMLP: Feedforward networks for image classification with data-efficient training&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resmlp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.resmlp import ResMLP&#xA;import torch&#xA;&#xA;input=torch.randn(50,3,14,14)&#xA;resmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)&#xA;out=resmlp(input)&#xA;print(out.shape) #the last dimention is class_num&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. gMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.08050&#34;&gt;&#34;Pay Attention to MLPs&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/gMLP.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.g_mlp import gMLP&#xA;import torch&#xA;&#xA;num_tokens=10000&#xA;bs=50&#xA;len_sen=49&#xA;num_layers=6&#xA;input=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen&#xA;gmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)&#xA;output=gmlp(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. sMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;5.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.05422&#34;&gt;&#34;Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/sMLP.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.sMLP_block import sMLPBlock&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,3,224,224)&#xA;    smlp=sMLPBlock(h=224,w=224)&#xA;    out=smlp(input)&#xA;    print(out.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Re-Parameter Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2101.03697&#34;&gt;&#34;RepVGG: Making VGG-style ConvNets Great Again---CVPR2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1908.03930&#34;&gt;&#34;ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.13425&#34;&gt;&#34;Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1. RepVGG Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.03697&#34;&gt;&#34;RepVGG: Making VGG-style ConvNets Great Again&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/repvgg.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.rep.repvgg import RepBlock&#xA;import torch&#xA;&#xA;&#xA;input=torch.randn(50,512,49,49)&#xA;repblock=RepBlock(512,512)&#xA;repblock.eval()&#xA;out=repblock(input)&#xA;repblock._switch_to_deploy()&#xA;out2=repblock(input)&#xA;print(&#39;difference between vgg and repvgg&#39;)&#xA;print(((out2-out)**2).sum())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. ACNet Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.03930&#34;&gt;&#34;ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/acnet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.acnet import ACNet&#xA;import torch&#xA;from torch import nn&#xA;&#xA;input=torch.randn(50,512,49,49)&#xA;acnet=ACNet(512,512)&#xA;acnet.eval()&#xA;out=acnet(input)&#xA;acnet._switch_to_deploy()&#xA;out2=acnet(input)&#xA;print(&#39;difference:&#39;)&#xA;print(((out2-out)**2).sum())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Diverse Branch Block Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.13425&#34;&gt;&#34;Diverse Branch Block: Building a Convolution as an Inception-like Unit&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ddb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;h5&gt;2.3.1 Transform I&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transI_conv_bn&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;#conv+bn&#xA;conv1=nn.Conv2d(64,64,3,padding=1)&#xA;bn1=nn.BatchNorm2d(64)&#xA;bn1.eval()&#xA;out1=bn1(conv1(input))&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.2 Transform II&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transII_conv_branch&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1=nn.Conv2d(64,64,3,padding=1)&#xA;conv2=nn.Conv2d(64,64,3,padding=1)&#xA;out1=conv1(input)+conv2(input)&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.3 Transform III&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transIII_conv_sequential&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1=nn.Conv2d(64,64,1,padding=0,bias=False)&#xA;conv2=nn.Conv2d(64,64,3,padding=1,bias=False)&#xA;out1=conv2(conv1(input))&#xA;&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1,bias=False)&#xA;conv_fuse.weight.data=transIII_conv_sequential(conv1,conv2)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.4 Transform IV&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transIV_conv_concat&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1=nn.Conv2d(64,32,3,padding=1)&#xA;conv2=nn.Conv2d(64,32,3,padding=1)&#xA;out1=torch.cat([conv1(input),conv2(input)],dim=1)&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.5 Transform V&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transV_avg&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;avg=nn.AvgPool2d(kernel_size=3,stride=1)&#xA;out1=avg(input)&#xA;&#xA;conv=transV_avg(64,3)&#xA;out2=conv(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.6 Transform VI&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transVI_conv_scale&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1x1=nn.Conv2d(64,64,1)&#xA;conv1x3=nn.Conv2d(64,64,(1,3),padding=(0,1))&#xA;conv3x1=nn.Conv2d(64,64,(3,1),padding=(1,0))&#xA;out1=conv1x1(input)+conv1x3(input)+conv3x1(input)&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Convolution Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;&#34;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;http://proceedings.mlr.press/v97/tan19a.html&#34;&gt;&#34;Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.06255&#34;&gt;&#34;Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1912.03458&#34;&gt;&#34;Dynamic Convolution: Attention over Convolution Kernels---CVPR2020 Oral&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1904.04971&#34;&gt;&#34;CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1. Depthwise Separable Convolution Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;&#34;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/DepthwiseSeparableConv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,3,224,224)&#xA;dsconv=DepthwiseSeparableConvolution(3,64)&#xA;out=dsconv(input)&#xA;print(out.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. MBConv Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v97/tan19a.html&#34;&gt;&#34;Efficientnet: Rethinking model scaling for convolutional neural networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MBConv.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.MBConv import MBConvBlock&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,3,224,224)&#xA;mbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)&#xA;out=mbconv(input)&#xA;print(out.shape)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Involution Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.06255&#34;&gt;&#34;Involution: Inverting the Inherence of Convolution for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/Involution.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.Involution import Involution&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,4,64,64)&#xA;involution=Involution(kernel_size=3,in_channel=4,stride=2)&#xA;out=involution(input)&#xA;print(out.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. DynamicConv Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.03458&#34;&gt;&#34;Dynamic Convolution: Attention over Convolution Kernels&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/DynamicConv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.DynamicConv import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(2,32,64,64)&#xA;    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)&#xA;    out=m(input)&#xA;    print(out.shape) # 2,32,64,64&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. CondConv Usage&lt;/h3&gt; &#xA;&lt;h4&gt;5.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.04971&#34;&gt;&#34;CondConv: Conditionally Parameterized Convolutions for Efficient Inference&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CondConv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.CondConv import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(2,32,64,64)&#xA;    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)&#xA;    out=m(input)&#xA;    print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>bigb0sss/RedTeam-OffensiveSecurity</title>
    <updated>2022-06-25T01:32:05Z</updated>
    <id>tag:github.com,2022-06-25:/bigb0sss/RedTeam-OffensiveSecurity</id>
    <link href="https://github.com/bigb0sss/RedTeam-OffensiveSecurity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tools &amp; Interesting Things for RedTeam Ops&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; height=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/bigb0sss/RedTeam-OffensiveSecurity/master/images/redteam_logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;OSINT&lt;/h2&gt; &#xA;&lt;h3&gt;Passive Discovery&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amass - &lt;a href=&#34;https://github.com/OWASP/Amass&#34;&gt;https://github.com/OWASP/Amass&lt;/a&gt; (Attack Surface Mapping)&lt;/li&gt; &#xA; &lt;li&gt;Metabigor - &lt;a href=&#34;https://github.com/j3ssie/metabigor&#34;&gt;https://github.com/j3ssie/metabigor&lt;/a&gt; (Non-API OSINT)&lt;/li&gt; &#xA; &lt;li&gt;AsINT_Collection - &lt;a href=&#34;https://start.me/p/b5Aow7/asint_collection&#34;&gt;https://start.me/p/b5Aow7/asint_collection&lt;/a&gt; (Massive OSINT Collection)&lt;/li&gt; &#xA; &lt;li&gt;Email --&amp;gt; Phone# - &lt;a href=&#34;https://github.com/iansangaji/email2phonenumber&#34;&gt;https://github.com/iansangaji/email2phonenumber&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MFASweep - &lt;a href=&#34;https://github.com/dafthack/MFASweep&#34;&gt;https://github.com/dafthack/MFASweep&lt;/a&gt; (MFA Check for Microsoft endpoints)&lt;/li&gt; &#xA; &lt;li&gt;Fast-Google-Dorks-Scan - &lt;a href=&#34;https://github.com/IvanGlinkin/Fast-Google-Dorks-Scan?mc_cid=70cff8af7c&amp;amp;mc_eid=eff0f218d6&#34;&gt;https://github.com/IvanGlinkin/Fast-Google-Dorks-Scan?mc_cid=70cff8af7c&amp;amp;mc_eid=eff0f218d6&lt;/a&gt; (Google Dork)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Target User Population Collection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linkedin UserEnum - &lt;a href=&#34;https://github.com/bigb0sss/LinkedinMama&#34;&gt;https://github.com/bigb0sss/LinkedinMama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;US Staff UserEnum - &lt;a href=&#34;https://github.com/bigb0sss/USStaffMama&#34;&gt;https://github.com/bigb0sss/USStaffMama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NameSpi - &lt;a href=&#34;https://github.com/waffl3ss/NameSpi&#34;&gt;https://github.com/waffl3ss/NameSpi&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Public Site Lookup (Github, Gitlab, etc.)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gitrob - &lt;a href=&#34;https://github.com/michenriksen/gitrob/&#34;&gt;https://github.com/michenriksen/gitrob/&lt;/a&gt; (Github Search)&lt;/li&gt; &#xA; &lt;li&gt;truffleHog - &lt;a href=&#34;https://github.com/dxa4481/truffleHog&#34;&gt;https://github.com/dxa4481/truffleHog&lt;/a&gt; (Github Regex Search)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cloud Recon&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloud_Security_Wiki - &lt;a href=&#34;https://cloudsecwiki.com/azure_cloud.html&#34;&gt;https://cloudsecwiki.com/azure_cloud.html&lt;/a&gt; (Awesome cloud resources)&lt;/li&gt; &#xA; &lt;li&gt;cloud_enum - &lt;a href=&#34;https://github.com/initstring/cloud_enum&#34;&gt;https://github.com/initstring/cloud_enum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MicroBurst - &lt;a href=&#34;https://github.com/NetSPI/MicroBurst&#34;&gt;https://github.com/NetSPI/MicroBurst&lt;/a&gt; (AZURE)&lt;/li&gt; &#xA; &lt;li&gt;pacu - &lt;a href=&#34;https://github.com/RhinoSecurityLabs/pacu&#34;&gt;https://github.com/RhinoSecurityLabs/pacu&lt;/a&gt; (AWS)&lt;/li&gt; &#xA; &lt;li&gt;FestIn - &lt;a href=&#34;https://github.com/cr0hn/festin&#34;&gt;https://github.com/cr0hn/festin&lt;/a&gt; (AWS)&lt;/li&gt; &#xA; &lt;li&gt;s3viewer - &lt;a href=&#34;https://github.com/SharonBrizinov/s3viewer&#34;&gt;https://github.com/SharonBrizinov/s3viewer&lt;/a&gt; (AWS)&lt;/li&gt; &#xA; &lt;li&gt;Cloud_Pentest_Cheatsheet - &lt;a href=&#34;https://github.com/dafthack/CloudPentestCheatsheets&#34;&gt;https://github.com/dafthack/CloudPentestCheatsheets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;endgame - &lt;a href=&#34;https://github.com/salesforce/endgame&#34;&gt;https://github.com/salesforce/endgame&lt;/a&gt; (AWS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Microsoft / Windows&lt;/h3&gt; &#xA;&lt;h4&gt;Active Discovery&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ZGrab - &lt;a href=&#34;https://github.com/zmap/zgrab&#34;&gt;https://github.com/zmap/zgrab&lt;/a&gt; (Banner grabber)&lt;/li&gt; &#xA; &lt;li&gt;Hardenize - &lt;a href=&#34;https://www.hardenize.com/&#34;&gt;https://www.hardenize.com/&lt;/a&gt; (Domain Lookup)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;ADFS&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ADFSpoof - &lt;a href=&#34;https://github.com/fireeye/ADFSpoof&#34;&gt;https://github.com/fireeye/ADFSpoof&lt;/a&gt; (Forge ADFS security tokens)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Web App&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wordpress-Exploit-Framework - &lt;a href=&#34;https://github.com/rastating/wordpress-exploit-framework&#34;&gt;https://github.com/rastating/wordpress-exploit-framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Awesome-Web-Security - &lt;a href=&#34;https://github.com/qazbnm456/awesome-web-security&#34;&gt;https://github.com/qazbnm456/awesome-web-security&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java Deserialization - &lt;a href=&#34;https://github.com/frohoff/ysoserial&#34;&gt;https://github.com/frohoff/ysoserial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PHP Deserialization - &lt;a href=&#34;https://github.com/ambionics/phpggc&#34;&gt;https://github.com/ambionics/phpggc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kubernetes - &lt;a href=&#34;https://github.com/loodse/kubectl-hacking&#34;&gt;https://github.com/loodse/kubectl-hacking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SSRF - &lt;a href=&#34;https://github.com/jdonsec/AllThingsSSRF&#34;&gt;https://github.com/jdonsec/AllThingsSSRF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Skf-labs - &lt;a href=&#34;https://owasp-skf.gitbook.io/asvs-write-ups/&#34;&gt;https://owasp-skf.gitbook.io/asvs-write-ups/&lt;/a&gt; (Great Write-ups) &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Phishing&lt;/h2&gt; &#xA;&lt;h3&gt;Phishing Techniques - &lt;a href=&#34;https://blog.sublimesecurity.com/&#34;&gt;https://blog.sublimesecurity.com/&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Microsfot 365 Device Code Phishing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;devicePhish - &lt;a href=&#34;https://github.com/bigb0sss/Microsoft365_devicePhish&#34;&gt;https://github.com/bigb0sss/Microsoft365_devicePhish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TokenTactics - &lt;a href=&#34;https://github.com/rvrsh3ll/TokenTactics&#34;&gt;https://github.com/rvrsh3ll/TokenTactics&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Password Spray&lt;/h2&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MSOLSpray - &lt;a href=&#34;https://github.com/dafthack/MSOLSpray&#34;&gt;https://github.com/dafthack/MSOLSpray&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;o365enum.py - &lt;a href=&#34;https://github.com/gremwell/o365enum&#34;&gt;https://github.com/gremwell/o365enum&lt;/a&gt; (Microsoft ActiveSync)&lt;/li&gt; &#xA; &lt;li&gt;goPassGen - &lt;a href=&#34;https://github.com/bigb0sss/goPassGen&#34;&gt;https://github.com/bigb0sss/goPassGen&lt;/a&gt; (PasswordSpray List Generator)&lt;/li&gt; &#xA; &lt;li&gt;go365 - &lt;a href=&#34;https://github.com/optiv/Go365&#34;&gt;https://github.com/optiv/Go365&lt;/a&gt; (Microsoft SOAP API endpoint on login.microsoftonline.com)&lt;/li&gt; &#xA; &lt;li&gt;Okta - &lt;a href=&#34;https://github.com/Rhynorater/Okta-Password-Sprayer&#34;&gt;https://github.com/Rhynorater/Okta-Password-Sprayer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;o365Spray - &lt;a href=&#34;https://github.com/0xZDH/o365spray&#34;&gt;https://github.com/0xZDH/o365spray&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Spray365 - &lt;a href=&#34;https://github.com/MarkoH17/Spray365&#34;&gt;https://github.com/MarkoH17/Spray365&lt;/a&gt; (Microsoft365 / Azure AD)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;IP Rotators&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Burp IPRotate - &lt;a href=&#34;https://github.com/PortSwigger/ip-rotate&#34;&gt;https://github.com/PortSwigger/ip-rotate&lt;/a&gt; (Utilizes AWS IP Gateway)&lt;/li&gt; &#xA; &lt;li&gt;ProxyCannon-NG - &lt;a href=&#34;https://github.com/proxycannon/proxycannon-ng&#34;&gt;https://github.com/proxycannon/proxycannon-ng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cloud-proxy - &lt;a href=&#34;https://github.com/tomsteele/cloud-proxy&#34;&gt;https://github.com/tomsteele/cloud-proxy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Proxy-NG - &lt;a href=&#34;https://github.com/jamesbcook/proxy-ng&#34;&gt;https://github.com/jamesbcook/proxy-ng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mubeng - &lt;a href=&#34;https://github.com/kitabisa/mubeng#proxy-ip-rotator&#34;&gt;https://github.com/kitabisa/mubeng#proxy-ip-rotator&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Default Password Check&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CIRT - &lt;a href=&#34;https://cirt.net/passwords&#34;&gt;https://cirt.net/passwords&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DefaultCreds-cheat-sheet - &lt;a href=&#34;https://github.com/ihebski/DefaultCreds-cheat-sheet&#34;&gt;https://github.com/ihebski/DefaultCreds-cheat-sheet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Infrastructure&lt;/h2&gt; &#xA;&lt;h3&gt;Cobal Strike&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Beacon Command Cheatsheet - &lt;a href=&#34;https://github.com/bigb0sss/RedTeam/tree/master/CobaltStrike&#34;&gt;CS Commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cobalt Strike Training Review &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://medium.com/@bigb0ss/red-team-review-of-red-team-operations-with-cobalt-strike-2019-training-course-part-1-962c510565aa&#34;&gt;Part 1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;SharpeningCobaltStrike - &lt;a href=&#34;https://github.com/cube0x0/SharpeningCobaltStrike&#34;&gt;https://github.com/cube0x0/SharpeningCobaltStrike&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alternative ExecuteAssembly - &lt;a href=&#34;https://github.com/med0x2e/ExecuteAssembly&#34;&gt;https://github.com/med0x2e/ExecuteAssembly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inline ExecuteAssembly - &lt;a href=&#34;https://github.com/anthemtotheego/InlineExecute-Assembly&#34;&gt;https://github.com/anthemtotheego/InlineExecute-Assembly&lt;/a&gt; (Executing .NET Assembly in the same process unline CS&#39;s Execute-Assembly)&lt;/li&gt; &#xA; &lt;li&gt;BOF (Beacon Object Files) - &lt;a href=&#34;https://github.com/trustedsec/CS-Situational-Awareness-BOF&#34;&gt;https://github.com/trustedsec/CS-Situational-Awareness-BOF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Malleable C2&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Malleable C2 (Guideline) - &lt;a href=&#34;https://github.com/bigb0sss/RedTeam/raw/master/CobaltStrike/malleable_C2_profile/CS4.0_guideline.profile&#34;&gt;CS4.0_guideline.profile&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Malleable C2 Randomizer - &lt;a href=&#34;https://fortynorthsecurity.com/blog/introducing-c2concealer/&#34;&gt;https://fortynorthsecurity.com/blog/introducing-c2concealer/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SourcePoint - &lt;a href=&#34;https://github.com/Tylous/SourcePoint&#34;&gt;https://github.com/Tylous/SourcePoint&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;C2 (Opensource)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OffensiveNotion - &lt;a href=&#34;https://github.com/mttaggart/OffensiveNotion&#34;&gt;https://github.com/mttaggart/OffensiveNotion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Redirectors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Domain Fronting - &lt;a href=&#34;https://www.bamsoftware.com/papers/fronting/&#34;&gt;https://www.bamsoftware.com/papers/fronting/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Proxy Infrastructure Setup&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloud-proxy - &lt;a href=&#34;https://github.com/tomsteele/cloud-proxy&#34;&gt;https://github.com/tomsteele/cloud-proxy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Proxy-ng - &lt;a href=&#34;https://github.com/jamesbcook/proxy-ng&#34;&gt;https://github.com/jamesbcook/proxy-ng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ProxyCannon - &lt;a href=&#34;https://github.com/proxycannon/proxycannon-ng&#34;&gt;https://github.com/proxycannon/proxycannon-ng&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Living Off Trusted Sites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LOTS - &lt;a href=&#34;https://lots-project.com/&#34;&gt;https://lots-project.com/&lt;/a&gt; (Trusted sites for C2/Phishing/Downloading)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Post-Exploitation&lt;/h2&gt; &#xA;&lt;h3&gt;Windows Active Directory Recon/Survey&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seatbelt - &lt;a href=&#34;https://github.com/GhostPack/Seatbelt&#34;&gt;https://github.com/GhostPack/Seatbelt&lt;/a&gt; (Ghostpack)&lt;/li&gt; &#xA; &lt;li&gt;DNS Enum - &lt;a href=&#34;https://github.com/dirkjanm/adidnsdump&#34;&gt;https://github.com/dirkjanm/adidnsdump&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows Active Directory Attacks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Attacking &amp;amp; Securing Active Directory - &lt;a href=&#34;https://rmusser.net/docs/Active_Directory.html&#34;&gt;https://rmusser.net/docs/Active_Directory.html&lt;/a&gt; (Awesome references)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Internal Phishing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pickl3 - &lt;a href=&#34;https://github.com/hlldz/pickl3&#34;&gt;https://github.com/hlldz/pickl3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CredPhisher - &lt;a href=&#34;https://github.com/matterpreter/OffensiveCSharp/tree/master/CredPhisher&#34;&gt;https://github.com/matterpreter/OffensiveCSharp/tree/master/CredPhisher&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Credential Theft&lt;/h3&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mimikatz Command References - &lt;a href=&#34;https://adsecurity.org/?page_id=1821&#34;&gt;https://adsecurity.org/?page_id=1821&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Internet Browsers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SharpChromium - &lt;a href=&#34;https://github.com/djhohnstein/SharpChromium&#34;&gt;https://github.com/djhohnstein/SharpChromium&lt;/a&gt; (Chrome)&lt;/li&gt; &#xA; &lt;li&gt;EvilSeleium - &lt;a href=&#34;https://github.com/mrd0x/EvilSelenium&#34;&gt;https://github.com/mrd0x/EvilSelenium&lt;/a&gt; (Chrome)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;LSASS&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SharpDump - &lt;a href=&#34;https://github.com/GhostPack/SharpDump&#34;&gt;https://github.com/GhostPack/SharpDump&lt;/a&gt; (Highly IOC&#39;d)&lt;/li&gt; &#xA; &lt;li&gt;SharpMiniDump - &lt;a href=&#34;https://github.com/b4rtik/SharpMiniDump&#34;&gt;https://github.com/b4rtik/SharpMiniDump&lt;/a&gt; (Uses dynamic API calls, direct syscall and Native API unhooking to evade the AV / EDR detection - Win10 - WinServer2016)&lt;/li&gt; &#xA; &lt;li&gt;Dumper2020 - &lt;a href=&#34;https://github.com/gitjdm/dumper2020&#34;&gt;https://github.com/gitjdm/dumper2020&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nanodump - &lt;a href=&#34;https://github.com/helpsystems/nanodump&#34;&gt;https://github.com/helpsystems/nanodump&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Lateral Movement&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SpectorOps - &lt;a href=&#34;https://posts.specterops.io/offensive-lateral-movement-1744ae62b14f&#34;&gt;https://posts.specterops.io/offensive-lateral-movement-1744ae62b14f&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pypykatz - &lt;a href=&#34;https://github.com/skelsec/pypykatz&#34;&gt;https://github.com/skelsec/pypykatz&lt;/a&gt; (Python implementation of Mimikatz)&lt;/li&gt; &#xA; &lt;li&gt;Internal-Monologue - &lt;a href=&#34;https://github.com/eladshamir/Internal-Monologue&#34;&gt;https://github.com/eladshamir/Internal-Monologue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MSSQL - &lt;a href=&#34;https://research.nccgroup.com/2021/01/21/mssql-lateral-movement/&#34;&gt;https://research.nccgroup.com/2021/01/21/mssql-lateral-movement/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LiquidSnake - &lt;a href=&#34;https://github.com/RiccardoAncarani/LiquidSnake&#34;&gt;https://github.com/RiccardoAncarani/LiquidSnake&lt;/a&gt; (Fileless LM using WMI Event Subscriptions and GadgetToJScript)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Offensive C#&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OffensiveCSharp - &lt;a href=&#34;https://github.com/matterpreter/OffensiveCSharp&#34;&gt;https://github.com/matterpreter/OffensiveCSharp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C# Collection - &lt;a href=&#34;https://github.com/midnightslacker/Sharp/raw/master/README.md&#34;&gt;https://github.com/midnightslacker/Sharp/blob/master/README.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LiveOffTheLand&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LOLBAS - &lt;a href=&#34;https://lolbas-project.github.io/&#34;&gt;https://lolbas-project.github.io/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AV/AMSI Evasion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;xencrypt - &lt;a href=&#34;https://github.com/the-xentropy/xencrypt&#34;&gt;https://github.com/the-xentropy/xencrypt&lt;/a&gt; (PowerShell)&lt;/li&gt; &#xA; &lt;li&gt;FalconStrike - &lt;a href=&#34;https://github.com/slaeryan/FALCONSTRIKE&#34;&gt;https://github.com/slaeryan/FALCONSTRIKE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AV_Bypass - &lt;a href=&#34;https://github.com/Techryptic/AV_Bypass&#34;&gt;https://github.com/Techryptic/AV_Bypass&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DotNetToJScript - &lt;a href=&#34;https://github.com/tyranid/DotNetToJScript&#34;&gt;https://github.com/tyranid/DotNetToJScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GadgetToJScript - &lt;a href=&#34;https://github.com/med0x2e/GadgetToJScript&#34;&gt;https://github.com/med0x2e/GadgetToJScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GadgetToJScript - &lt;a href=&#34;https://github.com/rasta-mouse/GadgetToJScript&#34;&gt;https://github.com/rasta-mouse/GadgetToJScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Shellcodeloader - &lt;a href=&#34;https://github.com/knownsec/shellcodeloader&#34;&gt;https://github.com/knownsec/shellcodeloader&lt;/a&gt; (ShellcodeLoader of windows can bypass AV)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;EDR Evasion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SharpBlock - &lt;a href=&#34;https://github.com/CCob/SharpBlock&#34;&gt;https://github.com/CCob/SharpBlock&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ScareCrow - &lt;a href=&#34;https://github.com/optiv/ScareCrow&#34;&gt;https://github.com/optiv/ScareCrow&lt;/a&gt; (EDR Bypass Payload Creation Framework)&lt;/li&gt; &#xA; &lt;li&gt;Cobalt Strike Tradecraft &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://hausec.com/2021/07/26/cobalt-strike-and-tradecraft/amp/?__twitter_impression=true&#34;&gt;https://hausec.com/2021/07/26/cobalt-strike-and-tradecraft/amp/?__twitter_impression=true&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.cobaltstrike.com/help-opsec&#34;&gt;https://www.cobaltstrike.com/help-opsec&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PowerShell&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;p3nt4 - &lt;a href=&#34;https://github.com/p3nt4&#34;&gt;https://github.com/p3nt4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Log/Trace Deletion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;moonwalk - &lt;a href=&#34;https://github.com/mufeedvh/moonwalk&#34;&gt;https://github.com/mufeedvh/moonwalk&lt;/a&gt; (Linux logs/filesystem timestamps deletion)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Exploit Dev&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ondrik8/exploit&#34;&gt;https://github.com/Ondrik8/exploit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Undocumented Func (Win NT/2000/XP/Win7) - &lt;a href=&#34;http://undocumented.ntinternals.net/&#34;&gt;http://undocumented.ntinternals.net/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows Syscall - &lt;a href=&#34;https://j00ru.vexillium.org/syscalls/nt/64/&#34;&gt;https://j00ru.vexillium.org/syscalls/nt/64/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows Undocumented Func - &lt;a href=&#34;http://undocumented.ntinternals.net/&#34;&gt;http://undocumented.ntinternals.net/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows Kernel Exploit Training - &lt;a href=&#34;https://codemachine.com/&#34;&gt;https://codemachine.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Anti-Debug - &lt;a href=&#34;https://anti-debug.checkpoint.com/&#34;&gt;https://anti-debug.checkpoint.com/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Nix&lt;/h3&gt; &#xA;&lt;h2&gt;RedTeam Researchers/Githubs/Gitbooks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vincent Yiu - &lt;a href=&#34;https://vincentyiu.com&#34;&gt;https://vincentyiu.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Outflank - &lt;a href=&#34;https://github.com/outflanknl&#34;&gt;https://github.com/outflanknl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bank Security - &lt;a href=&#34;https://github.com/BankSecurity/Red_Team&#34;&gt;https://github.com/BankSecurity/Red_Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Infosecn1nja - &lt;a href=&#34;https://github.com/infosecn1nja&#34;&gt;https://github.com/infosecn1nja&lt;/a&gt; (Redteam-Toolkit = AWESOME)&lt;/li&gt; &#xA; &lt;li&gt;Yeyintminthuhtut - &lt;a href=&#34;https://github.com/yeyintminthuhtut&#34;&gt;https://github.com/yeyintminthuhtut&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;RedCanary (Atomic RedTeam) - &lt;a href=&#34;https://github.com/redcanaryco/atomic-red-team&#34;&gt;https://github.com/redcanaryco/atomic-red-team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;kmkz - &lt;a href=&#34;https://github.com/kmkz/Pentesting&#34;&gt;https://github.com/kmkz/Pentesting&lt;/a&gt; (Good cheat-sheets)&lt;/li&gt; &#xA; &lt;li&gt;Rastamouse - &lt;a href=&#34;https://offensivedefence.co.uk/authors/rastamouse/&#34;&gt;https://offensivedefence.co.uk/authors/rastamouse/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(Gitbook) dmcxblue - &lt;a href=&#34;https://dmcxblue.gitbook.io/red-team-notes-2-0/&#34;&gt;https://dmcxblue.gitbook.io/red-team-notes-2-0/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lab Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows Server VMs - &lt;a href=&#34;https://www.microsoft.com/en-us/evalcenter&#34;&gt;https://www.microsoft.com/en-us/evalcenter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows 10 - &lt;a href=&#34;https://www.microsoft.com/en-us/software-download/windows10ISO&#34;&gt;https://www.microsoft.com/en-us/software-download/windows10ISO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Archive of WinVMs - &lt;a href=&#34;https://archive.org/search.php?query=subject%3A%22IEVM%22&#34;&gt;https://archive.org/search.php?query=subject%3A%22IEVM%22&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public MSDN - &lt;a href=&#34;https://the-eye.eu/public/MSDN/&#34;&gt;Link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adversary Tactics: PowerShell - &lt;a href=&#34;https://github.com/specterops/at-ps&#34;&gt;https://github.com/specterops/at-ps&lt;/a&gt; (Specterops)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cloud&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AWS Threat Simulation and Detection - &lt;a href=&#34;https://github.com/sbasu7241/AWS-Threat-Simulation-and-Detection&#34;&gt;https://github.com/sbasu7241/AWS-Threat-Simulation-and-Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stratus Red Team - &lt;a href=&#34;https://github.com/DataDog/stratus-red-team&#34;&gt;https://github.com/DataDog/stratus-red-team&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sexy Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MITRE ATT&amp;amp;CK - &lt;a href=&#34;https://attack.mitre.org/&#34;&gt;https://attack.mitre.org/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MalwareNews - &lt;a href=&#34;https://malware.news/&#34;&gt;https://malware.news/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CWE - &lt;a href=&#34;http://cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html&#34;&gt;http://cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CTID - &lt;a href=&#34;https://github.com/center-for-threat-informed-defense&#34;&gt;https://github.com/center-for-threat-informed-defense&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SpritesMods - &lt;a href=&#34;http://spritesmods.com/?art=main&#34;&gt;http://spritesmods.com/?art=main&lt;/a&gt; (Product Security)&lt;/li&gt; &#xA; &lt;li&gt;Joeware - &lt;a href=&#34;http://www.joeware.net/&#34;&gt;http://www.joeware.net/&lt;/a&gt; (Windows AD Guru - Many AD Recon bins and amazing blogs)&lt;/li&gt; &#xA; &lt;li&gt;Tenable - &lt;a href=&#34;https://github.com/tenable/poc&#34;&gt;https://github.com/tenable/poc&lt;/a&gt; (Exploit POCs)&lt;/li&gt; &#xA; &lt;li&gt;MalwareUnicorn - &lt;a href=&#34;https://malwareunicorn.org/&#34;&gt;https://malwareunicorn.org/&lt;/a&gt; (Malware/Reversing)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Security Testing Practice Lab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hackthebox - &lt;a href=&#34;https://www.hackthebox.eu/&#34;&gt;https://www.hackthebox.eu/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cyberseclab - &lt;a href=&#34;https://www.cyberseclabs.co.uk/&#34;&gt;https://www.cyberseclabs.co.uk/&lt;/a&gt; (AD Focus)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BlueTeam&lt;/h2&gt; &#xA;&lt;h3&gt;Lab Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Detection Lab - &lt;a href=&#34;https://github.com/clong/DetectionLab&#34;&gt;https://github.com/clong/DetectionLab&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Threat Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KQL - &lt;a href=&#34;https://github.com/DebugPrivilege/KQL&#34;&gt;https://github.com/DebugPrivilege/KQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sigma - &lt;a href=&#34;https://github.com/Neo23x0/sigma&#34;&gt;https://github.com/Neo23x0/sigma&lt;/a&gt; (Generic Signature Format for SIEM)&lt;/li&gt; &#xA; &lt;li&gt;Splunk Security Essential Docs - &lt;a href=&#34;https://docs.splunksecurityessentials.com/content-detail/&#34;&gt;https://docs.splunksecurityessentials.com/content-detail/&lt;/a&gt; (Various IOCs)&lt;/li&gt; &#xA; &lt;li&gt;Cobalt Strike Defense - &lt;a href=&#34;https://github.com/MichaelKoczwara/Awesome-CobaltStrike-Defence&#34;&gt;https://github.com/MichaelKoczwara/Awesome-CobaltStrike-Defence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dorothy - &lt;a href=&#34;https://github.com/elastic/dorothy&#34;&gt;https://github.com/elastic/dorothy&lt;/a&gt; (Okta SSO Monitoring and Detection)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows Security (What will BlueTeam look for?)&lt;/h3&gt; &#xA;&lt;h4&gt;LDAP (Lightweight Directory Access Protocol)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-defender-for-endpoint/hunting-for-reconnaissance-activities-using-ldap-search-filters/ba-p/824726&#34;&gt;Hunting for reconnaissance activities using LDAP search filter (Microsoft)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;All the credits belong to the original authors and publishers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;@bigb0ss&lt;/li&gt; &#xA; &lt;li&gt;@T145&lt;/li&gt; &#xA; &lt;li&gt;@threat-punter&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>vcheckzen/KeepAliveE5</title>
    <updated>2022-06-25T01:32:05Z</updated>
    <id>tag:github.com,2022-06-25:/vcheckzen/KeepAliveE5</id>
    <link href="https://github.com/vcheckzen/KeepAliveE5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Set the following repo secrets, then trigger &lt;code&gt;Register APP&lt;/code&gt; workflow manually. Read &lt;a href=&#34;https://sl.al/ABzD&#34;&gt;https://sl.al/ABzD&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PAT&lt;/td&gt; &#xA;   &lt;td&gt;Github personal access token with &lt;code&gt;workflow&lt;/code&gt; permission&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;USER&lt;/td&gt; &#xA;   &lt;td&gt;E5 admin emails line seperated&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PASSWD&lt;/td&gt; &#xA;   &lt;td&gt;E5 admin passwords line seperated&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>