<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-08T01:43:37Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ddPn08/Lsmith</title>
    <updated>2023-02-08T01:43:37Z</updated>
    <id>tag:github.com,2023-02-08:/ddPn08/Lsmith</id>
    <link href="https://github.com/ddPn08/Lsmith" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StableDiffusionWebUI accelerated using TensorRT&lt;/p&gt;&lt;hr&gt;&lt;div&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/docs/images/readme-top.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; Lsmith is a fast StableDiffusionWebUI using high-speed inference technology with TensorRT&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#benchmark&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#docker-all-platform--easy&#34;&gt;Docker (All platform) | Easy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#linux--difficult&#34;&gt;Linux | Difficult&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#requirements&#34;&gt;requirements&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#windows--difficult&#34;&gt;Windows | Difficult&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#requirements-1&#34;&gt;requirements&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#building-the-tensorrt-engine&#34;&gt;Building the TensorRT engine&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/#generate-images&#34;&gt;Generate images&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Benchmark&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/docs/images/readme-benchmark.png&#34; alt=&#34;benchmark&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Docker (All platform) | Easy&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/ddPn08/Lsmith.git&#xA;cd Lsmith&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Launch using Docker compose&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose up --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Linux | Difficult&lt;/h2&gt; &#xA;&lt;h3&gt;requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;node.js (recommended version is 18)&lt;/li&gt; &#xA; &lt;li&gt;pnpm&lt;/li&gt; &#xA; &lt;li&gt;python 3.10&lt;/li&gt; &#xA; &lt;li&gt;pip&lt;/li&gt; &#xA; &lt;li&gt;CUDA&lt;/li&gt; &#xA; &lt;li&gt;cuDNN &amp;lt; 8.6.0&lt;/li&gt; &#xA; &lt;li&gt;TensorRT 8.5.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Follow the instructions on &lt;a href=&#34;https://github.com/NVIDIA/TensorRT/tree/main/demo/Diffusion#build-tensorrt-plugins-library&#34;&gt;this&lt;/a&gt; page to build TensorRT OSS and get &lt;code&gt;libnvinfer_plugin.so&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone Lsmith repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/ddPn08/Lsmith.git&#xA;cd Lsmith&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Enter the repository directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd Lsmith&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Enter frontend directory and build frontend&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd frontend&#xA;pnpm i&#xA;pnpm build --out-dir ../dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Run launch.sh with the path to libnvinfer_plugin.so in the LD_PRELOAD variable.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ex.)&#xA;bash launch.sh --host 0.0.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Windows | Difficult&lt;/h2&gt; &#xA;&lt;h3&gt;requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;node.js (recommended version is 18)&lt;/li&gt; &#xA; &lt;li&gt;pnpm&lt;/li&gt; &#xA; &lt;li&gt;python 3.10&lt;/li&gt; &#xA; &lt;li&gt;pip&lt;/li&gt; &#xA; &lt;li&gt;CUDA&lt;/li&gt; &#xA; &lt;li&gt;cuDNN &amp;lt; 8.6.0&lt;/li&gt; &#xA; &lt;li&gt;TensorRT 8.5.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install nvidia gpu driver&lt;/li&gt; &#xA; &lt;li&gt;Instal cuda 11.x (Click &lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/&#34;&gt;here&lt;/a&gt; for the official guide)&lt;/li&gt; &#xA; &lt;li&gt;Instal cudnn 8.6.0 (Click &lt;a href=&#34;https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html&#34;&gt;here&lt;/a&gt; for the official guide)&lt;/li&gt; &#xA; &lt;li&gt;Install tensorrt 8.5.3.1 (Click &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html&#34;&gt;here&lt;/a&gt; for the official guide)&lt;/li&gt; &#xA; &lt;li&gt;Clone Lsmith repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/ddPn08/Lsmith.git&#xA;cd Lsmith&#xA;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Enter frontend directory and build frontend&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd frontend&#xA;pnpm i&#xA;pnpm build --out-dir ../dist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Launch &lt;code&gt;launch-user.bat&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Once started, access &lt;code&gt;&amp;lt;ip address&amp;gt;:&amp;lt;port number&amp;gt;&lt;/code&gt; (ex &lt;code&gt;http://localhost:8000&lt;/code&gt;) to open the WebUI.&lt;/p&gt; &#xA;&lt;p&gt;First of all, we need to convert our existing diffusers model to the tensorrt engine.&lt;/p&gt; &#xA;&lt;h2&gt;Building the TensorRT engine&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click on the &#34;engine&#34; tab &lt;img src=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/docs/images/readme-usage-screenshot-01.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter Hugging Face&#39;s Diffusers model ID in &lt;code&gt;Model ID&lt;/code&gt; (ex: &lt;code&gt;CompVis/stable-diffusion-v1-4&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Enter your Hugging Face access token in &lt;code&gt;HuggingFace Access Token&lt;/code&gt; (required for some repositories). Access tokens can be obtained or created from &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;this page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Build&lt;/code&gt; button to start building the engine. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;There may be some warnings during the engine build, but you can safely ignore them unless the build fails.&lt;/li&gt; &#xA;   &lt;li&gt;The build can take tens of minutes. For reference it takes an average of 15 minutes on the RTX3060 12GB.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Generate images&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Select the model in the header dropdown.&lt;/li&gt; &#xA; &lt;li&gt;Click on the &#34;txt2img&#34; tab&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Generate&#34; button.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ddPn08/Lsmith/main/docs/images/readme-usage-screenshot-02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Special thanks to the technical members of the &lt;a href=&#34;https://discord.gg/ai-art&#34;&gt;AI絵作り研究会&lt;/a&gt;, a Japanese AI image generation community.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yerfor/GeneFace</title>
    <updated>2023-02-08T01:43:37Z</updated>
    <id>tag:github.com,2023-02-08:/yerfor/GeneFace</id>
    <link href="https://github.com/yerfor/GeneFace" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GeneFace: Generalized and High-Fidelity 3D Talking Face Synthesis; ICLR 2023; Official code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis | ICLR&#39;23&lt;/h1&gt; &#xA;&lt;h4&gt;Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He, Zhou Zhao | Zhejiang University, ByteDance&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.13430&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;| &lt;a href=&#34;https://github.com/yerfor/GeneFace&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yerfor/GeneFace&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; | &lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=yerfor/GeneFace&#34; alt=&#34;visitors&#34;&gt; | &lt;a href=&#34;https://github.com/yerfor/GeneFace/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/yerfor/GeneFace/total.svg?sanitize=true&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/yerfor/GeneFace/main/README-zh.md&#34;&gt;中文文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository is the official PyTorch implementation of our &lt;a href=&#34;https://arxiv.org/abs/2301.13430&#34;&gt;ICLR-2023 paper&lt;/a&gt;, in which we propose &lt;strong&gt;GeneFace&lt;/strong&gt; for generalized and high-fidelity audio-driven talking face generation. The inference pipeline is as follows:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yerfor/GeneFace/main/assets/GeneFace.png&#34; width=&#34;1000&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;Our GeneFace achieves better lip synchronization and expressiveness to out-of-domain audios. Watch &lt;a href=&#34;https://geneface.github.io/GeneFace/example_show_improvement.mp4&#34;&gt;this video&lt;/a&gt; for a clear lip-sync comparison against previous NeRF-based methods. You can also visit our &lt;a href=&#34;https://geneface.github.io/&#34;&gt;project page&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start!&lt;/h2&gt; &#xA;&lt;p&gt;We provide pre-trained models of GeneFace in &lt;a href=&#34;https://github.com/yerfor/GeneFace/releases/tag/v1.0.0&#34;&gt;this release&lt;/a&gt; to enable a quick start. In the following, we show how to infer the pre-trained models in 4 steps. If you want to train GeneFace on your own target person video, please reach to the following sections (&lt;code&gt;Prepare Environments&lt;/code&gt;, &lt;code&gt;Prepare Datasets&lt;/code&gt;, and &lt;code&gt;Train Models&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Step1. Create a new python env named &lt;code&gt;geneface&lt;/code&gt; following the guide in &lt;code&gt;docs/prepare_env/install_guide_nerf.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Step2. Download the &lt;code&gt;lrs3.zip&lt;/code&gt; in the release and unzip it into the &lt;code&gt;checkpoints&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Step3. Download the &lt;code&gt;May.zip&lt;/code&gt; in the release and unzip it into the &lt;code&gt;checkpoints&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;After the above steps, the structure of your &lt;code&gt;checkpoints&lt;/code&gt; directory should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; checkpoints&#xA;    &amp;gt; lrs3&#xA;        &amp;gt; lm3d_vae&#xA;        &amp;gt; syncnet&#xA;    &amp;gt; May&#xA;        &amp;gt; postnet&#xA;        &amp;gt; lm3d_nerf&#xA;        &amp;gt; lm3d_nerf_torso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Step4. Run the scripts below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/infer_postnet.sh&#xA;bash scripts/infer_lm3d_nerf.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find a output video named &lt;code&gt;infer_out/May/pred_video/zozo.mp4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Prepare Environments&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the steps in &lt;code&gt;docs/prepare_env&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Prepare Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the steps in &lt;code&gt;docs/process_data&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Train Models&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the steps in &lt;code&gt;docs/train_models&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Train GeneFace on other target person videos&lt;/h1&gt; &#xA;&lt;p&gt;Apart from the &lt;code&gt;May.mp4&lt;/code&gt; provided in this repo, we also provide 8 target person videos that were used in our experiments. You can download them at &lt;a href=&#34;https://drive.google.com/drive/folders/1FwQoBd1ZrBJMrJE3ZzlNhK8xAe1OYGjX?usp=share_link&#34;&gt;this link&lt;/a&gt;. To train on a new video named &lt;code&gt;&amp;lt;video_id&amp;gt;.mp4&lt;/code&gt;, you should place it into the &lt;code&gt;data/raw/videos/&lt;/code&gt; directory, then create a new folder at &lt;code&gt;egs/datasets/videos/&amp;lt;video_id&amp;gt;&lt;/code&gt; and edit config files, according to the provided example folder &lt;code&gt;egs/datasets/videos/May&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also record your own video and train a unique GeneFace model for yourself!&lt;/p&gt; &#xA;&lt;h1&gt;Todo List&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GeneFace use 3D landmark as the intermediate between the audio2motion and motion2image mapping. However, the 3D landmark sequence generated by the postnet sometimes have bad cases (such as shaking head, or extra-large mouth) and influence the quality of the rendered video. Currently, we partially alleviate this problem by postprocessing the predicted 3D landmark sequence. We call for better postprocessing methods.&lt;/li&gt; &#xA; &lt;li&gt;The inference process of NeRF-based renderer is relatively slow (it takes about 2 hours on 1 RTX2080Ti to render 250 frames at 512x512 resolution). Currently, we could partially alleviate this problem by setting &lt;code&gt;--n_samples_per_ray&lt;/code&gt; and &lt;code&gt;--n_samples_per_ray_fine&lt;/code&gt; to a lower value. In the future we will add acceleration techniques on the NeRF-based renderer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ye2023geneface,&#xA;  title={GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis},&#xA;  author={Ye, Zhenhui and Jiang, Ziyue and Ren, Yi and Liu, Jinglin and He, Jinzheng and Zhao, Zhou},&#xA;  journal={arXiv preprint arXiv:2301.13430},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our codes are based on the following repos:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NATSpeech/NATSpeech&#34;&gt;NATSpeech&lt;/a&gt; (For the code template)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YudongGuo/AD-NeRF&#34;&gt;AD-NeRF&lt;/a&gt; (For NeRF-related implementation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wuhaozhe/style_avatar&#34;&gt;style_avatar&lt;/a&gt; (For 3DMM parameters extraction)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>AIGCT/EASYChatGPT</title>
    <updated>2023-02-08T01:43:37Z</updated>
    <id>tag:github.com,2023-02-08:/AIGCT/EASYChatGPT</id>
    <link href="https://github.com/AIGCT/EASYChatGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is an application project of &#39;chatgpt&#39;,only applicable to desktop environment.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;☆ 只需两步， 轻松玩[ChatGPT] ☆&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;无需账号即可体验喽~&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;据说诱导式可以得到匪夷所思的结果！&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;图源网络，仅供娱乐&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://ai-studio-static-online.cdn.bcebos.com/69ea30db07b741f3b3ffa9fb806634b8404312886bce4049b1c189eb559a2d37&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;0 下载到本地打开项目&lt;/h2&gt; &#xA;&lt;p&gt;欢迎 &lt;strong&gt;fork&lt;/strong&gt; &amp;amp; &lt;strong&gt;star&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;只需两步完成对话调用&lt;/p&gt; &#xA;&lt;h2&gt;注意事项&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;本项目仅用于方便更多开发者体验ChatGPT,生成的内容与本平台无关&lt;/li&gt; &#xA; &lt;li&gt;暂无实现多轮对话。&lt;/li&gt; &#xA; &lt;li&gt;有问题可以在评论区讨论&lt;/li&gt; &#xA; &lt;li&gt;好玩的对话也可以放评论区大家看看，但是禁止搞zz、ys和任何敏感问题。&lt;/li&gt; &#xA; &lt;li&gt;需要自行更换apikey在config.json中&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;1 安装环境&lt;/h2&gt; &#xA;&lt;h1&gt;运行一次即可 安装代码环境&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2 开始对话！&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;python3 app.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;☆PS☆&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;你要在没有开启代理的情况下运行app.py&lt;/li&gt; &#xA; &lt;li&gt;看到下图你就成功了！ &lt;img src=&#34;https://raw.githubusercontent.com/AIGCT/EASYChatGPT/main/su.png&#34; alt=&#34;demo&#34; title=&#34;demo&#34;&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;项目总结&lt;/h2&gt; &#xA;&lt;p&gt;项目借助ChatGPT的接口和个人账户实现。&lt;/p&gt; &#xA;&lt;p&gt;请支持原版ChatGPT,此版本为个人娱乐使用，切勿上升层面。&lt;/p&gt; &#xA;&lt;h2&gt;个人总结&lt;/h2&gt; &#xA;&lt;p&gt;其他精选有趣项目：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/4677578&#34;&gt;[PaddleSpeech] 音色克隆之原神角色 &amp;lt;胡桃&amp;gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/4506607&#34;&gt;中秋款文心带你轻松搞定MV制作&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/4374178&#34;&gt;你的专属AI女友给你唱爱你&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/3971273&#34;&gt;[畊宏女孩]全民健身热潮之AI帮你仰卧起坐计数&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/3877807&#34;&gt;[疫情信息统计进阶篇]PPOCR和QPT的落地实战&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/3752669&#34;&gt;[PaddleSpeech]助力视频字幕生成演讲稿提取&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/projectdetail/2219455&#34;&gt;基于PaddleClas2.2的从零到落地安卓部署的奥特曼分类实战&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;and 若干小白和进阶项目等你发现....&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;我在AI Studio上获得至尊等级，点亮10个徽章，来互关呀~&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/643467&#34;&gt;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/643467&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Disclaimers&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official OpenAI product. This is a personal project and is not affiliated with OpenAI in any way. Don&#39;t sue me.&lt;/p&gt; &#xA;&lt;h2&gt;感谢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/acheong08/ChatGPT&#34;&gt;acheong08&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;and so on...&lt;/p&gt;</summary>
  </entry>
</feed>