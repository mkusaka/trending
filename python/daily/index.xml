<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-15T01:31:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DLR-RM/stable-baselines3</title>
    <updated>2022-07-15T01:31:56Z</updated>
    <id>tag:github.com,2022-07-15:/DLR-RM/stable-baselines3</id>
    <link href="https://github.com/DLR-RM/stable-baselines3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch version of Stable Baselines, reliable implementations of reinforcement learning algorithms.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;docs/\_static/img/logo.png&#34; align=&#34;right&#34; width=&#34;40%&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitlab.com/araffin/stable-baselines3/-/commits/master&#34;&gt;&lt;img src=&#34;https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg?sanitize=true&#34; alt=&#34;pipeline status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/?badge=master&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/stable-baselines/badge/?version=master&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.com/araffin/stable-baselines3/-/commits/master&#34;&gt;&lt;img src=&#34;https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg?sanitize=true&#34; alt=&#34;coverage report&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;codestyle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Stable Baselines3&lt;/h1&gt; &#xA;&lt;p&gt;Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of &lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34;&gt;Stable Baselines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can read a detailed presentation of Stable Baselines3 in the &lt;a href=&#34;https://araffin.github.io/post/sb3/&#34;&gt;v1.0 blog post&lt;/a&gt; or our &lt;a href=&#34;https://jmlr.org/papers/volume22/20-1364/20-1364.pdf&#34;&gt;JMLR paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL).&lt;/strong&gt; You should not utilize this library without some practice. To that extent, we provide good resources in the &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/rl.html&#34;&gt;documentation&lt;/a&gt; to get started with RL.&lt;/p&gt; &#xA;&lt;h2&gt;Main Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The performance of each algorithm was tested&lt;/strong&gt; (see &lt;em&gt;Results&lt;/em&gt; section in their respective page), you can take a look at the issues &lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/48&#34;&gt;#48&lt;/a&gt; and &lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/49&#34;&gt;#49&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Stable-Baselines3&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;State of the art RL methods&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Documentation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Custom environments&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Custom policies&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Common interface&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Dict&lt;/code&gt; observation space support&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ipython / Notebook friendly&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tensorboard support&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PEP8 code style&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Custom callback&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;High code coverage&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Type hints&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Planned features&lt;/h3&gt; &#xA;&lt;p&gt;Please take a look at the &lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/1&#34;&gt;Roadmap&lt;/a&gt; and &lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/milestones&#34;&gt;Milestones&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Migration guide: from Stable-Baselines (SB2) to Stable-Baselines3 (SB3)&lt;/h2&gt; &#xA;&lt;p&gt;A migration guide from SB2 to SB3 can be found in the &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/migration.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Documentation is available online: &lt;a href=&#34;https://stable-baselines3.readthedocs.io/&#34;&gt;https://stable-baselines3.readthedocs.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;Stable-Baselines3 has some integration with other libraries/services like Weights &amp;amp; Biases for experiment tracking or Hugging Face for storing/sharing trained models. You can find out more in the &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html&#34;&gt;dedicated section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34;&gt;RL Baselines3 Zoo&lt;/a&gt; is a training framework for Reinforcement Learning (RL).&lt;/p&gt; &#xA;&lt;p&gt;It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.&lt;/p&gt; &#xA;&lt;p&gt;In addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings.&lt;/p&gt; &#xA;&lt;p&gt;Goals of this repository:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Provide a simple interface to train and enjoy RL agents&lt;/li&gt; &#xA; &lt;li&gt;Benchmark the different Reinforcement Learning algorithms&lt;/li&gt; &#xA; &lt;li&gt;Provide tuned hyperparameters for each environment and RL algorithm&lt;/li&gt; &#xA; &lt;li&gt;Have fun with the trained agents!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Github repo: &lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34;&gt;https://github.com/DLR-RM/rl-baselines3-zoo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Documentation: &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html&#34;&gt;https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;SB3-Contrib: Experimental RL Features&lt;/h2&gt; &#xA;&lt;p&gt;We implement experimental features in a separate contrib repository: &lt;a href=&#34;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&#34;&gt;SB3-Contrib&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This allows SB3 to maintain a stable and compact core, while still providing the latest features, like Recurrent PPO (PPO LSTM), Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO).&lt;/p&gt; &#xA;&lt;p&gt;Documentation is available online: &lt;a href=&#34;https://sb3-contrib.readthedocs.io/&#34;&gt;https://sb3-contrib.readthedocs.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Stable-Baselines3 supports PyTorch &amp;gt;= 1.11&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Stable Baselines3 requires Python 3.7+.&lt;/p&gt; &#xA;&lt;h4&gt;Windows 10&lt;/h4&gt; &#xA;&lt;p&gt;To install stable-baselines on Windows, please look at the &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/install.html#prerequisites&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Install using pip&lt;/h3&gt; &#xA;&lt;p&gt;Install the Stable Baselines3 package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install stable-baselines3[extra]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Some shells such as Zsh require quotation marks around brackets, i.e. &lt;code&gt;pip install &#39;stable-baselines3[extra]&#39;&lt;/code&gt; (&lt;a href=&#34;https://stackoverflow.com/a/30539963&#34;&gt;More Info&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;This includes an optional dependencies like Tensorboard, OpenCV or &lt;code&gt;atari-py&lt;/code&gt; to train on atari games. If you do not need those, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install stable-baselines3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please read the &lt;a href=&#34;https://stable-baselines3.readthedocs.io/&#34;&gt;documentation&lt;/a&gt; for more details and alternatives (from source, using docker).&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.&lt;/p&gt; &#xA;&lt;p&gt;Here is a quick example of how to train and run PPO on a cartpole environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym&#xA;&#xA;from stable_baselines3 import PPO&#xA;&#xA;env = gym.make(&#34;CartPole-v1&#34;)&#xA;&#xA;model = PPO(&#34;MlpPolicy&#34;, env, verbose=1)&#xA;model.learn(total_timesteps=10_000)&#xA;&#xA;obs = env.reset()&#xA;for i in range(1000):&#xA;    action, _states = model.predict(obs, deterministic=True)&#xA;    obs, reward, done, info = env.step(action)&#xA;    env.render()&#xA;    if done:&#xA;      obs = env.reset()&#xA;&#xA;env.close()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or just train a model with a one liner if &lt;a href=&#34;https://github.com/openai/gym/wiki/Environments&#34;&gt;the environment is registered in Gym&lt;/a&gt; and if &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html&#34;&gt;the policy is registered&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3 import PPO&#xA;&#xA;model = PPO(&#34;MlpPolicy&#34;, &#34;CartPole-v1&#34;).learn(10_000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please read the &lt;a href=&#34;https://stable-baselines3.readthedocs.io/&#34;&gt;documentation&lt;/a&gt; for more examples.&lt;/p&gt; &#xA;&lt;h2&gt;Try it online with Colab Notebooks !&lt;/h2&gt; &#xA;&lt;p&gt;All the following examples can be executed online using Google colab notebooks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/araffin/rl-tutorial-jnrr19&#34;&gt;Full Tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3&#34;&gt;All Notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/saving_loading_dqn.ipynb&#34;&gt;Training, Saving, Loading&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb&#34;&gt;Multiprocessing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb&#34;&gt;Monitor Training and Plotting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb&#34;&gt;Atari Games&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb&#34;&gt;RL Baselines Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb&#34;&gt;PyBullet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implemented Algorithms&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Recurrent&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;Box&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;Discrete&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;MultiDiscrete&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;MultiBinary&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Multi Processing&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARS&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/#f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A2C&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DDPG&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DQN&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HER&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PPO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QR-DQN&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/#f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RecurrentPPO&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/#f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAC&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TD3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TQC&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/#f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TRPO&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/#f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Maskable PPO&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/#f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;b id=&#34;f1&#34;&gt;1&lt;/b&gt;: Implemented in &lt;a href=&#34;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&#34;&gt;SB3 Contrib&lt;/a&gt; GitHub repository.&lt;/p&gt; &#xA;&lt;p&gt;Actions &lt;code&gt;gym.spaces&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Box&lt;/code&gt;: A N-dimensional box that containes every point in the action space.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Discrete&lt;/code&gt;: A list of possible actions, where each timestep only one of the actions can be used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MultiDiscrete&lt;/code&gt;: A list of possible actions, where each timestep only one action of each discrete set can be used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MultiBinary&lt;/code&gt;: A list of possible actions, where each timestep any of the actions can be used in any combination.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing the installation&lt;/h2&gt; &#xA;&lt;p&gt;All unit tests in stable baselines3 can be run using &lt;code&gt;pytest&lt;/code&gt; runner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install pytest pytest-cov&#xA;make pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also do a static type check using &lt;code&gt;pytype&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install pytype&#xA;make type&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Codestyle check with &lt;code&gt;flake8&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install flake8&#xA;make lint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Projects Using Stable-Baselines3&lt;/h2&gt; &#xA;&lt;p&gt;We try to maintain a list of project using stable-baselines3 in the &lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/misc/projects.html&#34;&gt;documentation&lt;/a&gt;, please tell us when if you want your project to appear on this page ;)&lt;/p&gt; &#xA;&lt;h2&gt;Citing the Project&lt;/h2&gt; &#xA;&lt;p&gt;To cite this repository in publications:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{stable-baselines3,&#xA;  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},&#xA;  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},&#xA;  journal = {Journal of Machine Learning Research},&#xA;  year    = {2021},&#xA;  volume  = {22},&#xA;  number  = {268},&#xA;  pages   = {1-8},&#xA;  url     = {http://jmlr.org/papers/v22/20-1364.html}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;p&gt;Stable-Baselines3 is currently maintained by &lt;a href=&#34;https://github.com/hill-a&#34;&gt;Ashley Hill&lt;/a&gt; (aka @hill-a), &lt;a href=&#34;https://araffin.github.io/&#34;&gt;Antonin Raffin&lt;/a&gt; (aka &lt;a href=&#34;https://github.com/araffin&#34;&gt;@araffin&lt;/a&gt;), &lt;a href=&#34;https://github.com/ernestum&#34;&gt;Maximilian Ernestus&lt;/a&gt; (aka @ernestum), &lt;a href=&#34;https://github.com/adamgleave&#34;&gt;Adam Gleave&lt;/a&gt; (@AdamGleave) and &lt;a href=&#34;https://github.com/Miffyli&#34;&gt;Anssi Kanervisto&lt;/a&gt; (@Miffyli).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Note: We do not do technical support, nor consulting&lt;/strong&gt; and don&#39;t answer personal questions per email. Please post your question on the &lt;a href=&#34;https://discord.com/invite/xhfNqQv&#34;&gt;RL Discord&lt;/a&gt;, &lt;a href=&#34;https://www.reddit.com/r/reinforcementlearning/&#34;&gt;Reddit&lt;/a&gt; or &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;Stack Overflow&lt;/a&gt; in that case.&lt;/p&gt; &#xA;&lt;h2&gt;How To Contribute&lt;/h2&gt; &#xA;&lt;p&gt;To any interested in making the baselines better, there is still some documentation that needs to be done. If you want to contribute, please read &lt;a href=&#34;https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/CONTRIBUTING.md&#34;&gt;&lt;strong&gt;CONTRIBUTING.md&lt;/strong&gt;&lt;/a&gt; guide first.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The initial work to develop Stable Baselines3 was partially funded by the project &lt;em&gt;Reduced Complexity Models&lt;/em&gt; from the &lt;em&gt;Helmholtz-Gemeinschaft Deutscher Forschungszentren&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The original version, Stable Baselines, was created in the &lt;a href=&#34;http://u2is.ensta-paristech.fr/index.php?lang=en&#34;&gt;robotics lab U2IS&lt;/a&gt; (&lt;a href=&#34;https://flowers.inria.fr/&#34;&gt;INRIA Flowers&lt;/a&gt; team) at &lt;a href=&#34;http://www.ensta-paristech.fr/en&#34;&gt;ENSTA ParisTech&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Logo credits: &lt;a href=&#34;https://www.instagram.com/lucillehue/&#34;&gt;L.M. Tenkes&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleOCR</title>
    <updated>2022-07-15T01:31:56Z</updated>
    <id>tag:github.com,2022-07-15:/PaddlePaddle/PaddleOCR</id>
    <link href="https://github.com/PaddlePaddle/PaddleOCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Awesome multilingual OCR toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/README_ch.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/PaddleOCR_log.png&#34; align=&#34;middle&#34; width=&#34;600&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleOCR?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/format/PaddleOCR?color=c77&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/PaddleOCR/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/PaddleOCR?color=9cf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/en/en_4.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/ch_ppocr_mobile_v2.0/00006737.jpg&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Recent updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üî•2022.5.9 Release PaddleOCR &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.5&#34;&gt;release/2.5&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/ppocr_introduction_en.md#pp-ocrv3&#34;&gt;PP-OCRv3&lt;/a&gt;: With comparable speed, the effect of Chinese scene is further improved by 5% compared with PP-OCRv2, the effect of English scene is improved by 11%, and the average recognition accuracy of 80 language multilingual models is improved by more than 5%.&lt;/li&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/PPOCRLabel&#34;&gt;PPOCRLabelv2&lt;/a&gt;: Add the annotation function for table recognition task, key information extraction task and irregular text image.&lt;/li&gt; &#xA;   &lt;li&gt;Release interactive e-book &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/ocr_book_en.md&#34;&gt;&lt;em&gt;&#34;Dive into OCR&#34;&lt;/em&gt;&lt;/a&gt;, covers the cutting-edge theory and code practice of OCR full stack technology.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021.12.21 Release PaddleOCR &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.4&#34;&gt;release/2.4&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release 1 text detection algorithm (&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithm_det_psenet_en.md&#34;&gt;PSENet&lt;/a&gt;), 3 text recognition algorithms (&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithm_rec_nrtr_en.md&#34;&gt;NRTR&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithm_rec_seed_en.md&#34;&gt;SEED&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithm_rec_nrtr_en.md&#34;&gt;SAR&lt;/a&gt;).&lt;/li&gt; &#xA;   &lt;li&gt;Release 1 key information extraction algorithm &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/kie_en.md&#34;&gt;SDMGR&lt;/a&gt; and 3 &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/vqa&#34;&gt;DocVQA&lt;/a&gt; algorithms (LayoutLM, LayoutLMv2, LayoutXLM).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021.9.7 Release PaddleOCR &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.3&#34;&gt;release/2.3&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/ppocr_introduction_en.md#pp-ocrv2&#34;&gt;PP-OCRv2&lt;/a&gt;. The inference speed of PP-OCRv2 is 220% higher than that of PP-OCR server in CPU device. The F-score of PP-OCRv2 is 7% higher than that of PP-OCR mobile.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2021.8.3 Release PaddleOCR &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.2&#34;&gt;release/2.2&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Release a new structured documents analysis toolkit, i.e., &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/README.md&#34;&gt;PP-Structure&lt;/a&gt;, support layout analysis and table recognition (One-key to export chart images to Excel files).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/update_en.md&#34;&gt;more&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;PaddleOCR support a variety of cutting-edge algorithms related to OCR, and developed industrial featured models/solution &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/ppocr_introduction_en.md&#34;&gt;PP-OCR&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/README.md&#34;&gt;PP-Structure&lt;/a&gt; on this basis, and get through the whole process of data production, model training, compression, inference and deployment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/features_en.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;It is recommended to start with the ‚Äúquick start‚Äù in the document tutorial&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Quick Experience&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Web online experience for the ultra-lightweight OCR: &lt;a href=&#34;https://www.paddlepaddle.org.cn/hub/scene/ocr&#34;&gt;Online Experience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mobile DEMO experience (based on EasyEdge and Paddle-Lite, supports iOS and Android systems): &lt;a href=&#34;https://ai.baidu.com/easyedge/app/openSource?from=paddlelite&#34;&gt;Sign in to the website to obtain the QR code for installing the App&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;One line of code quick use: &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/quickstart_en.md&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;book&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;E-book: &lt;em&gt;Dive Into OCR&lt;/em&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/ocr_book_en.md&#34;&gt;Dive Into OCR üìö&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;Community&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Communityüë¨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For international developers, we regard &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/discussions&#34;&gt;PaddleOCR Discussions&lt;/a&gt; as our international community platform. All ideas and questions can be discussed here in English.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For Chinese develops, Scan the QR code below with your Wechat, you can join the official technical discussion group. For richer community content, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/README_ch.md&#34;&gt;‰∏≠ÊñáREADME&lt;/a&gt;, looking forward to your participation.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/dygraph/doc/joinus.PNG&#34; width=&#34;150&#34; height=&#34;150&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a name=&#34;Supported-Chinese-model-list&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;PP-OCR Series Model ListÔºàUpdate on September 8thÔºâ&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model introduction&lt;/th&gt; &#xA;   &lt;th&gt;Model name&lt;/th&gt; &#xA;   &lt;th&gt;Recommended scene&lt;/th&gt; &#xA;   &lt;th&gt;Detection model&lt;/th&gt; &#xA;   &lt;th&gt;Direction classifier&lt;/th&gt; &#xA;   &lt;th&gt;Recognition model&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese and English ultra-lightweight PP-OCRv3 modelÔºà16.2MÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;ch_PP-OCRv3_xx&lt;/td&gt; &#xA;   &lt;td&gt;Mobile &amp;amp; Server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_distill_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English ultra-lightweight PP-OCRv3 modelÔºà13.4MÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;en_PP-OCRv3_xx&lt;/td&gt; &#xA;   &lt;td&gt;Mobile &amp;amp; Server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_distill_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese and English ultra-lightweight PP-OCRv2 modelÔºà11.6MÔºâ&lt;/td&gt; &#xA;   &lt;td&gt;ch_PP-OCRv2_xx&lt;/td&gt; &#xA;   &lt;td&gt;Mobile &amp;amp; Server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_distill_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese and English ultra-lightweight PP-OCR model (9.4M)&lt;/td&gt; &#xA;   &lt;td&gt;ch_ppocr_mobile_v2.0_xx&lt;/td&gt; &#xA;   &lt;td&gt;Mobile &amp;amp; server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese and English general PP-OCR model (143.4M)&lt;/td&gt; &#xA;   &lt;td&gt;ch_ppocr_server_v2.0_xx&lt;/td&gt; &#xA;   &lt;td&gt;Server&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_det_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_infer.tar&#34;&gt;inference model&lt;/a&gt; / &lt;a href=&#34;https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_server_v2.0_rec_train.tar&#34;&gt;trained model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more model downloads (including multiple languages), please refer to &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/models_list_en.md&#34;&gt;PP-OCR series model downloads&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For a new language request, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/#language_requests&#34;&gt;Guideline for new language_requests&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For structural document analysis models, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/models_list_en.md&#34;&gt;PP-Structure models&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/environment_en.md&#34;&gt;Environment Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/ppocr_introduction_en.md&#34;&gt;PP-OCR üî•&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/quickstart_en.md&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/models_en.md&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/training_en.md&#34;&gt;Model training&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/detection_en.md&#34;&gt;Text Detection&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/recognition_en.md&#34;&gt;Text Recognition&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/angle_class_en.md&#34;&gt;Text Direction Classification&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Model Compression &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/slim/quantization/README_en.md&#34;&gt;Model Quantization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/slim/prune/README_en.md&#34;&gt;Model Pruning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/knowledge_distillation_en.md&#34;&gt;Knowledge Distillation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/README.md&#34;&gt;Inference and Deployment&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/inference_ppocr_en.md&#34;&gt;Python Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/cpp_infer/readme.md&#34;&gt;C++ Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/pdserving/README.md&#34;&gt;Serving&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/lite/readme.md&#34;&gt;Mobile&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/paddle2onnx/readme.md&#34;&gt;Paddle2ONNX&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/paddlecloud/README.md&#34;&gt;PaddleCloud&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/benchmark_en.md&#34;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/README.md&#34;&gt;PP-Structure üî•&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/quickstart_en.md&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/models_list_en.md&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/training_en.md&#34;&gt;Model training&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/layout/README.md&#34;&gt;Layout Parser&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/table/README.md&#34;&gt;Table Recognition&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/vqa/README.md&#34;&gt;DocVQA&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/kie_en.md&#34;&gt;Key Information Extraction&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/README.md&#34;&gt;Inference and Deployment&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/inference_en.md&#34;&gt;Python Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;&#34;&gt;C++ Inference&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/deploy/pdserving/README.md&#34;&gt;Serving&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithms_en.md&#34;&gt;Academic algorithms&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithm_overview_en.md&#34;&gt;Text detection&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithm_overview_en.md&#34;&gt;Text recognition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/algorithm_overview_en.md&#34;&gt;End-to-end&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/add_new_algorithm_en.md&#34;&gt;Add New Algorithms to PaddleOCR&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Data Annotation and Synthesis &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/PPOCRLabel/README.md&#34;&gt;Semi-automatic Annotation Tool: PPOCRLabel&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/StyleText/README.md&#34;&gt;Data Synthesis Tool: Style-Text&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/data_annotation_en.md&#34;&gt;Other Data Annotation Tools&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/data_synthesis_en.md&#34;&gt;Other Data Synthesis Tools&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Datasets &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/dataset/datasets_en.md&#34;&gt;General OCR Datasets(Chinese/English)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/dataset/handwritten_datasets_en.md&#34;&gt;HandWritten_OCR_Datasets(Chinese)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/dataset/vertical_and_multilingual_datasets_en.md&#34;&gt;Various OCR Datasets(multilingual)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/dataset/layout_datasets_en.md&#34;&gt;layout analysis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/dataset/table_datasets_en.md&#34;&gt;table recognition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/dataset/docvqa_datasets_en.md&#34;&gt;DocVQA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/tree_en.md&#34;&gt;Code Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/#Visualization&#34;&gt;Visualization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/#Community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/#language_requests&#34;&gt;New language requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/FAQ_en.md&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/reference_en.md&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/#LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;Visualization&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Visualization &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/doc_en/visualization_en.md&#34;&gt;more&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;PP-OCRv3 Chinese model&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic001.jpg&#34; width=&#34;800&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic002.jpg&#34; width=&#34;800&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic003.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;PP-OCRv3 English model&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/en/en_1.png&#34; width=&#34;800&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/en/en_2.png&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;PP-OCRv3 Multilingual model&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/multi_lang/japan_2.jpg&#34; width=&#34;800&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/doc/imgs_results/PP-OCRv3/multi_lang/korean_1.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;PP-Structure&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;layout analysis + table recognition&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/table/ppstructure.GIF&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;SER (Semantic entity recognition)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/vqa/result_ser/zh_val_0_ser.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;RE (Relation Extraction)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppstructure/docs/vqa/result_re/zh_val_21_re.jpg&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;a name=&#34;language_requests&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Guideline for New Language Requests&lt;/h2&gt; &#xA;&lt;p&gt;If you want to request a new language support, a PR with 1 following files are neededÔºö&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In folder &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/release/2.5/ppocr/utils/dict&#34;&gt;ppocr/utils/dict&lt;/a&gt;, it is necessary to submit the dict text to this path and name it with &lt;code&gt;{language}_dict.txt&lt;/code&gt; that contains a list of all characters. Please see the format example from other files in that folder.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If your language has unique elements, please tell me in advance within any way, such as useful links, wikipedia and so on.&lt;/p&gt; &#xA;&lt;p&gt;More details, please refer to &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/issues/1048&#34;&gt;Multilingual OCR Development Plan&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;LICENSE&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR/raw/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ultralytics/yolov5</title>
    <updated>2022-07-15T01:31:56Z</updated>
    <id>tag:github.com,2022-07-15:/ultralytics/yolov5</id>
    <link href="https://github.com/ultralytics/yolov5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YOLOv5 üöÄ in PyTorch &gt; ONNX &gt; CoreML &gt; TFLite&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a align=&#34;left&#34; href=&#34;https://ultralytics.com/yolov5&#34; target=&#34;_blank&#34;&gt; &lt;img width=&#34;850&#34; src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/splash.jpg&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/ultralytics/yolov5/master/.github/README_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml&#34;&gt;&lt;img src=&#34;https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg?sanitize=true&#34; alt=&#34;CI CPU testing&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://zenodo.org/badge/latestdoi/264818686&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/264818686.svg?sanitize=true&#34; alt=&#34;YOLOv5 Citation&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://hub.docker.com/r/ultralytics/yolov5&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://www.kaggle.com/ultralytics/yolov5&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Open In Kaggle&#34;&gt;&lt;/a&gt; &#xA;  &lt;a href=&#34;https://join.slack.com/t/ultralytics/shared_invite/zt-w29ei8bp-jczz7QYUmDtgo6r6KcMIAg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-Join_Forum-blue.svg?logo=slack&#34; alt=&#34;Join Forum&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt; YOLOv5 üöÄ is a family of object detection architectures and models pretrained on the COCO dataset, and represents &lt;a href=&#34;https://ultralytics.com&#34;&gt;Ultralytics&lt;/a&gt; open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. &lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://github.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-github.png&#34; width=&#34;2%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img width=&#34;2%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.linkedin.com/company/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-linkedin.png&#34; width=&#34;2%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img width=&#34;2%&#34;&gt; &#xA;  &lt;a href=&#34;https://twitter.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-twitter.png&#34; width=&#34;2%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img width=&#34;2%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.producthunt.com/@glenn_jocher&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-producthunt.png&#34; width=&#34;2%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img width=&#34;2%&#34;&gt; &#xA;  &lt;a href=&#34;https://youtube.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-youtube.png&#34; width=&#34;2%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img width=&#34;2%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.facebook.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-facebook.png&#34; width=&#34;2%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img width=&#34;2%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.instagram.com/ultralytics/&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-instagram.png&#34; width=&#34;2%&#34;&gt; &lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;!--&#xA;&lt;a align=&#34;center&#34; href=&#34;https://ultralytics.com/yolov5&#34; target=&#34;_blank&#34;&gt;&#xA;&lt;img width=&#34;800&#34; src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/banner-api.png&#34;&gt;&lt;/a&gt;&#xA;--&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Documentation&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.ultralytics.com&#34;&gt;YOLOv5 Docs&lt;/a&gt; for full documentation on training, testing and deployment.&lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Quick Start Examples&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Install&lt;/summary&gt; &#xA; &lt;p&gt;Clone repo and install &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;Python&amp;gt;=3.7.0&lt;/strong&gt;&lt;/a&gt; environment, including &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;&lt;strong&gt;PyTorch&amp;gt;=1.7&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ultralytics/yolov5  # clone&#xA;cd yolov5&#xA;pip install -r requirements.txt  # install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Inference&lt;/summary&gt; &#xA; &lt;p&gt;YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/36&#34;&gt;PyTorch Hub&lt;/a&gt; inference. &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/models&#34;&gt;Models&lt;/a&gt; download automatically from the latest YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;release&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;# Model&#xA;model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)  # or yolov5n - yolov5x6, custom&#xA;&#xA;# Images&#xA;img = &#39;https://ultralytics.com/images/zidane.jpg&#39;  # or file, Path, PIL, OpenCV, numpy, list&#xA;&#xA;# Inference&#xA;results = model(img)&#xA;&#xA;# Results&#xA;results.print()  # or .show(), .save(), .crop(), .pandas(), etc.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Inference with detect.py&lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt; runs inference on a variety of sources, downloading &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/models&#34;&gt;models&lt;/a&gt; automatically from the latest YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;release&lt;/a&gt; and saving results to &lt;code&gt;runs/detect&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python detect.py --source 0  # webcam&#xA;                          img.jpg  # image&#xA;                          vid.mp4  # video&#xA;                          path/  # directory&#xA;                          path/*.jpg  # glob&#xA;                          &#39;https://youtu.be/Zgi9g1ksQHc&#39;  # YouTube&#xA;                          &#39;rtsp://example.com/media.mp4&#39;  # RTSP, RTMP, HTTP stream&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Training&lt;/summary&gt; &#xA; &lt;p&gt;The commands below reproduce YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/data/scripts/get_coco.sh&#34;&gt;COCO&lt;/a&gt; results. &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/models&#34;&gt;Models&lt;/a&gt; and &lt;a href=&#34;https://github.com/ultralytics/yolov5/tree/master/data&#34;&gt;datasets&lt;/a&gt; download automatically from the latest YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;release&lt;/a&gt;. Training times for YOLOv5n/s/m/l/x are 1/2/4/6/8 days on a V100 GPU (&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/475&#34;&gt;Multi-GPU&lt;/a&gt; times faster). Use the largest &lt;code&gt;--batch-size&lt;/code&gt; possible, or pass &lt;code&gt;--batch-size -1&lt;/code&gt; for YOLOv5 &lt;a href=&#34;https://github.com/ultralytics/yolov5/pull/5092&#34;&gt;AutoBatch&lt;/a&gt;. Batch sizes shown for V100-16GB.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py --data coco.yaml --cfg yolov5n.yaml --weights &#39;&#39; --batch-size 128&#xA;                                       yolov5s                                64&#xA;                                       yolov5m                                40&#xA;                                       yolov5l                                24&#xA;                                       yolov5x                                16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;img width=&#34;800&#34; src=&#34;https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Tutorials&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&#34;&gt;Train Custom Data&lt;/a&gt;&amp;nbsp; üöÄ RECOMMENDED&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results&#34;&gt;Tips for Best Training Results&lt;/a&gt;&amp;nbsp; ‚òòÔ∏è RECOMMENDED&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/1289&#34;&gt;Weights &amp;amp; Biases Logging&lt;/a&gt;&amp;nbsp; üåü NEW&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/4975&#34;&gt;Roboflow for Datasets, Labeling, and Active Learning&lt;/a&gt;&amp;nbsp; üåü NEW&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/475&#34;&gt;Multi-GPU Training&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/36&#34;&gt;PyTorch Hub&lt;/a&gt;&amp;nbsp; ‚≠ê NEW&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/251&#34;&gt;TFLite, ONNX, CoreML, TensorRT Export&lt;/a&gt; üöÄ&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/303&#34;&gt;Test-Time Augmentation (TTA)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/318&#34;&gt;Model Ensembling&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/304&#34;&gt;Model Pruning/Sparsity&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/607&#34;&gt;Hyperparameter Evolution&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/1314&#34;&gt;Transfer Learning with Frozen Layers&lt;/a&gt;&amp;nbsp; ‚≠ê NEW&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/6998&#34;&gt;Architecture Summary&lt;/a&gt;&amp;nbsp; ‚≠ê NEW&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Environments&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Get started in seconds with our verified environments. Click each icon below for details.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-colab-small.png&#34; width=&#34;15%&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://www.kaggle.com/ultralytics/yolov5&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-kaggle-small.png&#34; width=&#34;15%&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://hub.docker.com/r/ultralytics/yolov5&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-docker-small.png&#34; width=&#34;15%&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-aws-small.png&#34; width=&#34;15%&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-gcp-small.png&#34; width=&#34;15%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Integrations&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://wandb.ai/site?utm_campaign=repo_yolo_readme&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-wb-long.png&#34; width=&#34;49%&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://roboflow.com/?ref=ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-roboflow-long.png&#34; width=&#34;49%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Weights and Biases&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Roboflow ‚≠ê NEW&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Automatically track and visualize all your YOLOv5 training runs in the cloud with &lt;a href=&#34;https://wandb.ai/site?utm_campaign=repo_yolo_readme&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Label and export your custom datasets directly to YOLOv5 for training with &lt;a href=&#34;https://roboflow.com/?ref=ultralytics&#34;&gt;Roboflow&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- ## &lt;div align=&#34;center&#34;&gt;Compete and Win&lt;/div&gt;&#xA;&#xA;We are super excited about our first-ever Ultralytics YOLOv5 üöÄ EXPORT Competition with **$10,000** in cash prizes!&#xA;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;  &lt;a href=&#34;https://github.com/ultralytics/yolov5/discussions/3213&#34;&gt;&#xA;  &lt;img width=&#34;850&#34; src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/banner-export-competition.png&#34;&gt;&lt;/a&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Why YOLOv5&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;left&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://user-images.githubusercontent.com/26833433/155040763-93c22a27-347c-4e3c-847a-8094621d3f4e.png&#34;&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;YOLOv5-P5 640 Figure (click to expand)&lt;/summary&gt; &#xA; &lt;p align=&#34;left&#34;&gt;&lt;img width=&#34;800&#34; src=&#34;https://user-images.githubusercontent.com/26833433/155040757-ce0934a3-06a6-43dc-a979-2edbbd69ea0e.png&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Figure Notes (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;COCO AP val&lt;/strong&gt; denotes mAP@0.5:0.95 metric measured on the 5000-image &lt;a href=&#34;http://cocodataset.org&#34;&gt;COCO val2017&lt;/a&gt; dataset over various inference sizes from 256 to 1536.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;GPU Speed&lt;/strong&gt; measures average inference time per image on &lt;a href=&#34;http://cocodataset.org&#34;&gt;COCO val2017&lt;/a&gt; dataset using a &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/p3/&#34;&gt;AWS p3.2xlarge&lt;/a&gt; V100 instance at batch-size 32.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;EfficientDet&lt;/strong&gt; data from &lt;a href=&#34;https://github.com/google/automl&#34;&gt;google/automl&lt;/a&gt; at batch size 8.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Reproduce&lt;/strong&gt; by &lt;code&gt;python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n6.pt yolov5s6.pt yolov5m6.pt yolov5l6.pt yolov5x6.pt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Pretrained Checkpoints&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;size&lt;br&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;br&gt;&lt;sup&gt;CPU b1&lt;br&gt;(ms)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;br&gt;&lt;sup&gt;V100 b1&lt;br&gt;(ms)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;br&gt;&lt;sup&gt;V100 b32&lt;br&gt;(ms)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;params&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;FLOPs&lt;br&gt;&lt;sup&gt;@640 (B)&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5n&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td&gt;28.0&lt;/td&gt; &#xA;   &lt;td&gt;45.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;45&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;6.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5s&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td&gt;37.4&lt;/td&gt; &#xA;   &lt;td&gt;56.8&lt;/td&gt; &#xA;   &lt;td&gt;98&lt;/td&gt; &#xA;   &lt;td&gt;6.4&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;   &lt;td&gt;7.2&lt;/td&gt; &#xA;   &lt;td&gt;16.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5m&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td&gt;45.4&lt;/td&gt; &#xA;   &lt;td&gt;64.1&lt;/td&gt; &#xA;   &lt;td&gt;224&lt;/td&gt; &#xA;   &lt;td&gt;8.2&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;   &lt;td&gt;21.2&lt;/td&gt; &#xA;   &lt;td&gt;49.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5l&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td&gt;49.0&lt;/td&gt; &#xA;   &lt;td&gt;67.3&lt;/td&gt; &#xA;   &lt;td&gt;430&lt;/td&gt; &#xA;   &lt;td&gt;10.1&lt;/td&gt; &#xA;   &lt;td&gt;2.7&lt;/td&gt; &#xA;   &lt;td&gt;46.5&lt;/td&gt; &#xA;   &lt;td&gt;109.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5x&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td&gt;50.7&lt;/td&gt; &#xA;   &lt;td&gt;68.9&lt;/td&gt; &#xA;   &lt;td&gt;766&lt;/td&gt; &#xA;   &lt;td&gt;12.1&lt;/td&gt; &#xA;   &lt;td&gt;4.8&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;205.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5n6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;54.4&lt;/td&gt; &#xA;   &lt;td&gt;153&lt;/td&gt; &#xA;   &lt;td&gt;8.1&lt;/td&gt; &#xA;   &lt;td&gt;2.1&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;4.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5s6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;44.8&lt;/td&gt; &#xA;   &lt;td&gt;63.7&lt;/td&gt; &#xA;   &lt;td&gt;385&lt;/td&gt; &#xA;   &lt;td&gt;8.2&lt;/td&gt; &#xA;   &lt;td&gt;3.6&lt;/td&gt; &#xA;   &lt;td&gt;12.6&lt;/td&gt; &#xA;   &lt;td&gt;16.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5m6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;51.3&lt;/td&gt; &#xA;   &lt;td&gt;69.3&lt;/td&gt; &#xA;   &lt;td&gt;887&lt;/td&gt; &#xA;   &lt;td&gt;11.1&lt;/td&gt; &#xA;   &lt;td&gt;6.8&lt;/td&gt; &#xA;   &lt;td&gt;35.7&lt;/td&gt; &#xA;   &lt;td&gt;50.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5l6&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;53.7&lt;/td&gt; &#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;   &lt;td&gt;1784&lt;/td&gt; &#xA;   &lt;td&gt;15.8&lt;/td&gt; &#xA;   &lt;td&gt;10.5&lt;/td&gt; &#xA;   &lt;td&gt;76.8&lt;/td&gt; &#xA;   &lt;td&gt;111.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/releases&#34;&gt;YOLOv5x6&lt;/a&gt;&lt;br&gt;+ &lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/303&#34;&gt;TTA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;br&gt;1536&lt;/td&gt; &#xA;   &lt;td&gt;55.0&lt;br&gt;&lt;strong&gt;55.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;72.7&lt;br&gt;&lt;strong&gt;72.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3136&lt;br&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;26.2&lt;br&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;19.4&lt;br&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;140.7&lt;br&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;209.8&lt;br&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Table Notes (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;All checkpoints are trained to 300 epochs with default settings. Nano and Small models use &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/data/hyps/hyp.scratch-low.yaml&#34;&gt;hyp.scratch-low.yaml&lt;/a&gt; hyps, all others use &lt;a href=&#34;https://github.com/ultralytics/yolov5/raw/master/data/hyps/hyp.scratch-high.yaml&#34;&gt;hyp.scratch-high.yaml&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on &lt;a href=&#34;http://cocodataset.org&#34;&gt;COCO val2017&lt;/a&gt; dataset.&lt;br&gt;Reproduce by &lt;code&gt;python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; averaged over COCO val images using a &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/p3/&#34;&gt;AWS p3.2xlarge&lt;/a&gt; instance. NMS times (~1 ms/img) not included.&lt;br&gt;Reproduce by &lt;code&gt;python val.py --data coco.yaml --img 640 --task speed --batch 1&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;TTA&lt;/strong&gt; &lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/303&#34;&gt;Test Time Augmentation&lt;/a&gt; includes reflection and scale augmentations.&lt;br&gt;Reproduce by &lt;code&gt;python val.py --data coco.yaml --img 1536 --iou 0.7 --augment&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Contribute&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We love your input! We want to make contributing to YOLOv5 as easy and transparent as possible. Please see our &lt;a href=&#34;https://raw.githubusercontent.com/ultralytics/yolov5/master/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; to get started, and fill out the &lt;a href=&#34;https://ultralytics.com/survey?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=Survey&#34;&gt;YOLOv5 Survey&lt;/a&gt; to send us feedback on your experiences. Thank you to all our contributors!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/ultralytics/contributors.svg?width=990&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  Contact&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;p&gt;For YOLOv5 bugs and feature requests please visit &lt;a href=&#34;https://github.com/ultralytics/yolov5/issues&#34;&gt;GitHub Issues&lt;/a&gt;. For business inquiries or professional support requests please visit &lt;a href=&#34;https://ultralytics.com/contact&#34;&gt;https://ultralytics.com/contact&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-github.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA; &lt;img width=&#34;3%&#34;&gt; &#xA; &lt;a href=&#34;https://www.linkedin.com/company/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-linkedin.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA; &lt;img width=&#34;3%&#34;&gt; &#xA; &lt;a href=&#34;https://twitter.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-twitter.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA; &lt;img width=&#34;3%&#34;&gt; &#xA; &lt;a href=&#34;https://www.producthunt.com/@glenn_jocher&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-producthunt.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA; &lt;img width=&#34;3%&#34;&gt; &#xA; &lt;a href=&#34;https://youtube.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-youtube.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA; &lt;img width=&#34;3%&#34;&gt; &#xA; &lt;a href=&#34;https://www.facebook.com/ultralytics&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-facebook.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA; &lt;img width=&#34;3%&#34;&gt; &#xA; &lt;a href=&#34;https://www.instagram.com/ultralytics/&#34;&gt; &lt;img src=&#34;https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-social-instagram.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>