<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-11T01:41:03Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hpcaitech/SwiftInfer</title>
    <updated>2024-01-11T01:41:03Z</updated>
    <id>tag:github.com,2024-01-11:/hpcaitech/SwiftInfer</id>
    <link href="https://github.com/hpcaitech/SwiftInfer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Efficient AI Inference &amp; Serving&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄ SwiftInfer&lt;/h1&gt; &#xA;&lt;h2&gt;üîó Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-swiftinfer&#34;&gt;üöÄ SwiftInfer&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-table-of-contents&#34;&gt;üîó Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-overview&#34;&gt;üìå Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-quick-start&#34;&gt;üöó Quick Start&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-installation&#34;&gt;üõ† Installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-run-llama-example&#34;&gt;üïπ Run Llama example&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-benchmark&#34;&gt;‚öñÔ∏è Benchmark&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-roadmap&#34;&gt;üó∫ Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-acknowledgement&#34;&gt;üìÉ Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#-citation&#34;&gt;üìù Citation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìå Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;&lt;strong&gt;Streaming-LLM&lt;/strong&gt;&lt;/a&gt; is a technique to support infinite input length for LLM inference. It leverages &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;&lt;strong&gt;Attention Sink&lt;/strong&gt;&lt;/a&gt; to prevent the model collapse when the attention window shifts. The original work is implemented in PyTorch, we offer &lt;strong&gt;SwiftInfer&lt;/strong&gt;, a TensorRT implementation to make StreamingLLM more production-grade. Our implementation was built upon the recently released &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;h2&gt;üöó Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;üõ† Installation&lt;/h3&gt; &#xA;&lt;p&gt;We use the API in &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;&lt;/a&gt; to construct the model and run inference. As the API of TensorRT-LLM is not stable and changing rapidly, we bind our implementation with the &lt;code&gt;42af740db51d6f11442fd5509ef745a4c043ce51&lt;/code&gt; commit whose version is &lt;code&gt;v0.6.0&lt;/code&gt;. We may upgrade this repository as TensorRT-LLM&#39;s APIs become more stable.&lt;/p&gt; &#xA;&lt;p&gt;If you have build &lt;strong&gt;TensorRT-LLM V0.6.0&lt;/strong&gt;, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hpcaitech/SwiftInfer.git&#xA;cd SwiftInfer&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise, you should install TensorRT-LLM first.&lt;/p&gt; &#xA;&lt;h4&gt;Install TensorRT-LLM with Docker&lt;/h4&gt; &#xA;&lt;p&gt;If using docker, you can follow &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/installation.md&#34;&gt;TensorRT-LLM Installation&lt;/a&gt; to install &lt;strong&gt;TensorRT-LLM V0.6.0&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By using docker, you can install SwiftInfer by simply running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hpcaitech/SwiftInfer.git&#xA;cd SwiftInfer&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install TensorRT-LLM without Docker&lt;/h4&gt; &#xA;&lt;p&gt;If not using docker, we provide a script to install TensorRT-LLM automatically.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please ensure that you have installed the following packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python&lt;/li&gt; &#xA; &lt;li&gt;build essentials, including gcc/g++, make, cmake&lt;/li&gt; &#xA; &lt;li&gt;CUDA toolkit&lt;/li&gt; &#xA; &lt;li&gt;cuDNN&lt;/li&gt; &#xA; &lt;li&gt;NCCL&lt;/li&gt; &#xA; &lt;li&gt;TensorRT&lt;/li&gt; &#xA; &lt;li&gt;PyTorch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure the version of TensorRT &amp;gt;= 9.1.0 and CUDA toolkit &amp;gt;= 12.2.&lt;/p&gt; &#xA;&lt;p&gt;To install tensorrt:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ARCH=$(uname -m)&#xA;if [ &#34;$ARCH&#34; = &#34;arm64&#34; ];then ARCH=&#34;aarch64&#34;;fi&#xA;if [ &#34;$ARCH&#34; = &#34;amd64&#34; ];then ARCH=&#34;x86_64&#34;;fi&#xA;if [ &#34;$ARCH&#34; = &#34;aarch64&#34; ];then OS=&#34;ubuntu-22.04&#34;; else OS=&#34;linux&#34;;fi&#xA;wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/secure/9.1.0/tars/tensorrt-9.1.0.4.$OS.$ARCH-gnu.cuda-12.2.tar.gz&#xA;tar xzvf tensorrt-9.1.0.4.linux.x86_64-gnu.cuda-12.2.tar.gz&#xA;PY_VERSION=$(python -c &#39;import sys; print(&#34;.&#34;.join(map(str, sys.version_info[0:2])))&#39;)&#xA;PARSED_PY_VERSION=$(echo &#34;${PY_VERSION//./}&#34;)&#xA;pip install TensorRT-9.1.0.4/python/tensorrt-*-cp${PARSED_PY_VERSION}-*.whl&#xA;export TRT_ROOT=$(realpath TensorRT-9.1.0.4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download nccl, follow &lt;a href=&#34;https://developer.nvidia.com/nccl/nccl-download&#34;&gt;NCCL download page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To download cudnn, follow &lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-download&#34;&gt;cuDNN download page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Commands&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before running the following commands, please ensure that you have set &lt;code&gt;nvcc&lt;/code&gt; correctly. To check it, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvcc --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install TensorRT-LLM and SwiftInfer, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hpcaitech/SwiftInfer.git&#xA;cd SwiftInfer&#xA;TRT_ROOT=xxx NCCL_ROOT=xxx CUDNN_ROOT=xxx pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üïπ Run Llama example&lt;/h3&gt; &#xA;&lt;p&gt;To run the Llama example, you need to first clone the Hugging Face repository for the &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;meta-llama/Llama-2-7b-chat-hf&lt;/a&gt; model or other Llama-based variants such as &lt;a href=&#34;https://huggingface.co/lmsys/vicuna-7b-v1.3&#34;&gt;lmsys/vicuna-7b-v1.3&lt;/a&gt;. Then, you can run the following command to build the TensorRT engine. &lt;strong&gt;You need to replace &lt;code&gt;&amp;lt;model-dir&amp;gt;&lt;/code&gt; with the actual path to the Llama model.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd examples/llama&#xA;&#xA;python build.py \&#xA;--model_dir &amp;lt;model-dir&amp;gt; \&#xA;--dtype float16 \&#xA;--enable_context_fmha \&#xA;--use_gemm_plugin float16 \&#xA;--max_input_len 2048 \&#xA;--max_output_len 1024 \&#xA;--output_dir ./output/7B-streaming-8k-1k-4-2000/trt_engines/fp16/1-gpu/ \&#xA;--max_batch_size 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, you need to download the &lt;a href=&#34;https://github.com/lm-sys/FastChat/raw/main/fastchat/llm_judge/README.md#mt-bench&#34;&gt;MT-Bench&lt;/a&gt; data provided by &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;LMSYS-FastChat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir mt_bench_data&#xA;wget -P ./mt_bench_data https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/llm_judge/data/mt_bench/question.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you are ready to run the Llama example with the following command.&lt;/p&gt; &#xA;&lt;p&gt;‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è &lt;strong&gt;Before that, please note that:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The &lt;code&gt;only_n_first&lt;/code&gt; argument is used to control the number of samples to be evaluated. If you want to evaluate all samples, please remove this argument.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ../run_conversation.py \&#xA;--max_input_length 2048 \&#xA;--max_output_len 1024 \&#xA;--tokenizer_dir &amp;lt;model-dir&amp;gt; \&#xA;--engine_dir ./output/7B-streaming-8k-1k-4-2000/trt_engines/fp16/1-gpu/ \&#xA;--input_file ./mt_bench_data/question.jsonl \&#xA;--streaming_llm_start_size 4 \&#xA;--only_n_first 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should expect to see the generation out as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/assets/inference-result.png&#34; alt=&#34;generation output&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚öñÔ∏è Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;We have benchmarked our implementations of Streaming-LLM with the &lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;original PyTorch version&lt;/a&gt;. The benchmark command for our implementation is given in the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/#%F0%9F%95%B9-run-llama-example&#34;&gt;Run Llama Example&lt;/a&gt; section while that for the original PyTorch implementation is given in the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/examples/torch_streamingllm/&#34;&gt;torch_streamingllm&lt;/a&gt; folder. The hardware used is listed below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU: Nvidia H800 (80GB)&lt;/li&gt; &#xA; &lt;li&gt;CPU: Intel(R) Xeon(R) Platinum 8468&lt;/li&gt; &#xA; &lt;li&gt;RAM: 2TB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The results (20 rounds of conversations) are:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/SwiftInfer/main/assets/performance.jpg&#34; alt=&#34;performance&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We are still working on further performance improvement and adapting to the TensorRT V0.7.1 APIs. We also notice that TensorRT-LLM has integrated StreamingLLM in their &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm&#34;&gt;example&lt;/a&gt; but it seems it is more suitable for single text generation instead of multi-round conversations.&lt;/p&gt; &#xA;&lt;h2&gt;üó∫ Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Streaming-LLM attention implementation based on TRT-LLM APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; KV cache adaptation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Early stop adaptation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Contiguous tensor fix&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Llama example for multi-round conversation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìÉ Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This work is inspired by Streaming-LLM to make it usable for production. Throughout development, we have referenced the following materials and we wish to acknowledge their efforts and contribution to the open-source community and academia.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streaming-LLM &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm/raw/main/assets/StreamingLLM.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TensorRT-LLM &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://nvidia.github.io/TensorRT-LLM/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìù Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find StreamingLLM and our TensorRT implementation useful, please kindly cite our repository and the original work proposed by &lt;a href=&#34;https://github.com/Guangxuan-Xiao&#34;&gt;Xiao et al.&lt;/a&gt; from &lt;a href=&#34;https://github.com/mit-han-lab&#34;&gt;MIT Han Lab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;# our repository&#xA;# NOTE: the listed authors have equal contribution&#xA;@misc{streamingllmtrt2023,&#xA;  title = {SwiftInfer},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/hpcaitech/SwiftInfer}},&#xA;}&#xA;&#xA;# Xiao&#39;s original paper&#xA;@article{xiao2023streamingllm,&#xA;        title={Efficient Streaming Language Models with Attention Sinks},&#xA;        author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},&#xA;        journal={arXiv},&#xA;        year={2023}&#xA;        }&#xA;&#xA;# TensorRT-LLM repo&#xA;# as TensorRT-LLM team does not provide a bibtex&#xA;# please let us know if there is any change needed&#xA;@misc{trtllm2023,&#xA;  title = {TensorRT-LLM},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/NVIDIA/TensorRT-LLM}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>plotly/plotly.py</title>
    <updated>2024-01-11T01:41:03Z</updated>
    <id>tag:github.com,2024-01-11:/plotly/plotly.py</id>
    <link href="https://github.com/plotly/plotly.py" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The interactive graphing library for Python ‚ú® This project now includes Plotly Express!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;plotly.py&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latest Release&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://pypi.org/project/plotly/&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://badge.fury.io/py/plotly.svg?sanitize=true&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;User forum&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://community.plotly.com/&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/help_forum-discourse-blue.svg?sanitize=true&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PyPI Downloads&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://pepy.tech/project/plotly&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://pepy.tech/badge/plotly/month&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;License&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install plotly==5.18.0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inside &lt;a href=&#34;https://jupyter.org/install&#34;&gt;Jupyter&lt;/a&gt; (installable with &lt;code&gt;pip install &#34;jupyterlab&amp;gt;=3&#34; &#34;ipywidgets&amp;gt;=7.6&#34;&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import plotly.express as px&#xA;fig = px.bar(x=[&#34;a&#34;, &#34;b&#34;, &#34;c&#34;], y=[1, 3, 2])&#xA;fig.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://plotly.com/python/&#34;&gt;Python documentation&lt;/a&gt; for more examples.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://plotly.com/python/&#34;&gt;plotly.py&lt;/a&gt; is an interactive, open-source, and browser-based graphing library for Python &lt;span&gt;‚ú®&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;Built on top of &lt;a href=&#34;https://github.com/plotly/plotly.js&#34;&gt;plotly.js&lt;/a&gt;, &lt;code&gt;plotly.py&lt;/code&gt; is a high-level, declarative charting library. plotly.js ships with over 30 chart types, including scientific charts, 3D graphs, statistical charts, SVG maps, financial charts, and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;plotly.py&lt;/code&gt; is &lt;a href=&#34;https://github.com/plotly/plotly.py/raw/master/LICENSE.txt&#34;&gt;MIT Licensed&lt;/a&gt;. Plotly graphs can be viewed in Jupyter notebooks, standalone HTML files, or integrated into &lt;a href=&#34;https://dash.plotly.com/&#34;&gt;Dash applications&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://plotly.com/consulting-and-oem/&#34;&gt;Contact us&lt;/a&gt; for consulting, dashboard development, application integration, and feature additions.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://plotly.com/python/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cldougl/plot_images/add_r_img/plotly_2017.png&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://plotly.com/python/&#34;&gt;Online Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plotly/plotly.py/raw/master/contributing.md&#34;&gt;Contributing to plotly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plotly/plotly.py/raw/master/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plotly/plotly.py/raw/master/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://plotly.com/python/v4-migration/&#34;&gt;Version 4 Migration Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/plotly/welcoming-dash-1-0-0-f3af4b84bae&#34;&gt;New! Announcing Dash 1.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://community.plotly.com&#34;&gt;Community forum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;plotly.py may be installed using pip...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install plotly==5.18.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or conda.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c plotly plotly=5.18.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;JupyterLab Support&lt;/h3&gt; &#xA;&lt;p&gt;For use in &lt;a href=&#34;https://jupyterlab.readthedocs.io/en/stable/&#34;&gt;JupyterLab&lt;/a&gt;, install the &lt;code&gt;jupyterlab&lt;/code&gt; and &lt;code&gt;ipywidgets&lt;/code&gt; packages using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;jupyterlab&amp;gt;=3&#34; &#34;ipywidgets&amp;gt;=7.6&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install &#34;jupyterlab&amp;gt;=3&#34; &#34;ipywidgets&amp;gt;=7.6&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The instructions above apply to JupyterLab 3.x. &lt;strong&gt;For JupyterLab 2 or earlier&lt;/strong&gt;, run the following commands to install the required JupyterLab extensions (note that this will require &lt;a href=&#34;https://nodejs.org/&#34;&gt;&lt;code&gt;node&lt;/code&gt;&lt;/a&gt; to be installed):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# JupyterLab 2.x renderer support&#xA;jupyter labextension install jupyterlab-plotly@5.18.0 @jupyter-widgets/jupyterlab-manager&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://plotly.com/python/troubleshooting/&#34;&gt;Troubleshooting guide&lt;/a&gt; if you run into any problems with JupyterLab.&lt;/p&gt; &#xA;&lt;h3&gt;Jupyter Notebook Support&lt;/h3&gt; &#xA;&lt;p&gt;For use in the Jupyter Notebook, install the &lt;code&gt;notebook&lt;/code&gt; and &lt;code&gt;ipywidgets&lt;/code&gt; packages using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;notebook&amp;gt;=5.3&#34; &#34;ipywidgets&amp;gt;=7.5&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install &#34;notebook&amp;gt;=5.3&#34; &#34;ipywidgets&amp;gt;=7.5&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Static Image Export&lt;/h3&gt; &#xA;&lt;p&gt;plotly.py supports &lt;a href=&#34;https://plotly.com/python/static-image-export/&#34;&gt;static image export&lt;/a&gt;, using either the &lt;a href=&#34;https://github.com/plotly/Kaleido&#34;&gt;&lt;code&gt;kaleido&lt;/code&gt;&lt;/a&gt; package (recommended, supported as of &lt;code&gt;plotly&lt;/code&gt; version 4.9) or the &lt;a href=&#34;https://github.com/plotly/orca&#34;&gt;orca&lt;/a&gt; command line utility (legacy as of &lt;code&gt;plotly&lt;/code&gt; version 4.9).&lt;/p&gt; &#xA;&lt;h4&gt;Kaleido&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/plotly/Kaleido&#34;&gt;&lt;code&gt;kaleido&lt;/code&gt;&lt;/a&gt; package has no dependencies and can be installed using pip...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U kaleido&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or conda.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c conda-forge python-kaleido&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Orca&lt;/h4&gt; &#xA;&lt;p&gt;While Kaleido is now the recommended image export approach because it is easier to install and more widely compatible, &lt;a href=&#34;https://plotly.com/python/static-image-export/&#34;&gt;static image export&lt;/a&gt; can also be supported by the legacy &lt;a href=&#34;https://github.com/plotly/orca&#34;&gt;orca&lt;/a&gt; command line utility and the &lt;a href=&#34;https://github.com/giampaolo/psutil&#34;&gt;&lt;code&gt;psutil&lt;/code&gt;&lt;/a&gt; Python package.&lt;/p&gt; &#xA;&lt;p&gt;These dependencies can both be installed using conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c plotly plotly-orca==1.3.1 psutil&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, &lt;code&gt;psutil&lt;/code&gt; can be installed using pip...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install psutil&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and orca can be installed according to the instructions in the &lt;a href=&#34;https://github.com/plotly/orca&#34;&gt;orca README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Extended Geo Support&lt;/h3&gt; &#xA;&lt;p&gt;Some plotly.py features rely on fairly large geographic shape files. The county choropleth figure factory is one such example. These shape files are distributed as a separate &lt;code&gt;plotly-geo&lt;/code&gt; package. This package can be installed using pip...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install plotly-geo==1.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or conda&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c plotly plotly-geo=1.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Migration&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re migrating from plotly.py v3 to v4, please check out the &lt;a href=&#34;https://plotly.com/python/v4-migration/&#34;&gt;Version 4 migration guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re migrating from plotly.py v2 to v3, please check out the &lt;a href=&#34;https://github.com/plotly/plotly.py/raw/master/migration-guide.md&#34;&gt;Version 3 migration guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Copyright and Licenses&lt;/h2&gt; &#xA;&lt;p&gt;Code and documentation copyright 2019 Plotly, Inc.&lt;/p&gt; &#xA;&lt;p&gt;Code released under the &lt;a href=&#34;https://github.com/plotly/plotly.py/raw/master/LICENSE.txt&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Docs released under the &lt;a href=&#34;https://github.com/plotly/documentation/raw/source/LICENSE&#34;&gt;Creative Commons license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kyegomez/MultiModalMamba</title>
    <updated>2024-01-11T01:41:03Z</updated>
    <id>tag:github.com,2024-01-11:/kyegomez/MultiModalMamba</id>
    <link href="https://github.com/kyegomez/MultiModalMamba" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A novel implementation of fusing ViT with Mamba into a fast, agile, and high performance Multi-Modal Model. Powered by Zeta, the simplest AI framework ever.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/MultiModalMamba/main/agorabanner.png&#34; alt=&#34;Multi-Modality&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Multi Modal Mamba - [MMM]&lt;/h1&gt; &#xA;&lt;p&gt;Multi Modal Mamba (MMM) is an all-new AI model that integrates Vision Transformer (ViT) and Mamba, creating a high-performance multi-modal model. MMM is built on Zeta, a minimalist yet powerful AI framework, designed to streamline and enhance machine learning model management.&lt;/p&gt; &#xA;&lt;p&gt;The capacity to process and interpret multiple data types concurrently is essential, the world isn&#39;t 1dimensional. MMM addresses this need by leveraging the capabilities of Vision Transformer and Mamba, enabling efficient handling of both text and image data. This makes MMM a versatile solution for a broad spectrum of AI tasks. MMM stands out for its significant speed and efficiency improvements over traditional transformer architectures, such as GPT-4 and LLAMA. This enhancement allows MMM to deliver high-quality results without sacrificing performance, making it an optimal choice for real-time data processing and complex AI algorithm execution. A key feature of MMM is its proficiency in processing extremely long sequences.&lt;/p&gt; &#xA;&lt;p&gt;This capability is particularly beneficial for tasks that involve substantial data volumes or necessitate a comprehensive understanding of context, such as natural language processing or image recognition. With MMM, you&#39;re not just adopting a state-of-the-art AI model. You&#39;re integrating a fast, efficient, and robust tool that is equipped to meet the demands of contemporary AI tasks. Experience the power and versatility of Multi Modal Mamba - MMM now!&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip3 install mmm-zeta&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;MultiModalMambaBlock&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the necessary libraries&#xA;import torch &#xA;from torch import nn&#xA;from mm_mamba import MultiModalMambaBlock&#xA;&#xA;# Create some random input tensors&#xA;x = torch.randn(1, 16, 64)  # Tensor with shape (batch_size, sequence_length, feature_dim)&#xA;y = torch.randn(1, 3, 64, 64)  # Tensor with shape (batch_size, num_channels, image_height, image_width)&#xA;&#xA;# Create an instance of the MultiModalMambaBlock model&#xA;model = MultiModalMambaBlock(&#xA;    dim = 64,  # Dimension of the token embeddings&#xA;    depth = 5,  # Number of transformer layers&#xA;    dropout = 0.1,  # Dropout probability&#xA;    heads = 4,  # Number of attention heads&#xA;    d_state = 16,  # Dimension of the state embeddings&#xA;    image_size = 64,  # Size of the input image&#xA;    patch_size = 16,  # Size of each image patch&#xA;    encoder_dim = 64,  # Dimension of the encoder token embeddings&#xA;    encoder_depth = 5,  # Number of encoder transformer layers&#xA;    encoder_heads = 4,  # Number of encoder attention heads&#xA;    fusion_method=&#34;mlp&#34;,&#xA;)&#xA;&#xA;# Pass the input tensors through the model&#xA;out = model(x, y)&#xA;&#xA;# Print the shape of the output tensor&#xA;print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;MMM&lt;/code&gt;, Ready to Train Model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Flexibility in Data Types: The MMM model can handle both text and image data simultaneously. This allows it to be trained on a wider variety of datasets and tasks, including those that require understanding of both text and image data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Customizable Architecture: The MMM model has numerous parameters such as depth, dropout, heads, d_state, image_size, patch_size, encoder_dim, encoder_depth, encoder_heads, and fusion_method. These parameters can be tuned according to the specific requirements of the task at hand, allowing for a high degree of customization in the model architecture.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Option to Return Embeddings: The MMM model has a return_embeddings option. When set to True, the model will return the embeddings instead of the final output. This can be useful for tasks that require access to the intermediate representations learned by the model, such as transfer learning or feature extraction tasks.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch  # Import the torch library&#xA;&#xA;# Import the MMM model from the mm_mamba module&#xA;from mm_mamba import MMM&#xA;&#xA;# Generate a random tensor &#39;x&#39; of size (1, 224) with random elements between 0 and 10000&#xA;x = torch.randint(0, 10000, (1, 196))&#xA;&#xA;# Generate a random image tensor &#39;img&#39; of size (1, 3, 224, 224)&#xA;img = torch.randn(1, 3, 224, 224)&#xA;&#xA;# Create a MMM model object with the following parameters:&#xA;model = MMM(&#xA;    vocab_size=10000,&#xA;    dim=512,&#xA;    depth=6,&#xA;    dropout=0.1,&#xA;    heads=8,&#xA;    d_state=512,&#xA;    image_size=224,&#xA;    patch_size=16,&#xA;    encoder_dim=512,&#xA;    encoder_depth=6,&#xA;    encoder_heads=8,&#xA;    fusion_method=&#34;mlp&#34;,&#xA;    return_embeddings=False,&#xA;    post_fuse_norm=True,&#xA;)&#xA;&#xA;# Pass the tensor &#39;x&#39; and &#39;img&#39; through the model and store the output in &#39;out&#39;&#xA;out = model(x, img)&#xA;&#xA;# Print the shape of the output tensor &#39;out&#39;&#xA;print(out.shape)&#xA;&#xA;&#xA;# After much training&#xA;model.eval()&#xA;&#xA;# Tokenize texts&#xA;text_tokens = tokenize(text)&#xA;&#xA;# Send text tokens to the model&#xA;logits = model(text_tokens)&#xA;&#xA;text = detokenize(logits)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Real-World Deployment&lt;/h1&gt; &#xA;&lt;p&gt;Are you an enterprise looking to leverage the power of AI? Do you want to integrate state-of-the-art models into your workflow? Look no further!&lt;/p&gt; &#xA;&lt;p&gt;Multi Modal Mamba (MMM) is a cutting-edge AI model that fuses Vision Transformer (ViT) with Mamba, providing a fast, agile, and high-performance solution for your multi-modal needs.&lt;/p&gt; &#xA;&lt;p&gt;But that&#39;s not all! With Zeta, our simple yet powerful AI framework, you can easily customize and fine-tune MMM to perfectly fit your unique quality standards.&lt;/p&gt; &#xA;&lt;p&gt;Whether you&#39;re dealing with text, images, or both, MMM has got you covered. With its deep configuration and multiple fusion layers, you can handle complex AI tasks with ease and efficiency.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;span&gt;üåü&lt;/span&gt; Why Choose Multi Modal Mamba?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versatile&lt;/strong&gt;: Handle both text and image data with a single model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Powerful&lt;/strong&gt;: Leverage the power of Vision Transformer and Mamba.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Fine-tune the model to your specific needs with Zeta.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient&lt;/strong&gt;: Achieve high performance without compromising on speed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Don&#39;t let the complexities of AI slow you down. Choose Multi Modal Mamba and stay ahead of the curve!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://calendly.com/swarm-corp/30min&#34;&gt;Contact us here&lt;/a&gt; today to learn how you can integrate Multi Modal Mamba into your workflow and supercharge your AI capabilities!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>