<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-28T01:38:13Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>spcl/graph-of-thoughts</title>
    <updated>2023-08-28T01:38:13Z</updated>
    <id>tag:github.com,2023-08-28:/spcl/graph-of-thoughts</id>
    <link href="https://github.com/spcl/graph-of-thoughts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation of &#34;Graph of Thoughts: Solving Elaborate Problems with Large Language Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Graph of Thoughts (GoT)&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/paper/pics/preview.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of &lt;a href=&#34;https://arxiv.org/pdf/2308.09687.pdf&#34;&gt;Graph of Thoughts: Solving Elaborate Problems with Large Language Models&lt;/a&gt;.&lt;br&gt; This framework gives you the ability to solve complex problems by modeling them as a Graph of Operations (GoO), which is automatically executed with a Large Language Model (LLM) as the engine.&lt;br&gt; This framework is designed to be flexible and extensible, allowing you to not only solve problems using the new GoT approach, but also to implement GoOs resembling previous approaches like CoT or ToT.&lt;/p&gt; &#xA;&lt;h2&gt;Setup Guide&lt;/h2&gt; &#xA;&lt;p&gt;In order to use this framework, you need to have a working installation of Python 3.8 or newer.&lt;/p&gt; &#xA;&lt;h3&gt;Installing GoT&lt;/h3&gt; &#xA;&lt;p&gt;Before running either of the following two installation methods, make sure to activate your Python environment (if any) beforehand.&lt;br&gt; If you are a user and you just want to use &lt;code&gt;graph_of_thoughts&lt;/code&gt;, you can install it directly from PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install graph_of_thoughts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are a developer and you want to modify the code, you can install it in editable mode from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/spcl/graph-of-thoughts.git&#xA;cd graph-of-thoughts&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuring the LLM&lt;/h3&gt; &#xA;&lt;p&gt;In order to use the framework, you need to have access to an LLM. Please follow the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/graph_of_thoughts/controller/README.md&#34;&gt;Controller README&lt;/a&gt; to configure the LLM of your choice.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The following code snippet shows how to use the framework to solve the sorting problem for a list of 32 numbers using a CoT-like approach.&lt;br&gt; Make sure you have followed the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/#setup-guide&#34;&gt;Setup Guide&lt;/a&gt; before running the code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from examples.sorting.sorting_032 import SortingPrompter, SortingParser, utils&#xA;from graph_of_thoughts import controller, operations&#xA;&#xA;# Problem input&#xA;&#xA;to_be_sorted = &#34;[0, 2, 6, 3, 8, 7, 1, 1, 6, 7, 7, 7, 7, 9, 3, 0, 1, 7, 9, 1, 3, 5, 1, 3, 6, 4, 5, 4, 7, 3, 5, 7]&#34;&#xA;&#xA;# Create the Graph of Operations&#xA;gop = operations.GraphOfOperations()&#xA;gop.append_operation(operations.Generate())&#xA;gop.append_operation(operations.Score(scoring_function=utils.num_errors))&#xA;gop.append_operation(operations.GroundTruth(utils.test_sorting))&#xA;&#xA;# Configure the Language Model (Assumes config.json is in the current directory with OpenAI API key)&#xA;lm = controller.ChatGPT(&#34;config.json&#34;, model_name=&#34;chatgpt&#34;)&#xA;&#xA;# Create the Controller&#xA;ctrl = controller.Controller(&#xA;  lm, &#xA;  gop, &#xA;  SortingPrompter(), &#xA;  SortingParser(),&#xA;  # The following dictionary is used to configure the initial thought state&#xA;  {&#xA;    &#34;original&#34;: to_be_sorted,&#xA;    &#34;current&#34;: &#34;&#34;,&#xA;    &#34;method&#34;: &#34;cot&#34;&#xA;  }&#xA;)&#xA;&#xA;# Run the Controller and generate the output graph&#xA;ctrl.run()&#xA;ctrl.output_graph(&#34;output_cot.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the more sophisticated GoT approach, you can use the following code snippet.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from examples.sorting.sorting_032 import SortingPrompter, SortingParser, got, utils&#xA;from graph_of_thoughts import controller, operations&#xA;&#xA;# Problem input&#xA;&#xA;to_be_sorted = &#34;[0, 2, 6, 3, 8, 7, 1, 1, 6, 7, 7, 7, 7, 9, 3, 0, 1, 7, 9, 1, 3, 5, 1, 3, 6, 4, 5, 4, 7, 3, 5, 7]&#34;&#xA;&#xA;# Retrieve the Graph of Operations&#xA;gop = got()&#xA;&#xA;# Configure the Language Model (Assumes config.json is in the current directory with OpenAI API key)&#xA;lm = controller.ChatGPT(&#34;config.json&#34;, model_name=&#34;chatgpt&#34;)&#xA;&#xA;# Create the Controller&#xA;ctrl = controller.Controller(&#xA;  lm, &#xA;  gop, &#xA;  SortingPrompter(), &#xA;  SortingParser(),&#xA;  # The following dictionary is used to configure the initial thought state&#xA;  {&#xA;    &#34;original&#34;: to_be_sorted,&#xA;    &#34;current&#34;: &#34;&#34;,&#xA;    &#34;phase&#34;: 0,&#xA;    &#34;method&#34;: &#34;got&#34;&#xA;  }&#xA;)&#xA;&#xA;# Run the Controller and generate the output graph&#xA;ctrl.run()&#xA;ctrl.output_graph(&#34;output_got.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can compare the two results by inspecting the output graphs &lt;code&gt;output_cot.json&lt;/code&gt; and &lt;code&gt;output_got.json&lt;/code&gt;.&lt;br&gt; The final thought states&#39; scores indicate the number of errors in the sorted list.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The paper gives a high-level overview of the framework and its components.&lt;br&gt; In order to understand the framework in more detail, you can read the documentation of the individual modules.&lt;br&gt; Especially the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/graph_of_thoughts/controller/README.md&#34;&gt;Controller&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/graph_of_thoughts/operations/README.md&#34;&gt;Operations&lt;/a&gt; modules are important for understanding how to make the most out of the framework.&lt;br&gt; We took extra care to fully document the code, so that you can easily understand how it works and how to extend it.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/examples&#34;&gt;examples&lt;/a&gt; directory contains several examples of problems that can be solved using the framework, including the ones presented in the paper.&lt;br&gt; It is a great starting point for learning how to use the framework to solve real problems.&lt;br&gt; Each example contains a &lt;code&gt;README.md&lt;/code&gt; file with instructions on how to run it and play with it. The code is fully documented and should be easy to follow.&lt;/p&gt; &#xA;&lt;h2&gt;Paper Results&lt;/h2&gt; &#xA;&lt;p&gt;You can run the experiments from the paper by following the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;br&gt; However, if you just want to inspect and replot the results, you can use the &lt;a href=&#34;https://raw.githubusercontent.com/spcl/graph-of-thoughts/main/paper&#34;&gt;paper&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository valuable, please give it a star!&lt;br&gt; Got any questions or feedback? Feel free to reach out to &lt;a href=&#34;mailto:nils.blach@inf.ethz.ch&#34;&gt;nils.blach@inf.ethz.ch&lt;/a&gt; or open an issue.&lt;br&gt; Using this in your work? Please reference us using the provided citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{besta2023got,&#xA;  title = {{Graph of Thoughts: Solving Elaborate Problems with Large Language Models}},&#xA;  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Micha{\l} and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},&#xA;  year = 2023,&#xA;  eprinttype = {arXiv},&#xA;  eprint = {2308.09687}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Plachtaa/VALL-E-X</title>
    <updated>2023-08-28T01:38:13Z</updated>
    <id>tag:github.com,2023-08-28:/Plachtaa/VALL-E-X</id>
    <link href="https://github.com/Plachtaa/VALL-E-X" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source implementation of Microsoft&#39;s VALL-E X zero-shot TTS model. Demo is available in https://plachtaa.github.io&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VALL-E X: Multilingual Text-to-Speech Synthesis and Voice Cloning 🔊&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qCBRmAnTxg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;br&gt; English | &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/README-ZH.md&#34;&gt;中文&lt;/a&gt; &lt;br&gt; An open source implementation of Microsoft&#39;s &lt;a href=&#34;https://arxiv.org/pdf/2303.03926&#34;&gt;VALL-E X&lt;/a&gt; zero-shot TTS model.&lt;br&gt; &lt;strong&gt;We release our trained model to the public for research or application usage.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/images/vallex_framework.jpg&#34; alt=&#34;vallex-framework&#34; title=&#34;VALL-E X framework&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;VALL-E X is an amazing multilingual text-to-speech (TTS) model proposed by Microsoft. While Microsoft initially publish in their research paper, they did not release any code or pretrained models. Recognizing the potential and value of this technology, our team took on the challenge to reproduce the results and train our own model. We are glad to share our trained VALL-E X model with the community, allowing everyone to experience the power next-generation TTS! 🎧 &lt;br&gt; &lt;br&gt; More details about the model are presented in &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/model-card.md&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📖 Quick Index&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-updates&#34;&gt;🚀 Updates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-features&#34;&gt;📢 Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-installation&#34;&gt;💻 Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-demos&#34;&gt;🎧 Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-usage-in-python&#34;&gt;🐍 Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-faq&#34;&gt;❓ FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-todo&#34;&gt;🧠 TODO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.08.23&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added long text generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.08.20&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/README-ZH.md&#34;&gt;Chinese README&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.08.14&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretrained VALL-E X checkpoint is now released. Download it &lt;a href=&#34;https://drive.google.com/file/d/10gdQWvP-K_e1undkvv0p2b7SU6I4Egyl/view?usp=sharing&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💻 Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install with pip, recommended with Python 3.10, CUDA 11.7 ~ 12.0, PyTorch 2.0+&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;git clone https://github.com/Plachtaa/VALL-E-X.git&#xA;cd VALL-E-X&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: If you want to make prompt, you need to install ffmpeg and add its folder to the environment variable PATH.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🎧 Demos&lt;/h2&gt; &#xA;&lt;p&gt;Not ready to set up the environment on your local machine just yet? No problem! We&#39;ve got you covered with our online demos. You can try out VALL-E X directly on Hugging Face or Google Colab, experiencing the model&#39;s capabilities hassle-free! &lt;br&gt; &lt;a href=&#34;https://huggingface.co/spaces/Plachta/VALL-E-X&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📢 Features&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X comes packed with cutting-edge functionalities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual TTS&lt;/strong&gt;: Speak in three languages - English, Chinese, and Japanese - with natural and expressive speech synthesis.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot Voice Cloning&lt;/strong&gt;: Enroll a short 3~10 seconds recording of an unseen speaker, and watch VALL-E X create personalized, high-quality speech that sounds just like them!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/a7baa51d-a53a-41cc-a03d-6970f25fcca7&#34;&gt;prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/b895601a-d126-4138-beff-061aabdc7985&#34;&gt;output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speech Emotion Control&lt;/strong&gt;: Experience the power of emotions! VALL-E X can synthesize speech with the same emotion as the acoustic prompt provided, adding an extra layer of expressiveness to your audio.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266&#34;&gt;https://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1&#34;&gt;https://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-shot Cross-Lingual Speech Synthesis&lt;/strong&gt;: Take monolingual speakers on a linguistic journey! VALL-E X can produce personalized speech in another language without compromising on fluency or accent. Below is a Japanese speaker talk in Chinese &amp;amp; English. 🇯🇵 🗣&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/ea6e2ee4-139a-41b4-837e-0bd04dda6e19&#34;&gt;jp-prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/db8f9782-923f-425e-ba94-e8c1bd48f207&#34;&gt;en-output.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/15829d79-e448-44d3-8965-fafa7a3f8c28&#34;&gt;zh-output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accent Control&lt;/strong&gt;: Get creative with accents! VALL-E X allows you to experiment with different accents, like speaking Chinese with an English accent or vice versa. 🇨🇳 💬&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/f688d7f6-70ef-46ec-b1cc-355c31e78b3b&#34;&gt;en-prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/be59c7ca-b45b-44ca-a30d-4d800c950ccc&#34;&gt;zh-accent-output.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/8b4f4f9b-f299-4ea4-a548-137437b71738&#34;&gt;en-accent-output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Acoustic Environment Maintenance&lt;/strong&gt;: No need for perfectly clean audio prompts! VALL-E X adapts to the acoustic environment of the input, making speech generation feel natural and immersive.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/68986d88-abd0-4d1d-96e4-4f893eb9259e&#34;&gt;noise-prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/96c4c612-4516-4683-8804-501b70938608&#34;&gt;noise-output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Explore our &lt;a href=&#34;https://plachtaa.github.io/&#34;&gt;demo page&lt;/a&gt; for a lot more examples!&lt;/p&gt; &#xA;&lt;h2&gt;🐍 Usage in Python&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;🪑 Basics&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from utils.generation import SAMPLE_RATE, generate_audio, preload_models&#xA;from scipy.io.wavfile import write as write_wav&#xA;from IPython.display import Audio&#xA;&#xA;# download and load all models&#xA;preload_models()&#xA;&#xA;# generate audio from text&#xA;text_prompt = &#34;&#34;&#34;&#xA;Hello, my name is Nose. And uh, and I like hamburger. Hahaha... But I also have other interests such as playing tactic toast.&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt)&#xA;&#xA;# save audio to disk&#xA;write_wav(&#34;vallex_generation.wav&#34;, SAMPLE_RATE, audio_array)&#xA;&#xA;# play text in notebook&#xA;Audio(audio_array, rate=SAMPLE_RATE)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/578d7bbe-cda9-483e-898c-29646edc8f2e&#34;&gt;hamburger.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;🌎 Foreign Language&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;br&gt; This VALL-E X implementation also supports Chinese and Japanese. All three languages have equally awesome performance! &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;text_prompt = &#34;&#34;&#34;&#xA;    チュソクは私のお気に入りの祭りです。 私は数日間休んで、友人や家族との時間を過ごすことができます。&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/ee57a688-3e83-4be5-b0fe-019d16eec51c&#34;&gt;vallex_japanese.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Note: VALL-E X controls accent perfectly even when synthesizing code-switch text. However, you need to manually denote language of respective sentences (since our g2p tool is rule-base)&lt;/em&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_prompt = &#34;&#34;&#34;&#xA;    [EN]The Thirty Years&#39; War was a devastating conflict that had a profound impact on Europe.[EN]&#xA;    [ZH]这是历史的开始。 如果您想听更多，请继续。[ZH]&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt, language=&#39;mix&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/d8667abf-bd08-499f-a383-a861d852f98a&#34;&gt;vallex_codeswitch.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;📼 Voice Presets&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;VALL-E X provides tens of speaker voices which you can directly used for inference! Browse all voices in the &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/presets&#34;&gt;code&lt;/a&gt;&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;VALL-E X tries to match the tone, pitch, emotion and prosody of a given preset. The model also attempts to preserve music, ambient noise, etc.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_prompt = &#34;&#34;&#34;&#xA;I am an innocent boy with a smoky voice. It is a great honor for me to speak at the United Nations today.&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt, prompt=&#34;dingzhen&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/d3f55732-b1cd-420f-87d6-eab60db14dc5&#34;&gt;smoky.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;🎙Voice Cloning&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;VALL-E X supports voice cloning! You can make a voice prompt with any person, character or even your own voice, and use it like other voice presets.&lt;br&gt; To make a voice prompt, you need to provide a speech of 3~10 seconds long, as well as the transcript of the speech. You can also leave the transcript blank to let the &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; model to generate the transcript.&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;VALL-E X tries to match the tone, pitch, emotion and prosody of a given prompt. The model also attempts to preserve music, ambient noise, etc.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from utils.prompt_making import make_prompt&#xA;&#xA;### Use given transcript&#xA;make_prompt(name=&#34;paimon&#34;, audio_prompt_path=&#34;paimon_prompt.wav&#34;,&#xA;                transcript=&#34;Just, what was that? Paimon thought we were gonna get eaten.&#34;)&#xA;&#xA;### Alternatively, use whisper&#xA;make_prompt(name=&#34;paimon&#34;, audio_prompt_path=&#34;paimon_prompt.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Now let&#39;s try out the prompt we&#39;ve just made!&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_prompt = &#34;&#34;&#34;&#xA;Hey, Traveler, Listen to this, This machine has taken my voice, and now it can talk just like me!&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt, prompt=&#34;paimon&#34;)&#xA;&#xA;write_wav(&#34;paimon_cloned.wav&#34;, SAMPLE_RATE, audio_array)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/e7922859-9d12-4e2a-8651-e156e4280311&#34;&gt;paimon_prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/60d3b7e9-5ead-4024-b499-a897ce5f3d5e&#34;&gt;paimon_cloned.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;🎢User Interface&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Not comfortable with codes? No problem! We&#39;ve also created a user-friendly graphical interface for VALL-E X. It allows you to interact with the model effortlessly, making voice cloning and multilingual speech synthesis a breeze. &lt;br&gt; You can launch the UI by the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python launch-ui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🛠️ Hardware and Inference Speed&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X works well on both CPU and GPU (&lt;code&gt;pytorch 2.0+&lt;/code&gt;, CUDA 11.7 and CUDA 12.0).&lt;/p&gt; &#xA;&lt;p&gt;A GPU VRAM of 6GB is enough for running VALL-E X without offloading.&lt;/p&gt; &#xA;&lt;h2&gt;⚙️ Details&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X is similar to &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;Bark&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;VALL-E&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2209.03143&#34;&gt;AudioLM&lt;/a&gt;, which generates audio in GPT-style by predicting audio tokens quantized by &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec&lt;/a&gt;. &lt;br&gt; Comparing to &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;Bark&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;✔ &lt;strong&gt;Light-weighted&lt;/strong&gt;: 3️⃣ ✖ smaller,&lt;/li&gt; &#xA; &lt;li&gt;✔ &lt;strong&gt;Efficient&lt;/strong&gt;: 4️⃣ ✖ faster,&lt;/li&gt; &#xA; &lt;li&gt;✔ &lt;strong&gt;Better quality on Chinese &amp;amp; Japanese&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;✔ &lt;strong&gt;Cross-lingual speech without foreign accent&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;✔ &lt;strong&gt;Easy voice-cloning&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;❌ &lt;strong&gt;Less languages&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;❌ &lt;strong&gt;No special tokens for music / sound effects&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported Languages&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English (en)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese (ja)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese, simplified (zh)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;❓ FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Where can I download the model checkpoint?&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use &lt;code&gt;gdown&lt;/code&gt; to download the model to directory &lt;code&gt;./checkpoints/&lt;/code&gt; when you run &lt;code&gt;preload_models()&lt;/code&gt; for the first time.&lt;/li&gt; &#xA; &lt;li&gt;If you cannot access Google, please manually download from &lt;a href=&#34;https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt&#34;&gt;this link&lt;/a&gt;, and put the file under directory &lt;code&gt;./checkpoints/&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;How much VRAM do I need?&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;6GB GPU VRAM - Almost all NVIDIA GPUs satisfy the requirement.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Why the model fails to generate long text?&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transformer&#39;s computation complexity increases quadratically while the sequence length increases. Hence, all training are kept under 22 seconds. Please make sure the total length of audio prompt and generated audio is less than 22 seconds to ensure acceptable performance.&lt;/li&gt; &#xA; &lt;li&gt;To generate long text, a huge paragraph must be breakdown into short sentences. We are currently working on this.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;MORE TO BE ADDED...&lt;/h4&gt; &#xA;&lt;h2&gt;🧠 TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Chinese README&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;code&gt;.bat&lt;/code&gt; scripts for non-python users&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Long text generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-tuning for better voice adaptation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Replace Encodec decoder with Vocos decoder&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; To be added...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🙏 Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.03926&#34;&gt;VALL-E X paper&lt;/a&gt; for the brilliant idea&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;lifeiteng&#39;s vall-e&lt;/a&gt; for related training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;bark&lt;/a&gt; for the amazing pioneering work in neuro-codec TTS model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⭐️ Show Your Support&lt;/h2&gt; &#xA;&lt;p&gt;If you find VALL-E X interesting and useful, give us a star on GitHub! ⭐️ It encourages us to keep improving the model and adding exciting features.&lt;/p&gt; &#xA;&lt;h2&gt;📜 License&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Have questions or need assistance? Feel free to &lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/issues/new&#34;&gt;open an issue&lt;/a&gt; or join our &lt;a href=&#34;https://discord.gg/qCBRmAnTxg&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Happy voice cloning! 🎤&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen-VL</title>
    <updated>2023-08-28T01:38:13Z</updated>
    <id>tag:github.com,2023-08-28:/QwenLM/Qwen-VL</id>
    <link href="https://github.com/QwenLM/Qwen-VL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repo of Qwen-VL (通义千问-VL) chat &amp; pretrained large vision language model proposed by Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/logo.jpg&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; Qwen-VL &lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-VL/summary&#34;&gt;🤖 &lt;/a&gt;&lt;a&gt; | &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL&#34;&gt;🤗&lt;/a&gt;&amp;nbsp; ｜ Qwen-VL-Chat &lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary&#34;&gt;🤖 &lt;/a&gt;&lt;a&gt;| &lt;/a&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL-Chat&#34;&gt;🤗&lt;/a&gt;&amp;nbsp; ｜ &amp;nbsp;&lt;a href=&#34;https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary&#34;&gt;Demo&lt;/a&gt;&amp;nbsp; ｜ &amp;nbsp;&lt;a href=&#34;https://arxiv.org/pdf/2308.12966.pdf&#34;&gt;Report&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/README_CN.md&#34;&gt;中文&lt;/a&gt;&amp;nbsp; ｜ &amp;nbsp; English ｜ &amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/README_JA.md&#34;&gt;日本語&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Qwen-VL&lt;/strong&gt; (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text and bounding box. The features of Qwen-VL include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt;: It significantly surpasses existing open-sourced Large Vision Language Models (LVLM) under similar model scale on multiple English evaluation benchmarks (including Zero-shot Captioning, VQA, DocVQA, and Grounding).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-lingual LVLM supporting text recognition&lt;/strong&gt;: Qwen-VL naturally supports English, Chinese, and multi-lingual conversation, and it promotes end-to-end recognition of Chinese and English bi-lingual text in images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-image interleaved conversations&lt;/strong&gt;: This feature allows for the input and comparison of multiple images, as well as the ability to specify questions related to the images and engage in multi-image storytelling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;First generalist model supporting grounding in Chinese&lt;/strong&gt;: Detecting bounding boxes through open-domain language expression in both Chinese and English.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fine-grained recognition and understanding&lt;/strong&gt;: Compared to the 224*224 resolution currently used by other open-sourced LVLM, the 448*448 resolution promotes fine-grained text recognition, document QA, and bounding box annotation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_vl.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;We release two models of the Qwen-VL series:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Qwen-VL: The pre-trained LVLM model uses Qwen-7B as the initialization of the LLM, and &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;Openclip ViT-bigG&lt;/a&gt; as the initialization of the visual encoder. And connects them with a randomly initialized cross-attention layer.&lt;/li&gt; &#xA; &lt;li&gt;Qwen-VL-Chat: A multimodal LLM-based AI assistant, which is trained with alignment techniques. Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We evaluated the model&#39;s abilities from two perspectives:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Standard Benchmarks&lt;/strong&gt;: We evaluate the model&#39;s basic task capabilities on four major categories of multimodal tasks:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Zero-shot Captioning: Evaluate model&#39;s zero-shot image captioning ability on unseen datasets;&lt;/li&gt; &#xA;   &lt;li&gt;General VQA: Evaluate the general question-answering ability of pictures, such as the judgment, color, number, category, etc;&lt;/li&gt; &#xA;   &lt;li&gt;Text-based VQA: Evaluate the model&#39;s ability to recognize text in pictures, such as document QA, chart QA, etc;&lt;/li&gt; &#xA;   &lt;li&gt;Referring Expression Comprehension: Evaluate the ability to localize a target object in an image described by a referring expression.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TouchStone&lt;/strong&gt;: To evaluate the overall text-image dialogue capability and alignment level with humans, we have constructed a benchmark called TouchStone, which is based on scoring with GPT4 to evaluate the LVLM model.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The TouchStone benchmark covers a total of 300+ images, 800+ questions, and 27 categories. Such as attribute-based Q&amp;amp;A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc;&lt;/li&gt; &#xA;   &lt;li&gt;In order to break the current limitation of GPT4 in terms of direct image input, TouchStone provides fine-grained image annotations by human labeling. These detailed annotations, along with the questions and the model&#39;s output, are then presented to GPT4 for scoring.&lt;/li&gt; &#xA;   &lt;li&gt;The benchmark includes both English and Chinese versions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The results of the evaluation are as follows:&lt;/p&gt; &#xA;&lt;p&gt;Qwen-VL outperforms current SOTA generalist models on multiple VL tasks and has a more comprehensive coverage in terms of capability range.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/radar.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Zero-shot Captioning &amp;amp; General VQA&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model type&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Zero-shot Captioning&lt;/th&gt; &#xA;   &lt;th colspan=&#34;5&#34;&gt;General VQA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;NoCaps&lt;/th&gt; &#xA;   &lt;th&gt;Flickr30K&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;sup&gt;dev&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;OK-VQA&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;SciQA-Img&lt;br&gt;(0-shot)&lt;/th&gt; &#xA;   &lt;th&gt;VizWiz&lt;br&gt;(0-shot)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody align=&#34;center&#34;&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;10&#34;&gt;Generalist&lt;br&gt;Models&lt;/td&gt; &#xA;   &lt;td&gt;Flamingo-9B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;51.8&lt;/td&gt; &#xA;   &lt;td&gt;44.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;28.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flamingo-80B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;67.2&lt;/td&gt; &#xA;   &lt;td&gt;56.3&lt;/td&gt; &#xA;   &lt;td&gt;50.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;31.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unified-IO-XL&lt;/td&gt; &#xA;   &lt;td&gt;100.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;77.9&lt;/td&gt; &#xA;   &lt;td&gt;54.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kosmos-1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;67.1&lt;/td&gt; &#xA;   &lt;td&gt;51.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;29.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kosmos-2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;80.5&lt;/td&gt; &#xA;   &lt;td&gt;51.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLIP-2 (Vicuna-13B)&lt;/td&gt; &#xA;   &lt;td&gt;103.9&lt;/td&gt; &#xA;   &lt;td&gt;71.6&lt;/td&gt; &#xA;   &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;td&gt;45.9&lt;/td&gt; &#xA;   &lt;td&gt;32.3&lt;/td&gt; &#xA;   &lt;td&gt;61.0&lt;/td&gt; &#xA;   &lt;td&gt;19.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InstructBLIP (Vicuna-13B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;121.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;82.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;49.5&lt;/td&gt; &#xA;   &lt;td&gt;63.1&lt;/td&gt; &#xA;   &lt;td&gt;33.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shikra (Vicuna-13B)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;73.9&lt;/td&gt; &#xA;   &lt;td&gt;77.36&lt;/td&gt; &#xA;   &lt;td&gt;47.16&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Qwen-VL (Qwen-7B)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;121.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;85.8&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;78.8&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;58.6&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;59.3&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;67.1&lt;/td&gt; &#xA;   &lt;td&gt;35.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- &lt;tr&gt;&#xA;    &lt;td&gt;Qwen-VL (4-shot)&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;63.6&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;39.1&lt;/td&gt;&#xA;  &lt;/tr&gt; --&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;   &lt;td&gt;120.2&lt;/td&gt; &#xA;   &lt;td&gt;81.0&lt;/td&gt; &#xA;   &lt;td&gt;78.2&lt;/td&gt; &#xA;   &lt;td&gt;56.6&lt;/td&gt; &#xA;   &lt;td&gt;57.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;68.2&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;38.9&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- &lt;tr&gt;&#xA;    &lt;td&gt;Qwen-VL-Chat (4-shot)&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;60.6&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;-&lt;/td&gt;&#xA;    &lt;td&gt;44.45&lt;/td&gt;&#xA;  &lt;/tr&gt; --&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Previous SOTA&lt;br&gt;(Per Task Fine-tuning)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;127.0&lt;br&gt;(PALI-17B)&lt;/td&gt; &#xA;   &lt;td&gt;84.5&lt;br&gt;(InstructBLIP&lt;br&gt;-FlanT5-XL)&lt;/td&gt; &#xA;   &lt;td&gt;86.1&lt;br&gt;(PALI-X&lt;br&gt;-55B)&lt;/td&gt; &#xA;   &lt;td&gt;66.1&lt;br&gt;(PALI-X&lt;br&gt;-55B)&lt;/td&gt; &#xA;   &lt;td&gt;72.1&lt;br&gt;(CFR)&lt;/td&gt; &#xA;   &lt;td&gt;92.53&lt;br&gt;(LLaVa+&lt;br&gt;GPT-4)&lt;/td&gt; &#xA;   &lt;td&gt;70.9&lt;br&gt;(PALI-X&lt;br&gt;-55B)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For zero-shot image captioning, Qwen-VL achieves the &lt;strong&gt;SOTA&lt;/strong&gt; on Flickr30K and competitive results on Nocaps with InstructBlip.&lt;/li&gt; &#xA; &lt;li&gt;For general VQA, Qwen-VL achieves the &lt;strong&gt;SOTA&lt;/strong&gt; under the same generalist LVLM scale settings.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Text-oriented VQA (Focused on text understanding capabilities in images)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model type&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;TextVQA&lt;/th&gt; &#xA;   &lt;th&gt;DocVQA&lt;/th&gt; &#xA;   &lt;th&gt;ChartQA&lt;/th&gt; &#xA;   &lt;th&gt;AI2D&lt;/th&gt; &#xA;   &lt;th&gt;OCR-VQA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody align=&#34;center&#34;&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Generalist Models&lt;/td&gt; &#xA;   &lt;td&gt;BLIP-2 (Vicuna-13B)&lt;/td&gt; &#xA;   &lt;td&gt;42.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InstructBLIP (Vicuna-13B)&lt;/td&gt; &#xA;   &lt;td&gt;50.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mPLUG-DocOwl (LLaMA-7B)&lt;/td&gt; &#xA;   &lt;td&gt;52.6&lt;/td&gt; &#xA;   &lt;td&gt;62.2&lt;/td&gt; &#xA;   &lt;td&gt;57.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pix2Struct-Large (1.3B)&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;76.6&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;58.6&lt;/td&gt; &#xA;   &lt;td&gt;42.1&lt;/td&gt; &#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL (Qwen-7B)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;63.8&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;65.7&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;62.3&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;75.7&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Specialist SOTAs&lt;br&gt;(Specialist/Finetuned)&lt;/td&gt; &#xA;   &lt;td&gt;PALI-X-55B (Single-task FT)&lt;br&gt;(Without OCR Pipeline)&lt;/td&gt; &#xA;   &lt;td&gt;71.44&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;70.0&lt;/td&gt; &#xA;   &lt;td&gt;81.2&lt;/td&gt; &#xA;   &lt;td&gt;75.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In text-related recognition/QA evaluation, Qwen-VL achieves the SOTA under the generalist LVLM scale settings.&lt;/li&gt; &#xA; &lt;li&gt;Resolution is important for several above evaluations. While most open-sourced LVLM models with 224 resolution are incapable of these evaluations or can only solve these by cutting images, Qwen-VL scales the resolution to 448 so that it can be evaluated end-to-end. Qwen-VL even outperforms Pix2Struct-Large models of 1024 resolution on some tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Referring Expression Comprehension&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model type&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;RefCOCO&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;RefCOCO+&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;RefCOCOg&lt;/th&gt; &#xA;   &lt;th&gt;GRIT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;val&lt;/th&gt; &#xA;   &lt;th&gt;test-A&lt;/th&gt; &#xA;   &lt;th&gt;test-B&lt;/th&gt; &#xA;   &lt;th&gt;val&lt;/th&gt; &#xA;   &lt;th&gt;test-A&lt;/th&gt; &#xA;   &lt;th&gt;test-B&lt;/th&gt; &#xA;   &lt;th&gt;val-u&lt;/th&gt; &#xA;   &lt;th&gt;test-u&lt;/th&gt; &#xA;   &lt;th&gt;refexp&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody align=&#34;center&#34;&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;8&#34;&gt;Generalist Models&lt;/td&gt; &#xA;   &lt;td&gt;GPV-2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;51.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OFA-L*&lt;/td&gt; &#xA;   &lt;td&gt;79.96&lt;/td&gt; &#xA;   &lt;td&gt;83.67&lt;/td&gt; &#xA;   &lt;td&gt;76.39&lt;/td&gt; &#xA;   &lt;td&gt;68.29&lt;/td&gt; &#xA;   &lt;td&gt;76.00&lt;/td&gt; &#xA;   &lt;td&gt;61.75&lt;/td&gt; &#xA;   &lt;td&gt;67.57&lt;/td&gt; &#xA;   &lt;td&gt;67.58&lt;/td&gt; &#xA;   &lt;td&gt;61.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unified-IO&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;78.61&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VisionLLM-H&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;86.70&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shikra-7B&lt;/td&gt; &#xA;   &lt;td&gt;87.01&lt;/td&gt; &#xA;   &lt;td&gt;90.61&lt;/td&gt; &#xA;   &lt;td&gt;80.24 &lt;/td&gt; &#xA;   &lt;td&gt;81.60&lt;/td&gt; &#xA;   &lt;td&gt;87.36&lt;/td&gt; &#xA;   &lt;td&gt;72.12&lt;/td&gt; &#xA;   &lt;td&gt;82.27&lt;/td&gt; &#xA;   &lt;td&gt;82.19&lt;/td&gt; &#xA;   &lt;td&gt;69.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Shikra-13B&lt;/td&gt; &#xA;   &lt;td&gt;87.83 &lt;/td&gt; &#xA;   &lt;td&gt;91.11&lt;/td&gt; &#xA;   &lt;td&gt;81.81&lt;/td&gt; &#xA;   &lt;td&gt;82.89&lt;/td&gt; &#xA;   &lt;td&gt;87.79&lt;/td&gt; &#xA;   &lt;td&gt;74.41&lt;/td&gt; &#xA;   &lt;td&gt;82.64&lt;/td&gt; &#xA;   &lt;td&gt;83.16&lt;/td&gt; &#xA;   &lt;td&gt;69.03&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;89.36&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;92.26&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;85.34&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;83.12&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;88.25&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;77.21&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;85.58&lt;/td&gt; &#xA;   &lt;td&gt;85.48&lt;/td&gt; &#xA;   &lt;td&gt;78.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL-7B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;88.55&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;92.27&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;84.51&lt;/td&gt; &#xA;   &lt;td&gt;82.82&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;88.59&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;76.79&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;85.96&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;86.32&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Specialist SOTAs&lt;br&gt;(Specialist/Finetuned)&lt;/td&gt; &#xA;   &lt;td&gt;G-DINO-L&lt;/td&gt; &#xA;   &lt;td&gt;90.56&amp;nbsp;&amp;nbsp;&lt;/td&gt; &#xA;   &lt;td&gt;93.19&lt;/td&gt; &#xA;   &lt;td&gt;88.24&lt;/td&gt; &#xA;   &lt;td&gt;82.75&lt;/td&gt; &#xA;   &lt;td&gt;88.95&lt;/td&gt; &#xA;   &lt;td&gt;75.92&lt;/td&gt; &#xA;   &lt;td&gt;86.13&lt;/td&gt; &#xA;   &lt;td&gt;87.02&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UNINEXT-H&lt;/td&gt; &#xA;   &lt;td&gt;92.64 &lt;/td&gt; &#xA;   &lt;td&gt;94.33&lt;/td&gt; &#xA;   &lt;td&gt;91.46&lt;/td&gt; &#xA;   &lt;td&gt;85.24&lt;/td&gt; &#xA;   &lt;td&gt;89.63&lt;/td&gt; &#xA;   &lt;td&gt;79.79&lt;/td&gt; &#xA;   &lt;td&gt;88.73&lt;/td&gt; &#xA;   &lt;td&gt;89.37&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ONE-PEACE&lt;/td&gt; &#xA;   &lt;td&gt;92.58 &lt;/td&gt; &#xA;   &lt;td&gt;94.18&lt;/td&gt; &#xA;   &lt;td&gt;89.26&lt;/td&gt; &#xA;   &lt;td&gt;88.77&lt;/td&gt; &#xA;   &lt;td&gt;92.21&lt;/td&gt; &#xA;   &lt;td&gt;83.23&lt;/td&gt; &#xA;   &lt;td&gt;89.22&lt;/td&gt; &#xA;   &lt;td&gt;89.27&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Qwen-VL achieves the &lt;strong&gt;SOTA&lt;/strong&gt; in all above referring expression comprehension benchmarks.&lt;/li&gt; &#xA; &lt;li&gt;Qwen-VL has not been trained on any Chinese grounding data, but it can still generalize to the Chinese Grounding tasks in a zero-shot way by training Chinese Caption data and English Grounding data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We provide all of the above evaluation scripts for reproducing our experimental results. Please read &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/eval_mm/EVALUATION.md&#34;&gt;eval_mm/EVALUATION.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h3&gt;Chat evaluation&lt;/h3&gt; &#xA;&lt;p&gt;TouchStone is a benchmark based on scoring with GPT4 to evaluate the abilities of the LVLM model on text-image dialogue and alignment levels with humans. It covers a total of 300+ images, 800+ questions, and 27 categories, such as attribute-based Q&amp;amp;A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc. Please read &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/touchstone/README.md&#34;&gt;touchstone/README.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h4&gt;English evaluation&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PandaGPT&lt;/td&gt; &#xA;   &lt;td&gt;488.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniGPT4&lt;/td&gt; &#xA;   &lt;td&gt;531.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InstructBLIP&lt;/td&gt; &#xA;   &lt;td&gt;552.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-AdapterV2&lt;/td&gt; &#xA;   &lt;td&gt;590.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA&lt;/td&gt; &#xA;   &lt;td&gt;602.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mPLUG-Owl&lt;/td&gt; &#xA;   &lt;td&gt;605.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;   &lt;td&gt;645.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Chinese evaluation&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VisualGLM&lt;/td&gt; &#xA;   &lt;td&gt;247.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;   &lt;td&gt;401.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Qwen-VL-Chat has achieved the best results in both Chinese and English alignment evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.8 and above&lt;/li&gt; &#xA; &lt;li&gt;pytorch 1.12 and above, 2.0 and above are recommended&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.4 and above are recommended (this is for GPU users)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Below, we provide simple examples to show how to use Qwen-VL and Qwen-VL-Chat with 🤖 ModelScope and 🤗 Transformers.&lt;/p&gt; &#xA;&lt;p&gt;Before running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can start with ModelScope or Transformers. More usage aboue vision encoder, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/TUTORIAL.md&#34;&gt;tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;🤗 Transformers&lt;/h4&gt; &#xA;&lt;p&gt;To use Qwen-VL-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. However, &lt;strong&gt;please make sure that you are using the latest code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from transformers.generation import GenerationConfig&#xA;import torch&#xA;torch.manual_seed(1234)&#xA;&#xA;# Note: The default behavior now has injection attack prevention off.&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen-VL-Chat&#34;, trust_remote_code=True)&#xA;&#xA;# use bf16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL-Chat&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, bf16=True).eval()&#xA;# use fp16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL-Chat&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;# use cpu only&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL-Chat&#34;, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;# use cuda device&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL-Chat&#34;, device_map=&#34;cuda&#34;, trust_remote_code=True).eval()&#xA;&#xA;# Specify hyperparameters for generation&#xA;model.generation_config = GenerationConfig.from_pretrained(&#34;Qwen/Qwen-VL-Chat&#34;, trust_remote_code=True)&#xA;&#xA;# 1st dialogue turn&#xA;query = tokenizer.from_list_format([&#xA;    {&#39;image&#39;: &#39;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#39;}, # Either a local path or an url&#xA;    {&#39;text&#39;: &#39;这是什么?&#39;},&#xA;])&#xA;response, history = model.chat(tokenizer, query=query, history=None)&#xA;print(response)&#xA;# 图中是一名女子在沙滩上和狗玩耍，旁边是一只拉布拉多犬，它们处于沙滩上。&#xA;&#xA;# 2nd dialogue turn&#xA;response, history = model.chat(tokenizer, &#39;框出图中击掌的位置&#39;, history=history)&#xA;print(response)&#xA;# &amp;lt;ref&amp;gt;击掌&amp;lt;/ref&amp;gt;&amp;lt;box&amp;gt;(536,509),(588,602)&amp;lt;/box&amp;gt;&#xA;image = tokenizer.draw_bbox_on_latest_picture(response, history)&#xA;if image:&#xA;  image.save(&#39;1.jpg&#39;)&#xA;else:&#xA;  print(&#34;no box&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_highfive.jpg&#34; width=&#34;500&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;details&gt; &#xA; &lt;summary&gt;Running Qwen-VL&lt;/summary&gt; &#xA; &lt;p&gt;Running Qwen-VL pretrained base model is also simple.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;from transformers.generation import GenerationConfig&#xA;import torch&#xA;torch.manual_seed(1234)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen-VL&#34;, trust_remote_code=True)&#xA;&#xA;# use bf16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, bf16=True).eval()&#xA;# use fp16&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL&#34;, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;# use cpu only&#xA;# model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL&#34;, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;# use cuda device&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen-VL&#34;, device_map=&#34;cuda&#34;, trust_remote_code=True).eval()&#xA;&#xA;# Specify hyperparameters for generation&#xA;model.generation_config = GenerationConfig.from_pretrained(&#34;Qwen/Qwen-VL&#34;, trust_remote_code=True)&#xA;&#xA;query = tokenizer.from_list_format([&#xA;    {&#39;image&#39;: &#39;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#39;}, # Either a local path or an url&#xA;    {&#39;text&#39;: &#39;Generate the caption in English with grounding:&#39;},&#xA;])&#xA;inputs = tokenizer(query, return_tensors=&#39;pt&#39;)&#xA;inputs = inputs.to(model.device)&#xA;pred = model.generate(**inputs)&#xA;response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)&#xA;print(response)&#xA;# &amp;lt;img&amp;gt;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&amp;lt;/img&amp;gt;Generate the caption in English with grounding:&amp;lt;ref&amp;gt; Woman&amp;lt;/ref&amp;gt;&amp;lt;box&amp;gt;(451,379),(731,806)&amp;lt;/box&amp;gt; and&amp;lt;ref&amp;gt; her dog&amp;lt;/ref&amp;gt;&amp;lt;box&amp;gt;(219,424),(576,896)&amp;lt;/box&amp;gt; playing on the beach&amp;lt;|endoftext|&amp;gt;&#xA;image = tokenizer.draw_bbox_on_latest_picture(response)&#xA;if image:&#xA;  image.save(&#39;2.jpg&#39;)&#xA;else:&#xA;  print(&#34;no box&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_spotting_caption.jpg&#34; width=&#34;500&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt; &lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h4&gt;🤖 ModelScope&lt;/h4&gt; &#xA;&lt;p&gt;ModelScope is an opensource platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modelscope import (&#xA;    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig&#xA;)&#xA;import torch&#xA;model_id = &#39;qwen/Qwen-VL-Chat&#39;&#xA;revision = &#39;v1.0.0&#39;&#xA;&#xA;model_dir = snapshot_download(model_id, revision=revision)&#xA;torch.manual_seed(1234)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)&#xA;if not hasattr(tokenizer, &#39;model_dir&#39;):&#xA;    tokenizer.model_dir = model_dir&#xA;# use bf16&#xA;# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&#34;auto&#34;, trust_remote_code=True, bf16=True).eval()&#xA;# use fp16&#xA;model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&#34;auto&#34;, trust_remote_code=True, fp16=True).eval()&#xA;# use cpu&#xA;# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&#34;cpu&#34;, trust_remote_code=True).eval()&#xA;# use auto&#xA;# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&#34;auto&#34;, trust_remote_code=True).eval()&#xA;&#xA;# Specify hyperparameters for generation&#xA;model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)&#xA;&#xA;# 1st dialogue turn&#xA;# Either a local path or an url between &amp;lt;img&amp;gt;&amp;lt;/img&amp;gt; tags.&#xA;image_path = &#39;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#39;&#xA;response, history = model.chat(tokenizer, query=f&#39;&amp;lt;img&amp;gt;{image_path}&amp;lt;/img&amp;gt;这是什么&#39;, history=None)&#xA;print(response)&#xA;# 图中是一名年轻女子在沙滩上和她的狗玩耍，狗的品种是拉布拉多。她们坐在沙滩上，狗的前腿抬起来，与人互动。&#xA;&#xA;# 2nd dialogue turn&#xA;response, history = model.chat(tokenizer, &#39;输出击掌的检测框&#39;, history=history)&#xA;print(response)&#xA;# &amp;lt;ref&amp;gt;&#34;击掌&#34;&amp;lt;/ref&amp;gt;&amp;lt;box&amp;gt;(211,412),(577,891)&amp;lt;/box&amp;gt;&#xA;image = tokenizer.draw_bbox_on_latest_picture(response, history)&#xA;if image:&#xA;  image.save(&#39;output_chat.jpg&#39;)&#xA;else:&#xA;  print(&#34;no box&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_highfive.jpg&#34; width=&#34;500&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;We provide code for users to build a web UI demo. Before you start, make sure you install the following packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements_web_demo.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the command below and click on the generated link:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python web_demo_mm.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;If you meet problems, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/FAQ.md&#34;&gt;FAQ&lt;/a&gt; and the issues first to search a solution before you launch a new issue.&lt;/p&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;Researchers and developers are free to use the codes and model weights of both Qwen-VL and Qwen-VL-Chat. We also allow their commercial use. Check our license at &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, feel free to send an email to &lt;a href=&#34;mailto:qianwen_opensource@alibabacloud.com&#34;&gt;qianwen_opensource@alibabacloud.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>