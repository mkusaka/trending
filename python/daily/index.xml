<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-17T01:44:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepmind/mctx</title>
    <updated>2023-02-17T01:44:45Z</updated>
    <id>tag:github.com,2023-02-17:/deepmind/mctx</id>
    <link href="https://github.com/deepmind/mctx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Monte Carlo tree search in JAX&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mctx: MCTS-in-JAX&lt;/h1&gt; &#xA;&lt;p&gt;Mctx is a library with a &lt;a href=&#34;https://github.com/google/jax&#34;&gt;JAX&lt;/a&gt;-native implementation of Monte Carlo tree search (MCTS) algorithms such as &lt;a href=&#34;https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go&#34;&gt;AlphaZero&lt;/a&gt;, &lt;a href=&#34;https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules&#34;&gt;MuZero&lt;/a&gt;, and &lt;a href=&#34;https://openreview.net/forum?id=bERaNdoegnO&#34;&gt;Gumbel MuZero&lt;/a&gt;. For computation speed up, the implementation fully supports JIT-compilation. Search algorithms in Mctx are defined for and operate on batches of inputs, in parallel. This allows to make the most of the accelerators and enables the algorithms to work with large learned environment models parameterized by deep neural networks.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install the latest released version of Mctx from PyPI via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install mctx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can install the latest development version from GitHub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://github.com/deepmind/mctx.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;Learning and search have been important topics since the early days of AI research. In the &lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34;&gt;words of Rich Sutton&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;One thing that should be learned [...] is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are &lt;em&gt;search&lt;/em&gt; and &lt;em&gt;learning&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Recently, search algorithms have been successfully combined with learned models parameterized by deep neural networks, resulting in some of the most powerful and general reinforcement learning algorithms to date (e.g. MuZero). However, using search algorithms in combination with deep neural networks requires efficient implementations, typically written in fast compiled languages; this can come at the expense of usability and hackability, especially for researchers that are not familiar with C++. In turn, this limits adoption and further research on this critical topic.&lt;/p&gt; &#xA;&lt;p&gt;Through this library, we hope to help researchers everywhere to contribute to such an exciting area of research. We provide JAX-native implementations of core search algorithms such as MCTS, that we believe strike a good balance between performance and usability for researchers that want to investigate search-based algorithms in Python. The search methods provided by Mctx are heavily configurable to allow researchers to explore a variety of ideas in this space, and contribute to the next generation of search based agents.&lt;/p&gt; &#xA;&lt;h2&gt;Search in Reinforcement Learning&lt;/h2&gt; &#xA;&lt;p&gt;In Reinforcement Learning the &lt;em&gt;agent&lt;/em&gt; must learn to interact with the &lt;em&gt;environment&lt;/em&gt; in order to maximize a scalar &lt;em&gt;reward&lt;/em&gt; signal. On each step the agent must select an action and receives in exchange an observation and a reward. We may call whatever mechanism the agent uses to select the action the agent&#39;s &lt;em&gt;policy&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Classically, policies are parameterized directly by a function approximator (as in REINFORCE), or policies are inferred by inspecting a set of learned estimates of the value of each action (as in Q-learning). Alternatively, search allows to select actions by constructing on the fly, in each state, a policy or a value function local to the current state, by &lt;em&gt;searching&lt;/em&gt; using a learned &lt;em&gt;model&lt;/em&gt; of the environment.&lt;/p&gt; &#xA;&lt;p&gt;Exhaustive search over all possible future courses of actions is computationally prohibitive in any non trivial environment, hence we need search algorithms that can make the best use of a finite computational budget. Typically priors are needed to guide which nodes in the search tree to expand (to reduce the &lt;em&gt;breadth&lt;/em&gt; of the tree that we construct), and value functions are used to estimate the value of incomplete paths in the tree that don&#39;t reach an episode termination (to reduce the &lt;em&gt;depth&lt;/em&gt; of the search tree).&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Mctx provides a low-level generic &lt;code&gt;search&lt;/code&gt; function and high-level concrete policies: &lt;code&gt;muzero_policy&lt;/code&gt; and &lt;code&gt;gumbel_muzero_policy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The user needs to provide several learned components to specify the representation, dynamics and prediction used by &lt;a href=&#34;https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules&#34;&gt;MuZero&lt;/a&gt;. In the context of the Mctx library, the representation of the &lt;em&gt;root&lt;/em&gt; state is specified by a &lt;code&gt;RootFnOutput&lt;/code&gt;. The &lt;code&gt;RootFnOutput&lt;/code&gt; contains the &lt;code&gt;prior_logits&lt;/code&gt; from a policy network, the estimated &lt;code&gt;value&lt;/code&gt; of the root state, and any &lt;code&gt;embedding&lt;/code&gt; suitable to represent the root state for the environment model.&lt;/p&gt; &#xA;&lt;p&gt;The dynamics environment model needs to be specified by a &lt;code&gt;recurrent_fn&lt;/code&gt;. A &lt;code&gt;recurrent_fn(params, rng_key, action, embedding)&lt;/code&gt; call takes an &lt;code&gt;action&lt;/code&gt; and a state &lt;code&gt;embedding&lt;/code&gt;. The call should return a tuple &lt;code&gt;(recurrent_fn_output, new_embedding)&lt;/code&gt; with a &lt;code&gt;RecurrentFnOutput&lt;/code&gt; and the embedding of the next state. The &lt;code&gt;RecurrentFnOutput&lt;/code&gt; contains the &lt;code&gt;reward&lt;/code&gt; and &lt;code&gt;discount&lt;/code&gt; for the transition, and &lt;code&gt;prior_logits&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; for the new state.&lt;/p&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://github.com/deepmind/mctx/raw/main/examples/visualization_demo.py&#34;&gt;&lt;code&gt;examples/visualization_demo.py&lt;/code&gt;&lt;/a&gt;, you can see calls to a policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;policy_output = mctx.gumbel_muzero_policy(params, rng_key, root, recurrent_fn,&#xA;                                          num_simulations=32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;policy_output.action&lt;/code&gt; contains the action proposed by the search. That action can be passed to the environment. To improve the policy, the &lt;code&gt;policy_output.action_weights&lt;/code&gt; contain targets usable to train the policy probabilities.&lt;/p&gt; &#xA;&lt;p&gt;We recommend to use the &lt;code&gt;gumbel_muzero_policy&lt;/code&gt;. &lt;a href=&#34;https://openreview.net/forum?id=bERaNdoegnO&#34;&gt;Gumbel MuZero&lt;/a&gt; guarantees a policy improvement if the action values are correctly evaluated. The policy improvement is demonstrated in &lt;a href=&#34;https://github.com/deepmind/mctx/raw/main/examples/policy_improvement_demo.py&#34;&gt;&lt;code&gt;examples/policy_improvement_demo.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Example projects&lt;/h3&gt; &#xA;&lt;p&gt;The following projects demonstrate the Mctx usage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kenjyoung/mctx_learning_demo&#34;&gt;Basic Learning Demo with Mctx&lt;/a&gt; — AlphaZero on random mazes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NTT123/a0-jax&#34;&gt;a0-jax&lt;/a&gt; — AlphaZero on Connect Four, Gomoku, and Go.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Tell us about your project.&lt;/p&gt; &#xA;&lt;h2&gt;Citing Mctx&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product. Mctx is part of the &lt;a href=&#34;https://deepmind.com/blog/article/using-jax-to-accelerate-our-research&#34; title=&#34;DeepMind JAX Ecosystem&#34;&gt;DeepMind JAX Ecosystem&lt;/a&gt;; to cite Mctx, please use the &lt;a href=&#34;https://github.com/deepmind/jax/raw/main/deepmind2020jax.txt&#34; title=&#34;Citation&#34;&gt;DeepMind JAX Ecosystem citation&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cure-lab/LTSF-Linear</title>
    <updated>2023-02-17T01:44:45Z</updated>
    <id>tag:github.com,2023-02-17:/cure-lab/LTSF-Linear</id>
    <link href="https://github.com/cure-lab/LTSF-Linear" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the official implementation for AAAI-23 Oral paper &#34;Are Transformers Effective for Time Series Forecasting?&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Are Transformers Effective for Time Series Forecasting? (AAAI 2023)&lt;/h1&gt; &#xA;&lt;p&gt;This repo is the official Pytorch implementation of LTSF-Linear: &#34;&lt;a href=&#34;https://arxiv.org/pdf/2205.13504.pdf&#34;&gt;Are Transformers Effective for Time Series Forecasting?&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2022/11/23] Accepted to AAAI 2023 with three strong accept! We also release a &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cure-lab/LTSF-Linear/main/LTSF-Benchmark.md&#34;&gt;benchmark for long-term time series forecasting&lt;/a&gt;&lt;/strong&gt; for further research.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2022/08/25] We update our &lt;a href=&#34;https://arxiv.org/pdf/2205.13504.pdf&#34;&gt;paper&lt;/a&gt; with comprehensive analyses on why existing LTSF-Transformers do not work well on the LTSF problem!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2022/08/25] Besides DLinear, we&#39;re exicted to add two Linear models to the paper and this repo. Now we have a LTSF-Linear family!&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linear: Just one linear layer.&lt;/li&gt; &#xA;   &lt;li&gt;DLinear: Decomposition Linear to handle data with trend and seasonality patterns.&lt;/li&gt; &#xA;   &lt;li&gt;NLinear: A Normalized Linear to deal with train-test set distribution shifts. See section &#39;LTSF-Linear&#39; for more details.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2022/08/25] We update some scripts of LTSF-Linear.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linear, NLinear, and DLinear use the same scripts.&lt;/li&gt; &#xA;   &lt;li&gt;Some results of DLinear are slightly different now.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add a &lt;a href=&#34;https://raw.githubusercontent.com/cure-lab/LTSF-Linear/main/LTSF-Benchmark.md&#34;&gt;benchmark&lt;/a&gt; for long-term time series forecasting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support both &lt;a href=&#34;https://github.com/cure-lab/DLinear/tree/main/scripts/EXP-LongForecasting/DLinear/univariate&#34;&gt;Univariate&lt;/a&gt; and &lt;a href=&#34;https://github.com/cure-lab/DLinear/tree/main/scripts/EXP-LongForecasting/DLinear&#34;&gt;Multivariate&lt;/a&gt; long-term time series forecasting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support visualization of weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support scripts on different &lt;a href=&#34;https://github.com/cure-lab/DLinear/tree/main/scripts/EXP-LookBackWindow&#34;&gt;look-back window size&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Beside LTSF-Linear, we provide five significant forecasting Transformers to re-implement the results in the paper.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer&lt;/a&gt; (NeuIPS 2017)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07436&#34;&gt;Informer&lt;/a&gt; (AAAI 2021 Best paper)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.13008&#34;&gt;Autoformer&lt;/a&gt; (NeuIPS 2021)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://openreview.net/pdf?id=0EXmFzUn5I&#34;&gt;Pyraformer&lt;/a&gt; (ICLR 2022 Oral)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.12740&#34;&gt;FEDformer&lt;/a&gt; (ICML 2022)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Detailed Description&lt;/h2&gt; &#xA;&lt;p&gt;We provide all experiment script files in &lt;code&gt;./scripts&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Files&lt;/th&gt; &#xA;   &lt;th&gt;Interpretation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EXP-LongForecasting&lt;/td&gt; &#xA;   &lt;td&gt;Long-term Time Series Forecasting Task&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EXP-LookBackWindow&lt;/td&gt; &#xA;   &lt;td&gt;Study the impact of different look-back window sizes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EXP-Embedding&lt;/td&gt; &#xA;   &lt;td&gt;Study the effects of different embedding strategies&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This code is simply built on the code base of Autoformer. We appreciate the following GitHub repos a lot for their valuable code base or datasets:&lt;/p&gt; &#xA;&lt;p&gt;The implementation of Autoformer, Informer, Transformer is from &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;https://github.com/thuml/Autoformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The implementation of FEDformer is from &lt;a href=&#34;https://github.com/MAZiqing/FEDformer&#34;&gt;https://github.com/MAZiqing/FEDformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The implementation of Pyraformer is from &lt;a href=&#34;https://github.com/alipay/Pyraformer&#34;&gt;https://github.com/alipay/Pyraformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;LTSF-Linear&lt;/h2&gt; &#xA;&lt;h3&gt;LTSF-Linear family&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cure-lab/LTSF-Linear/main/pics/Linear.png&#34; alt=&#34;image&#34;&gt; LTSF-Linear is a set of linear models.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linear: It is just a one-layer linear model, but it outperforms Transformers.&lt;/li&gt; &#xA; &lt;li&gt;NLinear: &lt;strong&gt;To boost the performance of Linear when there is a distribution shift in the dataset&lt;/strong&gt;, NLinear first subtracts the input by the last value of the sequence. Then, the input goes through a linear layer, and the subtracted part is added back before making the final prediction. The subtraction and addition in NLinear are a simple normalization for the input sequence.&lt;/li&gt; &#xA; &lt;li&gt;DLinear: It is a combination of a Decomposition scheme used in Autoformer and FEDformer with linear layers. It first decomposes a raw data input into a trend component by a moving average kernel and a remainder (seasonal) component. Then, two one-layer linear layers are applied to each component and we sum up the two features to get the final prediction. By explicitly handling trend, &lt;strong&gt;DLinear enhances the performance of a vanilla linear when there is a clear trend in the data.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Although LTSF-Linear is simple, it has some compelling characteristics:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An O(1) maximum signal traversing path length: The shorter the path, the better the dependencies are captured, making LTSF-Linear capable of capturing both short-range and long-range temporal relations.&lt;/li&gt; &#xA; &lt;li&gt;High-efficiency: As each branch has only one linear layer, it costs much lower memory and fewer parameters and has a faster inference speed than existing Transformers.&lt;/li&gt; &#xA; &lt;li&gt;Interpretability: After training, we can visualize weights to have some insights on the predicted values.&lt;/li&gt; &#xA; &lt;li&gt;Easy-to-use: LTSF-Linear can be obtained easily without tuning model hyper-parameters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Comparison with Transformers&lt;/h3&gt; &#xA;&lt;p&gt;Univariate Forecasting: &lt;img src=&#34;https://raw.githubusercontent.com/cure-lab/LTSF-Linear/main/pics/Uni-results.png&#34; alt=&#34;image&#34;&gt; Multivariate Forecasting: &lt;img src=&#34;https://raw.githubusercontent.com/cure-lab/LTSF-Linear/main/pics/Mul-results.png&#34; alt=&#34;image&#34;&gt; LTSF-Linear outperforms all transformer-based methods by a large margin.&lt;/p&gt; &#xA;&lt;h3&gt;Efficiency&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cure-lab/LTSF-Linear/main/pics/efficiency.png&#34; alt=&#34;image&#34;&gt; Comparison of method efficiency with Look-back window size 96 and Forecasting steps 720 on Electricity. MACs are the number of multiply-accumulate operations. We use DLinear for comparison, since it has the double cost in LTSF-Linear. The inference time averages 5 runs.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Requirements&lt;/h3&gt; &#xA;&lt;p&gt;First, please make sure you have installed Conda. Then, our environment can be installed by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n LTSF_Linear python=3.6.9&#xA;conda activate LTSF_Linear&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;You can obtain all the nine benchmarks from &lt;a href=&#34;https://drive.google.com/drive/folders/1ZOYpTUa82_jCcxIdTmyr0LXQfvaM9vIy&#34;&gt;Google Drive&lt;/a&gt; provided in Autoformer. All the datasets are well pre-processed and can be used easily.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please put them in the &lt;code&gt;./dataset&lt;/code&gt; directory&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Training Example&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;scripts/ &lt;/code&gt;, we provide the model implementation &lt;em&gt;Dlinear/Autoformer/Informer/Transformer&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;FEDformer/scripts/&lt;/code&gt;, we provide the &lt;em&gt;FEDformer&lt;/em&gt; implementation&lt;/li&gt; &#xA; &lt;li&gt;In &lt;code&gt;Pyraformer/scripts/&lt;/code&gt;, we provide the &lt;em&gt;Pyraformer&lt;/em&gt; implementation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;p&gt;To train the &lt;strong&gt;LTSF-Linear&lt;/strong&gt; on &lt;strong&gt;Exchange-Rate dataset&lt;/strong&gt;, you can use the scipt &lt;code&gt;scripts/EXP-LongForecasting/Linear/exchange_rate.sh&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh scripts/EXP-LongForecasting/Linear/exchange_rate.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will start to train DLinear by default, the results will be shown in &lt;code&gt;logs/LongForecasting&lt;/code&gt;. You can specify the name of the model in the script. (Linear, DLinear, NLinear)&lt;/p&gt; &#xA;&lt;p&gt;All scripts about using LTSF-Linear on long forecasting task is in &lt;code&gt;scripts/EXP-LongForecasting/Linear/&lt;/code&gt;, you can run them in a similar way. The default look-back window in scripts is 336, LTSF-Linear generally achieves better results with longer look-back window as dicussed in the paper.&lt;/p&gt; &#xA;&lt;p&gt;Scripts about look-back window size and long forecasting of FEDformer and Pyraformer is in &lt;code&gt;FEDformer/scripts&lt;/code&gt; and &lt;code&gt;Pyraformer/scripts&lt;/code&gt;, respectively. To run them, you need to first &lt;code&gt;cd FEDformer&lt;/code&gt; or &lt;code&gt;cd Pyraformer&lt;/code&gt;. Then, you can use sh to run them in a similar way. Logs will store in &lt;code&gt;logs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Each experiment in &lt;code&gt;scripts/EXP-LongForecasting/Linear/&lt;/code&gt; takes 5min-20min. For other Transformer scripts, since we put all related experiments in one script file, directly running them will take 8 hours-1 day. You can keep the experiments you interested in and comment out the others.&lt;/p&gt; &#xA;&lt;h3&gt;Weights Visualization&lt;/h3&gt; &#xA;&lt;p&gt;As shown in our paper, the weights of LTSF-Linear can reveal some charateristic of the data, i.e., the periodicity. As an example, we provide the weight visualization of DLinear in &lt;code&gt;weight_plot.py&lt;/code&gt;. To run the visualization, you need to input the model path (model_name) of DLinear (the model directory in &lt;code&gt;./checkpoint&lt;/code&gt; by default). To obtain smooth and clear patterns, you can use the initialization we provided in the file of linear models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cure-lab/LTSF-Linear/main/pics/Visualization_DLinear.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful for your work, please consider citing it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Zeng2022AreTE,&#xA;  title={Are Transformers Effective for Time Series Forecasting?},&#xA;  author={Ailing Zeng and Muxi Chen and Lei Zhang and Qiang Xu},&#xA;  journal={Proceedings of the AAAI Conference on Artificial Intelligence},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please remember to cite all the datasets and compared methods if you use them in your experiments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>HugoTini/DeepBump</title>
    <updated>2023-02-17T01:44:45Z</updated>
    <id>tag:github.com,2023-02-17:/HugoTini/DeepBump</id>
    <link href="https://github.com/HugoTini/DeepBump" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Normal &amp; height maps generation from single pictures&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HugoTini/DeepBump/master/banner.jpg&#34; alt=&#34;DeepBump&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;DeepBump&lt;/h1&gt; &#xA;&lt;p&gt;DeepBump is a machine-learning driven tool to generate normal &amp;amp; height maps from single pictures. See this &lt;a href=&#34;https://hugotini.github.io/deepbump&#34;&gt;blog post&lt;/a&gt; for an introduction.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/HugoTini/DeepBump/releases&#34;&gt;Download DeepBump as a ZIP&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In Blender, go to &lt;em&gt;Edit -&amp;gt; Preferences -&amp;gt; Add-ons -&amp;gt; Install&lt;/em&gt; and select the downloaded ZIP file. Then enable the add-on.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the add-on preference, click the &#39;&lt;em&gt;Install dependencies&lt;/em&gt;&#39; button (this requires an internet connection and might take a while). In case of error, try running Blender as administrator for this step.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;By installing those dependencies, be aware of &lt;a href=&#34;https://github.com/microsoft/onnxruntime/raw/master/docs/Privacy.md&#34;&gt;Microsoft conditions&lt;/a&gt;. This add-on use available APIs to disable telemetry.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://hugotini.github.io/deepbump&#34;&gt;blog post&lt;/a&gt; first video.&lt;/p&gt; &#xA;&lt;p&gt;In the Shader Editor, in the right panel under the &lt;em&gt;DeepBump&lt;/em&gt; tab:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Color (albedo) → Normals&lt;/strong&gt; : Select a color image node and click &lt;em&gt;Generate Normal Map&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Normals → Height (displacement)&lt;/strong&gt; : Select the generated normal map image node and click &lt;em&gt;Generate Height Map&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Normals → Curvature&lt;/strong&gt; : Select the generated normal map image node and click &lt;em&gt;Generate Curvature Map&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This repo is under the &lt;a href=&#34;https://raw.githubusercontent.com/HugoTini/DeepBump/master/LICENSE&#34;&gt;GPL license&lt;/a&gt;. The training code is currently not available.&lt;/p&gt;</summary>
  </entry>
</feed>