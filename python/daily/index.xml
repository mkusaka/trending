<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-11T01:37:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MetaGLM/FinGLM</title>
    <updated>2023-10-11T01:37:46Z</updated>
    <id>tag:github.com,2023-10-11:/MetaGLM/FinGLM</id>
    <link href="https://github.com/MetaGLM/FinGLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;ğŸŒ FinGLM&lt;/h1&gt; &#xA;&lt;!--  &lt;h3 align=&#34;center&#34;&gt;SMP 2023 ChatGLM é‡‘èå¤§æ¨¡å‹æŒ‘æˆ˜èµ›&lt;/h3&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;./img/&#34; alt=&#34;FinGLM Logo&#34;&gt; ---&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;èµ›é¢˜é“¾æ¥&lt;/a&gt; | &lt;a href=&#34;https://tianchi.aliyun.com/specials/promotion/SMP2023ChatGLMChallenge&#34;&gt;èµ›é¢˜å®£ä¼ é¡µ&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;ğŸ“ƒ &lt;strong&gt;FinGLM&lt;/strong&gt;: è‡´åŠ›äºæ„å»ºä¸€ä¸ªå¼€æ”¾çš„ã€å…¬ç›Šçš„ã€æŒä¹…çš„é‡‘èå¤§æ¨¡å‹é¡¹ç›®ï¼Œåˆ©ç”¨å¼€æºå¼€æ”¾æ¥ä¿ƒè¿›ã€ŒAI+é‡‘èã€ã€‚&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ğŸš€ ç›®å½•&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D&#34;&gt;é¡¹ç›®ä»‹ç»&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E6%A1%86%E6%9E%B6&#34;&gt;é¡¹ç›®æ¡†æ¶&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E6%B5%81%E7%A8%8B&#34;&gt;1. æ•°æ®å‡†å¤‡æµç¨‹&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%A8%A1%E5%9E%8B%E5%87%86%E5%A4%87%E6%B5%81%E7%A8%8B&#34;&gt;2. æ¨¡å‹å‡†å¤‡æµç¨‹&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%97%AE%E7%AD%94%E6%B5%81%E7%A8%8B&#34;&gt;3. é—®ç­”æµç¨‹&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%BC%80%E6%BA%90%E8%B7%AF%E7%BA%BF%E5%9B%BE&#34;&gt;å¼€æºè·¯çº¿å›¾&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%BC%80%E6%BA%90%E7%AD%96%E7%95%A5&#34;&gt;1. å¼€æºç­–ç•¥&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%BC%80%E6%BA%90%E8%BF%9B%E5%BA%A6&#34;&gt;2. å¼€æºè¿›åº¦&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%AF%94%E8%B5%9B%E9%A1%B9%E7%9B%AE&#34;&gt;æ¯”èµ›é¡¹ç›®&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#FinGLM_all&#34;&gt;0. FinGLM_all&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;1. é¦’å¤´ç§‘æŠ€&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;2. å—å“ªéƒ½é˜Ÿ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;3. Chatglmåå·æ€»å±€&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#nsddd&#34;&gt;4. nsddd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;5. é¾™ç›ˆæˆ˜é˜Ÿ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;6. ç»“å©šä¹°æˆ¿ä»£ä»£éŸ­èœ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#TabIsabaopilong&#34;&gt;7. TabIsabaopilong&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A5%BA%E5%AD%90%E7%A0%94%E7%A9%B6%E9%99%A2&#34;&gt;8. é¥ºå­ç ”ç©¶é™¢&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%B5%81%E5%AE%9D%E7%9C%9F%E4%BA%BA&#34;&gt;9. æµå®çœŸäºº&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8F%8F%E8%BF%B0&#34;&gt;æ•°æ®é›†æè¿°&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%B9%B4%E6%8A%A5%E6%95%B0%E6%8D%AE%E9%9B%86&#34;&gt;1. å¹´æŠ¥æ•°æ®é›†&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#PDF%E4%B8%8B%E8%BD%BD&#34;&gt;PDFä¸‹è½½&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#TXT%E4%B8%8B%E8%BD%BD&#34;&gt;TXTä¸‹è½½&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#HTML%E4%B8%8B%E8%BD%BD&#34;&gt;HTMLä¸‹è½½&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E4%BD%BF%E7%94%A8%E5%BB%BA%E8%AE%AE&#34;&gt;ä½¿ç”¨å»ºè®®&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE&#34;&gt;2. æ ‡æ³¨æ•°æ®&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E9%97%AE%E7%AD%94%E6%BC%94%E7%A4%BA&#34;&gt;é¡¹ç›®é—®ç­”æ¼”ç¤º&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E8%B4%A1%E7%8C%AE%E8%80%85&#34;&gt;è´¡çŒ®è€…&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E&#34;&gt;å…è´£å£°æ˜&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/README.md#%E9%A1%B9%E7%9B%AE%E8%81%94%E7%B3%BB&#34;&gt;é¡¹ç›®è”ç³»&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ğŸ“– é¡¹ç›®ä»‹ç»&lt;/h2&gt; &#xA;&lt;p&gt;ğŸ“ˆ ä¸€ä¸ªæ—¨åœ¨æ·±åº¦è§£æä¸Šå¸‚å…¬å¸å¹´æŠ¥çš„å¯¹è¯äº¤äº’æ™ºèƒ½ç³»ç»Ÿã€‚é¢å¯¹é‡‘èæ–‡æœ¬ä¸­çš„ä¸“ä¸šæœ¯è¯­ä¸æš—å«ä¿¡æ¯ï¼Œæˆ‘ä»¬è‡´åŠ›äºç”¨AIå®ç°ä¸“å®¶çº§åˆ«çš„é‡‘èåˆ†æã€‚&lt;/p&gt; &#xA;&lt;p&gt;ğŸš€ åœ¨AIé¢†åŸŸï¼Œè™½ç„¶å·²åœ¨æ–‡æœ¬å¯¹è¯å–å¾—è¿›å±•ï¼Œä½†çœŸæ­£çš„é‡‘èäº¤äº’åœºæ™¯ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚å¤šæ–¹æœºæ„è”æ‰‹ä¸¾åŠæ­¤æ¬¡ç«èµ›ï¼Œæ¢ç´¢é‡‘èé¢†åŸŸAIçš„è¾¹ç•Œã€‚&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“˜ ä¸Šå¸‚å…¬å¸å¹´æŠ¥ä¸ºæŠ•èµ„è€…å‘ˆç°äº†å…¬å¸çš„ç»è¥çŠ¶å†µã€è´¢åŠ¡çŠ¶å†µå’Œæœªæ¥è§„åˆ’ã€‚ä¸“ä¸šçŸ¥è¯†æ˜¯è§£è¯»çš„å…³é”®ï¼Œè€Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡AIæŠ€æœ¯è®©è¿™ä¸€è¿‡ç¨‹å˜å¾—æ›´ç®€å•ã€æ›´å‡†ç¡®ã€‚&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ğŸ›  é¡¹ç›®æ¡†æ¶&lt;/h2&gt; &#xA;&lt;h3&gt;1. æ•°æ®å‡†å¤‡æµç¨‹&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/tools/pdf_to_txt&#34;&gt;PDF è½¬ TXT&lt;/a&gt;&lt;/strong&gt;ï¼š&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;è½¬æ¢ä¸º TXT æ ¼å¼ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;ä¿ç•™è¡¨æ ¼å¹¶åˆå¹¶å•å…ƒæ ¼ã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®åˆ‡åˆ†&lt;/strong&gt;ï¼š&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;åŸºç¡€ä¿¡æ¯ï¼šä¾‹å¦‚å…¬å¸åç§°ç­‰ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;è´¢åŠ¡æ•°æ®ï¼šä¾‹å¦‚èµ„äº§è´Ÿå€ºè¡¨ç­‰ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;ç»¼åˆä¿¡æ¯ï¼šä¾‹å¦‚è´¢åŠ¡æŒ‡æ ‡ç­‰ã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®å¤„ç†&lt;/strong&gt;ï¼š&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;è®¡ç®—åŸºç¡€å…¬å¼ï¼šå¦‚è¥ä¸šæˆæœ¬ç‡ç­‰ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;è®¡ç®—å¢é•¿ç‡ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;è®¡ç®—è¡Œä¸šå‡å€¼å’Œæ’åã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;å­˜å…¥æ•°æ®åº“&lt;/strong&gt;ï¼š&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;å­˜å…¥ SQLã€Mongo å’Œ ES ä¸­ã€‚&lt;/li&gt; &#xA;   &lt;li&gt;åŒ…æ‹¬å»ºè¡¨åŠå­˜å‚¨ã€‚&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. æ¨¡å‹å¾®è°ƒæµç¨‹&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®åˆ†ç±»&lt;/strong&gt;ï¼šå¦‚ SQL æ•°æ®ã€ES æ•°æ®ç­‰ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;é€‰æ‹©å¾®è°ƒç­–ç•¥&lt;/strong&gt;ï¼šä¾‹å¦‚ ptuningv2ã€loraç­‰ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ‰§è¡Œå¾®è°ƒ&lt;/strong&gt;ï¼šæ ¹æ®é€‰å®šç­–ç•¥ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. é—®ç­”æµç¨‹&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;è¾“å…¥é—®é¢˜&lt;/strong&gt;ï¼šç”¨æˆ·è¾“å…¥é—®é¢˜ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt å‡†å¤‡&lt;/strong&gt;ï¼šæ ¹æ®é—®é¢˜ç”Ÿæˆ promptã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ç”ŸæˆæŸ¥è¯¢è¯­å¥&lt;/strong&gt;ï¼šåŸºäº GPU ä½¿ç”¨ç‡é€‰æ‹©ç”Ÿæˆæ–¹æ³•ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æŸ¥è¯¢æ•°æ®åº“&lt;/strong&gt;ï¼šå¹¶è¿”å›ç»“æœã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ç­”æ¡ˆç”Ÿæˆ&lt;/strong&gt;ï¼šç»“åˆé—®é¢˜å’ŒæŸ¥è¯¢ç»“æœç”Ÿæˆç­”æ¡ˆã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ğŸŒ± å¼€æºè·¯çº¿å›¾&lt;/h2&gt; &#xA;&lt;h3&gt;1. å¼€æºç­–ç•¥&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1) èµ›äº‹è½¬å‹&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¯”èµ›è½¬å‹ä¸º&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;å­¦ä¹ èµ›&lt;/a&gt;ï¼Œå…è®¸ä»»ä½•äººå­¦ä¹ ä½¿ç”¨ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2) æ•°æ®å¼€æº&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç›®å‰å¼€æºæ•°æ®æœ‰ &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/reports&#34;&gt;70G/1w+ä»½å¹´æŠ¥æ•°æ®&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/&#34;&gt;10000æ¡äººå·¥æ ‡æ³¨è¯„æµ‹æ•°æ®&lt;/a&gt;ç­‰ã€‚&lt;/li&gt; &#xA; &lt;li&gt;åç»­æˆ‘ä»¬ä¹Ÿå°†æ ¹æ®é¡¹ç›®éœ€æ±‚ï¼ŒæŒç»­è¿­ä»£æ›´æ–°æ•°æ®ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3) æ–¹æ¡ˆ/ä»£ç /æ¨¡å‹å¼€æº&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç»åŒæ„ï¼Œ&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;ç»“å©šä¹°æˆ¿ä»£ä»£éŸ­èœ&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;Chatglmåå·æ€»å±€&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/nsddd&#34;&gt;nsddd&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;é¦’å¤´ç§‘æŠ€&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;å—å“ªéƒ½é˜Ÿ&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;é¾™ç›ˆæˆ˜é˜Ÿ&lt;/a&gt;ã€&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/(./code/finglm_all)&#34;&gt;å®‰ç¡•ç¡•çœ¼æ¢ä¼&lt;/a&gt;ç­‰å›¢é˜Ÿçš„æ–¹æ¡ˆã€ä»£ç ã€æ¨¡å‹å®Œå…¨å¼€æºï¼Œçº³å…¥FinGLMé¡¹ç›®ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æˆ‘ä»¬å°†é•¿æœŸç»´æŠ¤ä¼˜åŒ– &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/finglm_all&#34;&gt;FinGLM é¡¹ç›®&lt;/a&gt;ï¼Œæä¾›ä¾¿æ·è§£å†³æ–¹æ¡ˆã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;4) å¼€æ”¾äº¤æµ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä»¥ä¸Šå›¢é˜Ÿæˆå‘˜å°†å…±åŒç»´æŠ¤ FinGLMé¡¹ç›®ï¼Œç¡®ä¿é¡¹ç›®æŒç»­è¿­ä»£ã€‚æˆ‘ä»¬ä¹Ÿæ¬¢è¿æ›´å¤šå›¢é˜Ÿæ¥å…±åŒè´¡çŒ®é—®é¢˜å’Œæ–¹æ¡ˆã€‚&lt;/li&gt; &#xA; &lt;li&gt;æˆ‘ä»¬å°†ä¸å®šæœŸç»„ç»‡çº¿ä¸Šã€çº¿ä¸‹äº¤æµï¼Œå°†æ›´ä¼˜ç§€çš„æŠ€æœ¯æ¨å¹¿ç»™æ¯ä¸ªé¡¹ç›®æˆå‘˜ã€‚&lt;/li&gt; &#xA; &lt;li&gt;FinGLM å¼€æºé¡¹ç›®å‡ºäºå®Œå…¨å…¬ç›Šç›®çš„ï¼Œæ¬¢è¿æ‰€æœ‰å¼€å‘è€…&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/#%E8%B4%A1%E7%8C%AE%E8%80%85&#34;&gt;ç”³è¯·åŠ å…¥&lt;/a&gt;ï¼Œå½“ç„¶æˆ‘ä»¬ä¼šè¿›è¡Œä¸¥æ ¼å®¡æ ¸ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;5) å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åŸºäºFinGLMé¡¹ç›®çš„å¼€å‘ï¼Œæˆ‘ä»¬å°†æ•´åˆå¹¶åˆ¶ä½œä»¥ä¸‹ï¼ˆåŒ…å«ä¸”ä¸é™äºï¼‰å­¦ä¹ æ•™ç¨‹. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;æ•°æ®é¢„å¤„ç†æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;æ•°æ®åº“ä½¿ç”¨æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;GLMçš„ä½¿ç”¨æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;Promptç¼–å†™æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;æ¨¡å‹å¾®è°ƒæ•°æ®å‡†å¤‡&lt;/li&gt; &#xA;   &lt;li&gt;æ¨¡å‹å¾®è°ƒæŠ€å·§å’Œæ­¥éª¤&lt;/li&gt; &#xA;   &lt;li&gt;å…¨æµç¨‹è½åœ°&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;6) é¡¹ç›®èµ„æºæ± &lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä¸ºäº†ç»´æŒé¡¹ç›®çš„å¥åº·å‘å±•ï¼Œéƒ¨åˆ†é¡¹ç›®ç»„ç»‡è€…ï¼ˆä¸ªäººå’Œä¼ä¸šï¼‰æä¾› 10 ä¸‡å…ƒä½œä¸ºå¼€æºé¡¹ç›®èµ„é‡‘æ± ï¼Œä»¥åŠæä¾›é¡¹ç›®ç®—åŠ›ã€æ•°æ®å’Œæ¨¡å‹æ”¯æŒã€‚&lt;/li&gt; &#xA; &lt;li&gt;æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å—ç›Šäºæœ¬é¡¹ç›®çš„ä¸ªäººæˆ–å•ä½æ¥èµåŠ©æœ¬é¡¹ç›®ï¼ŒåŒ…æ‹¬ä¸”ä¸é™äºä»¥ä¸Šå†…å®¹ï¼Œæ¬¢è¿&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/#%E5%BC%80%E6%BA%90%E8%B5%9E%E5%8A%A9&#34;&gt;è”ç³»æˆ‘ä»¬&lt;/a&gt;ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. å¼€æºè¿›åº¦&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç¬¬ä¸€æœŸï¼š&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;ç»„ç»‡ SMP 2023 ChatGLM é‡‘èå¤§æ¨¡å‹æŒ‘æˆ˜èµ›&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;æ¯”èµ›æ•°æ®é›†å¼€æº&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;å¹´æŠ¥æ•°æ®é›† &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;PDF æ–‡ä»¶&lt;/a&gt;ã€‚åŒ…å« 11588 ä»½ 2019 å¹´è‡³ 2021 å¹´æœŸé—´çš„éƒ¨åˆ†ä¸Šå¸‚å…¬å¸å¹´åº¦æŠ¥å‘Šã€‚&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;TXT æ–‡ä»¶&lt;/a&gt;ã€‚åˆ©ç”¨ &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/tool/pdf_to_txt&#34;&gt;&lt;code&gt;pdf2txt.py&lt;/code&gt;&lt;/a&gt; å¯¹ PDF æ–‡ä»¶è§£æè€Œæ¥ã€‚&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;HTML æ–‡ä»¶&lt;/a&gt;ã€‚&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;æ•°æ®åº“æ¥å…¥ &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; sqlite&lt;/li&gt; &#xA;     &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; mongodb&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; å†³èµ›é¡¹ç›®å¼€æºï¼š &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;é¦’å¤´ç§‘æŠ€&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;å—å“ªéƒ½é˜Ÿ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;Chatglmåå·æ€»å±€&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/nsddd&#34;&gt;nsddd&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;é¾™ç›ˆæˆ˜é˜Ÿ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;ç»“å©šä¹°æˆ¿ä»£ä»£éŸ­èœ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/.code/finglm_all&#34;&gt;å®‰ç¡•ç¡•çœ¼æ¢ä¼&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç¬¬äºŒæœŸï¼š&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¾®è°ƒFintune&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å®Œå–„nl2sql&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¢åŠ å­¦ä¹ æ•™ç¨‹ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ•°æ®é¢„å¤„ç†æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ•°æ®åº“ä½¿ç”¨æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GLMçš„ä½¿ç”¨æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Promptç¼–å†™æ•™ç¨‹&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ¨¡å‹å¾®è°ƒæ•°æ®å‡†å¤‡&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ¨¡å‹å¾®è°ƒæŠ€å·§å’Œæ­¥éª¤&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å…¨æµç¨‹è½åœ°&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; é—®ç­”ç³»ç»Ÿå¼‚å¸¸å¤„ç†&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æä¾›è¯¦ç»†ä½¿ç”¨æ‰‹å†Œ&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; æ–‡æ¡£æ³¨é‡Šå®Œå–„&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ğŸ† å¼€æºé¡¹ç›®&lt;/h2&gt; &#xA;&lt;h3&gt;0. FinGLM_all&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/.slides/%E5%AE%89%E7%A1%95%E7%A1%95%E7%9C%BC%E6%8E%A2%E4%BC%81%E5%88%86%E4%BA%AB%E5%8F%8AFinGLM%E5%BC%80%E6%BA%90%E5%8F%91%E5%B8%83.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1ip4y1F7Gw/&#34;&gt;[è§†é¢‘]&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/finglm_all&#34;&gt;[ä»£ç ]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä¸ºå®‰ç¡•ç¡•çœ¼æ¢ä¼å›¢é˜Ÿï¼Œæ ¹æ®è‡ªå·±çš„é¡¹ç›®ä»¥åŠå…¶ä»–å‡ é˜Ÿçš„é¡¹ç›®æ•´åˆè€Œæˆã€‚åç»­æˆ‘ä»¬ä¹Ÿå°†å›´ç»•æ­¤é¡¹ç›®è¿›è¡ŒæŒç»­è¿­ä»£å‡çº§ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/finglm_all.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. é¦’å¤´ç§‘æŠ€&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV18h4y187UU/&#34;&gt;[è§†é¢‘]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%A6%92%E5%A4%B4%E7%A7%91%E6%8A%80&#34;&gt;[ä»£ç ]&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/mantou.jpg&#34; alt=&#34;mantou&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2. å—å“ªéƒ½é˜Ÿ&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1Gm4y1V7LD/&#34;&gt;[è§†é¢‘]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E5%8D%97%E5%93%AA%E9%83%BD%E9%98%9F&#34;&gt;[ä»£ç ]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/nanna.jpg&#34; alt=&#34;nanna&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. Chatglmåå·æ€»å±€&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/ChatGLM%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1hu4y147EW/&#34;&gt;[è§†é¢‘]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/Chatglm%E5%8F%8D%E5%8D%B7%E6%80%BB%E5%B1%80&#34;&gt;[ä»£ç ]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/chatglmfanjuan.jpg&#34; alt=&#34;chatglmfanjuan&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;4. nsddd&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/nsddd.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV15u4y147Xx&#34;&gt;[è§†é¢‘]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/nsddd&#34;&gt;[ä»£ç ]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/nsddd.jpg&#34; alt=&#34;nsddd&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5. é¾™ç›ˆæˆ˜é˜Ÿ&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1Ju4y167ew&#34;&gt;[è§†é¢‘]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E9%BE%99%E7%9B%88%E6%88%98%E9%98%9F&#34;&gt;[ä»£ç ]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/longying.jpg&#34; alt=&#34;longying&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;6. ç»“å©šä¹°æˆ¿ä»£ä»£éŸ­èœ&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1rm4y1G7uj&#34;&gt;[è§†é¢‘]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/code/%E7%BB%93%E5%A9%9A%E4%B9%B0%E6%88%BF%E4%BB%A3%E4%BB%A3%E9%9F%AD%E8%8F%9C&#34;&gt;[ä»£ç ]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jiehun1.jpg&#34; alt=&#34;jiehun&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jiehun2.jpg&#34; alt=&#34;jiehun2&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;7. TabIsabaopilong&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/TabIsabaopilong.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1894y1a7NJ/&#34;&gt;[è§†é¢‘]&lt;/a&gt; [ä»£ç ]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/tab1.jpg&#34; alt=&#34;tab1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/tab2.jpg&#34; alt=&#34;tab2&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;8. é¥ºå­ç ”ç©¶é™¢&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E5%90%83%E8%BE%A3%E5%AD%90.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV12z4y1V7S3/?spm_id_from=333.999.0.0&amp;amp;vd_source=df16438efe36af5724526b8869fb54c1&#34;&gt;[è§†é¢‘]&lt;/a&gt; [ä»£ç ]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jiaozi.jpg&#34; alt=&#34;jiaozi&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;9. æµå®çœŸäºº&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/slides/%E6%B5%81%E5%AE%9D%E7%9C%9F%E4%BA%BA.pdf&#34;&gt;[PPT]&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1QF411m7ap&#34;&gt;[è§†é¢‘]&lt;/a&gt; [ä»£ç ]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/liubao.jpg&#34; alt=&#34;liubao&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ğŸ“š æ•°æ®é›†&lt;/h2&gt; &#xA;&lt;h3&gt;1. å¹´æŠ¥æ•°æ®é›†&lt;/h3&gt; &#xA;&lt;p&gt;æˆ‘ä»¬å¼€æºçš„æ•°æ®é›†æ¶µç›–äº†2019-2021å¹´æœŸé—´éƒ¨åˆ†ä¸Šå¸‚å…¬å¸çš„å¹´åº¦æŠ¥å‘Šã€‚è¯¥æ•°æ®é›†å…±åŒ…å« 11588 ä¸ªè¯¦å°½çš„ PDF æ–‡ä»¶ï¼ˆ&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/reports/reports_list.csv&#34;&gt;list&lt;/a&gt;ï¼‰ã€‚æ‚¨å¯ä»¥åˆ©ç”¨è¿™äº›PDFæ–‡ä»¶çš„å†…å®¹æ¥æ„å»ºæ‚¨éœ€è¦çš„æ•°æ®åº“æˆ–è€…å‘é‡åº“ã€‚ä¸ºäº†é¿å…è®¡ç®—èµ„æºæµªè´¹ï¼Œæˆ‘ä»¬ä¹Ÿå°†ç›¸åº”çš„æ–‡ä»¶è½¬æ¢æˆ TXTæ–‡ä»¶å’Œ HTMLæ–‡ä»¶ï¼Œä¾›å¤§å®¶ä½¿ç”¨ã€‚&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;PDFä¸‹è½½&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å¤§å°ï¼š69GB æ–‡ä»¶æ ¼å¼ï¼špdfæ–‡ä»¶ æ–‡ä»¶æ•°é‡ï¼š11588&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;gitåŠ è½½&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# è¦æ±‚å®‰è£… git lfs&#xA;git clone http://www.modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;sdkåŠ è½½&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Note: &#xA;# 1. ã€é‡è¦ã€‘è¯·å°†modelscope sdkå‡çº§åˆ°v1.7.2rc0ï¼Œæ‰§è¡Œï¼š pip3 install &#34;modelscope==1.7.2rc0&#34; -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html&#xA;# 2. ã€é‡è¦ã€‘datasetsç‰ˆæœ¬é™åˆ¶ä¸º &amp;gt;=2.8.0, &amp;lt;=2.13.0ï¼Œå¯æ‰§è¡Œï¼š pip3 install datasets==2.13.0&#xA;&#xA;from modelscope.msdatasets import MsDataset&#xA;&#xA;# ä½¿ç”¨æµå¼æ–¹å¼åŠ è½½ã€Œæ¨èã€&#xA;# æ— éœ€å…¨é‡åŠ è½½åˆ°cacheï¼Œéšä¸‹éšå¤„ç†&#xA;# å…¶ä¸­ï¼Œé€šè¿‡è®¾ç½® stream_batch_size å¯ä»¥ä½¿ç”¨batchçš„æ–¹å¼åŠ è½½&#xA;&#xA;ds = MsDataset.load(&#39;chatglm_llm_fintech_raw_dataset&#39;, split=&#39;train&#39;, use_streaming=True, stream_batch_size=1)&#xA;for item in ds:&#xA;    print(item)&#xA;&#xA;# åŠ è½½ç»“æœç¤ºä¾‹ï¼ˆå•æ¡ï¼Œpdf:FILEå­—æ®µå€¼ä¸ºè¯¥pdfæ–‡ä»¶æœ¬åœ°ç¼“å­˜è·¯å¾„ï¼Œæ–‡ä»¶ååšäº†SHAè½¬ç ï¼Œå¯ä»¥ç›´æ¥æ‰“å¼€ï¼‰ &#xA;{&#39;name&#39;: [&#39;2020-03-24__åŒ—äº¬é¼æ±‰æŠ€æœ¯é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸__300011__é¼æ±‰æŠ€æœ¯__2019å¹´__å¹´åº¦æŠ¥å‘Š.pdf&#39;], &#39;pdf:FILE&#39;: [&#39;~/.cache/modelscope/hub/datasets/modelscope/chatglm_llm_fintech_raw_dataset/master/data_files/430da7c46fb80d4d095a57b4fb223258ffa1afe8bf53d0484e3f2650f5904b5c&#39;]}&#xA;&#xA;&#xA;# å¤‡æ³¨: &#xA;1. è‡ªå®šä¹‰ç¼“å­˜è·¯å¾„ï¼Œå¯ä»¥è‡ªè¡Œè®¾ç½®cache_dirå‚æ•°ï¼Œå³ MsDataset.load(..., cache_dir=&#39;/to/your/path&#39;)&#xA;2. è¡¥å……æ•°æ®åŠ è½½ï¼ˆä»9493æ¡å¢åŠ åˆ°11588æ¡ï¼‰ï¼ŒsdkåŠ è½½æ³¨æ„äº‹é¡¹&#xA;    a) åˆ é™¤ç¼“å­˜ä¸­çš„csvæ˜ å°„æ–‡ä»¶(é»˜è®¤è·¯å¾„ä¸º)ï¼š ~/.cache/modelscope/hub/datasets/modelscope/chatglm_llm_fintech_raw_dataset/master/data_files/732dc4f3b18fc52380371636931af4c8&#xA;    b) ä½¿ç”¨MsDataset.load(...) åŠ è½½ï¼Œé»˜è®¤ä¼šreuseå·²ä¸‹è½½è¿‡çš„æ–‡ä»¶ï¼Œä¸ä¼šé‡å¤ä¸‹è½½ã€‚&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary&#34;&gt;TXTä¸‹è½½&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Note: pdfè½¬txtæ ¼å¼æ–‡ä»¶ï¼Œæ–¹ä¾¿å¤§å®¶å¤ç”¨ï¼ˆæœ‰ä¸ªæ–‡ä»¶æŸåäº†ï¼Œæ‰€ä»¥æ€»æ•°æ¯”pdfå°‘1ä¸ªï¼Œå…±11587 ä¸ªï¼‰&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Linux&#xA;wget https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/alltxt.zip&#xA;&#xA;# Windowsç¤ºä¾‹&#xA;Invoke-WebRequest -Uri https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/alltxt.zip -OutFile D:\\alltxt.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;HTMLä¸‹è½½&lt;/h4&gt; &#xA;&lt;p&gt;Note: pdfè½¬htmlæ ¼å¼æ–‡ä»¶ï¼Œæ–¹ä¾¿å¤§å®¶å¤ç”¨ï¼ˆæœ‰ä¸ªæ–‡ä»¶æŸåäº†ï¼Œæ‰€ä»¥æ€»æ•°æ¯”pdfå°‘ï¼Œå…±11582 ä¸ªï¼‰&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Linux&#xA;wget https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/allhtml.zip&#xA;&#xA;# Windowsç¤ºä¾‹&#xA;Invoke-WebRequest -Uri https://sail-moe.oss-cn-hangzhou.aliyuncs.com/open_data/hackathon_chatglm_fintech/allhtml.zip -OutFile D:\\allhtml.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;ä½¿ç”¨å»ºè®®&lt;/h4&gt; &#xA;&lt;p&gt;ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ¨èçš„å¤„ç†æ­¥éª¤ï¼š&lt;/p&gt; &#xA;&lt;p&gt;1ã€PDFæ–‡æœ¬å’Œè¡¨æ ¼æå–ï¼šæ‚¨å¯ä»¥ä½¿ç”¨å¦‚pdfplumberã€pdfminerç­‰å·¥å…·åŒ…æå–PDFæ–‡ä»¶ä¸­çš„æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;2ã€æ•°æ®åˆ‡åˆ†ï¼šæ ¹æ®PDFæ–‡ä»¶çš„ç›®å½•ã€å­ç›®å½•å’Œç« èŠ‚ä¿¡æ¯ï¼Œå¯¹å†…å®¹è¿›è¡Œç²¾ç¡®çš„åˆ‡å—å¤„ç†ã€‚&lt;/p&gt; &#xA;&lt;p&gt;3ã€æ„å»ºåŸºç¡€é‡‘èæ•°æ®åº“ï¼šä¾æ®é‡‘èçŸ¥è¯†å’ŒPDFå†…å®¹ï¼Œè®¾è®¡ä¸“ä¸šçš„é‡‘èæ•°æ®åº“å­—æ®µå’Œæ ¼å¼ã€‚ä¾‹å¦‚ï¼Œå®šä¹‰èµ„äº§è´Ÿå€ºè¡¨ã€ç°é‡‘æµé‡è¡¨å’Œåˆ©æ¶¦è¡¨ç­‰ã€‚&lt;/p&gt; &#xA;&lt;p&gt;4ã€ä¿¡æ¯æå–ï¼šä½¿ç”¨å¤§æ¨¡å‹çš„ä¿¡æ¯æå–èƒ½åŠ›å’ŒNLPæŠ€æœ¯æ¥æŠ½å–å¯¹åº”çš„é‡‘èå­—æ®µä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œè¯·ä½¿ç”¨jsonæ–¹å¼è¾“å‡ºç›®å½•çš„å†…å®¹ï¼Œå…¶ä¸­ç« èŠ‚çš„åç§°ä½œä¸ºkeyï¼Œé¡µç ä½œä¸ºvalueã€‚åŒæ—¶ï¼Œè¯·è¯¦ç»†åœ°æŠ½å–è¡¨æ ¼å†…çš„æ•°æ®ï¼Œä»¥JSONæ ¼å¼è¾“å‡ºã€‚&lt;/p&gt; &#xA;&lt;p&gt;5ã€æ„å»ºé‡‘èçŸ¥è¯†é—®ç­”åº“ï¼šç»“åˆæ„å»ºçš„é‡‘èæ•°æ®åº“ï¼Œåº”ç”¨å¤§æ¨¡å‹æ„å»ºåŸºç¡€çš„é‡‘èé—®ç­”åº“ã€‚ä¾‹å¦‚ï¼Œ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#34;question&#34;ï¼š&#34;æŸå…¬å¸2021å¹´çš„è´¢åŠ¡è´¹ç”¨ä¸ºå¤šå°‘å…ƒï¼Ÿ&#34;, &#34;answer&#34;: &#34;æŸå…¬å¸2021å¹´çš„è´¢åŠ¡è´¹ç”¨ä¸ºXXXXå…ƒã€‚&#34;}&#xA;prompt:ç”¨å¤šç§å¥å¼ä¿®æ”¹questionåŠanswerçš„å†…å®¹ã€‚&#xA;&#xA;{&#34;question&#34;:&#34;ä¸ºä»€ä¹ˆè´¢åŠ¡è´¹ç”¨å¯ä»¥æ˜¯è´Ÿçš„ï¼Ÿ&#34;, &#34;answer&#34;: &#34;&#34;}&#xA;promptï¼šè¯·æ¨¡ä»¿ä¸Šé¢çš„questionç»™å‡º100ä¸ªç±»ä¼¼çš„é—®é¢˜ä¸å¯¹åº”çš„ç­”æ¡ˆï¼Œç”¨jsonè¾“å‡ºã€‚&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;6ã€æ„å»ºå‘é‡åº“ï¼šå€ŸåŠ©äºå¦‚Word2Vecã€Text2Vecç­‰æŠ€æœ¯ï¼Œä»åŸå§‹æ–‡æœ¬æ•°æ®ä¸­æå–å‡ºè¯­ä¹‰å‘é‡ã€‚ä½¿ç”¨pgvectorè¿™ç§åŸºäºPostgreSQLçš„æ‰©å±•æ¥å­˜å‚¨å’Œç´¢å¼•è¿™äº›å‘é‡ï¼Œä»è€Œå»ºç«‹èµ·ä¸€ä¸ªå¯ä¾›é«˜æ•ˆæŸ¥è¯¢çš„å¤§è§„æ¨¡å‘é‡åº“ã€‚&lt;/p&gt; &#xA;&lt;p&gt;7ã€åº”ç”¨ï¼šç»“åˆå‘é‡åº“ã€å¤§æ¨¡å‹ã€langchainç­‰å·¥å…·ï¼Œæå‡åº”ç”¨æ•ˆæœã€‚&lt;/p&gt; &#xA;&lt;h3&gt;2. æ ‡æ³¨æ•°æ®&lt;/h3&gt; &#xA;&lt;p&gt;åœ¨ &lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/532126&#34;&gt;SMP 2023 ChatGLM é‡‘èå¤§æ¨¡å‹æŒ‘æˆ˜èµ›&lt;/a&gt; ä¸­æˆ‘ä»¬åˆ†åˆ«è¿›è¡Œäº†åˆèµ›ã€å¤èµ›Aã€å¤èµ›Bã€å¤èµ›Cã€‚é’ˆå¯¹è¿™å‡ è½®æ¯”èµ›ï¼Œæˆ‘ä»¬åˆ†åˆ«äººå·¥æ ‡æ³¨äº†ç›¸å…³æ•°æ®ï¼Œç´¯è®¡æ€»å…±æœ‰ 10000 æ¡ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/pre-data&#34;&gt;[åˆèµ›æ•°æ®]&lt;/a&gt; ï¼š5000 æ¡&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/A-data&#34;&gt;[å¤èµ› A æ•°æ®]&lt;/a&gt; ï¼š2000 æ¡&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/B-data&#34;&gt;[å¤èµ› B æ•°æ®]&lt;/a&gt; ï¼š2000 æ¡&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/C-data&#34;&gt;[å¤èµ› C æ•°æ®]&lt;/a&gt; ï¼š1000 æ¡&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ•°æ®ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#34;ID&#34;: 1,&#xA;&#34;question&#34;: &#34;2019å¹´ä¸­å›½å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯å¤šå°‘å…ƒ?&#34;,&#xA;&#34;answer&#34;:&#34;2019å¹´ä¸­å›½å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚&#34;}&#xA;&#xA;{&#34;ID&#34;: 2,&#xA;&#34;question&#34;: &#34;å·¥å•†é“¶è¡Œ2019å¹´è¥ä¸šå¤–æ”¯å‡ºå’Œè¥ä¸šå¤–æ”¶å…¥åˆ†åˆ«æ˜¯å¤šå°‘å…ƒ?&#34;,&#xA;&#34;answer&#34;: &#34;å·¥å•†é“¶è¡Œ2019å¹´è¥ä¸šå¤–æ”¯å‡ºä¸º12345678.9å…ƒï¼Œè¥ä¸šå¤–æ”¶å…¥ä¸º2345678.9å…ƒã€‚&#34;}&#xA;&#xA;{&#34;ID&#34;:3,&#xA;&#34;question&#34;: &#34;ä¸­å›½å·¥å•†é“¶è¡Œ2021å¹´å‡€åˆ©æ¶¦å¢é•¿ç‡æ˜¯å¤šå°‘?ä¿ç•™2ä½å°æ•°ã€‚&#34;,&#xA;&#34;answer&#34;: &#34;ä¸­å›½å·¥å•†é“¶è¡Œ2020å¹´å‡€åˆ©æ¶¦ä¸º12345678.90å…ƒï¼Œ2021å¹´å‡€åˆ©æ¶¦ä¸º22345678.90å…ƒï¼Œæ ¹æ®å…¬å¼ï¼Œå‡€åˆ©æ¶¦å¢é•¿ç‡=(å‡€åˆ©æ¶¦-ä¸Šå¹´å‡€åˆ©æ¶¦)/ä¸Šå¹´å‡€åˆ©æ¶¦ï¼Œå¾—å‡ºç»“æœä¸­å›½å·¥å•†é“¶è¡Œ2021å¹´å‡€åˆ©æ¶¦å¢é•¿ç‡81.00%ã€‚&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿé’ˆå¯¹æ¯”èµ›æ’°å†™äº†&lt;a href=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/data/evaluate.py&#34;&gt;è¯„æµ‹ä»£ç &lt;/a&gt;ã€‚æˆ‘ä»¬ä¾æ®ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;&#xA;\begin{array}{ll}&#xA;max_{similar}(sentence1,sentence2,sentence3), &amp;amp; æ— åŸºç¡€ä¿¡æ¯åŠå…³é”®è¯\\&#xA;0.25+0.25+max_{similar}(sentence1,sentence2,sentence3)*0.5, &amp;amp; åŸºç¡€ä¿¡æ¯æ­£ç¡®ï¼Œå…³é”®è¯æ­£ç¡® \\&#xA;0.25 + 0 + max_{similar}(sentence1,sentence2,sentence3)*0.5, &amp;amp; åŸºç¡€ä¿¡æ¯æ­£ç¡®ï¼Œå…³é”®è¯é”™è¯¯\\&#xA;0, &amp;amp; åŸºç¡€ä¿¡æ¯é”™è¯¯&#xA;&#xA;\end{array}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è¯„æµ‹ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#34;question&#34;: &#34;2019å¹´ä¸­å›½å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯å¤šå°‘å…ƒ?&#34;,&#xA;&#xA;&#34;prompt&#34;: {&#34;è´¢åŠ¡è´¹ç”¨&#34;: &#34;12345678.9å…ƒ&#34;, &#34;key_word&#34;:&#34;è´¢åŠ¡è´¹ç”¨ã€2019&#34;, &#34;prom_answer&#34;: &#34;12345678.9å…ƒ&#34;},&#xA;&#xA;&#34;answer&#34;: [&#xA;&#xA;&#34;2019å¹´ä¸­å›½å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚&#34;,&#xA;&#xA;&#34;2019å¹´å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚&#34;,&#xA;&#xA;&#34;ä¸­å›½å·¥å•†é“¶è¡Œ2019å¹´çš„è´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚&#34; ]&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è¯„æµ‹è®¡ç®—ç¤ºä¾‹ï¼š&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç­”æ¡ˆä¸€ï¼šå·¥å•†é“¶è¡Œ2019å¹´è´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;most similar sentences:&lt;/p&gt; &#xA;&lt;p&gt;2019å¹´å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚ (Score: 0.9915)&lt;/p&gt; &#xA;&lt;p&gt;ä¸­å›½å·¥å•†é“¶è¡Œ2019å¹´çš„è´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚ (Score: 0.9820)&lt;/p&gt; &#xA;&lt;p&gt;2019å¹´ä¸­å›½å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚ (Score: 0.9720)&lt;/p&gt; &#xA;&lt;p&gt;è¯„åˆ†ï¼š0.25+0.25+0.9915*0.5=0.9958åˆ†ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;è¯„åˆ†è§£é‡Šï¼šprom_answeræ­£ç¡®ã€åŒ…å«æ‰€æœ‰key_wordã€ç›¸ä¼¼åº¦æœ€é«˜0.9915ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç­”æ¡ˆäºŒï¼š2019å¹´ä¸­å›½å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯335768.91å…ƒã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;è¯„åˆ†ï¼š0åˆ†ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;è¯„åˆ†è§£é‡Šï¼šprom_answeré”™è¯¯ä¸å¾—åˆ†ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç­”æ¡ˆä¸‰ï¼š12345678.9å…ƒã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;most similar sentences:&lt;/p&gt; &#xA;&lt;p&gt;2019å¹´å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚ (Score: 0.6488)&lt;/p&gt; &#xA;&lt;p&gt;2019å¹´ä¸­å›½å·¥å•†é“¶è¡Œè´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚ (Score: 0.6409)&lt;/p&gt; &#xA;&lt;p&gt;ä¸­å›½å·¥å•†é“¶è¡Œ2019å¹´çš„è´¢åŠ¡è´¹ç”¨æ˜¯12345678.9å…ƒã€‚ (Score: 0.6191)&lt;/p&gt; &#xA;&lt;p&gt;è¯„åˆ†ï¼š0.25+0+0.6488*0.5=0.5744åˆ†ã€‚&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;è¯„åˆ†è§£é‡Šï¼šprom_answeræ­£ç¡®ã€æœªåŒ…å«æ‰€æœ‰key_wordã€ç›¸ä¼¼åº¦æœ€é«˜0.6488ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ğŸ“Š é¡¹ç›®é—®ç­”æ¼”ç¤º&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#34;id&#34;: 0, &#34;question&#34;: &#34;2021å¹´å…¶ä»–æµåŠ¨èµ„äº§ç¬¬12é«˜çš„æ˜¯å“ªå®¶ä¸Šå¸‚å…¬å¸ï¼Ÿ&#34;, &#34;answer&#34;: &#34;2021å¹´å…¶ä»–æµåŠ¨èµ„äº§ç¬¬12é«˜çš„å…¬å¸æ˜¯è‹ç¾è¾¾è‚¡ä»½æœ‰é™å…¬å¸ã€‚&#34;}&#xA;{&#34;id&#34;: 1, &#34;question&#34;: &#34;æ³¨å†Œåœ°å€åœ¨é‡åº†çš„ä¸Šå¸‚å…¬å¸ä¸­ï¼Œ2021å¹´è¥ä¸šæ”¶å…¥å¤§äº5äº¿çš„æœ‰å¤šå°‘å®¶ï¼Ÿ&#34;, &#34;answer&#34;: &#34;2021å¹´æ³¨å†Œåœ¨é‡åº†ï¼Œè¥ä¸šæ”¶å…¥å¤§äº5äº¿çš„å…¬å¸ä¸€å…±æœ‰4å®¶ã€‚&#34;}&#xA;{&#34;id&#34;: 2, &#34;question&#34;: &#34;å¹¿ä¸œåç‰¹æ°”ä½“è‚¡ä»½æœ‰é™å…¬å¸2021å¹´çš„èŒå·¥æ€»äººæ•°ä¸ºï¼Ÿ&#34;, &#34;answer&#34;: &#34;2021å¹´å¹¿ä¸œåç‰¹æ°”ä½“è‚¡ä»½æœ‰é™å…¬å¸èŒå·¥æ€»äººæ•°æ˜¯1044äººã€‚&#34;}&#xA;{&#34;id&#34;: 3, &#34;question&#34;: &#34;åœ¨ä¿ç•™ä¸¤ä½å°æ•°çš„æƒ…å†µä¸‹ï¼Œè¯·è®¡ç®—å‡ºé‡‘é’¼è‚¡ä»½2019å¹´çš„æµåŠ¨è´Ÿå€ºæ¯”ç‡&#34;, &#34;answer&#34;: &#34;2019é‡‘é’¼è‚¡ä»½æµåŠ¨è´Ÿå€ºæ¯”ç‡æ˜¯61.10%ã€‚å…¶ä¸­æµåŠ¨è´Ÿå€ºæ˜¯1068418275.97å…ƒï¼›æ€»è´Ÿå€ºæ˜¯1748627619.69å…ƒï¼›&#34;}&#xA;{&#34;id&#34;: 4, &#34;question&#34;: &#34;2019å¹´è´Ÿå€ºæ€»é‡‘é¢æœ€é«˜çš„ä¸Šå¸‚å…¬å¸ä¸ºï¼Ÿ&#34;, &#34;answer&#34;: &#34;2019å¹´è´Ÿå€ºåˆè®¡æœ€é«˜çš„æ˜¯ä¸Šæµ·æ±½è½¦é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸ã€‚&#34;}&#xA;{&#34;id&#34;: 5, &#34;question&#34;: &#34;2019å¹´æ€»èµ„äº§æœ€é«˜çš„å‰äº”å®¶ä¸Šå¸‚å…¬å¸æ˜¯å“ªäº›å®¶ï¼Ÿ&#34;, &#34;answer&#34;: &#34;2019å¹´èµ„äº§æ€»è®¡æœ€é«˜å‰äº”å®¶æ˜¯ä¸Šæµ·æ±½è½¦é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸ã€ä¸­è¿œæµ·è¿æ§è‚¡è‚¡ä»½æœ‰é™å…¬å¸ã€å›½æŠ•ç”µåŠ›æ§è‚¡è‚¡ä»½æœ‰é™å…¬å¸ã€ååŸŸæ±½è½¦ç³»ç»Ÿè‚¡ä»½æœ‰é™å…¬å¸ã€å¹¿å·æ±½è½¦é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸ã€‚&#34;}&#xA;{&#34;id&#34;: 6, &#34;question&#34;: &#34;2020å¹´è¥ä¸šæ”¶å…¥æœ€é«˜çš„3å®¶å¹¶ä¸”æ›¾ç»åœ¨å®æ³¢æ³¨å†Œçš„ä¸Šå¸‚å…¬å¸æ˜¯ï¼Ÿé‡‘é¢æ˜¯ï¼Ÿ&#34;, &#34;answer&#34;: &#34;æ³¨å†Œåœ¨å®æ³¢ï¼Œ2020å¹´è¥ä¸šæ”¶å…¥æœ€é«˜çš„3å®¶æ˜¯å®æ³¢å‡èƒœç”µå­è‚¡ä»½æœ‰é™å…¬å¸è¥ä¸šæ”¶å…¥47889837616.15å…ƒï¼›å®æ³¢å»ºå·¥è‚¡ä»½æœ‰é™å…¬å¸è¥ä¸šæ”¶å…¥19796854240.57å…ƒï¼›å®æ³¢ç»§å³°æ±½è½¦é›¶éƒ¨ä»¶è‚¡ä»½æœ‰é™å…¬å¸è¥ä¸šæ”¶å…¥15732749552.37å…ƒã€‚&#34;}&#xA;{&#34;id&#34;: 7, &#34;question&#34;: &#34;æ³¨å†Œåœ°å€åœ¨è‹å·çš„ä¸Šå¸‚å…¬å¸ä¸­ï¼Œ2020å¹´åˆ©æ¶¦æ€»é¢å¤§äº5äº¿çš„æœ‰å¤šå°‘å®¶ï¼Ÿ&#34;, &#34;answer&#34;: &#34;2020å¹´æ³¨å†Œåœ¨è‹å·ï¼Œåˆ©æ¶¦æ€»é¢å¤§äº5äº¿çš„å…¬å¸ä¸€å…±æœ‰2å®¶ã€‚&#34;}&#xA;{&#34;id&#34;: 8, &#34;question&#34;: &#34;æµ™æ±Ÿè¿è¾¾é£ç”µè‚¡ä»½æœ‰é™å…¬å¸åœ¨2019å¹´çš„æ—¶å€™åº”æ”¶æ¬¾é¡¹èèµ„æ˜¯å¤šå°‘å…ƒï¼Ÿ&#34;, &#34;answer&#34;: &#34;2019å¹´æµ™æ±Ÿè¿è¾¾é£ç”µè‚¡ä»½æœ‰é™å…¬å¸åº”æ”¶æ¬¾é¡¹èèµ„æ˜¯51086824.07å…ƒã€‚&#34;}&#xA;{&#34;id&#34;: 9, &#34;question&#34;: &#34;ç¥é©°æœºç”µè‚¡ä»½æœ‰é™å…¬å¸2020å¹´çš„æ³¨å†Œåœ°å€ä¸ºï¼Ÿ&#34;, &#34;answer&#34;: &#34;2020å¹´ç¥é©°æœºç”µè‚¡ä»½æœ‰é™å…¬å¸æ³¨å†Œåœ°å€æ˜¯é‡åº†å¸‚åŒ—ç¢šåŒºç«¥å®¶æºªé•‡åŒå…´åŒ—è·¯200å·ã€‚&#34;}&#xA;{&#34;id&#34;: 10, &#34;question&#34;: &#34;2019å¹´å±±ä¸œæƒ å‘é£Ÿå“è‚¡ä»½æœ‰é™å…¬å¸è¥ä¸šå¤–æ”¯å‡ºå’Œè¥ä¸šå¤–æ”¶å…¥åˆ†åˆ«æ˜¯å¤šå°‘å…ƒï¼Ÿ&#34;, &#34;answer&#34;: &#34;2019å¹´å±±ä¸œæƒ å‘é£Ÿå“è‚¡ä»½æœ‰é™å…¬å¸è¥ä¸šå¤–æ”¶å…¥æ˜¯1018122.97å…ƒï¼›è¥ä¸šå¤–æ”¯å‡ºæ˜¯2513885.46å…ƒã€‚&#34;}&#xA;{&#34;id&#34;: 11, &#34;question&#34;: &#34;ç¦å»ºå¹¿ç”Ÿå ‚è¯ä¸šè‚¡ä»½æœ‰é™å…¬å¸2020å¹´å¹´æŠ¥ä¸­æåŠçš„è´¢åŠ¡è´¹ç”¨å¢é•¿ç‡å…·ä½“æ˜¯ä»€ä¹ˆï¼Ÿ&#34;, &#34;answer&#34;: &#34;2020ç¦å»ºå¹¿ç”Ÿå ‚è¯ä¸šè‚¡ä»½æœ‰é™å…¬å¸è´¢åŠ¡è´¹ç”¨å¢é•¿ç‡æ˜¯34.33%ã€‚å…¶ä¸­ï¼Œè´¢åŠ¡è´¹ç”¨æ˜¯7766850.48å…ƒï¼›ä¸Šå¹´è´¢åŠ¡è´¹ç”¨æ˜¯5781839.51å…ƒã€‚&#34;}&#xA;{&#34;id&#34;: 12, &#34;question&#34;: &#34;åç¿å…‰ç”µè‚¡ä»½æœ‰é™å…¬å¸2021å¹´çš„æ³•å®šä»£è¡¨äººä¸ä¸Šå¹´ç›¸æ¯”ç›¸åŒå—ï¼Ÿ&#34;, &#34;answer&#34;: &#34;ä¸ç›¸åŒï¼Œåç¿å…‰ç”µè‚¡ä»½æœ‰é™å…¬å¸2020å¹´æ³•å®šä»£è¡¨äººæ˜¯ä¿ä¿¡åï¼Œ2021å¹´æ³•å®šä»£è¡¨äººæ˜¯éƒ­ç‘¾ã€‚&#34;}&#xA;{&#34;id&#34;: 13, &#34;question&#34;: &#34;è¯·å…·ä½“æè¿°ä¸€ä¸‹2020å¹´ä»²æ™¯é£Ÿå“æ§è‚¡è‚¡ä¸œæ˜¯å¦å‘ç”Ÿå˜æ›´ã€‚&#34;, &#34;answer&#34;: &#34;2020å¹´ï¼Œä»²æ™¯é£Ÿå“æ§è‚¡è‚¡ä¸œæ²¡æœ‰å‘ç”Ÿå˜æ›´ã€‚&#34;}&#xA;{&#34;id&#34;: 14, &#34;question&#34;: &#34;ä»€ä¹ˆæ˜¯å…¶ä»–å€ºæƒæŠ•èµ„ï¼Ÿ&#34;, &#34;answer&#34;: &#34;å…¶ä»–å€ºæƒæŠ•èµ„æ˜¯æŒ‡ä¼ä¸šæˆ–æœºæ„æŠ•èµ„è€…é€šè¿‡è´­ä¹°å€ºåˆ¸ã€è´·æ¬¾ã€å®šæœŸå­˜æ¬¾ç­‰é‡‘èäº§å“è·å¾—çš„å›ºå®šæ”¶ç›Šã€‚è¿™äº›é‡‘èäº§å“é€šå¸¸ç”±æ”¿åºœã€å…¬å¸æˆ–å…¶ä»–æœºæ„å‘è¡Œï¼Œå…·æœ‰ä¸€å®šçš„ä¿¡ç”¨ç­‰çº§å’Œé£é™©ã€‚\n\nå…¶ä»–å€ºæƒæŠ•èµ„æ˜¯ä¼ä¸šæˆ–æœºæ„æŠ•èµ„ç»„åˆä¸­çš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸ç”¨äºç¨³å®šæ”¶ç›Šå’Œåˆ†æ•£é£é™©ã€‚ä¸è‚¡ç¥¨æŠ•èµ„ç›¸æ¯”ï¼Œå…¶ä»–å€ºæƒæŠ•èµ„çš„é£é™©è¾ƒä½ï¼Œä½†æ”¶ç›Šä¹Ÿç›¸å¯¹è¾ƒä½ã€‚\n\nå…¶ä»–å€ºæƒæŠ•èµ„çš„ç®¡ç†å’ŒæŠ•èµ„ç­–ç•¥ä¸å…¶ä»–èµ„äº§ç±»åˆ«ç±»ä¼¼ï¼ŒåŒ…æ‹¬åˆ†æ•£æŠ•èµ„ã€é£é™©æ§åˆ¶ã€æ”¶ç›Šæœ€å¤§åŒ–ç­‰ã€‚ç„¶è€Œï¼Œç”±äºå…¶ä»–å€ºæƒæŠ•èµ„çš„ç§ç±»ç¹å¤šï¼Œå…¶æŠ•èµ„å’Œç®¡ç†ä¹Ÿå­˜åœ¨ä¸€å®šçš„ç‰¹æ®Šæ€§ã€‚&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ¤ è´¡çŒ®è€…&lt;/h2&gt; &#xA;&lt;p&gt;ä»¥ä¸‹æ˜¯ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å›¢é˜Ÿå’Œä¸ªäººï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸŒŸ å®‰ç¡•ç¡•çœ¼æ¢ä¼&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ é¦’å¤´ç§‘æŠ€&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ å—å“ªéƒ½é˜Ÿ&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ Chatglmåå·æ€»å±€&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ nsddd&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ é¾™ç›ˆæˆ˜é˜Ÿ&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ ç»“å©šä¹°æˆ¿ä»£ä»£éŸ­èœ&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ å°æ‰“å°é—¹&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ ä¸œåŒ—å¤§åœŸè±†&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒŸ ... æ›´å¤šè´¡çŒ®è€…&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;FinGLM å¼€æºé¡¹ç›®å‡ºäºå®Œå…¨å…¬ç›Šç›®çš„ï¼Œæ¬¢è¿æ‰€æœ‰å¼€å‘è€…ç”³è¯·åŠ å…¥ï¼Œå½“ç„¶æˆ‘ä»¬ä¼šè¿›è¡Œä¸¥æ ¼å®¡æ ¸ã€‚å¦‚æœ‰æ„å‘ï¼Œè¯·å¡«å†™ &lt;a href=&#34;https://lslfd0slxc.feishu.cn/share/base/form/shrcncipvYdAVitiTqNqxwIjglc&#34;&gt;è¡¨å•&lt;/a&gt; ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å…è´£å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ç›¸å…³èµ„æºä»…ä¾›ç ”ç©¶ã€äº¤æµä½¿ç”¨ï¼Œä¸€èˆ¬ä¸å»ºè®®ç”¨äºå•†ä¸šç”¨é€”ï¼›å¦‚ç”¨äºå•†ä¸šç”¨é€”ï¼Œç”±æ­¤æ‰€å¸¦æ¥çš„æ³•å¾‹é£é™©ï¼Œè¯·è‡ªè¡Œæ‰¿æ‹…ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æ¶‰åŠåˆ°æ¨¡å‹å•†ä¸šä½¿ç”¨é—®é¢˜ï¼Œè¯·åŠ¡å¿…éµå¾ªç›¸å…³æ¨¡å‹çš„åè®®ï¼Œä¾‹å¦‚ &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM-6B&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ” é¡¹ç›®è”ç³»&lt;/h2&gt; &#xA;&lt;h3&gt;é¡¹ç›®äº¤æµç¾¤&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/qun.png&#34; alt=&#34;é¡¹ç›®ç­”ç–‘ç¾¤&#34; width=&#34;200&#34; height=&#34;200&#34; title=&#34;é¡¹ç›®ç­”ç–‘ç¾¤&#34;&gt; &#xA;&lt;h3&gt;å¼€æºèµåŠ©&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jw.jpg&#34; alt=&#34;å¼€æºèµåŠ©è”ç³»äºº&#34; width=&#34;200&#34; height=&#34;200&#34; title=&#34;å¼€æºèµåŠ©è”ç³»äºº&#34;&gt; &#xA;&lt;h3&gt;å¼€æºè´¡çŒ®&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MetaGLM/FinGLM/main/img/jl.png&#34; alt=&#34;å¼€æºè´¡çŒ®è”ç³»äºº&#34; width=&#34;200&#34; height=&#34;200&#34; title=&#34;å¼€æºè´¡çŒ®è”ç³»äºº&#34;&gt;</summary>
  </entry>
  <entry>
    <title>okuvshynov/slowllama</title>
    <updated>2023-10-11T01:37:46Z</updated>
    <id>tag:github.com,2023-10-11:/okuvshynov/slowllama</id>
    <link href="https://github.com/okuvshynov/slowllama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Finetune llama2-70b and codellama on MacBook Air without quantization&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;slowllama&lt;/h2&gt; &#xA;&lt;p&gt;Fine-tune Llama2 and CodeLLama models, including 70B/35B on Apple M1/M2 devices (for example, Macbook Air or Mac Mini) or consumer nVidia GPUs.&lt;/p&gt; &#xA;&lt;p&gt;slowllama is not using any quantization. Instead, it offloads parts of model to SSD or main memory on both forward/backward passes. In contrast with training large models from scratch (unattainable) or inference, where we are likely to care about interactivity, we can still get something finetuned if you let it run for a while.&lt;/p&gt; &#xA;&lt;p&gt;Current version is using LoRA to limit the updates to a smaller set of parameters. First version supported full finetuning as well, but I decided to remove it for now, more on that below.&lt;/p&gt; &#xA;&lt;p&gt;Finetuning is the only focus, there&#39;s nothing special done for inference, consider &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For CUDA-specific experiments, see &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/docs/a10.md&#34;&gt;report on a10&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It is all very experimental, but even more so for CUDA.&lt;/p&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;p&gt;Tests were done on Apple M1 with 16Gb memory and Apple M2 with 24Gb memory.&lt;/p&gt; &#xA;&lt;p&gt;In order to fine-tune llama2 model we need to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies: &lt;code&gt;pip install torch sentencepiece numpy&lt;/code&gt;. Optional: install &lt;code&gt;pip install fewlines&lt;/code&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/docs/lora_weights.md&#34;&gt;weight/gradient distribution logging&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;llama2&lt;/a&gt; and follow instructions to download the models. The script will download tokenizer as well. &lt;code&gt;tokenizer.model&lt;/code&gt; should be put into the same directory as llama model itself. Use &lt;a href=&#34;https://github.com/facebookresearch/codellama&#34;&gt;codellama&lt;/a&gt; for CodeLLama models. Example folder structure could look like:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;/parent/&#xA;    /slowllama/...   # &amp;lt;- this repo&#xA;    /codellama/...   # &amp;lt;-- this is Meta&#39;s codellama repository.&#xA;    /llama-2-7b/...  # &amp;lt;- put tokenizer.model here&#xA;    /llama-2-13b/... # &amp;lt;- and here&#xA;    /llama-2-70b/... # &amp;lt;- and here as well&#xA;    /CodeLlama-34b-Python/... # and here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s start with a &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/test_data/cubestat.txt&#34;&gt;tiny example&lt;/a&gt;. It is an intro to the description of another open-source project - &lt;a href=&#34;https://github.com/okuvshynov/cubestat&#34;&gt;cubestat&lt;/a&gt;. Text is short enough to just be included as part of the prompt, but it&#39;s ok as an illustration and you can read it in seconds youself. As I just published that project recently, there&#39;s no way original llama would know anything about it.&lt;/p&gt; &#xA;&lt;p&gt;Asking base llama2-7b to complete the prompt &lt;em&gt;&#34;Cubestat reports the following metrics: &#34;&lt;/em&gt; results in &lt;em&gt;&#34;1) the number of cubes in the system, 2) the number of cubes that are in the process of being created&#34;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;First step is to transform the model to the sequential format more suitable for loading to/from storage block-by-block.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python prepare_model.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify the input/output paths in the script itself.&lt;/p&gt; &#xA;&lt;p&gt;Now we can try not-finetuned llama2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_gen.py ../llama7b mps # use path to transformed model here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now let&#39;s finetune the 7b model. &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/finetune.py&#34;&gt;finetune.py&lt;/a&gt; is a very simple script which trains LoRA weights based on the plaintext data. There are some settings you could change here, like sequence length, batch size, learning rate, dropout rate, number of iterations. Current settings are pretty much a guess, change this if desired. Adjust accordingly. Currently it uses AdamW optimizer.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python finetune.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s train dataset loss:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2023-09-10 22:05:35,569 backprop done, loss after forward pass = 2.9539270401000977&#xA;2023-09-10 22:06:08,022 backprop done, loss after forward pass = 2.9073102474212646&#xA;2023-09-10 22:06:40,223 backprop done, loss after forward pass = 2.7192320823669434&#xA;2023-09-10 22:07:12,468 backprop done, loss after forward pass = 2.7223477363586426&#xA;2023-09-10 22:07:44,626 backprop done, loss after forward pass = 2.5889995098114014&#xA;2023-09-10 22:08:16,899 backprop done, loss after forward pass = 2.4459967613220215&#xA;2023-09-10 22:08:49,072 backprop done, loss after forward pass = 2.3632657527923584&#xA;2023-09-10 22:09:21,335 backprop done, loss after forward pass = 2.250361442565918&#xA;2023-09-10 22:09:53,511 backprop done, loss after forward pass = 2.165428638458252&#xA;2023-09-10 22:10:25,738 backprop done, loss after forward pass = 2.031874656677246&#xA;2023-09-10 22:13:45,794 backprop done, loss after forward pass = 1.8926434516906738&#xA;2023-09-10 22:14:18,049 backprop done, loss after forward pass = 1.7222942113876343&#xA;2023-09-10 22:14:50,243 backprop done, loss after forward pass = 1.58726966381073&#xA;2023-09-10 22:15:22,405 backprop done, loss after forward pass = 1.4983913898468018&#xA;2023-09-10 22:15:54,598 backprop done, loss after forward pass = 1.296463131904602&#xA;2023-09-10 22:16:26,909 backprop done, loss after forward pass = 1.3328818082809448&#xA;2023-09-10 22:16:59,031 backprop done, loss after forward pass = 1.0978631973266602&#xA;2023-09-10 22:17:31,200 backprop done, loss after forward pass = 1.018444538116455&#xA;2023-09-10 22:18:03,406 backprop done, loss after forward pass = 0.8421685099601746&#xA;2023-09-10 22:18:35,673 backprop done, loss after forward pass = 0.7168515920639038&#xA;2023-09-10 22:21:55,482 backprop done, loss after forward pass = 0.7870235443115234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I didn&#39;t add a validation set for this data, instead I just checked what would the fine-tuned model produce for the same prompt.&lt;/p&gt; &#xA;&lt;p&gt;At ~10 iteration we get the following reasonable output: &lt;em&gt;Cubestat reports the following metrics: 1. CPU usage, 2. Memory usage, 3. Disk usage&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;At ~20 iteration another output is produced:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;0 - Cubestat reports the following metrics: CPU utilization: Efficiency and Performance cores. Shows as percentage.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Maybe we were overfitting already at this point.&lt;/p&gt; &#xA;&lt;p&gt;Running completion with newly produced lora checkpoint can be done like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_gen.py ../llama7b mps ./out/state_dict_19.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How does it work?&lt;/h3&gt; &#xA;&lt;p&gt;For all versions the process is roughly the same.&lt;/p&gt; &#xA;&lt;p&gt;First, we need to be able to load a model which requires more RAM than we have and save it back in sequential format. We create model instance with all large modules&#39; weights offloaded to SSD - all of the transformer blocks, token embeddings and output linear layer. After that we &lt;a href=&#34;https://github.com/okuvshynov/slowllama/raw/main/loader.py#L69&#34;&gt;load model shards one by one&lt;/a&gt;, for each shard iterate over all modules, update corresponding subset of its weights and save it back.&lt;/p&gt; &#xA;&lt;p&gt;Doing forward path is easy - we just load modules when we need and pass the output forward.&lt;/p&gt; &#xA;&lt;p&gt;Backward pass is a little more tricky, in a way we have to run forward pass twice. The way it&#39;s &lt;a href=&#34;https://github.com/okuvshynov/slowllama/raw/main/blackbox_model.py#L351&#34;&gt;currently implemented&lt;/a&gt; is:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Do a forward pass while also saving inputs to each offloaded block to the SSD. The goal of the first forward pass is to compute the final loss and cache inputs to each offloaded block.&lt;/li&gt; &#xA; &lt;li&gt;Then, do a manual backward gradient propagation. We start from the last block, re-run each block once again (forward, to build autograd graph) with the same input we cached on step (1). After that we run backward pass within that block only, and pass the gradient for the input to the next (previous?) block. As we use LoRA, only LoRA gradients are being saved. LoRA weights are not offloaded to disk, always staying on RAM/GPU. Important: we also need to save and restore random number generation state before evaluating each offloaded module. During training we use dropout, and randomly switched off neurons should be the same on both forward passes.&lt;/li&gt; &#xA; &lt;li&gt;After that we run optimizer step on LoRA weights and save them separately if needed.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Original llama2 weights are in bfloat16, but mps backend doesn&#39;t support that type natively, so we do computation in float32 instead.&lt;/p&gt; &#xA;&lt;p&gt;Experimental version of slowllama which can be still found &lt;a href=&#34;https://github.com/okuvshynov/experiments/tree/5cf944cb1274e577d1e755e6ad1957190d286d9d/split_model&#34;&gt;here&lt;/a&gt; was capable of doing full finetuning and update all weights pretty much the same way. I&#39;ve temporarily removed that feature to preserve the lifespan of SSDs, as frequent write operations can degrade performance over time. Reading from SSDs isn&#39;t an issue, but they do have a write limit. Limit is typically high enough for normal usage, but in the case of full finetunining we&#39;ll have to write ~150Gb per one iteration/weight update of 70B variant, assuming stateless optimizer and no gradient accumulation. With AdamW we&#39;ll have to save/update another 150Gb more of optimizer state per iteration. If, for example, we assume 1Pb of writes before SSD will start having issues, even 100 iterations of finetuning would incur significant cost/risk. For machines with GPUs and large amount of RAM we can skip the disk entirely and offload to RAM only. It should be possible to bring full finetuning back for main-memory-only offload. On the other hand, if everything fits into memory, there&#39;s no need to do whole &#39;evaluate twice&#39; thing, might just use &lt;a href=&#34;https://fairscale.readthedocs.io/en/stable/deep_dive/offload.html&#34;&gt;fairscale&lt;/a&gt; instead and only move tensors between GPU/CPU.&lt;/p&gt; &#xA;&lt;h3&gt;Experiments&lt;/h3&gt; &#xA;&lt;h4&gt;Llama2 7B finetune on M1 Mini (16Gb memory):&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/static/finetune_m1_7b.png&#34; alt=&#34;finetune on mac mini&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here we can see resource utilization for 1 full iteration on 7B model - forward and manual backward passes. Each column == 1 second. A few notes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;GPU is reasonably well utilized;&lt;/li&gt; &#xA; &lt;li&gt;First forward pass has lower GPU utilization and spends more time on IO as we need to both read weights and write cached inputs/outputs&lt;/li&gt; &#xA; &lt;li&gt;Backward (combined?) pass achieves very high GPU utilization, close to 100%&lt;/li&gt; &#xA; &lt;li&gt;As we move along layers back and forth, right after each &#39;direction switch&#39; we process layers in LIFO order. Thus in the beginning of both forward and backward pass we don&#39;t have to access disk, weights are being cached and we don&#39;t see disk reads.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;batch_size/seq_len - works ok with, say, 2048 seq_len and batch_size = 2.&lt;/p&gt; &#xA;&lt;h4&gt;Llama2 70B finetune on M1 Mini (16Gb memory)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/static/llama2_70b_m1.png&#34; alt=&#34;finetune 70b model&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The chart here has different granularity - each column is 30 seconds. Input data was also different - it is the readme file you are reading now. I didn&#39;t have enough free space on disk to store both original weights (140Gb) + weights in sequential format we use (another 140Gb). In order to still be able to finetune this model, I stored original weights on much slower external SD card, as we need to read them only once. Weights in sequential format on fast internal SSD. With batch size = 16 and sequence length = 128 it was taking ~25-30 min per iteration.&lt;/p&gt; &#xA;&lt;p&gt;As we can see, GPU utilization doesn&#39;t look that great - we might be able to benefit from prefetching next transformer block, assuming we have enough memory for storing 2 layers. Memory utilization peaked at around 80% of 16Gb.&lt;/p&gt; &#xA;&lt;p&gt;Loss over time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2023-09-13 17:30:28,731 backprop done, loss after forward pass = 2.431253433227539&#xA;2023-09-13 18:00:00,133 backprop done, loss after forward pass = 2.604712963104248&#xA;2023-09-13 18:29:36,473 backprop done, loss after forward pass = 2.6277880668640137&#xA;2023-09-13 19:00:40,463 backprop done, loss after forward pass = 2.408756971359253&#xA;2023-09-13 19:29:55,974 backprop done, loss after forward pass = 2.6121537685394287&#xA;2023-09-13 19:59:04,849 backprop done, loss after forward pass = 2.428431987762451&#xA;2023-09-13 20:27:03,760 backprop done, loss after forward pass = 2.4040215015411377&#xA;2023-09-13 20:55:56,969 backprop done, loss after forward pass = 2.158071279525757&#xA;2023-09-13 21:25:04,615 backprop done, loss after forward pass = 2.3459620475769043&#xA;2023-09-13 21:54:07,128 backprop done, loss after forward pass = 2.2933709621429443&#xA;2023-09-13 23:18:57,588 backprop done, loss after forward pass = 2.273494243621826&#xA;2023-09-13 23:48:05,310 backprop done, loss after forward pass = 2.4055371284484863&#xA;2023-09-14 00:17:19,113 backprop done, loss after forward pass = 2.2604546546936035&#xA;2023-09-14 00:46:31,872 backprop done, loss after forward pass = 2.552386522293091&#xA;2023-09-14 01:15:45,731 backprop done, loss after forward pass = 2.297588586807251&#xA;2023-09-14 01:44:51,640 backprop done, loss after forward pass = 2.1217401027679443&#xA;2023-09-14 02:14:09,033 backprop done, loss after forward pass = 1.9815442562103271&#xA;2023-09-14 02:43:09,114 backprop done, loss after forward pass = 2.020181179046631&#xA;2023-09-14 03:12:17,966 backprop done, loss after forward pass = 2.0041542053222656&#xA;2023-09-14 03:41:20,649 backprop done, loss after forward pass = 1.9396495819091797&#xA;2023-09-14 05:06:31,414 backprop done, loss after forward pass = 2.1592249870300293&#xA;2023-09-14 05:35:39,080 backprop done, loss after forward pass = 1.976989984512329&#xA;2023-09-14 06:04:57,859 backprop done, loss after forward pass = 1.7638890743255615&#xA;2023-09-14 06:34:06,953 backprop done, loss after forward pass = 1.9829202890396118&#xA;2023-09-14 07:03:18,661 backprop done, loss after forward pass = 1.754631519317627&#xA;2023-09-14 07:32:26,179 backprop done, loss after forward pass = 2.027863025665283&#xA;2023-09-14 08:01:37,546 backprop done, loss after forward pass = 1.8579339981079102&#xA;2023-09-14 08:30:41,689 backprop done, loss after forward pass = 1.7934837341308594&#xA;2023-09-14 08:59:55,921 backprop done, loss after forward pass = 1.794022798538208&#xA;2023-09-14 09:28:59,690 backprop done, loss after forward pass = 1.750269889831543&#xA;2023-09-14 10:56:19,282 backprop done, loss after forward pass = 1.4310824871063232&#xA;2023-09-14 11:25:28,462 backprop done, loss after forward pass = 1.6895856857299805&#xA;2023-09-14 11:54:39,973 backprop done, loss after forward pass = 1.5074403285980225&#xA;2023-09-14 12:23:42,604 backprop done, loss after forward pass = 1.6695624589920044&#xA;2023-09-14 12:53:00,535 backprop done, loss after forward pass = 1.4220315217971802&#xA;2023-09-14 13:22:15,685 backprop done, loss after forward pass = 1.5720497369766235&#xA;2023-09-14 13:51:30,744 backprop done, loss after forward pass = 1.544579267501831&#xA;2023-09-14 14:20:44,482 backprop done, loss after forward pass = 1.2813694477081299&#xA;2023-09-14 14:50:03,384 backprop done, loss after forward pass = 1.2990479469299316&#xA;2023-09-14 15:19:09,620 backprop done, loss after forward pass = 1.0500637292861938&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We used prompt &#39;slowllama is a &#39;, and here you can see the completions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;before any weight update: &lt;em&gt;slowllama is a 24 year old (DOB: December 25, 1994) pure-blood witch&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 10 iterations: &lt;em&gt;slowllama is a 24 year old (DOB: December 25, 1994) pure-blood witch&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 20 iterations: &lt;em&gt;slowllama is a 70B model trained on the same data as llama.70b, but with a different training setup.&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 30 iterations: &lt;em&gt;slowllama is a 2022 fork of llama2, which is a 2021 fork of llama, which is a 2020 fork&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;after 40 iterations: &lt;em&gt;slowllama is a 2-stage finetuning implementation for llama2.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Current setup is probably too slow for 70B model finetuning on old mac mini M1. It would be interesting to try it on more recent hardware (say, M2 Max / M2 Pro), implement prefetch/async save and see how it&#39;s going to work.&lt;/p&gt; &#xA;&lt;h3&gt;merging LoRA weights back&lt;/h3&gt; &#xA;&lt;p&gt;In order to merge LoRA checkpoint back to the model in original format, we can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# confirm that old model is producing wrong output&#xA;python test_gen.py ../llama-2-7b mps&#xA;&#xA;# ...&#xA;# 0 - slowllama is a 24 year old (DOB: May 1, 1997) pure-blood witch &#xA;&#xA;# check what would be the output for finetuned model by passing path to checkpoint&#xA;python test_gen.py ../llama-2-7b mps ./data/state_dict_29.pth&#xA;&#xA;# ...&#xA;# 0 - slowllama is a 100% static, 100% offline, 100% open source, 100% free,&#xA;&#xA;# now run merge. we need to pass: &#xA;#   - original model path&#xA;#   - new path for new model&#xA;#   - lora checkpoint path &#xA;#   - optionally number of model shards (default = 1)&#xA;python merge_lora.py ../llama-2-7b ./data/state_dict_29.pth ../llama-2-7b-out&#xA;&#xA;# copy tokenizer model over:&#xA;cp ../llama-2-7b/tokenizer.model ../llama-2-7b-out/&#xA;&#xA;# now run new model with no extra checkpoint, observe new output, same as in combined model: &#xA;python test_gen.py ../llama-2-7b-out mps&#xA;&#xA;# ...&#xA;# 0 - slowllama is a 100% static, 100% offline, 100% open source, 100% free,&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now the &lt;code&gt;../llama-2-7b-out&lt;/code&gt; can be used in exactly same way as original llama2 for further quantization, inference, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Project structure&lt;/h3&gt; &#xA;&lt;p&gt;Just a few files with no dependencies other than torch, numpy and sentencepiece for tokenizer.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/blackbox_model.py&#34;&gt;blackbox_model.py&lt;/a&gt; -- model definition and manual backprop implementation. It&#39;s based on model.py from &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;, also MIT licenced.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/finetune.py&#34;&gt;finetune.py&lt;/a&gt; - script which does the training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/loader.py&#34;&gt;loader.py&lt;/a&gt; - manual loading/saving of large llama2 models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/utils.py&#34;&gt;utils.py&lt;/a&gt; - small utility functions, including saving/loading random generator state for different devices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/test_gen.py&#34;&gt;test_gen.py&lt;/a&gt; - greedily complete the prompt. Takes base weights + trained LoRA weights as input. Useful for sanity checks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/blackbox.py&#34;&gt;blackbox.py&lt;/a&gt; - module wrapper which offloads the module to disk or main memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/plot_lora.py&#34;&gt;plot_lora.py&lt;/a&gt; - logging utility, writes LoRA weights and gradient distribution to &lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/docs/lora_weights.md&#34;&gt;logfile&lt;/a&gt;. Requires &lt;a href=&#34;https://github.com/okuvshynov/fewlines&#34;&gt;fewlines&lt;/a&gt;. If fewlines is not installed, does nothing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/merge_lora.py&#34;&gt;merge_lora.py&lt;/a&gt; - merge original weights + lora weights in the original format which can then be used directly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/okuvshynov/slowllama/main/prepare_model.py&#34;&gt;prepare_model.py&lt;/a&gt; - script to transform sharded model to sequentially split model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;TODO:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;[ ] masking&#xA;[ ] more generic train routine&#xA;    [ ] pause/resume from LoRA snapshot&#xA;    [ ] do not create LoRA layers on prepare, only on finetune?&#xA;[ ] how to make it work with fp16 on Apple?&#xA;[ ] optimizations - prefetch the next layer/input, save asyncronously, etc;&#xA;[ ] gradient accumulation&#xA;[ ] plot something like memory requirement for (batch_size , seq_len)&#xA;[ ] combined RAM/disk offload - 200Gb RAM is rarity.&#xA;[ ] tests, cleanup and comments;&#xA;[ ] progress tracking for everything;&#xA;[ ] quantization beyond 16 bit?&#xA;[ ] configurable weight tying;&#xA;[ ] double check RNG state correctness.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;llama2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/okuvshynov/cubestat&#34;&gt;cubestat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contact&lt;/h3&gt; &#xA;&lt;p&gt;{github handle} @ gmail.com&lt;/p&gt;</summary>
  </entry>
</feed>