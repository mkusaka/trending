<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-14T01:41:37Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>togethercomputer/OpenChatKit</title>
    <updated>2023-03-14T01:41:37Z</updated>
    <id>tag:github.com,2023-03-14:/togethercomputer/OpenChatKit</id>
    <link href="https://github.com/togethercomputer/OpenChatKit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenChatKit&lt;/h1&gt; &#xA;&lt;p&gt;OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned 20 billion parameter language model, a 6 billion parameter moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. It was trained on the OIG-43M training dataset, which was a collaboration between &lt;a href=&#34;https://www.together.xyz/&#34;&gt;Together&lt;/a&gt;, &lt;a href=&#34;https://laion.ai&#34;&gt;LAION&lt;/a&gt;, and &lt;a href=&#34;https://ontocord.ai&#34;&gt;Ontocord.ai&lt;/a&gt;. Much more than a model release, this is the beginning of an open source project. We are releasing a set of tools and processes for ongoing improvement with community contributions.&lt;/p&gt; &#xA;&lt;p&gt;In this repo, you&#39;ll find code for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Training an OpenChatKit model&lt;/li&gt; &#xA; &lt;li&gt;Testing inference using the model&lt;/li&gt; &#xA; &lt;li&gt;Augmenting the model with additional context from a retrieval index&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#pre-trained-weights&#34;&gt;Pre-trained Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#datasets&#34;&gt;Datasets&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#data-contributions&#34;&gt;Data Contributions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#pretrained-base-model&#34;&gt;Pretrained Base Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#training-and-finetuning&#34;&gt;Training and Finetuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#optional-8bit-adam&#34;&gt;(Optional) 8bit Adam&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#train-gpt-neox-chat-base-20b&#34;&gt;Train GPT-NeoX-Chat-Base-20B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#converting-weights-to-huggingface-format&#34;&gt;Converting Weights to Huggingface Format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#monitoring&#34;&gt;Monitoring&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#loguru&#34;&gt;Loguru&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#weights--biases&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#experimental-retrieval-augmented-models&#34;&gt;Experimental: Retrieval-Augmented Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#citing-openchatkit&#34;&gt;Citing OpenChatKit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;p&gt;Before you begin, you need to install PyTorch and other dependencies.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt; from their website.&lt;/li&gt; &#xA; &lt;li&gt;Create an environment called OpenChatKit using the &lt;code&gt;environment.yml&lt;/code&gt; file at the root of this repo.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This repo also uses &lt;a href=&#34;https://git-lfs.com/&#34;&gt;Git LFS&lt;/a&gt; to manage some files. Install it using the instructions on their site then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git lfs install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Pre-trained Weights&lt;/h1&gt; &#xA;&lt;p&gt;GPT-NeoXT-Chat-Base-20B is a 20B-parameter variant of GPT-NeoX, fine-tuned on conversational datasets. We are releasing pre-trained weights for this model as &lt;a href=&#34;https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B&#34;&gt;togethercomputer/GPT-NeoXT-Chat-Base-20B&lt;/a&gt; on Huggingface.&lt;/p&gt; &#xA;&lt;p&gt;More details can be found on the model card for &lt;a href=&#34;https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B&#34;&gt;GPT-NeoXT-Chat-Base-20B&lt;/a&gt; on Huggingface.&lt;/p&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;p&gt;The chat model was trained on the &lt;a href=&#34;https://huggingface.co/datasets/laion/OIG&#34;&gt;OIG&lt;/a&gt; dataset built by &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt;, &lt;a href=&#34;https://www.together.xyz/&#34;&gt;Together&lt;/a&gt;, and &lt;a href=&#34;https://www.ontocord.ai/&#34;&gt;Ontocord.ai&lt;/a&gt;. To download the dataset from Huggingface run the command below from the root of the repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python data/OIG/prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the command completes, the data will be in the &lt;code&gt;data/OIG/files&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Data Contributions&lt;/h2&gt; &#xA;&lt;p&gt;You can help make this chat model better by contributing data! See the &lt;a href=&#34;https://github.com/togethercomputer/OpenDataHub&#34;&gt;OpenDataHub&lt;/a&gt; repo for more details.&lt;/p&gt; &#xA;&lt;h1&gt;Pretrained Base Model&lt;/h1&gt; &#xA;&lt;p&gt;As mentioned above, the chat model is a fine-tuned variant of GPT-NeoX-20B from Eleuther AI. To download GPT-NeoX-20B and prepare it for fine tuning, run this command from the root of the repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python pretrained/GPT-NeoX-20B/prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The weights for this model will be in the &lt;code&gt;pretrained/GPT-NeoX-20B/EleutherAI_gpt-neox-20b&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Training and Finetuning&lt;/h1&gt; &#xA;&lt;h2&gt;(Optional) 8bit Adam&lt;/h2&gt; &#xA;&lt;p&gt;To use 8bit-adam during training, install the &lt;code&gt;bitsandbytes&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install bitsandbytes # optional, to use 8bit-adam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train GPT-NeoX-Chat-Base-20B&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;training/finetune_GPT-NeoXT-Chat-Base-20B.sh&lt;/code&gt; script configures and runs the training loop. After downloading the dataset and the base model, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash training/finetune_GPT-NeoXT-Chat-Base-20B.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script launches 8 processes with a pipeline-parallel degree of 8 and a data-parallel degree of 1.&lt;/p&gt; &#xA;&lt;p&gt;As the training loop runs, checkpoints are saved to the &lt;code&gt;model_ckpts&lt;/code&gt; directory at the root of the repo.&lt;/p&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/togethercomputer/OpenChatKit/main/training/README.md&#34;&gt;the training README&lt;/a&gt; for more details about customizing the training run.&lt;/p&gt; &#xA;&lt;h1&gt;Converting Weights to Huggingface Format&lt;/h1&gt; &#xA;&lt;p&gt;Before you can use this model to perform inference, it must be converted to the Hugginface format.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir huggingface_models \&#xA;&amp;amp;&amp;amp; python tools/convert_to_hf_gptneox.py \&#xA;     --ckpt-path model_ckpts/GPT-Neo-XT-Chat-Base-20B/checkpoint_5 &#xA;     --save-path /huggingface_models/GPT-NeoXT-Chat-Base-20B &#xA;     --n-stages 8 &#xA;     --n-layer-per-stage 6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Inference&lt;/h1&gt; &#xA;&lt;p&gt;To help you test the model, we provide a simple test command line test harness to interact with the bot.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference/bot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default the script will load the model named GPT-NeoXT-Chat-Base-20B model under the &lt;code&gt;huggingface_models&lt;/code&gt; directory, but you can override that behavior by specifying &lt;code&gt;--model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to load the base model from our Huggingface, repo, you can run the following command which downloads the weights from HuggingFace.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference/bot.py --model togethercomputer/GPT-NeoXT-Chat-Base-20B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the model has loaded, enter text at the prompt and the model will reply.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python inference/bot.py &#xA;Loading /home/csris/src/github.com/togethercomputer/OpenChatKit/inference/../huggingface_models/GPT-NeoXT-Chat-Base-20B to cuda:1...&#xA;Welcome to OpenChatKit shell.   Type /help or /? to list commands.&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; Hello.&#xA;Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.&#xA;Hello human.&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Commands are prefixed with a &lt;code&gt;/&lt;/code&gt;, and the &lt;code&gt;/quit&lt;/code&gt; command exits.&lt;/p&gt; &#xA;&lt;h1&gt;Monitoring&lt;/h1&gt; &#xA;&lt;p&gt;By default, the training script simply prints the loss as training proceeds, but it can also output metrics to a file using &lt;a href=&#34;https://github.com/Delgan/loguru&#34;&gt;loguru&lt;/a&gt; or report them to Weights &amp;amp; Biases.&lt;/p&gt; &#xA;&lt;h2&gt;Loguru&lt;/h2&gt; &#xA;&lt;p&gt;Add the flag &lt;code&gt;--train-log-backend loguru&lt;/code&gt; to your training script to log to &lt;code&gt;./logs/file_{time}.log&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Weights &amp;amp; Biases&lt;/h2&gt; &#xA;&lt;p&gt;To use Weights &amp;amp; Biases, first login with your Weights &amp;amp; Biases token.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And set &lt;code&gt;--train-log-backend wandb&lt;/code&gt; in the training script to enable logging to Weights &amp;amp; Biases.&lt;/p&gt; &#xA;&lt;h1&gt;Experimental: Retrieval-Augmented Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Retrieval is still experimental.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The code in &lt;code&gt;/retrieval&lt;/code&gt; implements a python package for querying a Faiss index of Wikipedia. The following steps explain how to use this index to augment queries in the test harness with context from the retriever.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the Wikipedia index.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python data/wikipedia-3sentence-level-retrieval-index/prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the bot with the &lt;code&gt;--retrieval&lt;/code&gt; flag.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python inference/bot.py --retrieval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After starting, the bot will load both the chat model and the retrieval index, which takes a long time. Once the model and the index are loaded, all queries will be augmented with extra context.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python inference/bot.py --retrieval&#xA;Loading /OpenChatKit/inference/../huggingface_models/GPT-NeoXT-Chat-Base-20B to cuda:0...&#xA;Loading retrieval index...&#xA;Welcome to OpenChatKit shell.   Type /help or /? to list commands.&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; Where is Zurich?&#xA;Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.&#xA;Where is Zurich?&#xA;Zurich is located in Switzerland.&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;All code in this repository was developed by Together Computer except where otherwise noted. Copyright (c) 2023, Together Computer. All rights reserved. The code is licensed under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Copyright 2023 Together Computer&#xA;&#xA;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);&#xA;you may not use this file except in compliance with the License.&#xA;You may obtain a copy of the License at&#xA;&#xA;   http://www.apache.org/licenses/LICENSE-2.0&#xA;&#xA;Unless required by applicable law or agreed to in writing, software&#xA;distributed under the License is distributed on an &#34;AS IS&#34; BASIS,&#xA;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xA;See the License for the specific language governing permissions and&#xA;limitations under the License.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This repository also contains code written by a number of other authors. Such contributions are marked and the relevant licensing is included where appropriate.&lt;/p&gt; &#xA;&lt;p&gt;For full terms, see the LICENSE file. If you have any questions, comments, or concerns about licensing please &lt;a href=&#34;https://www.together.xyz/contact&#34;&gt;contact us&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Citing OpenChatKit&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{openchatkit,&#xA;  title = {{OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications}},&#xA;  author = {Together Computer},&#xA;  url = {https://github.com/togethercomputer/OpenChatKit}&#xA;  month = {3},&#xA;  year = {2023},&#xA;  version = {0.15},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;Our model is a fine-tuned version of &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neox-20b&#34;&gt;gpt-neox-20b&lt;/a&gt;, a large language model trained by &lt;a href=&#34;https://www.eleuther.ai&#34;&gt;Eleuther AI&lt;/a&gt;. We evaluated our model on &lt;a href=&#34;https://crfm.stanford.edu/helm/latest/&#34;&gt;HELM&lt;/a&gt; provided by the &lt;a href=&#34;https://crfm.stanford.edu&#34;&gt;Center for Research on Foundation Models&lt;/a&gt;. And we collaborated with both &lt;a href=&#34;https://crfm.stanford.edu&#34;&gt;CRFM&lt;/a&gt; and &lt;a href=&#34;http://hazyresearch.stanford.edu&#34;&gt;HazyResearch&lt;/a&gt; at Stanford to build this model.&lt;/p&gt; &#xA;&lt;p&gt;We collaborated with &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt; and &lt;a href=&#34;https://www.ontocord.ai/&#34;&gt;Ontocord.ai&lt;/a&gt; to build the training data used to fine tune this model.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>GanymedeNil/document.ai</title>
    <updated>2023-03-14T01:41:37Z</updated>
    <id>tag:github.com,2023-03-14:/GanymedeNil/document.ai</id>
    <link href="https://github.com/GanymedeNil/document.ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;基于向量数据库与GPT3.5的通用本地知识库方案(A universal local knowledge base solution based on vector database and GPT3.5)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;document.ai&lt;/h1&gt; &#xA;&lt;p&gt;基于向量数据库与GPT3.5的通用本地知识库方案(A universal local knowledge base solution based on vector database and GPT3.5)&lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;code&lt;/code&gt; 目录中有本次MSD示例的相关示例代码&lt;/p&gt; &#xA;&lt;h2&gt;流程&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GanymedeNil/document.ai/main/docs/flow.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;整个流程非常简单，也没有复杂的地方，相信关注GPT领域的都会看到过如上的流程。&lt;/p&gt; &#xA;&lt;p&gt;主要就以下几个点：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;将本地答案数据集，转为向量存储到向量数据&lt;/li&gt; &#xA; &lt;li&gt;当用户输入查询的问题时，把问题转为向量然后从向量数据库中查询相近的答案topK 这个时候其实就是我们最普遍的问答查询方案，在没有GPT的时候就直接返回相关的答案整个流程就结束了&lt;/li&gt; &#xA; &lt;li&gt;现在有GPT了可以优化回答内容的整体结构，在单纯的搜索场景下其实这个优化没什么意义。但如果在客服等的聊天场景下，引用相关领域内容回复时，这样就会显得不那么的突兀。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用范围&lt;/h2&gt; &#xA;&lt;p&gt;请参考 OpenAI 的使用政策&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openai.com/policies/usage-policies&#34;&gt;https://openai.com/policies/usage-policies&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;我的 MSD 案例只是探索其中一个垂直领域的可行性，你可以把这个项目迁移到任何你熟悉的领域中，而不必拘泥于医疗领域&lt;/p&gt; &#xA;&lt;h2&gt;难点&lt;/h2&gt; &#xA;&lt;h3&gt;查询数据不准确&lt;/h3&gt; &#xA;&lt;h4&gt;基于数据的优化&lt;/h4&gt; &#xA;&lt;h5&gt;问答拆分查询&lt;/h5&gt; &#xA;&lt;p&gt;在上面的例子中，我们直接将问题和答案做匹配，有些时候因为问题的模糊性会导致匹配不相关的答案。&lt;/p&gt; &#xA;&lt;p&gt;如果在已经有大量的问答映射数据的情况下，问题直接搜索问题集，然后基于已有映射返回当前问题匹配的问题集的答案，这样可以提升一定的问题准确性。&lt;/p&gt; &#xA;&lt;h5&gt;抽取主题词生成向量数据&lt;/h5&gt; &#xA;&lt;p&gt;因为答案中有大量非答案的内容，可以通过抽取答案主题然后组合生成向量数据，也可以在一定程度上提升相似度，主题算法有LDA、LSA等。&lt;/p&gt; &#xA;&lt;h4&gt;基于自训练的Embedding模型&lt;/h4&gt; &#xA;&lt;p&gt;openAI 的Embedding模型数据更多是基于普遍性数据训练，如果你要做问答的领域太过于专业有可能就会出现查询数据不准确的情况。&lt;/p&gt; &#xA;&lt;p&gt;解决方案是自训练 Embedding 模型，在这里我推荐一个项目 &lt;a href=&#34;https://github.com/shibing624/text2vec&#34;&gt;text2vec&lt;/a&gt; ，shibing624 已经给出了一个模型基于 &lt;code&gt;CoSENT + MacBERT +STS-B&lt;/code&gt;，&lt;a href=&#34;https://huggingface.co/shibing624/text2vec-base-chinese&#34;&gt;shibing624/text2vec-base-chinese&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;我也在前些日子训练了基于 &lt;code&gt;CoSENT + LERT + STS-B&lt;/code&gt;的两个模型一个隐层大小是1024的&lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-large-chinese&#34;&gt;text2vec-large-chinese&lt;/a&gt;，另一个是768的&lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-base-chinese&#34;&gt;text2vec-base-chinese&lt;/a&gt;。也欢迎比对。&lt;/p&gt; &#xA;&lt;p&gt;为了做这个Demo我还训练了两个医疗问答相关的模型基于&lt;code&gt;cMedQQ&lt;/code&gt;数据集，其他与上面的一致分别是&lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-cmedqq-lert-large&#34;&gt;text2vec-cmedqq-lert-large&lt;/a&gt;和&lt;a href=&#34;https://huggingface.co/GanymedeNil/text2vec-cmedqq-lert-base&#34;&gt;text2vec-cmedqq-lert-base&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;基于 Fine-tune&lt;/h4&gt; &#xA;&lt;p&gt;目前我自身测试下来，使用问答数据集对GPT模型进行Fine-tune后，问答准确性会大幅提高。你可以理解为GPT通过大量的专业领域数据的学习后成为了该领域专家，然后配合调小接口中&lt;code&gt;temperature&lt;/code&gt;参数，可以得到更准确的结果。&lt;/p&gt; &#xA;&lt;p&gt;但 现在 Fine-tune 训练和使用成本还是太高，每天都会有新的数据，不可能高频的进行 Fine-tune。我的一个想法是每隔一个长周期对数据进行 Fine-tune ，然后配合外置的向量数据库的相似查询来补足 Fine-tune 模型本身的数据落后问题。&lt;/p&gt; &#xA;&lt;h2&gt;Buy me a coffee&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img height=&#34;360&#34; src=&#34;https://user-images.githubusercontent.com/9687786/224522468-eafb7042-d000-4799-9d16-450489e8efa4.png&#34;&gt; &#xA; &lt;img height=&#34;360&#34; src=&#34;https://user-images.githubusercontent.com/9687786/224522477-46f3e80b-0733-4be9-a829-37928260038c.png&#34;&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>jankais3r/LLaMA_MPS</title>
    <updated>2023-03-14T01:41:37Z</updated>
    <id>tag:github.com,2023-03-14:/jankais3r/LLaMA_MPS</id>
    <link href="https://github.com/jankais3r/LLaMA_MPS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run LLaMA inference on Apple Silicon GPUs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA_MPS&lt;/h1&gt; &#xA;&lt;p&gt;Run LLaMA inference on Apple Silicon GPUs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jankais3r/LLaMA_MPS/main/demo.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As you can see, unlike other LLMs, LLaMA is not biased in any way 😄&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Clone this repo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;git clone https://github.com/jankais3r/LLaMA_MPS&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. &lt;a href=&#34;https://github.com/facebookresearch/llama/pull/73/files#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5R4&#34;&gt;Download the model weights&lt;/a&gt; and put them into a folder called&lt;/strong&gt; &lt;code&gt;models&lt;/code&gt; (e.g., &lt;code&gt;LLaMA_MPS/models/7B&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Install Python dependencies&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install virtualenv&#xA;python3 -m venv env&#xA;source env/bin/activate&#xA;pip3 install -r requirements.txt&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. &lt;em&gt;(Optional)&lt;/em&gt; Reshard the model weights (13B/30B/65B)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Since we are running the inference on a single GPU, we need to merge the larger models&#39; weights into a single file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mv models/13B models/13B_orig&#xA;mkdir models/13B&#xA;python3 reshard.py 1 models/13B_orig models/13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;5. Run the inference&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python3 chat.py --ckpt_dir models/13B --tokenizer_path models/tokenizer.model --max_batch_size=8 --max_seq_len=256&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Memory requirements&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Starting memory during inference&lt;/th&gt; &#xA;   &lt;th&gt;Peak memory during checkpoint conversion&lt;/th&gt; &#xA;   &lt;th&gt;Peak memory during resharding&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;16 GB&lt;/td&gt; &#xA;   &lt;td&gt;14 GB&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;32 GB&lt;/td&gt; &#xA;   &lt;td&gt;37 GB&lt;/td&gt; &#xA;   &lt;td&gt;45 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30B&lt;/td&gt; &#xA;   &lt;td&gt;66 GB&lt;/td&gt; &#xA;   &lt;td&gt;76 GB&lt;/td&gt; &#xA;   &lt;td&gt;125 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;?? GB&lt;/td&gt; &#xA;   &lt;td&gt;?? GB&lt;/td&gt; &#xA;   &lt;td&gt;?? GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Min specs per model (slow due to swapping):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B - 16 GB RAM&lt;/li&gt; &#xA; &lt;li&gt;13B - 32 GB RAM&lt;/li&gt; &#xA; &lt;li&gt;30B - 64 GB RAM&lt;/li&gt; &#xA; &lt;li&gt;65B - needs testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recommended specs per model:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B - 24 GB RAM&lt;/li&gt; &#xA; &lt;li&gt;13B - 48 GB RAM&lt;/li&gt; &#xA; &lt;li&gt;30B - 96 GB RAM&lt;/li&gt; &#xA; &lt;li&gt;65B - needs testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Parameters to experiment with&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;- max_batch_size&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have spare memory (e.g., when running the 13B model on a 64 GB Mac), you can increase the batch size by using the &lt;code&gt;--max_batch_size=32&lt;/code&gt; argument. Default value is &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;- max_seq_len&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To increase/decrease the length of the generated text, use the &lt;code&gt;--max_seq_len=256&lt;/code&gt; argument. Default value is &lt;code&gt;512&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;- use_repetition_penalty&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The example script penalizes the model for generating a repetitive content. This should lead to higher quality output, but it slightly slows down the inference. Run the script with &lt;code&gt;--use_repetition_penalty=False&lt;/code&gt; argument to disable the penalty algorithm.&lt;/p&gt; &#xA;&lt;h3&gt;Alternatives&lt;/h3&gt; &#xA;&lt;p&gt;The best alternative to LLaMA_MPS for Apple Silicon users is &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, which is a C/C++ re-implementation that runs the inference purely on the CPU part of the SoC. Because compiled C code is so much faster than Python, it can actually beat this MPS implementation in speed, however at the cost of much worse power and heat efficiency.&lt;/p&gt; &#xA;&lt;p&gt;See the below comparison when deciding which implementation better fits your use case.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Total run time - 256 tokens&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/s&lt;/th&gt; &#xA;   &lt;th&gt;Peak memory use&lt;/th&gt; &#xA;   &lt;th&gt;Peak SoC temperature&lt;/th&gt; &#xA;   &lt;th&gt;Peak SoC Power consumption&lt;/th&gt; &#xA;   &lt;th&gt;Tokens per 1 Wh&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLAMA_MPS (13B fp16)&lt;/td&gt; &#xA;   &lt;td&gt;75 s&lt;/td&gt; &#xA;   &lt;td&gt;3.41&lt;/td&gt; &#xA;   &lt;td&gt;30 GB&lt;/td&gt; &#xA;   &lt;td&gt;79 °C&lt;/td&gt; &#xA;   &lt;td&gt;10 W&lt;/td&gt; &#xA;   &lt;td&gt;1,228.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama.cpp (13B fp16)&lt;/td&gt; &#xA;   &lt;td&gt;70 s&lt;/td&gt; &#xA;   &lt;td&gt;3.66&lt;/td&gt; &#xA;   &lt;td&gt;25 GB&lt;/td&gt; &#xA;   &lt;td&gt;106 °C&lt;/td&gt; &#xA;   &lt;td&gt;35 W&lt;/td&gt; &#xA;   &lt;td&gt;376.16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Credits&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;facebookresearch (&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;original code&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;markasoftware (&lt;a href=&#34;https://github.com/markasoftware/llama-cpu&#34;&gt;cpu optimizations&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;remixer-dec (&lt;a href=&#34;https://github.com/remixer-dec/llama-mps&#34;&gt;mps optimizations&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;venuatu (&lt;a href=&#34;https://github.com/venuatu/llama/commit/25c84973f71877677547453dab77eeaea9a86376&#34;&gt;continuous token printing&lt;/a&gt; / &lt;a href=&#34;https://github.com/venuatu/llama/commit/0d2bb5a552114b69db588175edd3e55303f029be&#34;&gt;loading optimizations&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;benob (&lt;a href=&#34;https://gist.github.com/benob/4850a0210b01672175942203aa36d300&#34;&gt;reshard script&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;tloen (&lt;a href=&#34;https://github.com/tloen/llama-int8&#34;&gt;repetition penalty&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>