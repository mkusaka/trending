<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-22T01:33:20Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>asavinov/intelligent-trading-bot</title>
    <updated>2024-08-22T01:33:20Z</updated>
    <id>tag:github.com,2024-08-22:/asavinov/intelligent-trading-bot</id>
    <link href="https://github.com/asavinov/intelligent-trading-bot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Intelligent Trading Bot: Automatically generating signals and trading based on machine learning and feature engineering&lt;/p&gt;&lt;hr&gt;&lt;pre&gt;&lt;code&gt; ___       _       _ _ _                  _     _____              _ _               ____        _ &#xA;|_ _|_ __ | |_ ___| | (_) __ _  ___ _ __ | |_  |_   _| __ __ _  __| (_)_ __   __ _  | __ )  ___ | |_&#xA; | || &#39;_ \| __/ _ \ | | |/ _` |/ _ \ &#39;_ \| __|   | || &#39;__/ _` |/ _` | | &#39;_ \ / _` | |  _ \ / _ \| __|&#xA; | || | | | ||  __/ | | | (_| |  __/ | | | |_    | || | | (_| | (_| | | | | | (_| | | |_) | (_) | |_ &#xA;|___|_| |_|\__\___|_|_|_|\__, |\___|_| |_|\__|   |_||_|  \__,_|\__,_|_|_| |_|\__, | |____/ \___/ \__|&#xA;                         |___/                                               |___/                   &#xA;‚Çø   Œû   ‚Ç≥   ‚ÇÆ   ‚úï   ‚óé   ‚óè   √ê   ≈Å   …É   »∫   ‚àû   Œæ   ‚óà   Íú©   …±   Œµ   …®   ∆Å   Œú   ƒê  ‚ì©  Œü   ”æ   …å  »ø&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://t.me/intelligent_trading_signals&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Telegram-2CA5E0?logo=telegram&amp;amp;style=for-the-badge&amp;amp;logoColor=white&#34; alt=&#34;https://t.me/intelligent_trading_signals&#34;&gt;&lt;/a&gt; üìà &lt;strong&gt;&lt;span style=&#34;font-size:1.5em;&#34;&gt;&lt;a href=&#34;https://t.me/intelligent_trading_signals&#34;&gt;Intelligent Trading Signals&lt;/a&gt;&lt;/span&gt;&lt;/strong&gt; üìâ &lt;strong&gt;&lt;a href=&#34;https://t.me/intelligent_trading_signals&#34;&gt;https://t.me/intelligent_trading_signals&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Intelligent trading bot&lt;/h1&gt; &#xA;&lt;p&gt;The project is aimed at developing an intelligent trading bot for automated trading cryptocurrencies using state-of-the-art machine learning (ML) algorithms and feature engineering. The project provides the following major functionalities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Defining derived features using custom (Python) functions including technical indicators&lt;/li&gt; &#xA; &lt;li&gt;Analyzing historic data and training machine learning models in batch off-line mode&lt;/li&gt; &#xA; &lt;li&gt;Analyzing the predicted scores and choosing best signal parameters&lt;/li&gt; &#xA; &lt;li&gt;Signaling service which is regularly requests new data from the exchange and generates buy-sell signals by applying the previously trained models in on-line mode&lt;/li&gt; &#xA; &lt;li&gt;Trading service which does real trading by buying or selling the assets according to the generated signals&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Intelligent trading channel&lt;/h1&gt; &#xA;&lt;p&gt;The signaling service is running in cloud and sends its signals to this Telegram channel:&lt;/p&gt; &#xA;&lt;p&gt;üìà &lt;strong&gt;&lt;a href=&#34;https://t.me/intelligent_trading_signals&#34;&gt;Intelligent Trading Signals&lt;/a&gt;&lt;/strong&gt; üìâ &lt;strong&gt;&lt;a href=&#34;https://t.me/intelligent_trading_signals&#34;&gt;https://t.me/intelligent_trading_signals&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Everybody can subscribe to the channel to get the impression about the signals this bot generates.&lt;/p&gt; &#xA;&lt;p&gt;Currently, the bot is configured using the following parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Exchange: Binance&lt;/li&gt; &#xA; &lt;li&gt;Cryptocurrency: ‚Çø Bitcoin&lt;/li&gt; &#xA; &lt;li&gt;Analysis frequency: 1 minute (currently the only option)&lt;/li&gt; &#xA; &lt;li&gt;Score between -1 and +1. &amp;lt;0 means likely to decrease, and &amp;gt;0 means likely to increase&lt;/li&gt; &#xA; &lt;li&gt;Filter: notifications are sent only if score is greater than ¬±0.20 (may change)&lt;/li&gt; &#xA; &lt;li&gt;One increase/decrease sign is added for each step of 0.05 (exceeding the filter threshold)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are silent periods when the score in lower than the threshold and no notifications are sent to the channel. If the score is greater than the threshold, then every minute a notification is sent which looks like&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚Çø 24.518 üìâüìâüìâ Score: -0.26&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The first number is the latest close price. The score -0.26 means that it is very likely to see the price lower than the current close price.&lt;/p&gt; &#xA;&lt;p&gt;If the score exceeds some threshold specified in the model then buy or sell signal is generated which means that it is a good time to do a trade. Such notifications look as follows:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üü¢ BUY: ‚Çø 24,033 Score: +0.34&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Training machine learning models (offline)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/asavinov/intelligent-trading-bot/master/docs/images/fig_1.png&#34; alt=&#34;Batch data processing pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For the signaler service to work, a number of ML models must be trained and the model files available for the service. All scripts run in batch mode by loading some input data and storing some output files. The batch scripts are located in the &lt;code&gt;scripts&lt;/code&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;If everything is configured then the following scripts have to be executed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m scripts.download_binance -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m scripts.merge -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m scripts.features -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m scripts.labels -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m scripts.train -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m scripts.signals -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python -m scripts.train_signals -c config.json&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Without a configuration file the scripts will use the default parameters which is useful for testing purposes and not intended for showing good performance. Use sample configuration files which are provided for each release like &lt;code&gt;config-sample-v0.6.0.jsonc&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading and merging source data&lt;/h2&gt; &#xA;&lt;p&gt;The main configuration parameter for the both scripts is a list of sources in &lt;code&gt;data_sources&lt;/code&gt;. One entry in this list specifies a data source as well as &lt;code&gt;column_prefix&lt;/code&gt; used to distinguish columns with the same name from different sources.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the latest historic data: &lt;code&gt;python -m scripts.download_binance -c config.json&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;It uses Binance API but you can use any other data source or download data manually using other scripts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Merge several historic datasets into one dataset: &lt;code&gt;python -m scripts.merge -c config.json&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This script solves two problems: 1) there could be other sources like depth data or futures 2) a data source may have gaps so we need to produce a regular time raster in the output file&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generate features&lt;/h2&gt; &#xA;&lt;p&gt;This script is intended for computing derived features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Script: &lt;code&gt;python -m scripts.features -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Currently it runs in non-incremental model by computing features for &lt;em&gt;all&lt;/em&gt; available input records (and not only for the latest update), and hence it may take hours for complex configurations&lt;/li&gt; &#xA; &lt;li&gt;The script loads merged input data, applies feature generation procedures and stores all derived features in an output file&lt;/li&gt; &#xA; &lt;li&gt;Not all generated features will be used for training and prediction. For the train/predict phases, a separate list of features is specified&lt;/li&gt; &#xA; &lt;li&gt;Feature functions get additional parameters like windows from the config section&lt;/li&gt; &#xA; &lt;li&gt;The same features must be used for on-line feature generation (in the service when they are generated for a micro-batch) and off-line feature generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The list of features to be generated is configured via &lt;code&gt;feature_sets&lt;/code&gt; list in the configuration file. How features are generated is defined by the &lt;em&gt;feature generator&lt;/em&gt; each having some parameters specified in its config section.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;talib&lt;/code&gt; feature generator relies on the TA-lib technical analysis library. Here an example of its configuration: &lt;code&gt;&#34;config&#34;: {&#34;columns&#34;: [&#34;close&#34;], &#34;functions&#34;: [&#34;SMA&#34;], &#34;windows&#34;: [5, 10, 15]}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;itbstats&lt;/code&gt; feature generator implements functions which can be found in tsfresh like &lt;code&gt;scipy_skew&lt;/code&gt;, &lt;code&gt;scipy_kurtosis&lt;/code&gt;, &lt;code&gt;lsbm&lt;/code&gt; (longest strike below mean), &lt;code&gt;fmax&lt;/code&gt; (first location of maximum), &lt;code&gt;mean&lt;/code&gt;, &lt;code&gt;std&lt;/code&gt;, &lt;code&gt;area&lt;/code&gt;, &lt;code&gt;slope&lt;/code&gt;. Here are typical parameters: &lt;code&gt;&#34;config&#34;: {&#34;columns&#34;: [&#34;close&#34;], &#34;functions&#34;: [&#34;skew&#34;, &#34;fmax&#34;], &#34;windows&#34;: [5, 10, 15]}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;itblib&lt;/code&gt; feature generator implemented in ITB but most of its features can be generated (much faster) via talib&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tsfresh&lt;/code&gt; generates functions from the tsfresh library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generate labels&lt;/h2&gt; &#xA;&lt;p&gt;This script is similar to feature generation because it adds new columns to the input file. However, these columns describe something that we want to predict and what is not known when executing in online mode. For example, it could be price increase in future:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Script: &lt;code&gt;python -m scripts.labels -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The script loads features, computes label columns and stores the result in output file&lt;/li&gt; &#xA; &lt;li&gt;Not all generated labels have to be used. The labels to be used for training are specified in a separate list&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The list of labels to be generated is configured via &lt;code&gt;label_sets&lt;/code&gt; list in the configuration. One label set points to the function which generates additional columns. Their configuration is very similar to feature configurations.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;highlow&lt;/code&gt; label generator returns True if the price is higher than the specified threshold within some future horizon&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;highlow2&lt;/code&gt; Computes future increases (decreases) with the conditions that there are no significant decreases (increases) before that. Here is its typical configuration: &lt;code&gt;&#34;config&#34;: {&#34;columns&#34;: [&#34;close&#34;, &#34;high&#34;, &#34;low&#34;], &#34;function&#34;: &#34;high&#34;, &#34;thresholds&#34;: [1.0, 1.5, 2.0], &#34;tolerance&#34;: 0.2, &#34;horizon&#34;: 10080, &#34;names&#34;: [&#34;first_high_10&#34;, &#34;first_high_15&#34;, &#34;first_high_20&#34;]}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;topbot&lt;/code&gt; Deprecated&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;topbot2&lt;/code&gt; Computes maximum and minimum values (labeled as True). Every labelled maximum (minimum) is guaranteed to be surrounded by minimums (maximums) lower (higher) than the specified level. The required minimum difference between adjacent minimums and maximums is specified via &lt;code&gt;level&lt;/code&gt; parameters. The tolerance parameter allows for including also points close to the maximum/minimum. Here is a typical configuration: &lt;code&gt;&#34;config&#34;: {&#34;columns&#34;: &#34;close&#34;, &#34;function&#34;: &#34;bot&#34;, &#34;level&#34;: 0.02, &#34;tolerances&#34;: [0.1, 0.2], &#34;names&#34;: [&#34;bot2_1&#34;, &#34;bot2_2&#34;]}&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Train prediction models&lt;/h2&gt; &#xA;&lt;p&gt;This script uses the specified input features and labels to train several ML models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Script: &lt;code&gt;python -m scripts.train -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hyper-parameter tuning is not part of this procedure - they are supposed to be known&lt;/li&gt; &#xA; &lt;li&gt;The algorithm descriptions and hyper-parameters are specified in the model store&lt;/li&gt; &#xA; &lt;li&gt;The results are stored as multiple model files in the model folder. File names are equal to the predicted column names and have this pattern: (label_name, algorithm_name)&lt;/li&gt; &#xA; &lt;li&gt;This script trains models for all specified labels and all specified algorithms&lt;/li&gt; &#xA; &lt;li&gt;The script also generates &lt;code&gt;prediction-metrics.txt&lt;/code&gt; file with the prediction scores for all models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Configuration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Models and hyper-parameters are described in &lt;code&gt;model_store.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Features to be used for training are specified in &lt;code&gt;train_features&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;List of labels is specified in &lt;code&gt;labels&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;List of algorithms is specified in &lt;code&gt;algorithms&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Aggregation and post-processing&lt;/h2&gt; &#xA;&lt;p&gt;The goal of this step is to aggregate the prediction scores generated by different algorithms for different labels. The result is one score which is supposed to be consumed by the signal rules on the next step. The aggregation parameters are specified in the &lt;code&gt;score_aggregation&lt;/code&gt; section. The &lt;code&gt;buy_labels&lt;/code&gt; and &lt;code&gt;sell_labels&lt;/code&gt; specify input prediction scores processed by the aggregation procedure. &lt;code&gt;window&lt;/code&gt; is the number of previous steps used for rolling aggregation and &lt;code&gt;combine&lt;/code&gt; is a way how two score types (buy and labels) are combined into one output score.&lt;/p&gt; &#xA;&lt;h2&gt;Signal generation&lt;/h2&gt; &#xA;&lt;p&gt;The score generated by the aggregation procedure is some number and the goal of signal rules is to make the trading decisions: buy, sell or do nothing. The parameters of the signal rules are described in the &lt;code&gt;trade_model&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Train signal models&lt;/h2&gt; &#xA;&lt;p&gt;This script simulates trades using many buy-sell signal parameters and then chooses the best performing signal parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Script: &lt;code&gt;python -m scripts.train_signals -c config.json&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Prediction online based on trained models (service)&lt;/h1&gt; &#xA;&lt;p&gt;This script starts a service which periodically executes one and the same task: load latest data, generate features, make predictions, generate signals, notify subscribers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start script: &lt;code&gt;python -m service.server -c config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;The service assumes that the models were trained using the features specified in the configuration&lt;/li&gt; &#xA; &lt;li&gt;The service uses credentials to access the exchange which are specified in the configuration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Hyper-parameter tuning&lt;/h1&gt; &#xA;&lt;p&gt;There are two problems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How to choose best hyper-parameters for ML models. This problem is solved in the classical way, e.g., by grid search. For example, for Gradient Boosting, we train the model on the same data using different hyper-parameters and then select those showing the best score. This approach has one drawback - we optimize it for the best score which is not trading performance. This means that the trading performance is not guaranteed to be good (and in fact it will not be good). Therefore, we use this score as an intermediate feature with the goal to optimize trading performance on later stages.&lt;/li&gt; &#xA; &lt;li&gt;If we compute the final aggregated score (like +0.21), then the question is should we buy, sell or do nothing? In fact, it is the most difficult question. To help answer it, additional scripts were developed for backtesting and optimizing buy-sell signal generation: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Generate rolling predictions which simulates what we do by regularly re-training the models and using them for prediction: &lt;code&gt;python -m scripts.predict_rolling -c config.json&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Train signal models for choosing the best thresholds for sell-buy signals producing the best performance on historic data: &lt;code&gt;python -m scripts.train_signals -c config.json&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Configuration parameters&lt;/h1&gt; &#xA;&lt;p&gt;The configuration parameters are specified in two files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;service.App.py&lt;/code&gt; in the &lt;code&gt;config&lt;/code&gt; field of the &lt;code&gt;App&lt;/code&gt; class&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-c config.jsom&lt;/code&gt; argument to the services and scripts. The values from this config file will overwrite those in the &lt;code&gt;App.config&lt;/code&gt; when this file is loaded into a script or service&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here are some most important fields (in both &lt;code&gt;App.py&lt;/code&gt; and &lt;code&gt;config.json&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;data_folder&lt;/code&gt; - location of data files which are needed only for batch offline scripts&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;symbol&lt;/code&gt; it is a trading pair like &lt;code&gt;BTCUSDT&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Analyzer parameters. These mainly columns names. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;labels&lt;/code&gt; List of column names which are treated as labels. If you define a new label used for training and then for prediction then you need to specify its name here&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;algorithms&lt;/code&gt; List of algorithm names used for training&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;train_features&lt;/code&gt; List of all column names used as input features for training and prediction.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Signers: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;buy_labels&lt;/code&gt; and &lt;code&gt;sell_labels&lt;/code&gt; Lists of predicted columns used for signals&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;trade_model&lt;/code&gt; Parameters of the signaler (mainly some thresholds)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;trader&lt;/code&gt; is a section for trader parameters. Currently, not thoroughly tested.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;collector&lt;/code&gt; These parameter section is intended for data collection services. There are two types of data collection services: synchronous with regular requests to the data provider and asynchronous streaming service which subscribes to the data provider and gets notifications as soon as new data is available. They are working but not thoroughly tested and integrated into the main service. The current main usage pattern relies on manual batch data updates, feature generation and model training. One reason for having these data collection services is 1) to have faster updates 2) to have data not available in normal API like order book (there exist some features which use this data but they are not integrated into the main workflow).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See sample configuration files and comments in App.config for more details.&lt;/p&gt; &#xA;&lt;h1&gt;Signaler service&lt;/h1&gt; &#xA;&lt;p&gt;Every minute, the signaler performs the following steps to make a prediction about whether the price is likely to increase or decrease:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Retrieve the latest data from the server and update the current data window which includes some history (the history length is defined by a configuration parameter)&lt;/li&gt; &#xA; &lt;li&gt;Compute derived features based on the nearest history collected (which now includes the latest data). The features to be computed are described in the configuration file and are exactly the same as used in batch mode during model training&lt;/li&gt; &#xA; &lt;li&gt;Apply several (previously trained) ML models by forecasting some future values (not necessarily prices) which are also treated as (more complex) derived features. We apply several forecasting models (currently, Gradient Boosting, Neural network, and Linear regression) to several target variables (labels)&lt;/li&gt; &#xA; &lt;li&gt;Aggregate the results of forecasting produced by different ML models and compute the final signal score which reflects the strength of the upward or downward trend. Here we use many previously computed scores as inputs and derive one output score. Currently, it is implemented as an aggregation procedure but it could be based on a dedicated ML model trained on previously collected scores and the target variable. Positive score means growth and negative score means fall&lt;/li&gt; &#xA; &lt;li&gt;Use the final score for notifications&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The final result of the signaler is the score (between -1 and +1). The score should be used for further decisions about buying or selling by taking into account other parameters and data sources&lt;/li&gt; &#xA; &lt;li&gt;For the signaler service to work, trained models have to be available and stored in the &#34;MODELS&#34; folder. The models are trained in batch mode and the process is described in the corresponding section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Starting the service: &lt;code&gt;python3 -m service.server -c config.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Trader&lt;/h1&gt; &#xA;&lt;p&gt;The trader is working but not thoroughly debugged, particularly, not tested for stability and reliability. Therefore, it should be considered a prototype with basic functionality. It is currently integrated with the Signaler but in a better design should be a separate service.&lt;/p&gt; &#xA;&lt;h1&gt;Related projects&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CryptoSignal/Crypto-Signal&#34;&gt;https://github.com/CryptoSignal/Crypto-Signal&lt;/a&gt; Github.com/CryptoSignal - #1 Quant Trading &amp;amp; Technical Analysis Bot&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensortrade-org/tensortrade&#34;&gt;https://github.com/tensortrade-org/tensortrade&lt;/a&gt; An open source reinforcement learning framework for training, evaluating, and deploying robust trading agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Superalgos/Superalgos&#34;&gt;https://github.com/Superalgos/Superalgos&lt;/a&gt; Free, open-source crypto trading bot, automated bitcoin / cryptocurrency trading software, algorithmic trading bots. Visually design your crypto trading bot, leveraging an integrated charting system, data-mining, backtesting, paper trading, and multi-server crypto bot deployments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kieran-mackle/AutoTrader&#34;&gt;https://github.com/kieran-mackle/AutoTrader&lt;/a&gt; A Python-based development platform for automated trading systems - from backtesting to optimisation to livetrading&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/areed1192/python-trading-robot&#34;&gt;https://github.com/areed1192/python-trading-robot&lt;/a&gt; A trading robot, that can submit basic orders in an automated fashion using the TD API&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jmrichardson/tuneta&#34;&gt;https://github.com/jmrichardson/tuneta&lt;/a&gt; Intelligently optimizes technical indicators and optionally selects the least intercorrelated for use in machine learning models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Erfaniaa/binance-futures-trading-bot&#34;&gt;https://github.com/Erfaniaa/binance-futures-trading-bot&lt;/a&gt; Easy-to-use multi-strategic automatic trading for Binance Futures with Telegram integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/smileinnovation/cryptocurrency-trading&#34;&gt;https://github.com/smileinnovation/cryptocurrency-trading&lt;/a&gt; How to make profits in cryptocurrency trading with machine learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Backtesting&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mementum/backtrader&#34;&gt;https://github.com/mementum/backtrader&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kernc/backtesting.py&#34;&gt;https://github.com/kernc/backtesting.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;External integrations&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ccxt/ccxt&#34;&gt;https://github.com/ccxt/ccxt&lt;/a&gt; A JavaScript / Python / PHP cryptocurrency trading API with support for more than 100 bitcoin/altcoin exchanges&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aiogram/aiogram&#34;&gt;https://github.com/aiogram/aiogram&lt;/a&gt; Is a pretty simple and fully asynchronous framework for Telegram Bot API&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sammchardy/python-binance&#34;&gt;https://github.com/sammchardy/python-binance&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>pymupdf/PyMuPDF</title>
    <updated>2024-08-22T01:33:20Z</updated>
    <id>tag:github.com,2024-08-22:/pymupdf/PyMuPDF</id>
    <link href="https://github.com/pymupdf/PyMuPDF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyMuPDF is a high performance Python library for data extraction, analysis, conversion &amp; manipulation of PDF (and other) documents.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PyMuPDF&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyMuPDF&lt;/strong&gt; is a high performance &lt;strong&gt;Python&lt;/strong&gt; library for data extraction, analysis, conversion &amp;amp; manipulation of &lt;a href=&#34;https://pymupdf.readthedocs.io/en/latest/the-basics.html#supported-file-types&#34;&gt;PDF (and other) documents&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;p&gt;Join us on &lt;strong&gt;Discord&lt;/strong&gt; here: &lt;a href=&#34;https://discord.gg/TSpYGBW4eq&#34;&gt;#pymupdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyMuPDF&lt;/strong&gt; requires &lt;strong&gt;Python 3.8 or later&lt;/strong&gt;, install using &lt;strong&gt;pip&lt;/strong&gt; with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install PyMuPDF&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are &lt;strong&gt;no mandatory&lt;/strong&gt; external dependencies. However, some &lt;a href=&#34;https://raw.githubusercontent.com/pymupdf/PyMuPDF/main/#pymupdf-optional-features&#34;&gt;optional features&lt;/a&gt; become available only if additional packages are installed.&lt;/p&gt; &#xA;&lt;p&gt;You can also try without installing by visiting &lt;a href=&#34;https://pymupdf.io/#examples&#34;&gt;PyMuPDF.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Basic usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymupdf # imports the pymupdf library&#xA;doc = pymupdf.open(&#34;example.pdf&#34;) # open a document&#xA;for page in doc: # iterate the document pages&#xA;  text = page.get_text() # get plain text encoded as UTF-8&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;Full documentation can be found on &lt;a href=&#34;https://pymupdf.readthedocs.io&#34;&gt;pymupdf.readthedocs.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a id=&#34;pymupdf-optional-features&#34;&gt;&lt;/a&gt;Optional Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/fonttools/&#34;&gt;fontTools&lt;/a&gt; for creating font subsets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pypi.org/project/pymupdf-fonts/&#34;&gt;pymupdf-fonts&lt;/a&gt; contains some nice fonts for your text output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tesseract-ocr/tesseract&#34;&gt;Tesseract-OCR&lt;/a&gt; for optical character recognition in images and document pages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyMuPDF&lt;/strong&gt; adds &lt;strong&gt;Python&lt;/strong&gt; bindings and abstractions to &lt;a href=&#34;https://mupdf.com/&#34;&gt;MuPDF&lt;/a&gt;, a lightweight &lt;strong&gt;PDF&lt;/strong&gt;, &lt;strong&gt;XPS&lt;/strong&gt;, and &lt;strong&gt;eBook&lt;/strong&gt; viewer, renderer, and toolkit. Both &lt;strong&gt;PyMuPDF&lt;/strong&gt; and &lt;strong&gt;MuPDF&lt;/strong&gt; are maintained and developed by &lt;a href=&#34;https://artifex.com&#34;&gt;Artifex Software, Inc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyMuPDF&lt;/strong&gt; was originally written by &lt;a href=&#34;mailto:jorj.x.mckie@outlook.de&#34;&gt;Jorj X. McKie&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License and Copyright&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyMuPDF&lt;/strong&gt; is available under &lt;a href=&#34;https://www.gnu.org/licenses/agpl-3.0.html&#34;&gt;open-source AGPL&lt;/a&gt; and commercial license agreements. If you determine you cannot meet the requirements of the &lt;strong&gt;AGPL&lt;/strong&gt;, please contact &lt;a href=&#34;https://artifex.com/contact/pymupdf-inquiry.php&#34;&gt;Artifex&lt;/a&gt; for more information regarding a commercial license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/VILA</title>
    <updated>2024-08-22T01:33:20Z</updated>
    <id>tag:github.com,2024-08-22:/NVlabs/VILA</id>
    <link href="https://github.com/NVlabs/VILA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VILA - a multi-image visual language model with training, inference and evaluation recipe, deployable from cloud to edge (Jetson Orin and laptops)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/vila-logo.jpg&#34; width=&#34;20%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;VILA: On Pre-training for Visual Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/CODE_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/MODEL_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MODEL%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Model License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;VILA arxiv&lt;/a&gt; / &lt;a href=&#34;https://vila-demo.hanlab.ai/&#34;&gt;VILA Demo&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/vila-on-pre-training-for-visual-language-models-65d8022a3a52cd9bcd62698e&#34;&gt;VILA Huggingface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí° Introduction&lt;/h2&gt; &#xA;&lt;p&gt;VILA is a visual language model (VLM) pretrained with interleaved image-text data at scale, enabling &lt;strong&gt;video understanding&lt;/strong&gt; and &lt;strong&gt;multi-image understanding&lt;/strong&gt; capabilities. VILA is deployable on the edge by &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt; 4bit quantization and &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; framework. We find: (1) image-text pairs are not enough, interleaved image-text is essential; (2) unfreezing LLM during interleaved image-text pre-training enables in-context learning; (3)re-blending text-only instruction data is crucial to boost both VLM and text-only performance; (4) token compression extends #video frames. VILA unveils appealing capabilities, including: video reasoning, in-context learning, visual chain-of-thought, and better world knowledge.&lt;/p&gt; &#xA;&lt;h2&gt;üí° News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/08] We release &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/LongVILA.md&#34;&gt;LongVILA&lt;/a&gt; that supports long video understanding (Captioning, QA, Needle-in-a-Haystack) up to 1024 frames.&lt;/li&gt; &#xA; &lt;li&gt;[2024/07] VILA1.5 also ranks 1st place (OSS model) on &lt;a href=&#34;https://github.com/JUNJIE99/MLVU&#34;&gt;MLVU test leaderboard&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] VILA1.5 is now the best open sourced VLM on &lt;a href=&#34;https://mmmu-benchmark.github.io/#leaderboard&#34;&gt;MMMU leaderboard&lt;/a&gt; and &lt;a href=&#34;https://video-mme.github.io/home_page.html#leaderboard&#34;&gt;Video-MME&lt;/a&gt; leaderboard!&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We release VILA-1.5, which offers &lt;strong&gt;video understanding capability&lt;/strong&gt;. VILA-1.5 comes with four model sizes: 3B/8B/13B/40B.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA-1.5 models. VILA-1.5 is efficiently deployable on diverse NVIDIA GPUs (A100, 4090, 4070 Laptop, Orin, Orin Nano) by &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_trt_llm&#34;&gt;TensorRT-LLM&lt;/a&gt; backends.&lt;/li&gt; &#xA; &lt;li&gt;[2024/03] VILA has been accepted by CVPR 2024!&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA models, deployable on Jetson Orin and laptops through &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] VILA is released. We propose interleaved image-text pretraining that enables &lt;strong&gt;multi-image&lt;/strong&gt; VLM. VILA comes with impressive in-context learning capabilities. We open source everything: including training code, evaluation code, datasets, model ckpts.&lt;/li&gt; &#xA; &lt;li&gt;[2023/12] &lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;Paper&lt;/a&gt; is on Arxiv!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;h3&gt;Image QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;VizWiz&lt;/th&gt; &#xA;   &lt;th&gt;SQA-I&lt;/th&gt; &#xA;   &lt;th&gt;VQA-T&lt;/th&gt; &#xA;   &lt;th&gt;POPE&lt;/th&gt; &#xA;   &lt;th&gt;MME&lt;/th&gt; &#xA;   &lt;th&gt;MMB&lt;/th&gt; &#xA;   &lt;th&gt;MMB-CN&lt;/th&gt; &#xA;   &lt;th&gt;SEED&lt;/th&gt; &#xA;   &lt;th&gt;SEED-I&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (val)&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (test)&lt;/th&gt; &#xA;   &lt;th&gt;llava-bench&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.4&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;53.5&lt;/td&gt; &#xA;   &lt;td&gt;69.0&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1442.44&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;52.7&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;   &lt;td&gt;67.9&lt;/td&gt; &#xA;   &lt;td&gt;33.3&lt;/td&gt; &#xA;   &lt;td&gt;30.8&lt;/td&gt; &#xA;   &lt;td&gt;75.9&lt;/td&gt; &#xA;   &lt;td&gt;35.4&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;61.1&lt;/td&gt; &#xA;   &lt;td&gt;53.8&lt;/td&gt; &#xA;   &lt;td&gt;67.8&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1437.34&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;   &lt;td&gt;66.6&lt;/td&gt; &#xA;   &lt;td&gt;32.7&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;   &lt;td&gt;75.0&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;   &lt;td&gt;59.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;79.8&lt;/td&gt; &#xA;   &lt;td&gt;61.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;69.6&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;85.3&lt;/td&gt; &#xA;   &lt;td&gt;1431.65&lt;/td&gt; &#xA;   &lt;td&gt;62.8&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;   &lt;td&gt;60.0&lt;/td&gt; &#xA;   &lt;td&gt;66.4&lt;/td&gt; &#xA;   &lt;td&gt;32.8&lt;/td&gt; &#xA;   &lt;td&gt;31.3&lt;/td&gt; &#xA;   &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;td&gt;38.6&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;79.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;69.2&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;85.8&lt;/td&gt; &#xA;   &lt;td&gt;1417.06&lt;/td&gt; &#xA;   &lt;td&gt;61.6&lt;/td&gt; &#xA;   &lt;td&gt;51.5&lt;/td&gt; &#xA;   &lt;td&gt;59.1&lt;/td&gt; &#xA;   &lt;td&gt;65.7&lt;/td&gt; &#xA;   &lt;td&gt;33.4&lt;/td&gt; &#xA;   &lt;td&gt;30.4&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;36.7&lt;/td&gt; &#xA;   &lt;td&gt;60.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.9&lt;/td&gt; &#xA;   &lt;td&gt;61.9&lt;/td&gt; &#xA;   &lt;td&gt;58.7&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;84.4&lt;/td&gt; &#xA;   &lt;td&gt;1577.01&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;66.2&lt;/td&gt; &#xA;   &lt;td&gt;64.2&lt;/td&gt; &#xA;   &lt;td&gt;71.4&lt;/td&gt; &#xA;   &lt;td&gt;36.9&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;38.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.3&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;65.4&lt;/td&gt; &#xA;   &lt;td&gt;82.9&lt;/td&gt; &#xA;   &lt;td&gt;1593.65&lt;/td&gt; &#xA;   &lt;td&gt;71.0&lt;/td&gt; &#xA;   &lt;td&gt;64.9&lt;/td&gt; &#xA;   &lt;td&gt;64.0&lt;/td&gt; &#xA;   &lt;td&gt;71.1&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;36.1&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;37.2&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;82.8&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;td&gt;86.3&lt;/td&gt; &#xA;   &lt;td&gt;1569.55&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;   &lt;td&gt;33.6&lt;/td&gt; &#xA;   &lt;td&gt;80.8&lt;/td&gt; &#xA;   &lt;td&gt;44.3&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;82.7&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;79.7&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;1531.35&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;66.7&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.8&lt;/td&gt; &#xA;   &lt;td&gt;34.0&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;46.4&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;84.3&lt;/td&gt; &#xA;   &lt;td&gt;64.6&lt;/td&gt; &#xA;   &lt;td&gt;62.2&lt;/td&gt; &#xA;   &lt;td&gt;87.2&lt;/td&gt; &#xA;   &lt;td&gt;73.6&lt;/td&gt; &#xA;   &lt;td&gt;87.3&lt;/td&gt; &#xA;   &lt;td&gt;1726.82&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;   &lt;td&gt;80.2&lt;/td&gt; &#xA;   &lt;td&gt;69.1&lt;/td&gt; &#xA;   &lt;td&gt;75.8&lt;/td&gt; &#xA;   &lt;td&gt;51.9&lt;/td&gt; &#xA;   &lt;td&gt;46.9&lt;/td&gt; &#xA;   &lt;td&gt;81.3&lt;/td&gt; &#xA;   &lt;td&gt;53.0&lt;/td&gt; &#xA;   &lt;td&gt;72.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;84.1&lt;/td&gt; &#xA;   &lt;td&gt;64.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;88.2&lt;/td&gt; &#xA;   &lt;td&gt;1714.79&lt;/td&gt; &#xA;   &lt;td&gt;83.2&lt;/td&gt; &#xA;   &lt;td&gt;79.6&lt;/td&gt; &#xA;   &lt;td&gt;68.9&lt;/td&gt; &#xA;   &lt;td&gt;75.6&lt;/td&gt; &#xA;   &lt;td&gt;49.3&lt;/td&gt; &#xA;   &lt;td&gt;46.2&lt;/td&gt; &#xA;   &lt;td&gt;83.0&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;72.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: VQAV2 and VizWiz are test-dev, the average accuracy is calculated over all datasets and MME numbers are divided by 20.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;Perception Test&lt;/th&gt; &#xA;   &lt;th&gt;ActivityNet&lt;/th&gt; &#xA;   &lt;th&gt;MSVD&lt;/th&gt; &#xA;   &lt;th&gt;MSRVTT&lt;/th&gt; &#xA;   &lt;th&gt;TGIF&lt;/th&gt; &#xA;   &lt;th&gt;EgoSchema (test)&lt;/th&gt; &#xA;   &lt;th&gt;CinePile&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;50.2&lt;/td&gt; &#xA;   &lt;td&gt;76.6&lt;/td&gt; &#xA;   &lt;td&gt;57.5&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;   &lt;td&gt;42.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;50.7&lt;/td&gt; &#xA;   &lt;td&gt;76.9&lt;/td&gt; &#xA;   &lt;td&gt;57.6&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;   &lt;td&gt;54.3&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;   &lt;td&gt;50.4&lt;/td&gt; &#xA;   &lt;td&gt;48.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;53.6&lt;/td&gt; &#xA;   &lt;td&gt;54.7&lt;/td&gt; &#xA;   &lt;td&gt;77.9&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;   &lt;td&gt;50.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;58.2&lt;/td&gt; &#xA;   &lt;td&gt;58.7&lt;/td&gt; &#xA;   &lt;td&gt;51.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference speed ( Token/sec )&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;A100&lt;/th&gt; &#xA;   &lt;th&gt;4090&lt;/th&gt; &#xA;   &lt;th&gt;Orin&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.6&lt;/td&gt; &#xA;   &lt;td&gt;137.6&lt;/td&gt; &#xA;   &lt;td&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;182.8&lt;/td&gt; &#xA;   &lt;td&gt;215.5&lt;/td&gt; &#xA;   &lt;td&gt;42.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.3&lt;/td&gt; &#xA;   &lt;td&gt;137.2&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;180.2&lt;/td&gt; &#xA;   &lt;td&gt;219.3&lt;/td&gt; &#xA;   &lt;td&gt;40.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;57.4&lt;/td&gt; &#xA;   &lt;td&gt;10.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;168.9&lt;/td&gt; &#xA;   &lt;td&gt;150.2&lt;/td&gt; &#xA;   &lt;td&gt;28.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;6.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;115.9&lt;/td&gt; &#xA;   &lt;td&gt;105.7&lt;/td&gt; &#xA;   &lt;td&gt;20.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: Measured using the &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tinychat&#34;&gt;TinyChat&lt;/a&gt; backend at batch size = 1.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VILA Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Video captioning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: Elaborate on the visual and narrative elements of the video in detail.&lt;/p&gt; &#xA;&lt;p&gt;Caption: The video shows a person&#39;s hands working on a white surface. They are folding a piece of fabric with a checkered pattern in shades of blue and white. The fabric is being folded into a smaller, more compact shape. The person&#39;s fingernails are painted red, and they are wearing a black and red garment. There are also a ruler and a pencil on the surface, suggesting that measurements and precision are involved in the process.&lt;/p&gt; &#xA;&lt;h3&gt;In context learning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_1.png&#34; height=&#34;239&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_2.png&#34; height=&#34;250&#34;&gt; &#xA;&lt;h3&gt;Multi-image reasoning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_3.png&#34; height=&#34;193&#34;&gt; &#xA;&lt;h3&gt;VILA on Jetson Orin&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;VILA on RTX 4090&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./environment_setup.sh vila&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;VILA training contains three steps, for specific hyperparameters, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/scripts/v1_5&#34;&gt;scripts/v1_5&lt;/a&gt; folder:&lt;/p&gt; &#xA;&lt;h3&gt;Step-1: Alignment&lt;/h3&gt; &#xA;&lt;p&gt;We utilize LLaVA-CC3M-Pretrain-595K dataset to align the textual and visual modalities.&lt;/p&gt; &#xA;&lt;p&gt;The stage 1 script takes in two parameters and it can run on a single 8xA100 node. &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; points to a online or local huggingface repository, such as &lt;code&gt;NousResearch/Llama-2-7b-hf&lt;/code&gt;. &lt;code&gt;OUTPUT_NAME&lt;/code&gt; points to a target directory under &lt;code&gt;checkpoints&lt;/code&gt;, which will save the trained multimodal projector afterwards.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/1_mm_align.sh [BASE_MODEL_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step-2: Pretraining&lt;/h3&gt; &#xA;&lt;p&gt;We use MMC4 and Coyo dataset to train VLM with interleaved image-text pairs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/2_pretrain_mmc4_coyo.sh [CODE_PATH] [BASE_MODEL_PATH] [STAGE1_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 2 script takes in four arguments. &lt;code&gt;CODE_PATH&lt;/code&gt; is the absolute path to our VILA codebase, &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; has similar meaning to what is presented in the stage 1 script. &lt;code&gt;STAGE1_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of stage 1 (i.e. where the stage 1 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that saves the pretraining checkpoint. The script we provided for this stage is executed on slurm, and we expect it to execute on 16 nodes (128 GPUs).&lt;/p&gt; &#xA;&lt;h3&gt;Step-3: Supervised fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;This is the last stage of VILA training, in which we tune the model to follow multimodal instructions on a subset of M3IT, FLAN and ShareGPT4V. This stage runs on a 8xA100 node.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/3_sft.sh [STAGE2_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 3 script takes in two arguments. &lt;code&gt;STAGE2_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of the stage 2 script (i.e. where the stage 2 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that stores the final checkpoint.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;h3&gt;Image Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;You can follow &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;Llava1.5 eval&lt;/a&gt; to download all datasets. After downloading all datasets, please put them under &lt;code&gt;playground/data/eval&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please make the following changes to the MME evaluation script. Please search for:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path = &#34;MME_Benchmark_release_version&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and replace it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path = os.path.join(script_dir, &#34;MME_Benchmark_release_version&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide a push-the-button script to perform evaluation on all 10 datasets that do not require GPT-assisted evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/eval_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script takes in two parameters, &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; points to the stage 3 model checkpoint, and &lt;code&gt;MODEL_NAME&lt;/code&gt; will be the name of evaluation results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/830/my-submission&#34;&gt;VQAv2&lt;/a&gt; and &lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/2185/my-submission&#34;&gt;Vizwiz&lt;/a&gt; evaluations are hosted on eval.ai. You need to register an account and create a team to be able to submit eval.&lt;/p&gt; &#xA;&lt;p&gt;MMBench and MMBench_CN eval are hosted on another &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;evaluation server&lt;/a&gt;. Make sure you change the name of the file before submitting, otherwise the server caches results and will always return wrong result to you.&lt;/p&gt; &#xA;&lt;p&gt;We provide a quick script to automatically organize the prediction files that need to be submitted to servers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/v1_5/eval/copy_predictions.py [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will be able to find the predictions under &lt;code&gt;playground/data/predictions_upload/[MODEL_NAME]&lt;/code&gt; after executing this script.&lt;/p&gt; &#xA;&lt;h3&gt;Video Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the evaluation steps in &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/TRAIN_AND_VALIDATE.md#data-for-validating&#34;&gt;Video-LLaVA&lt;/a&gt; for dataset preparation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/video_chatgpt/run_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;./scripts/v1_5/eval/video_chatgpt/eval_all.sh [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We provide snippets for quick inference with user prompts and images.&lt;/p&gt; &#xA;&lt;p&gt;Llama-3-VILA1.5-8B inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/Llama-3-VILA1.5-8b \&#xA;    --conv-mode llama_3 \&#xA;    --query &#34;&amp;lt;image&amp;gt;\n Please describe the traffic condition.&#34; \&#xA;    --image-file &#34;av.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VILA1.5-40B inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/VILA1.5-40b \&#xA;    --conv-mode hermes-2 \&#xA;    --query &#34;&amp;lt;image&amp;gt;\n Please describe the traffic condition.&#34; \&#xA;    --image-file &#34;av.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VILA1.5-3B video inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/VILA1.5-3b \&#xA;    --conv-mode vicuna_v1 \&#xA;    --query &#34;&amp;lt;video&amp;gt;\n Please describe this video.&#34; \&#xA;    --video-file &#34;demo.mp4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization and Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Our VILA models are quantized by &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt; into 4 bits for efficient inference on the edge. We provide a push-the-button &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/raw/main/scripts/vila_example.sh&#34;&gt;script&lt;/a&gt; to quantize VILA with AWQ.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on desktop GPUs and edge GPUs&lt;/h3&gt; &#xA;&lt;p&gt;We support AWQ-quantized 4bit VILA on GPU platforms via &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt;. We provide a &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat#support-vlm-models-vila--llava&#34;&gt;tutorial&lt;/a&gt; to run the model with TinyChat after quantization. We also provide an &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat/serve&#34;&gt;instruction&lt;/a&gt; to launch a Gradio server (powered by TinyChat and AWQ) to serve 4-bit quantized VILA models.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on laptops&lt;/h3&gt; &#xA;&lt;p&gt;We further support our AWQ-quantized 4bit VILA models on various CPU platforms with both x86 and ARM architectures with our &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;. We also provide a detailed &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine/tree/main?tab=readme-ov-file#deploy-vision-language-model-vlm-chatbot-with-tinychatengine&#34;&gt;tutorial&lt;/a&gt; to help the users deploy VILA on different CPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA API server&lt;/h3&gt; &#xA;&lt;p&gt;A simple API server has been provided to serve VILA models. The server is built on top of &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/transformers/&#34;&gt;Huggingface Transformers&lt;/a&gt;. The server can be run with the following command:&lt;/p&gt; &#xA;&lt;h4&gt;With CLI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore server.py \&#xA;    --port 8000 \&#xA;    --model-path Efficient-Large-Model/VILA1.5-3B \&#xA;    --conv-mode vicuna_v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;With Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t vila-server:latest .&#xA;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \&#xA;    -v ./hub:/root/.cache/huggingface/hub \&#xA;    -it --rm -p 8000:8000 \&#xA;    -e VILA_MODEL_PATH=Efficient-Large-Model/VILA1.5-3B \&#xA;    -e VILA_CONV_MODE=vicuna_v1 \&#xA;    vila-server:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can call the endpoint with the OpenAI SDK as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    base_url=&#34;http://localhost:8000&#34;,&#xA;    api_key=&#34;fake-key&#34;,&#xA;)&#xA;response = client.chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What‚Äôs in this image?&#34;},&#xA;                {&#xA;                    &#34;type&#34;: &#34;image_url&#34;,&#xA;                    &#34;image_url&#34;: {&#xA;                        &#34;url&#34;: &#34;https://blog.logomyway.com/wp-content/uploads/2022/01/NVIDIA-logo.jpg&#34;,&#xA;                        # Or you can pass in a base64 encoded image&#xA;                        # &#34;url&#34;: &#34;data:image/png;base64,&amp;lt;base64_encoded_image&amp;gt;&#34;,&#xA;                    },&#xA;                },&#xA;            ],&#xA;        }&#xA;    ],&#xA;    max_tokens=300,&#xA;    model=&#34;VILA1.5-3B&#34;,&#xA;    # You can pass in extra parameters as follows&#xA;    extra_body={&#34;num_beams&#34;: 1, &#34;use_cache&#34;: False},&#xA;)&#xA;print(response.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: This API server is intended for evaluation purposes only and has not been optimized for production use. It has only been tested on A100 and H100 GPUs.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b&#34;&gt;VILA1.5-3B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2&#34;&gt;VILA1.5-3B-S2&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b&#34;&gt;Llama-3-VILA1.5-8B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b&#34;&gt;VILA1.5-13B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b&#34;&gt;VILA1.5-40B&lt;/a&gt; and the 4-bit &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;-quantized models &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-AWQ&#34;&gt;VILA1.5-3B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2-AWQ&#34;&gt;VILA1.5-3B-S2-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b-AWQ&#34;&gt;Llama-3-VILA1.5-8B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b-AWQ&#34;&gt;VILA1.5-13B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b-AWQ&#34;&gt;VILA1.5-40B-AWQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üîí License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The pretrained weights are released under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en&#34;&gt;CC-BY-NC-SA-4.0 license&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only, and is subject to the following licenses and terms: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Model License&lt;/a&gt; of LLaMA. For LLAMA3-VILA checkpoints terms of use, please refer to the &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;LLAMA3 License&lt;/a&gt; for additional details.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;Terms of Use&lt;/a&gt; of the data generated by OpenAI&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/data_prepare/LICENSE&#34;&gt;Dataset Licenses&lt;/a&gt; for each one used during training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Team&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=OI7zFmwAAAAJ&amp;amp;hl=en&#34;&gt;*Yao Lu&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hongxu-yin.github.io/&#34;&gt;*Hongxu Yin&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linji.me/&#34;&gt;*Ji Lin&lt;/a&gt;: OpenAI (work done at Nvidia and MIT)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6gKEYRgAAAAJ&amp;amp;hl=en&#34;&gt;Wei Ping&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pmolchanov.com/&#34;&gt;Pavlo Molchanov&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=Wel9l1wAAAAJ&amp;amp;hl=en&#34;&gt;Andrew Tao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://kentang.net/&#34;&gt;Haotian Tang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ys-2020.github.io/&#34;&gt;Shang Yang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lzhu.me/&#34;&gt;Ligeng Zhu&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://weichenwang.me/&#34;&gt;Wei-Chen Wang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://xuefuzhao.github.io/&#34;&gt;Fuzhao Xue&lt;/a&gt;: Nvidia, NUS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seerkfang.github.io/&#34;&gt;Yunhao Fang&lt;/a&gt;: Nvidia, UCSD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://yukangchen.com/&#34;&gt;Yukang Chen&lt;/a&gt;: Nvidia, CUHK&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/profile?id=~Zhuoyang_Zhang1&#34;&gt;Zhuoyang Zhang&lt;/a&gt;: Nvidia, Tsinghua Univ.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/yue-james-shen/&#34;&gt;Yue Shen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6xFvyJwAAAAJ&amp;amp;hl=en&#34;&gt;Wei-Ming Chen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=r5WezOYAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Huizi Mao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bfshi.github.io/&#34;&gt;Baifeng Shi&lt;/a&gt;: Nvidia, UC Berkeley&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jankautz.com/&#34;&gt;Jan Kautz&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=62ElavIAAAAJ&amp;amp;hl=en&#34;&gt;Mohammad Shoeybi&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://songhan.mit.edu/&#34;&gt;Song Han&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lin2023vila,&#xA;      title={VILA: On Pre-training for Visual Language Models},&#xA;      author={Ji Lin and Hongxu Yin and Wei Ping and Yao Lu and Pavlo Molchanov and Andrew Tao and Huizi Mao and Jan Kautz and Mohammad Shoeybi and Song Han},&#xA;      year={2023},&#xA;      eprint={2312.07533},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: the codebase we built upon. Thanks for their wonderful work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;InternVL&lt;/a&gt;: for open-sourcing InternViT (used in VILA1.5-40b) and the &lt;a href=&#34;https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets&#34;&gt;InternVL-SFT&lt;/a&gt; data blend (inspired by LLaVA-1.6) used in all VILA1.5 models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the amazing open-sourced large language model!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;Video-ChatGPT&lt;/a&gt;: we borrowed video evaluation script from this repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/allenai/mmc4&#34;&gt;MMC4&lt;/a&gt;, &lt;a href=&#34;https://github.com/kakaobrain/coyo-dataset&#34;&gt;COYO-700M&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/MMInstruction/M3IT&#34;&gt;M3IT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/FLAN&#34;&gt;OpenORCA/FLAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/google-research-datasets/wit&#34;&gt;WIT&lt;/a&gt;, &lt;a href=&#34;https://github.com/OFA-Sys/gsm8k-ScRel/raw/main/data/train_use.jsonl&#34;&gt;GSM8K-ScRel&lt;/a&gt;, &lt;a href=&#34;https://visualgenome.org/api/v0/api_home.html&#34;&gt;VisualGenome&lt;/a&gt;, &lt;a href=&#34;https://visualcommonsense.com/download/&#34;&gt;VCR&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/derek-thomas/ScienceQA&#34;&gt;ScienceQA&lt;/a&gt;, &lt;a href=&#34;https://github.com/bytedance/Shot2Story/raw/master/DATA.md&#34;&gt;Shot2Story&lt;/a&gt;, &lt;a href=&#34;http://youcook2.eecs.umich.edu/&#34;&gt;Youcook2&lt;/a&gt;, &lt;a href=&#34;https://eric-xw.github.io/vatex-website/download.html&#34;&gt;Vatex&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction&#34;&gt;ShareGPT-Video&lt;/a&gt; for providing datasets used in this research.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>