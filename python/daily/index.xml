<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-31T01:39:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>EricGuo5513/momask-codes</title>
    <updated>2024-12-31T01:39:54Z</updated>
    <id>tag:github.com,2024-12-31:/EricGuo5513/momask-codes</id>
    <link href="https://github.com/EricGuo5513/momask-codes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;MoMask: Generative Masked Modeling of 3D Human Motions&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MoMask: Generative Masked Modeling of 3D Human Motions&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://ericguo5513.github.io/momask&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.00063&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/MeYourHint/MoMask&#34;&gt;[Huggingface Demo]&lt;/a&gt; &lt;a href=&#34;https://github.com/camenduru/MoMask-colab&#34;&gt;[Colab Demo]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://ericguo5513.github.io/momask/static/images/teaser.png&#34; alt=&#34;teaser_image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find our code or paper helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{guo2023momask,&#xA;      title={MoMask: Generative Masked Modeling of 3D Human Motions}, &#xA;      author={Chuan Guo and Yuxuan Mu and Muhammad Gohar Javed and Sen Wang and Li Cheng},&#xA;      year={2023},&#xA;      eprint={2312.00063},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìÆ&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;p&gt;üì¢ &lt;strong&gt;2023-12-30&lt;/strong&gt; --- For easy webUI BVH visulization, you could try this website &lt;a href=&#34;https://vrm-c.github.io/bvh2vrma/&#34;&gt;bvh2vrma&lt;/a&gt; from this &lt;a href=&#34;https://github.com/vrm-c/bvh2vrma?tab=readme-ov-file&#34;&gt;github&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üì¢ &lt;strong&gt;2023-12-29&lt;/strong&gt; --- Thanks to Camenduru for supporting the &lt;a href=&#34;https://github.com/camenduru/MoMask-colab&#34;&gt;ü§óColab&lt;/a&gt; demo.&lt;/p&gt; &#xA;&lt;p&gt;üì¢ &lt;strong&gt;2023-12-27&lt;/strong&gt; --- Release WebUI demo. Quickly try our work on &lt;a href=&#34;https://huggingface.co/spaces/MeYourHint/MoMask&#34;&gt;ü§óHuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üì¢ &lt;strong&gt;2023-12-19&lt;/strong&gt; --- Release scripts for temporal inpainting.&lt;/p&gt; &#xA;&lt;p&gt;üì¢ &lt;strong&gt;2023-12-15&lt;/strong&gt; --- Release codes and models for momask. Including training/eval/generation scripts.&lt;/p&gt; &#xA;&lt;p&gt;üì¢ &lt;strong&gt;2023-11-29&lt;/strong&gt; --- Initialized the webpage and git project.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìç&lt;/span&gt; Get You Ready&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h3&gt;1. Conda Environment&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate momask&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;We test our code on Python 3.7.13 and PyTorch 1.7.1&lt;/p&gt; &#xA; &lt;h4&gt;Alternative: Pip Installation&lt;/h4&gt; &#xA; &lt;details&gt;&#xA;   We provide an alternative pip installation in case you encounter difficulties setting up the conda environment. &#xA;  &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;h3&gt;2. Models and Dependencies&lt;/h3&gt; &#xA; &lt;h4&gt;Download Pre-trained Models&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code&gt;bash prepare/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Download Evaluation Models and Gloves&lt;/h4&gt; &#xA; &lt;p&gt;For evaluation only.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;bash prepare/download_evaluator.sh&#xA;bash prepare/download_glove.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA; &lt;p&gt;To address the download error related to gdown: &#34;Cannot retrieve the public link of the file. You may need to change the permission to &#39;Anyone with the link&#39;, or have had many accesses&#34;. A potential solution is to run &lt;code&gt;pip install --upgrade --no-cache-dir gdown&lt;/code&gt;, as suggested on &lt;a href=&#34;https://github.com/wkentaro/gdown/issues/43&#34;&gt;https://github.com/wkentaro/gdown/issues/43&lt;/a&gt;. This should help resolve the issue.&lt;/p&gt; &#xA; &lt;h4&gt;(Optional) Download Manually&lt;/h4&gt; &#xA; &lt;p&gt;Visit &lt;a href=&#34;https://drive.google.com/drive/folders/1b3GnAbERH8jAoO5mdWgZhyxHB73n23sK?usp=drive_link&#34;&gt;[Google Drive]&lt;/a&gt; to download the models and evaluators mannually.&lt;/p&gt; &#xA; &lt;h3&gt;3. Get Data&lt;/h3&gt; &#xA; &lt;p&gt;You have two options here:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Skip getting data&lt;/strong&gt;, if you just want to generate motions using &lt;em&gt;own&lt;/em&gt; descriptions.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Get full data&lt;/strong&gt;, if you want to &lt;em&gt;re-train&lt;/em&gt; and &lt;em&gt;evaluate&lt;/em&gt; the model.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;strong&gt;(a). Full data (text + motion)&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Follow the instruction in &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt;, then copy the result dataset to our repository:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;-Download from &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt;, then place result in &lt;code&gt;./dataset/KIT-ML&lt;/code&gt;&lt;/p&gt; &#xA; &lt;h4&gt;&lt;/h4&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Demo&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h3&gt;(a) Generate from a single prompt&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python gen_t2m.py --gpu_id 1 --ext exp1 --text_prompt &#34;A person is running on a treadmill.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;(b) Generate from a prompt file&lt;/h3&gt; &#xA; &lt;p&gt;An example of prompt file is given in &lt;code&gt;./assets/text_prompt.txt&lt;/code&gt;. Please follow the format of &lt;code&gt;&amp;lt;text description&amp;gt;#&amp;lt;motion length&amp;gt;&lt;/code&gt; at each line. Motion length indicates the number of poses, which must be integeter and will be rounded by 4. In our work, motion is in 20 fps.&lt;/p&gt; &#xA; &lt;p&gt;If you write &lt;code&gt;&amp;lt;text description&amp;gt;#NA&lt;/code&gt;, our model will determine a length. Note once there is &lt;strong&gt;one&lt;/strong&gt; NA, all the others will be &lt;strong&gt;NA&lt;/strong&gt; automatically.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python gen_t2m.py --gpu_id 1 --ext exp2 --text_path ./assets/text_prompt.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;A few more parameters you may be interested:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--repeat_times&lt;/code&gt;: number of replications for generation, default &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--motion_length&lt;/code&gt;: specify the number of poses for generation, only applicable in (a).&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The output files are stored under folder &lt;code&gt;./generation/&amp;lt;ext&amp;gt;/&lt;/code&gt;. They are&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;numpy files&lt;/code&gt;: generated motions with shape of (nframe, 22, 3), under subfolder &lt;code&gt;./joints&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;video files&lt;/code&gt;: stick figure animation in mp4 format, under subfolder &lt;code&gt;./animation&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;bvh files&lt;/code&gt;: bvh files of the generated motion, under subfolder &lt;code&gt;./animation&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;We also apply naive foot ik to the generated motions, see files with suffix &lt;code&gt;_ik&lt;/code&gt;. It sometimes works well, but sometimes will fail.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;üëØ&lt;/span&gt; Visualization&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;All the animations are manually rendered in blender. We use the characters from &lt;a href=&#34;https://www.mixamo.com/#/&#34;&gt;mixamo&lt;/a&gt;. You need to download the characters in T-Pose with skeleton.&lt;/p&gt; &#xA; &lt;h3&gt;Retargeting&lt;/h3&gt; &#xA; &lt;p&gt;For retargeting, we found rokoko usually leads to large error on foot. On the other hand, &lt;a href=&#34;https://github.com/nkeeline/Keemap-Blender-Rig-ReTargeting-Addon/releases&#34;&gt;keemap.rig.transfer&lt;/a&gt; shows more precise retargetting. You could watch the &lt;a href=&#34;https://www.youtube.com/watch?v=EG-VCMkVpxg&#34;&gt;tutorial&lt;/a&gt; here.&lt;/p&gt; &#xA; &lt;p&gt;Following these steps:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Download keemap.rig.transfer from the github, and install it in blender.&lt;/li&gt; &#xA;  &lt;li&gt;Import both the motion files (.bvh) and character files (.fbx) in blender.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;Shift + Select&lt;/code&gt; the both source and target skeleton. (Do not need to be Rest Position)&lt;/li&gt; &#xA;  &lt;li&gt;Switch to &lt;code&gt;Pose Mode&lt;/code&gt;, then unfold the &lt;code&gt;KeeMapRig&lt;/code&gt; tool at the top-right corner of the view window.&lt;/li&gt; &#xA;  &lt;li&gt;Load and read the bone mapping file &lt;code&gt;./assets/mapping.json&lt;/code&gt;(or &lt;code&gt;mapping6.json&lt;/code&gt; if it doesn&#39;t work). This file is manually made by us. It works for most characters in mixamo. You could make your own.&lt;/li&gt; &#xA;  &lt;li&gt;Adjust the &lt;code&gt;Number of Samples&lt;/code&gt;, &lt;code&gt;Source Rig&lt;/code&gt;, &lt;code&gt;Destination Rig Name&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Clik &lt;code&gt;Transfer Animation from Source Destination&lt;/code&gt;, wait a few seconds.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;We didn&#39;t tried other retargetting tools. Welcome to comment if you find others are more useful.&lt;/p&gt; &#xA; &lt;h3&gt;Scene&lt;/h3&gt; &#xA; &lt;p&gt;We use this &lt;a href=&#34;https://drive.google.com/file/d/1lg62nugD7RTAIz0Q_YP2iZsxpUzzOkT1/view?usp=sharing&#34;&gt;scene&lt;/a&gt; for animation.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;üé¨&lt;/span&gt; Temporal Inpainting&lt;/h2&gt; &#xA;&lt;details&gt;&#xA;  We conduct mask-based editing in the m-transformer stage, followed by the regeneration of residual tokens for the entire sequence. To load your own motion, provide the path through `--source_motion`. Utilize `-msec` to specify the mask section, supporting either ratio or frame index. For instance, `-msec 0.3,0.6` with `max_motion_length=196` is equivalent to `-msec 59,118`, indicating the editing of the frame section [59, 118]. &#xA; &lt;pre&gt;&lt;code&gt;python edit_t2m.py --gpu_id 1 --ext exp3 --use_res_model -msec 0.4,0.7 --text_prompt &#34;A man picks something from the ground using his right hand.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note: Presently, the source motion must adhere to the format of a HumanML3D dim-263 feature vector. An example motion vector data from the HumanML3D test set is available in &lt;code&gt;example_data/000612.npy&lt;/code&gt;. To process your own motion data, you can utilize the &lt;code&gt;process_file&lt;/code&gt; function from &lt;code&gt;utils/motion_process.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;üëæ&lt;/span&gt; Train Your Own Models&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You have to train RVQ &lt;strong&gt;BEFORE&lt;/strong&gt; training masked/residual transformers. The latter two can be trained simultaneously.&lt;/p&gt; &#xA; &lt;h3&gt;Train RVQ&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train_vq.py --name rvq_name --gpu_id 1 --dataset_name t2m --batch_size 512 --num_quantizers 6  --max_epoch 500 --quantize_drop_prob 0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Train Masked Transformer&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train_t2m_transformer.py --name mtrans_name --gpu_id 2 --dataset_name t2m --batch_size 64 --vq_name rvq_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Train Residual Transformer&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code&gt;python train_res_transformer.py --name rtrans_name  --gpu_id 2 --dataset_name t2m --batch_size 64 --vq_name rvq_name --cond_drop_prob 0.2 --share_weight&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--dataset_name&lt;/code&gt;: motion dataset, &lt;code&gt;t2m&lt;/code&gt; for HumanML3D and &lt;code&gt;kit&lt;/code&gt; for KIT-ML.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--name&lt;/code&gt;: name your model. This will create to model space as &lt;code&gt;./checkpoints/&amp;lt;dataset_name&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--gpu_id&lt;/code&gt;: GPU id.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--batch_size&lt;/code&gt;: we use &lt;code&gt;512&lt;/code&gt; for rvq training. For masked/residual transformer, we use &lt;code&gt;64&lt;/code&gt; on HumanML3D and &lt;code&gt;16&lt;/code&gt; for KIT-ML.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--num_quantizers&lt;/code&gt;: number of quantization layers, &lt;code&gt;6&lt;/code&gt; is used in our case.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--quantize_drop_prob&lt;/code&gt;: quantization dropout ratio, &lt;code&gt;0.2&lt;/code&gt; is used.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--vq_name&lt;/code&gt;: when training masked/residual transformer, you need to specify the name of rvq model for tokenization.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--cond_drop_prob&lt;/code&gt;: condition drop ratio, for classifier-free guidance. &lt;code&gt;0.2&lt;/code&gt; is used.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--share_weight&lt;/code&gt;: whether to share the projection/embedding weights in residual transformer.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;All the pre-trained models and intermediate results will be saved in space &lt;code&gt;./checkpoints/&amp;lt;dataset_name&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Evaluation&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;h3&gt;Evaluate RVQ Reconstruction:&lt;/h3&gt; &#xA; &lt;p&gt;HumanML3D:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_vq.py --gpu_id 0 --name rvq_nq6_dc512_nc512_noshare_qdp0.2 --dataset_name t2m --ext rvq_nq6&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;KIT-ML:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_vq.py --gpu_id 0 --name rvq_nq6_dc512_nc512_noshare_qdp0.2_k --dataset_name kit --ext rvq_nq6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Evaluate Text2motion Generation:&lt;/h3&gt; &#xA; &lt;p&gt;HumanML3D:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_trans_res.py --res_name tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw --dataset_name t2m --name t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns --gpu_id 1 --cond_scale 4 --time_steps 10 --ext evaluation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;KIT-ML:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python eval_t2m_trans_res.py --res_name tres_nlayer8_ld384_ff1024_rvq6ns_cdp0.2_sw_k --dataset_name kit --name t2m_nlayer8_nhead6_ld384_ff1024_cdp0.1_rvq6ns_k --gpu_id 0 --cond_scale 2 --time_steps 10 --ext evaluation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--res_name&lt;/code&gt;: model name of &lt;code&gt;residual transformer&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--name&lt;/code&gt;: model name of &lt;code&gt;masked transformer&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--cond_scale&lt;/code&gt;: scale of classifer-free guidance.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--time_steps&lt;/code&gt;: number of iterations for inference.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--ext&lt;/code&gt;: filename for saving evaluation results.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;The final evaluation results will be saved in &lt;code&gt;./checkpoints/&amp;lt;dataset_name&amp;gt;/&amp;lt;name&amp;gt;/eval/&amp;lt;ext&amp;gt;.log&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowlegements&lt;/h2&gt; &#xA;&lt;p&gt;We sincerely thank the open-sourcing of these works where our code is based on:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/DeepMotionEditing/deep-motion-editing&#34;&gt;deep-motion-editing&lt;/a&gt;, &lt;a href=&#34;https://github.com/lucidrains/muse-maskgit-pytorch&#34;&gt;Muse&lt;/a&gt;, &lt;a href=&#34;https://github.com/lucidrains/vector-quantize-pytorch&#34;&gt;vector-quantize-pytorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/Mael-zys/T2M-GPT&#34;&gt;T2M-GPT&lt;/a&gt;, &lt;a href=&#34;https://github.com/GuyTevet/motion-diffusion-model/tree/main&#34;&gt;MDM&lt;/a&gt; and &lt;a href=&#34;https://github.com/ChenFengYe/motion-latent-diffusion/tree/main&#34;&gt;MLD&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code is distributed under an &lt;a href=&#34;https://github.com/EricGuo5513/momask-codes/tree/main?tab=MIT-1-ov-file#readme&#34;&gt;MIT LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that our code depends on other libraries, including SMPL, SMPL-X, PyTorch3D, and uses datasets which each have their own respective licenses that must also be followed.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KwaiKEG/KwaiAgents</title>
    <updated>2024-12-31T01:39:54Z</updated>
    <id>tag:github.com,2024-12-31:/KwaiKEG/KwaiAgents</id>
    <link href="https://github.com/KwaiKEG/KwaiAgents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A generalized information-seeking agent system with Large Language Models (LLMs).&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; English ÔΩú &lt;a href=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/README_ZH.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/blob/logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; üìö &lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentInstruct&#34;&gt;Dataset&lt;/a&gt; | üìö &lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentBench&#34;&gt;Benchmark&lt;/a&gt; | ü§ó &lt;a href=&#34;https://huggingface.co/collections/kwaikeg/kagentlms-6551e685b5ec9f9a077d42ef&#34;&gt;Models&lt;/a&gt; | üìë &lt;a href=&#34;http://arxiv.org/abs/2312.04889&#34;&gt;Paper&lt;/a&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;KwaiAgents is a series of Agent-related works open-sourced by the &lt;a href=&#34;https://github.com/KwaiKEG&#34;&gt;KwaiKEG&lt;/a&gt; from &lt;a href=&#34;https://www.kuaishou.com/en&#34;&gt;Kuaishou Technology&lt;/a&gt;. The open-sourced content includes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentSys-Lite&lt;/strong&gt;: a lite version of the KAgentSys in the paper. While retaining some of the original system&#39;s functionality, KAgentSys-Lite has certain differences and limitations when compared to its full-featured counterpart, such as: (1) a more limited set of tools; (2) a lack of memory mechanisms; (3) slightly reduced performance capabilities; and (4) a different codebase, as it evolves from open-source projects like BabyAGI and Auto-GPT. Despite these modifications, KAgentSys-Lite still delivers comparable performance among numerous open-source Agent systems available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentLMs&lt;/strong&gt;: a series of large language models with agent capabilities such as planning, reflection, and tool-use, acquired through the Meta-agent tuning proposed in the paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentInstruct&lt;/strong&gt;: over 200k Agent-related instructions finetuning data (partially human-edited) proposed in the paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KAgentBench&lt;/strong&gt;: over 3,000 human-edited, automated evaluation data for testing Agent capabilities, with evaluation dimensions including planning, tool-use, reflection, concluding, and profiling.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt;&#xA;   &lt;th&gt;Training Data&lt;/th&gt;&#xA;   &lt;th&gt;Benchmark Data&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/kwaikeg/kagentlms_qwen_7b_mat&#34;&gt;Qwen-7B-MAT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentInstruct&#34;&gt;KAgentInstruct&lt;/a&gt;&lt;p&gt;(upcoming)&lt;/p&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentBench&#34;&gt;KAgentBench&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/kwaikeg/kagentlms_baichuan2_13b_mat&#34;&gt;Baichuan2-13B-MAT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/blob/example.gif&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/blob/overview.png&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.12.13 - The benchmark and evaluation code &lt;a href=&#34;https://huggingface.co/datasets/kwaikeg/KAgentBench&#34;&gt;[link]&lt;/a&gt; released&lt;/li&gt; &#xA; &lt;li&gt;2023.12.08 - Technical report &lt;a href=&#34;https://arxiv.org/abs/2312.04889&#34;&gt;[link]&lt;/a&gt; released&lt;/li&gt; &#xA; &lt;li&gt;2023.11.17 - Initial release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Benchmark Results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Scale&lt;/th&gt; &#xA;   &lt;th&gt;Planning&lt;/th&gt; &#xA;   &lt;th&gt;Tool-use&lt;/th&gt; &#xA;   &lt;th&gt;Reflection&lt;/th&gt; &#xA;   &lt;th&gt;Concluding&lt;/th&gt; &#xA;   &lt;th&gt;Profile&lt;/th&gt; &#xA;   &lt;th&gt;Overall Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5-turbo&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;18.55&lt;/td&gt; &#xA;   &lt;td&gt;15.89&lt;/td&gt; &#xA;   &lt;td&gt;5.32&lt;/td&gt; &#xA;   &lt;td&gt;37.26&lt;/td&gt; &#xA;   &lt;td&gt;35.42&lt;/td&gt; &#xA;   &lt;td&gt;21.72&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;0.15&lt;/td&gt; &#xA;   &lt;td&gt;0.23&lt;/td&gt; &#xA;   &lt;td&gt;0.08&lt;/td&gt; &#xA;   &lt;td&gt;16.60&lt;/td&gt; &#xA;   &lt;td&gt;17.73&lt;/td&gt; &#xA;   &lt;td&gt;5.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM3&lt;/td&gt; &#xA;   &lt;td&gt;6B&lt;/td&gt; &#xA;   &lt;td&gt;7.87&lt;/td&gt; &#xA;   &lt;td&gt;6.82&lt;/td&gt; &#xA;   &lt;td&gt;4.49&lt;/td&gt; &#xA;   &lt;td&gt;30.01&lt;/td&gt; &#xA;   &lt;td&gt;30.14&lt;/td&gt; &#xA;   &lt;td&gt;13.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;13.34&lt;/td&gt; &#xA;   &lt;td&gt;10.87&lt;/td&gt; &#xA;   &lt;td&gt;4.73&lt;/td&gt; &#xA;   &lt;td&gt;36.24&lt;/td&gt; &#xA;   &lt;td&gt;34.99&lt;/td&gt; &#xA;   &lt;td&gt;18.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;6.70&lt;/td&gt; &#xA;   &lt;td&gt;10.11&lt;/td&gt; &#xA;   &lt;td&gt;4.25&lt;/td&gt; &#xA;   &lt;td&gt;24.97&lt;/td&gt; &#xA;   &lt;td&gt;19.08&lt;/td&gt; &#xA;   &lt;td&gt;12.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ToolLlama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;0.20&lt;/td&gt; &#xA;   &lt;td&gt;3.44&lt;/td&gt; &#xA;   &lt;td&gt;0.54&lt;/td&gt; &#xA;   &lt;td&gt;15.62&lt;/td&gt; &#xA;   &lt;td&gt;10.66&lt;/td&gt; &#xA;   &lt;td&gt;5.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AgentLM&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;0.17&lt;/td&gt; &#xA;   &lt;td&gt;0.09&lt;/td&gt; &#xA;   &lt;td&gt;0.05&lt;/td&gt; &#xA;   &lt;td&gt;16.30&lt;/td&gt; &#xA;   &lt;td&gt;15.22&lt;/td&gt; &#xA;   &lt;td&gt;4.86&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-MAT&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;31.64&lt;/td&gt; &#xA;   &lt;td&gt;28.26&lt;/td&gt; &#xA;   &lt;td&gt;29.50&lt;/td&gt; &#xA;   &lt;td&gt;44.85&lt;/td&gt; &#xA;   &lt;td&gt;44.78&lt;/td&gt; &#xA;   &lt;td&gt;34.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2-MAT&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;37.27&lt;/td&gt; &#xA;   &lt;td&gt;34.82&lt;/td&gt; &#xA;   &lt;td&gt;32.06&lt;/td&gt; &#xA;   &lt;td&gt;48.01&lt;/td&gt; &#xA;   &lt;td&gt;41.83&lt;/td&gt; &#xA;   &lt;td&gt;38.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Human evaluation. Each result cell shows the pass rate (%) and the average score (in parentheses)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Scale&lt;/th&gt; &#xA;   &lt;th&gt;NoAgent&lt;/th&gt; &#xA;   &lt;th&gt;ReACT&lt;/th&gt; &#xA;   &lt;th&gt;Auto-GPT&lt;/th&gt; &#xA;   &lt;th&gt;KAgentSys&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;57.21% (3.42)&lt;/td&gt; &#xA;   &lt;td&gt;68.66% (3.88)&lt;/td&gt; &#xA;   &lt;td&gt;79.60% (4.27)&lt;/td&gt; &#xA;   &lt;td&gt;83.58% (4.47)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-3.5-turbo&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;47.26% (3.08)&lt;/td&gt; &#xA;   &lt;td&gt;54.23% (3.33)&lt;/td&gt; &#xA;   &lt;td&gt;61.74% (3.53)&lt;/td&gt; &#xA;   &lt;td&gt;64.18% (3.69)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;52.74% (3.23)&lt;/td&gt; &#xA;   &lt;td&gt;51.74% (3.20)&lt;/td&gt; &#xA;   &lt;td&gt;50.25% (3.11)&lt;/td&gt; &#xA;   &lt;td&gt;54.23% (3.27)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;54.23% (3.31)&lt;/td&gt; &#xA;   &lt;td&gt;55.72% (3.36)&lt;/td&gt; &#xA;   &lt;td&gt;57.21% (3.37)&lt;/td&gt; &#xA;   &lt;td&gt;58.71% (3.54)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-MAT&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;58.71% (3.53)&lt;/td&gt; &#xA;   &lt;td&gt;65.67% (3.77)&lt;/td&gt; &#xA;   &lt;td&gt;67.66% (3.87)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2-MAT&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;61.19% (3.60)&lt;/td&gt; &#xA;   &lt;td&gt;66.67% (3.86)&lt;/td&gt; &#xA;   &lt;td&gt;74.13% (4.11)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;User Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Using AgentLMs&lt;/h3&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt; to deploy the model inference service. First, you need to install the corresponding packages (for detailed usage, please refer to the documentation of the two projects):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For Qwen-7B-MAT, install the corresponding packages with the following commands&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install vllm&#xA;pip install &#34;fschat[model_worker,webui]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;For Baichuan-13B-MAT, install the corresponding packages with the following commands&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;fschat[model_worker,webui]&#34;&#xA;pip install vllm==0.2.0&#xA;pip install transformers==4.33.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To deploy KAgentLMs, you first need to start the controller in one terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastchat.serve.controller&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Secondly, you should use the following command in another terminal for single-gpu inference service deployment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where &lt;code&gt;$model_path&lt;/code&gt; is the local path of the model downloaded. If the GPU does not support Bfloat16, you can add &lt;code&gt;--dtype half&lt;/code&gt; to the command line.&lt;/p&gt; &#xA;&lt;p&gt;Thirdly, start the REST API server in the third terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m fastchat.serve.openai_api_server --host localhost --port 8888&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can use the curl command to invoke the model same as the OpenAI calling format. Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:8888/v1/chat/completions \&#xA;-H &#34;Content-Type: application/json&#34; \&#xA;-d &#39;{&#34;model&#34;: &#34;kagentlms_qwen_7b_mat&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who is Andy Lau&#34;}]}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, change &lt;code&gt;kagentlms_qwen_7b_mat&lt;/code&gt; to the model you deployed.&lt;/p&gt; &#xA;&lt;h3&gt;Using KAgentSys-Lite&lt;/h3&gt; &#xA;&lt;p&gt;Download and install the KwaiAgents, recommended Python&amp;gt;=3.10&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:KwaiKEG/KwaiAgents.git&#xA;cd KwaiAgents&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;ChatGPT usage&lt;/strong&gt; Declare some environment variables&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=sk-xxxxx&#xA;export WEATHER_API_KEY=xxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The WEATHER_API_KEY is not mandatory, but you need to configure it when asking weather-related questions. You can obtain the API key from &lt;a href=&#34;https://www.weatherapi.com/&#34;&gt;this website&lt;/a&gt; (Same for local model usage).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kagentsys --query=&#34;Who is Andy Lau&#39;s wife?&#34; --llm_name=&#34;gpt-3.5-turbo&#34; --lang=&#34;en&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local model usage&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To use a local model, you need to deploy the corresponding model service as described in the previous chapter&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kagentsys --query=&#34;Who is Andy Lau&#39;s wife?&#34; --llm_name=&#34;kagentlms_qwen_7b_mat&#34; \&#xA;--use_local_llm --local_llm_host=&#34;localhost&#34; --local_llm_port=8888 --lang=&#34;en&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Full command arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;options:&#xA;  -h, --help            show this help message and exit&#xA;  --id ID               ID of this conversation&#xA;  --query QUERY         User query&#xA;  --history HISTORY     History of conversation&#xA;  --llm_name LLM_NAME   the name of llm&#xA;  --use_local_llm       Whether to use local llm&#xA;  --local_llm_host LOCAL_LLM_HOST&#xA;                        The host of local llm service&#xA;  --local_llm_port LOCAL_LLM_PORT&#xA;                        The port of local llm service&#xA;  --tool_names TOOL_NAMES&#xA;                        the name of llm&#xA;  --max_iter_num MAX_ITER_NUM&#xA;                        the number of iteration of agents&#xA;  --agent_name AGENT_NAME&#xA;                        The agent name&#xA;  --agent_bio AGENT_BIO&#xA;                        The agent bio, a short description&#xA;  --agent_instructions AGENT_INSTRUCTIONS&#xA;                        The instructions of how agent thinking, acting, or talking&#xA;  --external_knowledge EXTERNAL_KNOWLEDGE&#xA;                        The link of external knowledge&#xA;  --lang {en,zh}        The language of the overall system&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you need to use the &lt;code&gt;browse_website&lt;/code&gt; tool, you need to configure the &lt;a href=&#34;https://chromedriver.chromium.org/getting-started&#34;&gt;chromedriver&lt;/a&gt; on your server.&lt;/li&gt; &#xA; &lt;li&gt;If the search fails multiple times, it may be because the network cannot access duckduckgo_search. You can solve this by setting the &lt;code&gt;http_proxy&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Using KAgentBench Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We only need two lines to evaluate the agent capabilities like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd benchmark&#xA;python infer_qwen.py qwen_benchmark_res.jsonl&#xA;python benchmark_eval.py ./benchmark_eval.jsonl ./qwen_benchmark_res.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command will give the results like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;plan : 31.64, tooluse : 28.26, reflextion : 29.50, conclusion : 44.85, profile : 44.78, overall : 34.20&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/KwaiKEG/KwaiAgents/main/benchmark/&#34;&gt;benchmark&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{pan2023kwaiagents,&#xA;  author    = {Haojie Pan and&#xA;               Zepeng Zhai and&#xA;               Hao Yuan and&#xA;               Yaojia Lv and&#xA;               Ruiji Fu and&#xA;               Ming Liu and&#xA;               Zhongyuan Wang and&#xA;               Bing Qin&#xA;               },&#xA;  title     = {KwaiAgents: Generalized Information-seeking Agent System with Large Language Models},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2312.04889},&#xA;  year      = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ihmily/DouyinLiveRecorder</title>
    <updated>2024-12-31T01:39:54Z</updated>
    <id>tag:github.com,2024-12-31:/ihmily/DouyinLiveRecorder</id>
    <link href="https://github.com/ihmily/DouyinLiveRecorder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ÂèØÂæ™ÁéØÂÄºÂÆàÂíåÂ§ö‰∫∫ÂΩïÂà∂ÁöÑÁõ¥Êí≠ÂΩïÂà∂ËΩØ‰ª∂ÔºåÊîØÊåÅÊäñÈü≥„ÄÅTiktok„ÄÅÂø´Êâã„ÄÅËôéÁâô„ÄÅÊñóÈ±º„ÄÅBÁ´ô„ÄÅÂ∞èÁ∫¢‰π¶Á≠âÂπ≥Âè∞Áõ¥Êí≠ÂΩïÂà∂ÔºåÊäìÂèñÂ§öÂπ≥Âè∞Áõ¥Êí≠Ê∫êÂú∞ÂùÄÔºåÊäñÈü≥Êó†Ê∞¥Âç∞Ëß£ÊûêÔºåÂø´ÊâãÊó†Ê∞¥Âç∞Ëß£Êûê&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://socialify.git.ci/ihmily/DouyinLiveRecorder/image?font=Inter&amp;amp;forks=1&amp;amp;language=1&amp;amp;owner=1&amp;amp;pattern=Circuit%20Board&amp;amp;stargazers=1&amp;amp;theme=Light&#34; alt=&#34;video_spider&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üí°ÁÆÄ‰ªã&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-3116/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.11.6-blue.svg?sanitize=true&#34; alt=&#34;Python Version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/issues/ihmily/DouyinLiveRecorder.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/downloads/ihmily/DouyinLiveRecorder/total&#34; alt=&#34;Downloads&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;‰∏ÄÊ¨æÂèØÂæ™ÁéØÂÄºÂÆàÁöÑÁõ¥Êí≠ÂΩïÂà∂Â∑•ÂÖ∑ÔºåÂü∫‰∫éFFmpegÂÆûÁé∞Â§öÂπ≥Âè∞Áõ¥Êí≠Ê∫êÂΩïÂà∂ÔºåÊîØÊåÅËá™ÂÆö‰πâÈÖçÁΩÆÂΩïÂà∂‰ª•ÂèäÁõ¥Êí≠Áä∂ÊÄÅÊé®ÈÄÅ„ÄÇ&lt;/p&gt;  &#xA;&lt;h2&gt;üò∫Â∑≤ÊîØÊåÅÂπ≥Âè∞&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ÊäñÈü≥&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; TikTok&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Âø´Êâã&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ËôéÁâô&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ÊñóÈ±º&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YY&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; BÁ´ô&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Â∞èÁ∫¢‰π¶&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; bigo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; blued&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AfreecaTV&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Êõ¥Â§öÂπ≥Âè∞Ê≠£Âú®Êõ¥Êñ∞‰∏≠&lt;/li&gt; &#xA;&lt;/ul&gt;  &#xA;&lt;h2&gt;üéàÈ°πÁõÆÁªìÊûÑ&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îî‚îÄ‚îÄ DouyinLiveRecorder/&#xA;    ‚îú‚îÄ‚îÄ /api -&amp;gt; (get live stream api )&#xA;    ‚îú‚îÄ‚îÄ /config -&amp;gt; (config record)&#xA;    ‚îú‚îÄ‚îÄ /log -&amp;gt; (save runing log file)&#xA;    ‚îú‚îÄ‚îÄ /backup_config -&amp;gt; (backup file)&#xA;    ‚îú‚îÄ‚îÄ /libs -&amp;gt; (dll file)&#xA;    ‚îú‚îÄ‚îÄ main.py -&amp;gt; (main file)&#xA;    ‚îú‚îÄ‚îÄ spider.py-&amp;gt; (get live url)&#xA;    ‚îú‚îÄ‚îÄ utils.py -&amp;gt; (contains utility functions)&#xA;    ‚îú‚îÄ‚îÄ web_rid.py -&amp;gt; (get web_rid)&#xA;    ‚îú‚îÄ‚îÄ msg_push.py -&amp;gt; (send live status update message)&#xA;    ‚îú‚îÄ‚îÄ cookies.py -&amp;gt; (get douyin cookies)&#xA;    ‚îú‚îÄ‚îÄ x-bogus.js -&amp;gt; (get douyin xbogus token)&#xA;    ‚îú‚îÄ‚îÄ ffmpeg.exe -&amp;gt; (record video)&#xA;    ‚îú‚îÄ‚îÄ index.html -&amp;gt; (play m3u8 and flv video)&#xA;    ‚îú‚îÄ‚îÄ requirements.txt -&amp;gt; (library dependencies)&#xA;&lt;/code&gt;&lt;/pre&gt;  &#xA;&lt;h2&gt;üå±‰ΩøÁî®ËØ¥Êòé&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ËøêË°å‰∏ªÊñá‰ª∂main.pyÂêØÂä®Á®ãÂ∫è&lt;/li&gt; &#xA; &lt;li&gt;Âú® &lt;code&gt;config&lt;/code&gt; Êñá‰ª∂Â§πÂÜÖÁöÑÈÖçÁΩÆÊñá‰ª∂‰∏≠ÂØπÂΩïÂà∂ËøõË°åÈÖçÁΩÆÔºåÂπ∂Âú® &lt;code&gt;URL_config.ini&lt;/code&gt; ‰∏≠Ê∑ªÂä†ÂΩïÂà∂Áõ¥Êí≠Èó¥Âú∞ÂùÄ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊäñÈü≥ÂΩïÂà∂ÈúÄË¶Å‰ΩøÁî®Âà∞PCÁΩëÈ°µÁ´ØÁõ¥Êí≠Èó¥È°µÈù¢ÁöÑCookieÔºåËØ∑ÂÖàÂú®config.iniÈÖçÁΩÆÊñá‰ª∂‰∏≠Ê∑ªÂä†ÂêéÂÜçËøõË°åÊäñÈü≥ÂΩïÂà∂ÔºàÊúâÈªòËÆ§ÁöÑcookieÔºå‰ΩÜÊúÄÂ•ΩËøòÊòØËá™Â∑±Ê∑ªÂä†Ëá™Â∑±ÁöÑÔºâ&lt;/li&gt; &#xA; &lt;li&gt;ÂΩïÂà∂TiktokÊó∂ÈúÄË¶ÅÁßëÂ≠¶‰∏äÁΩëÔºåËØ∑ÂÖàÂú®ÈÖçÁΩÆÊñá‰ª∂‰∏≠ËÆæÁΩÆÂºÄÂêØ‰ª£ÁêÜÂπ∂Ê∑ªÂä†proxy_addrÈìæÊé• Â¶ÇÔºö&lt;code&gt;http://127.0.0.1:7890&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ÂèØ‰ª•Âú®URL_config.ini‰∏≠ÁöÑÈìæÊé•ÂºÄÂ§¥Âä†‰∏ä#ÔºåÊ≠§Êó∂Â∞Ü‰∏ç‰ºöÂΩïÂà∂ËØ•Êù°ÈìæÊé•ÂØπÂ∫îÁöÑÁõ¥Êí≠&lt;/li&gt; &#xA; &lt;li&gt;ÊµãËØïÊó∂ÊúâÂèØËÉΩ‰ºöÂá∫Áé∞Âú®IDEÂ¶ÇPycharm‰∏≠ËøêË°å‰ª£Á†ÅËøõË°åÁõ¥Êí≠ÂΩïÂà∂ÔºåÂΩïÂà∂Âá∫Êù•ÁöÑËßÜÈ¢ëÂç¥Êó†Ê≥ïÊ≠£Â∏∏Êí≠ÊîæÁöÑÁé∞Ë±°ÔºåÂ¶ÇÊûúÈÅáÂà∞Ëøô‰∏™ÈóÆÈ¢ò Âú®ÂëΩ‰ª§ÊéßÂà∂Âè∞DOSÁïåÈù¢ËøêË°å‰ª£Á†ÅÔºåÂΩïÂà∂Âá∫Êù•ÁöÑËßÜÈ¢ëÂç≥ÂèØÊ≠£Â∏∏Êí≠Êîæ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÂΩìÂêåÊó∂Âú®ÂΩïÂà∂Â§ö‰∏™Áõ¥Êí≠Êó∂ÔºåÊúÄÂ•ΩÁ∫øÁ®ãÊï∞ËÆæÁΩÆÂ§ß‰∏Ä‰∫õÔºåÂê¶ÂàôÂèØËÉΩÂá∫Áé∞ÂÖ∂‰∏≠‰∏Ä‰∏™Áõ¥Êí≠ÂΩïÂà∂Âá∫Èîô„ÄÇÂΩìÁÑ∂ËÆæÁΩÆÁöÑËøáÂ§ß‰πüÊ≤°Áî®ÔºåË¶ÅÂêåÊó∂ËÄÉËôëËá™Ë∫´ÁîµËÑëÁöÑÈÖçÁΩÆÔºåÂ¶ÇCPUÂÜÖÊ†∏Êï∞„ÄÅÁΩëÁªúÂ∏¶ÂÆΩÁ≠âÈôêÂà∂„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Â¶ÇÊûúÊÉ≥Áõ¥Êé•‰ΩøÁî®ÊâìÂåÖÂ•ΩÁöÑÂΩïÂà∂ËΩØ‰ª∂ÔºåËøõÂÖ•&lt;a href=&#34;https://github.com/ihmily/DouyinLiveRecorder/releases&#34;&gt;Releases&lt;/a&gt; ‰∏ãËΩΩÊúÄÊñ∞ÂèëÂ∏ÉÁöÑ zipÂéãÁº©ÂåÖÂç≥ÂèØÔºåÊúâ‰∫õÁîµËÑëÂèØËÉΩ‰ºöÊä•ÊØíÔºåÁõ¥Êé•ÂøΩÁï•Âç≥ÂèØ„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Â¶ÇÊûúË¶ÅÈïøÊó∂Èó¥ÊåÇÁùÄËΩØ‰ª∂Âæ™ÁéØÁõëÊµãÁõ¥Êí≠ÔºåÊúÄÂ•ΩÂæ™ÁéØÊó∂Èó¥ËÆæÁΩÆÈïø‰∏ÄÁÇπÔºåÈÅøÂÖçÂõ†ËØ∑Ê±ÇÈ¢ëÁπÅÂØºËá¥Ë¢´ÂÆòÊñπÂ∞ÅÁ¶ÅIP „ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;ÊúÄÂêéÔºåÊ¨¢ËøéÂ§ßÂÆ∂fork‰ª•Âèäpr„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚ÄÉ&lt;/p&gt; &#xA;&lt;p&gt;Áõ¥Êí≠Èó¥ÈìæÊé•Á§∫‰æãÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;ÊäñÈü≥Ôºö&#xA;https://live.douyin.com/745964462470&#xA;https://v.douyin.com/iQFeBnt/&#xA;&#xA;TikTokÔºö&#xA;https://www.tiktok.com/@pearlgaga88/live&#xA;&#xA;Âø´ÊâãÔºö&#xA;https://live.kuaishou.com/u/yall1102&#xA;&#xA;ËôéÁâôÔºö&#xA;https://www.huya.com/52333&#xA;&#xA;ÊñóÈ±ºÔºö&#xA;https://www.douyu.com/3637778?dyshid=&#xA;https://www.douyu.com/topic/wzDBLS6?rid=4921614&amp;amp;dyshid=&#xA;&#xA;YY:&#xA;https://www.yy.com/22490906/22490906&#xA;&#xA;BÁ´ôÔºö&#xA;https://live.bilibili.com/320&#xA;&#xA;Â∞èÁ∫¢‰π¶Ôºö&#xA;https://www.xiaohongshu.com/hina/livestream/568980065082002402?appuid=5f3f478a00000000010005b3&amp;amp;apptime=&#xA;&#xA;bigoÁõ¥Êí≠Ôºö&#xA;https://www.bigo.tv/cn/716418802&#xA;&#xA;buledÁõ¥Êí≠Ôºö&#xA;https://app.blued.cn/live?id=Mp6G2R&#xA;&#xA;AfreecaTVÔºö&#xA;https://play.afreecatv.com/sw7love/249471484&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Áõ¥Êí≠Èó¥ÂàÜ‰∫´Âú∞ÂùÄÂíåÁΩëÈ°µÁ´ØÈïøÂú∞ÂùÄÈÉΩËÉΩÊ≠£Â∏∏ËøõË°åÂΩïÂà∂ÔºàÊäñÈü≥Â∞ΩÈáèÁî®ÈïøÈìæÊé•ÔºåÈÅøÂÖçÂõ†Áü≠ÈìæÊé•ËΩ¨Êç¢Â§±ÊïàÂØºËá¥‰∏çËÉΩÊ≠£Â∏∏ÂΩïÂà∂Ôºâ„ÄÇ&lt;/p&gt;  &#xA;&lt;p&gt;Ëß£ÊûêÊé•Âè£Ôºö&lt;/p&gt; &#xA;&lt;p&gt;ËØ•Ëß£ÊûêÊé•Âè£ &lt;del&gt;‰ªÖ‰æõÊºîÁ§∫&lt;/del&gt;(ÊºîÁ§∫Êé•Âè£ÊöÇÊó∂ÂÅúÊ≠¢ÔºåÂêéÁª≠ÂÜçÂºÄÊîæ)ÔºåÂπ∂‰∏îÂè™ÂåÖÂê´ÊäñÈü≥„ÄÅÂø´Êâã„ÄÅËôéÁâôÁõ¥Êí≠ÁöÑËß£ÊûêÔºåÂÖ∂‰ªñÂπ≥Âè∞Â¶ÇÊúâÈúÄË¶ÅËØ∑Ëá™Ë°åÊ∑ªÂä†ÔºåÊ∫êÁ†ÅÂú®ËøôÈáå &lt;a href=&#34;https://github.com/ihmily/DouyinLiveRecorder/tree/main/api&#34;&gt;DouyinLiveRecorder/api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-HTTP&#34;&gt;GET https://hmily.vip/api/jx/live/?url=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ËØ∑Ê±ÇÁ§∫‰æãÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-HTTP&#34;&gt;GET https://hmily.vip/api/jx/live/?url=https://live.douyin.com/573716250978&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ëã•ÈúÄË¶ÅÂ∞ÜÊäñÈü≥Áõ¥Êí≠Èó¥Áü≠ÈìæÊé•ËΩ¨Êç¢‰∏∫ÈïøÈìæÊé•Ôºå‰ΩøÁî®‰ª•‰∏ãÊé•Âè£Ôºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-HTTP&#34;&gt;GET https://hmily.vip/api/jx/live/convert.php?url=https://v.douyin.com/iQLgKSj/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Âú®Á∫øÊí≠Êîæm3u8ÂíåflvËßÜÈ¢ëÁΩëÁ´ôÔºö&lt;a href=&#34;https://jx.hmily.vip/play/&#34;&gt;M3U8 Âú®Á∫øËßÜÈ¢ëÊí≠ÊîæÂô® &lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ÄÉ&lt;/p&gt; &#xA;&lt;h2&gt;‚ù§Ô∏èË¥°ÁåÆËÄÖ&lt;/h2&gt; &#xA;&lt;p&gt;‚ÄÇ‚ÄÇ &lt;a href=&#34;https://github.com/ihmily&#34;&gt;&lt;img src=&#34;https://github.com/ihmily.png?size=50&#34; alt=&#34;Hmily&#34;&gt;&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;h2&gt;‚è≥Êèê‰∫§Êó•Âøó&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;20231210&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰øÆÂ§çÂΩïÂà∂ÂàÜÊÆµbugÔºå‰øÆÂ§çbigoÂΩïÂà∂Ê£ÄÊµãbug&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Êñ∞Â¢ûËá™ÂÆö‰πâ‰øÆÊîπÂΩïÂà∂‰∏ªÊí≠Âêç&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Êñ∞Â¢ûAfreecaTVÁõ¥Êí≠ÂΩïÂà∂Ôºå‰øÆÂ§çÊüê‰∫õÂèØËÉΩ‰ºöÂèëÁîüÁöÑbug&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231207&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êñ∞Â¢ûbluedÁõ¥Êí≠ÂΩïÂà∂Ôºå‰øÆÂ§çYYÁõ¥Êí≠ÂΩïÂà∂ÔºåÊñ∞Â¢ûÁõ¥Êí≠ÁªìÊùüÊ∂àÊÅØÊé®ÈÄÅ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231206&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êñ∞Â¢ûbigoÁõ¥Êí≠ÂΩïÂà∂&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231203&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êñ∞Â¢ûÂ∞èÁ∫¢‰π¶Áõ¥Êí≠ÂΩïÂà∂ÔºàÂÖ®ÁΩëÈ¶ñÂèëÔºâÔºåÁõÆÂâçÂ∞èÁ∫¢‰π¶ÂÆòÊñπÊ≤°ÊúâÂàáÊç¢Ê∏ÖÊô∞Â∫¶ÂäüËÉΩÔºåÂõ†Ê≠§Áõ¥Êí≠ÂΩïÂà∂‰πüÂè™ÊúâÈªòËÆ§ÁîªË¥®&lt;/li&gt; &#xA;   &lt;li&gt;Â∞èÁ∫¢‰π¶ÂΩïÂà∂ÊöÇÊó∂Êó†Ê≥ïÂæ™ÁéØÁõëÊµãÔºåÊØèÊ¨°‰∏ªÊí≠ÂºÄÂêØÁõ¥Êí≠ÔºåÈÉΩË¶ÅÈáçÊñ∞Ëé∑Âèñ‰∏ÄÊ¨°ÈìæÊé•&lt;/li&gt; &#xA;   &lt;li&gt;Ëé∑ÂèñÈìæÊé•ÁöÑÊñπÂºè‰∏∫ Â∞ÜÁõ¥Êí≠Èó¥ËΩ¨ÂèëÂà∞ÂæÆ‰ø°ÔºåÂú®ÂæÆ‰ø°‰∏≠ÊâìÂºÄÂêéÔºåÂ§çÂà∂È°µÈù¢ÁöÑÈìæÊé•„ÄÇ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20231030&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êú¨Ê¨°Êõ¥Êñ∞Âè™ÊòØËøõË°å‰øÆÂ§çÔºåÊ≤°Êó∂Èó¥Êñ∞Â¢ûÂäüËÉΩ„ÄÇ&lt;/li&gt; &#xA;   &lt;li&gt;Ê¨¢ËøéÂêÑ‰ΩçÂ§ß‰Ω¨Êèêpr Â∏ÆÂøôÊõ¥Êñ∞Áª¥Êä§&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230930&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Êñ∞Â¢ûÊäñÈü≥‰ªéÊé•Âè£Ëé∑ÂèñÁõ¥Êí≠ÊµÅÔºåÂ¢ûÂº∫Á®≥ÂÆöÊÄß&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰øÆÊîπÂø´ÊâãËé∑ÂèñÁõ¥Êí≠ÊµÅÁöÑÊñπÂºèÔºåÊîπÁî®‰ªéÂÆòÊñπÊé•Âè£Ëé∑Âèñ&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Á•ùÂ§ßÂÆ∂‰∏≠ÁßãËäÇÂø´‰πêÔºÅ&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230919&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;‰øÆÂ§ç‰∫ÜÂø´ÊâãÁâàÊú¨Êõ¥Êñ∞ÂêéÂΩïÂà∂Âá∫ÈîôÁöÑÈóÆÈ¢òÔºåÂ¢ûÂä†‰∫ÜÂÖ∂Ëá™Âä®Ëé∑Âèñcookie(&lt;del&gt;Á®≥ÂÆöÊÄßÊú™Áü•&lt;/del&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;‰øÆÂ§ç‰∫ÜTikTokÊòæÁ§∫Ê≠£Âú®Áõ¥Êí≠‰ΩÜ‰∏çËøõË°åÂΩïÂà∂ÁöÑÈóÆÈ¢ò&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230907&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰øÆÂ§ç‰∫ÜÂõ†ÊäñÈü≥ÂÆòÊñπÊõ¥Êñ∞‰∫ÜÁâàÊú¨ÂØºËá¥ÁöÑÂΩïÂà∂Âá∫Èîô‰ª•ÂèäÁü≠ÈìæÊé•ËΩ¨Êç¢Âá∫Èîô&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰øÆÂ§çBÁ´ôÊó†Ê≥ïÂΩïÂà∂ÂéüÁîªËßÜÈ¢ëÁöÑbug&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰øÆÊîπ‰∫ÜÈÖçÁΩÆÊñá‰ª∂Â≠óÊÆµÔºåÊñ∞Â¢ûÂêÑÂπ≥Âè∞Ëá™ÂÆö‰πâËÆæÁΩÆCookie&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230903&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;‰øÆÂ§ç‰∫ÜTikTokÂΩïÂà∂Êó∂Êä•644Êó†Ê≥ïÂΩïÂà∂ÁöÑÈóÆÈ¢ò&lt;/li&gt; &#xA;   &lt;li&gt;Êñ∞Â¢ûÁõ¥Êí≠Áä∂ÊÄÅÊé®ÈÄÅÂà∞ÈíâÈíâÂíåÂæÆ‰ø°ÁöÑÂäüËÉΩÔºåÂ¶ÇÊúâÈúÄË¶ÅËØ∑Áúã &lt;a href=&#34;https://d04vqdiqwr3.feishu.cn/docx/XFPwdDDvfobbzlxhmMYcvouynDh?from=from_copylink&#34;&gt;ËÆæÁΩÆÊé®ÈÄÅÊïôÁ®ã&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ÊúÄËøëÊØîËæÉÂøôÔºåÂÖ∂‰ªñÈóÆÈ¢òÊúâÊó∂Èó¥ÂÜçÊõ¥Êñ∞&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230816&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;‰øÆÂ§çÊñóÈ±ºÁõ¥Êí≠ÔºàÂÆòÊñπÊõ¥Êñ∞‰∫ÜÂ≠óÊÆµÔºâÂíåÂø´ÊâãÁõ¥Êí≠ÂΩïÂà∂Âá∫ÈîôÁöÑÈóÆÈ¢ò&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230814&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êñ∞Â¢ûBÁ´ôÁõ¥Êí≠ÂΩïÂà∂&lt;/li&gt; &#xA;   &lt;li&gt;ÂÜô‰∫Ü‰∏Ä‰∏™Âú®Á∫øÊí≠ÊîæM3U8ÂíåFLVËßÜÈ¢ëÁöÑÁΩëÈ°µÊ∫êÁ†ÅÔºåÊâìÂºÄÂç≥ÂèØÈ£üÁî®&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230812&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êñ∞Â¢ûYYÁõ¥Êí≠ÂΩïÂà∂&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230808&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;‰øÆÂ§ç‰∏ªÊí≠ÈáçÊñ∞ÂºÄÊí≠Êó†Ê≥ïÂÜçÊ¨°ÂΩïÂà∂ÁöÑÈóÆÈ¢ò&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230807&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Êñ∞Â¢û‰∫ÜÊñóÈ±ºÁõ¥Êí≠ÂΩïÂà∂&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;‰øÆÂ§çÊòæÁ§∫ÂΩïÂà∂ÂÆåÊàê‰πãÂêé‰ºöÈáçÊñ∞ÂºÄÂßãÂΩïÂà∂ÁöÑÈóÆÈ¢ò&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230805&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Êñ∞Â¢û‰∫ÜËôéÁâôÁõ¥Êí≠ÂΩïÂà∂ÔºåÂÖ∂ÊöÇÊó∂Âè™ËÉΩÁî®flvËßÜÈ¢ëÊµÅËøõË°åÂΩïÂà∂&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Web API Êñ∞Â¢û‰∫ÜÂø´ÊâãÂíåËôéÁâôËøô‰∏§‰∏™Âπ≥Âè∞ÁöÑÁõ¥Êí≠ÊµÅËß£ÊûêÔºàTikTokË¶Å‰ª£ÁêÜÔºâ&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230804&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êñ∞Â¢û‰∫ÜÂø´ÊâãÁõ¥Êí≠ÂΩïÂà∂Ôºå‰ºòÂåñ‰∫ÜÈÉ®ÂàÜ‰ª£Á†Å&lt;/li&gt; &#xA;   &lt;li&gt;‰∏ä‰º†‰∫Ü‰∏Ä‰∏™Ëá™Âä®ÂåñËé∑ÂèñÊäñÈü≥Áõ¥Êí≠Èó¥È°µÈù¢CookieÁöÑ‰ª£Á†ÅÔºåÂèØ‰ª•Áî®‰∫éÂΩïÂà∂&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230803&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ÈÄöÂÆµÊõ¥Êñ∞&lt;/li&gt; &#xA;   &lt;li&gt;Êñ∞Â¢û‰∫ÜÂõΩÈôÖÁâàÊäñÈü≥TikTokÁöÑÁõ¥Êí≠ÂΩïÂà∂ÔºåÂéªÈô§ÂÜó‰Ωô ÁÆÄÂåñ‰∫ÜÈÉ®ÂàÜ‰ª£Á†Å&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20230724&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Êñ∞Â¢û‰∫Ü‰∏Ä‰∏™ÈÄöËøáÊäñÈü≥Áõ¥Êí≠Èó¥Âú∞ÂùÄËé∑ÂèñÁõ¥Êí≠ËßÜÈ¢ëÊµÅÈìæÊé•ÁöÑAPIÊé•Âè£Ôºå‰∏ä‰º†Âç≥ÂèØÁî®&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;‚ÄÉ&lt;/p&gt; &#xA;&lt;h2&gt;ÊúâÈóÆÈ¢òÂèØ‰ª•Êèêissue ÔºåÂêéÁª≠Êàë‰ºöÂú®ËøôÈáå‰∏çÊñ≠Êõ¥Êñ∞ÂÖ∂‰ªñÁõ¥Êí≠Âπ≥Âè∞ÁöÑÂΩïÂà∂ Ê¨¢ËøéStar&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;/h4&gt;</summary>
  </entry>
</feed>