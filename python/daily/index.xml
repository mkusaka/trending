<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-16T01:35:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Ucas-HaoranWei/Vary</title>
    <updated>2023-12-16T01:35:47Z</updated>
    <id>tag:github.com,2023-12-16:/Ucas-HaoranWei/Vary</id>
    <link href="https://github.com/Ucas-HaoranWei/Vary" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official code implementation of Vary: Scaling Up the Vision Vocabulary of Large Vision Language Models.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;a href=&#34;&#34;&gt;Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;a href=&#34;https://varybase.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &#xA;&lt;a href=&#34;https://arxiv.org/abs/2312.06109&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-orange&#34;&gt;&lt;/a&gt; &#xA;&lt;a href=&#34;http://region-31.seetacloud.com:22701/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-blue&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=J4naK0MAAAAJ&amp;amp;hl=en&#34;&gt;Haoran Wei*&lt;/a&gt;, Lingyu Kong*, Jinyue Chen, Liang Zhao, &lt;a href=&#34;https://joker316701882.github.io/&#34;&gt;Zheng Ge&lt;/a&gt;, &lt;a href=&#34;https://yancie-yjr.github.io/&#34;&gt;Jinrong Yang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=MVZrGkYAAAAJ&amp;amp;hl=en&#34;&gt;Jianjian Sun&lt;/a&gt;, Chunrui Han, &lt;a href=&#34;https://scholar.google.com/citations?user=yuB-cfoAAAAJ&amp;amp;hl=en&#34;&gt;Xiangyu Zhang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Ucas-HaoranWei/Vary/main/assets/logo.jpg&#34; style=&#34;width: 200px&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/12/11] We released the online demo, have fun!&lt;/li&gt; &#xA; &lt;li&gt;[2023/12/11] We released the codes of Vary (train and inference)!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data, code, and checkpoint are intended and licensed for research use only. They are also restricted to use that follow the license agreement of LLaMA, Vicuna, GPT-4, Qwen, and LLaVA.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ucas-HaoranWei/Vary/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ucas-HaoranWei/Vary/main/#vary-weights&#34;&gt;Vary Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ucas-HaoranWei/Vary/main/#Demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Ucas-HaoranWei/Vary/main/#train&#34;&gt;Train&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to the Vary folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Ucas-HaoranWei/Vary.git&#xA;cd Vary&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n vary python=3.10 -y&#xA;conda activate vary&#xA;pip install e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install Flash-Attention&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install ninja&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Vary Weights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Due to download speed issues with Baiduyun, we have temporarily closed the download link. Our weights will be reorganized and open source again in the next few days.&lt;/li&gt; &#xA; &lt;li&gt;Download the CLIP-VIT-L in &lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14/tree/main&#34;&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Update the CLIP-VIT path in the codes (/cache/vit-large-patch14/) to your path.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python vary/demo/run_qwen_vary.py  --model-name  /vary/model/path/ --image-file /an/image/file.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We currently do not plan to open source the weights of the intermediate.&lt;/li&gt; &#xA; &lt;li&gt;However, we release the train codes. So you can train on your own dataset. If you want to do this, you can try this:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For Vary-base (one machine, if you have multiple machines you need to prepare your host file)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;deepspeed   Vary/train/train_qwen_vary.py  --deepspeed /Vary/zero_config/zero2.json&#xA;            --model_name_or_path /Qwen-7B/path/&#xA;            --vision_tower /vit-large-patch14/path/&#xA;            --freeze_vision_tower True&#xA;            --freeze_lm_model False&#xA;            --vision_select_layer  -2&#xA;            --use_im_start_end True&#xA;            --bf16 True&#xA;            --per_device_eval_batch_size 4&#xA;            --gradient_accumulation_steps 1&#xA;            --evaluation_strategy &#34;no&#34;&#xA;            --save_strategy &#34;steps&#34;&#xA;            --save_steps 5000&#xA;            --save_total_limit 1&#xA;            --weight_decay 0.&#xA;            --warmup_ratio 0.03&#xA;            --lr_scheduler_type &#34;cosine&#34;&#xA;            --logging_steps 1 --tf32 True&#xA;            --model_max_length 4096&#xA;            --gradient_checkpointing True&#xA;            --dataloader_num_workers 4&#xA;            --report_to none&#xA;            --per_device_train_batch_size 4&#xA;            --num_train_epochs 1&#xA;            --learning_rate 5e-5&#xA;            --datasets  data_name1+data_name2+data_name3&#xA;            --output_dir /path/to/output/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;For Vary-tiny&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;deepspeed   Vary/train/train_opt.py  --deepspeed /Vary/zero_config/zero2.json&#xA;            --model_name_or_path /opt125m/path/&#xA;            --conversation_version opt&#xA;            --freeze_vision_tower False&#xA;            --freeze_lm_model False&#xA;            --use_im_start_end True&#xA;            --bf16 True&#xA;            --per_device_eval_batch_size 4&#xA;            --gradient_accumulation_steps 1&#xA;            --evaluation_strategy &#34;no&#34;&#xA;            --save_strategy &#34;steps&#34;&#xA;            --save_steps 5000&#xA;            --save_total_limit 1&#xA;            --weight_decay 0.&#xA;            --warmup_ratio 0.03&#xA;            --lr_scheduler_type &#34;cosine&#34;&#xA;            --logging_steps 1 --tf32 True&#xA;            --model_max_length 4096&#xA;            --gradient_checkpointing True&#xA;            --dataloader_num_workers 4&#xA;            --report_to none&#xA;            --per_device_train_batch_size 16&#xA;            --num_train_epochs 1&#xA;            --learning_rate 5e-5&#xA;            --datasets  data_name1+data_name2+data_name3&#xA;            --output_dir /path/to/output/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions related to the code or the paper, feel free to email (&lt;code&gt;weihaoran18@mails.ucas.ac.cn&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;LLaVA&lt;/a&gt;: the codebase we built upon!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;Qwen&lt;/a&gt;: the LLM base model of Vary, which is good at both English and Chinese!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, please consider citing Vary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wei2023vary,&#xA;  title={Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models},&#xA;  author={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},&#xA;  journal={arXiv preprint arXiv:2312.06109},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>OpenLLMAI/OpenRLHF</title>
    <updated>2023-12-16T01:35:47Z</updated>
    <id>tag:github.com,2023-12-16:/OpenLLMAI/OpenRLHF</id>
    <link href="https://github.com/OpenLLMAI/OpenRLHF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Ray-based High-performance RLHF framework (for large models)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenRLHF&lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;OpenRLHF&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;h3&gt;A Ray-based High-performance RLHF framework!&lt;/h3&gt; &#xA; &lt;a href=&#34;https://github.com/openllmai/OpenRLHF/graphs/contributors&#34;&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/github/contributors/openllmai/OpenRLHF&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/openllmai/OpenRLHF/issues&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/issues/openllmai/OpenRLHF?color=0088ff&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/openllmai/OpenRLHF/discussions&#34;&gt; &lt;img alt=&#34;Issues&#34; src=&#34;https://img.shields.io/github/discussions/openllmai/OpenRLHF?color=0088ff&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/openllmai/OpenRLHF/pulls&#34;&gt; &lt;img alt=&#34;GitHub pull requests&#34; src=&#34;https://img.shields.io/github/issues-pr/openllmai/OpenRLHF?color=0088ff&#34;&gt; &lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/openllmai/OpenRLHF/stargazers&#34;&gt; &lt;img alt=&#34;GitHub stars&#34; src=&#34;https://img.shields.io/github/stars/openllmai/OpenRLHF?color=ccf&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;em&gt;Open-source ChatGPT / Comprehensive / Lightweight / Easy-to-use&lt;/em&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;The code is open-source, feel free to use it, contributions are welcome! Note: The license of the model depends on the provider of the model.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;OpenRLHF is a high-performance RLHF framework built on Ray, DeepSpeed and HuggingFace Transformers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple and easy to use&lt;/strong&gt;: OpenRLHF is one of the simplest high-performance RLHF libraries currently available, enabling 34B model RLHF training with just a single DGXA100 node (see the training &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/examples/scripts/train_ppo_llama_ray_34b.sh&#34;&gt;script&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed RLHF&lt;/strong&gt;: The key idea behind OpenRLHF is to distribute the Actor, Reward, Reference, and Critic models onto separate GPUs using Ray, while placing the Adam optimizer on the CPU. This enables full-scale fine-tuning of 7B models across multiple 24GB RTX 4090 GPUs (or 34B models with multiple A100 80G GPUs).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Thanks to the ability to use a large inference batch size with Ray and DeepSpeed&#39;s CPUAdam, the performance of OpenRLHF with the 13B LLaMA2 model is 4x that of DeepSpeedChat.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Compatible with HuggingFace Transformers model.&lt;/li&gt; &#xA; &lt;li&gt;Distributed &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/examples/scripts/train_ppo_llama_ray.sh&#34;&gt;PPO based on Ray&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support Multiple Reward models.&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/examples/scripts/train_rejection_sampling_llama.sh&#34;&gt;Rejection Sampling&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/examples/scripts/train_dpo_llama.sh&#34;&gt;DPO (direct-preference-optimization)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/examples/scripts/train_conditional_llama.sh&#34;&gt;Conditional Alignment&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/2308.12050&#34;&gt;https://arxiv.org/abs/2308.12050&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://github.com/OpenLLMAI/OpenRLHF/issues/116&#34;&gt;top chinese models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multi-nodes &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/examples/scripts/train_llama_slurm.sh&#34;&gt;training scripts&lt;/a&gt; for Slurm.&lt;/li&gt; &#xA; &lt;li&gt;Support Wandb log (--wandb).&lt;/li&gt; &#xA; &lt;li&gt;Support FlashAttention2 (--flash_attn).&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/evaluation/gpt4/README.md&#34;&gt;GPT4 evaluation&lt;/a&gt; &amp;amp; PPO vs SFT &lt;a href=&#34;https://raw.githubusercontent.com/OpenLLMAI/OpenRLHF/main/docs/ppo_examples.md&#34;&gt;examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pre-trained 7B/13B llama2 &lt;a href=&#34;https://huggingface.co/OpenLLMAI/openrlhf_checkpoint&#34;&gt;checkpoints&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;RLHF compatible with models larger than 100B using vLLM&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Allows saving and loading training checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;Integrates with the QLora.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Support Matrix&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;PPO-max &amp;amp; Best Hyperparameters&lt;/th&gt; &#xA;   &lt;th&gt;Ray&lt;/th&gt; &#xA;   &lt;th&gt;34B Full Tuning with 4 A100&lt;/th&gt; &#xA;   &lt;th&gt;7B Full Tuning with 1 A100 (80G)&lt;/th&gt; &#xA;   &lt;th&gt;7B Full Tuning with 4 RTX4090&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenRLHF&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSpeedChat&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ColossalAIChat&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TRL&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;   &lt;td&gt;✖️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;7B llama2 RLHF&lt;/th&gt; &#xA;   &lt;th&gt;13B llama2 RLHF (50k samples)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenRLHF&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;22 hours with 8 A100&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSpeedChat&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;48 hours with 16 A100&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Configs for Ray and DeepSpeed:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;4 A100 80G for Actor, 2 A100 80G for Critic, 1 A100 80G for RM, and 1 A100 80G for InitPolicy&lt;/li&gt; &#xA; &lt;li&gt;ZeRO2 with Adam Offload&lt;/li&gt; &#xA; &lt;li&gt;Max Sequence Length: 2048&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Throughput:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;7B llama2: 0.105 samples/gpu/secs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;micro_batch_size = 16/8 (rollout/train), generation_length = 100~300&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;13B llama2: 0.04 samples/gpu/secs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;micro_batch_size = 8/4 (rollout/train), generation_length = 200~400&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;34B codellama: 0.007 samples/gpu/secs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;micro_batch_size = 2/1 (rollout/train), generation_length = 300~800&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;samples/gpu/secs = Number of PPO Samples / Number of A100 GPUS / Seconds&lt;/p&gt; &#xA;&lt;h2&gt;Running Example&lt;/h2&gt; &#xA;&lt;p&gt;You can build openrlhf from &lt;strong&gt;nvidia-docker(recommended)&lt;/strong&gt; or from conda envs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Clone the repository: &#xA;git clone https://github.com/openllmai/OpenRLHF.git&#xA;&#xA;# Download the pre-trained SFT/RM checkpoints (Optional)&#xA;git lfs install&#xA;git clone --depth=1 https://huggingface.co/OpenLLMAI/openrlhf_checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Single-node training with nvidia-docker&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd examples/scripts&#xA;&#xA;# install nvidia-docker (Optional)&#xA;./nvidia_docker_install.sh&#xA;&#xA;# launch nvidia container&#xA;./docker_run.sh&#xA;&#xA;# cd in container&#xA;cd /openrlhf/examples/scripts&#xA;&#xA;# build OpenRLHF (i.e, pip install)&#xA;./build_openrlhf.sh&#xA;&#xA;# huggingface login &#xA;~/.local/bin/huggingface-cli login&#xA;&#xA;# train SFT model&#xA;./train_sft_llama.sh&#xA;&#xA;# train RM model&#xA;./train_rm_llama.sh&#xA;&#xA;# train PPO model&#xA;./train_ppo_llama.sh&#xA;&#xA;# train DPO model&#xA;./train_dpo_llama.sh&#xA;&#xA;# train Rejection Sampling model&#xA;./train_rejection_sampling_llama.sh&#xA;&#xA;# train Conditional Alignment model&#xA;./train_conditional_llama.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PPO training with Ray&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;for 13B/34B models on A100/H100.. or 7B models on RTX4090&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd examples/scripts&#xA;&#xA;# launch nvidia container&#xA;./docker_run.sh&#xA;&#xA;# cd in container&#xA;cd /openrlhf/examples/scripts&#xA;&#xA;# build OpenRLHF (i.e, pip install)&#xA;./build_openrlhf.sh&#xA;&#xA;# huggingface login &#xA;~/.local/bin/huggingface-cli login&#xA;&#xA;# launch ray in container&#xA;nohup ray start --head --node-ip-address 0.0.0.0 --num-gpus 8 --block &amp;amp;&amp;gt; ray.log &amp;amp;&#xA;&#xA;# if you want to launch ray on more nodes, use&#xA;# ray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8 --block&#xA;&#xA;# train ray PPO model, requires 8 gpus in default config&#xA;./train_ppo_llama_ray.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-nodes training on Slurm&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd examples/scripts&#xA;&#xA;# huggingface login on Slurm &#xA;pip install transformers&#xA;huggingface-cli login&#xA;&#xA;# Moidfy the Slurm Account/Nodes ... in `train_llama_slurm.sh`&#xA;&#xA;# For SFT, RM, and PPO and DPO training:&#xA;# Modify the variable `training_script` in `train_llama_slurm.sh` to&#xA;readonly training_script=&#34;train_sft_llama.sh&#34;&#xA;readonly training_script=&#34;train_rm_llama.sh&#34;&#xA;readonly training_script=&#34;train_ppo_llama.sh&#34;&#xA;readonly training_script=&#34;train_dpo_llama.sh&#34;&#xA;&#xA;# set `GPUS_PER_NODE` in `train_llama_slurm.sh`&#xA;readonly GPUS_PER_NODE=8&#xA;&#xA;# run multi-nodes training script&#xA;# train_llama_slurm.sh will load the training args from `training_script`&#xA;sbatch ./train_llama_slurm.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference and Evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After completing the training, you can evaluate your model by using the &lt;code&gt;inference&lt;/code&gt; script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# interactive_chat&#xA;./interactive_chat_llama.sh { model_path }&#xA;&#xA;# batch generate&#xA;python examples/batch_inference.py {args}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;build openrlhf from conda envs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you really don&#39;t want to use nvidia-docker, we also provide tutorials for building openrlhf from a conda environment. (We prefer nvidia-docker to avoid errors caused by the environment.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# we need conda&#xA;conda create -n openrlhf python=3.10&#xA;# so, we need install some package manually: when installing torch, you may need to match the corresponding cuda version.&#xA;pip install packaging ninja&#xA;pip install torch --index-url https://download.pytorch.org/whl/cu118&#xA;# check ninjia&#xA;ninja --version&#xA;echo $? # output: 0&#xA;# install flash-attn: may take some time.&#xA;# For network error: you can download specified version from https://github.com/Dao-AILab/flash-attention/releases.&#xA;pip install flash-attn==2.1.1 --no-build-isolation&#xA;./build_openrlhf.sh&#xA;# enjoy it!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our gratitude to the following projects and organizations for their contributions to the field of AI and NLP:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Hugging Face Transformers ↗&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/gpt-3&#34;&gt;OpenAI GPT ↗&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;LLaMA2 ↗&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;DeepSpeed ↗&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ray-project/ray&#34;&gt;Ray ↗&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Join Us&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to Join?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Email us at &lt;a href=&#34;mailto:xianyuai@openllmai.top&#34;&gt;xianyuai@openllmai.top&lt;/a&gt;(official email) or &lt;a href=&#34;mailto:janhu9527@gmail.com&#34;&gt;janhu9527@gmail.com&lt;/a&gt;/jjgxw@outlook.com(PIC). Please include the following details: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Your name&lt;/li&gt; &#xA;   &lt;li&gt;Your GitHub username&lt;/li&gt; &#xA;   &lt;li&gt;Your areas of interest&lt;/li&gt; &#xA;   &lt;li&gt;Your skills and experience related to NLP and/or AI&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You can also join us through the official GitHub &lt;a href=&#34;https://github.com/openllmai/OpenRLHF&#34;&gt;OpenRLHF ↗&lt;/a&gt; project page. Just create an issue about your interest to contribute and we will get back to you.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;What can you do?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Join the team and participate in the development of the OpenRLHF project.&lt;/li&gt; &#xA; &lt;li&gt;Contribute to the project by submitting pull requests.&lt;/li&gt; &#xA; &lt;li&gt;Help improve documentation, fix bugs, or create new features.&lt;/li&gt; &#xA; &lt;li&gt;Share the project and help us grow the community.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Sponsor Us&lt;/h2&gt; &#xA;&lt;p&gt;Your sponsorship can help us maintain and improve OpenRLHF. If you find this project useful, please consider sponsoring us. You can sponsor us on &lt;a href=&#34;https://opencollective.com/openllmai&#34;&gt;Open Collective ↗&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Starchart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#openllmai/OpenRLHF&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=openllmai/OpenRLHF&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;A big thank you to all our contributors! If you want to contribute, feel free to make a pull request or create an issue.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/openllmai/OpenRLHF/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=openllmai/OpenRLHF&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{hu23openrlhf,&#xA;   author = {Jian Hu and Xibin Wu and Xianyu and Chen Su and Leon Qiu and Daoning Jiang and Qing Wang and Weixun Wang},&#xA;   title = {OpenRLHF: A Ray-based High-performance RLHF framework},&#xA;   year={2023},&#xA;   publisher = {GitHub},&#xA;   journal = {GitHub repository},&#xA;   howpublished = {\url{https://github.com/OpenLLMAI/OpenRLHF}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;OpenRLHF © 2023 OpenLLMAI. All Rights Reserved.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Xunzi-LLM-of-Chinese-classics/XunziALLM</title>
    <updated>2023-12-16T01:35:47Z</updated>
    <id>tag:github.com,2023-12-16:/Xunzi-LLM-of-Chinese-classics/XunziALLM</id>
    <link href="https://github.com/Xunzi-LLM-of-Chinese-classics/XunziALLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; 中文&amp;nbsp; ｜ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/README_en.md&#34;&gt;English&lt;/a&gt; &lt;/p&gt; # 荀子系列大语言模型 &#xA;&lt;p&gt;随着科技的飞速发展，人工智能已深入到各个领域。为响应古籍活化利用号召，推动大语言模型与古籍处理深度融合，以古籍智能化的研究为目的，南京农业大学国家社科基金重大项目“中国古代典籍跨语言知识库构建及应用研究”课题组与中华书局古联公司推出了一系列古籍处理领域大语言模型：荀子古籍大语言模型。荀子不仅是我国先秦时期伟大的朴素唯物主义的思想家，也是一位散文大家。他在语言学理论的阐述上又是一位开拓者、奠基人。荀子系列专为古籍智能处理而设计，这一系列模型的推出将推动古籍研究与保护工作的新发展，提高中华传统文化传承的效率与质量。&lt;/p&gt; &#xA;&lt;p&gt;本次荀子系列模型开源包括两个部分：基座模型&lt;a href=&#34;https://modelscope.cn/models/Xunzillm4cc/Xunzi-Qwen&#34;&gt;&lt;strong&gt;XunziALLM&lt;/strong&gt;&lt;/a&gt;与对话模型&lt;a href=&#34;https://modelscope.cn/models/Xunzillm4cc/Xunzi-Qwen-Chat&#34;&gt;&lt;strong&gt;XunziChat&lt;/strong&gt;&lt;/a&gt;，模型的调用方式与阿里云的Qwen系列大模型一致。&lt;/p&gt; &#xA;&lt;h2&gt;荀子系列模型亮点：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;古籍智能标引，荀子模型具备强大的古籍文献标引能力，能够对古籍中的内容进行高质量主题标引，帮助研究人员快速了解文章主题。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/examples/index.png&#34; alt=&#34;index&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;古籍信息抽取，荀子模型能够自动从古籍中抽取关键信息，如人物、事件、地点等，大大节省了研究人员的信息整理时间。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/examples/ner.png&#34; alt=&#34;ner&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;诗歌生成：荀子模型还具备诗歌生成的能力，能够根据给定的主题或关键词，自动生成符合语法规则和韵律要求的古诗，为诗词爱好者提供创作灵感。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/examples/poetry.png&#34; alt=&#34;poetry&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;古籍高质量翻译：对于那些难以理解的古籍文献，荀子模型能够提供高质量的翻译服务，帮助研究人员更好地理解原文含义。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/examples/translation.png&#34; alt=&#34;translation&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;阅读理解：荀子模型能够对给出的古文文本进行分析解释，实现对古籍文本的自动阅读。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/examples/reading_comprehension.png&#34; alt=&#34;reading_comprehension&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;词法分析：荀子模型可以完成古籍文本的自动分词和词性标注，能够有效提升语言学工作者的研究效率。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/examples/pos.png&#34; alt=&#34;pos&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;自动标点：荀子大模型可以快速完成古籍文本的断句和标点，提升研究者以及业余爱好者对古籍文本的阅读体验。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Xunzi-LLM-of-Chinese-classics/XunziALLM/main/examples/punctuation.png&#34; alt=&#34;punctuation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;由于我们同时发布了基座模型，用户也可以根据自己的需求，使用本地的训练语料微调荀子基座模型，使得其能够在古籍下游处理任务上取得更佳的处理性能。&lt;/p&gt; &#xA;&lt;h2&gt;致谢：&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;南京理工大学沈思副教授团队，提供大模型训练技术支持&lt;/li&gt; &#xA; &lt;li&gt;江苏警官学院朱丹浩博士，提供模型参数调优&lt;/li&gt; &#xA; &lt;li&gt;南京师范大学李斌教授团队，提供古文领域知识支撑&lt;/li&gt; &#xA; &lt;li&gt;国家图书馆马学良副研究员，提供古典文献学相关知识&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;声明：&lt;/h2&gt; &#xA;&lt;p&gt;大语言模型庞大的参数量也带来了更多的随机性，虽然我们在训练数据选取时已经尽可能保证了数据的合规性，但由于数据和模型的复杂性，仍有可能存在一些无法避免的问题。因此，如果由于使用本开源模型而导致的各种问题，包括但不限于数据安全问题、公共舆论风险，或模型被误导、滥用、传播或不当利用所带来的任何风险和问题，我们将不承担任何责任。&lt;/p&gt; &#xA;&lt;p&gt;此外，根据国家网信办等七部门联合发布的&lt;a href=&#34;http://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm&#34;&gt;《生成式人工智能服务管理暂行办法》&lt;/a&gt;，在训练、使用本模型以及其他生成式模型，请依据相关法律法规，为构建和谐、健康、可持续的生成式人工智能社区共同努力。&lt;/p&gt; &#xA;&lt;p&gt;如果您在模型使用过程中有任何疑问，欢迎联系我们(&lt;a href=&#34;mailto:zhaozhixiao@stu.njau.edu.cn&#34;&gt;zhaozhixiao@stu.njau.edu.cn&lt;/a&gt;)&lt;/p&gt;</summary>
  </entry>
</feed>