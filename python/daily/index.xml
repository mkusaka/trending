<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-02T01:42:59Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nomic-ai/nomic</title>
    <updated>2023-04-02T01:42:59Z</updated>
    <id>tag:github.com,2023-04-02:/nomic-ai/nomic</id>
    <link href="https://github.com/nomic-ai/nomic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Interact with Massive Embedding and Text Datasets in Your Web Browser&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Atlas&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Explore, label, search and share massive datasets in your web browser.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://colab.research.google.com/drive/1bquOLIaGlu7O_CFc0Wz74HITzWs4UEa4?usp=sharing&#34;&gt;Colab Demo&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.nomic.ai&#34;&gt;&lt;span&gt;📕&lt;/span&gt; Atlas Python Client Documentation&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://discord.gg/myY5YDR8z8&#34;&gt;&lt;span&gt;🛖&lt;/span&gt; Discord&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;b&gt;Example Maps&lt;/b&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://atlas.nomic.ai/map/twitter&#34;&gt;&lt;span&gt;🗺&lt;/span&gt; Map of Twitter&lt;/a&gt; (5.4 million tweets) &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://atlas.nomic.ai/map/stablediffusion&#34;&gt;&lt;span&gt;🗺&lt;/span&gt; Map of StableDiffusion Generations&lt;/a&gt; (6.4 million images) &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://atlas.nomic.ai/map/neurips&#34;&gt;&lt;span&gt;🗺&lt;/span&gt; Map of NeurIPS Proceedings&lt;/a&gt; (16,623 abstracts) &lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Install the Nomic client with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install nomic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Login/create your Nomic account:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nomic login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the instructions to obtain your access token. Enter your access token with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nomic login [token]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make your first map:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nomic import atlas&#xA;import numpy as np&#xA;&#xA;num_embeddings = 10000&#xA;embeddings = np.random.rand(num_embeddings, 256)&#xA;&#xA;response = atlas.map_embeddings(embeddings=embeddings)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Explore Atlas&#39; &lt;a href=&#34;https://docs.nomic.ai&#34;&gt;documentation&lt;/a&gt; to make more advanced maps.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PrefectHQ/marvin</title>
    <updated>2023-04-02T01:42:59Z</updated>
    <id>tag:github.com,2023-04-02:/PrefectHQ/marvin</id>
    <link href="https://github.com/PrefectHQ/marvin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🤖🪄 A batteries-included library for GPT-powered bots and AI functions&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Marvin 🤖🏖️&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PrefectHQ/marvin/main/docs/img/heroes/ai_fn_fruits_hero.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Meet Marvin: a batteries-included library for building AI-powered software. Marvin&#39;s job is to integrate AI directly into your codebase by making it look and feel like any other function.&lt;/p&gt; &#xA;&lt;p&gt;Marvin introduces a new concept called &lt;a href=&#34;https://askmarvin.ai/guide/concepts/ai_functions&#34;&gt;&lt;strong&gt;AI Functions&lt;/strong&gt;&lt;/a&gt;. These functions differ from conventional ones in that they don’t rely on source code, but instead generate their outputs on-demand through AI. With AI functions, you don&#39;t have to write complex code for tasks like extracting entities from web pages, scoring sentiment, or categorizing items in your database. Just describe your needs, call the function, and you&#39;re done!&lt;/p&gt; &#xA;&lt;p&gt;AI functions work with native data types, so you can seamlessly integrate them into any codebase and chain them into sophisticated pipelines. Technically speaking, Marvin transforms the signature of using AI from &lt;code&gt;(str) -&amp;gt; str&lt;/code&gt; to &lt;code&gt;(**kwargs) -&amp;gt; Any&lt;/code&gt;. We call this &lt;strong&gt;&#34;functional prompt engineering.&#34;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;In addition to AI functions, Marvin also introduces more flexible bots. &lt;a href=&#34;https://www.askmarvin.ai/guide/concepts/bots.md&#34;&gt;&lt;strong&gt;Bots&lt;/strong&gt;&lt;/a&gt; are highly capable AI assistants that can be given specific instructions and personalities or roles. They can use custom plugins and leverage external knowledge, and automatically create a history of every thread. Under the hood, AI functions are actually a type of bot.&lt;/p&gt; &#xA;&lt;p&gt;Developers can use&amp;nbsp;Marvin to add AI capabilities wherever they will be most impactful, without needing to start from scratch. Marvin&#39;s documentation is available at &lt;a href=&#34;https://www.askmarvin.ai&#34;&gt;askmarvin.ai&lt;/a&gt;, and say hello on our &lt;a href=&#34;https://discord.gg/Kgw4HpcuYG&#34;&gt;Discord server&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PrefectHQ/marvin/main/docs/img/heroes/gpp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;🪄 Write &lt;a href=&#34;https://askmarvin.ai/guide/concepts/ai_functions&#34;&gt;AI functions&lt;/a&gt; that process structured data without source code&lt;/p&gt; &#xA;&lt;p&gt;🤖 Create &lt;a href=&#34;https://www.askmarvin.ai/guide/concepts/bots&#34;&gt;bots&lt;/a&gt; that have personalities and follow instructions&lt;/p&gt; &#xA;&lt;p&gt;🔌 Build &lt;a href=&#34;https://askmarvin.ai/guide/concepts/plugins&#34;&gt;plugins&lt;/a&gt; to give bots new abilities&lt;/p&gt; &#xA;&lt;p&gt;📚 Store &lt;a href=&#34;https://askmarvin.ai/guide/concepts/loaders_and_documents&#34;&gt;knowledge&lt;/a&gt; that bots can access and use&lt;/p&gt; &#xA;&lt;p&gt;📡 Available as a Python API, interactive CLI, or FastAPI server&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install&lt;/strong&gt;: &lt;code&gt;pip install marvin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: &lt;code&gt;marvin chat&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;marvin chat -p &#34;knows every Star Wars meme&#34; Hello there&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PrefectHQ/marvin/main/docs/img/marvin_hello_there_chat.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.askmarvin.ai/getting_started/installation/&#34;&gt;getting started&lt;/a&gt; docs for more!&lt;/p&gt; &#xA;&lt;h2&gt;Open Source&lt;/h2&gt; &#xA;&lt;p&gt;Marvin is open-source with an Apache 2.0 license and built on standards like Pydantic, FastAPI, Langchain, and Prefect.&lt;/p&gt; &#xA;&lt;h3&gt;🚧 Construction Zone&lt;/h3&gt; &#xA;&lt;p&gt;Marvin is under active development and is likely to change.&lt;/p&gt; &#xA;&lt;p&gt;Coming soon:&lt;/p&gt; &#xA;&lt;p&gt;♻️ Interactive AI functions&lt;/p&gt; &#xA;&lt;p&gt;🖼️ Admin and chat UIs&lt;/p&gt; &#xA;&lt;p&gt;🏗️ Advanced data loading and preprocessing&lt;/p&gt; &#xA;&lt;p&gt;🔭 AI observability platform&lt;/p&gt; &#xA;&lt;p&gt;🖥️ Deployment guides&lt;/p&gt; &#xA;&lt;p&gt;🎁 Quickstarts for common use cases&lt;/p&gt; &#xA;&lt;h2&gt;When should you use Marvin?&lt;/h2&gt; &#xA;&lt;p&gt;Marvin is an opinionated, high-level library with the goal of integrating AI tools into software development. There are a few major reasons to use Marvin:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You want an &lt;a href=&#34;https://askmarvin.ai/guide/concepts/ai_functions&#34;&gt;AI function&lt;/a&gt; that can process structured data.&lt;/strong&gt; Marvin brings the power of AI to native data structures, letting you build functions that would otheriwse be difficult or even impossible to write. For example, you can use AI functions to make a list of all the animals in a paragraph, generate JSON documents from HTML content, extract keywords that match some criteria, or categorize sentiment -- without any traditional source code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You want an &lt;a href=&#34;https://askmarvin.ai/guide/concepts/bots&#34;&gt;AI assistant&lt;/a&gt; in your code.&lt;/strong&gt; Marvin&#39;s bots can follow instructions and hold conversations to solve complex problems. They can use custom plugins and take advantage of external knowledge. They are designed to be integrated into your codebase, but of course you can expose them directly to your users as well!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You want to deploy cutting-edge AI technology with confidence, but without having to make too many decisions.&lt;/strong&gt; Using LLMs successfully requires very careful consideration of prompts, data preprocessing, and infrastructure. Our target user is more interested in &lt;em&gt;using&lt;/em&gt; AI systems than &lt;em&gt;building&lt;/em&gt; AI systems. Therefore, Marvin is designed to make adopting this technology as straightforward as possible by optimizing for useful outcomes. Marvin&#39;s prompts have been hardened by months of real-world use and will continue to improve over time.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;When should you NOT use Marvin?&lt;/h2&gt; &#xA;&lt;p&gt;There are a few reasons NOT to use Marvin:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You want full control of the AI.&lt;/strong&gt; Marvin is a high-level library and (with few exceptions) does not generally expose LLM configuration to users. We have chosen settings that give the best results under most circumstances, taking Marvin&#39;s built-in prompts into consideration.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You want an AI copilot for writing code.&lt;/strong&gt; Marvin&#39;s job isn&#39;t to help you write source code; it&#39;s to help you do things that are difficult or impossible to express in source code. That could range from mundane activities to writing a function that can extract the names of animals commonly found in North America from an email (yes, it&#39;s a ridiculous example - but it&#39;s possible). Modern LLMs excel at complex reasoning, and Marvin lets you bring that into your code in a way that feels native and natural.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You want to use other LLM models.&lt;/strong&gt; Marvin is designed to run against OpenAI&#39;s GPT-4 and GPT-3.5 models. While we may expand those models in the future, we&#39;ve discovered that prompts designed for one model rarely translate well to others without modification. In order to maximize the usefulness of the library, we&#39;ve decided to focus on just these popular models for now.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You want full control of your prompts.&lt;/strong&gt; As a &#34;functional prompt engineering&#34; platform, Marvin takes user inputs and generates prompts that are likely to deliver the outcome the user wants, even if they are not verbatim what the user said. Marvin does not expect users to send completely raw prompts to the LLM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;You&#39;re searching for the Ultimate Question.&lt;/strong&gt; While Marvin is highly intelligent, even he couldn&#39;t come up with the Ultimate Question of Life, the Universe, and Everything. If you&#39;re seeking existential enlightenment, you might need to look beyond our beloved paranoid android.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>RosettaCommons/RFdiffusion</title>
    <updated>2023-04-02T01:42:59Z</updated>
    <id>tag:github.com,2023-04-02:/RosettaCommons/RFdiffusion</id>
    <link href="https://github.com/RosettaCommons/RFdiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for running RFdiffusion&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RF&lt;em&gt;diffusion&lt;/em&gt;&lt;/h1&gt; &#xA;&lt;!--&#xA;&lt;img width=&#34;1115&#34; alt=&#34;Screen Shot 2023-01-19 at 5 56 33 PM&#34; src=&#34;https://user-images.githubusercontent.com/56419265/213588200-f8f44dba-276e-4dd2-b844-15acc441458d.png&#34;&gt;&#xA;--&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/diffusion_protein_gradient_2.jpg&#34; alt=&#34;alt text&#34; width=&#34;1100px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Image: Ian C. Haydon / UW Institute for Protein Design&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;RFdiffusion is an open source method for structure generation, with or without conditional information (a motif, target etc). It can perform a whole range of protein design challenges as we have outlined in &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1&#34;&gt;the RFdiffusion paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Things Diffusion can do&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Motif Scaffolding&lt;/li&gt; &#xA; &lt;li&gt;Unconditional protein generation&lt;/li&gt; &#xA; &lt;li&gt;Symmetric unconditional generation (cyclic, dihedral and tetrahedral symmetries currently implemented, more coming!)&lt;/li&gt; &#xA; &lt;li&gt;Symmetric motif scaffolding&lt;/li&gt; &#xA; &lt;li&gt;Binder design&lt;/li&gt; &#xA; &lt;li&gt;Design diversification (&#34;partial diffusion&#34;, sampling around a design)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Table of contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#rfdiffusion&#34;&gt;RF&lt;em&gt;diffusion&lt;/em&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#description&#34;&gt;Description&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#table-of-contents&#34;&gt;Table of contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#getting-started--installation&#34;&gt;Getting started / installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#conda-install-se3-transformer&#34;&gt;Conda Install SE3-Transformer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#get-ppi-scaffold-examples&#34;&gt;Get PPI Scaffold Examples&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#running-the-diffusion-script&#34;&gt;Running the diffusion script&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#basic-execution---an-unconditional-monomer&#34;&gt;Basic execution - an unconditional monomer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#motif-scaffolding&#34;&gt;Motif Scaffolding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#the-active-site-model-holds-very-small-motifs-in-place&#34;&gt;The &#34;active site&#34; model holds very small motifs in place&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#the-inpaint_seq-flag&#34;&gt;The &lt;code&gt;inpaint_seq&lt;/code&gt; flag&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#a-note-on-diffusert&#34;&gt;A note on &lt;code&gt;diffuser.T&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#partial-diffusion&#34;&gt;Partial diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#binder-design&#34;&gt;Binder Design&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#practical-considerations-for-binder-design&#34;&gt;Practical Considerations for Binder Design&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#fold-conditioning&#34;&gt;Fold Conditioning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#generation-of-symmetric-oligomers&#34;&gt;Generation of Symmetric Oligomers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#using-auxiliary-potentials&#34;&gt;Using Auxiliary Potentials&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#symmetric-motif-scaffolding&#34;&gt;Symmetric Motif Scaffolding.&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#a-note-on-model-weights&#34;&gt;A Note on Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#things-you-might-want-to-play-with-at-inference-time&#34;&gt;Things you might want to play with at inference time&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#understanding-the-output-files&#34;&gt;Understanding the output files&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting started / installation&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to Sergey Ovchinnikov, RFdiffusion is available as a &lt;a href=&#34;https://colab.research.google.com/github/sokrypton/ColabDesign/blob/v1.1.1/rf/examples/diffusion.ipynb&#34;&gt;Google Colab Notebook&lt;/a&gt; if you would like to run it there!&lt;/p&gt; &#xA;&lt;p&gt;We strongly recommend reading this README carefully before getting started with RFdiffusion, and working through some of the examples in the Colab Notebook.&lt;/p&gt; &#xA;&lt;p&gt;If you want to set up RFdiffusion locally, follow the steps below:&lt;/p&gt; &#xA;&lt;p&gt;To get started using RFdiffusion, clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/RosettaCommons/RFdiffusion.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll then need to download the model weights:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir models &amp;amp;&amp;amp; cd models&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/6f5902ac237024bdd0c176cb93063dc4/Base_ckpt.pt&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/e29311f6f1bf1af907f9ef9f44b8328b/Complex_base_ckpt.pt&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/60f09a193fb5e5ccdc4980417708dbab/Complex_Fold_base_ckpt.pt&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/74f51cfb8b440f50d70878e05361d8f0/InpaintSeq_ckpt.pt&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/76d00716416567174cdb7ca96e208296/InpaintSeq_Fold_ckpt.pt&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/5532d2e1f3a4738decd58b19d633b3c3/ActiveSite_ckpt.pt&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/12fc204edeae5b57713c5ad7dcb97d39/Base_epoch8_ckpt.pt&#xA;&#xA;Optional:&#xA;wget http://files.ipd.uw.edu/pub/RFdiffusion/f572d396fae9206628714fb2ce00f72e/Complex_beta_ckpt.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Conda Install SE3-Transformer&lt;/h3&gt; &#xA;&lt;p&gt;Ensure that you have either &lt;a href=&#34;https://conda.io/projects/conda/en/latest/user-guide/install/index.html&#34;&gt;Anaconda or Miniconda&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;You also need to install &lt;a href=&#34;https://developer.nvidia.com/blog/accelerating-se3-transformers-training-using-an-nvidia-open-source-model-implementation/&#34;&gt;NVIDIA&#39;s implementation of SE(3)-Transformers&lt;/a&gt; Here is how to install the NVIDIA SE(3)-Transformer code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f env/SE3nv.yml&#xA;&#xA;conda activate SE3nv&#xA;cd env/SE3Transformer&#xA;pip install --no-cache-dir -r requirements.txt&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Anytime you run diffusion you should be sure to activate this conda environment by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate SE3nv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Get PPI Scaffold Examples&lt;/h3&gt; &#xA;&lt;p&gt;To run the scaffolded protein binder design (PPI) examples, we have provided some example scaffold files (&lt;code&gt;examples/ppi_scaffolds_subset.tar.gz&lt;/code&gt;). You&#39;ll need to untar this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tar -xvf examples/ppi_scaffolds_subset.tar.gz -C examples/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We will explain what these files are and how to use them in the Fold Conditioning section.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;In this section we will demonstrate how to run diffusion.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/main.png&#34; alt=&#34;alt text&#34; width=&#34;1100px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Running the diffusion script&lt;/h3&gt; &#xA;&lt;p&gt;The actual script you will execute is called &lt;code&gt;run_inference.py&lt;/code&gt;. There are many ways to run it, governed by hydra configs. &lt;a href=&#34;https://hydra.cc/docs/configure_hydra/intro/&#34;&gt;Hydra configs&lt;/a&gt; are a nice way of being able to specify many different options, with sensible defaults drawn &lt;em&gt;directly&lt;/em&gt; from the model checkpoint, so inference should always, by default, match training. What this means is that the default values in &lt;code&gt;config/inference/base.yml&lt;/code&gt; might not match the actual values used during inference, with a specific checkpoint. This is all handled under the hood.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Basic execution - an unconditional monomer&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/cropped_uncond.png&#34; alt=&#34;alt text&#34; width=&#34;400px&#34; align=&#34;right&#34;&gt; &#xA;&lt;p&gt;Let&#39;s first look at how you would do unconditional design of a protein of length 150aa. For this, we just need to specify three things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The length of the protein&lt;/li&gt; &#xA; &lt;li&gt;The location where we want to write files to&lt;/li&gt; &#xA; &lt;li&gt;The number of designs we want&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;./run_inference.py &#39;contigmap.contigs=[150-150]&#39; inference.output_prefix=test_outputs/test inference.num_designs=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s look at this in detail. Firstly, what is &lt;code&gt;contigmap.contigs&lt;/code&gt;? Hydra configs tell the inference script how it should be run. To keep things organised, the config has different sub-configs, one of them being &lt;code&gt;contigmap&lt;/code&gt;, which pertains to everything related to the contig string (that defines the protein being built). Take a look at the config file if this isn&#39;t clear: &lt;code&gt;configs/inference/base.yml&lt;/code&gt; Anything in the config can be overwritten manually from the command line. You could, for example, change how the diffuser works:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;diffuser.crd_scale=0.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;... but don&#39;t do this unless you really know what you&#39;re doing!!&lt;/p&gt; &#xA;&lt;p&gt;Now, what does &lt;code&gt;&#39;contigmap.contigs=[150-150]&#39;&lt;/code&gt; mean? To those who have used RFjoint inpainting, this might look familiar, but a little bit different. Diffusion, in fact, uses the identical &#39;contig mapper&#39; as inpainting, except that, because we&#39;re using hydra, we have to give this to the model in a different way. The contig string has to be passed as a single-item in a list, rather than as a string, for hydra reasons and the entire argument MUST be enclosed in &lt;code&gt;&#39;&#39;&lt;/code&gt; so that the commandline does not attempt to parse any of the special characters.&lt;/p&gt; &#xA;&lt;p&gt;The contig string allows you to specify a length range, but here, we just want a protein of 150aa in length, so you just specify [150-150] This will then run 10 diffusion trajectories, saving the outputs to your specified output folder.&lt;/p&gt; &#xA;&lt;p&gt;NB the first time you run RFdiffusion, it will take a while &#39;Calculating IGSO3&#39;. Once it has done this, it&#39;ll be cached for future reference though! For an additional example of unconditional monomer generation, take a look at &lt;code&gt;./examples/design_unconditional.sh&lt;/code&gt; in the repo!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Motif Scaffolding&lt;/h3&gt; &#xA;&lt;!--&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;  &lt;img src=&#34;./img/motif.png&#34; alt=&#34;alt text&#34; width=&#34;700px&#34; align=&#34;middle&#34;/&gt;&#xA;&lt;/p&gt;&#xA;--&gt; &#xA;&lt;p&gt;RFdiffusion can be used to scaffold motifs, in a manner akin to &lt;a href=&#34;https://www.science.org/doi/10.1126/science.abn2100#:~:text=The%20binding%20and%20catalytic%20functions%20of%20proteins%20are,the%20fold%20or%20secondary%20structure%20of%20the%20scaffold.&#34;&gt;Constrained Hallucination and RFjoint Inpainting&lt;/a&gt;. In general, RFdiffusion significantly outperforms both Constrained Hallucination and RFjoint Inpainting.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/motif.png&#34; alt=&#34;alt text&#34; width=&#34;700px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;When scaffolding protein motifs, we need a way of specifying that we want to scaffold some particular protein input (one or more segments from a &lt;code&gt;.pdb&lt;/code&gt; file), and to be able to specify how we want these connected, and by how many residues, in the new scaffolded protein. What&#39;s more, we want to be able to sample different lengths of connecting protein, as we generally don&#39;t know &lt;em&gt;a priori&lt;/em&gt; precisely how many residues we&#39;ll need to best scaffold a motif. This job of specifying inputs is handled by contigs, governed by the contigmap config in the hydra config. For those familiar with Constrained Hallucination or RFjoint Inpainting, the logic is very similar. Briefly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anything prefixed by a letter indicates that this is a motif, with the letter corresponding to the chain letter in the input pdb files. E.g. A10-25 pertains to residues (&#39;A&#39;,1),(&#39;A&#39;,2)...(&#39;A&#39;,10) in the corresponding input pdb&lt;/li&gt; &#xA; &lt;li&gt;Anything not prefixed by a letter indicates protein &lt;em&gt;to be built&lt;/em&gt;. This can be input as a length range. These length ranges are randomly sampled each iteration of RFdiffusion inference.&lt;/li&gt; &#xA; &lt;li&gt;To specify chain breaks, we use &lt;code&gt;/0 &lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In more detail, if we want to scaffold a motif, the input is just like RFjoint Inpainting, except needing to navigate the hydra config input. If we want to scaffold residues 10-25 on chain A a pdb, this would be done with &lt;code&gt;&#39;contigmap.contigs=[5-15/A10-25/30-40]&#39;&lt;/code&gt;. This asks RFdiffusion to build 5-15 residues (randomly sampled at each inference cycle) N-terminally of A10-25 from the input pdb, followed by 30-40 residues (again, randomly sampled) to its C-terminus. If we wanted to ensure the length was always e.g. 55 residues, this can be specified with &lt;code&gt;contigmap.length=55-55&lt;/code&gt;. You need to obviously also provide a path to your pdb file: &lt;code&gt;inference.input_pdb=path/to/file.pdb&lt;/code&gt;. It doesn&#39;t matter if your input pdb has residues you &lt;em&gt;don&#39;t&lt;/em&gt; want to scaffold - the contig map defines which residues in the pdb are actually used as the &#34;motif&#34;. In other words, even if your pdb files has a B chain, and other residues on the A chain, &lt;em&gt;only&lt;/em&gt; A10-25 will be provided to RFdiffusion.&lt;/p&gt; &#xA;&lt;p&gt;To specify that we want to inpaint in the presence of a separate chain, this can be done as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;contigmap.contigs=[5-15/A10-25/30-40/0 B1-100]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Look at this carefully. &lt;code&gt;/0 &lt;/code&gt; is the indicator that we want a chain break. NOTE, the space is important here. This tells the diffusion model to add a big residue jump (200aa) to the input, so that the model sees the first chain as being on a separate chain to the second.&lt;/p&gt; &#xA;&lt;p&gt;An example of motif scaffolding can be found in &lt;code&gt;./examples/design_motifscaffolding.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;The &#34;active site&#34; model holds very small motifs in place&lt;/h3&gt; &#xA;&lt;p&gt;In the RFdiffusion preprint we noted that for very small motifs, RFdiffusion has the tendency to not keep them perfectly fixed in the output. Therefore, for scaffolding minimalist sites such as enzyme active sites, we fine-tuned RFdiffusion on examples similar to these tasks, allowing it to hold smaller motifs better in place, and better generate &lt;em&gt;in silico&lt;/em&gt; successes. If your input functional motif is very small, we reccomend using this model, which can easily be specified using the following syntax: &lt;code&gt;inference.ckpt_override_path=models/ActiveSite_ckpt.pt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The &lt;code&gt;inpaint_seq&lt;/code&gt; flag&lt;/h3&gt; &#xA;&lt;p&gt;For those familiar with RFjoint Inpainting, the contigmap.inpaint_seq input is equivalent. The idea is that often, when, for example, fusing two proteins, residues that were on the surface of a protein (and are therefore likely polar), now need to be packed into the &#39;core&#39; of the protein. We therefore want them to become hydrophobic residues. What we can do, rather than directly mutating them to hydrophobics, is to mask their sequence identity, and allow RFdiffusion to implicitly reason over their sequence, and better pack against them. This requires a different model than the &#39;base&#39; diffusion model, that has been trained to understand this paradigm, but this is automatically handled by the inference script (you don&#39;t need to do anything).&lt;/p&gt; &#xA;&lt;p&gt;To specify amino acids whose sequence should be hidden, use the following syntax:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;contigmap.inpaint_seq=[A1/A30-40]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, we&#39;re masking the residue identity of residue A1, and all residues between A30 and A40 (inclusive).&lt;/p&gt; &#xA;&lt;p&gt;An example of executing motif scaffolding with the &lt;code&gt;contigmap.inpaint_seq&lt;/code&gt; flag is located in &lt;code&gt;./examples/design_motifscaffolding_inpaintseq.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;A note on &lt;code&gt;diffuser.T&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;RFdiffusion was originally trained with 200 discrete timesteps. However, recent improvements have allowed us to reduce the number of timesteps we need to use at inference time. In many cases, running with as few as approximately 20 steps provides outputs of equivalent &lt;em&gt;in silico&lt;/em&gt; quality to running with 200 steps (providing a 10X speedup). The default is now set to 50 steps. Noting this is important for understanding the partial diffusion, described below.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Partial diffusion&lt;/h3&gt; &#xA;&lt;p&gt;Something we can do with diffusion is to partially noise and de-noise a structure, to get some diversity around a general fold. This can work really nicely (see &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.12.10.519862v4.abstract&#34;&gt;Vazquez-Torres et al., BioRxiv 2022&lt;/a&gt;). This is specified by using the diffuser.parial_T input, and setting a timestep to &#39;noise&#39; to.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/partial.png&#34; alt=&#34;alt text&#34; width=&#34;800px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; More noise == more diversity. In Vazquez-Torres et al., 2022, we typically used `diffuser.partial_T` of approximately 80, but this was with respect to the 200 timesteps we were using. Now that the default `diffuser.T` is 50, you will need to adjust diffuser.partial_T accordingly. E.g. now that `diffuser.T=50`, the equivalent of 80 noising steps is `diffuser.partial_T=20`. We strongly recommend sampling different values for `partial_T` however, to find the best parameters for your specific problem. &#xA;&lt;p&gt;When doing partial diffusion, because we are now diffusing from a known structure, this creates certain constraints. You can still use the contig input, but &lt;em&gt;this has to yield a contig string exactly the same length as the input protein&lt;/em&gt;. E.g. if you have a binder:target complex, and you want to diversify the binder (length 100, chain A), you would need to input something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;contigmap.contigs=[100-100/0 B1-150]&#39; diffuser.partial_T=20&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The reason for this is that, if your input protein was only 80 amino acids, but you&#39;ve specified a desired length of 100, we don&#39;t know where to diffuse those extra 20 amino acids from, and hence, they will not lie in the distribution that RFdiffusion has learned to denoise from.&lt;/p&gt; &#xA;&lt;p&gt;An example of partial diffusion can be found in &lt;code&gt;./examples/design_partialdiffusion.sh&lt;/code&gt;!&lt;/p&gt; &#xA;&lt;p&gt;You can also keep parts of the sequence of the diffused chain fixed, if you want. An example of why you might want to do this is in the context of helical peptide binding. If you&#39;ve threaded a helical peptide sequence onto an ideal helix, and now want to diversify the complex, allowing the helix to be predicted now not as an ideal helix, you might do something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;contigmap.contigs=[100-100/0 20-20]&#39; &#39;contigmap.provide_seq=[100-119]&#39; diffuser.partial_T=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this case, the 20aa chain is the helical peptide. The &lt;code&gt;contigmap.provide_seq&lt;/code&gt; input is zero-indexed, and you can provide a range (so 100-119 is an inclusive range, unmasking the whole sequence of the peptide).&lt;/p&gt; &#xA;&lt;p&gt;Note that the provide_seq option requires using a different model checkpoint, but this is automatically handled by the inference script.&lt;/p&gt; &#xA;&lt;p&gt;An example of partial diffusion with providing sequence in diffused regions can be found in &lt;code&gt;./examples/design_partialdiffusion_withseq.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Binder Design&lt;/h3&gt; &#xA;&lt;p&gt;Hopefully, it&#39;s now obvious how you might make a binder with diffusion! Indeed, RFdiffusion shows excellent &lt;em&gt;in silico&lt;/em&gt; and experimental ability to design &lt;em&gt;de novo&lt;/em&gt; binders.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/binder.png&#34; alt=&#34;alt text&#34; width=&#34;950px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;If chain B is your target, then you could do it like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./run_inference.py &#39;contigmap.contigs=[B1-100/0 100-100]&#39; inference.output_prefix=test_outputs/binder_test inference.num_designs=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate 100 residue long binders to residues 1-100 of chain B. You can find another example of&lt;/p&gt; &#xA;&lt;p&gt;However, this probably isn&#39;t the best way of making binders. Because diffusion is somewhat computationally-intensive, we need to try and make it as fast as possible. Providing the whole of your target, uncropped, is going to make diffusion very slow if your target is big (and most targets-of-interest, such as cell-surface receptors tend to be &lt;em&gt;very&lt;/em&gt; big). One tried-and-true method to speed up binder design is to crop the target protein around the desired interface location. BUT! This creates a problem: if you crop your target and potentially expose hydrophobic core residues which were buried before the crop, how can you guarantee the binder will go to the intended interface site on the surface of the target, and not target the tantalizing hydrophobic patch you have just artificially created?&lt;/p&gt; &#xA;&lt;p&gt;We solve this issue by providing the model with what we call &#34;hotspot residues&#34;. The complex models we refer to earlier in this README file have all been trained with hotspot residues, in this training regime, during each example, the model is told (some of) the residues on the target protein which contact the target (i.e., resides that are part of the interface). The model readily learns that it should be making an interface which involved these hotspot residues. At inference time then, we can provide our own hotspot residues to define a region which the binder must contact. These are specfied like this: &lt;code&gt;&#39;ppi.hotspots=[A30,A33,A34]&#39;&lt;/code&gt;, where &lt;code&gt;A&lt;/code&gt; is the chain ID in the input pdb file of the hotspot residue and the number is the residue index in the input pdb file of the hotspot residue.&lt;/p&gt; &#xA;&lt;p&gt;Finally, it has been observed that the default RFdiffusion model often generates mostly helical binders. These have high computational and experimental success rates. However, there may be cases where other kinds of topologies may be desired. For this, we include a &#34;beta&#34; model, which generates a greater diversity of topologies, but has not been extensively experimentally validated. Try this at your own risk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;inference.ckpt_override_path=models/Complex_beta_ckpt.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of binder design with RFdiffusion can be found in &lt;code&gt;./examples/design_ppi.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Practical Considerations for Binder Design&lt;/h2&gt; &#xA;&lt;p&gt;RFdiffusion is an extremely powerful binder design tool but it is not magic. In this section we will walk through some common pitfalls in RFdiffusion binder design and offer advice on how to get the most out of this method.&lt;/p&gt; &#xA;&lt;h3&gt;Selecting a Target Site&lt;/h3&gt; &#xA;&lt;p&gt;Not every site on a target protein is a good candidate for binder design. For a site to be an attractive candidate for binding it should have &amp;gt;~3 hydrophobic residues for the binder to interact with. Binding to charged polar sites is still quite hard. Binding to sites with glycans close to them is also hard since they often become ordered upon binding and you will take an energetic hit for that. Historically, binder design has also avoided unstructured loops, it is not clear if this is still a requirement as RFdiffusion has been used to bind unstructured peptides which share a lot in common with unstructured loops.&lt;/p&gt; &#xA;&lt;h3&gt;Truncating your Target Protein&lt;/h3&gt; &#xA;&lt;p&gt;RFdiffusion scales in runtime as O(N^2) where N is the number of residues in your system. As such, it is a very good idea to truncate large targets so that your computations are not unnecessarily expensive. RFdiffusion and all downstream steps (including AF2) are designed to allow for a truncated target. Truncating a target is an art. For some targets, such as multidomain extracellular membranes, a natural truncation point is where two domains are joined by a flexible linker. For other proteins, such as virus spike proteins, this truncation point is less obvious. Generally you want to preserve secondary structure and introduce as few chain breaks as possible. You should also try to leave ~10A of target protein on each side of your intended target site. We recommend using PyMol to truncate your target protein.&lt;/p&gt; &#xA;&lt;h3&gt;Picking Hotspots&lt;/h3&gt; &#xA;&lt;p&gt;Hotspots are a feature that we integrated into the model to allow for the control of the site on the target which the binder will interact with. In the paper we define a hotspot as a residue on the target protein which is within 10A Cbeta distance of the binder. Of all of the hotspots which are identified on the target 0-20% of these hotspots are actually provided to the model and the rest are masked. This is important for understanding how you should pick hotspots at inference time.; the model is expecting to have to make more contacts than you specify. We normally recommend between 3-6 hotspots, you should run a few pilot runs before generating thousands of designs to make sure the number of hotspots you are providing will give results you like.&lt;/p&gt; &#xA;&lt;p&gt;If you have run the previous PatchDock RifDock binder design pipeline, for the RFdiffusion paper we chose our hotspots to be the PatchDock residues of the target.&lt;/p&gt; &#xA;&lt;h3&gt;Binder Design Scale&lt;/h3&gt; &#xA;&lt;p&gt;In the paper, we generated ~10,000 RFdiffusion binder backbones for each target. From this set of backbones we then generated two sequences per backbone using ProteinMPNN-FastRelax. (described below) We screened these ~20,000 designs using AF2 with initial guess and target templating (described below). The larger the scale that you can go to, the more designs will pass your filters and the better your odds of identifying a binder experimentally.&lt;/p&gt; &#xA;&lt;h3&gt;Sequence Design for Binders&lt;/h3&gt; &#xA;&lt;p&gt;You may have noticed that the binders designed by RFdiffusion come out with a poly-Glycine sequence. This is not a bug. RFdiffusion is a backbone-generation model and does not generate sequence for the designed region, therefore, another method must be used to assign a sequence to the binders. In the paper we use the ProteinMPNN-FastRelax protocol to do sequence design. We recommend that you do this as well. The code for this protocol can be found in &lt;a href=&#34;https://github.com/nrbennet/dl_binder_design&#34;&gt;this GitHub repo&lt;/a&gt;. While we did not find the FastRelax part of the protocol to yield the large in silico success rate improvements that it yielded with the RifDock-generated docks, it is still a good way to increase your number of shots-on-goal for each (computationally expensive) RFdiffusion backbone. If you would prefer to simply run ProteinMPNN on your binders without the FastRelax step, that will work fine but will be more computationally expensive.&lt;/p&gt; &#xA;&lt;h3&gt;Binder Design Filtering&lt;/h3&gt; &#xA;&lt;p&gt;One of the most important parts of the binder design pipeline is a filtering step to evaluate if your binders are actually predicted to work. In the paper we filtered using AF2 with an initial guess and target templating, scripts for this protocol are available &lt;a href=&#34;https://github.com/nrbennet/dl_binder_design&#34;&gt;here&lt;/a&gt;. We have found that filtering at pae_interaction &amp;lt; 10 is a good predictor of a binder working experimentally.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Fold Conditioning&lt;/h3&gt; &#xA;&lt;p&gt;Something that works really well is conditioning binder design (or monomer generation) on particular topologies. This is achieved by providing (partial) secondary structure and block adjacency information (to a model that has been trained to condition on this).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/fold_cond.png&#34; alt=&#34;alt text&#34; width=&#34;950px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; We are still working out the best way to actually generate this input at inference time, but for now, we have settled upon generating inputs directly from pdb structures. This permits &#39;low resolution&#39; specification of output topology (i.e., I want a TIM barrel but I don&#39;t care precisely where resides are). In `helper_scripts/`, there&#39;s a script called `make_secstruc_adj.py`, which can be used as follows: &#xA;&lt;p&gt;e.g. 1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./make_secstruc_adj.py --input_pdb ./2KL8.pdb --out_dir /my/dir/for/adj_secstruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or e.g. 2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./make_secstruc_adj.py --pdb_dir ./pdbs/ --out_dir /my/dir/for/adj_secstruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will process either a single pdb, or a folder of pdbs, and output a secondary structure and adjacency pytorch file, ready to go into the model. For now (although this might not be necessary), you should also generate these files for the target protein (if you&#39;re doing PPI), and provide this to the model. You can then use these at inference as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./run_inference.py inference.output_prefix=./scaffold_conditioned_test/test scaffoldguided.scaffoldguided=True scaffoldguided.target_pdb=False scaffoldguided.scaffold_dir=./examples/ppi_scaffolds_subset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A few exra things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;As mentioned above, for PPI, you will want to provide a target protein, along with its secondary structure and block adjacency. This can be done by adding:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;scaffoldguided.target_pdb=True scaffoldguided.target_path=input_pdbs/insulin_target.pdb inference.output_prefix=insulin_binder/jordi_ss_insulin_noise0_job0 &#39;ppi.hotspot_res=[A59,A83,A91]&#39; scaffoldguided.target_ss=target_folds/insulin_target_ss.pt scaffoldguided.target_adj=target_folds/insulin_target_adj.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate these block adjacency and secondary structure inputs, you can use the helper script.&lt;/p&gt; &#xA;&lt;p&gt;This will now generate 3-helix bundles to the insulin target.&lt;/p&gt; &#xA;&lt;p&gt;For ppi, it&#39;s probably also worth adding this flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scaffoldguided.mask_loops=False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is quite important to understand. During training, we mask some of the secondary structure and block adjacency. This is convenient, because it allows us to, at inference, easily add extra residues without having to specify precise secondary structure for every residue. E.g. if you want to make a long 3 helix bundle, you could mask the loops, and add e.g. 20 more &#39;mask&#39; tokens to that loop. The model will then (presumbly) choose to make e.g. 15 of these residues into helices (to extend the 3HB), and then make a 5aa loop. But, you didn&#39;t have to specify that, which is nice. The way this would be done would be like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scaffoldguided.mask_loops=True scaffoldguided.sampled_insertion=15 scaffoldguided.sampled_N=5 scaffoldguided.sampled_C=5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will, at each run of inference, sample up to 15 residues to insert into loops in your 3HB input, and up to 5 additional residues at N and C terminus. This strategy is very useful if you don&#39;t have a large set of pdbs to make block adjacencies for. For example, we showed that we could generate loads of lengthened TIM barrels from a single starting pdb with this strategy. However, for PPI, if you&#39;re using the provided scaffold sets, it shouldn&#39;t be necessary (because there are so many scaffolds to start from, generating extra diversity isn&#39;t especially necessary).&lt;/p&gt; &#xA;&lt;p&gt;Finally, if you have a big directory of block adjacency/secondary structure files, but don&#39;t want to use all of them, you can make a &lt;code&gt;.txt&lt;/code&gt; file of the ones you want to use, and pass:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scaffoldguided.scaffold_list=path/to/list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For PPI, we&#39;ve consistently seen that reducing the noise added at inference improves designs. This comes at the expense of diversity, but, given that the scaffold sets are huge, this probably doesn&#39;t matter too much. We therefore recommend lowering the noise. 0.5 is probably a good compromise:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;denoiser.noise_scale_ca=0.5 denoiser.noise_scale_frame=0.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This just scales the amount of noise we add to the translations (&lt;code&gt;noise_scale_ca&lt;/code&gt;) and rotations (&lt;code&gt;noise_scale_frame&lt;/code&gt;) by, in this case, 0.5.&lt;/p&gt; &#xA;&lt;p&gt;An additional example of PPI with fold conditioning is available here: &lt;code&gt;./examples/design_ppi_scaffolded.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Generation of Symmetric Oligomers&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;re going to switch gears from discussing PPI and look at another task at which RFdiffusion performs well on: symmetric oligomer design. This is done by symmetrising the noise we sample at t=T, and symmetrising the input at every timestep. We have currently implemented the following for use (with the others coming soon!):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cyclic symmetry&lt;/li&gt; &#xA; &lt;li&gt;Dihedral symmetry&lt;/li&gt; &#xA; &lt;li&gt;Tetrahedral symmetry&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/olig2.png&#34; alt=&#34;alt text&#34; width=&#34;1000px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./run_inference.py --config-name symmetry  inference.symmetry=tetrahedral &#39;contigmap.contigs=[360]&#39; inference.output_prefix=test_sample/tetrahedral inference.num_designs=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here, we&#39;ve specified a different &lt;code&gt;config&lt;/code&gt; file (with &lt;code&gt;--config-name symmetry&lt;/code&gt;). Because symmetric diffusion is quite different from the diffusion described above, we packaged a whole load of symmetry-related configs into a new file (see &lt;code&gt;configs/inference/symmetry.yml&lt;/code&gt;). Using this config file now puts diffusion in &lt;code&gt;symmetry-mode&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The symmetry type is then specified with &lt;code&gt;inference.symmetry=&lt;/code&gt;. Here, we&#39;re specifying tetrahedral symmetry, but you could also choose cyclic (e.g. &lt;code&gt;c4&lt;/code&gt;) or dihedral (e.g. &lt;code&gt;d2&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The configmap.contigs length refers to the &lt;em&gt;total&lt;/em&gt; length of your oligomer. Therefore, it &lt;em&gt;must&lt;/em&gt; be divisible by &lt;em&gt;n&lt;/em&gt; chains.&lt;/p&gt; &#xA;&lt;p&gt;More examples of designing oligomers can be found here: &lt;code&gt;./examples/design_cyclic_oligos.sh&lt;/code&gt;, &lt;code&gt;./examples/design_dihedral_oligos.sh&lt;/code&gt;, &lt;code&gt;./examples/design_tetrahedral_oligos.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Using Auxiliary Potentials&lt;/h3&gt; &#xA;&lt;p&gt;Performing diffusion with symmetrized noise may give you the idea that we could use other external interventions during the denoising process to guide diffusion. One such intervention that we have implemented is auxiliary potentials. Auxiliary potentials can be very useful for guiding the inference process. E.g. whereas in RFjoint inpainting, we have little/no control over the final shape of an output, in diffusion we can readily force the network to make, for example, a well-packed protein. This is achieved in the updates we make at each step.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s go a little deeper into how the diffusion process works: At timestep T (the first step of the reverse-diffusion inference process), we sample noise from a known &lt;em&gt;prior&lt;/em&gt; distribution. The model then makes a prediction of what the final structure should be, and we use these two states (noise at time T, prediction of the structure at time 0) to back-calculate where t=T-1 would have been. We therefore have a vector pointing from each coordinate at time T, to their corresponding, back-calculated position at time T-1. But, we want to be able to bias this update, to &lt;em&gt;push&lt;/em&gt; the trajectory towards some desired state. This can be done by biasing that vector with another vector, which points towards a position where that residue would &lt;em&gt;reduce&lt;/em&gt; the &#39;loss&#39; as defined by your potential. E.g. if we want to use the &lt;code&gt;monomer_ROG&lt;/code&gt; potential, which seeks to minimise the radius of gyration of the final protein, if the models prediction of t=0 is very elongated, each of those distant residues will have a larger gradient when we differentiate the &lt;code&gt;monomer_ROG&lt;/code&gt; potential w.r.t. their positions. These gradients, along with the corresponding scale, can be combined into a vector, which is then combined with the original update vector to make a &#34;biased update&#34; at that timestep.&lt;/p&gt; &#xA;&lt;p&gt;The exact parameters used when applying these potentials matter. If you weight them too strongly, you&#39;re not going to end up with a good protein. Too weak, and they&#39;ll have little effect. We&#39;ve explored these potentials in a few different scenarios, and have set sensible defaults, if you want to use them. But, if you feel like they&#39;re too weak/strong, or you just fancy exploring, do play with the parameters (in the &lt;code&gt;potentials&lt;/code&gt; part of the config file).&lt;/p&gt; &#xA;&lt;p&gt;Potentials are specified as a list of strings with each string corresponding to a potential. The argument for potentials is &lt;code&gt;potentials.guiding_potentials&lt;/code&gt;. Within the string per-potential arguments may be specified in the following syntax: &lt;code&gt;arg_name1:arg_value1,arg_name2:arg_value2,...,arg_nameN:arg_valueN&lt;/code&gt;. The only argument that is required for each potential is the name of the potential that you wish to apply, the name of this argument is &lt;code&gt;type&lt;/code&gt; as-in the type of potential you wish to use. Some potentials such as &lt;code&gt;olig_contacts&lt;/code&gt; and &lt;code&gt;substrate_contacts&lt;/code&gt; take global options such as &lt;code&gt;potentials.substrate&lt;/code&gt;, see &lt;code&gt;config/inference/base.yml&lt;/code&gt; for all the global arguments associated with potentials. Additionally, it is useful to have the effect of the potential &#34;decay&#34; throughout the trajectory, such that in the beginning the effect of the potential is 1x strength, and by the end is much weaker. These decays (&lt;code&gt;constant&lt;/code&gt;,&lt;code&gt;linear&lt;/code&gt;,&lt;code&gt;quadratic&lt;/code&gt;,&lt;code&gt;cubic&lt;/code&gt;) can be set with the &lt;code&gt;potentials.guide_decay&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to specify a potential:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;potentials.guiding_potentials=[\&#34;type:olig_contacts,weight_intra:1,weight_inter:0.1\&#34;] potentials.olig_intra_all=True potentials.olig_inter_all=True potentials.guide_scale=2 potentials.guide_decay=&#39;quadratic&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We are still fully characterising how/when to use potentials, and we strongly recommend exploring different parameters yourself, as they are clearly somewhat case-dependent. So far, it is clear that they can be helpful for motif scaffolding and symmetric oligomer generation. However, they seem to interact weirdly with hotspot residues in PPI. We think we know why this is, and will work in the coming months to write better potentials for PPI. And please note, it is often good practice to start with &lt;em&gt;no potentials&lt;/em&gt; as a baseline, then slowly increase their strength. For the oligomer contacts potentials, start with the ones provided in the examples, and note that the &lt;code&gt;intra&lt;/code&gt; chain potential often should be higher than the &lt;code&gt;inter&lt;/code&gt; chain potential.&lt;/p&gt; &#xA;&lt;p&gt;We have already implemented several potentials but it is relatively straightforward to add more, if you want to push your designs towards some specified goal. The &lt;em&gt;only&lt;/em&gt; condition is that, whatever potential you write, it is differentiable. Take a look at &lt;code&gt;potentials.potentials.py&lt;/code&gt; for examples of the potentials we have implemented so far.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Symmetric Motif Scaffolding.&lt;/h3&gt; &#xA;&lt;p&gt;We can also combine symmetric diffusion with motif scaffolding to scaffold motifs symmetrically. Currently, we have one way for performing symmetric motif scaffolding. That is by specifying the position of the motif specified w.r.t. the symmetry axes.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RosettaCommons/RFdiffusion/main/img/sym_motif.png&#34; alt=&#34;alt text&#34; width=&#34;1000px&#34; align=&#34;middle&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Special input .pdb and contigs requirements&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For now, we require that a user have a symmetrized version of their motif in their input pdb for symmetric motif scaffolding. There are two main reasons for this. First, the model is trained by centering any motif at the origin, and thus the code also centers motifs at the origin automatically. Therefore, if your motif is not symmetrized, this centering action will result in an asymmetric unit that now has the origin and axes of symmetry running right through it (bad). Secondly, the diffusion code uses a canonical set of symmetry axes (rotation matrices) to propogate the asymmetric unit of a motif. In order to prevent accidentally running diffusion trajectories which are propogating your motif in ways you don&#39;t intend, we require that a user symmetrize an input using the RFdiffusion canonical symmetry axes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RFdiffusion canonical symmetry axes&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Group&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Axis&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Cyclic&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Z&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dihedral (cyclic)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Z&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dihedral (flip/reflection)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;X&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example: Inputs for symmetric motif scaffolding with motif position specified w.r.t the symetry axes.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This example script &lt;code&gt;examples/design_nickel.sh&lt;/code&gt; can be used to scaffold the C4 symmetric Nickel binding domains shown in the RFdiffusion paper. It combines many concepts discussed earlier, including symmetric oligomer generation, motif scaffolding, and use of guiding potentials.&lt;/p&gt; &#xA;&lt;p&gt;Note that the contigs should specify something that is precisely symmetric. Things will break if this is not the case.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;A Note on Model Weights&lt;/h3&gt; &#xA;&lt;p&gt;Because of everything we want diffusion to be able to do, there is not &lt;em&gt;One Model To Rule Them All&lt;/em&gt;. E.g., if you want to run with secondary structure conditioning, this requires a different model than if you don&#39;t. Under the hood, we take care of most of this by default - we parse your input and work out the most appropriate checkpoint. This is where the config setup is really useful. The exact model checkpoint used at inference contains in it all of the parameters is was trained with, so we can just populate the config file with those values, such that inference runs as designed. If you do want to specify a different checkpoint (if, for example, we train a new model and you want to test it), you just have to make sure it&#39;s compatible with what you&#39;re doing. E.g. if you try and give secondary structure features to a model that wasn&#39;t trained with them, it&#39;ll crash.&lt;/p&gt; &#xA;&lt;h3&gt;Things you might want to play with at inference time&lt;/h3&gt; &#xA;&lt;p&gt;Occasionally, it might good to try an alternative model (for example the active site model, or the beta binder model). These can be specified with &lt;code&gt;inference.ckpt_override_path&lt;/code&gt;. We do not recommend using these outside of the described use cases, however, as there is not a guarantee they will understand other kinds of inputs.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of things that are implemented at inference, see the config file (&lt;code&gt;configs/inference/base.yml&lt;/code&gt; or &lt;code&gt;configs/inference/symmetry.yml&lt;/code&gt;). Although you can modify everything, this is not recommended unless you know what you&#39;re doing. Generally, don&#39;t change the &lt;code&gt;model&lt;/code&gt;, &lt;code&gt;preprocess&lt;/code&gt; or &lt;code&gt;diffuser&lt;/code&gt; configs. These pertain to how the model was trained, so it&#39;s unwise to change how you use the model at inference time. However, the parameters below are definitely worth exploring: -inference.final_step: This is when we stop the trajectory. We have seen that you can stop early, and the model is already making a good prediction of the final structure. This speeds up inference. -denoiser.noise_scale_ca and denoiser.noise_scale_frame: These can be used to reduce the noise used during sampling (as discussed for PPI above). The default is 1 (the same noise added at training), but this can be reduced to e.g. 0.5, or even 0. This actually improves the quality of models coming out of diffusion, but at the expense of diversity. If you&#39;re not getting any good outputs, or if your problem is very constrained, you could try reducing the noise. While these parameters can be changed independently (for translations and rotations), we recommend keeping them tied.&lt;/p&gt; &#xA;&lt;h3&gt;Understanding the output files&lt;/h3&gt; &#xA;&lt;p&gt;We output several different files.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The &lt;code&gt;.pdb&lt;/code&gt; file. This is the final prediction out of the model. Note that every designed residue is output as a glycine (as we only designed the backbone), and no sidechains are output. This is because, even though RFdiffusion conditions on sidechains in an input motif, there is no loss applied to these predictions, so they can&#39;t strictly be trusted.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;.trb&lt;/code&gt; file. This contains useful metadata associated with that specific run, including the specific contig used (if length ranges were sampled), as well as the full config used by RFdiffusion. There are also a few other convenient items in this file: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;details about mapping (i.e. how residues in the input map to residues in the output) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;con_ref_pdb_idx&lt;/code&gt;/&lt;code&gt;con_hal_pdb_idx&lt;/code&gt; - These are two arrays including the input pdb indices (in con_ref_pdb_idx), and where they are in the output pdb (in con_hal_pdb_idx). This only contains the chains where inpainting took place (i.e. not any fixed receptor/target chains)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;con_ref_idx0&lt;/code&gt;/&lt;code&gt;con_hal_idx0&lt;/code&gt; - These are the same as above, but 0 indexed, and without chain information. This is useful for splicing coordinates out (to assess alignment etc).&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;inpaint_seq&lt;/code&gt; - This details any residues that were masked during inference.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Trajectory files. By default, we output the full trajectories into the &lt;code&gt;/traj/&lt;/code&gt; folder. These files can be opened in pymol, as multi-step pdbs. Note that these are ordered in reverse, so the first pdb is technically the last (t=1) prediction made by RFdiffusion during inference. We include both the &lt;code&gt;pX0&lt;/code&gt; predictions (what the model predicted at each timestep) and the &lt;code&gt;Xt-1&lt;/code&gt; trajectories (what went into the model at each timestep).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Conclusion&lt;/h3&gt; &#xA;&lt;p&gt;We are extremely excited to share RFdiffusion with the wider scientific community. We expect to push some updates as and when we make sizeable improvements in the coming months, so do stay tuned. We realize it may take some time to get used to executing RFdiffusion with perfect syntax (sometimes Hydra is hard), so please don&#39;t hesitate to create GitHub issues if you need help, we will respond as often as we can.&lt;/p&gt; &#xA;&lt;p&gt;Now, let&#39;s go make some proteins. Have fun!&lt;/p&gt; &#xA;&lt;p&gt;- Joe, David, Nate, Brian, Jason, and the RFdiffusion team.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;RFdiffusion builds directly on the architecture and trained parameters of RoseTTAFold. We therefore thank Frank DiMaio and Minkyung Baek, who developed RoseTTAFold. RFdiffusion is released under an open source BSD License (see LICENSE file). It is free for both non-profit and for-profit use.&lt;/p&gt;</summary>
  </entry>
</feed>