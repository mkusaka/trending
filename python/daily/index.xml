<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-20T01:42:38Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pengxiao-song/LaWGPT</title>
    <updated>2023-05-20T01:42:38Z</updated>
    <id>tag:github.com,2023-05-20:/pengxiao-song/LaWGPT</id>
    <link href="https://github.com/pengxiao-song/LaWGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LaWGPT：基于中文法律知识的大语言模型&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34; width=&#34;80%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-Wiki-brightgreen&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-beta1.0-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/pengxiao-song/lawgpt&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://www.lamda.nju.edu.cn/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/support-NJU--LAMDA-9cf.svg&#34;&gt;&lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p&gt;LaWGPT 是一系列基于中文法律知识的开源大语言模型。&lt;/p&gt; &#xA;&lt;p&gt;该系列模型在通用中文基座模型（如 Chinese-LLaMA、ChatGLM 等）的基础上扩充法律领域专有词表、&lt;strong&gt;大规模中文法律语料预训练&lt;/strong&gt;，增强了大模型在法律领域的基础语义理解能力。在此基础上，&lt;strong&gt;构造法律领域对话问答数据集、中国司法考试数据集进行指令精调&lt;/strong&gt;，提升了模型对法律内容的理解和执行能力。&lt;/p&gt; &#xA;&lt;p&gt;详细内容请参考&lt;a href=&#34;&#34;&gt;技术报告&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;本项目持续开展，法律领域数据集及系列模型后续相继开源，敬请关注。&lt;/p&gt; &#xA;&lt;h2&gt;更新&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🪴 2023/05/15：发布 &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;中文法律数据源汇总（Awesome Chinese Legal Resources）&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/raw/main/resources/legal_vocab.txt&#34;&gt;法律领域词表&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌟 2023/05/13：公开发布 &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Legal--Base--7B-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-LaWGPT--7B--beta1.0-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Legal-Base-7B&lt;/strong&gt;：法律基座模型，使用 50w 中文裁判文书数据二次预训练&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;LaWGPT-7B-beta1.0&lt;/strong&gt;：法律对话模型，构造 30w 高质量法律问答数据集基于 Legal-Base-7B 指令精调&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌟 2023/04/12：内部测试 &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Lawgpt--7B--alpha-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;LaWGPT-7B-alpha&lt;/strong&gt;：在 Chinese-LLaMA-7B 的基础上直接构造 30w 法律问答数据集指令精调&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;准备代码，创建环境&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:pengxiao-song/LaWGPT.git&#xA;cd LaWGPT&#xA;conda activate lawgpt&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;合并模型权重（可选）&lt;/p&gt; &lt;p&gt;&lt;strong&gt;如果您想使用 LaWGPT-7B-alpha 模型，可跳过改步，直接进入步骤3.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;如果您想使用 LaWGPT-7B-beta1.0 模型：&lt;/p&gt; &lt;p&gt;由于 &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-LLaMA&lt;/a&gt; 均未开源模型权重。根据相应开源许可，&lt;strong&gt;本项目只能发布 LoRA 权重&lt;/strong&gt;，无法发布完整的模型权重，请各位谅解。&lt;/p&gt; &lt;p&gt;本项目给出&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki/%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6&#34;&gt;合并方式&lt;/a&gt;，请各位获取原版权重后自行重构模型。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;启动示例&lt;/p&gt; &lt;p&gt;启动本地服务：&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate lawgpt&#xA;cd LaWGPT&#xA;sh src/scripts/generate.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;接入服务：&lt;/p&gt; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/demo.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;项目结构&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LaWGPT&#xA;├── assets # 项目静态资源&#xA;├── data   # 语料及精调数据&#xA;├── tools  # 数据清洗等工具&#xA;├── README.md&#xA;├── requirements.txt&#xA;└── src    # 源码&#xA;    ├── finetune.py&#xA;    ├── generate.py&#xA;    ├── models  # 基座模型及 Lora 权重&#xA;    │   ├── base_models&#xA;    │   └── lora_weights&#xA;    ├── outputs&#xA;    ├── scripts # 脚本文件&#xA;    │   ├── finetune.sh # 指令微调&#xA;    │   └── generate.sh # 服务创建&#xA;    ├── templates&#xA;    └── utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;数据构建&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于中文裁判文书网公开法律文书数据、司法考试数据等数据集展开，详情参考&lt;a href=&#34;&#34;&gt;中文法律数据汇总&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;初级数据生成：根据 &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#data-generation-process&#34;&gt;Stanford_alpaca&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt; 方式生成对话问答数据&lt;/li&gt; &#xA; &lt;li&gt;知识引导的数据生成：通过 Knowledge-based Self-Instruct 方式基于中文法律结构化知识生成数据。&lt;/li&gt; &#xA; &lt;li&gt;引入 ChatGPT 清洗数据，辅助构造高质量数据集。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;模型训练&lt;/h2&gt; &#xA;&lt;p&gt;LawGPT 系列模型的训练过程分为两个阶段：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;第一阶段：扩充法律领域词表，在大规模法律文书及法典数据上预训练 Chinese-LLaMA&lt;/li&gt; &#xA; &lt;li&gt;第二阶段：构造法律领域对话问答数据集，在预训练模型基础上指令精调&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;二次训练流程&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;参考 &lt;code&gt;src/data/example_instruction_train.json&lt;/code&gt; 构造二次训练数据集&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;src/scripts/train_lora.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;指令精调步骤&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;参考 &lt;code&gt;src/data/example_instruction_tune.json&lt;/code&gt; 构造指令微调数据集&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;src/scripts/finetune.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;计算资源&lt;/h3&gt; &#xA;&lt;p&gt;8 张 Tesla V100-SXM2-32GB&lt;/p&gt; &#xA;&lt;h2&gt;模型评估&lt;/h2&gt; &#xA;&lt;h3&gt;输出示例&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：请给出判决意见。&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-05.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：请介绍赌博罪的定义。&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-06.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：请问加班工资怎么算？&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-04.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：民间借贷受国家保护的合法利息是多少?&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-02.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：欠了信用卡的钱还不上要坐牢吗？&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-01.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;问题：你能否写一段抢劫罪罪名的案情描述？&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-03.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;局限性&lt;/h3&gt; &#xA;&lt;p&gt;由于计算资源、数据规模等因素限制，当前阶段 LawGPT 存在诸多局限性：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;数据资源有限、模型容量较小，导致其相对较弱的模型记忆和语言能力。因此，在面对事实性知识任务时，可能会生成不正确的结果。&lt;/li&gt; &#xA; &lt;li&gt;该系列模型只进行了初步的人类意图对齐。因此，可能产生不可预测的有害内容以及不符合人类偏好和价值观的内容。&lt;/li&gt; &#xA; &lt;li&gt;自我认知能力存在问题，中文理解能力有待增强。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;请诸君在使用前了解上述问题，以免造成误解和不必要的麻烦。&lt;/p&gt; &#xA;&lt;h2&gt;协作者&lt;/h2&gt; &#xA;&lt;p&gt;如下各位合作开展（按字母序排列）：&lt;a href=&#34;https://github.com/herobrine19&#34;&gt;@cainiao&lt;/a&gt;、&lt;a href=&#34;https://github.com/njuyxw&#34;&gt;@njuyxw&lt;/a&gt;、&lt;a href=&#34;https://github.com/pengxiao-song&#34;&gt;@pengxiao-song&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;请各位严格遵守如下约定：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;本项目任何资源&lt;strong&gt;仅供学术研究使用，严禁任何商业用途&lt;/strong&gt;。&lt;/li&gt; &#xA; &lt;li&gt;模型输出受多种不确定性因素影响，本项目当前无法保证其准确性，&lt;strong&gt;严禁用于真实法律场景&lt;/strong&gt;。&lt;/li&gt; &#xA; &lt;li&gt;本项目不承担任何法律责任，亦不对因使用相关资源和输出结果而可能产生的任何损失承担责任。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;问题反馈&lt;/h2&gt; &#xA;&lt;p&gt;如有问题，请在 GitHub Issue 中提交。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;提交问题之前，建议查阅 FAQ 及以往的 issue 看是否能解决您的问题。&lt;/li&gt; &#xA; &lt;li&gt;请礼貌讨论，构建和谐社区。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;协作者科研之余推进项目进展，由于人力有限难以实时反馈，给诸君带来不便，敬请谅解！&lt;/p&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于如下开源项目展开，在此对相关项目和开发人员表示诚挚的感谢：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chinese-LLaMA-Alpaca: &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLaMA: &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;https://github.com/facebookresearch/llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alpaca: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;alpaca-lora: &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ChatGLM-6B: &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;https://github.com/THUDM/ChatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;此外，本项目基于开放数据资源，详见 &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;Awesome Chinese Legal Resources&lt;/a&gt;，一并表示感谢。&lt;/p&gt; &#xA;&lt;h2&gt;引用&lt;/h2&gt; &#xA;&lt;p&gt;如果您觉得我们的工作对您有所帮助，请考虑引用该项目&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>haotian-liu/LLaVA</title>
    <updated>2023-05-20T01:42:38Z</updated>
    <id>tag:github.com,2023-05-20:/haotian-liu/LLaVA</id>
    <link href="https://github.com/haotian-liu/LLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🌋 LLaVA: Large Language and Vision Assistant&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0&#34;&gt;Model&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://hliu.cc&#34;&gt;Haotian Liu*&lt;/a&gt;, &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&amp;amp;hl=en/&#34;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&#34;https://pages.cs.wisc.edu/~yongjaelee/&#34;&gt;Yong Jae Lee&lt;/a&gt; (*Equal Contribution)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png&#34; width=&#34;50%&#34;&gt; &lt;br&gt; Generated by &lt;a href=&#34;https://gligen.github.io/&#34;&gt;GLIGEN&lt;/a&gt; via &#34;a cute lava llama with glasses&#34; and box prompt &lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[5/6] 🔥 We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[5/2] 🔥 We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[5/2] We upgrade LLaVA package to v0.1 to support Vicuna v0 and v1 checkpoints, please upgrade following instructions &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[4/30] Our checkpoint with Vicuna-7b-v0 has been released &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-7b&#34;&gt;here&lt;/a&gt;! This checkpoint is more accessible and device friendly. Stay tuned for a major upgrade next week!&lt;/li&gt; &#xA; &lt;li&gt;[4/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[4/17] 🔥 We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#data-download&#34;&gt;Data Download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights&#34;&gt;LLaVA Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#serving&#34;&gt;Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Download&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data file name&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/llava_instruct_150k.json&#34;&gt;llava_instruct_150k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;229 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/llava_instruct_80k.json&#34;&gt;llava_instruct_80k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;229 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/conversation_58k.json&#34;&gt;conversation_58k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;126 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/detail_23k.json&#34;&gt;detail_23k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;20.5 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/complex_reasoning_77k.json&#34;&gt;complex_reasoning_77k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.6 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To download our langauge-image multimodal instruction-folllowing dataset &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;&lt;code&gt;LLaVA-Instruct-150K&lt;/code&gt;&lt;/a&gt;, please run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh download_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretraining Dataset&lt;/h3&gt; &#xA;&lt;p&gt;The pretraining dataset used in this release is a subset of CC-3M dataset, filtered with a more balanced concept coverage distribution. Please see &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K&#34;&gt;here&lt;/a&gt; for a detailed description on the dataset structure and how to download the images.&lt;/p&gt; &#xA;&lt;p&gt;If you already have CC-3M dataset on your disk, the image names follow this format: &lt;code&gt;GCC_train_000000000.jpg&lt;/code&gt;. You may edit the &lt;code&gt;image&lt;/code&gt; field correspondingly if necessary.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;Chat File&lt;/th&gt; &#xA;   &lt;th&gt;Meta Data&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CC-3M Concept-balanced 595K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/raw/main/chat.json&#34;&gt;chat.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/raw/main/metadata.json&#34;&gt;metadata.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;211 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LAION/CC/SBU BLIP-Caption Concept-balanced 558K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/raw/main/blip_laion_cc_sbu_558k.json&#34;&gt;blip_laion_cc_sbu_558k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#&#34;&gt;metadata.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;181 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important notice&lt;/strong&gt;: Upon the request from the community, as ~15% images of the original CC-3M dataset are no longer accessible, we upload &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/images.zip&#34;&gt;&lt;code&gt;images.zip&lt;/code&gt;&lt;/a&gt; for better reproducing our work in research community. It must not be used for any other purposes. The use of these images must comply with the CC-3M license. This may be taken down at any time when requested by the original CC-3M dataset owner or owners of the referenced images.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-4 Prompts&lt;/h3&gt; &#xA;&lt;p&gt;We provide our prompts and few-shot samples for GPT-4 queries, to better facilitate research in this domain. Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/playground/data/prompts&#34;&gt;&lt;code&gt;prompts&lt;/code&gt;&lt;/a&gt; folder for three kinds of questions: conversation, detail description, and complex reasoning.&lt;/p&gt; &#xA;&lt;p&gt;They are organized in a format of &lt;code&gt;system_message.txt&lt;/code&gt; for system message, pairs of &lt;code&gt;abc_caps.txt&lt;/code&gt; for few-shot sample user input, and &lt;code&gt;abc_conv.txt&lt;/code&gt; for few-shot sample reference output.&lt;/p&gt; &#xA;&lt;p&gt;Note that you may find them in different format. For example, &lt;code&gt;conversation&lt;/code&gt; is in &lt;code&gt;jsonl&lt;/code&gt;, and detail description is answer-only. The selected format in our preliminary experiments work slightly better than a limited set of alternatives that we tried: &lt;code&gt;jsonl&lt;/code&gt;, more natural format, answer-only. If interested, you may try other variants or conduct more careful study in this. Contributions are welcomed!&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to LLaVA folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/haotian-liu/LLaVA.git&#xA;cd LLaVA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: [Update 4/30/23] We have successfully moved LLaVA framework to this repo, without the need of a special &lt;code&gt;transformers&lt;/code&gt; modified by us. If you install our repo before &lt;code&gt;4/30/23&lt;/code&gt;, please reinstall &lt;code&gt;transformers&lt;/code&gt; following the instructions &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#upgrade-to-v01&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install ninja&#xA;pip install flash-attn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upgrade to v0.1&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you install our package before 4/30/23, please make sure to execute the command below to correctly upgrade to v0.1. You may try a &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;clean install&lt;/a&gt; as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git pull&#xA;pip uninstall transformers&#xA;pip install git+https://github.com/huggingface/transformers@cae78c46&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LLaVA Weights&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt; weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the LLaVA weights.&lt;/p&gt; &#xA;&lt;p&gt;Instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the original LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get LLaVA weights by applying our delta (&lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0&#34;&gt;13b-v0&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0&#34;&gt;7b-v0&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1&#34;&gt;lightning-7B-v1-1&lt;/a&gt;). It will automatically download delta weights from our Hugging Face account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;LLaVA-13B&lt;/h3&gt; &#xA;&lt;p&gt;This conversion command needs around 60 GB of CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m llava.model.apply_delta \&#xA;    --base /path/to/llama-13b \&#xA;    --target /output/path/to/LLaVA-13B-v0 \&#xA;    --delta liuhaotian/LLaVA-13b-delta-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaVA-7B&lt;/h3&gt; &#xA;&lt;p&gt;This conversion command needs around 30 GB of CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m llava.model.apply_delta \&#xA;    --base /path/to/llama-7b \&#xA;    --target /output/path/to/LLaVA-7B-v0 \&#xA;    --delta liuhaotian/LLaVA-7b-delta-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaVA pretrained projector weights&lt;/h3&gt; &#xA;&lt;p&gt;The initial release is pretrained on &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K&#34;&gt;LLaVA-filtered CC3M 595K&lt;/a&gt; with 1 epoch. The pretrained weights are released &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may perform instruction tuning on our pretrained checkpoints, by using our &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;visual instruction tuning&lt;/a&gt; data following the instructions &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#fine-tuning-with-local-gpus&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Serving&lt;/h2&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If your the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal --num-gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.gradio_web_server --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;You can open your browser and chat with a model now.&lt;/h4&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;p&gt;A starting script for inference with LLaVA without the need of Gradio interface. The current implementation only supports for a single-turn Q-A session, and the interactive CLI is WIP. This also serves as an example for users to build customized inference scripts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.eval.run_llava \&#xA;    --model-name /path/to/LLaVA-13B-v0 \&#xA;    --image-file &#34;https://llava-vl.github.io/static/images/view.jpg&#34; \&#xA;    --query &#34;What are the things I should be cautious about when I visit here?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example output (varies in different runs):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;When visiting this picturesque location with a serene lake and a wooden pier extending over the water, one should be cautious about various safety aspects. Some important considerations include:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Ensuring that the pier is structurally sound andstable, as old or weakened pier structures might not support the weight of visitors.&lt;/li&gt; &#xA;  &lt;li&gt;Being aware of the water depth around the pier and lake, as sudden drop-offs or strong currents may pose a risk to swimmers, boaters, or those who venture too close to the edge.&lt;/li&gt; &#xA;  &lt;li&gt;Staying vigilant about the presence of wildlife in the area, such as slippery, stealthy fish or other animals that might cause harm or inconvenience.&lt;/li&gt; &#xA;  &lt;li&gt;Maintaining a safe distance from the water&#39;s edge, particularly for children, elderly individuals, or those who are not strong swimmers.&lt;/li&gt; &#xA;  &lt;li&gt;Following any posted signs or guidelines related to safety and the use of the pier and surrounding areas.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;By considering these safety precautions, visitors can enjoy the natural beauty of the location while minimizing risks and ensuring a safe and pleasant experience.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;GPT-assisted Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models. Please see our paper for more details.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate LLaVA responses&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python model_vqa.py \&#xA;    --model-name ./checkpoints/LLaVA-13B-v0 \&#xA;    --question-file \&#xA;    playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --image-folder \&#xA;    /path/to/coco2014_val \&#xA;    --answers-file \&#xA;    /path/to/answer-file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Evaluate the generated responses. In our case, &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/playground/data/coco2014_val_qa_eval/qa90_gpt4_answer.jsonl&#34;&gt;&lt;code&gt;answer-file-1.jsonl&lt;/code&gt;&lt;/a&gt; is the response generated by text-only GPT-4 (0314), with the context captions/boxes provided.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;OPENAI_API_KEY=&#34;sk-***********************************&#34; python eval_gpt_review_visual.py \&#xA;    --question playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --context table/caps_boxes_coco2014_val_80.jsonl \&#xA;    --answer-list \&#xA;    /path/to/answer-file-1.jsonl \&#xA;    /path/to/answer-file-2.jsonl \&#xA;    --rule table/rule.json \&#xA;    --output /path/to/review.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Summarize the evaluation results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python summarize_gpt_review.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ScienceQA&lt;/h3&gt; &#xA;&lt;h4&gt;Prepare Data&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Please see ScienceQA &lt;a href=&#34;https://github.com/lupantech/ScienceQA&#34;&gt;repo&lt;/a&gt; for setting up the dataset.&lt;/li&gt; &#xA; &lt;li&gt;Generate ScienceQA dataset for LLaVA conversation-style format.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python scripts/convert_sqa_to_llava \&#xA;    convert_to_llava \&#xA;    --base-dir /path/to/ScienceQA/data/scienceqa \&#xA;    --split {train,val,minival,test,minitest}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Evaluation&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download our pretrained LLaVA-13B (delta) weights for ScienceQA dataset &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0-science_qa&#34;&gt;here&lt;/a&gt;. Convert the delta weights to actual weights.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.model.apply_delta \&#xA;    --base /path/to/llama-13b \&#xA;    --target /path/to/LLaVA-13b-v0-science_qa \&#xA;    --delta liuhaotian/LLaVA-13b-delta-v0-science_qa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;[Option 1] Multiple-GPU inference You may evaluate this with multiple GPUs, and concatenate the generated jsonl files. Please refer to our script for &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/scripts/sqa_eval_batch.sh&#34;&gt;batch evaluation&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/scripts/sqa_eval_gather.sh&#34;&gt;results gathering&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Option 2] Single-GPU inference&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;(a) Generate LLaVA responses on ScienceQA dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.eval.model_vqa_science \&#xA;    --model-name /path/to/LLaVA-13b-v0-science_qa \&#xA;    --question-file /path/to/ScienceQA/data/scienceqa/llava_test.json \&#xA;    --image-folder /path/to/ScienceQA/data/scienceqa/images/test \&#xA;    --answers-file vqa/results/ScienceQA/test_llava-13b.jsonl \&#xA;    --answer-prompter&#xA;    --conv-mode simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(b) Evaluate the generated responses&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python eval_science_qa.py \&#xA;    --base-dir /path/to/ScienceQA/data/scienceqa \&#xA;    --result-file vqa/results/ScienceQA/test_llava-13b.jsonl \&#xA;    --output-file vqa/results/ScienceQA/test_llava-13b_output.json \&#xA;    --output-result vqa/results/ScienceQA/test_llava-13b_result.json \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For reference, we attach our prediction file &lt;code&gt;test_llava-13b_result.json&lt;/code&gt; &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/llava/eval/table/results/test_sqa_llava_13b_v0.json&#34;&gt;here&lt;/a&gt; for comparison when reproducing our results, as well as for further analysis in detail.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;The current version of LLaVA is fine-tuned from a Vicuna-13B model. We use approximately 600K filtered CC3M in feature alignment pretraining and 150K GPT-generated multimodal instruction-following data in finetuning. For detailed description of the data generation pipeline, please refer see our &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are working on a more capable model that is pretrained with the data at a larger scale. Stay tuned!&lt;/p&gt; &#xA;&lt;p&gt;We release all three types of multimodal instruction-following data. The use of these data is subject to OpenAI &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;TOS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Code and Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tune the model using the code from &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. We use a similar set of hyperparameters as Vicuna in finetuning. Both hyperparameters used in pretraining and finetuning are provided below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fine-tuning with Local GPUs&lt;/h3&gt; &#xA;&lt;p&gt;LLaVA is trained on 8 A100 GPUs with 80GB memory with the following code. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly to keep the global batch size the same.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-13B, 8x A100 (80G). Time: ~4 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You may run this with a single A100 GPU with the following code. Please note that the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; * &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; should be equal to 128 to keep the global batch size the same.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-13B, 1x A100 (80G). Time: ~33 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-7B, 1x A100 (80G/40G). Time: ~19 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-7b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-7b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Experimental: use FSDP to save memory in pretraining&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Learn more&lt;/summary&gt; &#xA; &lt;p&gt;Currently, PyTorch and Huggingface does not yet have stable/native support for FSDP on parameter efficient tuning (part of the parameters are frozen). However, the feature is being developed in PyTorch nightly and shall be shipped in the next release. We provide an experimental script to enable FSDP in pretraining. To use it, please &lt;strong&gt;create a new enviroment&lt;/strong&gt; (to be safe), install PyTorch nightly (&lt;strong&gt;MUST&lt;/strong&gt;), and &lt;code&gt;LLaVA&lt;/code&gt; package following the instructions below.&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Prepare environment&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava_beta python=3.10 -y&#xA;conda activate llava_beta&#xA;pip install --upgrade pip&#xA;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu117&#xA;pip install -e .&#xA;pip install einops ninja&#xA;pip install flash-attn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Run pretraining with FSDP (experimental)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain_fsdp \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Extract projector features&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python scripts/extract_mm_projector.py \&#xA;  --model_name_or_path ./checkpoints/llava-13b-pretrain \&#xA;  --output ./checkpoints/mm_projector/llava-13b-pretrain.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path /path/to/llama-vicuna-13b \&#xA;    --data_path /path/to/llava_instruct_150k.json \&#xA;    --image_folder /Data/haotian/coco/train2014 \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain.bin \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end True \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 5000 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train LLaVA Lightning&lt;/h3&gt; &#xA;&lt;p&gt;LLaVA-Lightning can be trained on 8x A100 GPUs in just 3 hours, including both pretraining and finetuning. When using spot instances, it costs just ~$40. &lt;em&gt;We are working on &lt;a href=&#34;https://github.com/skypilot-org/skypilot.git&#34;&gt;SkyPilot&lt;/a&gt; tutorial to make spot instance training even easier, stay tuned!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please make sure to: (1) &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;install&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#upgrade-to-v01&#34;&gt;upgrade&lt;/a&gt; to the latest code base, and (2) pass the correct model version identifier &lt;code&gt;v0&lt;/code&gt;/&lt;code&gt;v1&lt;/code&gt; to ensure the correct conversation template is loaded.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;bash ./scripts/train_lightning.sh {v0,v1}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Hyperparameters&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Lightning-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Lightning-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;LLaVA-MPT-7b&lt;/h4&gt; &#xA;&lt;p&gt;Thanks to LLaVA-Lightning, we are able to train a checkpoint based on MPT-7b-Chat on 8x A100 GPUs in just 3 hours, including both pretraining and finetuning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This is a research preview of the LLaVA-Lightning based on MPT-7B-chat checkpoint. The usage of the model should comply with MPT-7B-chat license and agreements.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Unlike other LLaVA models, this model should be used directly without delta weights conversion!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: You need to upgrade to our latest code base to use LLaVA-MPT-7b!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Usage&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You do not need to download our checkpoint, it will directly load from our Hugging Face model: &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;&lt;code&gt;liuhaotian/LLaVA-Lightning-MPT-7B-preview&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview&#xA;python -m llava.serve.gradio_web_server --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We use the same set of training dataset, and the hyperparameters as other Lightning checkpoints.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;bash ./scripts/train_lightning_mpt.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning on ScienceQA&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Due to that ScienceQA experiments were done earlier, the current checkpoints are trained &lt;em&gt;without&lt;/em&gt; &lt;code&gt;&amp;lt;im_start&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;im_end&amp;gt;&lt;/code&gt; tokens. Checkpoints with these tokens will be updated later. Here we provide our training scripts for the current checkpoints.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;1. Pretraining&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain-no_im_start_end_token \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;2. Extract projector features&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python scripts/extract_mm_projector.py \&#xA;  --model_name_or_path ./checkpoints/llava-13b-pretrain-no_im_start_end_token \&#xA;  --output ./checkpoints/mm_projector/llava-13b-pretrain-no_im_start_end_token.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;3. Finetuning&lt;/summary&gt; &#xA; &lt;p&gt;You may download our pretrained &lt;code&gt;llava-13b-pretrain-no_im_start_end_token.bin&lt;/code&gt; &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path /path/to/llama-vicuna-13b \&#xA;    --data_path /path/to/scienceqa/llava_train_QCM-LEPA.json \&#xA;    --image_folder /path/to/scienceqa/images/train \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain-no_im_start_end_token.bin \&#xA;    --mm_vision_select_layer -2 \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain-no_im_start_end_token-finetune_scienceqa \&#xA;    --num_train_epochs 12 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 5000 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you find LLaVA useful for your your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={arXiv:2304.08485},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, pleae check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>JaidedAI/EasyOCR</title>
    <updated>2023-05-20T01:42:38Z</updated>
    <id>tag:github.com,2023-05-20:/JaidedAI/EasyOCR</id>
    <link href="https://github.com/JaidedAI/EasyOCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EasyOCR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/easyocr&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/easyocr.svg?sanitize=true&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.to/easyocr&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/JaidedAI&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ready-to-use OCR with 80+ &lt;a href=&#34;https://www.jaided.ai/easyocr&#34;&gt;supported languages&lt;/a&gt; and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jaided.ai/easyocr&#34;&gt;Try Demo on our website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces 🤗&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/tomofi/EasyOCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s new&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;15 September 2022 - Version 1.6.2&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add CPU support for DBnet&lt;/li&gt; &#xA;   &lt;li&gt;DBnet will only be compiled when users initialize DBnet detector.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;1 September 2022 - Version 1.6.1&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fix DBnet path bug for Windows&lt;/li&gt; &#xA;   &lt;li&gt;Add new built-in model &lt;code&gt;cyrillic_g2&lt;/code&gt;. This model is a new default for Cyrillic script.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;24 August 2022 - Version 1.6.0&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Restructure code to support alternative text detectors.&lt;/li&gt; &#xA;   &lt;li&gt;Add detector &lt;code&gt;DBnet&lt;/code&gt;, see &lt;a href=&#34;https://arxiv.org/abs/2202.10304v1&#34;&gt;paper&lt;/a&gt;. It can be used by initializing like this &lt;code&gt;reader = easyocr.Reader([&#39;en&#39;], detect_network = &#39;dbnet18&#39;)&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2 June 2022 - Version 1.5.0&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add trainer for CRAFT detection model (thanks&lt;a href=&#34;https://github.com/gmuffiness&#34;&gt;@gmuffiness&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/739&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;9 April 2022 - Version 1.4.2&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Update dependencies (opencv and pillow issues)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;11 September 2021 - Version 1.4.1&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add trainer folder&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;readtextlang&lt;/code&gt; method (thanks&lt;a href=&#34;https://github.com/arkya-art&#34;&gt;@arkya-art&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/525&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Extend &lt;code&gt;rotation_info&lt;/code&gt; argument to support all possible angles (thanks&lt;a href=&#34;https://github.com/abde0103&#34;&gt;abde0103&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/515&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;29 June 2021 - Version 1.4&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/custom_model.md&#34;&gt;Instructions&lt;/a&gt; on training/using custom recognition models&lt;/li&gt; &#xA;   &lt;li&gt;Example &lt;a href=&#34;https://www.jaided.ai/easyocr/modelhub&#34;&gt;dataset&lt;/a&gt; for model training&lt;/li&gt; &#xA;   &lt;li&gt;Batched image inference for GPUs (thanks &lt;a href=&#34;https://github.com/SamSamhuns&#34;&gt;@SamSamhuns&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/458&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Vertical text support (thanks &lt;a href=&#34;https://github.com/interactivetech&#34;&gt;@interactivetech&lt;/a&gt;). This is for rotated text, not to be confused with vertical Chinese or Japanese text. (see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/450&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Output in dictionary format (thanks &lt;a href=&#34;https://github.com/A2va&#34;&gt;@A2va&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/441&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;30 May 2021 - Version 1.3.2&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Faster greedy decoder (thanks &lt;a href=&#34;https://github.com/samayala22&#34;&gt;@samayala22&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Fix bug when a text box&#39;s aspect ratio is disproportional (thanks &lt;a href=&#34;https://iquartic.com/&#34;&gt;iQuartic&lt;/a&gt; for bug report)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20 April 2021 - Version 1.3.1&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add support for PIL image (thanks &lt;a href=&#34;https://github.com/prays&#34;&gt;@prays&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Add Tajik language (tjk)&lt;/li&gt; &#xA;   &lt;li&gt;Update argument setting for command line&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;x_ths&lt;/code&gt; and &lt;code&gt;y_ths&lt;/code&gt; to control merging behavior when &lt;code&gt;paragraph=True&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;21 March 2021 - Version 1.3&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Second-generation models: multiple times smaller size, multiple times faster inference, additional characters and comparable accuracy to the first generation models. EasyOCR will choose the latest model by default but you can also specify which model to use by passing &lt;code&gt;recog_network&lt;/code&gt; argument when creating a &lt;code&gt;Reader&lt;/code&gt; instance. For example, &lt;code&gt;reader = easyocr.Reader([&#39;en&#39;,&#39;fr&#39;], recog_network=&#39;latin_g1&#39;)&lt;/code&gt; will use the 1st generation Latin model&lt;/li&gt; &#xA;   &lt;li&gt;List of all models: &lt;a href=&#34;https://www.jaided.ai/easyocr/modelhub&#34;&gt;Model hub&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/releasenotes.md&#34;&gt;Read all release notes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s coming next&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Handwritten text support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example.png&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example2.png&#34; alt=&#34;example2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example3.png&#34; alt=&#34;example3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For the latest stable release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install easyocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the latest development release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/JaidedAI/EasyOCR.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note 1: For Windows, please install torch and torchvision first by following the official instructions here &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt;. On the pytorch website, be sure to select the right CUDA version you have. If you intend to run on CPU mode only, select &lt;code&gt;CUDA = None&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note 2: We also provide a Dockerfile &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/Dockerfile&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import easyocr&#xA;reader = easyocr.Reader([&#39;ch_sim&#39;,&#39;en&#39;]) # this needs to run only once to load the model into memory&#xA;result = reader.readtext(&#39;chinese.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[([[189, 75], [469, 75], [469, 165], [189, 165]], &#39;愚园路&#39;, 0.3754989504814148),&#xA; ([[86, 80], [134, 80], [134, 128], [86, 128]], &#39;西&#39;, 0.40452659130096436),&#xA; ([[517, 81], [565, 81], [565, 123], [517, 123]], &#39;东&#39;, 0.9989598989486694),&#xA; ([[78, 126], [136, 126], [136, 156], [78, 156]], &#39;315&#39;, 0.8125889301300049),&#xA; ([[514, 126], [574, 126], [574, 156], [514, 156]], &#39;309&#39;, 0.4971577227115631),&#xA; ([[226, 170], [414, 170], [414, 220], [226, 220]], &#39;Yuyuan Rd.&#39;, 0.8261902332305908),&#xA; ([[79, 173], [125, 173], [125, 213], [79, 213]], &#39;W&#39;, 0.9848111271858215),&#xA; ([[529, 173], [569, 173], [569, 213], [529, 213]], &#39;E&#39;, 0.8405593633651733)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note 1: &lt;code&gt;[&#39;ch_sim&#39;,&#39;en&#39;]&lt;/code&gt; is the list of languages you want to read. You can pass several languages at once but not all languages can be used together. English is compatible with every language and languages that share common characters are usually compatible with each other.&lt;/p&gt; &#xA;&lt;p&gt;Note 2: Instead of the filepath &lt;code&gt;chinese.jpg&lt;/code&gt;, you can also pass an OpenCV image object (numpy array) or an image file as bytes. A URL to a raw image is also acceptable.&lt;/p&gt; &#xA;&lt;p&gt;Note 3: The line &lt;code&gt;reader = easyocr.Reader([&#39;ch_sim&#39;,&#39;en&#39;])&lt;/code&gt; is for loading a model into memory. It takes some time but it needs to be run only once.&lt;/p&gt; &#xA;&lt;p&gt;You can also set &lt;code&gt;detail=0&lt;/code&gt; for simpler output.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reader.readtext(&#39;chinese.jpg&#39;, detail = 0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Result:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[&#39;愚园路&#39;, &#39;西&#39;, &#39;东&#39;, &#39;315&#39;, &#39;309&#39;, &#39;Yuyuan Rd.&#39;, &#39;W&#39;, &#39;E&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weights for the chosen language will be automatically downloaded or you can download them manually from the &lt;a href=&#34;https://www.jaided.ai/easyocr/modelhub&#34;&gt;model hub&lt;/a&gt; and put them in the &#39;~/.EasyOCR/model&#39; folder&lt;/p&gt; &#xA;&lt;p&gt;In case you do not have a GPU, or your GPU has low memory, you can run the model in CPU-only mode by adding &lt;code&gt;gpu=False&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reader = easyocr.Reader([&#39;ch_sim&#39;,&#39;en&#39;], gpu=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, read the &lt;a href=&#34;https://www.jaided.ai/easyocr/tutorial&#34;&gt;tutorial&lt;/a&gt; and &lt;a href=&#34;https://www.jaided.ai/easyocr/documentation&#34;&gt;API Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Run on command line&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ easyocr -l ch_sim en -f chinese.jpg --detail=1 --gpu=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train/use your own model&lt;/h2&gt; &#xA;&lt;p&gt;For recognition model, &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/custom_model.md&#34;&gt;Read here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detection model (CRAFT), &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/trainer/craft/README.md&#34;&gt;Read here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Implementation Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Handwritten support&lt;/li&gt; &#xA; &lt;li&gt;Restructure code to support swappable detection and recognition algorithms The api should be as easy as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reader = easyocr.Reader([&#39;en&#39;], detection=&#39;DB&#39;, recognition = &#39;Transformer&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The idea is to be able to plug in any state-of-the-art model into EasyOCR. There are a lot of geniuses trying to make better detection/recognition models, but we are not trying to be geniuses here. We just want to make their works quickly accessible to the public ... for free. (well, we believe most geniuses want their work to create a positive impact as fast/big as possible) The pipeline should be something like the below diagram. Grey slots are placeholders for changeable light blue modules.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/easyocr_framework.jpeg&#34; alt=&#34;plan&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement and References&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on research and code from several papers and open-source repositories.&lt;/p&gt; &#xA;&lt;p&gt;All deep learning execution is based on &lt;a href=&#34;https://pytorch.org&#34;&gt;Pytorch&lt;/a&gt;. &lt;span&gt;❤️&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;Detection execution uses the CRAFT algorithm from this &lt;a href=&#34;https://github.com/clovaai/CRAFT-pytorch&#34;&gt;official repository&lt;/a&gt; and their &lt;a href=&#34;https://arxiv.org/abs/1904.01941&#34;&gt;paper&lt;/a&gt; (Thanks @YoungminBaek from &lt;a href=&#34;https://github.com/clovaai&#34;&gt;@clovaai&lt;/a&gt;). We also use their pretrained model. Training script is provided by &lt;a href=&#34;https://github.com/gmuffiness&#34;&gt;@gmuffiness&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The recognition model is a CRNN (&lt;a href=&#34;https://arxiv.org/abs/1507.05717&#34;&gt;paper&lt;/a&gt;). It is composed of 3 main components: feature extraction (we are currently using &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;Resnet&lt;/a&gt;) and VGG, sequence labeling (&lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;LSTM&lt;/a&gt;) and decoding (&lt;a href=&#34;https://www.cs.toronto.edu/~graves/icml_2006.pdf&#34;&gt;CTC&lt;/a&gt;). The training pipeline for recognition execution is a modified version of the &lt;a href=&#34;https://github.com/clovaai/deep-text-recognition-benchmark&#34;&gt;deep-text-recognition-benchmark&lt;/a&gt; framework. (Thanks &lt;a href=&#34;https://github.com/ku21fan&#34;&gt;@ku21fan&lt;/a&gt; from &lt;a href=&#34;https://github.com/clovaai&#34;&gt;@clovaai&lt;/a&gt;) This repository is a gem that deserves more recognition.&lt;/p&gt; &#xA;&lt;p&gt;Beam search code is based on this &lt;a href=&#34;https://github.com/githubharald/CTCDecoder&#34;&gt;repository&lt;/a&gt; and his &lt;a href=&#34;https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&#34;&gt;blog&lt;/a&gt;. (Thanks &lt;a href=&#34;https://github.com/githubharald&#34;&gt;@githubharald&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Data synthesis is based on &lt;a href=&#34;https://github.com/Belval/TextRecognitionDataGenerator&#34;&gt;TextRecognitionDataGenerator&lt;/a&gt;. (Thanks &lt;a href=&#34;https://github.com/Belval&#34;&gt;@Belval&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;And a good read about CTC from distill.pub &lt;a href=&#34;https://distill.pub/2017/ctc/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Want To Contribute?&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s advance humanity together by making AI available to everyone!&lt;/p&gt; &#xA;&lt;p&gt;3 ways to contribute:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coder:&lt;/strong&gt; Please send a PR for small bugs/improvements. For bigger ones, discuss with us by opening an issue first. There is a list of possible bug/improvement issues tagged with &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/issues?q=is%3Aissue+is%3Aopen+label%3A%22PR+WELCOME%22&#34;&gt;&#39;PR WELCOME&#39;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; Tell us how EasyOCR benefits you/your organization to encourage further development. Also post failure cases in &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/issues&#34;&gt;Issue Section&lt;/a&gt; to help improve future models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tech leader/Guru:&lt;/strong&gt; If you found this library useful, please spread the word! (See &lt;a href=&#34;https://www.facebook.com/yann.lecun/posts/10157018122787143&#34;&gt;Yann Lecun&#39;s post&lt;/a&gt; about EasyOCR)&lt;/p&gt; &#xA;&lt;h2&gt;Guideline for new language request&lt;/h2&gt; &#xA;&lt;p&gt;To request a new language, we need you to send a PR with the 2 following files:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In folder &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/character&#34;&gt;easyocr/character&lt;/a&gt;, we need &#39;yourlanguagecode_char.txt&#39; that contains list of all characters. Please see format examples from other files in that folder.&lt;/li&gt; &#xA; &lt;li&gt;In folder &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/dict&#34;&gt;easyocr/dict&lt;/a&gt;, we need &#39;yourlanguagecode.txt&#39; that contains list of words in your language. On average, we have ~30000 words per language with more than 50000 words for more popular ones. More is better in this file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If your language has unique elements (such as 1. Arabic: characters change form when attached to each other + write from right to left 2. Thai: Some characters need to be above the line and some below), please educate us to the best of your ability and/or give useful links. It is important to take care of the detail to achieve a system that really works.&lt;/p&gt; &#xA;&lt;p&gt;Lastly, please understand that our priority will have to go to popular languages or sets of languages that share large portions of their characters with each other (also tell us if this is the case for your language). It takes us at least a week to develop a new model, so you may have to wait a while for the new model to be released.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/issues/91&#34;&gt;List of languages in development&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Github Issues&lt;/h2&gt; &#xA;&lt;p&gt;Due to limited resources, an issue older than 6 months will be automatically closed. Please open an issue again if it is critical.&lt;/p&gt; &#xA;&lt;h2&gt;Business Inquiries&lt;/h2&gt; &#xA;&lt;p&gt;For Enterprise Support, &lt;a href=&#34;https://www.jaided.ai/&#34;&gt;Jaided AI&lt;/a&gt; offers full service for custom OCR/AI systems from implementation, training/finetuning and deployment. Click &lt;a href=&#34;https://www.jaided.ai/contactus?ref=github&#34;&gt;here&lt;/a&gt; to contact us.&lt;/p&gt;</summary>
  </entry>
</feed>