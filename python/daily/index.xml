<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-30T01:36:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zejun-Yang/AniPortrait</title>
    <updated>2024-03-30T01:36:28Z</updated>
    <id>tag:github.com,2024-03-30:/Zejun-Yang/AniPortrait</id>
    <link href="https://github.com/Zejun-Yang/AniPortrait" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AniPortrait&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animations&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Author: Huawei Wei, Zejun Yang, Zhisheng Wang&lt;/p&gt; &#xA;&lt;p&gt;Organization: Tencent Games Zhiji, Tencent&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zejun-Yang/AniPortrait/main/asset/zhiji_logo.png&#34; alt=&#34;zhiji_logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. You can also provide a video to achieve face reenacment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.17694&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zejun-Yang/AniPortrait/main/asset/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Now our paper is available on arXiv.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Update the code to generate pose_temp.npy for head pose control.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;We will release audio2pose pre-trained weight for audio2video after futher optimization. You can choose head pose template in &lt;code&gt;./configs/inference/head_pose_temp&lt;/code&gt; as substitution.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Various Generated Videos&lt;/h2&gt; &#xA;&lt;h3&gt;Self driven&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/82c0f0b0-9c7c-4aad-bf0e-27e6098ffbe1&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/51a502d9-1ce2-48d2-afbe-767a0b9b9166&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Face reenacment&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/849fce22-0db1-4257-a75f-a5dc655e6b9e&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/d4e0add6-20a2-4f4b-808c-530a6f4d3331&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Audio driven&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/63171e5a-e4c1-4383-8f20-9764524928d0&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/6fd74024-ba19-4f6b-b37a-10df5cf2c934&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/9e516cc5-bf09-4d45-b5e3-820030764982&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/7c68148b-8022-453f-be9a-c69590038197&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Build environment&lt;/h3&gt; &#xA;&lt;p&gt;We recommend a python version &amp;gt;=3.10 and cuda version =11.7. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;All the weights should be placed under the &lt;code&gt;./pretrained_weights&lt;/code&gt; direcotry. You can download weights manually as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/ZJYang/AniPortrait/tree/main&#34;&gt;weights&lt;/a&gt;, which include four parts: &lt;code&gt;denoising_unet.pth&lt;/code&gt;, &lt;code&gt;reference_unet.pth&lt;/code&gt;, &lt;code&gt;pose_guider.pth&lt;/code&gt;, &lt;code&gt;motion_module.pth&lt;/code&gt; and &lt;code&gt;audio2mesh.pt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained weight of based models and other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion V1.5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/wav2vec2-base-960h&#34;&gt;wav2vec2-base-960h&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be orgnized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;./pretrained_weights/&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   `-- pytorch_model.bin&#xA;|-- sd-vae-ft-mse&#xA;|   |-- config.json&#xA;|   |-- diffusion_pytorch_model.bin&#xA;|   `-- diffusion_pytorch_model.safetensors&#xA;|-- stable-diffusion-v1-5&#xA;|   |-- feature_extractor&#xA;|   |   `-- preprocessor_config.json&#xA;|   |-- model_index.json&#xA;|   |-- unet&#xA;|   |   |-- config.json&#xA;|   |   `-- diffusion_pytorch_model.bin&#xA;|   `-- v1-inference.yaml&#xA;|-- wav2vec2-base-960h&#xA;|   |-- config.json&#xA;|   |-- feature_extractor_config.json&#xA;|   |-- preprocessor_config.json&#xA;|   |-- pytorch_model.bin&#xA;|   |-- README.md&#xA;|   |-- special_tokens_map.json&#xA;|   |-- tokenizer_config.json&#xA;|   `-- vocab.json&#xA;|-- audio2mesh.pt&#xA;|-- denoising_unet.pth&#xA;|-- motion_module.pth&#xA;|-- pose_guider.pth&#xA;`-- reference_unet.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: If you have installed some of the pretrained models, such as &lt;code&gt;StableDiffusion V1.5&lt;/code&gt;, you can specify their paths in the config file (e.g. &lt;code&gt;./config/prompts/animation.yaml&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Here are the cli commands for running inference scripts:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kindly note that you can set -L to the desired number of generating frames in the command, for example, -L 300.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Self driven&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.pose2vid --config ./configs/prompts/animation.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can refer the format of animation.yaml to add your own reference images or pose videos. To convert the raw video into a pose video (keypoint sequence), you can run with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.vid2pose --video_path pose_video_path.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Face reenacment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.vid2vid --config ./configs/prompts/animation_facereenac.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add source face videos and reference images in the animation_facereenac.yaml.&lt;/p&gt; &#xA;&lt;h3&gt;Audio driven&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.audio2vid --config ./configs/prompts/animation_audio.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add audios and reference images in the animation_audio.yaml.&lt;/p&gt; &#xA;&lt;p&gt;You can use this command to generate a pose_temp.npy for head pose control:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.generate_ref_pose --ref_video ./configs/inference/head_pose_temp/pose_ref_video.mp4 --save_path ./configs/inference/head_pose_temp/pose.npy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Data preparation&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://liangbinxie.github.io/projects/vfhq/&#34;&gt;VFHQ&lt;/a&gt; and &lt;a href=&#34;https://github.com/CelebV-HQ/CelebV-HQ&#34;&gt;CelebV-HQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Extract keypoints from raw videos and write training json file (here is an example of processing VFHQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.preprocess_dataset --input_dir VFHQ_PATH --output_dir SAVE_PATH --training_json JSON_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update lines in the training config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;data:&#xA;  json_path: JSON_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage1&lt;/h3&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_1.py --config ./configs/train/stage1.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage2&lt;/h3&gt; &#xA;&lt;p&gt;Put the pretrained motion module weights &lt;code&gt;mm_sd_v15_v2.ckpt&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/guoyww/animatediff/blob/main/mm_sd_v15_v2.ckpt&#34;&gt;download link&lt;/a&gt;) under &lt;code&gt;./pretrained_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Specify the stage1 training weights in the config file &lt;code&gt;stage2.yaml&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;stage1_ckpt_dir: &#39;./exp_output/stage1&#39;&#xA;stage1_ckpt_step: 30000 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_2.py --config ./configs/train/stage2.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We first thank the authors of &lt;a href=&#34;https://github.com/HumanAIGC/EMO&#34;&gt;EMO&lt;/a&gt;, and part of the images and audios in our demos are from EMO. Additionally, we would like to thank the contributors to the &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt;, &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;majic-animate&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;animatediff&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoqincode/Open-AnimateAnyone&#34;&gt;Open-AnimateAnyone&lt;/a&gt; repositories, for their open research and exploration.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wei2024aniportrait,&#xA;      title={AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animations}, &#xA;      author={Huawei Wei and Zejun Yang and Zhisheng Wang},&#xA;      year={2024},&#xA;      eprint={2403.17694},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>NanmiCoder/CrawlerTutorial</title>
    <updated>2024-03-30T01:36:28Z</updated>
    <id>tag:github.com,2024-03-30:/NanmiCoder/CrawlerTutorial</id>
    <link href="https://github.com/NanmiCoder/CrawlerTutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;爬虫入门、爬虫进阶、高级爬虫&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;关于作者&lt;/h2&gt; &#xA;&lt;p&gt;大家好，我是程序员阿江-Relakkes，近期我会给大家出一些爬虫方面的教程，爬虫入门、进阶、高级都有，有需要的朋友，star仓库并持续关注本仓库的更新。&lt;/p&gt; &#xA;&lt;h3&gt;基本信息&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NanmiCoder/MediaCrawler&#34;&gt;Github万星爬虫仓库作者&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;全栈程序员，熟悉Python、Golang、JavaScript，工作中主要用Golang。&lt;/li&gt; &#xA; &lt;li&gt;曾经主导并参与过百万级爬虫采集系统架构设计与编码&lt;/li&gt; &#xA; &lt;li&gt;爬虫是一种技术兴趣爱好，参与爬虫有一种对抗的感觉，越难越兴奋。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;视频教程&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;自媒体账号名： 程序员阿江-Relakkes&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;B站：&lt;a href=&#34;https://space.bilibili.com/434377496&#34;&gt;https://space.bilibili.com/434377496&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;小红书：&lt;a href=&#34;https://www.xiaohongshu.com/user/profile/5f58bd990000000001003753&#34;&gt;https://www.xiaohongshu.com/user/profile/5f58bd990000000001003753&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;抖音：&lt;a href=&#34;https://www.douyin.com/user/MS4wLjABAAAATJPY7LAlaa5X-c8uNdWkvz0jUGgpw4eeXIwu_8BhvqE&#34;&gt;https://www.douyin.com/user/MS4wLjABAAAATJPY7LAlaa5X-c8uNdWkvz0jUGgpw4eeXIwu_8BhvqE&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;怎么联系我？&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Email：&lt;a href=&#34;mailto:relakkes@gmail.com&#34;&gt;relakkes@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Wechat：yzglan&lt;/li&gt; &#xA; &lt;li&gt;QQ: 524134442&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;爬虫入门&lt;/h2&gt; &#xA;&lt;h3&gt;爬虫入门教程目录大纲&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/01_%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%86%99%E8%BF%99%E4%B8%AA%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B.md&#34;&gt;01_为什么要写这个爬虫教程&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/02_%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%BC%9A%E7%88%AC%E8%99%AB%E8%83%BD%E8%B5%9A%E9%92%B1%E5%90%97.md&#34;&gt;02_个人学会爬虫能赚钱吗&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/03_%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88.md&#34;&gt;03_网络爬虫到底是什么&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/04_%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.md&#34;&gt;04_爬虫的基本工作原理&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/05_%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E6%9C%89%E9%82%A3%E4%BA%9B.md&#34;&gt;05_常用的抓包工具有那些&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/06_%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E7%94%A8Python%E5%86%99%E7%88%AC%E8%99%AB%E6%9C%89%E5%A4%A9%E7%94%9F%E4%BC%98%E5%8A%BF.md&#34;&gt;06_为什么说用Python写爬虫有天生优势&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/07_Python%E5%B8%B8%E8%A7%81%E7%9A%84%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E5%BA%93.md&#34;&gt;07_Python常见的网络请求库&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/08_%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%981_%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96.md&#34;&gt;08_爬虫入门实战1_静态网页数据提取&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/09_%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%982_%E5%8A%A8%E6%80%81%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96.md&#34;&gt;09_爬虫入门实战2_动态数据提取&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/10_%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%983_%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%AE%9E%E7%8E%B0.md&#34;&gt;10_爬虫入门实战3_数据存储实现&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/11_%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%984_%E9%AB%98%E6%95%88%E7%8E%87%E7%9A%84%E7%88%AC%E8%99%AB%E5%AE%9E%E7%8E%B0.md&#34;&gt;11_爬虫入门实战4_高效率的爬虫实现&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NanmiCoder/CrawlerTutorial/main/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/12_%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%985_%E7%BC%96%E5%86%99%E6%98%93%E4%BA%8E%E7%BB%B4%E6%8A%A4%E7%9A%84%E7%88%AC%E8%99%AB%E4%BB%A3%E7%A0%81.md&#34;&gt;12_爬虫入门实战5_编写易于维护的爬虫代码&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;爬虫入门教程源代码&lt;/h3&gt; &#xA;&lt;h2&gt;爬虫进阶&lt;/h2&gt; &#xA;&lt;h3&gt;爬虫进阶教程目录大纲&lt;/h3&gt; &#xA;&lt;p&gt;todo&lt;/p&gt; &#xA;&lt;h3&gt;爬虫进阶教程源代码&lt;/h3&gt; &#xA;&lt;p&gt;todo&lt;/p&gt; &#xA;&lt;h2&gt;高级爬虫&lt;/h2&gt; &#xA;&lt;h3&gt;高级爬虫教程目录大纲&lt;/h3&gt; &#xA;&lt;p&gt;todo&lt;/p&gt; &#xA;&lt;h3&gt;高级爬虫教程源代码&lt;/h3&gt; &#xA;&lt;p&gt;todo&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;本仓库的所有内容仅供学习和参考之用，禁止用于商业用途。任何人或组织不得将本仓库的内容用于非法用途或侵犯他人合法权益。本仓库所涉及的爬虫技术仅用于学习和研究，不得用于对其他平台进行大规模爬虫或其他非法行为。对于因使用本仓库内容而引起的任何法律责任，本仓库不承担任何责任。使用本仓库的内容即表示您同意本免责声明的所有条款和条件。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#NanmiCoder/CrawlerTutorial&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=NanmiCoder/CrawlerTutorial&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SunoAI-API/Suno-API</title>
    <updated>2024-03-30T01:36:28Z</updated>
    <id>tag:github.com,2024-03-30:/SunoAI-API/Suno-API</id>
    <link href="https://github.com/SunoAI-API/Suno-API" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is an unofficial Suno API based on Python and FastAPI. It currently supports generating songs, lyrics, etc. It comes with a built-in token maintenance and keep-alive feature, so you don&#39;t have to worry about the token expiring.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SunoAI-API/Suno-API/main/README_ZH.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Unofficial Suno API&lt;/h1&gt; &#xA;&lt;p&gt;This is an unofficial Suno API based on Python and FastAPI. It currently supports generating songs, lyrics, etc.&lt;br&gt; It comes with a built-in token maintenance and keep-alive feature, so you don&#39;t have to worry about the token expiring.&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatic token maintenance and keep-alive&lt;/li&gt; &#xA; &lt;li&gt;Fully asynchronous, fast, suitable for later expansion&lt;/li&gt; &#xA; &lt;li&gt;Simple code, easy to maintain, convenient for secondary development&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;h4&gt;Configuration&lt;/h4&gt; &#xA;&lt;p&gt;Edit the &lt;code&gt;.env.example&lt;/code&gt; file, rename to &lt;code&gt;.env&lt;/code&gt; and fill in the session_id and cookie.&lt;/p&gt; &#xA;&lt;p&gt;These are initially obtained from the browser, and will be automatically kept alive later.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SunoAI-API/Suno-API/main/images/cover.png&#34; alt=&#34;cookie&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Run&lt;/h4&gt; &#xA;&lt;p&gt;Install dependencies&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For this part, refer to the FastAPI documentation on your own.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn main:app &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose build &amp;amp;&amp;amp; docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Documentation&lt;/h4&gt; &#xA;&lt;p&gt;After setting up the service, visit /docs&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SunoAI-API/Suno-API/main/images/docs.png&#34; alt=&#34;docs&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Contanct me&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/SunoAI-API/Suno-API/main/images/wechat.jpg&#34; width=&#34;382px&#34; height=&#34;511px&#34;&gt;</summary>
  </entry>
</feed>