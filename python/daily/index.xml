<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-10T01:43:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xverse-ai/XVERSE-13B</title>
    <updated>2023-08-10T01:43:07Z</updated>
    <id>tag:github.com,2023-08-10:/xverse-ai/XVERSE-13B</id>
    <link href="https://github.com/xverse-ai/XVERSE-13B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;XVERSE-13B: A multilingual large language model developed by XVERSE Technology Inc.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; XVERSE-13B &lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/xverse/XVERSE-13B&#34;&gt;🤗 HuggingFace&lt;/a&gt;&amp;nbsp; ｜ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/xverse-ai/XVERSE-13B/main/resources/wechat.png&#34;&gt;💬 微信社区&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;left&#34;&gt; &lt;p&gt; &lt;b&gt;中文&lt;/b&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/xverse-ai/XVERSE-13B/main/README_EN.md&#34;&gt;English&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;h2&gt;模型介绍&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;XVERSE-13B&lt;/strong&gt; 是由深圳元象科技自主研发的支持多语言的大语言模型（Large Language Model），主要特点如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;模型结构&lt;/strong&gt;：XVERSE-13B 使用主流 Decoder-only 的标准 Transformer 网络结构，支持 8K 的上下文长度（Context Length），为同尺寸模型中最长，能满足更长的多轮对话、知识问答与摘要等需求，模型应用场景更广泛。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;训练数据&lt;/strong&gt;：构建了 1.4 万亿 token 的高质量、多样化的数据对模型进行充分训练，包含中、英、俄、西等 40 多种语言，通过精细化设置不同类型数据的采样比例，使得中英两种语言表现优异，也能兼顾其他语言效果。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;分词&lt;/strong&gt;：基于 BPE（Byte-Pair Encoding）算法，使用上百 GB 语料训练了一个词表大小为 100,278 的分词器，能够同时支持多语言，而无需额外扩展词表。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;训练框架&lt;/strong&gt;：自主研发多项关键技术，包括高效算子、显存优化、并行调度策略、数据-计算-通信重叠、平台和框架协同等，让训练效率更高，模型稳定性强，在千卡集群上的峰值算力利用率可达到 58.5%，位居业界前列。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;评测结果&lt;/h2&gt; &#xA;&lt;p&gt;为验证模型的各项能力，我们选取了多个学科综合能力评测集，包括 &lt;a href=&#34;https://arxiv.org/abs/2009.03300&#34;&gt;MMLU&lt;/a&gt;（英文）、 &lt;a href=&#34;https://cevalbenchmark.com/&#34;&gt;C-Eval&lt;/a&gt;（中文）、&lt;a href=&#34;https://arxiv.org/abs/2304.06364&#34;&gt;AGIEval&lt;/a&gt;（中英） 、&lt;a href=&#34;https://github.com/OpenLMLab/GAOKAO-Bench&#34;&gt;GAOKAO-Bench&lt;/a&gt;（中英）、&lt;a href=&#34;https://github.com/ExpressAI/AI-Gaokao&#34;&gt;GAOKAO-English&lt;/a&gt;（英文），评测结果如下：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;模型\数据集&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AGIEval&lt;sup&gt;1&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GAOKAO-Bench&lt;sup&gt;1&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GAOKAO-English&lt;sup&gt;1&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Baichuan-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.6&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.6&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama-1-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.9&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama-2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.8&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;moss-moon-003-base (16B)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.1&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenLLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OPT-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pythia-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ziya-LLaMA-13B-Pretrain-v1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;XVERSE-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;41.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;53.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;66.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;sup&gt;1：只针对其中的单项选择题进行测试，即排除了填空题、开放性问题和多项选择题&lt;/sup&gt;&lt;br&gt; &lt;sup&gt;2：来源于 &lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-13B&#34;&gt;Baichuan-13B&lt;/a&gt; 的汇报结果&lt;/sup&gt;&lt;br&gt; &lt;sup&gt;3：来源于 &lt;a href=&#34;https://cevalbenchmark.com/&#34;&gt;C-Eval&lt;/a&gt; 的汇报结果&lt;/sup&gt;&lt;br&gt; &lt;sup&gt;4：来源于&lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2 论文&lt;/a&gt;的汇报结果&lt;/sup&gt;&lt;/p&gt; &#xA; &lt;p&gt;对于 MMLU ，我们采用作者提供的&lt;a href=&#34;https://github.com/hendrycks/test&#34;&gt;评测工具&lt;/a&gt;，C-Eval、AGIEval、GAOKAO-Bench、GAOKAO-English 与 MMLU 的评测方式相同，且统一采用 &lt;strong&gt;5-shot&lt;/strong&gt; 构造测试样本。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;MMLU 各类别指标&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;模型\类别&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STEM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Social Science&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Humanities&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Others&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Baichuan-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama-1-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama-2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;moss-moon-003-base (16B)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenLLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OPT-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pythia-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ziya-LLaMA-13B-Pretrain-v1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;XVERSE-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;44.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;64.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;62.9&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;C-Eval 各类别指标&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;模型\类别&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STEM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Social Science&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Humanities&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Others&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Baichuan-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama-1-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama-2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;moss-moon-003-base (16B)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenLLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OPT-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pythia-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ziya-LLaMA-13B-Pretrain-v1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;XVERSE-13B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;45.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;66.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;58.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.9&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;使用方法&lt;/h2&gt; &#xA;&lt;h3&gt;环境安装&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;下载本仓库：&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/xverse-ai/XVERSE-13B&#xA;cd XVERSE-13B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;使用 pip 安装依赖：&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Transformers 加载方式&lt;/h3&gt; &#xA;&lt;p&gt;可通过以下代码加载 XVERSE-13B 模型进行推理：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(&#34;xverse/XVERSE-13B&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; model = AutoModelForCausalLM.from_pretrained(&#34;xverse/XVERSE-13B&#34;, trust_remote_code=True).half().cuda()&#xA;&amp;gt;&amp;gt;&amp;gt; model = model.eval()&#xA;&amp;gt;&amp;gt;&amp;gt; inputs = tokenizer(&#39;北京的景点：故宫、天坛、万里长城等。\n深圳的景点：&#39;, return_tensors=&#39;pt&#39;).input_ids&#xA;&amp;gt;&amp;gt;&amp;gt; inputs = inputs.cuda()&#xA;&amp;gt;&amp;gt;&amp;gt; generated_ids = model.generate(inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, repetition_penalty=1.1)&#xA;&amp;gt;&amp;gt;&amp;gt; print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;网页 Demo&lt;/h3&gt; &#xA;&lt;p&gt;可通过以下代码启动一个web server，在浏览器输入访问地址后，可使用 XVERSE-13B 模型进行推理：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python text_generation_demo.py --port=&#39;port&#39; --model_path=&#39;/path/to/model/&#39; --tokenizer_path=&#39;/path/to/tokenizer/&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;局限性与免责申明&lt;/h2&gt; &#xA;&lt;p&gt;XVERSE-13B 与其他所有 LLM 一样，在某些情况下可能会产生不准确、有偏见或其他令人反感的内容。因此，请谨慎使用模型生成的内容，请勿将生成的有害内容进行传播，在部署任何 XVERSE-13B 的应用之前，开发人员应根据其具体应用对模型进行安全测试和调优。&lt;/p&gt; &#xA;&lt;p&gt;我们强烈警告不要将 XVERSE-13B 模型用于制造或传播有害信息，或进行任何可能损害公众、国家、社会安全或违反法规的活动。如果使用 XVERSE-13B 模型产生任何问题，无论是数据安全问题、公共舆论风险，还是模型被误解、滥用、传播或不合规使用所引发的任何风险和问题，我们将不承担任何责任。&lt;/p&gt; &#xA;&lt;h2&gt;模型开源协议&lt;/h2&gt; &#xA;&lt;p&gt;使用本仓库的源码需要遵循 &lt;a href=&#34;https://raw.githubusercontent.com/xverse-ai/XVERSE-13B/main/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt; 开源协议，使用 XVERSE-13B 的模型权重则需要遵循&lt;a href=&#34;https://raw.githubusercontent.com/xverse-ai/XVERSE-13B/main/MODEL_LICENSE.pdf&#34;&gt;模型许可协议&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;XVERSE-13B 模型权重对学术研究&lt;strong&gt;完全开放&lt;/strong&gt;，并且支持&lt;strong&gt;免费商用&lt;/strong&gt;，商用需申请商业使用授权，可以发送邮件到 &lt;a href=&#34;mailto:opensource@xverse.cn&#34;&gt;opensource@xverse.cn&lt;/a&gt; 进行申请。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>keras-team/keras-core</title>
    <updated>2023-08-10T01:43:07Z</updated>
    <id>tag:github.com,2023-08-10:/keras-team/keras-core</id>
    <link href="https://github.com/keras-team/keras-core" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A multi-backend implementation of the Keras API, with support for TensorFlow, JAX, and PyTorch.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Keras Core: A new multi-backend Keras&lt;/h1&gt; &#xA;&lt;p&gt;Keras Core is a new multi-backend implementation of the Keras API, with support for TensorFlow, JAX, and PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; At this time, this package is experimental. It has rough edges and not everything might work as expected. We are currently hard at work improving it.&lt;/p&gt; &#xA;&lt;p&gt;Once ready, this package will become Keras 3.0 and subsume &lt;code&gt;tf.keras&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Local installation&lt;/h2&gt; &#xA;&lt;p&gt;Keras Core is compatible with Linux and MacOS systems. To install a local development version:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run installation command from the root directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python pip_build.py --install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should also install your backend of choice: &lt;code&gt;tensorflow&lt;/code&gt;, &lt;code&gt;jax&lt;/code&gt;, or &lt;code&gt;torch&lt;/code&gt;. Note that &lt;code&gt;tensorflow&lt;/code&gt; is required for using certain Keras Core features: certain preprocessing layers as well as &lt;code&gt;tf.data&lt;/code&gt; pipelines.&lt;/p&gt; &#xA;&lt;h2&gt;Configuring your backend&lt;/h2&gt; &#xA;&lt;p&gt;You can export the environment variable &lt;code&gt;KERAS_BACKEND&lt;/code&gt; or you can edit your local config file at &lt;code&gt;~/.keras/keras.json&lt;/code&gt; to configure your backend. Available backend options are: &lt;code&gt;&#34;tensorflow&#34;&lt;/code&gt;, &lt;code&gt;&#34;jax&#34;&lt;/code&gt;, &lt;code&gt;&#34;torch&#34;&lt;/code&gt;. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export KERAS_BACKEND=&#34;jax&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Colab, you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#34;KERAS_BACKEND&#34;] = &#34;jax&#34;&#xA;&#xA;import keras_core as keras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Backwards compatibility&lt;/h2&gt; &#xA;&lt;p&gt;Keras Core is intended to work as a drop-in replacement for &lt;code&gt;tf.keras&lt;/code&gt; (when using the TensorFlow backend). Just take your existing &lt;code&gt;tf.keras&lt;/code&gt; code, change the &lt;code&gt;keras&lt;/code&gt; imports to &lt;code&gt;keras_core&lt;/code&gt;, make sure that your calls to &lt;code&gt;model.save()&lt;/code&gt; are using the up-to-date &lt;code&gt;.keras&lt;/code&gt; format, and you&#39;re done.&lt;/p&gt; &#xA;&lt;p&gt;If your &lt;code&gt;tf.keras&lt;/code&gt; model does not include custom components, you can start running it on top of JAX or PyTorch immediately.&lt;/p&gt; &#xA;&lt;p&gt;If it does include custom components (e.g. custom layers or a custom &lt;code&gt;train_step()&lt;/code&gt;), it is usually possible to convert it to a backend-agnostic implementation in just a few minutes.&lt;/p&gt; &#xA;&lt;p&gt;In addition, Keras models can consume datasets in any format, regardless of the backend you&#39;re using: you can train your models with your existing &lt;code&gt;tf.data.Dataset&lt;/code&gt; pipelines or PyTorch &lt;code&gt;DataLoaders&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why use Keras Core?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework, e.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.&lt;/li&gt; &#xA;   &lt;li&gt;You can take a Keras model and use it as part of a PyTorch-native &lt;code&gt;Module&lt;/code&gt; or as part of a JAX-native model function.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Make your ML code future-proof by avoiding framework lock-in.&lt;/li&gt; &#xA; &lt;li&gt;As a PyTorch user: get access to power and usability of Keras, at last!&lt;/li&gt; &#xA; &lt;li&gt;As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>vietanhdev/anylabeling</title>
    <updated>2023-08-10T01:43:07Z</updated>
    <id>tag:github.com,2023-08-10:/vietanhdev/anylabeling</id>
    <link href="https://github.com/vietanhdev/anylabeling" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Effortless AI-assisted data labeling with AI support from YOLO, Segment Anything, MobileSAM!!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;AnyLabeling&#34; style=&#34;width: 128px; max-width: 100%; height: auto;&#34; src=&#34;https://user-images.githubusercontent.com/18329471/232250539-2b15b9ee-5593-41d0-ba22-e0442f314cce.png&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;🌟 AnyLabeling 🌟&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;Effortless data labeling with AI support from &lt;b&gt;YOLO&lt;/b&gt; and &lt;b&gt;Segment Anything&lt;/b&gt;!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;AnyLabeling = LabelImg + Labelme + Improved UI + Auto-labeling&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/18329471/234640541-a6a65fbc-d7a5-4ec3-9b65-55305b01a7aa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/anylabeling&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/anylabeling&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/vietanhdev/anylabeling/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/vietanhdev/anylabeling.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/vietanhdev/anylabeling/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/vietanhdev/anylabeling.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/anylabeling/&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/anylabeling&#34; alt=&#34;Pypi Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anylabeling.nrl.ai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Read-Documentation-green&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://youtu.be/5qVJiYNX5Kk&#34;&gt; &lt;img alt=&#34;AnyLabeling&#34; src=&#34;https://raw.githubusercontent.com/vietanhdev/anylabeling/master/assets/screenshot.png&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;strong&gt;Auto Labeling with Segment Anything&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://youtu.be/5qVJiYNX5Kk&#34;&gt; &lt;img style=&#34;width: 800px; margin-left: auto; margin-right: auto; display: block;&#34; alt=&#34;AnyLabeling-SegmentAnything&#34; src=&#34;https://user-images.githubusercontent.com/18329471/236625792-07f01838-3f69-48b0-a12e-30bad27bd921.gif&#34;&gt; &lt;/a&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Youtube Demo:&lt;/strong&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=5qVJiYNX5Kk&#34;&gt;https://www.youtube.com/watch?v=5qVJiYNX5Kk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href=&#34;https://anylabeling.nrl.ai&#34;&gt;https://anylabeling.nrl.ai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Image annotation for polygon, rectangle, circle, line and point.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Auto-labeling with YOLOv5 and Segment Anything.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Text detection, recognition and KIE (Key Information Extraction) labeling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multiple languages availables: English, Vietnamese, Chinese.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;I. Install and run&lt;/h2&gt; &#xA;&lt;h3&gt;1. Download and run executable&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and run newest version from &lt;a href=&#34;https://github.com/vietanhdev/anylabeling/releases&#34;&gt;Releases&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For MacOS: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;After installing, go to Applications folder&lt;/li&gt; &#xA;   &lt;li&gt;Right click on the app and select Open&lt;/li&gt; &#xA;   &lt;li&gt;From the second time, you can open the app normally using Launchpad&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. Install from Pypi&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Requirements: Python &amp;gt;= 3.8, &amp;lt;= 3.10.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Recommended: &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda/Anaconda&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create environment:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n anylabeling python=3.8&#xA;conda activate anylabeling&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;(For macOS only)&lt;/strong&gt; Install PyQt5 using Conda:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c conda-forge pyqt==5.15.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install anylabeling:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install anylabeling # or pip install anylabeling-gpu for GPU support&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start labeling:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;anylabeling&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;II. Development&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate resources:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pyrcc5 -o anylabeling/resources/resources.py anylabeling/resources/resources.qrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run app:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python anylabeling/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;III. Build executable&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install PyInstaller:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements-dev.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash build_executable.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check the outputs in: &lt;code&gt;dist/&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;IV. Contribution&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute to &lt;strong&gt;AnyLabeling&lt;/strong&gt;, please read &lt;a href=&#34;https://anylabeling.nrl.ai/docs/contribution&#34;&gt;Contribution Guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;V. Star history&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#vietanhdev/anylabeling&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=vietanhdev/anylabeling&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VI. References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Labeling UI built with ideas and components from &lt;a href=&#34;https://github.com/heartexlabs/labelImg&#34;&gt;LabelImg&lt;/a&gt;, &lt;a href=&#34;https://github.com/wkentaro/labelme&#34;&gt;LabelMe&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Auto-labeling with &lt;a href=&#34;https://segment-anything.com/&#34;&gt;Segment Anything Models&lt;/a&gt;, &lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;MobileSAM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Auto-labeling with &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;, &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLOv8&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>