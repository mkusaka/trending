<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-30T01:37:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Visualize-ML/Book3_Elements-of-Mathematics</title>
    <updated>2022-11-30T01:37:44Z</updated>
    <id>tag:github.com,2022-11-30:/Visualize-ML/Book3_Elements-of-Mathematics</id>
    <link href="https://github.com/Visualize-ML/Book3_Elements-of-Mathematics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Book_3_《数学要素》 | 鸢尾花书：从加减乘除到机器学习；本册有，583幅图，136个代码文件，其中24个Streamlit App；状态：清华社五审五校中；Github稿件基本稳定，欢迎提意见，会及时修改&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Book3_Elements-of-Mathematics&lt;/h1&gt; &#xA;&lt;h1&gt;BK3 《数学要素》 | 从加减乘除到机器学习&lt;/h1&gt; &#xA;&lt;p&gt;书稿持续更新，注意下载最新版本！ 本PDF文件为作者草稿，发布目的为方便读者在移动终端学习，终稿内容以清华大学出版社纸质出版物为准。 版权归清华大学出版社所有，请勿商用，引用请注明出处。 代码及PDF文件下载：&lt;a href=&#34;https://github.com/Visualize-ML&#34;&gt;https://github.com/Visualize-ML&lt;/a&gt; 本书配套微课视频均发布在B站——生姜DrGinger：&lt;a href=&#34;https://space.bilibili.com/513194466&#34;&gt;https://space.bilibili.com/513194466&lt;/a&gt; 欢迎大家批评指教，本书专属邮箱：&lt;a href=&#34;mailto:jiang.visualize.ml@gmail.com&#34;&gt;jiang.visualize.ml@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/torchscale</title>
    <updated>2022-11-30T01:37:44Z</updated>
    <id>tag:github.com,2022-11-30:/microsoft/torchscale</id>
    <link href="https://github.com/microsoft/torchscale" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Transformers at any scale&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TorchScale - A Library for Transformers at (Any) Scale&lt;/h1&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/microsoft/torchscale/raw/main/LICENSE&#34;&gt;&lt;img alt=&#34;MIT License&#34; src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/torchscale&#34;&gt;&lt;img alt=&#34;MIT License&#34; src=&#34;https://badge.fury.io/py/torchscale.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;TorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively. It has the implementation of fundamental research to improve modeling generality and capability as well as training stability and efficiency of scaling Transformers.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stability - &lt;a href=&#34;https://arxiv.org/abs/2203.00555&#34;&gt;&lt;strong&gt;DeepNet&lt;/strong&gt;&lt;/a&gt;: scaling Transformers to 1,000 Layers and beyond&lt;/li&gt; &#xA; &lt;li&gt;Generality - &lt;a href=&#34;https://arxiv.org/abs/2210.06423&#34;&gt;&lt;strong&gt;Foundation Transformers (Magneto)&lt;/strong&gt;&lt;/a&gt;: towards true general-purpose modeling across tasks and modalities (including language, vision, speech, and multimodal)&lt;/li&gt; &#xA; &lt;li&gt;Efficiency - &lt;a href=&#34;https://arxiv.org/abs/2204.09179&#34;&gt;&lt;strong&gt;X-MoE&lt;/strong&gt;&lt;/a&gt;: scalable &amp;amp; finetunable sparse Mixture-of-Experts (MoE)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;November, 2022: TorchScale 0.1.1 released [&lt;a href=&#34;https://arxiv.org/abs/2211.13184&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pypi.org/project/torchscale/&#34;&gt;PyPI&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torchscale&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can develop it locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/microsoft/torchscale.git&#xA;cd torchscale&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;It takes only several lines of code to create a model with the above fundamental research features enabled. Here is how to quickly obtain a BERT-like encoder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.config import EncoderConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.encoder import Encoder&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; config = EncoderConfig(vocab_size=64000)&#xA;&amp;gt;&amp;gt;&amp;gt; model = Encoder(config)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also support the &lt;code&gt;Decoder&lt;/code&gt; architecture and the &lt;code&gt;EncoderDecoder&lt;/code&gt; architecture:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Creating a decoder model&#xA;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.config import DecoderConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.decoder import Decoder&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; config = DecoderConfig(vocab_size=64000)&#xA;&amp;gt;&amp;gt;&amp;gt; decoder = Decoder(config)&#xA;&amp;gt;&amp;gt;&amp;gt; print(decoder)&#xA;&#xA;# Creating a encoder-decoder model&#xA;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.config import EncoderDecoderConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.encoder_decoder import EncoderDecoder&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; config = EncoderDecoderConfig(vocab_size=64000)&#xA;&amp;gt;&amp;gt;&amp;gt; encdec = EncoderDecoder(config)&#xA;&amp;gt;&amp;gt;&amp;gt; print(encdec)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.00555&#34;&gt;DeepNorm to improve the training stability of Post-LayerNorm Transformers&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;enabled by setting &lt;em&gt;deepnorm=True&lt;/em&gt; in the &lt;code&gt;Config&lt;/code&gt; class.&lt;/li&gt; &#xA;   &lt;li&gt;It adjusts both the residual connection and the initialization method according to the model architecture (i.e., encoder, decoder, or encoder-decoder).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.06423&#34;&gt;SubLN for the model generality and the training stability&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;enabled by &lt;em&gt;subln=True&lt;/em&gt;. This is enabled by default.&lt;/li&gt; &#xA;   &lt;li&gt;It introduces another LayerNorm to each sublayer and adjusts the initialization according to the model architecture.&lt;/li&gt; &#xA;   &lt;li&gt;Note that SubLN and DeepNorm cannot be used in one single model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.09179&#34;&gt;X-MoE: efficient and finetunable sparse MoE modeling&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;enabled by &lt;em&gt;use_xmoe=True&lt;/em&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;It replaces every &lt;em&gt;&#39;moe_freq&#39;&lt;/em&gt; &lt;code&gt;FeedForwardNetwork&lt;/code&gt; layers with the X-MoE layers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.10442&#34;&gt;Multiway architecture for multimodality&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;enabled by &lt;em&gt;multiway=True&lt;/em&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;It provides a pool of Transformer&#39;s parameters used for different modalities.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;Relative position bias&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;enabled by adjusting &lt;em&gt;rel_pos_buckets&lt;/em&gt; and &lt;em&gt;max_rel_pos&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.13184&#34;&gt;SparseClip: improving the gradient clipping for sparse MoE models&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;we provide a &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/torchscale/main/examples/fairseq/utils/sparse_clip.py&#34;&gt;sample code&lt;/a&gt; that can be easily adapted to the FairSeq (or other) repo.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most of the features above can be used by simply passing the corresponding parameters to the config. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.config import EncoderConfig&#xA;&amp;gt;&amp;gt;&amp;gt; from torchscale.architecture.encoder import Encoder&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; config = EncoderConfig(vocab_size=64000, deepnorm=True, multiway=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model = Encoder(config)&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;We have the examples of how to use TorchScale in the following scenarios/tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Language&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/torchscale/main/examples/fairseq/README.md#example-gpt-pretraining&#34;&gt;Decoder/GPT&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/torchscale/main/examples/fairseq/README.md#example-machine-translation&#34;&gt;Encoder-Decoder/Neural Machine Translation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/torchscale/main/examples/fairseq/README.md#example-bert-pretraining&#34;&gt;Encoder/BERT&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Vision&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ViT/BEiT [In progress]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Speech&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Multimodal&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/torchscale/main/torchscale/model/BEiT3.py&#34;&gt;Multiway Transformers/BEiT-3&lt;/a&gt; [In progress]&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We plan to provide more examples regarding different tasks (e.g. vision pretraining and speech recognition) and various deep learning toolkits (e.g. &lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;DeepSpeed&lt;/a&gt; and &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt;). Any comments or PRs are welcome!&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Stability Evaluation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://publicmodel.blob.core.windows.net/torchscale/pic/convergence.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The training curve is smooth by using TorchScale, while the baseline Transformer cannot converge.&lt;/p&gt; &#xA;&lt;h3&gt;Scaling-up Experiments&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://publicmodel.blob.core.windows.net/torchscale/pic/scaling_curve.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;TorchScale supports arbitrary depths and widths, successfully scaling-up the models without pain.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Some implementations in TorchScale are either adapted from or inspired by the &lt;a href=&#34;https://github.com/facebookresearch/fairseq&#34;&gt;FairSeq&lt;/a&gt; repository and the &lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;UniLM&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{torchscale,&#xA;  author    = {Shuming Ma and Hongyu Wang and Shaohan Huang and Wenhui Wang and Zewen Chi and Li Dong and Alon Benhaim and Barun Patra and Vishrav Chaudhary and Xia Song and Furu Wei},&#xA;  title     = {{TorchScale}: {Transformers} at Scale},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2211.13184},&#xA;  year      = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{deepnet,&#xA;  author    = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},&#xA;  title     = {{DeepNet}: Scaling {Transformers} to 1,000 Layers},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2203.00555},&#xA;  year      = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{magneto,&#xA;  author    = {Hongyu Wang and Shuming Ma and Shaohan Huang and Li Dong and Wenhui Wang and Zhiliang Peng and Yu Wu and Payal Bajaj and Saksham Singhal and Alon Benhaim and Barun Patra and Zhun Liu and Vishrav Chaudhary and Xia Song and Furu Wei},&#xA;  title     = {Foundation {Transformers}},&#xA;  journal   = {CoRR},&#xA;  volume    = {abs/2210.06423},&#xA;  year      = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{xmoe,&#xA;  title={On the Representation Collapse of Sparse Mixture of Experts},&#xA;  author={Zewen Chi and Li Dong and Shaohan Huang and Damai Dai and Shuming Ma and Barun Patra and Saksham Singhal and Payal Bajaj and Xia Song and Xian-Ling Mao and Heyan Huang and Furu Wei},&#xA;  booktitle={Advances in Neural Information Processing Systems},&#xA;  year={2022},&#xA;  url={https://openreview.net/forum?id=mWaYC6CZf5}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:fuwei@microsoft.com&#34;&gt;Furu Wei&lt;/a&gt; and &lt;a href=&#34;mailto:shumma@microsoft.com&#34;&gt;Shuming Ma&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jiggy-ai/hn_summary</title>
    <updated>2022-11-30T01:37:44Z</updated>
    <id>tag:github.com,2022-11-30:/jiggy-ai/hn_summary</id>
    <link href="https://github.com/jiggy-ai/hn_summary" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Summarizes top stories from Hacker News using a large language model and post them to a Telegram channel.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jiggy-ai/hn_summary/raw/master/HN_Summary.jpg&#34; alt=&#34;HN Summary Bot avatar&#34; width=&#34;256&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HN Summary&lt;/strong&gt; is an open source bot which sumarizes top stories on Hacker News and publishes the summaries to a Telegram channel.&lt;/p&gt; &#xA;&lt;p&gt;Join the &lt;strong&gt;HN Summary&lt;/strong&gt; channel on Telegram to see the bot in action and enjoy the story summaries: &lt;br&gt; &lt;a href=&#34;https://t.me/hn_summary&#34;&gt;https://t.me/hn_summary&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Flag bad summaries on the telegram channel with 👎 to help mitigate and improve.&lt;/p&gt; &#xA;&lt;p&gt;You can find summaries of the current top Hacker News articles here as well: &lt;br&gt; &lt;a href=&#34;https://news.jiggy.ai&#34;&gt;https://news.jiggy.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Whenever a new story appears on the Hacker News API /topstories.json endpoint, this bot summarizes it (currently using OpenAI GPT-3 text-davinci-003) and sends the Story title, summary, and url to the hn_summary channel on Telegram.&lt;/p&gt; &#xA;&lt;p&gt;The purpose of this project is to help build intuition on the capabilities of the current generation of large language models while surfacing a broader swath of top Hacker News content. It could also serve as a platform for experimentation with other language model capabilites such as semantic search.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Large language models such as GPT-3 are prone to crazy hallucinations and sometimes make things up while writing in a very authoritative tone.&lt;/p&gt; &#xA;&lt;p&gt;The code for extracting text from html is very basic and error prone. (PR&#39;s welcome.) In addition many sites (such as news sites) are either paywalled or make it difficult to extract text. We now attempt to catch this case via prompt engineering but when one does slip through we tend to get fanciful hallucinations based on just the title and FQDN.&lt;/p&gt; &#xA;&lt;p&gt;Links to content types other than PDF and HTML are currently ignored.&lt;/p&gt; &#xA;&lt;p&gt;Text extraction from reddit and twitter and other commercial links are broken and probably produce wildly hallucinated summaries.&lt;/p&gt; &#xA;&lt;p&gt;Telegram messages are limited to 4K. Currently the response is truncated to 4K.&lt;/p&gt; &#xA;&lt;h2&gt;Major Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;The following environment variables are used to inject credentials and other required configuration for the major dependencies:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OPENAI_API_KEY # your OpenAI API key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;PostgresQL&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Database for keeping track of items we have already seen and associated item info.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HNSUM_POSTGRES_HOST # The database FQDN&lt;/li&gt; &#xA; &lt;li&gt;HNSUM_POSTGRES_USER # The database username&lt;/li&gt; &#xA; &lt;li&gt;HNSUM_POSTGRES_PASS # The database password&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Telegram&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HNSUM_TELEGRAM_API_TOKEN # The bot&#39;s telegram API token&lt;/li&gt; &#xA; &lt;li&gt;HNSUM_TELEGRAM_CHANNEL_ID # the telegram chat where the bot will post the summaries&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>