<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-17T01:44:28Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chatanywhere/GPT_API_free</title>
    <updated>2023-12-17T01:44:28Z</updated>
    <id>tag:github.com,2023-12-17:/chatanywhere/GPT_API_free</id>
    <link href="https://github.com/chatanywhere/GPT_API_free" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free ChatGPT API Keyï¼Œå…è´¹ChatGPT APIï¼Œæ”¯æŒGPT4 APIï¼ˆå…è´¹ï¼‰ï¼ŒChatGPTå›½å†…å¯ç”¨å…è´¹è½¬å‘APIï¼Œç›´è¿æ— éœ€ä»£ç†ã€‚å¯ä»¥æ­é…ChatBoxç­‰è½¯ä»¶/æ’ä»¶ä½¿ç”¨ï¼Œæå¤§é™ä½æ¥å£ä½¿ç”¨æˆæœ¬ã€‚å›½å†…å³å¯æ— é™åˆ¶ç•…å¿«èŠå¤©ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/logo.png&#34; alt=&#34;icon&#34; width=&#34;50px&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;GPT-API-free&lt;/h1&gt; &#xA; &lt;p&gt;æ”¯æŒ &lt;strong&gt;GPT-4&lt;/strong&gt; / GPT-3.5-Turbo / GPT-3.5-Turbo-16K / embeddings / DALLÂ·E / whisper / text-davinci&lt;/p&gt; &#xA; &lt;p&gt;å›½å†…åŠ¨æ€åŠ é€Ÿ ç›´è¿æ— éœ€ä»£ç†&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8&#34;&gt;å¿«é€Ÿå¼€å§‹&lt;/a&gt; / &lt;a href=&#34;https://chatanywhere.apifox.cn/&#34;&gt;APIæ–‡æ¡£&lt;/a&gt; / &lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;ç”³è¯·å†…æµ‹å…è´¹Key&lt;/a&gt; / &lt;a href=&#34;https://peiqi.shop/&#34;&gt;æ”¯æŒä»˜è´¹Key&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://qm.qq.com/cgi-bin/qm/qr?k=OFhxu4Z3qI-c-76QJfC2LLXfKGr0g-57&amp;amp;jump_from=webapi&amp;amp;authKey=Kzuf7g4fsE0ZAM7RN+7XvivEANxgDVqDbUs3WI6cB98pt4pFzq/3L8NMiMOy+mo1&#34;&gt;QQç¾¤: 780366686&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;éšç§å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;è¯¥é¡¹ç›®é«˜åº¦é‡è§†éšç§ï¼Œè‡´åŠ›äºä¿æŠ¤å…¶ç”¨æˆ·çš„éšç§ã€‚è¯¥é¡¹ç›®ä¸ä¼šä»¥ä»»ä½•æ–¹å¼æ”¶é›†ã€è®°å½•æˆ–å­˜å‚¨ç”¨æˆ·è¾“å…¥çš„ä»»ä½•æ–‡æœ¬æˆ–ç”± OpenAI æœåŠ¡å™¨è¿”å›çš„ä»»ä½•æ–‡æœ¬ã€‚è¯¥é¡¹ç›®ä¸ä¼šå‘ OpenAI æˆ–ä»»ä½•ç¬¬ä¸‰æ–¹æä¾›æœ‰å…³ API è°ƒç”¨è€…çš„èº«ä»½çš„ä»»ä½•ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº IP åœ°å€å’Œç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä½†OpenAIå®˜æ–¹ä¼šæ ¹æ®å…¶&lt;a href=&#34;https://platform.openai.com/docs/data-usage-policies&#34;&gt;æ•°æ®ä½¿ç”¨æ”¿ç­–&lt;/a&gt;ä¿ç•™ 30 å¤©çš„æ•°æ®ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æ›´æ–°æ—¥å¿—&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´6æœˆ14æ—¥&lt;/strong&gt; é€‚é…GPT-3.5-Turbo-16Kï¼Œå…è´¹keyä¹Ÿæ”¯æŒ16kæ¨¡å‹ï¼›ä»˜è´¹keyè·Ÿéšå®˜æ–¹ä»·æ ¼é™ä½æ”¶è´¹ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´6æœˆ15æ—¥&lt;/strong&gt; é€‚é…0613ç‰ˆæœ¬æ–°å¢çš„functionsã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´6æœˆ18æ—¥&lt;/strong&gt; æ–°å¢å¯¹è¯­éŸ³è½¬æ–‡å­—æ¨¡å‹Whisperæ”¯æŒã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´8æœˆ4æ—¥&lt;/strong&gt; å…è´¹Keyä¸å†æ”¯æŒgpt-3.5-turbo-16kæ¨¡å‹è°ƒç”¨ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´9æœˆ7æ—¥&lt;/strong&gt; chatapi.chatanywhere.cné•œåƒç«™ä¸å†å‘å›½å†…ç”¨æˆ·æä¾›æœåŠ¡ï¼Œä¸å½±å“APIçš„æ­£å¸¸ä½¿ç”¨ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´11æœˆ8æ—¥&lt;/strong&gt; æ”¯æŒ1106ç‰ˆæœ¬å„æ¨¡å‹ï¼Œæ”¯æŒTTSæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´11æœˆ19æ—¥&lt;/strong&gt; æ”¯æŒgpt-4-1106-previewæ¨¡å‹ï¼Œä»·æ ¼ä»…åŸå…ˆgpt-4æ¨¡å‹çš„ä¸‰åˆ†ä¹‹ä¸€åˆ°äºŒåˆ†ä¹‹ä¸€ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;2023å¹´11æœˆ29æ—¥&lt;/strong&gt; å¼€æ”¾å…è´¹APIçš„gpt-4æƒé™ï¼Œæ¯å¤©å¯ä»¥å…è´¹ä½¿ç”¨10æ¬¡ã€‚ï¼ˆä¸ä¿è¯èƒ½é•¿æœŸæä¾›ï¼‰&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç‰¹ç‚¹&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æ”¯æŒModels, Embedding, text-davinci, GPT-3.5-Turbo, GPT-3.5-Turbo-16K(å…è´¹ç‰ˆä¸æ”¯æŒ), &lt;em&gt;&lt;strong&gt;GPT-4&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;DALLE&lt;/strong&gt;&lt;/em&gt;(å…è´¹ç‰ˆä¸æ”¯æŒ), &lt;em&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/em&gt;(å…è´¹ç‰ˆä¸æ”¯æŒ)ã€‚ï¼ˆå…è´¹ç‰ˆå°±å¯ä»¥æ”¯æŒAutoGPT, gpt_academic, langchainç­‰ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;å…è´¹ç‰ˆæ”¯æŒGPT-4ï¼Œä¸€å¤©10æ¬¡ã€‚ï¼ˆå…è´¹ç‰ˆgpt-4ç›¸å¯¹æ…¢ä¸€äº›ï¼Œä»˜è´¹ç‰ˆæ›´ç¨³å®šï¼‰&lt;/li&gt; &#xA; &lt;li&gt;ä¸å®˜æ–¹å®Œå…¨ä¸€è‡´çš„æ¥å£æ ‡å‡†ï¼Œå…¼å®¹å„ç§è½¯ä»¶/æ’ä»¶ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ”¯æŒæµå¼å“åº”ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å›½å†…çº¿è·¯ä½¿ç”¨åŠ¨æ€åŠ é€Ÿï¼Œä½“éªŒè¿œä¼˜äºä½¿ç”¨ä»£ç†è¿æ¥å®˜æ–¹ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ— éœ€ç§‘å­¦ä¸Šç½‘ï¼Œå›½å†…ç¯å¢ƒç›´æ¥å¯ç”¨ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä¸ªäººå®Œå…¨å…è´¹ä½¿ç”¨ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸš©æ³¨æ„äº‹é¡¹&lt;/h2&gt; &#xA;&lt;p&gt;â—ï¸&lt;em&gt;è¿‘æœŸOpenAIé¢‘ç¹å‡ºé”™ï¼Œå¦‚æœé‡åˆ°æ— å›å¤ï¼ŒæŠ¥é”™ç­‰æƒ…å†µï¼Œå¯ä»¥æŸ¥çœ‹ status.openai.com ï¼Œå¾ˆå¤§å¯èƒ½æ˜¯OpenAIå®˜æ–¹æœåŠ¡é—®é¢˜ã€‚&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;â—ï¸&lt;strong&gt;å…è´¹API Keyä»…å¯ç”¨äºä¸ªäººéå•†ä¸šç”¨é€”ï¼Œæ•™è‚²ï¼Œéè¥åˆ©æ€§ç§‘ç ”å·¥ä½œä¸­ã€‚ä¸¥ç¦å•†ç”¨ï¼Œä¸¥ç¦å¤§è§„æ¨¡è®­ç»ƒå•†ç”¨æ¨¡å‹ï¼è®­ç»ƒç§‘ç ”ç”¨æ¨¡å‹è¯·æå‰åŠ ç¾¤è”ç³»æˆ‘ä»¬ã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;â—ï¸æˆ‘ä»¬å°†ä¸å®šæœŸå¯¹è¢«æ»¥ç”¨çš„Keyè¿›è¡Œå°ç¦ï¼Œå¦‚å‘ç°è‡ªå·±çš„keyè¢«è¯¯å°è¯·é€šè¿‡QQç¾¤è”ç³»æˆ‘ä»¬ã€‚&lt;/p&gt; &#xA;&lt;p&gt;â—ï¸æˆ‘ä»¬çš„ç³»ç»Ÿä»…ä¾›å†…éƒ¨è¯„ä¼°æµ‹è¯•ä½¿ç”¨ï¼Œå•†ç”¨æˆ–é¢å‘å¤§ä¼—ä½¿ç”¨è¯·è‡ªè¡Œæ‰¿æ‹…é£é™©ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¸ºäº†è¯¥é¡¹ç›®é•¿ä¹…å‘å±•ï¼Œå…è´¹API Keyé™åˆ¶&lt;strong&gt;60è¯·æ±‚/å°æ—¶/IP&amp;amp;Key&lt;/strong&gt;è°ƒç”¨é¢‘ç‡ï¼Œä¹Ÿå°±æ˜¯è¯´ä½ å¦‚æœåœ¨ä¸€ä¸ªIPä¸‹ä½¿ç”¨å¤šä¸ªKeyï¼Œæ‰€æœ‰Keyçš„æ¯å°æ—¶è¯·æ±‚æ•°æ€»å’Œä¸èƒ½è¶…è¿‡60ï¼›åŒç†ï¼Œä½ å¦‚æœå°†ä¸€ä¸ªKeyç”¨äºå¤šä¸ªIPï¼Œè¿™ä¸ªKeyçš„æ¯å°æ—¶è¯·æ±‚æ•°ä¹Ÿä¸èƒ½è¶…è¿‡60ã€‚(&lt;strong&gt;ä»˜è´¹ç‰ˆAPIæ²¡æœ‰è¿™ä¸ªé™åˆ¶&lt;/strong&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;å…è´¹ä½¿ç”¨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ğŸš€&lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;ç”³è¯·é¢†å–å†…æµ‹å…è´¹API Key&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è½¬å‘Host1: &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; (å›½å†…ä¸­è½¬ï¼Œå»¶æ—¶æ›´ä½ï¼Œhost1å’Œhost2äºŒé€‰ä¸€)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è½¬å‘Host2: &lt;code&gt;https://api.chatanywhere.com.cn&lt;/code&gt; (å›½å†…ä¸­è½¬ï¼Œå»¶æ—¶æ›´ä½ï¼Œhost1å’Œhost2äºŒé€‰ä¸€)&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è½¬å‘Host3: &lt;code&gt;https://api.chatanywhere.cn&lt;/code&gt; (å›½å¤–ä½¿ç”¨,å›½å†…éœ€è¦å…¨å±€ä»£ç†)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æˆ‘ä»¬ä¼šå®šæœŸæ ¹æ®ä½¿ç”¨é‡è¿›è¡Œç›¸åº”çš„æ‰©å®¹ï¼Œåªè¦ä¸è¢«å®˜æ–¹åˆ¶è£æˆ‘ä»¬ä¼šä¸€ç›´æä¾›å…è´¹APIï¼Œå¦‚æœè¯¥é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¿˜è¯·ä¸ºæˆ‘ä»¬ç‚¹ä¸€ä¸ª&lt;em&gt;&lt;strong&gt;Star&lt;/strong&gt;&lt;/em&gt;ã€‚å¦‚æœé‡åˆ°é—®é¢˜å¯ä»¥åœ¨&lt;a href=&#34;https://github.com/chatanywhere/GPT_API_free/issues&#34;&gt;Issues&lt;/a&gt;ä¸­åé¦ˆï¼Œæœ‰ç©ºä¼šè§£ç­”ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¯¥API Keyç”¨äºè½¬å‘APIï¼Œéœ€è¦å°†Hostæ”¹ä¸º&lt;code&gt;api.chatanywhere.tech&lt;/code&gt;(å›½å†…é¦–é€‰)æˆ–è€…&lt;code&gt;api.chatanywhere.cn&lt;/code&gt;(å›½å¤–ä½¿ç”¨ï¼Œå›½å†…éœ€è¦å…¨å±€ä»£ç†)ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;ä»˜è´¹ç‰ˆAPI&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;çº¯å…¬ç›Šæä¾›å…è´¹Keyæ˜¾ç„¶ä¸æ˜¯èƒ½æŒä¹…è¿è¥ä¸‹å»çš„æ–¹æ¡ˆï¼Œæ‰€ä»¥æˆ‘ä»¬å¼•å…¥ä»˜è´¹API Keyç»´æŒé¡¹ç›®çš„æ—¥å¸¸å¼€é”€ï¼Œä»¥ä¿ƒè¿›é¡¹ç›®çš„è‰¯æ€§å¾ªç¯ï¼Œè¿˜æœ›å¤§å®¶ç†è§£ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://peiqi.shop/&#34;&gt;è´­ä¹°ä½ä»·ä»˜è´¹Key&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ”¯æŒGPT4 API&lt;/strong&gt;ï¼Œä»·æ ¼ä»…å®˜æ–¹ä»·æ ¼85æŠ˜ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ€§ä»·æ¯”é«˜ï¼Œé™¤äº†GPT4çš„å…¶ä»–æ¨¡å‹ä»·æ ¼ç›¸å½“äºå®˜ç½‘ä»·æ ¼ä¸ƒåˆ†ä¹‹ä¸€ã€‚&lt;/li&gt; &#xA; &lt;li&gt;åŒå®˜ç½‘è®¡è´¹ç­–ç•¥ï¼Œæµå¼é—®ç­”ä½¿ç”¨tiktokenåº“å‡†ç¡®è®¡ç®—Tokensï¼Œéæµå¼é—®ç­”ç›´æ¥ä½¿ç”¨å®˜æ–¹è¿”å›Tokensç”¨é‡è®¡è´¹ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä½™é¢ä¸ä¼šè¿‡æœŸï¼Œæ°¸ä¹…æœ‰æ•ˆã€‚æ ¹æ®ç”¨æˆ·åé¦ˆ30å—é’±ä¸ªäººä¸­åº¦ä½¿ç”¨GPT3.5ä¼°è®¡èƒ½ç”¨ä¸€å¹´ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ‰€æœ‰çš„æ¥å£éƒ½ä¿è¯è½¬å‘è‡ªOpenAIå®˜æ–¹æ¥å£ï¼Œépeoã€plusç­‰ä¸ç¨³å®šæ–¹æ¡ˆï¼Œæ— æ°´åˆ†ï¼Œä¸æºå‡ï¼Œä¿è¯ç¨³å®šæ€§ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;å¦‚ä½•ä½¿ç”¨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç”±äºé¢‘ç¹çš„æ¶æ„è¯·æ±‚ï¼Œæˆ‘ä»¬ä¸å†ç›´æ¥æä¾›å…¬å…±çš„å…è´¹Keyï¼Œç°åœ¨éœ€è¦ä½ ä½¿ç”¨ä½ çš„Githubè´¦å·ç»‘å®šæ¥é¢†å–ä½ è‡ªå·±çš„å…è´¹Keyã€‚&lt;/li&gt; &#xA; &lt;li&gt;ğŸš€&lt;a href=&#34;https://api.chatanywhere.org/v1/oauth/free/github/render&#34;&gt;ç”³è¯·é¢†å–å†…æµ‹å…è´¹API Key&lt;/a&gt; æˆ– &lt;a href=&#34;https://peiqi.shop/&#34;&gt;è´­ä¹°å†…æµ‹ä»˜è´¹API Key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;è½¬å‘Host1: &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; (å›½å†…ä¸­è½¬ï¼Œå»¶æ—¶æ›´ä½ï¼Œhost1å’Œhost2äºŒé€‰ä¸€)&lt;/li&gt; &#xA; &lt;li&gt;è½¬å‘Host2: &lt;code&gt;https://api.chatanywhere.com.cn&lt;/code&gt; (å›½å†…ä¸­è½¬ï¼Œå»¶æ—¶æ›´ä½ï¼Œhost1å’Œhost2äºŒé€‰ä¸€)&lt;/li&gt; &#xA; &lt;li&gt;è½¬å‘Host3: &lt;code&gt;https://api.chatanywhere.cn&lt;/code&gt; (å›½å¤–ä½¿ç”¨,å›½å†…éœ€è¦å…¨å±€ä»£ç†)&lt;/li&gt; &#xA; &lt;li&gt;ä½™é¢å’Œä½¿ç”¨è®°å½•æŸ¥è¯¢ï¼ˆé€šçŸ¥å…¬å‘Šä¹Ÿä¼šå‘åœ¨è¿™é‡Œï¼‰: &lt;a href=&#34;https://api.chatanywhere.tech/&#34;&gt;ä½™é¢æŸ¥è¯¢åŠå…¬å‘Š&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;è½¬å‘APIæ— æ³•ç›´æ¥å‘å®˜æ–¹æ¥å£api.openai.comå‘èµ·è¯·æ±‚ï¼Œéœ€è¦å°†è¯·æ±‚åœ°å€æ”¹ä¸ºapi.chatanywhere.techæ‰å¯ä»¥ä½¿ç”¨ï¼Œå¤§éƒ¨åˆ†æ’ä»¶å’Œè½¯ä»¶éƒ½å¯ä»¥ä¿®æ”¹ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å¸¸è§è½¯ä»¶/æ’ä»¶ä½¿ç”¨æ–¹æ³•&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;python openaiå®˜æ–¹åº“ï¼ˆä½¿ç”¨AutoGPTï¼Œlangchainç­‰ï¼‰&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ç¤ºä¾‹ä»£ç è¯·å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/demo.py&#34;&gt;demo.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;æ–¹æ³•ä¸€&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;openai.api_base = &#34;https://api.chatanywhere.tech/v1&#34;&#xA;# openai.api_base = &#34;https://api.chatanywhere.cn/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;æ–¹æ³•äºŒï¼ˆæ–¹æ³•ä¸€ä¸èµ·ä½œç”¨ç”¨è¿™ä¸ªï¼‰&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¿®æ”¹ç¯å¢ƒå˜é‡OPENAI_API_BASEï¼Œå„ä¸ªç³»ç»Ÿæ€ä¹ˆæ”¹ç¯å¢ƒå˜é‡è¯·è‡ªè¡Œæœç´¢ï¼Œä¿®æ”¹ç¯å¢ƒå˜é‡åä¸èµ·ä½œç”¨è¯·é‡å¯ç³»ç»Ÿã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_BASE=https://api.chatanywhere.tech/v1&#xA;æˆ– OPENAI_API_BASE=https://api.chatanywhere.cn/v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;å¼€æºgpt_academic&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;æ‰¾åˆ°&lt;code&gt;config.py&lt;/code&gt;æ–‡ä»¶ä¸­çš„&lt;code&gt;API_URL_REDIRECT&lt;/code&gt;é…ç½®å¹¶ä¿®æ”¹ä¸ºä»¥ä¸‹å†…å®¹ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;API_URL_REDIRECT = {&#34;https://api.openai.com/v1/chat/completions&#34;: &#34;https://api.chatanywhere.tech/v1/chat/completions&#34;}&#xA;# API_URL_REDIRECT = {&#34;https://api.openai.com/v1/chat/completions&#34;: &#34;https://api.chatanywhere.cn/v1/chat/completions&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;BotGem(AMA)&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ChatGPTæ¡Œé¢åº”ç”¨ï¼Œæ”¯æŒå…¨å¹³å°ï¼Œ&lt;em&gt;&lt;strong&gt;æ”¯æŒgpt-4-vision&lt;/strong&gt;&lt;/em&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¸‹è½½é“¾æ¥ï¼š&lt;a href=&#34;https://bytemyth.com/ama&#34;&gt;https://bytemyth.com/ama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä½¿ç”¨æ–¹æ³•ï¼šä¸‹è½½å®‰è£…ååœ¨è®¾ç½®ä¸­å¦‚å›¾è®¾ç½®ï¼Œå¹¶ç‚¹å‡»æ›´æ–°ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/botgem.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;ChatBox&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;ChatGPTå¼€æºæ¡Œé¢åº”ç”¨ï¼Œæ”¯æŒå…¨éƒ¨æ¡Œé¢å¹³å°ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ä¸‹è½½é“¾æ¥ï¼š&lt;a href=&#34;https://github.com/Bin-Huang/chatbox/releases&#34;&gt;https://github.com/Bin-Huang/chatbox/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä½¿ç”¨æ–¹æ³•ï¼šå¦‚å›¾åœ¨è®¾ç½®ä¸­å¡«å…¥è´­ä¹°çš„å¯†é’¥ï¼Œå¹¶å°†ä»£ç†è®¾ç½®ä¸º&lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt;å³å¯&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/chatbox.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Zoteroæ’ä»¶&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;pdfé˜…è¯»æ’ä»¶zotero-gpt&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸‹è½½é“¾æ¥ï¼š&lt;a href=&#34;https://github.com/MuiseDestiny/zotero-gpt/releases&#34;&gt;https://github.com/MuiseDestiny/zotero-gpt/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å®‰è£…å¥½æ’ä»¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¾ç½®ï¼Œè¿˜æ˜¯ä¸ä¼šå¯ä»¥å»bç«™æœæ•™ç¨‹ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/api https://api.chatanywhere.tech&#xA;&#xA;/secretKey è´­ä¹°çš„è½¬å‘key è®°ä½åˆ«å¿˜è®°å¸¦sk-&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/zotero-gpt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç¿»è¯‘æ’ä»¶zotero-pdf-translate&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸‹è½½é“¾æ¥ï¼š&lt;a href=&#34;https://github.com/windingwind/zotero-pdf-translate/releases&#34;&gt;https://github.com/windingwind/zotero-pdf-translate/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;æ¥å£åœ°å€å¡«å†™: &lt;a href=&#34;https://api.chatanywhere.tech/v1/chat/completions&#34;&gt;https://api.chatanywhere.tech/v1/chat/completions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ä¸ç”¨ç®¡çŠ¶æ€æ˜¯å¦æ˜¾ç¤ºå¯ç”¨ å¡«ä¸Šä¹‹åå°±å¯ä»¥äº†&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/zotero-pdf-translate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;æµè§ˆå™¨æ’ä»¶ChatGPT Sidebar&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;å®˜ç½‘é“¾æ¥ï¼š&lt;a href=&#34;https://chatgpt-sidebar.com/&#34;&gt;https://chatgpt-sidebar.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;å®‰è£…å¥½æ’ä»¶åè¿›å…¥è®¾ç½®é¡µé¢ï¼Œå¦‚å›¾æ‰€ç¤ºä¿®æ”¹è®¾ç½®ï¼Œå°†urlä¿®æ”¹ä¸º &lt;code&gt;https://api.chatanywhere.tech&lt;/code&gt; ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/sidebar.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Jetbrainsæ’ä»¶ChatGPT - Easycode&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/jet1.png&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;å®‰è£…å¥½æ’ä»¶ååœ¨Settings &amp;gt; Tools &amp;gt; OpenAI &amp;gt; GPT 3.5 Turboä¸­å¦‚å›¾æ‰€ç¤ºé…ç½®å¥½æ’ä»¶ï¼Œé‡ç‚¹è¦å°†Server Settings ä¿®æ”¹ä¸º &lt;code&gt;https://api.chatanywhere.tech/v1/chat/completions&lt;/code&gt; ã€‚å¹¶å‹¾é€‰Customize Serverã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/jet2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Raycast æ’ä»¶ ChatGPT&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;åœ¨ Raycast Store ä¸­æ‰¾åˆ° ChatGPT æ’ä»¶ï¼Œå¹¶æŒ‰ç…§æç¤ºå®‰è£…ï¼š &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®‰è£…å®Œæˆååœ¨è¯¥æ’ä»¶é…ç½®ä¸­çš„ &lt;code&gt;API Key&lt;/code&gt; ä¸­å¡«å…¥æˆ‘ä»¬çš„API Keyï¼Œä»¥åŠé€‰ä¸­ &lt;code&gt;Change API Endpoint&lt;/code&gt;ï¼Œå¹¶åœ¨ &lt;code&gt;API Endpoint&lt;/code&gt; ä¸­å¡«å…¥ &lt;code&gt;https://api.chatanywhere.tech/v1&lt;/code&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast2.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸº enjoy it~ &lt;img src=&#34;https://raw.githubusercontent.com/chatanywhere/GPT_API_free/main/images/raycast4.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;APIæŠ¥é”™è¯´æ˜&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Overloadé”™è¯¯&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;å…·ä½“é”™è¯¯ä¿¡æ¯ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;error&#34;: {&#xA;    &#34;message&#34;: &#34;That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID xxxxxxxxxxxx in your message.)&#34;,&#xA;    &#34;type&#34;: &#34;server_error&#34;,&#xA;    &#34;param&#34;: null,&#xA;    &#34;code&#34;: null&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è¯¥é”™è¯¯ç”±äºOpenAIå®˜æ–¹æœåŠ¡å™¨è´Ÿè½½é«˜å¼•èµ·ï¼Œä¸è½¬å‘æœåŠ¡å™¨è´Ÿè½½æ— å…³ã€‚ä¸€èˆ¬ä¸€æ®µæ—¶é—´åæ¢å¤ï¼Œå¯ä»¥ç­‰å‡ ç§’åå†è¯•ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#star-history/star-history&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=chatanywhere/GPT_API_free&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>damo-vilab/i2vgen-xl</title>
    <updated>2023-12-17T01:44:28Z</updated>
    <id>tag:github.com,2023-12-17:/damo-vilab/i2vgen-xl</id>
    <link href="https://github.com/damo-vilab/i2vgen-xl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repo for VGen: a holistic video generation ecosystem for video generation building on diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VGen&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/source/VGen.jpg&#34; alt=&#34;figure1&#34; title=&#34;figure1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;VGen is an open-source video synthesis codebase developed by the Tongyi Lab of Alibaba Group, featuring state-of-the-art video generative models. This repository includes implementations of the following methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://i2vgen-xl.github.io/&#34;&gt;I2VGen-xl: High-quality image-to-video synthesis via cascaded diffusion models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://videocomposer.github.io/&#34;&gt;VideoComposer: Compositional Video Synthesis with Motion Controllability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://higen-t2v.github.io/&#34;&gt;Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;A Recipe for Scaling up Text-to-Video Generation with Text-free Videos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;InstructVideo: Instructing Video Diffusion Models with Human Feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dreamvideo-t2v.github.io/&#34;&gt;DreamVideo: Composing Your Dream Videos with Customized Subject and Motion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.09109&#34;&gt;VideoLCM: Video Latent Consistency Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.06571&#34;&gt;Modelscope text-to-video technical report&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;VGen can produce high-quality videos from the input text, images, desired motion, desired subjects, and even the feedback signals provided. It also offers a variety of commonly used video generation tools such as visualization, sampling, training, inference, join training using images and videos, acceleration, and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://i2vgen-xl.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.04145&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/XUi0y7dxqEQ&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441039979087.mp4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/source/logo.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”¥News!!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the high-efficiency video generation method &lt;a href=&#34;https://arxiv.org/abs/2312.09109&#34;&gt;VideoLCM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the code and model of I2VGen-XL and the ModelScope T2V&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the T2V method &lt;a href=&#34;https://higen-t2v.github.io&#34;&gt;HiGen&lt;/a&gt; and customizing T2V method &lt;a href=&#34;https://dreamvideo-t2v.github.io&#34;&gt;DreamVideo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We write an &lt;a href=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/doc/introduction.pdf&#34;&gt;introduction docment&lt;/a&gt; for VGen and compare I2VGen-XL with SVD.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11]&lt;/strong&gt; We release a high-quality I2VGen-XL model, please refer to the &lt;a href=&#34;https://i2vgen-xl.github.io&#34;&gt;Webpage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the technical papers and webpage of &lt;a href=&#34;https://raw.githubusercontent.com/damo-vilab/i2vgen-xl/main/doc/i2vgen-xl.md&#34;&gt;I2VGen-XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the code and pretrained models that can generate 1280x720 videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release models optimized specifically for the human body and faces&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Updated version can fully maintain the ID and capture large and accurate motions simultaneously&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release other methods and the corresponding models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;p&gt;The main features of VGen are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Expandability, allowing for easy management of your own experiments.&lt;/li&gt; &#xA; &lt;li&gt;Completeness, encompassing all common components for video generation.&lt;/li&gt; &#xA; &lt;li&gt;Excellent performance, featuring powerful pre-trained models in multiple tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n vgen python=3.8&#xA;conda activate vgen&#xA;pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We have provided a &lt;strong&gt;demo dataset&lt;/strong&gt; that includes images and videos, along with their lists in &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please note that the demo images used here are for testing purposes and were not included in the training.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Clone codeb&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/damo-vilab/i2vgen-xl.git&#xA;cd i2vgen-xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started with VGen&lt;/h2&gt; &#xA;&lt;h3&gt;(1) Train your text-to-video model&lt;/h3&gt; &#xA;&lt;p&gt;Executing the following command to enable distributed training is as easy as that.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py --cfg configs/t2v_train.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the &lt;code&gt;t2v_train.yaml&lt;/code&gt; configuration file, you can specify the data, adjust the video-to-image ratio using &lt;code&gt;frame_lens&lt;/code&gt;, and validate your ideas with different Diffusion settings, and so on.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before the training, you can download any of our open-source models for initialization. Our codebase supports custom initialization and &lt;code&gt;grad_scale&lt;/code&gt; settings, all of which are included in the &lt;code&gt;Pretrain&lt;/code&gt; item in yaml file.&lt;/li&gt; &#xA; &lt;li&gt;During the training, you can view the saved models and intermediate inference results in the &lt;code&gt;workspace/experiments/t2v_train&lt;/code&gt;directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After the training is completed, you can perform inference on the model using the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/t2v_infer.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can find the videos you generated in the &lt;code&gt;workspace/experiments/test_img_01&lt;/code&gt; directory. For specific configurations such as data, models, seed, etc., please refer to the &lt;code&gt;t2v_infer.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;!-- &lt;table&gt;&#xA;&lt;center&gt;&#xA;  &lt;tr&gt;&#xA;    &lt;td &gt;&lt;center&gt;&#xA;      &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441754174077.mp4&#34;&gt;&lt;/video&gt;&#x9;&#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;    &lt;td &gt;&lt;center&gt;&#xA;      &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441138824052.mp4&#34;&gt;&lt;/video&gt;&#x9;&#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;&lt;/center&gt;&#xA;&lt;/table&gt;&#xA;&lt;/center&gt; --&gt; &#xA;&lt;center&gt; &#xA;&lt;/center&gt;&#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01Ya2I5I25utrJwJ9Jf_!!6000000007587-2-tps-1280-720.png&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01CrmYaz1zXBetmg3dd_!!6000000006723-2-tps-1280-720.png&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441754174077.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441138824052.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;  &#xA;&lt;h3&gt;(2) Run the I2VGen-XL model&lt;/h3&gt; &#xA;&lt;p&gt;(i) Download model and test data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;!pip install modelscope&#xA;from modelscope.hub.snapshot_download import snapshot_download&#xA;model_dir = snapshot_download(&#39;damo/I2VGen-XL&#39;, cache_dir=&#39;models/&#39;, revision=&#39;v1.0.0&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(ii) Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/i2vgen_xl_infer.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/i2vgen_xl_infer.yaml  test_list_path data/test_list_for_i2vgen.txt test_model models/i2vgen_xl_00854500.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;test_list_path&lt;/code&gt; represents the input image path and its corresponding caption. Please refer to the specific format and suggestions within demo file &lt;code&gt;data/test_list_for_i2vgen.txt&lt;/code&gt;. &lt;code&gt;test_model&lt;/code&gt; is the path for loading the model. In a few minutes, you can retrieve the high-definition video you wish to create from the &lt;code&gt;workspace/experiments/test_list_for_i2vgen&lt;/code&gt; directory. At present, we find that the current model performs inadequately on &lt;strong&gt;anime images&lt;/strong&gt; and &lt;strong&gt;images with a black background&lt;/strong&gt; due to the lack of relevant training data. We are consistently working to optimize it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;Due to the compression of our video quality in GIF format, please click &#39;HERE&#39; below to view the original video.&lt;/span&gt;&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;center&gt; &#xA; &lt;/center&gt;&#xA; &lt;table&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01CCEq7K1ZeLpNQqrWu_!!6000000003219-0-tps-1280-720.jpg&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442125067544.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01hIQcvG1spmQMLqBo0_!!6000000005816-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442125067544.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01ZXY7UN23K8q4oQ3uG_!!6000000007236-2-tps-1280-720.png&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441385957074.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01iaSiiv1aJZURUEY53_!!6000000003309-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441385957074.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01NHpVGl1oat4H54Hjf_!!6000000005242-2-tps-1280-720.png&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442102706767.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;!-- &lt;image muted=&#34;true&#34; height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01DgLj1T240jfpzKoaQ_!!6000000007329-1-tps-1280-704.gif&#34;&gt;&lt;/image&gt;&#x9;&#xA;       --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01DgLj1T240jfpzKoaQ_!!6000000007329-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442102706767.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01odS61s1WW9tXen21S_!!6000000002795-0-tps-1280-720.jpg&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442163934688.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01Jyk1HT28JkZtpAtY6_!!6000000007912-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442163934688.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt;  &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/center&gt; &#xA;&lt;h3&gt;(3) Other methods&lt;/h3&gt; &#xA;&lt;p&gt;In preparation.&lt;/p&gt; &#xA;&lt;h2&gt;Customize your own approach&lt;/h2&gt; &#xA;&lt;p&gt;Our codebase essentially supports all the commonly used components in video generation. You can manage your experiments flexibly by adding corresponding registration classes, including &lt;code&gt;ENGINE, MODEL, DATASETS, EMBEDDER, AUTO_ENCODER, DISTRIBUTION, VISUAL, DIFFUSION, PRETRAIN&lt;/code&gt;, and can be compatible with all our open-source algorithms according to your own needs. If you have any questions, feel free to give us your feedback at any time.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If this repo is useful to you, please cite our corresponding technical paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{2023i2vgenxl,&#xA;  title={I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models},&#xA;  author={Zhang, Shiwei and Wang, Jiayu and Zhang, Yingya and Zhao, Kang and Yuan, Hangjie and Qing, Zhiwu and Wang, Xiang  and Zhao, Deli and Zhou, Jingren},&#xA;  booktitle={arXiv preprint arXiv:2311.04145},&#xA;  year={2023}&#xA;}&#xA;@article{2023videocomposer,&#xA;  title={VideoComposer: Compositional Video Synthesis with Motion Controllability},&#xA;  author={Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu, and Zhang, Yingya, and Shen, Yujun, and Zhao, Deli and Zhou, Jingren},&#xA;  booktitle={arXiv preprint arXiv:2306.02018},&#xA;  year={2023}&#xA;}&#xA;@article{wang2023modelscope,&#xA;  title={Modelscope text-to-video technical report},&#xA;  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},&#xA;  journal={arXiv preprint arXiv:2308.06571},&#xA;  year={2023}&#xA;}&#xA;@article{dreamvideo,&#xA;  title={DreamVideo: Composing Your Dream Videos with Customized Subject and Motion},&#xA;  author={Wei, Yujie and Zhang, Shiwei and Qing, Zhiwu and Yuan, Hangjie and Liu, Zhiheng and Liu, Yu and Zhang, Yingya and Zhou, Jingren and Shan, Hongming},&#xA;  journal={arXiv preprint arXiv:2312.04433},&#xA;  year={2023}&#xA;}&#xA;@article{qing2023higen,&#xA;  title={Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation},&#xA;  author={Qing, Zhiwu and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Wei, Yujie and Zhang, Yingya and Gao, Changxin and Sang, Nong },&#xA;  journal={arXiv preprint arXiv:2312.04483},&#xA;  year={2023}&#xA;}&#xA;@article{wang2023videolcm,&#xA;  title={VideoLCM: Video Latent Consistency Model},&#xA;  author={Wang, Xiang and Zhang, Shiwei and Zhang, Han and Liu, Yu and Zhang, Yingya and Gao, Changxin and Sang, Nong },&#xA;  journal={arXiv preprint arXiv:2312.09109},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our gratitude for the contributions of several previous works to the development of VideoComposer. This includes, but is not limited to &lt;a href=&#34;https://arxiv.org/abs/2302.09778&#34;&gt;Composer&lt;/a&gt;, &lt;a href=&#34;https://modelscope.cn/models/damo/text-to-video-synthesis/summary&#34;&gt;ModelScopeT2V&lt;/a&gt;, &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP&lt;/a&gt;, &lt;a href=&#34;https://m-bain.github.io/webvid-dataset/&#34;&gt;WebVid-10M&lt;/a&gt;, &lt;a href=&#34;https://laion.ai/blog/laion-400-open-dataset/&#34;&gt;LAION-400M&lt;/a&gt;, &lt;a href=&#34;https://github.com/zhuoinoulu/pidinet&#34;&gt;Pidinet&lt;/a&gt; and &lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS&lt;/a&gt;. We are committed to building upon these foundations in a way that respects their original contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This open-source model is trained with using &lt;a href=&#34;https://m-bain.github.io/webvid-dataset/&#34;&gt;WebVid-10M&lt;/a&gt; and &lt;a href=&#34;https://laion.ai/blog/laion-400-open-dataset/&#34;&gt;LAION-400M&lt;/a&gt; datasets and is intended for &lt;strong&gt;RESEARCH/NON-COMMERCIAL USE ONLY&lt;/strong&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/weak-to-strong</title>
    <updated>2023-12-17T01:44:28Z</updated>
    <id>tag:github.com,2023-12-17:/openai/weak-to-strong</id>
    <link href="https://github.com/openai/weak-to-strong" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;STATUS&lt;/strong&gt;: This codebase is not well tested and does not use the exact same settings we used in the paper, but in our experience gives qualitatively similar results when using large model size gaps and multiple seeds. Expected results can be found for two datasets below. We may update the code significantly in the coming week.&lt;/p&gt; &#xA;&lt;h1&gt;Weak-to-strong generalization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/weak-to-strong-setup.png&#34; alt=&#34;Our setup and how it relates to superhuman AI alignment&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project contains code for implementing our &lt;a href=&#34;https://cdn.openai.com/papers/weak-to-strong-generalization.pdf&#34;&gt;paper on weak-to-strong generalization&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary codebase contains a re-implementation of our weak-to-strong learning setup for binary classification tasks. The codebase contains code for fine-tuning pretrained language models, and also training against the labels from another language model. We support various losses described in the paper as well, such as the confidence auxiliary loss.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;vision&lt;/code&gt; directory contains stand-alone code for weak-to-strong in the vision models setting (AlexNet -&amp;gt; DINO on ImageNet).&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;You need to have Python installed on your machine. The project uses &lt;code&gt;pyproject.toml&lt;/code&gt; to manage dependencies. To install the dependencies, you can use a package manager like &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running the Script&lt;/h4&gt; &#xA;&lt;p&gt;The main script of the project is train_weak_to_strong.py. It can be run from the command line using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_weak_to_strong.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script accepts several command-line arguments to customize the training process. Here are some examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_weak_to_strong.py --batch_size 32 --max_ctx 512 --ds_name &#34;sciq&#34; --loss &#34;logconf&#34; --n_docs 1000 --n_test_docs 100 --weak_model_size &#34;gpt2-medium&#34; --strong_model_size &#34;gpt2-large&#34; --seed 42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Expected results&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/amazon_polarity_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/sciq_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/Anthropic-hh-rlhf_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adrien Ecoffet&lt;/li&gt; &#xA; &lt;li&gt;Manas Joglekar&lt;/li&gt; &#xA; &lt;li&gt;Jeffrey Wu&lt;/li&gt; &#xA; &lt;li&gt;Jan Hendrik Kirchner&lt;/li&gt; &#xA; &lt;li&gt;Pavel Izmailov (vision)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE.md file for details.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face for their open-source transformer models&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>