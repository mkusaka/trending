<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-26T01:35:06Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>glucauze/sd-webui-faceswaplab</title>
    <updated>2023-08-26T01:35:06Z</updated>
    <id>tag:github.com,2023-08-26:/glucauze/sd-webui-faceswaplab</id>
    <link href="https://github.com/glucauze/sd-webui-faceswaplab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Extended faceswap extension for StableDiffusion web-ui with multiple faceswaps, inpainting, checkpoints, ....&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FaceSwapLab for a1111/Vlad&lt;/h1&gt; &#xA;&lt;p&gt;V1.2.3 : Breaking change for settings, please read changelog.&lt;/p&gt; &#xA;&lt;p&gt;Please read the documentation here : &lt;a href=&#34;https://glucauze.github.io/sd-webui-faceswaplab/&#34;&gt;https://glucauze.github.io/sd-webui-faceswaplab/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also read the &lt;a href=&#34;https://github.com/glucauze/sd-webui-faceswaplab/discussions/categories/guide-doc&#34;&gt;doc discussion section&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/glucauze/sd-webui-faceswaplab/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for changes in last versions.&lt;/p&gt; &#xA;&lt;p&gt;FaceSwapLab is an extension for Stable Diffusion that simplifies face-swapping. It has evolved from sd-webui-faceswap and some part of sd-webui-roop. However, a substantial amount of the code has been rewritten to improve performance and to better manage masks.&lt;/p&gt; &#xA;&lt;p&gt;Some key features include the ability to reuse faces via checkpoints, multiple face units, batch process images, sort faces based on size or gender, and support for vladmantic. It also provides a face inpainting feature.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/glucauze/sd-webui-faceswaplab/main/docs/assets/images/main_interface.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;While FaceSwapLab is still under development, it has reached a good level of stability. This makes it a reliable tool for those who are interested in face-swapping within the Stable Diffusion environment. As with all projects of this type, it‚Äôs expected to improve and evolve over time.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer and license&lt;/h2&gt; &#xA;&lt;p&gt;In short:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ethical Guideline:&lt;/strong&gt; NSFW is now configurable due to performance issue. Please don&#39;t use this to do harm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; This software is distributed under the terms of the GNU Affero General Public License (AGPL), version 3 or later.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model License:&lt;/strong&gt; This software uses InsightFace&#39;s pre-trained models, which are available for non-commercial research purposes only.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More on this here : &lt;a href=&#34;https://glucauze.github.io/sd-webui-faceswaplab/&#34;&gt;https://glucauze.github.io/sd-webui-faceswaplab/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Known problems (wontfix):&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Older versions of gradio don&#39;t work well with the extension. See this bug : &lt;a href=&#34;https://github.com/glucauze/sd-webui-faceswaplab/issues/5&#34;&gt;https://github.com/glucauze/sd-webui-faceswaplab/issues/5&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Simple&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Put a face in the reference.&lt;/li&gt; &#xA; &lt;li&gt;Select a face number.&lt;/li&gt; &#xA; &lt;li&gt;Select &#34;Enable.&#34;&lt;/li&gt; &#xA; &lt;li&gt;Select &#34;CodeFormer&#34; in &lt;strong&gt;Global Post-Processing&lt;/strong&gt; tab.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Once you&#39;re happy with some results but want to improve, the next steps are to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use advanced settings in face units (which are not as complex as they might seem, it&#39;s basically fine tuning post-processing for each faces).&lt;/li&gt; &#xA; &lt;li&gt;Use pre/post inpainting to tweak the image a bit for more natural results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Better&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Put a face in the reference.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select a face number.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select &#34;Enable.&#34;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;strong&gt;Post-Processing&lt;/strong&gt; accordeon:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Select &#34;CodeFormer&#34;&lt;/li&gt; &#xA;   &lt;li&gt;Select &#34;LDSR&#34; or a faster model &#34;003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN&#34; in upscaler. See &lt;a href=&#34;https://github.com/glucauze/sd-webui-faceswaplab/discussions/29&#34;&gt;here for a list of upscalers&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Use sharpen, color_correction and improved mask&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Disable &#34;CodeFormer&#34; in &lt;strong&gt;Global Post-Processing&lt;/strong&gt; tab (otherwise it will be applied twice)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Don&#39;t hesitate to share config in the &lt;a href=&#34;https://github.com/glucauze/sd-webui-faceswaplab/discussions&#34;&gt;discussion section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Face Unit Concept&lt;/strong&gt;: Similar to controlNet, the program introduces the concept of a face unit. You can configure up to 10 units (3 units are the default setting) in the program settings (sd).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vladmantic and a1111 Support&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inpainting Fixes&lt;/strong&gt; : supports ‚Äúonly masked‚Äù and mask inpainting.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;: The overall performance of the software has been enhanced.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FaceSwapLab Tab&lt;/strong&gt;: providing various tools (build, compare, extract, batch)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FaceSwapLab Settings&lt;/strong&gt;: FaceSwapLab settings are now part of the sd settings. To access them, navigate to the sd settings section.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Face Reuse Via Checkpoints&lt;/strong&gt;: The FaceTools tab now allows creating checkpoints, which facilitate face reuse. When a checkpoint is used, it takes precedence over the reference image, and the reference source image is discarded.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Gender Detection&lt;/strong&gt;: The program can now detect gender based on faces.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Face Combination (Blending)&lt;/strong&gt;: Multiple versions of a face can be combined to enhance the swapping result. This blending happens during checkpoint creation.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve Original Images&lt;/strong&gt;: You can opt to keep original images before the swapping process.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple Face Versions for Replacement&lt;/strong&gt;: The program allows the use of multiple versions of the same face for replacement.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Face Similarity and Filtering&lt;/strong&gt;: You can compare faces against the reference and/or source images.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Face Comparison&lt;/strong&gt;: face comparison feature.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Face Extraction&lt;/strong&gt;: face extraction with or without upscaling.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Improved Post-Processing&lt;/strong&gt;: codeformer, gfpgan, upscaling.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Post Inpainting&lt;/strong&gt;: This feature allows the application of image-to-image inpainting specifically to faces.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upscaled Inswapper&lt;/strong&gt;: The program now includes an upscaled inswapper option, which improves results by incorporating upsampling, sharpness adjustment, and color correction before face is merged to the original image.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;API with typing support&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;See the documentation here : &lt;a href=&#34;https://glucauze.github.io/sd-webui-faceswaplab/&#34;&gt;https://glucauze.github.io/sd-webui-faceswaplab/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/seamless_communication</title>
    <updated>2023-08-26T01:35:06Z</updated>
    <id>tag:github.com,2023-08-26:/facebookresearch/seamless_communication</id>
    <link href="https://github.com/facebookresearch/seamless_communication" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Foundational Models for State-of-the-Art Speech and Text Translation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/seamlessM4T.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;SeamlessM4T&lt;/h1&gt; &#xA;&lt;p&gt;SeamlessM4T is designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text.&lt;/p&gt; &#xA;&lt;p&gt;SeamlessM4T covers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üì• 101 languages for speech input.&lt;/li&gt; &#xA; &lt;li&gt;‚å®Ô∏è 96 Languages for text input/output.&lt;/li&gt; &#xA; &lt;li&gt;üó£Ô∏è 35 languages for speech output.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This unified model enables multiple tasks without relying on multiple separate models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speech-to-speech translation (S2ST)&lt;/li&gt; &#xA; &lt;li&gt;Speech-to-text translation (S2TT)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-speech translation (T2ST)&lt;/li&gt; &#xA; &lt;li&gt;Text-to-text translation (T2TT)&lt;/li&gt; &#xA; &lt;li&gt;Automatic speech recognition (ASR)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/blog/seamless-m4t&#34;&gt;Blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://seamless.metademolab.com/&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/facebook/seamless_m4t&#34;&gt;ü§ó Hugging Face space&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A temporary extra requirement for fairseq2 is &lt;a href=&#34;https://github.com/libsndfile/libsndfile&#34;&gt;libsndfile&lt;/a&gt;. From &lt;a href=&#34;https://docs.conda.io/en/latest/&#34;&gt;Conda&lt;/a&gt; environment it can be installed via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -y -c conda-forge libsndfile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At this point fairseq2 has a confirmed support only for Linux and macOS. Pre-built packages are only available for Linux (macOS is planned).&lt;/p&gt; &#xA;&lt;h2&gt;Running inference&lt;/h2&gt; &#xA;&lt;p&gt;Here‚Äôs an example of using the CLI from the root directory to run inference.&lt;/p&gt; &#xA;&lt;p&gt;S2ST task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;path_to_input_audio&amp;gt; s2st &amp;lt;tgt_lang&amp;gt; --output_path &amp;lt;path_to_save_audio&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;T2TT task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;m4t_predict &amp;lt;input_text&amp;gt; t2tt &amp;lt;tgt_lang&amp;gt; --src_lang &amp;lt;src_lang&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/scripts/m4t/predict&#34;&gt;inference README&lt;/a&gt; for detailed instruction on how to run inference and the list of supported languages on the source, target sides for speech, text modalities.&lt;/p&gt; &#xA;&lt;h1&gt;Libraries&lt;/h1&gt; &#xA;&lt;p&gt;Seamless Communication depends on 3 libraries developed by Meta.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/fairseq2&#34;&gt;fairseq2&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;fairseq2 is our next-generation open-source library of sequence modeling components that provides researchers and developers with building blocks for machine translation, language modeling, and other sequence generation tasks. All SeamlessM4T models in this repository are powered by fairseq2.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR and BLASER 2.0&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;SONAR, Sentence-level multimOdal and laNguage-Agnostic Representations is a new multilingual and -modal sentence embedding space which outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. SONAR provides text and speech encoders for many languages. SeamlessAlign was mined based on SONAR embeddings.&lt;/p&gt; &#xA;&lt;p&gt;BLASER 2.0 is our latest model-based evaluation metric for multimodal translation. It is an extension of BLASER, supporting both speech and text. It operates directly on the source signal, and as such, does not require any intermediate ASR system like ASR-BLEU. As in the first version, BLASER 2.0 leverages the similarity between input and output sentence embeddings. SONAR is the underlying embedding space for BLASER 2.0. Scripts to run evaluation with BLASER 2.0 can be found in the &lt;a href=&#34;https://github.com/facebookresearch/SONAR&#34;&gt;SONAR repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/facebookresearch/stopes&#34;&gt;stopes&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;As part of the seamless communication project, we&#39;ve extended the stopes library. Version 1 provided a text-to-text mining tool to build training dataset for translation models. Version 2 has been extended thanks to SONAR, to support tasks around training large speech translation models. In particular, we provide tools to read/write the fairseq audiozip datasets and a new mining pipeline that can do speech-to-speech, text-to-speech, speech-to-text and text-to-text mining, all based on the new SONAR embedding space.&lt;/p&gt; &#xA;&lt;h1&gt;Resources and usage&lt;/h1&gt; &#xA;&lt;h2&gt;SeamlessM4T models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;#params&lt;/th&gt; &#xA;   &lt;th&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;metrics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Large&lt;/td&gt; &#xA;   &lt;td&gt;2.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large&#34;&gt;ü§ó Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-large/resolve/main/multitask_unity_large.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamlessM4T/metrics/seamlessM4T_large.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeamlessM4T-Medium&lt;/td&gt; &#xA;   &lt;td&gt;1.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium&#34;&gt;ü§ó Model card&lt;/a&gt; - &lt;a href=&#34;https://huggingface.co/facebook/seamless-m4t-medium/resolve/main/multitask_unity_medium.pt&#34;&gt;checkpoint&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/seamlessM4T/metrics/seamlessM4T_medium.zip&#34;&gt;metrics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We provide the extensive evaluation results of seamlessM4T-Large and SeamlessM4T-Medium reported in the paper (as averages) in the &lt;code&gt;metrics&lt;/code&gt; files above.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluating SeamlessM4T models&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce our results, or to evaluate using the same metrics over your own test sets, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/eval_README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning SeamlessM4T models&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/scripts/m4t/finetune/README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Converting raw audio to units&lt;/h2&gt; &#xA;&lt;p&gt;Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/scripts/m4t/audio_to_units/README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;On-device models&lt;/h2&gt; &#xA;&lt;p&gt;Apart from Seamless-M4T large (2.3B) and medium (1.2B) models, we are also releasing a small model (281M) targeted for on-device inference. To learn more about the usage and model details check out the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/on_device_README.md&#34;&gt;README here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;SeamlessAlign mined dataset&lt;/h2&gt; &#xA;&lt;p&gt;We open-source the metadata to SeamlessAlign, the largest open dataset for multimodal translation, totaling 270k+ hours of aligned Speech and Text data. The dataset can be rebuilt by the community based on the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/seamless_communication/main/docs/m4t/seamless_align_README.md&#34;&gt;SeamlessAlign readme&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use SeamlessM4T in your work or any models/datasets/artifacts published in SeamlessM4T, please cite :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{seamlessm4t2023,&#xA;  title={SeamlessM4T‚ÄîMassively Multilingual \&amp;amp; Multimodal Machine Translation},&#xA;  author={{Seamless Communication}, Lo\&#34;{i}c Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye,  Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss\`{a} \footnotemark[3], Onur \,{C}elebi,Maha Elbayad,Cynthia Gao, Francisco Guzm\&#39;an, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang},&#xA;  journal={ArXiv},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;seamless_communication is CC-BY-NC 4.0 licensed, as found in LICENSE file&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ddbourgin/numpy-ml</title>
    <updated>2023-08-26T01:35:06Z</updated>
    <id>tag:github.com,2023-08-26:/ddbourgin/numpy-ml</id>
    <link href="https://github.com/ddbourgin/numpy-ml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine learning, in numpy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;numpy-ml&lt;/h1&gt; &#xA;&lt;p&gt;Ever wish you had an inefficient but somewhat legible collection of machine learning algorithms implemented exclusively in NumPy? No?&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;For rapid experimentation&lt;/h3&gt; &#xA;&lt;p&gt;To use this code as a starting point for ML prototyping / experimentation, just clone the repository, create a new &lt;a href=&#34;https://pypi.org/project/virtualenv/&#34;&gt;virtualenv&lt;/a&gt;, and start hacking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ git clone https://github.com/ddbourgin/numpy-ml.git&#xA;$ cd numpy-ml &amp;amp;&amp;amp; virtualenv npml &amp;amp;&amp;amp; source npml/bin/activate&#xA;$ pip3 install -r requirements-dev.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;As a package&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t plan to modify the source, you can also install numpy-ml as a Python package: &lt;code&gt;pip3 install -u numpy_ml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The reinforcement learning agents train on environments defined in the &lt;a href=&#34;https://github.com/openai/gym&#34;&gt;OpenAI gym&lt;/a&gt;. To install these alongside numpy-ml, you can use &lt;code&gt;pip3 install -u &#39;numpy_ml[rl]&#39;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For more details on the available models, see the &lt;a href=&#34;https://numpy-ml.readthedocs.io/&#34;&gt;project documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Available models&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand!&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Gaussian mixture model&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;EM training&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hidden Markov model&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Viterbi decoding&lt;/li&gt; &#xA;    &lt;li&gt;Likelihood computation&lt;/li&gt; &#xA;    &lt;li&gt;MLE parameter estimation via Baum-Welch/forward-backward algorithm&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Latent Dirichlet allocation&lt;/strong&gt; (topic model)&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Standard model with MLE parameter estimation via variational EM&lt;/li&gt; &#xA;    &lt;li&gt;Smoothed model with MAP parameter estimation via MCMC&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Neural networks&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Layers / Layer-wise ops &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Add&lt;/li&gt; &#xA;      &lt;li&gt;Flatten&lt;/li&gt; &#xA;      &lt;li&gt;Multiply&lt;/li&gt; &#xA;      &lt;li&gt;Softmax&lt;/li&gt; &#xA;      &lt;li&gt;Fully-connected/Dense&lt;/li&gt; &#xA;      &lt;li&gt;Sparse evolutionary connections&lt;/li&gt; &#xA;      &lt;li&gt;LSTM&lt;/li&gt; &#xA;      &lt;li&gt;Elman-style RNN&lt;/li&gt; &#xA;      &lt;li&gt;Max + average pooling&lt;/li&gt; &#xA;      &lt;li&gt;Dot-product attention&lt;/li&gt; &#xA;      &lt;li&gt;Embedding layer&lt;/li&gt; &#xA;      &lt;li&gt;Restricted Boltzmann machine (w. CD-n training)&lt;/li&gt; &#xA;      &lt;li&gt;2D deconvolution (w. padding and stride)&lt;/li&gt; &#xA;      &lt;li&gt;2D convolution (w. padding, dilation, and stride)&lt;/li&gt; &#xA;      &lt;li&gt;1D convolution (w. padding, dilation, stride, and causality)&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Modules &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Bidirectional LSTM&lt;/li&gt; &#xA;      &lt;li&gt;ResNet-style residual blocks (identity and convolution)&lt;/li&gt; &#xA;      &lt;li&gt;WaveNet-style residual blocks with dilated causal convolutions&lt;/li&gt; &#xA;      &lt;li&gt;Transformer-style multi-headed scaled dot product attention&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Regularizers &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Dropout&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Normalization &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Batch normalization (spatial and temporal)&lt;/li&gt; &#xA;      &lt;li&gt;Layer normalization (spatial and temporal)&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Optimizers &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;SGD w/ momentum&lt;/li&gt; &#xA;      &lt;li&gt;AdaGrad&lt;/li&gt; &#xA;      &lt;li&gt;RMSProp&lt;/li&gt; &#xA;      &lt;li&gt;Adam&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Learning Rate Schedulers &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Constant&lt;/li&gt; &#xA;      &lt;li&gt;Exponential&lt;/li&gt; &#xA;      &lt;li&gt;Noam/Transformer&lt;/li&gt; &#xA;      &lt;li&gt;Dlib scheduler&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Weight Initializers &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Glorot/Xavier uniform and normal&lt;/li&gt; &#xA;      &lt;li&gt;He/Kaiming uniform and normal&lt;/li&gt; &#xA;      &lt;li&gt;Standard and truncated normal&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Losses &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Cross entropy&lt;/li&gt; &#xA;      &lt;li&gt;Squared error&lt;/li&gt; &#xA;      &lt;li&gt;Bernoulli VAE loss&lt;/li&gt; &#xA;      &lt;li&gt;Wasserstein loss with gradient penalty&lt;/li&gt; &#xA;      &lt;li&gt;Noise contrastive estimation loss&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Activations &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;ReLU&lt;/li&gt; &#xA;      &lt;li&gt;Tanh&lt;/li&gt; &#xA;      &lt;li&gt;Affine&lt;/li&gt; &#xA;      &lt;li&gt;Sigmoid&lt;/li&gt; &#xA;      &lt;li&gt;Leaky ReLU&lt;/li&gt; &#xA;      &lt;li&gt;ELU&lt;/li&gt; &#xA;      &lt;li&gt;SELU&lt;/li&gt; &#xA;      &lt;li&gt;GELU&lt;/li&gt; &#xA;      &lt;li&gt;Exponential&lt;/li&gt; &#xA;      &lt;li&gt;Hard Sigmoid&lt;/li&gt; &#xA;      &lt;li&gt;Softplus&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Models &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Bernoulli variational autoencoder&lt;/li&gt; &#xA;      &lt;li&gt;Wasserstein GAN with gradient penalty&lt;/li&gt; &#xA;      &lt;li&gt;word2vec encoder with skip-gram and CBOW architectures&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Utilities &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;code&gt;col2im&lt;/code&gt; (MATLAB port)&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;im2col&lt;/code&gt; (MATLAB port)&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;conv1D&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;conv2D&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;deconv2D&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;minibatch&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tree-based models&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Decision trees (CART)&lt;/li&gt; &#xA;    &lt;li&gt;[Bagging] Random forests&lt;/li&gt; &#xA;    &lt;li&gt;[Boosting] Gradient-boosted decision trees&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linear models&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Ridge regression&lt;/li&gt; &#xA;    &lt;li&gt;Logistic regression&lt;/li&gt; &#xA;    &lt;li&gt;Ordinary least squares&lt;/li&gt; &#xA;    &lt;li&gt;Weighted linear regression&lt;/li&gt; &#xA;    &lt;li&gt;Generalized linear model (log, logit, and identity link)&lt;/li&gt; &#xA;    &lt;li&gt;Gaussian naive Bayes classifier&lt;/li&gt; &#xA;    &lt;li&gt;Bayesian linear regression w/ conjugate priors &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Unknown mean, known variance (Gaussian prior)&lt;/li&gt; &#xA;      &lt;li&gt;Unknown mean, unknown variance (Normal-Gamma / Normal-Inverse-Wishart prior)&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;n-Gram sequence models&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Maximum likelihood scores&lt;/li&gt; &#xA;    &lt;li&gt;Additive/Lidstone smoothing&lt;/li&gt; &#xA;    &lt;li&gt;Simple Good-Turing smoothing&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-armed bandit models&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;UCB1&lt;/li&gt; &#xA;    &lt;li&gt;LinUCB&lt;/li&gt; &#xA;    &lt;li&gt;Epsilon-greedy&lt;/li&gt; &#xA;    &lt;li&gt;Thompson sampling w/ conjugate priors &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Beta-Bernoulli sampler&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;LinUCB&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reinforcement learning models&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Cross-entropy method agent&lt;/li&gt; &#xA;    &lt;li&gt;First visit on-policy Monte Carlo agent&lt;/li&gt; &#xA;    &lt;li&gt;Weighted incremental importance sampling Monte Carlo agent&lt;/li&gt; &#xA;    &lt;li&gt;Expected SARSA agent&lt;/li&gt; &#xA;    &lt;li&gt;TD-0 Q-learning agent&lt;/li&gt; &#xA;    &lt;li&gt;Dyna-Q / Dyna-Q+ with prioritized sweeping&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nonparameteric models&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Nadaraya-Watson kernel regression&lt;/li&gt; &#xA;    &lt;li&gt;k-Nearest neighbors classification and regression&lt;/li&gt; &#xA;    &lt;li&gt;Gaussian process regression&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Matrix factorization&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Regularized alternating least-squares&lt;/li&gt; &#xA;    &lt;li&gt;Non-negative matrix factorization&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Discrete Fourier transform (1D signals)&lt;/li&gt; &#xA;    &lt;li&gt;Discrete cosine transform (type-II) (1D signals)&lt;/li&gt; &#xA;    &lt;li&gt;Bilinear interpolation (2D signals)&lt;/li&gt; &#xA;    &lt;li&gt;Nearest neighbor interpolation (1D and 2D signals)&lt;/li&gt; &#xA;    &lt;li&gt;Autocorrelation (1D signals)&lt;/li&gt; &#xA;    &lt;li&gt;Signal windowing&lt;/li&gt; &#xA;    &lt;li&gt;Text tokenization&lt;/li&gt; &#xA;    &lt;li&gt;Feature hashing&lt;/li&gt; &#xA;    &lt;li&gt;Feature standardization&lt;/li&gt; &#xA;    &lt;li&gt;One-hot encoding / decoding&lt;/li&gt; &#xA;    &lt;li&gt;Huffman coding / decoding&lt;/li&gt; &#xA;    &lt;li&gt;Byte pair encoding / decoding&lt;/li&gt; &#xA;    &lt;li&gt;Term frequency-inverse document frequency (TF-IDF) encoding&lt;/li&gt; &#xA;    &lt;li&gt;MFCC encoding&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Utilities&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Similarity kernels&lt;/li&gt; &#xA;    &lt;li&gt;Distance metrics&lt;/li&gt; &#xA;    &lt;li&gt;Priority queue&lt;/li&gt; &#xA;    &lt;li&gt;Ball tree&lt;/li&gt; &#xA;    &lt;li&gt;Discrete sampler&lt;/li&gt; &#xA;    &lt;li&gt;Graph processing and generators&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Am I missing your favorite model? Is there something that could be cleaner / less confusing? Did I mess something up? Submit a PR! The only requirement is that your models are written with just the &lt;a href=&#34;https://docs.python.org/3/library/&#34;&gt;Python standard library&lt;/a&gt; and &lt;a href=&#34;https://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt;. The &lt;a href=&#34;https://scipy.github.io/devdocs/&#34;&gt;SciPy library&lt;/a&gt; is also permitted under special circumstances ;)&lt;/p&gt; &#xA;&lt;p&gt;See full contributing guidelines &lt;a href=&#34;https://raw.githubusercontent.com/ddbourgin/numpy-ml/master/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>