<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-18T01:34:00Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIA/garak</title>
    <updated>2024-11-18T01:34:00Z</updated>
    <id>tag:github.com,2024-11-18:/NVIDIA/garak</id>
    <link href="https://github.com/NVIDIA/garak" rel="alternate"></link>
    <summary type="html">&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don&#39;t want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt;, it&#39;s &lt;code&gt;nmap&lt;/code&gt; for LLMs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dyanmic, and adaptive probes to explore this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;&#39;s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests/Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests/Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests/OSX&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://garak.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/garak/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/uVch4puUCs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true&#34; alt=&#34;discord-img&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/garak&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/garak&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/garak&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/garak.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/garak&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/garak&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/garak&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/garak/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;h3&gt;&amp;gt; See our user guide! &lt;a href=&#34;https://docs.garak.ai/&#34;&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; Join our &lt;a href=&#34;https://discord.gg/uVch4puUCs&#34;&gt;Discord&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href=&#34;https://garak.ai/&#34;&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; Twitter: &lt;a href=&#34;https://twitter.com/garak_llm&#34;&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&amp;gt; DEF CON &lt;a href=&#34;https://garak.ai/garak_aiv_slides.pdf&#34;&gt;slides&lt;/a&gt;!&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;LLM support&lt;/h2&gt; &#xA;&lt;p&gt;currently supports:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models&#34;&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/&#34;&gt;replicate&lt;/a&gt; text models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/introduction&#34;&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.litellm.ai/&#34;&gt;litellm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; &#xA; &lt;li&gt;gguf models like &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; &#xA; &lt;li&gt;.. and many more LLMs!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It&#39;s developed in Linux and OSX.&lt;/p&gt; &#xA;&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install -U garak&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version, from GitHub, try:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Clone from source&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name garak &#34;python&amp;gt;=3.10,&amp;lt;=3.12&#34;&#xA;conda activate garak&#xA;gh repo clone NVIDIA/garak&#xA;cd garak&#xA;python -m pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OK, if that went fine, you&#39;re probably good to go!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you&#39;re reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;The general syntax is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it&#39;ll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To specify a generator, use the &lt;code&gt;--model_type&lt;/code&gt; and, optionally, the &lt;code&gt;--model_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The &#34;Intro to generators&#34; section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--model_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; to the model&#39;s name on Hub (e.g. &lt;code&gt;&#34;RWKV/rwkv-4-169m-pile&#34;&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they&#39;ll let you know if they need that.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href=&#34;https://github.com/agencyenterprise/promptinject&#34;&gt;PromptInject&lt;/a&gt; framework&#39;s methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href=&#34;https://arxiv.org/abs/2303.18190&#34;&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; &#xA;&lt;p&gt;For help &amp;amp; inspiration, find us on &lt;a href=&#34;https://twitter.com/garak_llm&#34;&gt;twitter&lt;/a&gt; or &lt;a href=&#34;https://discord.gg/uVch4puUCs&#34;&gt;discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=&#34;sk-123XXXXXXXXXXXX&#34;&#xA;python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Reading the results&lt;/h2&gt; &#xA;&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe&#39;s results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; &#xA;&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src=&#34;https://i.imgur.com/8Dxf45N.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;And the same results for ChatGPT: &lt;img src=&#34;https://i.imgur.com/VKAF5if.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; &#xA;&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There&#39;s a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; &#xA;&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; &#xA;&lt;h2&gt;Intro to generators&lt;/h2&gt; &#xA;&lt;h3&gt;Hugging Face&lt;/h3&gt; &#xA;&lt;p&gt;Using the Pipeline API:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn&#39;t, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using the Inference API:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;&#34;mosaicml/mpt-7b-instruct&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using private endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the &#34;read&#34; role; see &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OpenAI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type openai&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OpenAI model you&#39;d like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. &#34;sk-19763ASDF87q6657&#34;); see &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you&#39;d like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; &#xA;&lt;h3&gt;Replicate&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. &#34;r8-123XXXXXXXXXXXX&#34;; see &lt;a href=&#34;https://replicate.com/account/api-tokens&#34;&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Public Replicate models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type replicate&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;&#34;stability-ai/stablelm-tuned-alpha-7b:c49dae36&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cohere&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type cohere&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you&#39;d like to test&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. &#34;aBcDeFgHiJ123456789&#34;; see &lt;a href=&#34;https://dashboard.cohere.ai/api-keys&#34;&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Groq&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type groq&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href=&#34;https://console.groq.com/docs/quickstart&#34;&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ggml&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type ggml&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The path to the ggml model you&#39;d like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;REST&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href=&#34;https://reference.garak.ai/en/latest/garak.generators.rest.html&#34;&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; &#xA;&lt;h3&gt;NIM&lt;/h3&gt; &#xA;&lt;p&gt;Use models from &lt;a href=&#34;https://build.nvidia.com/&#34;&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For chat models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type nim&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For completion models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OctoAI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set the &lt;code&gt;OCTO_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. &#34;r8-123XXXXXXXXXXXX&#34;; see &lt;a href=&#34;https://replicate.com/account/api-tokens&#34;&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Octo public endpoint:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type octo&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OctoAI public endpoint for the model, e.g. &lt;code&gt;mistral-7b-instruct-fp16&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Octo private endpoint:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_type octo.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the deployed endpoint URL, e.g. &lt;code&gt;https://llama-2-70b-chat-xxx.octoai.run/v1/chat/completions&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--model_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Intro to probes&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Probe&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;blank&lt;/td&gt; &#xA;   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;atkgen&lt;/td&gt; &#xA;   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href=&#34;https://huggingface.co/garak-llm/artgpt2tox&#34;&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;av_spam_scanning&lt;/td&gt; &#xA;   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;continuation&lt;/td&gt; &#xA;   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dan&lt;/td&gt; &#xA;   &lt;td&gt;Various &lt;a href=&#34;https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html&#34;&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;donotanswer&lt;/td&gt; &#xA;   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;encoding&lt;/td&gt; &#xA;   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gcg&lt;/td&gt; &#xA;   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;glitch&lt;/td&gt; &#xA;   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;grandma&lt;/td&gt; &#xA;   &lt;td&gt;Appeal to be reminded of one&#39;s grandmother.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;goodside&lt;/td&gt; &#xA;   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;leakerplay&lt;/td&gt; &#xA;   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lmrc&lt;/td&gt; &#xA;   &lt;td&gt;Subsample of the &lt;a href=&#34;https://arxiv.org/abs/2303.18190&#34;&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;malwaregen&lt;/td&gt; &#xA;   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;misleading&lt;/td&gt; &#xA;   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;packagehallucination&lt;/td&gt; &#xA;   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;promptinject&lt;/td&gt; &#xA;   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href=&#34;https://github.com/agencyenterprise/PromptInject/tree/main/promptinject&#34;&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;realtoxicityprompts&lt;/td&gt; &#xA;   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;snowball&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ofir.io/snowballed_hallucination.pdf&#34;&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xss&lt;/td&gt; &#xA;   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Logging&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; &#xA; &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry&#39;s &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; &#xA; &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a &#39;hit&#39;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How is the code structured?&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://reference.garak.ai/&#34;&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; &#xA;&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; &#xA;&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Developing your own plugin&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; &#xA; &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Override as little as possible&lt;/li&gt; &#xA; &lt;li&gt;You can test the new code in at least two ways: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Start an interactive Python session &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Run a scan with test plugins &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you&#39;re writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;We have an FAQ &lt;a href=&#34;https://github.com/NVIDIA/garak/raw/main/FAQ.md&#34;&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href=&#34;mailto:leon@garak.ai&#34;&gt;leon@garak.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Code reference documentation is at &lt;a href=&#34;https://garak.readthedocs.io/en/latest/&#34;&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing garak&lt;/h2&gt; &#xA;&lt;p&gt;You can read the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf&#34;&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{garak,&#xA;  title={{garak: A Framework for Security Probing Large Language Models}},&#xA;  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},&#xA;  year={2024},&#xA;  howpublished={\url{https://garak.ai}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;&#34;Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly&#34;&lt;/em&gt; - Elim&lt;/p&gt; &#xA;&lt;p&gt;For updates and news see &lt;a href=&#34;https://twitter.com/garak_llm&#34;&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;© 2023- Leon Derczynski; Apache license v2, see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NexaAI/nexa-sdk</title>
    <updated>2024-11-18T01:34:00Z</updated>
    <id>tag:github.com,2024-11-18:/NexaAI/nexa-sdk</id>
    <link href="https://github.com/NexaAI/nexa-sdk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Nexa SDK is a comprehensive toolkit for supporting ONNX and GGML models. It supports text generation, image generation, vision-language models (VLM), auto-speech-recognition (ASR), and text-to-speech (TTS) capabilities.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&#xA; &lt;video src=&#34;https://user-images.githubusercontent.com/assets/375570dc-0e7a-4a99-840d-c1ef6502e5aa.mp4&#34; autoplay muted loop playsinline style=&#34;max-width: 100%;&#34;&gt;&lt;/video&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Nexa SDK - Local On-Device Inference Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NexaAI/nexa-sdk/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-MacOS-black?logo=apple&#34; alt=&#34;MacOS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NexaAI/nexa-sdk/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Linux-333?logo=ubuntu&#34; alt=&#34;Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NexaAI/nexa-sdk/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/windows-0078D4?logo=windows&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://swiftpackageindex.com/NexaAI/nexa-sdk&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2FNexaAI%2Fnexa-sdk%2Fbadge%3Ftype%3Dplatforms&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NexaAI/nexa-sdk/actions/workflows/ci.yaml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/NexaAI/nexa-sdk/ci.yaml?label=CI&amp;amp;logo=github&#34; alt=&#34;Build workflow&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/NexaAI/nexa-sdk&#34; alt=&#34;GitHub License&#34;&gt; &lt;a href=&#34;https://github.com/NexaAI/nexa-sdk/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/NexaAI/nexa-sdk&#34; alt=&#34;GitHub Release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nexa.ai/models&#34;&gt;&lt;strong&gt;On-Device Model Hub&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://docs.nexa.ai/&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/thRu2HaK4D&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://nexa.ai/blogs&#34;&gt;&lt;strong&gt;Blogs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://x.com/nexa_ai&#34;&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nexa SDK&lt;/strong&gt; is a local on-device inference framework for ONNX and GGML models, supporting text generation, image generation, vision-language models (VLM), audio-language models, speech-to-text (ASR), and text-to-speech (TTS) capabilities. Installable via Python Package or Executable Installer.&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Device Support:&lt;/strong&gt; CPU, GPU (CUDA, Metal, ROCm), iOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Server:&lt;/strong&gt; OpenAI-compatible API, JSON schema for function calling and streaming support&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local UI:&lt;/strong&gt; Streamlit for interactive model deployment and testing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Latest News 🔥&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support Nexa AI&#39;s own vision language model (0.9B parameters): &lt;code&gt;nexa run omnivision&lt;/code&gt; and audio language model (2.9B): &lt;code&gt;nexa run omniaudio&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support audio language model: &lt;code&gt;nexa run qwen2audio&lt;/code&gt;, &lt;strong&gt;we are the first open-source toolkit to support audio language model with GGML tensor library.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support embedding model: &lt;code&gt;nexa embed &amp;lt;model_path&amp;gt; &amp;lt;prompt&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support pull and run supported Computer Vision models in GGUF format from HuggingFace: &lt;code&gt;nexa run -hf &amp;lt;model_id&amp;gt; -mt COMPUTER_VISION&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support pull and run NLP models in GGUF format from HuggingFace: &lt;code&gt;nexa run -hf &amp;lt;model_id&amp;gt; -mt NLP&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Welcome to submit your requests through &lt;a href=&#34;https://github.com/NexaAI/nexa-sdk/issues/new/choose&#34;&gt;issues&lt;/a&gt;, we ship weekly.&lt;/p&gt; &#xA;&lt;h2&gt;Install Option 1: Executable Installer&lt;/h2&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://public-storage.nexa4ai.com/nexa-sdk-executable-installer/nexa-sdk-0.0.9.2-macos-installer.pkg&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NexaAI/nexa-sdk/main/assets/mac.png&#34; style=&#34;height: 1em; width: auto&#34;&gt; &lt;strong&gt; macOS Installer &lt;/strong&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://public-storage.nexa4ai.com/nexa-sdk-executable-installer/nexa-sdk-0.0.9.2-windows-setup.exe&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NexaAI/nexa-sdk/main/assets/windows.png&#34; style=&#34;height: 1em; width: auto&#34;&gt; &lt;strong&gt;Windows Installer&lt;/strong&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NexaAI/nexa-sdk/main/assets/linux.png&#34; style=&#34;height: 1em; width: auto&#34;&gt; Linux Installer &lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -fsSL https://public-storage.nexa4ai.com/install.sh | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently, executable installer does not support local UI. For local UI, please install using python package below.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;FAQ: cannot use executable with nexaai python package already installed&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Try using &lt;code&gt;nexa-exe&lt;/code&gt; instead:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nexa-exe &amp;lt;command&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Install Option 2: Python Package&lt;/h2&gt; &#xA;&lt;p&gt;We have released pre-built wheels for various Python versions, platforms, and backends for convenient installation on our &lt;a href=&#34;https://nexaai.github.io/nexa-sdk/whl/&#34;&gt;index page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong style=&#34;font-size: 1.2em;&#34;&gt;CPU&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/cpu --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong style=&#34;font-size: 1.2em;&#34;&gt;Apple GPU (Metal)&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;For the GPU version supporting &lt;strong&gt;Metal (macOS)&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CMAKE_ARGS=&#34;-DGGML_METAL=ON -DSD_METAL=ON&#34; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/metal --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;strong&gt;FAQ: cannot use Metal/GPU on M1&lt;/strong&gt;&lt;/summary&gt; &#xA;  &lt;p&gt;Try the following command:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh&#xA;bash Miniforge3-MacOSX-arm64.sh&#xA;conda create -n nexasdk python=3.10&#xA;conda activate nexasdk&#xA;CMAKE_ARGS=&#34;-DGGML_METAL=ON -DSD_METAL=ON&#34; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/metal --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong style=&#34;font-size: 1.2em;&#34;&gt;Nvidia GPU (CUDA)&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;To install with CUDA support, make sure you have &lt;a href=&#34;https://developer.nvidia.com/cuda-12-0-0-download-archive&#34;&gt;CUDA Toolkit 12.0 or later&lt;/a&gt; installed.&lt;/p&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Linux&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CMAKE_ARGS=&#34;-DGGML_CUDA=ON -DSD_CUBLAS=ON&#34; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Windows PowerShell&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$env:CMAKE_ARGS=&#34;-DGGML_CUDA=ON -DSD_CUBLAS=ON&#34;; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Windows Command Prompt&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;set CMAKE_ARGS=&#34;-DGGML_CUDA=ON -DSD_CUBLAS=ON&#34; &amp;amp; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Windows Git Bash&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CMAKE_ARGS=&#34;-DGGML_CUDA=ON -DSD_CUBLAS=ON&#34; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;strong&gt;FAQ: Building Issues for llava&lt;/strong&gt;&lt;/summary&gt; &#xA;  &lt;p&gt;If you encounter the following issue while building:&lt;/p&gt; &#xA;  &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NexaAI/nexa-sdk/main/docs/.media/error.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;  &lt;p&gt;try the following command:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CMAKE_ARGS=&#34;-DCMAKE_CXX_FLAGS=-fopenmp&#34; pip install nexaai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong style=&#34;font-size: 1.2em;&#34;&gt;AMD GPU (ROCm)&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;To install with ROCm support, make sure you have &lt;a href=&#34;https://rocm.docs.amd.com/projects/install-on-linux/en/docs-6.2.1/install/quick-start.html&#34;&gt;ROCm 6.2.1 or later&lt;/a&gt; installed.&lt;/p&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Linux&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CMAKE_ARGS=&#34;-DGGML_HIPBLAS=on&#34; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/rocm621 --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong style=&#34;font-size: 1.2em;&#34;&gt;GPU (Vulkan)&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;To install with Vulkan support, make sure you have &lt;a href=&#34;https://vulkan.lunarg.com/sdk/home&#34;&gt;Vulkan SDK 1.3.261.1 or later&lt;/a&gt; installed.&lt;/p&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Windows PowerShell&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$env:CMAKE_ARGS=&#34;-DGGML_VULKAN=on&#34;; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/vulkan --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Windows Command Prompt&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;set CMAKE_ARGS=&#34;-DGGML_VULKAN=on&#34; &amp;amp; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/vulkan --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;For &lt;strong&gt;Windows Git Bash&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CMAKE_ARGS=&#34;-DGGML_VULKAN=on&#34; pip install nexaai --prefer-binary --index-url https://nexaai.github.io/nexa-sdk/whl/vulkan --extra-index-url https://pypi.org/simple --no-cache-dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong style=&#34;font-size: 1.2em;&#34;&gt;Local Build&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;How to clone this repo&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/NexaAI/nexa-sdk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;If you forget to use &lt;code&gt;--recursive&lt;/code&gt;, you can use below command to add submodule&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then you can build and install the package&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Differentiation&lt;/h2&gt; &#xA;&lt;p&gt;Below is our differentiation from other similar tools:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/NexaAI/nexa-sdk&#34;&gt;Nexa SDK&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;ollama&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/huggingface/optimum&#34;&gt;Optimum&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/lmstudio-ai&#34;&gt;LM Studio&lt;/a&gt;&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GGML Support&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ONNX Support&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Text Generation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Image Generation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Vision-Language Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Server Capability&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;User Interface&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Models &amp;amp; Model Hub&lt;/h2&gt; &#xA;&lt;p&gt;Our on-device model hub offers all types of quantized models (text, image, audio, multimodal) with filters for RAM, file size, Tasks, etc. to help you easily explore models with UI. Explore on-device models at &lt;a href=&#34;https://model-hub.nexa4ai.com/&#34;&gt;On-device Model Hub&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Supported model examples (full list at &lt;a href=&#34;https://nexa.ai/models&#34;&gt;Model Hub&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Format&lt;/th&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexa.ai/NexaAI/Octo-omni-audio/gguf-q4_0/readme&#34;&gt;omniaudio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AudioLM&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run omniaudio&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/qnguyen3/nanoLLaVA/gguf-fp16/readme&#34;&gt;qwen2audio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AudioLM&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run qwen2audio&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/NexaAI/Octopus-v2/gguf-q4_0/readme&#34;&gt;octopus-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Function Call&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run octopus-v2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/NexaAI/Octo-net/gguf-q4_0/readme&#34;&gt;octo-net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run octo-net&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexa.ai/NexaAI/Octo-omni-vision/gguf-fp16/readme&#34;&gt;omnivision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run omnivision&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/qnguyen3/nanoLLaVA/gguf-fp16/readme&#34;&gt;nanollava&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run nanollava&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/xtuner/llava-phi-3-mini/gguf-q4_0/readme&#34;&gt;llava-phi3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run llava-phi3&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/xtuner/llava-llama-3-8b-v1.1/gguf-q4_0/readme&#34;&gt;llava-llama3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run llava-llama3&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/liuhaotian/llava-v1.6-mistral-7b/gguf-q4_0/readme&#34;&gt;llava1.6-mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run llava1.6-mistral&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/liuhaotian/llava-v1.6-vicuna-7b/gguf-q4_0/readme&#34;&gt;llava1.6-vicuna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run llava1.6-vicuna&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/meta/Llama3.2-3B-Instruct/gguf-q4_0/readme&#34;&gt;llama3.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run llama3.2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/Orenguteng/Llama3-8B-Lexi-Uncensored/gguf-q4_K_M/readme&#34;&gt;llama3-uncensored&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run llama3-uncensored&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/google/gemma-2-2b-instruct/gguf-q4_0/readme&#34;&gt;gemma2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run gemma2&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/Qwen/Qwen2.5-1.5B-Instruct/gguf-q4_0/readme&#34;&gt;qwen2.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run qwen2.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/Qwen/Qwen2.5-Math-1.5B-Instruct/gguf-q4_0/readme&#34;&gt;mathqwen&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run mathqwen&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/Qwen/CodeQwen1.5-7B-Instruct/gguf-q4_0/readme&#34;&gt;codeqwen&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run codeqwen&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/mistralai/Mistral-7B-Instruct-v0.3/gguf-q4_0/readme&#34;&gt;mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF/ONNX&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run mistral&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/DeepSeek/deepseek-coder-1.3b-instruct/gguf-q4_0/readme&#34;&gt;deepseek-coder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run deepseek-coder&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/microsoft/Phi-3.5-mini-instruct/gguf-q4_0/readme&#34;&gt;phi3.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run phi3.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/apple/OpenELM-3B/gguf-q4_K_M/readme&#34;&gt;openelm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run openelm&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/StabilityAI/stable-diffusion-v2-1/gguf-q4_0/readme&#34;&gt;stable-diffusion-v2-1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Image Generation&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run sd2-1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/StabilityAI/stable-diffusion-3-medium/gguf-q4_0/readme&#34;&gt;stable-diffusion-3-medium&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Image Generation&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run sd3&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/BlackForestLabs/FLUX.1-schnell/gguf-q4_0/readme&#34;&gt;FLUX.1-schnell&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Image Generation&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run flux&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.nexaai.com/SimianLuo/lcm-dreamshaper-v7/gguf-fp16/readme&#34;&gt;lcm-dreamshaper&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Image Generation&lt;/td&gt; &#xA;   &lt;td&gt;GGUF/ONNX&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run lcm-dreamshaper&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/Systran/faster-whisper-large-v3-turbo/bin-cpu-fp16/readme&#34;&gt;whisper-large-v3-turbo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Speech-to-Text&lt;/td&gt; &#xA;   &lt;td&gt;BIN&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run faster-whisper-large-turbo&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexaai.com/openai/whisper-tiny.en/onnx-cpu-fp32/readme&#34;&gt;whisper-tiny.en&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Speech-to-Text&lt;/td&gt; &#xA;   &lt;td&gt;ONNX&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run whisper-tiny.en&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexa.ai/mixedbread-ai/mxbai-embed-large-v1/gguf-fp16/readme&#34;&gt;mxbai-embed-large-v1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Embedding&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa embed mxbai&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexa.ai/nomic-ai/nomic-embed-text-v1.5/gguf-fp16/readme&#34;&gt;nomic-embed-text-v1.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Embedding&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa embed nomic&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexa.ai/sentence-transformers/all-MiniLM-L12-v2/gguf-fp16/readme&#34;&gt;all-MiniLM-L12-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Embedding&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa embed all-MiniLM-L12-v2:fp16&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://nexa.ai/suno/bark-small/gguf-fp16/readme&#34;&gt;bark-small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Text-to-Speech&lt;/td&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;nexa run bark-small:fp16&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Run Models from 🤗 HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;You can pull, convert (to .gguf), quantize and run &lt;a href=&#34;https://github.com/ggerganov/llama.cpp#description&#34;&gt;llama.cpp supported&lt;/a&gt; text generation models from HF with Nexa SDK.&lt;/p&gt; &#xA;&lt;h3&gt;Run .gguf File&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;nexa run -hf &amp;lt;hf-model-id&amp;gt;&lt;/code&gt; to run models with provided .gguf files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nexa run -hf Qwen/Qwen2.5-Coder-7B-Instruct-GGUF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You will be prompted to select a single .gguf file. If your desired quantization version has multiple split files (like fp16-00001-of-00004), please use Nexa&#39;s conversion tool (see below) to convert and quantize the model locally.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Convert .safetensors Files&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-2-python-package&#34;&gt;Nexa Python package&lt;/a&gt;, and install Nexa conversion tool with &lt;code&gt;pip install &#34;nexaai[convert]&#34;&lt;/code&gt;, then convert models with &lt;code&gt;nexa convert &amp;lt;hf-model-id&amp;gt;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nexa convert HuggingFaceTB/SmolLM2-135M-Instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Check our &lt;a href=&#34;https://nexa.ai/leaderboard&#34;&gt;leaderboard&lt;/a&gt; for performance benchmarks of different quantized versions of mainstream language models and &lt;a href=&#34;https://huggingface.co/docs/optimum/en/concept_guides/quantization&#34;&gt;HuggingFace docs&lt;/a&gt; to learn about quantization options.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;📋 You can view downloaded and converted models with &lt;code&gt;nexa list&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;If you want to use &lt;strong&gt;ONNX model&lt;/strong&gt;, just replace &lt;code&gt;pip install nexaai&lt;/code&gt; with &lt;code&gt;pip install &#34;nexaai[onnx]&#34;&lt;/code&gt; in provided commands.&lt;/li&gt; &#xA;  &lt;li&gt;If you want to &lt;strong&gt;run benchmark evaluation&lt;/strong&gt;, just replace &lt;code&gt;pip install nexaai&lt;/code&gt; with &lt;code&gt;pip install &#34;nexaai[eval]&#34;&lt;/code&gt; in provided commands.&lt;/li&gt; &#xA;  &lt;li&gt;If you want to &lt;strong&gt;convert and quantize huggingface models to GGUF models&lt;/strong&gt;, just replace &lt;code&gt;pip install nexaai&lt;/code&gt; with &lt;code&gt;pip install &#34;nexaai[convert]&#34;&lt;/code&gt; in provided commands.&lt;/li&gt; &#xA;  &lt;li&gt;For Chinese developers, we recommend you to use &lt;strong&gt;Tsinghua Open Source Mirror&lt;/strong&gt; as extra index url, just replace &lt;code&gt;--extra-index-url https://pypi.org/simple&lt;/code&gt; with &lt;code&gt;--extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple&lt;/code&gt; in provided commands.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;CLI Reference&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s a brief overview of the main CLI commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa run&lt;/code&gt;: Run inference for various tasks using GGUF models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa onnx&lt;/code&gt;: Run inference for various tasks using ONNX models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa convert&lt;/code&gt;: Convert and quantize huggingface models to GGUF models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa server&lt;/code&gt;: Run the Nexa AI Text Generation Service.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa eval&lt;/code&gt;: Run the Nexa AI Evaluation Tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa pull&lt;/code&gt;: Pull a model from official or hub.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa remove&lt;/code&gt;: Remove a model from local machine.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa clean&lt;/code&gt;: Clean up all model files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa list&lt;/code&gt;: List all models in the local machine.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa login&lt;/code&gt;: Login to Nexa API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa whoami&lt;/code&gt;: Show current user information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nexa logout&lt;/code&gt;: Logout from Nexa API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For detailed information on CLI commands and usage, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NexaAI/nexa-sdk/main/CLI.md&#34;&gt;CLI Reference&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;h3&gt;Start Local Server&lt;/h3&gt; &#xA;&lt;p&gt;To start a local server using models on your local computer, you can use the &lt;code&gt;nexa server&lt;/code&gt; command. For detailed information on server setup, API endpoints, and usage examples, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NexaAI/nexa-sdk/main/SERVER.md&#34;&gt;Server Reference&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;h3&gt;Swift Package&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/NexaAI/nexa-sdk/tree/main/swift&#34;&gt;Swift SDK&lt;/a&gt;:&lt;/strong&gt; Provides a Swifty API, allowing Swift developers to easily integrate and use llama.cpp models in their projects.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nexa.ai/&#34;&gt;&lt;strong&gt;More Docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the following projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/leejet/stable-diffusion.cpp&#34;&gt;stable-diffusion.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PABannier/bark.cpp&#34;&gt;bark.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/optimum&#34;&gt;optimum&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>