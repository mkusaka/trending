<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-03T01:35:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>superlinear-ai/raglite</title>
    <updated>2024-12-03T01:35:33Z</updated>
    <id>tag:github.com,2024-12-03:/superlinear-ai/raglite</id>
    <link href="https://github.com/superlinear-ai/raglite" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü•§ RAGLite is a Python toolkit for Retrieval-Augmented Generation (RAG) with PostgreSQL or SQLite&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/superlinear-ai/raglite&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&#34; alt=&#34;Open in Dev Containers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/codespaces/new/superlinear-ai/raglite&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=GitHub%20Codespaces&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=github&#34; alt=&#34;Open in GitHub Codespaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ü•§ RAGLite&lt;/h1&gt; &#xA;&lt;p&gt;RAGLite is a Python toolkit for Retrieval-Augmented Generation (RAG) with PostgreSQL or SQLite.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h5&gt;Configurable&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß† Choose any LLM provider with &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;LiteLLM&lt;/a&gt;, including local &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;llama-cpp-python&lt;/a&gt; models&lt;/li&gt; &#xA; &lt;li&gt;üíæ Choose either &lt;a href=&#34;https://github.com/postgres/postgres&#34;&gt;PostgreSQL&lt;/a&gt; or &lt;a href=&#34;https://github.com/sqlite/sqlite&#34;&gt;SQLite&lt;/a&gt; as a keyword &amp;amp; vector search database&lt;/li&gt; &#xA; &lt;li&gt;ü•á Choose any reranker with &lt;a href=&#34;https://github.com/AnswerDotAI/rerankers&#34;&gt;rerankers&lt;/a&gt;, including multilingual &lt;a href=&#34;https://github.com/PrithivirajDamodaran/FlashRank&#34;&gt;FlashRank&lt;/a&gt; as the default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Fast and permissive&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ù§Ô∏è Only lightweight and permissive open source dependencies (e.g., no &lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;PyTorch&lt;/a&gt; or &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;LangChain&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;üöÄ Acceleration with Metal on macOS, and CUDA on Linux and Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Unhobbled&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ PDF to Markdown conversion on top of &lt;a href=&#34;https://github.com/VikParuchuri/pdftext&#34;&gt;pdftext&lt;/a&gt; and &lt;a href=&#34;https://github.com/pypdfium2-team/pypdfium2&#34;&gt;pypdfium2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üß¨ Multi-vector chunk embedding with &lt;a href=&#34;https://weaviate.io/blog/late-chunking&#34;&gt;late chunking&lt;/a&gt; and &lt;a href=&#34;https://d-star.ai/solving-the-out-of-context-chunk-problem-for-rag&#34;&gt;contextual chunk headings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÇÔ∏è Optimal &lt;a href=&#34;https://medium.com/@anuragmishra_27746/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d&#34;&gt;level 4 semantic chunking&lt;/a&gt; by solving a &lt;a href=&#34;https://en.wikipedia.org/wiki/Integer_programming&#34;&gt;binary integer programming problem&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîç &lt;a href=&#34;https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf&#34;&gt;Hybrid search&lt;/a&gt; with the database&#39;s native keyword &amp;amp; vector search (&lt;a href=&#34;https://www.postgresql.org/docs/current/datatype-textsearch.html&#34;&gt;tsvector&lt;/a&gt;+&lt;a href=&#34;https://github.com/pgvector/pgvector&#34;&gt;pgvector&lt;/a&gt;, &lt;a href=&#34;https://www.sqlite.org/fts5.html&#34;&gt;FTS5&lt;/a&gt;+&lt;a href=&#34;https://github.com/asg017/sqlite-vec&#34;&gt;sqlite-vec&lt;/a&gt;[^1])&lt;/li&gt; &#xA; &lt;li&gt;üåÄ Optimal &lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/src/raglite/_query_adapter.py&#34;&gt;closed-form linear query adapter&lt;/a&gt; by solving an &lt;a href=&#34;https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem&#34;&gt;orthogonal Procrustes problem&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Extensible&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ Optional customizable ChatGPT-like frontend for &lt;a href=&#34;https://docs.chainlit.io/deploy/copilot&#34;&gt;web&lt;/a&gt;, &lt;a href=&#34;https://docs.chainlit.io/deploy/slack&#34;&gt;Slack&lt;/a&gt;, and &lt;a href=&#34;https://docs.chainlit.io/deploy/teams&#34;&gt;Teams&lt;/a&gt; with &lt;a href=&#34;https://github.com/Chainlit/chainlit&#34;&gt;Chainlit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úçÔ∏è Optional conversion of any input document to Markdown with &lt;a href=&#34;https://github.com/jgm/pandoc&#34;&gt;Pandoc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Optional evaluation of retrieval and generation performance with &lt;a href=&#34;https://github.com/explodinggradients/ragas&#34;&gt;Ragas&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[^1]: We use &lt;a href=&#34;https://github.com/lmcinnes/pynndescent&#34;&gt;PyNNDescent&lt;/a&gt; until &lt;a href=&#34;https://github.com/asg017/sqlite-vec&#34;&gt;sqlite-vec&lt;/a&gt; is more mature.&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;First, begin by installing spaCy&#39;s multilingual sentence model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Install spaCy&#39;s xx_sent_ud_sm:&#xA;pip install https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.7.0/xx_sent_ud_sm-3.7.0-py3-none-any.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, it is optional but recommended to install &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends&#34;&gt;an accelerated llama-cpp-python precompiled binary&lt;/a&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Configure which llama-cpp-python precompiled binary to install (‚ö†Ô∏è only v0.2.88 is supported right now):&#xA;LLAMA_CPP_PYTHON_VERSION=0.2.88&#xA;PYTHON_VERSION=310&#xA;ACCELERATOR=metal|cu121|cu122|cu123|cu124&#xA;PLATFORM=macosx_11_0_arm64|linux_x86_64|win_amd64&#xA;&#xA;# Install llama-cpp-python:&#xA;pip install &#34;https://github.com/abetlen/llama-cpp-python/releases/download/v$LLAMA_CPP_PYTHON_VERSION-$ACCELERATOR/llama_cpp_python-$LLAMA_CPP_PYTHON_VERSION-cp$PYTHON_VERSION-cp$PYTHON_VERSION-$PLATFORM.whl&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, install RAGLite with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install raglite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To add support for a customizable ChatGPT-like frontend, use the &lt;code&gt;chainlit&lt;/code&gt; extra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install raglite[chainlit]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To add support for filetypes other than PDF, use the &lt;code&gt;pandoc&lt;/code&gt; extra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install raglite[pandoc]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To add support for evaluation, use the &lt;code&gt;ragas&lt;/code&gt; extra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install raglite[ragas]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using&lt;/h2&gt; &#xA;&lt;h3&gt;Overview&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/#1-configuring-raglite&#34;&gt;Configuring RAGLite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/#2-inserting-documents&#34;&gt;Inserting documents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/#3-searching-and-retrieval-augmented-generation-rag&#34;&gt;Searching and Retrieval-Augmented Generation (RAG)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/#4-computing-and-using-an-optimal-query-adapter&#34;&gt;Computing and using an optimal query adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/#5-evaluation-of-retrieval-and-generation&#34;&gt;Evaluation of retrieval and generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/#6-serving-a-customizable-chatgpt-like-frontend&#34;&gt;Serving a customizable ChatGPT-like frontend&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;1. Configuring RAGLite&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] üß† RAGLite extends &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;LiteLLM&lt;/a&gt; with support for &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; models using &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;llama-cpp-python&lt;/a&gt;. To select a llama.cpp model (e.g., from &lt;a href=&#34;https://huggingface.co/bartowski&#34;&gt;bartowski&#39;s collection&lt;/a&gt;), use a model identifier of the form &lt;code&gt;&#34;llama-cpp-python/&amp;lt;hugging_face_repo_id&amp;gt;/&amp;lt;filename&amp;gt;@&amp;lt;n_ctx&amp;gt;&#34;&lt;/code&gt;, where &lt;code&gt;n_ctx&lt;/code&gt; is an optional parameter that specifies the context size of the model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] üíæ You can create a PostgreSQL database in a few clicks at &lt;a href=&#34;https://neon.tech&#34;&gt;neon.tech&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;First, configure RAGLite with your preferred PostgreSQL or SQLite database and &lt;a href=&#34;https://docs.litellm.ai/docs/providers/openai&#34;&gt;any LLM supported by LiteLLM&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from raglite import RAGLiteConfig&#xA;&#xA;# Example &#39;remote&#39; config with a PostgreSQL database and an OpenAI LLM:&#xA;my_config = RAGLiteConfig(&#xA;    db_url=&#34;postgresql://my_username:my_password@my_host:5432/my_database&#34;&#xA;    llm=&#34;gpt-4o-mini&#34;,  # Or any LLM supported by LiteLLM.&#xA;    embedder=&#34;text-embedding-3-large&#34;,  # Or any embedder supported by LiteLLM.&#xA;)&#xA;&#xA;# Example &#39;local&#39; config with a SQLite database and a llama.cpp LLM:&#xA;my_config = RAGLiteConfig(&#xA;    db_url=&#34;sqlite:///raglite.sqlite&#34;,&#xA;    llm=&#34;llama-cpp-python/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/*Q4_K_M.gguf@8192&#34;,&#xA;    embedder=&#34;llama-cpp-python/lm-kit/bge-m3-gguf/*F16.gguf&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also configure &lt;a href=&#34;https://github.com/AnswerDotAI/rerankers&#34;&gt;any reranker supported by rerankers&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rerankers import Reranker&#xA;&#xA;# Example remote API-based reranker:&#xA;my_config = RAGLiteConfig(&#xA;    db_url=&#34;postgresql://my_username:my_password@my_host:5432/my_database&#34;&#xA;    reranker=Reranker(&#34;cohere&#34;, lang=&#34;en&#34;, api_key=COHERE_API_KEY)&#xA;)&#xA;&#xA;# Example local cross-encoder reranker per language (this is the default):&#xA;my_config = RAGLiteConfig(&#xA;    db_url=&#34;sqlite:///raglite.sqlite&#34;,&#xA;    reranker=(&#xA;        (&#34;en&#34;, Reranker(&#34;ms-marco-MiniLM-L-12-v2&#34;, model_type=&#34;flashrank&#34;)),  # English&#xA;        (&#34;other&#34;, Reranker(&#34;ms-marco-MultiBERT-L-12&#34;, model_type=&#34;flashrank&#34;)),  # Other languages&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Inserting documents&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] ‚úçÔ∏è To insert documents other than PDF, install the &lt;code&gt;pandoc&lt;/code&gt; extra with &lt;code&gt;pip install raglite[pandoc]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Next, insert some documents into the database. RAGLite will take care of the &lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/src/raglite/_markdown.py&#34;&gt;conversion to Markdown&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/src/raglite/_split_chunks.py&#34;&gt;optimal level 4 semantic chunking&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/src/raglite/_embed.py&#34;&gt;multi-vector embedding with late chunking&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Insert documents:&#xA;from pathlib import Path&#xA;from raglite import insert_document&#xA;&#xA;insert_document(Path(&#34;On the Measure of Intelligence.pdf&#34;), config=my_config)&#xA;insert_document(Path(&#34;Special Relativity.pdf&#34;), config=my_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Searching and Retrieval-Augmented Generation (RAG)&lt;/h3&gt; &#xA;&lt;p&gt;Now, you can search for chunks with vector search, keyword search, or a hybrid of the two. You can also rerank the search results with the configured reranker. And you can use any search method of your choice (&lt;code&gt;hybrid_search&lt;/code&gt; is the default) together with reranking to answer questions with RAG:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Search for chunks:&#xA;from raglite import hybrid_search, keyword_search, vector_search&#xA;&#xA;prompt = &#34;How is intelligence measured?&#34;&#xA;chunk_ids_vector, _ = vector_search(prompt, num_results=20, config=my_config)&#xA;chunk_ids_keyword, _ = keyword_search(prompt, num_results=20, config=my_config)&#xA;chunk_ids_hybrid, _ = hybrid_search(prompt, num_results=20, config=my_config)&#xA;&#xA;# Retrieve chunks:&#xA;from raglite import retrieve_chunks&#xA;&#xA;chunks_hybrid = retrieve_chunks(chunk_ids_hybrid, config=my_config)&#xA;&#xA;# Rerank chunks:&#xA;from raglite import rerank_chunks&#xA;&#xA;chunks_reranked = rerank_chunks(prompt, chunks_hybrid, config=my_config)&#xA;&#xA;# Answer questions with RAG:&#xA;from raglite import rag&#xA;&#xA;prompt = &#34;What does it mean for two events to be simultaneous?&#34;&#xA;stream = rag(prompt, config=my_config)&#xA;for update in stream:&#xA;    print(update, end=&#34;&#34;)&#xA;&#xA;# You can also pass a search method or search results directly:&#xA;stream = rag(prompt, search=hybrid_search, config=my_config)&#xA;stream = rag(prompt, search=chunks_reranked, config=my_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Computing and using an optimal query adapter&lt;/h3&gt; &#xA;&lt;p&gt;RAGLite can compute and apply an &lt;a href=&#34;https://raw.githubusercontent.com/superlinear-ai/raglite/main/src/raglite/_query_adapter.py&#34;&gt;optimal closed-form query adapter&lt;/a&gt; to the prompt embedding to improve the output quality of RAG. To benefit from this, first generate a set of evals with &lt;code&gt;insert_evals&lt;/code&gt; and then compute and store the optimal query adapter with &lt;code&gt;update_query_adapter&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Improve RAG with an optimal query adapter:&#xA;from raglite import insert_evals, update_query_adapter&#xA;&#xA;insert_evals(num_evals=100, config=my_config)&#xA;update_query_adapter(config=my_config)  # From here, simply call vector_search to use the query adapter.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. Evaluation of retrieval and generation&lt;/h3&gt; &#xA;&lt;p&gt;If you installed the &lt;code&gt;ragas&lt;/code&gt; extra, you can use RAGLite to answer the evals and then evaluate the quality of both the retrieval and generation steps of RAG using &lt;a href=&#34;https://github.com/explodinggradients/ragas&#34;&gt;Ragas&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Evaluate retrieval and generation:&#xA;from raglite import answer_evals, evaluate, insert_evals&#xA;&#xA;insert_evals(num_evals=100, config=my_config)&#xA;answered_evals_df = answer_evals(num_evals=10, config=my_config)&#xA;evaluation_df = evaluate(answered_evals_df, config=my_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;6. Serving a customizable ChatGPT-like frontend&lt;/h3&gt; &#xA;&lt;p&gt;If you installed the &lt;code&gt;chainlit&lt;/code&gt; extra, you can serve a customizable ChatGPT-like frontend with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;raglite chainlit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The application is also deployable to &lt;a href=&#34;https://docs.chainlit.io/deploy/copilot&#34;&gt;web&lt;/a&gt;, &lt;a href=&#34;https://docs.chainlit.io/deploy/slack&#34;&gt;Slack&lt;/a&gt;, and &lt;a href=&#34;https://docs.chainlit.io/deploy/teams&#34;&gt;Teams&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can specify the database URL, LLM, and embedder directly in the Chainlit frontend, or with the CLI as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;raglite chainlit \&#xA;    --db_url sqlite:///raglite.sqlite \&#xA;    --llm llama-cpp-python/bartowski/Llama-3.2-3B-Instruct-GGUF/*Q4_K_M.gguf@4096 \&#xA;    --embedder llama-cpp-python/lm-kit/bge-m3-gguf/*F16.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use an API-based LLM, make sure to include your credentials in a &lt;code&gt;.env&lt;/code&gt; file or supply them inline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;OPENAI_API_KEY=sk-... raglite chainlit --llm gpt-4o-mini --embedder text-embedding-3-large&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/01cf98d3-6ddd-45bb-8617-cf290c09f187&#34;&gt;&lt;/video&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Prerequisites&lt;/summary&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;1. Set up Git to use SSH&lt;/summary&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#generating-a-new-ssh-key&#34;&gt;Generate an SSH key&lt;/a&gt; and &lt;a href=&#34;https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account&#34;&gt;add the SSH key to your GitHub account&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Configure SSH to automatically load your SSH keys: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; ~/.ssh/config&#xA;&#xA;Host *&#xA;  AddKeysToAgent yes&#xA;  IgnoreUnknown UseKeychain&#xA;  UseKeychain yes&#xA;  ForwardAgent yes&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;2. Install Docker&lt;/summary&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.docker.com/get-started&#34;&gt;Install Docker Desktop&lt;/a&gt;. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;Linux only&lt;/em&gt;: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Export your user&#39;s user id and group id so that &lt;a href=&#34;https://github.com/moby/moby/issues/3206&#34;&gt;files created in the Dev Container are owned by your user&lt;/a&gt;: &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; ~/.bashrc&#xA;&#xA;export UID=$(id --user)&#xA;export GID=$(id --group)&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;3. Install VS Code or PyCharm&lt;/summary&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;Install VS Code&lt;/a&gt; and &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers&#34;&gt;VS Code&#39;s Dev Containers extension&lt;/a&gt;. Alternatively, install &lt;a href=&#34;https://www.jetbrains.com/pycharm/download/&#34;&gt;PyCharm&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;Optional:&lt;/em&gt; install a &lt;a href=&#34;https://www.nerdfonts.com/font-downloads&#34;&gt;Nerd Font&lt;/a&gt; such as &lt;a href=&#34;https://github.com/ryanoasis/nerd-fonts/tree/master/patched-fonts/FiraCode&#34;&gt;FiraCode Nerd Font&lt;/a&gt; and &lt;a href=&#34;https://github.com/tonsky/FiraCode/wiki/VS-Code-Instructions&#34;&gt;configure VS Code&lt;/a&gt; or &lt;a href=&#34;https://github.com/tonsky/FiraCode/wiki/Intellij-products-instructions&#34;&gt;configure PyCharm&lt;/a&gt; to use it.&lt;/li&gt; &#xA;  &lt;/ol&gt; &#xA; &lt;/details&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Development environments&lt;/summary&gt; &#xA; &lt;p&gt;The following development environments are supported:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;‚≠êÔ∏è &lt;em&gt;GitHub Codespaces&lt;/em&gt;: click on &lt;em&gt;Code&lt;/em&gt; and select &lt;em&gt;Create codespace&lt;/em&gt; to start a Dev Container with &lt;a href=&#34;https://github.com/features/codespaces&#34;&gt;GitHub Codespaces&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;‚≠êÔ∏è &lt;em&gt;Dev Container (with container volume)&lt;/em&gt;: click on &lt;a href=&#34;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/superlinear-ai/raglite&#34;&gt;Open in Dev Containers&lt;/a&gt; to clone this repository in a container volume and create a Dev Container with VS Code.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;em&gt;Dev Container&lt;/em&gt;: clone this repository, open it with VS Code, and run &lt;kbd&gt;Ctrl/‚åò&lt;/kbd&gt; + &lt;kbd&gt;‚áß&lt;/kbd&gt; + &lt;kbd&gt;P&lt;/kbd&gt; ‚Üí &lt;em&gt;Dev Containers: Reopen in Container&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;em&gt;PyCharm&lt;/em&gt;: clone this repository, open it with PyCharm, and &lt;a href=&#34;https://www.jetbrains.com/help/pycharm/using-docker-compose-as-a-remote-interpreter.html#docker-compose-remote&#34;&gt;configure Docker Compose as a remote interpreter&lt;/a&gt; with the &lt;code&gt;dev&lt;/code&gt; service.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;em&gt;Terminal&lt;/em&gt;: clone this repository, open it with your terminal, and run &lt;code&gt;docker compose up --detach dev&lt;/code&gt; to start a Dev Container in the background, and then run &lt;code&gt;docker compose exec dev zsh&lt;/code&gt; to open a shell prompt in the Dev Container.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Developing&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;This project follows the &lt;a href=&#34;https://www.conventionalcommits.org/&#34;&gt;Conventional Commits&lt;/a&gt; standard to automate &lt;a href=&#34;https://semver.org/&#34;&gt;Semantic Versioning&lt;/a&gt; and &lt;a href=&#34;https://keepachangelog.com/&#34;&gt;Keep A Changelog&lt;/a&gt; with &lt;a href=&#34;https://github.com/commitizen-tools/commitizen&#34;&gt;Commitizen&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;poe&lt;/code&gt; from within the development environment to print a list of &lt;a href=&#34;https://github.com/nat-n/poethepoet&#34;&gt;Poe the Poet&lt;/a&gt; tasks available to run on this project.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;poetry add {package}&lt;/code&gt; from within the development environment to install a run time dependency and add it to &lt;code&gt;pyproject.toml&lt;/code&gt; and &lt;code&gt;poetry.lock&lt;/code&gt;. Add &lt;code&gt;--group test&lt;/code&gt; or &lt;code&gt;--group dev&lt;/code&gt; to install a CI or development dependency, respectively.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;poetry update&lt;/code&gt; from within the development environment to upgrade all dependencies to the latest versions allowed by &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;cz bump&lt;/code&gt; to bump the package&#39;s version, update the &lt;code&gt;CHANGELOG.md&lt;/code&gt;, and create a git tag.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
</feed>