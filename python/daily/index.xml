<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-15T01:42:49Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>raminmh/liquid_time_constant_networks</title>
    <updated>2023-07-15T01:42:49Z</updated>
    <id>tag:github.com,2023-07-15:/raminmh/liquid_time_constant_networks</id>
    <link href="https://github.com/raminmh/liquid_time_constant_networks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code Repository for Liquid Time-Constant Networks (LTCs)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Liquid time-constant Networks (LTCs)&lt;/h1&gt; &#xA;&lt;p&gt;[Update] A Pytorch version together with tutorials are added to our sister repository: &lt;a href=&#34;https://github.com/mlech26l/ncps&#34;&gt;https://github.com/mlech26l/keras-ncp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official repository for LTC networks described in the paper: &lt;a href=&#34;https://arxiv.org/abs/2006.04439&#34;&gt;https://arxiv.org/abs/2006.04439&lt;/a&gt; This repository allows you to train continuous-time models with backpropagation through-time (BPTT). Available Continuous-time models are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;References&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Liquid time-constant Networks&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.04439&#34;&gt;https://arxiv.org/abs/2006.04439&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural ODEs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf&#34;&gt;https://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Continuous-time RNNs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S089360800580125X&#34;&gt;https://www.sciencedirect.com/science/article/abs/pii/S089360800580125X&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Continuous-time Gated Recurrent Units (GRU)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.04110&#34;&gt;https://arxiv.org/abs/1710.04110&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Requisites&lt;/h2&gt; &#xA;&lt;p&gt;All models were implemented and tested with TensorFlow 1.14.0 and python3 on Ubuntu 16.04 and 18.04 machines. All the following steps assume that they are executed under these conditions.&lt;/p&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;p&gt;First, we have to download all datasets by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source download_datasets.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script creates a folder &lt;code&gt;data&lt;/code&gt;, where all downloaded datasets are stored.&lt;/p&gt; &#xA;&lt;h2&gt;Training and evaluating the models&lt;/h2&gt; &#xA;&lt;p&gt;There is exactly one Python module per dataset:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hand gesture segmentation: &lt;code&gt;gesture.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Room occupancy detection: &lt;code&gt;occupancy.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Human activity recognition: &lt;code&gt;har.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Traffic volume prediction: &lt;code&gt;traffic.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ozone level forecasting: &lt;code&gt;ozone.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each script accepts the following four arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model: lstm | ctrnn | ltc | ltc_rk | ltc_ex&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--epochs: number of training epochs (default 200)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--size: number of hidden RNN units (default 32)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--log: interval of how often to evaluate validation metric (default 1)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each script trains the specified model for the given number of epochs and evaluates the validation performance after every &lt;code&gt;log&lt;/code&gt; steps. At the end of the training, the best-performing checkpoint is restored and the model is evaluated on the test set. All results are stored in the &lt;code&gt;results&lt;/code&gt; folder by appending the result to CSV file.&lt;/p&gt; &#xA;&lt;p&gt;For example, we can train and evaluate the CT-RNN by executing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 har.py --model ctrnn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the script is finished there should be a file &lt;code&gt;results/har/ctrnn_32.csv&lt;/code&gt; created, containing the following columns:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;best epoch&lt;/code&gt;: Epoch number that achieved the best validation metric&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train loss&lt;/code&gt;: Training loss achieved at the best epoch&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;train accuracy&lt;/code&gt;: Training metric achieved at the best epoch&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;valid loss&lt;/code&gt;: Validation loss achieved at the best epoch&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;valid accuracy&lt;/code&gt;: Best validation metric achieved during training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;test loss&lt;/code&gt;: Loss on the test set&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;test accuracy&lt;/code&gt;: Metric on the test set&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hyperparameters&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Minibatch size&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;Number of training samples over which the gradient descent update is computed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning rate&lt;/td&gt; &#xA;   &lt;td&gt;0.001/0.02&lt;/td&gt; &#xA;   &lt;td&gt;0.01-0.02 for LTC, 0.001 for all other models.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hidden units&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;Number of hidden units of each model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Optimizer&lt;/td&gt; &#xA;   &lt;td&gt;Adam&lt;/td&gt; &#xA;   &lt;td&gt;See (Kingma and Ba, 2014)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;beta_1&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;   &lt;td&gt;Parameter of the Adam method&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;beta_2&lt;/td&gt; &#xA;   &lt;td&gt;0.999&lt;/td&gt; &#xA;   &lt;td&gt;Parameter of the Adam method&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;epsilon&lt;/td&gt; &#xA;   &lt;td&gt;1e-08&lt;/td&gt; &#xA;   &lt;td&gt;Epsilon-hat parameter of the Adam method&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Number of epochs&lt;/td&gt; &#xA;   &lt;td&gt;200&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of training epochs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BPTT length&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;Backpropagation through time length in time-steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ODE solver sreps&lt;/td&gt; &#xA;   &lt;td&gt;1/6&lt;/td&gt; &#xA;   &lt;td&gt;relative to input sampling period&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Validation evaluation interval&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Interval of training epochs when the metrics on the validation are evaluated&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Trajectory Length Analysis&lt;/h1&gt; &#xA;&lt;p&gt;Run the &lt;code&gt;main.m&lt;/code&gt; file to get trajectory length results for the desired setting tuneable in the code.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>assafelovic/gpt-researcher</title>
    <updated>2023-07-15T01:42:49Z</updated>
    <id>tag:github.com,2023-07-15:/assafelovic/gpt-researcher</id>
    <link href="https://github.com/assafelovic/gpt-researcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPT based autonomous agent that does online comprehensive research on any given topic&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ” GPT Researcher&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tavily.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Official%20Website-tavily.com-blue?style=flat&amp;amp;logo=world&amp;amp;logoColor=white&#34; alt=&#34;Official Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/rqw8dnM8&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/rqw8dnM8?style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/assafelovic/gpt-researcher?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/assaf_elovic&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/assaf_elovic?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPT Researcher is an autonomous agent designed for comprehensive online research on a variety of tasks.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The agent can produce detailed, factual and unbiased research reports, with customization options for focusing on relevant resources, outlines, and lessons. Inspired by &lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;AutoGPT&lt;/a&gt; and the recent &lt;a href=&#34;https://arxiv.org/abs/2305.04091&#34;&gt;Plan-and-Solve&lt;/a&gt; paper, GPT Researcher addresses issues of speed and determinism, offering a more stable performance and increased speed through parallelized agent work, as opposed to synchronous operations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our mission is to empower individuals and organizations with accurate, unbiased, and factual information by leveraging the power of AI.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why GPT Researcher?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To form objective conclusions for manual research tasks can take time, sometimes weeks to find the right resources and information.&lt;/li&gt; &#xA; &lt;li&gt;Current LLMs are trained on past and outdated information, with heavy risks of hallucinations, making them almost irrelevant for research tasks.&lt;/li&gt; &#xA; &lt;li&gt;Solutions that enable web search (such as ChatGPT + Web Plugin), only consider limited resources that in some cases result in superficial conclusions or biased answers.&lt;/li&gt; &#xA; &lt;li&gt;Using only a selection of resources can create bias in determining the right conclusions for research questions or tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The main idea is to run &#34;planner&#34; and &#34;execution&#34; agents, whereas the planner generates questions to research, and the execution agents seek the most related information based on each generated research question. Finally, the planner filters and aggregates all related information and creates a research report. The agents leverage both gpt3.5-turbo-16k and gpt-4 to complete a research task.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; height=&#34;500&#34; src=&#34;https://cowriter-images.s3.amazonaws.com/arch.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More specifcally:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate a set of research questions that together form an objective opinion on any given task.&lt;/li&gt; &#xA; &lt;li&gt;For each research question, trigger a crawler agent that scrapes online resources for information relevant to the given task.&lt;/li&gt; &#xA; &lt;li&gt;For each scraped resources, summarize based on relevant information and keep track of its sources.&lt;/li&gt; &#xA; &lt;li&gt;Finally, filter and aggregate all summarized sources and generate a final research report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/a00c89a6-a295-4dd0-b58d-098a31c40fda&#34;&gt;https://github.com/assafelovic/gpt-researcher/assets/13554167/a00c89a6-a295-4dd0-b58d-098a31c40fda&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea&#34;&gt;How to Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8&#34;&gt;Live Demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ“ Generate research, outlines, resources and lessons reports&lt;/li&gt; &#xA; &lt;li&gt;ğŸŒ Aggregates over 20 web sources per research to form objective and factual conclusions&lt;/li&gt; &#xA; &lt;li&gt;ğŸ–¥ï¸ Includes an easy-to-use web interface (HTML/CSS/JS)&lt;/li&gt; &#xA; &lt;li&gt;ğŸ” Scrapes web sources with javascript support&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“‚ Keeps track and context of visited and used web sources&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“„ Export research reports to PDF and more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 0&lt;/strong&gt; - Install Python 3.11 or later. &lt;a href=&#34;https://www.tutorialsteacher.com/python/install-python&#34;&gt;See here&lt;/a&gt; for a step-by-step guide.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - Download the project&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/assafelovic/gpt-researcher.git&#xA;$ cd gpt-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Install dependencies&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Create .env file with your OpenAI Key or simply export it&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export OPENAI_API_KEY={Your API Key here}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Run the agent with FastAPI&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ uvicorn main:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; - Go to &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; on any browser and enjoy researching!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;update:&lt;/strong&gt; if you are having issues with weasyprint, please visit their website and follow the installation instructions: &lt;a href=&#34;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&#34;&gt;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Try it with Docker&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - Install Docker&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Follow instructions at &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;https://docs.docker.com/engine/install/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Create .env file with your OpenAI Key or simply export it&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export OPENAI_API_KEY={Your API Key here}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Run the application&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Go to &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; on any browser and enjoy researching!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;update:&lt;/strong&gt; if you are having issues with weasyprint, please visit their website and follow the installation instructions: &lt;a href=&#34;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&#34;&gt;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ›¡ Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project, GPT Researcher, is an experimental application and is provided &#34;as-is&#34; without any warranty, express or implied. We are sharing codes for academic purposes under the MIT education license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.&lt;/p&gt; &#xA;&lt;p&gt;Our view on unbiased research claims:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The whole point of our scraping system is to reduce incorrect fact. How? The more sites we scrape the less chances of incorrect data. We are scraping 20 per research, the chances that they are all wrong is extremely low.&lt;/li&gt; &#xA; &lt;li&gt;We do not aim to eliminate biases; we aim to reduce it as much as possible. &lt;strong&gt;We are here as a community to figure out the most effective human/llm interactions.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that the use of the GPT-4 language model can be expensive due to its token usage.&lt;/strong&gt; By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”§ Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re constantly working to provide a more stable version. In the meantime, see here for known issues:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;cannot load library &#39;gobject-2.0-0&#39;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The issue relates to the library WeasyPrint (which is used to generate PDFs from the research report). Please follow this guide to resolve it: &lt;a href=&#34;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&#34;&gt;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Error processing the url&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re using &lt;a href=&#34;https://www.selenium.dev&#34;&gt;Selenium&lt;/a&gt; for site scraping. Some sites fail to be scraped. In these cases, restart and try running again.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pengzhile/pandora-cloud-serverless</title>
    <updated>2023-07-15T01:42:49Z</updated>
    <id>tag:github.com,2023-07-15:/pengzhile/pandora-cloud-serverless</id>
    <link href="https://github.com/pengzhile/pandora-cloud-serverless" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ä¸€ä¸ªç®€å•çš„ä»“åº“ï¼Œç”¨äºServerlesséƒ¨ç½²Pandora-Cloudã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pandora-Cloud-Serverless&lt;/h1&gt; &#xA;&lt;p&gt;ä¸€ä¸ªç®€å•çš„ä»“åº“ï¼Œç”¨äºServerlesséƒ¨ç½²Pandora-Cloudã€‚&lt;/p&gt; &#xA;&lt;h2&gt;Vercel&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fpengzhile%2Fpandora-cloud-serverless&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;éƒ¨ç½²ç¤ºä¾‹ &lt;a href=&#34;https://pandora-cloud-demo.vercel.app&#34;&gt;https://pandora-cloud-demo.vercel.app&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Railway&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://railway.app/template/AWK0rN&#34;&gt;&lt;img src=&#34;https://railway.app/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Gitpod&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/pengzhile/pandora-cloud-serverless&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¨è &lt;code&gt;VS Code&lt;/code&gt; æ‰“å¼€ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ‰“å¼€åå¯åœ¨ &lt;code&gt;PORTS&lt;/code&gt; é€‰é¡¹å¡çœ‹åˆ°åœ°å€ï¼Œç‚¹å‡»å³å¯è®¿é—®ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¯è®¾ç½®åœ°å€æ˜¯å¦å…¬å¼€ï¼Œç‚¹å‡»å°é”å›¾æ ‡å³å¯è®¾ç½®ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Zeabur&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç•¥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;è¯´æ˜&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä»¥ä¸ŠServerlesså¹³å°ä½¿ç”¨ï¼Œè¿˜è¯·é˜…è¯»å„è‡ªçš„å®˜æ–¹æ–‡æ¡£ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯¦ç»†ä»£ç çœ‹ &lt;code&gt;entrypoint.py&lt;/code&gt;ï¼Œå†…å«ä¸€äº›ç¯å¢ƒå˜é‡ï¼Œçœ‹ä¸æ‡‚ä¸å»ºè®®æ”¹åŠ¨ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ›´å¤šé—®é¢˜å¯ä»¥æŸ¥çœ‹ä¸»é¡¹ç›®ï¼š&lt;a href=&#34;https://github.com/pengzhile/pandora&#34;&gt;Pandora&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>