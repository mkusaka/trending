<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-02T01:31:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NUS-HPC-AI-Lab/OpenDiT</title>
    <updated>2024-03-02T01:31:14Z</updated>
    <id>tag:github.com,2024-03-02:/NUS-HPC-AI-Lab/OpenDiT</id>
    <link href="https://github.com/NUS-HPC-AI-Lab/OpenDiT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenDiT: An Easy, Fast and Memory-Efficient System for DiT Training and Inference&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;200px&#34; alt=&#34;OpenDiT&#34; src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/figure/logo.png?raw=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;b&gt;&lt;big&gt;An Easy, Fast and Memory-Efficient System for DiT Training and Inference&lt;/big&gt;&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/OpenDiT&#34;&gt;[Homepage]&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/yXF4n8Et&#34;&gt;[Discord]&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/figure/wechat.jpg&#34;&gt;[WeChat]&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/YangYou1991/status/1762447718105170185&#34;&gt;[Twitter]&lt;/a&gt; | &lt;a href=&#34;https://zhuanlan.zhihu.com/p/684457582&#34;&gt;[Zhihu]&lt;/a&gt; | &lt;a href=&#34;https://mp.weixin.qq.com/s/IBb9vlo8hfYKrj9ztxkhjg&#34;&gt;[Media]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Latest News üî•&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/03/01] Support DiT-based Latte for text-to-video generation.&lt;/li&gt; &#xA; &lt;li&gt;[2024/02/27] Officially release OpenDiT: An Easy, Fast and Memory-Efficent System for DiT Training and Inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;OpenDiT is an open-source project that provides a high-performance implementation of Diffusion Transformer (DiT) powered by Colossal-AI, specifically designed to enhance the efficiency of training and inference for DiT applications, including text-to-video generation and text-to-image generation.&lt;/p&gt; &#xA;&lt;p&gt;OpenDiT boasts the performance by the following techniques:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Up to 80% speedup and 50% memory reduction on GPU &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Kernel optimization including FlashAttention, Fused AdaLN, and Fused layernorm kernel.&lt;/li&gt; &#xA;   &lt;li&gt;Hybrid parallelism methods including ZeRO, Gemini, and DDP. Also, sharding the ema model further reduces the memory cost.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;FastSeq: A novel sequence parallelism method &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specially designed for DiT-like workloads where the activation size is large but the parameter size is small.&lt;/li&gt; &#xA;   &lt;li&gt;Up to 48% communication save for intra-node sequence parallel.&lt;/li&gt; &#xA;   &lt;li&gt;Break the memory limitation of a single GPU and reduce the overall training and inference time.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Ease of use &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Huge performance improvement gains with a few line changes&lt;/li&gt; &#xA;   &lt;li&gt;Users do not need to know the implementation of distributed training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Complete pipeline of text-to-image and text-to-video generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Researchers and engineers can easily use and adapt our pipeline to real-world applications without modifying the parallel part.&lt;/li&gt; &#xA;   &lt;li&gt;Verify the accuracy of OpenDiT with text-to-image training on ImageNet and release checkpoint.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;600px&#34; alt=&#34;end2end&#34; src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/figure/end2end.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Authors: &lt;a href=&#34;https://oahzxl.github.io/&#34;&gt;Xuanlei Zhao&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/zhongkai-zhao-kk2000/&#34;&gt;Zhongkai Zhao&lt;/a&gt;, &lt;a href=&#34;https://maruyamaaya.github.io/&#34;&gt;Ziming Liu&lt;/a&gt;, &lt;a href=&#34;https://github.com/ht-zhou&#34;&gt;Haotian Zhou&lt;/a&gt;, &lt;a href=&#34;https://fazzie-key.cool/about/index.html&#34;&gt;Qianli Ma&lt;/a&gt;, &lt;a href=&#34;https://www.comp.nus.edu.sg/~youy/&#34;&gt;Yang You&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;More features are coming soon!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Prerequisites:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.10&lt;/li&gt; &#xA; &lt;li&gt;PyTorch &amp;gt;= 1.13 (We recommend to use a &amp;gt;2.0 version)&lt;/li&gt; &#xA; &lt;li&gt;CUDA &amp;gt;= 11.6&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We strongly recommend using Anaconda to create a new environment (Python &amp;gt;= 3.10) to run our examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n opendit python=3.10 -y&#xA;conda activate opendit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install ColossalAI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/hpcaitech/ColossalAI.git&#xA;cd ColossalAI&#xA;git checkout adae123df3badfb15d044bd416f0cf29f250bc86&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install OpenDiT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/oahzxl/OpenDiT&#xA;cd OpenDiT&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Optional but recommended) Install libraries for training &amp;amp; inference speed up:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Install Triton for fused adaln kernel&#xA;pip install triton&#xA;&#xA;# Install FlashAttention&#xA;pip install flash-attn&#xA;&#xA;# Install apex for fused layernorm kernel&#xA;git clone https://github.com/NVIDIA/apex.git&#xA;cd apex&#xA;git checkout 741bdf50825a97664db08574981962d66436d16a&#xA;pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings &#34;--build-option=--cpp_ext&#34; --config-settings &#34;--build-option=--cuda_ext&#34; ./ --global-option=&#34;--cuda_ext&#34; --global-option=&#34;--cpp_ext&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Image&lt;/h3&gt; &#xA;&lt;p&gt;&lt;b&gt;Training.&lt;/b&gt; You can train the DiT model on CIFAR10 by executing the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Use script&#xA;bash train_img.sh&#xA;# Use command line&#xA;torchrun --standalone --nproc_per_node=2 train.py \&#xA;    --model DiT-XL/2 \&#xA;    --batch_size 2 \&#xA;    --num_classes 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We disable all speedup methods by default. Here are details of some key arguments for training:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--nproc_per_node&lt;/code&gt;: The GPU number you want to use for the current node.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--plugin&lt;/code&gt;: The booster plugin used by ColossalAI, &lt;code&gt;zero2&lt;/code&gt; and &lt;code&gt;ddp&lt;/code&gt; are supported. The default value is &lt;code&gt;zero2&lt;/code&gt;. Recommend to enable &lt;code&gt;zero2&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--mixed_precision&lt;/code&gt;: The data type for mixed precision training. The default value is &lt;code&gt;bf16&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--grad_checkpoint&lt;/code&gt;: Whether enable the gradient checkpointing. This saves the memory cost during training process. The default value is &lt;code&gt;False&lt;/code&gt;. Recommend to disable it when memory is enough.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--enable_layernorm_kernel&lt;/code&gt;: Whether enable the layernorm kernel optimization. This speeds up the training process. The default value is &lt;code&gt;False&lt;/code&gt;. Recommend to enable it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--enable_flashattn&lt;/code&gt;: Whether enable the FlashAttention. This speeds up the training process. The default value is &lt;code&gt;False&lt;/code&gt;. Recommend to enable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--enable_modulate_kernel&lt;/code&gt;: Whether enable the modulate kernel optimization. This speeds up the training process. The default value is &lt;code&gt;False&lt;/code&gt;. This kernel will cause NaN under some circumstances. So we recommend to disable it for now.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sequence_parallel_size&lt;/code&gt;: The sequence parallelism size. Will enable sequence parallelism when setting a value &amp;gt; 1. The default value is 1. Recommend to disable it if memory is enough.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--load&lt;/code&gt;: Load previous saved checkpoint dir and continue training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_classes&lt;/code&gt;: Label class number. Should be 10 for CIFAR10 and 1000 for ImageNet. Only used for label-to-image generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details on the configuration of the training process, please visit our code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;Multi-Node Training.&lt;/b&gt;&lt;/p&gt; &#xA;&lt;p&gt;To train OpenDiT on multiple nodes, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;colossalai run --nproc_per_node 8 --hostfile hostfile train.py \&#xA;    --model DiT-XL/2 \&#xA;    --batch_size 2 \&#xA;    --num_classes 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you need to create &lt;code&gt;hostfile&lt;/code&gt; under the current dir. It should contain all IP address of your nodes and you need to make sure all nodes can be connected without password by ssh. An example of hostfile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;111.111.111.111 # ip of node1&#xA;222.222.222.222 # ip of node2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;b&gt;Inference.&lt;/b&gt; You can perform inference using DiT model as follows. You need to replace the checkpoint path to your own trained model. Or you can download &lt;a href=&#34;https://github.com/facebookresearch/DiT?tab=readme-ov-file#sampling--&#34;&gt;official&lt;/a&gt; or &lt;a href=&#34;https://drive.google.com/file/d/1P4t2V3RDNcoCiEkbVWAjNetm3KC_4ueI/view?usp=drive_link&#34;&gt;our&lt;/a&gt; checkpoint for inference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Use script&#xA;bash sample_img.sh&#xA;# Use command line&#xA;python sample.py \&#xA;    --model DiT-XL/2 \&#xA;    --image_size 256 \&#xA;    --num_classes 10 \&#xA;    --ckpt ckpt_path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here are details of some addtional key arguments for inference:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--ckpt&lt;/code&gt;: The weight of ema model &lt;code&gt;ema.pt&lt;/code&gt;. To check your training progress, it can also be our saved base model &lt;code&gt;epochXX-global_stepXX/model&lt;/code&gt;, it will produce better results than ema in early training stage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_classes&lt;/code&gt;: Label class number. Should be 10 for CIFAR10, and 1000 for ImageNet (including official and our checkpoint).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video&lt;/h3&gt; &#xA;&lt;p&gt;&lt;b&gt;Training.&lt;/b&gt; We current support &lt;code&gt;VDiT&lt;/code&gt; and &lt;code&gt;Latte&lt;/code&gt; for video generation. VDiT adopts DiT structure and use video as inputs data. Latte further use more efficient spatial &amp;amp; temporal blocks based on VDiT (not exactly align with origin &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Our video training pipeline is a faithful implementation, and we encourage you to explore your own strategies using OpenDiT. You can train the video DiT model by executing the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# train with scipt&#xA;bash train_video.sh&#xA;# train with command line&#xA;# model can also be Latte-XL/1x2x2&#xA;torchrun --standalone --nproc_per_node=2 train.py \&#xA;    --model VDiT-XL/1x2x2 \&#xA;    --use_video \&#xA;    --data_path ./videos/demo.csv \&#xA;    --batch_size 1 \&#xA;    --num_frames 16 \&#xA;    --image_size 256 \&#xA;    --frame_interval 3&#xA;&#xA;# preprocess&#xA;# our code read video from csv using our toy data&#xA;# we provide a code to transfer ucf101 to csv format&#xA;python preprocess.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script shares the same speedup methods as we have shown in the image training part. For more details of the configuration of the training process, please visit our code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;Inference.&lt;/b&gt; You can perform video inference using DiT model as follows. We are still working on the video ckpt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Use script&#xA;bash sample_video.sh&#xA;# Use command line&#xA;# model can also be Latte-XL/1x2x2&#xA;python sample.py \&#xA;    --model VDiT-XL/1x2x2 \&#xA;    --use_video \&#xA;    --ckpt ckpt_path \&#xA;    --num_frames 16 \&#xA;    --image_size 256 \&#xA;    --frame_interval 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inference tips: 1) EMA model requires quite long time to converge and produce meaningful results. So you can sample base model (&lt;code&gt;--ckpt /epochXX-global_stepXX/model&lt;/code&gt;) instead of ema model (&lt;code&gt;--ckpt /epochXX-global_stepXX/ema.pt&lt;/code&gt;) to check your training process. But ema model should be your final result. 2) Modify the text condition in &lt;code&gt;sample.py&lt;/code&gt; which aligns with your datasets helps to produce better results in the early stage of training.&lt;/p&gt; &#xA;&lt;h2&gt;FastSeq&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/figure/fastseq_overview.png&#34; alt=&#34;fastseq_overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the realm of visual generation models, such as DiT, sequence parallelism is indispensable for effective long-sequence training and low-latency inference. Two key features can summarize the distinctive nature of these tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The model parameter is smaller compared with LLMs, but the sequence can be very long, making communication a bottleneck.&lt;/li&gt; &#xA; &lt;li&gt;As the model size is relatively small, it only needs sequence parallelism within a node.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;However, existing methods like DeepSpeed-Ulysses and Megatron-LM Sequence Parallelism face limitations when applied to such tasks. They either introduce excessive sequence communication or lack efficiency in handling small-scale sequence parallelism.&lt;/p&gt; &#xA;&lt;p&gt;To this end, we present FastSeq, a novel sequence parallelism for large sequences and small-scale parallelism. Our method focuses on minimizing sequence communication by employing only two communication operators for every transformer layer. We leverage AllGather to enhance communication efficiency, and we strategically employ an async ring to overlap AllGather communication with qkv computation, further optimizing performance.&lt;/p&gt; &#xA;&lt;p&gt;Here are the results of our experiments, more results will be coming soon:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/figure/fastseq_exp.png&#34; alt=&#34;fastseq_exp&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;DiT Reproduction Result&lt;/h2&gt; &#xA;&lt;p&gt;We have trained DiT using the origin method with OpenDiT to verify our accuracy. We have trained the model from scratch on ImageNet for 80k steps on 8xA100. Here are some results generated by our trained DiT:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/figure/dit_results.png&#34; alt=&#34;Results&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our loss also aligns with the results listed in the paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/figure/dit_loss.png&#34; alt=&#34;Loss&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To reproduce our results, you need to change the dataset in &lt;code&gt;train_img.py&lt;/code&gt; and execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --standalone --nproc_per_node=8 train.py \&#xA;    --model DiT-XL/2 \&#xA;    --batch_size 180 \&#xA;    --enable_layernorm_kernel \&#xA;    --enable_flashattn \&#xA;    --mixed_precision bf16 \&#xA;    --num_classes 1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We extend our gratitude to &lt;a href=&#34;https://zhengzangw.github.io/&#34;&gt;Zangwei Zheng&lt;/a&gt; for providing valuable insights into algorithms and aiding in the development of the video pipeline. Additionally, we acknowledge &lt;a href=&#34;https://shenggan.github.io/&#34;&gt;Shenggan Cheng&lt;/a&gt; for his guidance on code optimization and parallelism. Our appreciation also goes to &lt;a href=&#34;https://xuefuzhao.github.io/&#34;&gt;Fuzhao Xue&lt;/a&gt;, &lt;a href=&#34;https://littlepure2333.github.io/home/&#34;&gt;Shizun Wang&lt;/a&gt;, &lt;a href=&#34;https://ycgu.site/&#34;&gt;Yuchao Gu&lt;/a&gt;, &lt;a href=&#34;https://franklee.xyz/&#34;&gt;Shenggui Li&lt;/a&gt;, and &lt;a href=&#34;https://haofanwang.github.io/&#34;&gt;Haofan Wang&lt;/a&gt; for their invaluable advice and contributions.&lt;/p&gt; &#xA;&lt;p&gt;This codebase borrows from &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;Meta&#39;s DiT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter problems using OpenDiT or have a feature request, feel free to create an issue! We also welcome pull requests from the community.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhao2024opendit,&#xA;  author = {Xuanlei Zhao, Zhongkai Zhao, Ziming Liu, Haotian Zhou, Qianli Ma, and Yang You},&#xA;  title = {OpenDiT: An Easy, Fast and Memory-Efficient System for DiT Training and Inference},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/NUS-HPC-AI-Lab/OpenDiT}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mbzuai-oryx/MobiLlama</title>
    <updated>2024-03-02T01:31:14Z</updated>
    <id>tag:github.com,2024-03-02:/mbzuai-oryx/MobiLlama</id>
    <link href="https://github.com/mbzuai-oryx/MobiLlama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MobiLlama : Small Language Model tailored for edge devices&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üì±ü¶ô MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/MobiLlama/main/images/MobileLLaMa.png&#34; height=&#34;400px&#34; alt=&#34;Oryx MobiLLama&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.imgur.com/waxVImv.png&#34; alt=&#34;Oryx MobiLLama&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/mbzuai-oryx/MobiLlama/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=flvl5YQAAAAJ&amp;amp;hl=en&#34;&gt;Omkar Thawakar&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ashmal-vayani/&#34;&gt;Ashmal Vayani&lt;/a&gt;, &lt;a href=&#34;https://salman-h-khan.github.io/&#34;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=bZ3YBRcAAAAJ&#34;&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;authuser=1&amp;amp;user=_KlvMVoAAAAJ&#34;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=lkWfR08AAAAJ&amp;amp;hl=en&#34;&gt;Michael Felsberg&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=wjBD1dkAAAAJ&amp;amp;hl=en&#34;&gt;Timothy Baldwin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=5pKTRxEAAAAJ&amp;amp;hl=en&#34;&gt;Eric Xing&lt;/a&gt; and &lt;a href=&#34;https://sites.google.com/view/fahadkhans/home&#34;&gt;Fahad Khan&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;strong&gt;Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE&lt;/strong&gt; and LinkoÃàping University, Sweden&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.16840&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;paper&#34;&gt;&lt;/a&gt; ü§ó &lt;a href=&#34;https://huggingface.co/collections/MBZUAI/mobillama-65dd4182d588c91e8230332e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/HuggingFace-Page-F9D371&#34; alt=&#34;HuggingFace&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://f7ad4fb8ab0fcefaba.gradio.live/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gradio-Demo-red&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üì¢ Latest Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feb-26-24&lt;/strong&gt;- Arxiv Preprint is released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feb-25-24&lt;/strong&gt;- Code (Training and Evaluation scripts) is released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feb-25-24&lt;/strong&gt;- Final pre-trained models (including intermediate checkpoints) and chat version along with online demo links released!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;Bigger the better&lt;/code&gt; has been the predominant trend in recent Large Language Models (LLMs) development. However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency. These requisites are crucial for privacy, security, and sustainable deployment. This paper explores the &lt;code&gt;less is more&lt;/code&gt; paradigm by addressing the challenge of designing accurate yet efficient Small Language Models (SLMs) for resource constrained devices. Our primary contribution is the introduction of an accurate and fully transparent open-source 0.5 billion (0.5B) parameter SLM, named &lt;code&gt;MobiLlama&lt;/code&gt;, catering to the specific needs of resource-constrained computing with an emphasis on enhanced performance with reduced resource demands. &lt;code&gt;MobiLlama&lt;/code&gt; is a SLM design that initiates from a larger model and applies a careful parameter sharing scheme to reduce both the pre-training and the deployment cost.&lt;/p&gt; &#xA;&lt;h2&gt;‚ö° Model Download&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;Link Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobiLlama-05B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/MobiLlama-05B&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobiLlama-08B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/MobiLlama-08B&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobiLlama-1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/MobiLlama-1B&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobiLlama-05B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/MobiLlama-05B-Chat&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobiLlama-1B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/MBZUAI/MobiLlama-1B-Chat&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Generation with MobiLlama&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mbzuai-oryx/MobiLlama/main/images/mobillama_generation.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Model Description&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model type:&lt;/strong&gt; Language model designed using the architecture of LLaMA-7B&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language(s) (NLP):&lt;/strong&gt; English&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Apache 2.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resources for more information:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/MobiLlama&#34;&gt;Training Code&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/LLM360/amber-data-prep&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;&#34;&gt;Metrics&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/LLM360/AmberDatasets&#34;&gt;Fully processed Amber pretraining data&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Loading MobiLlama&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;MBZUAI/MobiLlama-05B&#34;, trust_remote_code=True)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;MBZUAI/MobiLlama-05B&#34;, trust_remote_code=True)&#xA;&#xA;model.to(&#39;cuda&#39;)&#xA;text = &#34;I was walking towards the river when &#34;&#xA;input_ids = tokenizer(text, return_tensors=&#34;pt&#34;).to(&#39;cuda&#39;).input_ids&#xA;outputs = model.generate(input_ids, max_length=1000, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id)&#xA;print(tokenizer.batch_decode(outputs[:, input_ids.shape[1]:-1])[0].strip())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Load intermediate Checkpoints&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#34;MBZUAI/MobiLlama-05B&#34;, revision=&#34;ckpt_352&#34;, trust_remote_code=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All the intermediate checkpoints are available from ckpt_100 to ckpt_358.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Download the preprocessed Amber data from &lt;a href=&#34;https://huggingface.co/datasets/LLM360/AmberDatasets&#34;&gt;huggingface&lt;/a&gt;. The entire training data has 360 chunks totalling the size of ~8 TB. Amber dataset contains total 1.2 Trillion tokens with gathered from different data sources shown below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Subset&lt;/th&gt; &#xA;   &lt;th&gt;Tokens (Billion)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arxiv&lt;/td&gt; &#xA;   &lt;td&gt;30.00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Book&lt;/td&gt; &#xA;   &lt;td&gt;28.86&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;C4&lt;/td&gt; &#xA;   &lt;td&gt;197.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Refined-Web&lt;/td&gt; &#xA;   &lt;td&gt;665.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder&lt;/td&gt; &#xA;   &lt;td&gt;291.92&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StackExchange&lt;/td&gt; &#xA;   &lt;td&gt;21.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wikipedia&lt;/td&gt; &#xA;   &lt;td&gt;23.90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Total&lt;/td&gt; &#xA;   &lt;td&gt;1259.13&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First install &lt;a href=&#34;https://pytorch.org&#34;&gt;PyTorch&lt;/a&gt; according to the instructions specific to your operating system.&lt;/p&gt; &#xA;&lt;p&gt;To install from source (recommended for training/fine-tuning) run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n mobillama python=3.10&#xA;conda activate mibillama&#xA;git clone https://github.com/mbzuai-oryx/MobiLlama.git&#xA;cd MobiLlama&#xA;pip install -r  requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;pretrain&lt;/h2&gt; &#xA;&lt;p&gt;For MobiLlama (using 20 nodes of A100 80GB GPUS)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sbatch pretrain.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;large-base&lt;/code&gt; use main_largebase.py in L:11 of pretrain.sh&lt;/p&gt; &#xA;&lt;h2&gt;üîé Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We used &lt;a href=&#34;https://github.com/LLM360/Analysis360&#34;&gt;Analysis-360&lt;/a&gt; to evaluate our model on different llm benchmarks.&lt;/p&gt; &#xA;&lt;h2&gt;üìä Results&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;#Params&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Truthfulqa&lt;/th&gt; &#xA;   &lt;th&gt;MMLU&lt;/th&gt; &#xA;   &lt;th&gt;Arc_C&lt;/th&gt; &#xA;   &lt;th&gt;CrowsPairs&lt;/th&gt; &#xA;   &lt;th&gt;piqa&lt;/th&gt; &#xA;   &lt;th&gt;race&lt;/th&gt; &#xA;   &lt;th&gt;siqa&lt;/th&gt; &#xA;   &lt;th&gt;winogrande&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-neo-125m&lt;/td&gt; &#xA;   &lt;td&gt;0.15B&lt;/td&gt; &#xA;   &lt;td&gt;30.26&lt;/td&gt; &#xA;   &lt;td&gt;45.58&lt;/td&gt; &#xA;   &lt;td&gt;25.97&lt;/td&gt; &#xA;   &lt;td&gt;22.95&lt;/td&gt; &#xA;   &lt;td&gt;61.55&lt;/td&gt; &#xA;   &lt;td&gt;62.46&lt;/td&gt; &#xA;   &lt;td&gt;27.56&lt;/td&gt; &#xA;   &lt;td&gt;40.33&lt;/td&gt; &#xA;   &lt;td&gt;51.78&lt;/td&gt; &#xA;   &lt;td&gt;40.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tiny-starcoder&lt;/td&gt; &#xA;   &lt;td&gt;0.17B&lt;/td&gt; &#xA;   &lt;td&gt;28.17&lt;/td&gt; &#xA;   &lt;td&gt;47.68&lt;/td&gt; &#xA;   &lt;td&gt;26.79&lt;/td&gt; &#xA;   &lt;td&gt;20.99&lt;/td&gt; &#xA;   &lt;td&gt;49.68&lt;/td&gt; &#xA;   &lt;td&gt;52.55&lt;/td&gt; &#xA;   &lt;td&gt;25.45&lt;/td&gt; &#xA;   &lt;td&gt;38.28&lt;/td&gt; &#xA;   &lt;td&gt;51.22&lt;/td&gt; &#xA;   &lt;td&gt;37.86&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cerebras-gpt-256m&lt;/td&gt; &#xA;   &lt;td&gt;0.26B&lt;/td&gt; &#xA;   &lt;td&gt;28.99&lt;/td&gt; &#xA;   &lt;td&gt;45.98&lt;/td&gt; &#xA;   &lt;td&gt;26.83&lt;/td&gt; &#xA;   &lt;td&gt;22.01&lt;/td&gt; &#xA;   &lt;td&gt;60.52&lt;/td&gt; &#xA;   &lt;td&gt;61.42&lt;/td&gt; &#xA;   &lt;td&gt;27.46&lt;/td&gt; &#xA;   &lt;td&gt;40.53&lt;/td&gt; &#xA;   &lt;td&gt;52.49&lt;/td&gt; &#xA;   &lt;td&gt;40.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;opt-350m&lt;/td&gt; &#xA;   &lt;td&gt;0.35B&lt;/td&gt; &#xA;   &lt;td&gt;36.73&lt;/td&gt; &#xA;   &lt;td&gt;40.83&lt;/td&gt; &#xA;   &lt;td&gt;26.02&lt;/td&gt; &#xA;   &lt;td&gt;23.55&lt;/td&gt; &#xA;   &lt;td&gt;64.12&lt;/td&gt; &#xA;   &lt;td&gt;64.74&lt;/td&gt; &#xA;   &lt;td&gt;29.85&lt;/td&gt; &#xA;   &lt;td&gt;41.55&lt;/td&gt; &#xA;   &lt;td&gt;52.64&lt;/td&gt; &#xA;   &lt;td&gt;42.22&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;megatron-gpt2-345m&lt;/td&gt; &#xA;   &lt;td&gt;0.38B&lt;/td&gt; &#xA;   &lt;td&gt;39.18&lt;/td&gt; &#xA;   &lt;td&gt;41.51&lt;/td&gt; &#xA;   &lt;td&gt;24.32&lt;/td&gt; &#xA;   &lt;td&gt;24.23&lt;/td&gt; &#xA;   &lt;td&gt;64.82&lt;/td&gt; &#xA;   &lt;td&gt;66.87&lt;/td&gt; &#xA;   &lt;td&gt;31.19&lt;/td&gt; &#xA;   &lt;td&gt;40.28&lt;/td&gt; &#xA;   &lt;td&gt;52.96&lt;/td&gt; &#xA;   &lt;td&gt;42.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LiteLlama&lt;/td&gt; &#xA;   &lt;td&gt;0.46B&lt;/td&gt; &#xA;   &lt;td&gt;38.47&lt;/td&gt; &#xA;   &lt;td&gt;41.59&lt;/td&gt; &#xA;   &lt;td&gt;26.17&lt;/td&gt; &#xA;   &lt;td&gt;24.91&lt;/td&gt; &#xA;   &lt;td&gt;62.90&lt;/td&gt; &#xA;   &lt;td&gt;67.73&lt;/td&gt; &#xA;   &lt;td&gt;28.42&lt;/td&gt; &#xA;   &lt;td&gt;40.27&lt;/td&gt; &#xA;   &lt;td&gt;49.88&lt;/td&gt; &#xA;   &lt;td&gt;42.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-sw3-356m&lt;/td&gt; &#xA;   &lt;td&gt;0.47B&lt;/td&gt; &#xA;   &lt;td&gt;37.05&lt;/td&gt; &#xA;   &lt;td&gt;42.55&lt;/td&gt; &#xA;   &lt;td&gt;25.93&lt;/td&gt; &#xA;   &lt;td&gt;23.63&lt;/td&gt; &#xA;   &lt;td&gt;61.59&lt;/td&gt; &#xA;   &lt;td&gt;64.85&lt;/td&gt; &#xA;   &lt;td&gt;32.15&lt;/td&gt; &#xA;   &lt;td&gt;41.56&lt;/td&gt; &#xA;   &lt;td&gt;53.04&lt;/td&gt; &#xA;   &lt;td&gt;42.48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pythia-410m&lt;/td&gt; &#xA;   &lt;td&gt;0.51B&lt;/td&gt; &#xA;   &lt;td&gt;40.85&lt;/td&gt; &#xA;   &lt;td&gt;41.22&lt;/td&gt; &#xA;   &lt;td&gt;27.25&lt;/td&gt; &#xA;   &lt;td&gt;26.19&lt;/td&gt; &#xA;   &lt;td&gt;64.20&lt;/td&gt; &#xA;   &lt;td&gt;67.19&lt;/td&gt; &#xA;   &lt;td&gt;30.71&lt;/td&gt; &#xA;   &lt;td&gt;41.40&lt;/td&gt; &#xA;   &lt;td&gt;53.12&lt;/td&gt; &#xA;   &lt;td&gt;43.57&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xglm-564m&lt;/td&gt; &#xA;   &lt;td&gt;0.56B&lt;/td&gt; &#xA;   &lt;td&gt;34.64&lt;/td&gt; &#xA;   &lt;td&gt;40.43&lt;/td&gt; &#xA;   &lt;td&gt;25.18&lt;/td&gt; &#xA;   &lt;td&gt;24.57&lt;/td&gt; &#xA;   &lt;td&gt;62.25&lt;/td&gt; &#xA;   &lt;td&gt;64.85&lt;/td&gt; &#xA;   &lt;td&gt;29.28&lt;/td&gt; &#xA;   &lt;td&gt;42.68&lt;/td&gt; &#xA;   &lt;td&gt;53.03&lt;/td&gt; &#xA;   &lt;td&gt;41.87&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lamini-GPT-LM&lt;/td&gt; &#xA;   &lt;td&gt;0.59B&lt;/td&gt; &#xA;   &lt;td&gt;31.55&lt;/td&gt; &#xA;   &lt;td&gt;40.72&lt;/td&gt; &#xA;   &lt;td&gt;25.53&lt;/td&gt; &#xA;   &lt;td&gt;24.23&lt;/td&gt; &#xA;   &lt;td&gt;63.09&lt;/td&gt; &#xA;   &lt;td&gt;63.87&lt;/td&gt; &#xA;   &lt;td&gt;29.95&lt;/td&gt; &#xA;   &lt;td&gt;40.78&lt;/td&gt; &#xA;   &lt;td&gt;47.75&lt;/td&gt; &#xA;   &lt;td&gt;40.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;MobiLlama (Ours)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.5B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;52.52&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;38.05&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;26.45&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;29.52&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;64.03&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;72.03&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;33.68&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;40.22&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57.53&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;46.00&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lamini-GPT-LM&lt;/td&gt; &#xA;   &lt;td&gt;0.77B&lt;/td&gt; &#xA;   &lt;td&gt;43.83&lt;/td&gt; &#xA;   &lt;td&gt;40.25&lt;/td&gt; &#xA;   &lt;td&gt;26.24&lt;/td&gt; &#xA;   &lt;td&gt;27.55&lt;/td&gt; &#xA;   &lt;td&gt;66.12&lt;/td&gt; &#xA;   &lt;td&gt;69.31&lt;/td&gt; &#xA;   &lt;td&gt;37.12&lt;/td&gt; &#xA;   &lt;td&gt;42.47&lt;/td&gt; &#xA;   &lt;td&gt;56.59&lt;/td&gt; &#xA;   &lt;td&gt;45.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;MobiLlama (Ours)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.8B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;54.09&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;38.48&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;26.92&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;30.20&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;64.82&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;73.17&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;33.37&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;41.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57.45&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;46.67&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;The table provides a comparative analysis of various models, including our MobiLlama, across several LLM benchmarks. It highlights MobiLlama&#39;s superior performance, particularly in its 0.5B and 0.8B configurations, showcasing its efficiency and effectiveness in processing complex language tasks. This comparison underscores MobiLlama&#39;s advancements in achieving higher accuracy and demonstrates its potential as a leading solution in the field of LLM.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;#Params&lt;/th&gt; &#xA;   &lt;th&gt;HellaSwag&lt;/th&gt; &#xA;   &lt;th&gt;Truthfulqa&lt;/th&gt; &#xA;   &lt;th&gt;MMLU&lt;/th&gt; &#xA;   &lt;th&gt;Arc_C&lt;/th&gt; &#xA;   &lt;th&gt;CrowsPairs&lt;/th&gt; &#xA;   &lt;th&gt;piqa&lt;/th&gt; &#xA;   &lt;th&gt;race&lt;/th&gt; &#xA;   &lt;th&gt;siqa&lt;/th&gt; &#xA;   &lt;th&gt;winogrande&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Boomer&lt;/td&gt; &#xA;   &lt;td&gt;1B&lt;/td&gt; &#xA;   &lt;td&gt;31.62&lt;/td&gt; &#xA;   &lt;td&gt;39.42&lt;/td&gt; &#xA;   &lt;td&gt;25.42&lt;/td&gt; &#xA;   &lt;td&gt;22.26&lt;/td&gt; &#xA;   &lt;td&gt;61.26&lt;/td&gt; &#xA;   &lt;td&gt;57.99&lt;/td&gt; &#xA;   &lt;td&gt;28.99&lt;/td&gt; &#xA;   &lt;td&gt;40.32&lt;/td&gt; &#xA;   &lt;td&gt;50.98&lt;/td&gt; &#xA;   &lt;td&gt;39.80&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-Dedup&lt;/td&gt; &#xA;   &lt;td&gt;1B&lt;/td&gt; &#xA;   &lt;td&gt;49.63&lt;/td&gt; &#xA;   &lt;td&gt;38.92&lt;/td&gt; &#xA;   &lt;td&gt;24.29&lt;/td&gt; &#xA;   &lt;td&gt;29.09&lt;/td&gt; &#xA;   &lt;td&gt;67.11&lt;/td&gt; &#xA;   &lt;td&gt;70.23&lt;/td&gt; &#xA;   &lt;td&gt;32.44&lt;/td&gt; &#xA;   &lt;td&gt;42.63&lt;/td&gt; &#xA;   &lt;td&gt;53.98&lt;/td&gt; &#xA;   &lt;td&gt;45.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon-RW&lt;/td&gt; &#xA;   &lt;td&gt;1B&lt;/td&gt; &#xA;   &lt;td&gt;63.12&lt;/td&gt; &#xA;   &lt;td&gt;35.96&lt;/td&gt; &#xA;   &lt;td&gt;25.36&lt;/td&gt; &#xA;   &lt;td&gt;35.06&lt;/td&gt; &#xA;   &lt;td&gt;69.04&lt;/td&gt; &#xA;   &lt;td&gt;74.10&lt;/td&gt; &#xA;   &lt;td&gt;36.07&lt;/td&gt; &#xA;   &lt;td&gt;40.23&lt;/td&gt; &#xA;   &lt;td&gt;61.88&lt;/td&gt; &#xA;   &lt;td&gt;48.98&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TinyLlama&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;60.22&lt;/td&gt; &#xA;   &lt;td&gt;37.59&lt;/td&gt; &#xA;   &lt;td&gt;26.11&lt;/td&gt; &#xA;   &lt;td&gt;33.61&lt;/td&gt; &#xA;   &lt;td&gt;70.60&lt;/td&gt; &#xA;   &lt;td&gt;73.28&lt;/td&gt; &#xA;   &lt;td&gt;36.45&lt;/td&gt; &#xA;   &lt;td&gt;41.65&lt;/td&gt; &#xA;   &lt;td&gt;59.18&lt;/td&gt; &#xA;   &lt;td&gt;48.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OLMo&lt;/td&gt; &#xA;   &lt;td&gt;1.2B&lt;/td&gt; &#xA;   &lt;td&gt;62.50&lt;/td&gt; &#xA;   &lt;td&gt;32.94&lt;/td&gt; &#xA;   &lt;td&gt;25.86&lt;/td&gt; &#xA;   &lt;td&gt;34.45&lt;/td&gt; &#xA;   &lt;td&gt;69.59&lt;/td&gt; &#xA;   &lt;td&gt;73.70&lt;/td&gt; &#xA;   &lt;td&gt;36.74&lt;/td&gt; &#xA;   &lt;td&gt;41.14&lt;/td&gt; &#xA;   &lt;td&gt;58.90&lt;/td&gt; &#xA;   &lt;td&gt;48.42&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cerebras-GPT&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;38.51&lt;/td&gt; &#xA;   &lt;td&gt;42.70&lt;/td&gt; &#xA;   &lt;td&gt;26.66&lt;/td&gt; &#xA;   &lt;td&gt;26.10&lt;/td&gt; &#xA;   &lt;td&gt;63.67&lt;/td&gt; &#xA;   &lt;td&gt;66.75&lt;/td&gt; &#xA;   &lt;td&gt;30.33&lt;/td&gt; &#xA;   &lt;td&gt;42.42&lt;/td&gt; &#xA;   &lt;td&gt;53.59&lt;/td&gt; &#xA;   &lt;td&gt;43.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lamini&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;38.05&lt;/td&gt; &#xA;   &lt;td&gt;36.43&lt;/td&gt; &#xA;   &lt;td&gt;28.47&lt;/td&gt; &#xA;   &lt;td&gt;26.62&lt;/td&gt; &#xA;   &lt;td&gt;64.62&lt;/td&gt; &#xA;   &lt;td&gt;67.89&lt;/td&gt; &#xA;   &lt;td&gt;33.39&lt;/td&gt; &#xA;   &lt;td&gt;43.19&lt;/td&gt; &#xA;   &lt;td&gt;50.59&lt;/td&gt; &#xA;   &lt;td&gt;43.25&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPT&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;54.50&lt;/td&gt; &#xA;   &lt;td&gt;38.67&lt;/td&gt; &#xA;   &lt;td&gt;24.63&lt;/td&gt; &#xA;   &lt;td&gt;29.60&lt;/td&gt; &#xA;   &lt;td&gt;70.70&lt;/td&gt; &#xA;   &lt;td&gt;72.47&lt;/td&gt; &#xA;   &lt;td&gt;34.16&lt;/td&gt; &#xA;   &lt;td&gt;42.47&lt;/td&gt; &#xA;   &lt;td&gt;59.74&lt;/td&gt; &#xA;   &lt;td&gt;47.43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-NEO&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;48.49&lt;/td&gt; &#xA;   &lt;td&gt;39.61&lt;/td&gt; &#xA;   &lt;td&gt;24.82&lt;/td&gt; &#xA;   &lt;td&gt;31.31&lt;/td&gt; &#xA;   &lt;td&gt;65.67&lt;/td&gt; &#xA;   &lt;td&gt;71.05&lt;/td&gt; &#xA;   &lt;td&gt;34.06&lt;/td&gt; &#xA;   &lt;td&gt;41.81&lt;/td&gt; &#xA;   &lt;td&gt;57.06&lt;/td&gt; &#xA;   &lt;td&gt;45.98&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia-Deduped&lt;/td&gt; &#xA;   &lt;td&gt;1.4B&lt;/td&gt; &#xA;   &lt;td&gt;55.00&lt;/td&gt; &#xA;   &lt;td&gt;38.63&lt;/td&gt; &#xA;   &lt;td&gt;25.45&lt;/td&gt; &#xA;   &lt;td&gt;32.59&lt;/td&gt; &#xA;   &lt;td&gt;67.33&lt;/td&gt; &#xA;   &lt;td&gt;72.68&lt;/td&gt; &#xA;   &lt;td&gt;34.64&lt;/td&gt; &#xA;   &lt;td&gt;42.68&lt;/td&gt; &#xA;   &lt;td&gt;56.90&lt;/td&gt; &#xA;   &lt;td&gt;47.32&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;large-base&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1.2B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;62.99&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;35.90&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;24.79&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;34.55&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;68.49&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;75.57&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;35.31&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;41.96&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;62.03&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;49.06&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;Comprehensive comparisons with existing &amp;lt; 2B params fully open-source LLM models on 9 benchmarks. Our 1.2B &#34;large-base&#34; model pre-trained on 1.2T tokens achieves superior performance compared to both the recent OLMo 1.17B model and TinyLlama 1.1B model, which are pre-trained on a substantially larger data of 3T tokens.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üì± MobiLlama on Android&lt;/h2&gt; &#xA;&lt;p&gt;To run our model on an android app, please download and install the APK from &lt;a href=&#34;https://mbzuaiac-my.sharepoint.com/:f:/g/personal/omkar_thawakar_mbzuai_ac_ae/EhRfGdmgFVVNvIRfy1EgLwEBjbk_eg3UmNg_zjz7PMTsmg?e=NBuJo8&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üôè Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We thank &lt;a href=&#34;https://github.com/LLM360/amber-train&#34;&gt;LLM-360&lt;/a&gt; for fully transparent and open-source implementation of their language model. MobiLlama repo is built using &lt;a href=&#34;https://github.com/LLM360/amber-train&#34;&gt;LLM-360&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìú Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{thawakar2024mobillama,&#xA;      title={MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT}, &#xA;      author={Omkar Thawakar and Ashmal Vayani and Salman Khan and Hisham Cholakkal and Rao Muhammad Anwer and Michael Felsberg and Timothy Baldwin and Eric P. Xing and Fahad Shahbaz Khan},&#xA;      year={2024},&#xA;      eprint={2402.16840},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>kyegomez/BitNet</title>
    <updated>2024-03-02T01:31:14Z</updated>
    <id>tag:github.com,2024-03-02:/kyegomez/BitNet</id>
    <link href="https://github.com/kyegomez/BitNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of &#34;BitNet: Scaling 1-bit Transformers for Large Language Models&#34; in pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qUtxnK2NMf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/BitNet/main/agorabanner.png&#34; alt=&#34;Multi-Modality&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;BitNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kyegomez/BitNet/main/bitnet.png&#34; alt=&#34;bitnet&#34;&gt; PyTorch Implementation of the linear methods and model from the paper &#34;BitNet: Scaling 1-bit Transformers for Large Language Models&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.11453.pdf&#34;&gt;Paper link:&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;BitLinear = tensor -&amp;gt; layernorm -&amp;gt; Binarize -&amp;gt; abs max quantization -&amp;gt; dequant&lt;/p&gt; &#xA;&lt;p&gt;&#34;The implementation of the BitNet architecture is quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch) in the Transformer. &#34; -- BitNet is really easy to implement just swap out the linears with the BitLinear modules!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;NEWS&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BitNet Transformer has been trained using the &lt;code&gt;train.py&lt;/code&gt; file that trains on enwiki8 a small 1gb dataset of wikipedia: &lt;a href=&#34;https://drive.google.com/file/d/1gBuZRFBqMV3cVD902LXA_hmZl4e0dLyY/view&#34;&gt;HERE IS THE LINK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New Iteration&lt;/strong&gt; üî• There is an all-new iteration from the paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2402.17764&#34;&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&#34;, we&#39;re implementing it now. Join the Agora discord and contribute! &lt;a href=&#34;https://discord.gg/hFzevCjG8c&#34;&gt;Join Here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New Optimizations&lt;/strong&gt; The first &lt;code&gt;BitLinear&lt;/code&gt; has been optimized and we now have a Bit Attention &lt;code&gt;BitMGQA&lt;/code&gt; That implements BitLinear into the attention mechanism. Multi Grouped Query Attention is also widely recognized as the best attention for its fast decoding and long context handling, thanks to Frank for his easy to use implementation!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dimitry, Nullonix for analysis and code review and revision&lt;/li&gt; &#xA; &lt;li&gt;Vyom, for providing 4080 to train!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install bitnet&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage:&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitLinear&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example of the BitLinear layer which is the main innovation of the paper!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;from bitnet import BitLinear&#xA;&#xA;# Input&#xA;x = torch.randn(10, 512)&#xA;&#xA;# BitLinear layer&#xA;layer = BitLinear(512, 400)&#xA;&#xA;# Output&#xA;y = layer(x)&#xA;&#xA;print(y)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitNetTransformer&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fully implemented Transformer as described in the diagram with MHA, and BitFeedforwards&lt;/li&gt; &#xA; &lt;li&gt;Can be utilized not just for text but for images and maybe even video or audio processing&lt;/li&gt; &#xA; &lt;li&gt;Complete with residuals and skip connections for gradient flow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;from bitnet import BitNetTransformer&#xA;&#xA;bitnet = BitNetTransformer(&#xA;    num_tokens=20000,&#xA;    dim=512,&#xA;    depth=6,&#xA;    dim_head=64,&#xA;    heads=8,&#xA;    ff_mult=4,&#xA;)&#xA;&#xA;tokens = torch.randint(0, 20000, (1, 512))&#xA;logits = bitnet(tokens)&#xA;print(logits.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitAttention&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This Attention has been modified to use BitLinear instead of the default linear projection. It&#39;s also using Multi-Grouped Query Attention instead of regular multi-head attention for faster decoding and longer context handling.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from bitnet import BitMGQA&#xA;&#xA;# Create a random tensor of shape (1, 10, 512)&#xA;x = torch.randn(1, 10, 512)&#xA;&#xA;# Create an instance of the BitMGQA model with input size 512, 8 attention heads, and 4 layers&#xA;gqa = BitMGQA(512, 8, 4)&#xA;&#xA;# Pass the input tensor through the BitMGQA model and get the output and attention weights&#xA;out, _ = gqa(x, x, x, need_weights=True)&#xA;&#xA;# Print the shapes of the output tensor and attention tensor&#xA;print(out)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;BitFeedForward&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Feedforward as shown in the diagram with BitLinear and a GELU:&lt;/li&gt; &#xA; &lt;li&gt;Linear -&amp;gt; GELU -&amp;gt; Linear&lt;/li&gt; &#xA; &lt;li&gt;You can add dropouts, or layernorms, or other layers for a better ffn&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from bitnet import BitFeedForward&#xA;&#xA;# Create a random input tensor of shape (10, 512)&#xA;x = torch.randn(10, 512)&#xA;&#xA;# Create an instance of the BitFeedForward class with the following parameters:&#xA;# - input_dim: 512&#xA;# - hidden_dim: 512&#xA;# - num_layers: 4&#xA;# - swish: True (use Swish activation function)&#xA;# - post_act_ln: True (apply Layer Normalization after each activation)&#xA;# - dropout: 0.1 (apply dropout with a probability of 0.1)&#xA;ff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)&#xA;&#xA;# Apply the BitFeedForward network to the input tensor x&#xA;y = ff(x)&#xA;&#xA;# Print the shape of the output tensor y&#xA;print(y)  # torch.Size([10, 512])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bitnet import BitNetInference&#xA;&#xA;bitnet = BitNetInference()&#xA;bitnet.load_model(&#34;../model_checkpoint.pth&#34;)  # Download model&#xA;output_str = bitnet.generate(&#34;The dog jumped over the &#34;, 512)&#xA;print(output_str)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Huggingface Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForSequenceClassification, AutoTokenizer&#xA;&#xA;from bitnet import replace_linears_in_hf&#xA;&#xA;# Load a model from Hugging Face&#39;s Transformers&#xA;model_name = &#34;bert-base-uncased&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForSequenceClassification.from_pretrained(model_name)&#xA;&#xA;# Replace Linear layers with BitLinear&#xA;replace_linears_in_hf(model)&#xA;&#xA;# Example text to classify&#xA;text = &#34;Replace this with your text&#34;&#xA;inputs = tokenizer(&#xA;    text, return_tensors=&#34;pt&#34;, padding=True, truncation=True, max_length=512&#xA;)&#xA;&#xA;# Perform inference&#xA;model.eval()  # Set the model to evaluation mode&#xA;with torch.no_grad():&#xA;    outputs = model(**inputs)&#xA;    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)&#xA;    print(predictions)&#xA;&#xA;# Process predictions&#xA;predicted_class_id = predictions.argmax().item()&#xA;print(f&#34;Predicted class ID: {predicted_class_id}&#34;)&#xA;&#xA;# Optionally, map the predicted class ID to a label, if you know the classification labels&#xA;# labels = [&#34;Label 1&#34;, &#34;Label 2&#34;, ...]  # Define your labels corresponding to the model&#39;s classes&#xA;# print(f&#34;Predicted label: {labels[predicted_class_id]}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2310.11453,&#xA;Author = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},&#xA;Title = {BitNet: Scaling 1-bit Transformers for Large Language Models},&#xA;Year = {2023},&#xA;Eprint = {arXiv:2310.11453},&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Todo&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Double check BitLinear implementation and make sure it works exactly as in paper&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement training script for &lt;code&gt;BitNetTransformer&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train on Enwiki8, copy and past code and data from Lucidrains repos&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Benchmark performance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Look into Straight Through Estimator for non-differentiable backprop&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement BitFeedForward&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Clean up codebase&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add unit tests for each module&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement the new BitNet1.5b from the &lt;a href=&#34;https://arxiv.org/abs/2402.17764&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement the BitNet15b in Cuda&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>