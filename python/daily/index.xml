<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-05T01:35:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>conda/conda</title>
    <updated>2025-06-05T01:35:47Z</updated>
    <id>tag:github.com,2025-06-05:/conda/conda</id>
    <link href="https://github.com/conda/conda" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A system-level, binary package and environment manager running on all major operating systems and platforms.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/conda/conda&#34;&gt;&lt;img src=&#34;https://s3.amazonaws.com/conda-dev/conda_logo.svg?sanitize=true&#34; alt=&#34;Conda Logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/conda/conda/actions/workflows/tests.yml?query=branch%3Amain+event%3Aschedule&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/conda/conda/tests.yml?branch=main&amp;amp;event=schedule&amp;amp;logo=github&amp;amp;label=tests&#34; alt=&#34;GitHub Scheduled Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/conda/conda/branch/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/conda/conda/main?logo=codecov&#34; alt=&#34;Codecov Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codspeed.io/conda/conda&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&#34; alt=&#34;CodSpeed Performance Benchmarks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://calver.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/calver-YY.MM.MICRO-22bfda.svg?sanitize=true&#34; alt=&#34;CalVer Versioning&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/conda/conda/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/conda/conda?logo=github&#34; alt=&#34;GitHub Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/anaconda/conda&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/anaconda/conda?logo=anaconda&#34; alt=&#34;Anaconda Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/conda&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/conda?logo=conda-forge&#34; alt=&#34;conda-forge Package&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Conda is a cross-platform, language-agnostic binary package manager. It is a package manager used in conda distributions like &lt;a href=&#34;https://github.com/conda-forge/miniforge&#34;&gt;Miniforge&lt;/a&gt; and the &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda Distribution&lt;/a&gt;, but it may be used for other systems as well. Conda makes environments first-class citizens, making it easy to create independent environments even for C libraries. The conda command line interface is written entirely in Python, and is BSD licensed open source.&lt;/p&gt; &#xA;&lt;p&gt;Conda is enhanced by organizations, tools, and repositories created and managed by the amazing members of the &lt;a href=&#34;https://conda.org/&#34;&gt;conda community&lt;/a&gt;. Some of them can be found &lt;a href=&#34;https://github.com/conda/conda/wiki/Conda-Community&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To bootstrap a minimal distribution, use a minimal installer such as &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/&#34;&gt;Miniconda&lt;/a&gt; or &lt;a href=&#34;https://conda-forge.org/download/&#34;&gt;Miniforge&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Conda is also included in the &lt;a href=&#34;https://repo.anaconda.com&#34;&gt;Anaconda Distribution&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Updating conda&lt;/h2&gt; &#xA;&lt;p&gt;To update &lt;code&gt;conda&lt;/code&gt; to the newest version, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda update -n base conda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] It is possible that &lt;code&gt;conda update&lt;/code&gt; does not install the newest version if the existing &lt;code&gt;conda&lt;/code&gt; version is far behind the current release. In this case, updating needs to be done in stages.&lt;/p&gt; &#xA; &lt;p&gt;For example, to update from &lt;code&gt;conda 4.12&lt;/code&gt; to &lt;code&gt;conda 23.10.0&lt;/code&gt;, &lt;code&gt;conda 22.11.1&lt;/code&gt; needs to be installed first:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;$ conda install -n base conda=22.11.1&#xA;$ conda update conda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;If you install the Anaconda Distribution, you will already have hundreds of packages installed. You can see what packages are installed by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to see all the packages that are available, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda search&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and to install a package, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda install &amp;lt;package-name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The real power of conda comes from its ability to manage environments. In conda, an environment can be thought of as a completely separate installation. Conda installs packages into environments efficiently using &lt;a href=&#34;https://en.wikipedia.org/wiki/Hard_link&#34;&gt;hard links&lt;/a&gt; by default when it is possible, so environments are space efficient, and take seconds to create.&lt;/p&gt; &#xA;&lt;p&gt;The default environment, which &lt;code&gt;conda&lt;/code&gt; itself is installed into, is called &lt;code&gt;base&lt;/code&gt;. To create another environment, use the &lt;code&gt;conda create&lt;/code&gt; command. For instance, to create an environment with PyTorch, you would run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda create --name ml-project pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This creates an environment called &lt;code&gt;ml-project&lt;/code&gt; with the latest version of PyTorch, and its dependencies.&lt;/p&gt; &#xA;&lt;p&gt;We can now activate this environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda activate ml-project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This puts the &lt;code&gt;bin&lt;/code&gt; directory of the &lt;code&gt;ml-project&lt;/code&gt; environment in the front of the &lt;code&gt;PATH&lt;/code&gt;, and sets it as the default environment for all subsequent conda commands.&lt;/p&gt; &#xA;&lt;p&gt;To go back to the base environment, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda deactivate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building Your Own Packages&lt;/h2&gt; &#xA;&lt;p&gt;You can easily build your own packages for conda, and upload them to &lt;a href=&#34;https://anaconda.org&#34;&gt;anaconda.org&lt;/a&gt;, a free service for hosting packages for conda, as well as other package managers. To build a package, create a recipe. Package building documentation is available &lt;a href=&#34;https://docs.conda.io/projects/conda-build/en/latest/&#34;&gt;here&lt;/a&gt;. See &lt;a href=&#34;https://github.com/AnacondaRecipes&#34;&gt;AnacondaRecipes&lt;/a&gt; for the recipes that make up the Anaconda Distribution and &lt;code&gt;defaults&lt;/code&gt; channel. &lt;a href=&#34;https://conda-forge.org/feedstocks/&#34;&gt;Conda-forge&lt;/a&gt; and &lt;a href=&#34;https://github.com/bioconda/bioconda-recipes&#34;&gt;Bioconda&lt;/a&gt; are community-driven conda-based distributions.&lt;/p&gt; &#xA;&lt;p&gt;To upload to anaconda.org, create an account. Then, install the anaconda-client and login:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda install anaconda-client&#xA;$ anaconda login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, after you build your recipe:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda build &amp;lt;recipe-dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;you will be prompted to upload to anaconda.org.&lt;/p&gt; &#xA;&lt;p&gt;To add your anaconda.org channel, or other&#39;s channels, to conda so that &lt;code&gt;conda install&lt;/code&gt; will find and install their packages, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda config --add channels https://conda.anaconda.org/username&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(replacing &lt;code&gt;username&lt;/code&gt; with the username of the person whose channel you want to add).&lt;/p&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/condaproject&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://conda.slack.com&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/conda/conda/issues&#34;&gt;Bug Reports/Feature Requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ContinuumIO/anaconda-issues/issues&#34;&gt;Installer/Package Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://conda.discourse.group/&#34;&gt;Discourse&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/conda/conda&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;open in gitpod for one-click development&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Contributions to conda are welcome. See the &lt;a href=&#34;https://raw.githubusercontent.com/conda/conda/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; documentation for instructions on setting up a development environment.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mit-han-lab/nunchaku</title>
    <updated>2025-06-05T01:35:47Z</updated>
    <id>tag:github.com,2025-06-05:/mit-han-lab/nunchaku</id>
    <link href="https://github.com/mit-han-lab/nunchaku" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICLR2025 Spotlight] SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; id=&#34;nunchaku_logo&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/477953fa1dd6f082fbec201cea7c7430117a810e/assets/nunchaku.svg?sanitize=true&#34; alt=&#34;logo&#34; width=&#34;220&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;a href=&#34;http://arxiv.org/abs/2411.05007&#34;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://hanlab.mit.edu/projects/svdquant&#34;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://hanlab.mit.edu/blog/svdquant&#34;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://svdquant.mit.edu&#34;&gt;&lt;b&gt;Demo&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/collections/mit-han-lab/nunchaku-6837e7498f680552f7bbb5ad&#34;&gt;&lt;b&gt;HuggingFace&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://modelscope.cn/collections/Nunchaku-519fed7f9de94e&#34;&gt;&lt;b&gt;ModelScope&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/mit-han-lab/ComfyUI-nunchaku&#34;&gt;&lt;b&gt;ComfyUI&lt;/b&gt;&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/README.md&#34;&gt;&lt;b&gt;English&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/README_ZH.md&#34;&gt;&lt;b&gt;中文&lt;/b&gt;&lt;/a&gt; &lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nunchaku&lt;/strong&gt; is a high-performance inference engine optimized for 4-bit neural networks, as introduced in our paper &lt;a href=&#34;http://arxiv.org/abs/2411.05007&#34;&gt;SVDQuant&lt;/a&gt;. For the underlying quantization library, check out &lt;a href=&#34;https://github.com/mit-han-lab/deepcompressor&#34;&gt;DeepCompressor&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Join our user groups on &lt;a href=&#34;https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q&#34;&gt;&lt;strong&gt;Slack&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://discord.gg/Wk6PnwX9Sm&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/wechat.jpg&#34;&gt;&lt;strong&gt;WeChat&lt;/strong&gt;&lt;/a&gt; to engage in discussions with the community! More details can be found &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/issues/149&#34;&gt;here&lt;/a&gt;. If you have any questions, run into issues, or are interested in contributing, don’t hesitate to reach out!&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-06-01]&lt;/strong&gt; 🚀 &lt;strong&gt;Release v0.3.0!&lt;/strong&gt; This update adds support for multiple-batch inference, &lt;a href=&#34;https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0&#34;&gt;&lt;strong&gt;ControlNet-Union-Pro 2.0&lt;/strong&gt;&lt;/a&gt;, initial integration of &lt;a href=&#34;https://github.com/ToTheBeginning/PuLID&#34;&gt;&lt;strong&gt;PuLID&lt;/strong&gt;&lt;/a&gt;, and introduces &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-double_cache.py&#34;&gt;&lt;strong&gt;Double FB Cache&lt;/strong&gt;&lt;/a&gt;. You can now load Nunchaku FLUX models as a single file, and our upgraded &lt;a href=&#34;https://huggingface.co/mit-han-lab/nunchaku-t5&#34;&gt;&lt;strong&gt;4-bit T5 encoder&lt;/strong&gt;&lt;/a&gt; now matches &lt;strong&gt;FP8 T5&lt;/strong&gt; in quality!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-04-16]&lt;/strong&gt; 🎥 Released tutorial videos in both &lt;a href=&#34;https://youtu.be/YHAVe-oM7U8?si=cM9zaby_aEHiFXk0&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.bilibili.com/video/BV1BTocYjEk5/?share_source=copy_web&amp;amp;vd_source=8926212fef622f25cc95380515ac74ee&#34;&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/a&gt; to assist installation and usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-04-09]&lt;/strong&gt; 📢 Published the &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/issues/266&#34;&gt;April roadmap&lt;/a&gt; and an &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/discussions/262&#34;&gt;FAQ&lt;/a&gt; to help the community get started and stay up to date with Nunchaku’s development.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-04-05]&lt;/strong&gt; 🚀 &lt;strong&gt;Nunchaku v0.2.0 released!&lt;/strong&gt; This release brings &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-multiple-lora.py&#34;&gt;&lt;strong&gt;multi-LoRA&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-controlnet-union-pro.py&#34;&gt;&lt;strong&gt;ControlNet&lt;/strong&gt;&lt;/a&gt; support with even faster performance powered by &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/#fp16-attention&#34;&gt;&lt;strong&gt;FP16 attention&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/#first-block-cache&#34;&gt;&lt;strong&gt;First-Block Cache&lt;/strong&gt;&lt;/a&gt;. We&#39;ve also added compatibility for &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-turing.py&#34;&gt;&lt;strong&gt;20-series GPUs&lt;/strong&gt;&lt;/a&gt; — Nunchaku is now more accessible than ever!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2025-03-07]&lt;/strong&gt; 🚀 &lt;strong&gt;Nunchaku v0.1.4 Released!&lt;/strong&gt; We&#39;ve supported &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/#Low-Memory-Inference&#34;&gt;4-bit text encoder and per-layer CPU offloading&lt;/a&gt;, reducing FLUX&#39;s minimum memory requirement to just &lt;strong&gt;4 GiB&lt;/strong&gt; while maintaining a &lt;strong&gt;2–3× speedup&lt;/strong&gt;. This update also fixes various issues related to resolution, LoRA, pin memory, and runtime stability. Check out the release notes for full details!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;More&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025-02-20]&lt;/strong&gt; 🚀 &lt;strong&gt;Support NVFP4 precision on NVIDIA RTX 5090!&lt;/strong&gt; NVFP4 delivers superior image quality compared to INT4, offering &lt;strong&gt;~3× speedup&lt;/strong&gt; on the RTX 5090 over BF16. Learn more in our &lt;a href=&#34;https://hanlab.mit.edu/blog/svdquant-nvfp4&#34;&gt;blog&lt;/a&gt;, checkout &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; for usage and try &lt;a href=&#34;https://svdquant.mit.edu/flux1-schnell/&#34;&gt;our demo&lt;/a&gt; online!&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025-02-18]&lt;/strong&gt; 🔥 &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/#Customized-LoRA&#34;&gt;&lt;strong&gt;Customized LoRA conversion&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/#Customized-Model-Quantization&#34;&gt;&lt;strong&gt;model quantization&lt;/strong&gt;&lt;/a&gt; instructions are now available! &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/comfyui&#34;&gt;ComfyUI&lt;/a&gt;&lt;/strong&gt; workflows now support &lt;strong&gt;customized LoRA&lt;/strong&gt;, along with &lt;strong&gt;FLUX.1-Tools&lt;/strong&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025-02-11]&lt;/strong&gt; 🎉 &lt;strong&gt;&lt;a href=&#34;http://arxiv.org/abs/2411.05007&#34;&gt;SVDQuant&lt;/a&gt; has been selected as a ICLR 2025 Spotlight! FLUX.1-tools Gradio demos are now available!&lt;/strong&gt; Check &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/#gradio-demos&#34;&gt;here&lt;/a&gt; for the usage details! Our new &lt;a href=&#34;https://svdquant.mit.edu/flux1-depth-dev/&#34;&gt;depth-to-image demo&lt;/a&gt; is also online—try it out!&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025-02-04]&lt;/strong&gt; &lt;strong&gt;🚀 4-bit &lt;a href=&#34;https://blackforestlabs.ai/flux-1-tools/&#34;&gt;FLUX.1-tools&lt;/a&gt; is here!&lt;/strong&gt; Enjoy a &lt;strong&gt;2-3× speedup&lt;/strong&gt; over the original models. Check out the &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples&#34;&gt;examples&lt;/a&gt; for usage. &lt;strong&gt;ComfyUI integration is coming soon!&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025-01-23]&lt;/strong&gt; 🚀 &lt;strong&gt;4-bit &lt;a href=&#34;https://nvlabs.github.io/Sana/&#34;&gt;SANA&lt;/a&gt; support is here!&lt;/strong&gt; Experience a 2-3× speedup compared to the 16-bit model. Check out the &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/sana1.6b_pag.py&#34;&gt;usage example&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/sana/t2i&#34;&gt;deployment guide&lt;/a&gt; for more details. Explore our live demo at &lt;a href=&#34;https://svdquant.mit.edu&#34;&gt;svdquant.mit.edu&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2025-01-22]&lt;/strong&gt; 🎉 &lt;a href=&#34;http://arxiv.org/abs/2411.05007&#34;&gt;&lt;strong&gt;SVDQuant&lt;/strong&gt;&lt;/a&gt; has been accepted to &lt;strong&gt;ICLR 2025&lt;/strong&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2024-12-08]&lt;/strong&gt; Support &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;. Please check &lt;a href=&#34;https://github.com/mit-han-lab/ComfyUI-nunchaku&#34;&gt;mit-han-lab/ComfyUI-nunchaku&lt;/a&gt; for the usage.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[2024-11-07]&lt;/strong&gt; 🔥 Our latest &lt;strong&gt;W4A4&lt;/strong&gt; Diffusion model quantization work &lt;a href=&#34;https://hanlab.mit.edu/projects/svdquant&#34;&gt;&lt;strong&gt;SVDQuant&lt;/strong&gt;&lt;/a&gt; is publicly released! Check &lt;a href=&#34;https://github.com/mit-han-lab/deepcompressor&#34;&gt;&lt;strong&gt;DeepCompressor&lt;/strong&gt;&lt;/a&gt; for the quantization library.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/teaser.jpg&#34; alt=&#34;teaser&#34;&gt; SVDQuant is a post-training quantization technique for 4-bit weights and activations that well maintains visual fidelity. On 12B FLUX.1-dev, it achieves 3.6× memory reduction compared to the BF16 model. By eliminating CPU offloading, it offers 8.7× speedup over the 16-bit model when on a 16GB laptop 4090 GPU, 3× faster than the NF4 W4A16 baseline. On PixArt-∑, it demonstrates significantly superior visual quality over other W4A4 or even W4A8 baselines. &#34;E2E&#34; means the end-to-end latency including the text encoder and VAE decoder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://lmxyy.me&#34;&gt;Muyang Li&lt;/a&gt;*, &lt;a href=&#34;https://yujunlin.com&#34;&gt;Yujun Lin&lt;/a&gt;*, &lt;a href=&#34;https://hanlab.mit.edu/team/zhekai-zhang&#34;&gt;Zhekai Zhang&lt;/a&gt;*, &lt;a href=&#34;https://www.tianle.website/#/&#34;&gt;Tianle Cai&lt;/a&gt;, &lt;a href=&#34;https://xiuyuli.com&#34;&gt;Xiuyu Li&lt;/a&gt;, &lt;a href=&#34;https://github.com/JerryGJX&#34;&gt;Junxian Guo&lt;/a&gt;, &lt;a href=&#34;https://xieenze.github.io&#34;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&#34;https://cs.stanford.edu/~chenlin/&#34;&gt;Chenlin Meng&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~junyanz/&#34;&gt;Jun-Yan Zhu&lt;/a&gt;, and &lt;a href=&#34;https://hanlab.mit.edu/songhan&#34;&gt;Song Han&lt;/a&gt; &lt;br&gt; &lt;em&gt;MIT, NVIDIA, CMU, Princeton, UC Berkeley, SJTU, and Pika Labs&lt;/em&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/fdd4ab68-6489-4c65-8768-259bd866e8f8&#34;&gt;https://github.com/user-attachments/assets/fdd4ab68-6489-4c65-8768-259bd866e8f8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;h4&gt;Quantization Method -- SVDQuant&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/intuition.gif&#34; alt=&#34;intuition&#34;&gt;Overview of SVDQuant. Stage1: Originally, both the activation $\boldsymbol{X}$ and weights $\boldsymbol{W}$ contain outliers, making 4-bit quantization challenging. Stage 2: We migrate the outliers from activations to weights, resulting in the updated activation $\hat{\boldsymbol{X}}$ and weights $\hat{\boldsymbol{W}}$. While $\hat{\boldsymbol{X}}$ becomes easier to quantize, $\hat{\boldsymbol{W}}$ now becomes more difficult. Stage 3: SVDQuant further decomposes $\hat{\boldsymbol{W}}$ into a low-rank component $\boldsymbol{L}_1\boldsymbol{L}_2$ and a residual $\hat{\boldsymbol{W}}-\boldsymbol{L}_1\boldsymbol{L}_2$ with SVD. Thus, the quantization difficulty is alleviated by the low-rank branch, which runs at 16-bit precision.&lt;/p&gt; &#xA;&lt;h4&gt;Nunchaku Engine Design&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/engine.jpg&#34; alt=&#34;engine&#34;&gt; (a) Naïvely running low-rank branch with rank 32 will introduce 57% latency overhead due to extra read of 16-bit inputs in &lt;em&gt;Down Projection&lt;/em&gt; and extra write of 16-bit outputs in &lt;em&gt;Up Projection&lt;/em&gt;. Nunchaku optimizes this overhead with kernel fusion. (b) &lt;em&gt;Down Projection&lt;/em&gt; and &lt;em&gt;Quantize&lt;/em&gt; kernels use the same input, while &lt;em&gt;Up Projection&lt;/em&gt; and &lt;em&gt;4-Bit Compute&lt;/em&gt; kernels share the same output. To reduce data movement overhead, we fuse the first two and the latter two kernels together.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/efficiency.jpg&#34; alt=&#34;efficiency&#34;&gt;SVDQuant reduces the 12B FLUX.1 model size by 3.6× and cuts the 16-bit model&#39;s memory usage by 3.5×. With Nunchaku, our INT4 model runs 3.0× faster than the NF4 W4A16 baseline on both desktop and laptop NVIDIA RTX 4090 GPUs. Notably, on the laptop 4090, it achieves a total 10.1× speedup by eliminating CPU offloading. Our NVFP4 model is also 3.1× faster than both BF16 and NF4 on the RTX 5090 GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We provide tutorial videos to help you install and use Nunchaku on Windows, available in both &lt;a href=&#34;https://youtu.be/YHAVe-oM7U8?si=cM9zaby_aEHiFXk0&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://www.bilibili.com/video/BV1BTocYjEk5/?share_source=copy_web&amp;amp;vd_source=8926212fef622f25cc95380515ac74ee&#34;&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/a&gt;. You can also follow the corresponding step-by-step text guide at &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/docs/setup_windows.md&#34;&gt;&lt;code&gt;docs/setup_windows.md&lt;/code&gt;&lt;/a&gt;. If you run into issues, these resources are a good place to start.&lt;/p&gt; &#xA;&lt;h3&gt;Wheels&lt;/h3&gt; &#xA;&lt;h4&gt;Prerequisites&lt;/h4&gt; &#xA;&lt;p&gt;Before installation, ensure you have &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&amp;gt;=2.5&lt;/a&gt; installed. For example, you can use the following command to install PyTorch 2.6:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install torch==2.6 torchvision==0.21 torchaudio==2.6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install nunchaku&lt;/h4&gt; &#xA;&lt;p&gt;Once PyTorch is installed, you can directly install &lt;code&gt;nunchaku&lt;/code&gt; from &lt;a href=&#34;https://huggingface.co/mit-han-lab/nunchaku/tree/main&#34;&gt;Hugging Face&lt;/a&gt;, &lt;a href=&#34;https://modelscope.cn/models/Lmxyy1999/nunchaku&#34;&gt;ModelScope&lt;/a&gt; or &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/releases&#34;&gt;GitHub release&lt;/a&gt;. Be sure to select the appropriate wheel for your Python and PyTorch version. For example, for Python 3.11 and PyTorch 2.6:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install https://huggingface.co/mit-han-lab/nunchaku/resolve/main/nunchaku-0.2.0+torch2.6-cp311-cp311-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;For ComfyUI Users&lt;/h5&gt; &#xA;&lt;p&gt;If you&#39;re using the &lt;strong&gt;ComfyUI portable package&lt;/strong&gt;, make sure to install &lt;code&gt;nunchaku&lt;/code&gt; into the correct Python environment bundled with ComfyUI. To find the right Python path, launch ComfyUI and check the log output. You&#39;ll see something like this in the first several lines:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;** Python executable: G:\ComfyuI\python\python.exe&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use that Python executable to install &lt;code&gt;nunchaku&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&#34;G:\ComfyUI\python\python.exe&#34; -m pip install &amp;lt;your-wheel-file&amp;gt;.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Installing for Python 3.11 and PyTorch 2.6:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&#34;G:\ComfyUI\python\python.exe&#34; -m pip install https://github.com/mit-han-lab/nunchaku/releases/download/v0.2.0/nunchaku-0.2.0+torch2.6-cp311-cp311-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;For Blackwell GPUs (50-series)&lt;/h5&gt; &#xA;&lt;p&gt;If you&#39;re using a Blackwell GPU (e.g., 50-series GPUs), install a wheel with PyTorch 2.7 and higher. Additionally, use &lt;strong&gt;FP4 models&lt;/strong&gt; instead of INT4 models.&#34;&lt;/p&gt; &#xA;&lt;h3&gt;Build from Source&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure your CUDA version is &lt;strong&gt;at least 12.2 on Linux&lt;/strong&gt; and &lt;strong&gt;at least 12.6 on Windows&lt;/strong&gt;. If you&#39;re using a Blackwell GPU (e.g., 50-series GPUs), CUDA &lt;strong&gt;12.8 or higher is required&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For Windows users, please refer to &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/issues/6&#34;&gt;this issue&lt;/a&gt; for the instruction. Please upgrade your MSVC compiler to the latest version.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We currently support only NVIDIA GPUs with architectures sm_75 (Turing: RTX 2080), sm_86 (Ampere: RTX 3090, A6000), sm_89 (Ada: RTX 4090), and sm_80 (A100). See &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/issues/1&#34;&gt;this issue&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n nunchaku python=3.11&#xA;conda activate nunchaku&#xA;pip install torch torchvision torchaudio&#xA;pip install ninja wheel diffusers transformers accelerate sentencepiece protobuf huggingface_hub&#xA;&#xA;# For gradio demos&#xA;pip install peft opencv-python gradio spaces GPUtil&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To enable NVFP4 on Blackwell GPUs (e.g., 50-series GPUs), please install nightly PyTorch&amp;gt;=2.7 with CUDA&amp;gt;=12.8. The installation command can be:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;code&gt;nunchaku&lt;/code&gt; package: Make sure you have &lt;code&gt;gcc/g++&amp;gt;=11&lt;/code&gt;. If you don&#39;t, you can install it via Conda on Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install -c conda-forge gxx=11 gcc=11&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For Windows users, you can download and install the latest &lt;a href=&#34;https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Community&amp;amp;channel=Release&amp;amp;version=VS2022&amp;amp;source=VSLandingPage&amp;amp;cid=2030&amp;amp;passive=false&#34;&gt;Visual Studio&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Then build the package from source with&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/mit-han-lab/nunchaku.git&#xA;cd nunchaku&#xA;git submodule init&#xA;git submodule update&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are building wheels for distribution, use:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;NUNCHAKU_INSTALL_MODE=ALL NUNCHAKU_BUILD_WHEELS=1 python -m build --wheel --no-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Make sure to set the environment variable &lt;code&gt;NUNCHAKU_INSTALL_MODE&lt;/code&gt; to &lt;code&gt;ALL&lt;/code&gt;. Otherwise, the generated wheels will only work on GPUs with the same architecture as the build machine.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage Example&lt;/h2&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples&#34;&gt;examples&lt;/a&gt;, we provide minimal scripts for running INT4 &lt;a href=&#34;https://github.com/black-forest-labs/flux&#34;&gt;FLUX.1&lt;/a&gt; and &lt;a href=&#34;https://github.com/NVlabs/Sana&#34;&gt;SANA&lt;/a&gt; models with Nunchaku. It shares the same APIs as &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; and can be used in a similar way. For example, the &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev.py&#34;&gt;script&lt;/a&gt; for &lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-dev&#34;&gt;FLUX.1-dev&lt;/a&gt; is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import FluxPipeline&#xA;&#xA;from nunchaku import NunchakuFluxTransformer2dModel&#xA;from nunchaku.utils import get_precision&#xA;&#xA;precision = get_precision()  # auto-detect your precision is &#39;int4&#39; or &#39;fp4&#39; based on your GPU&#xA;transformer = NunchakuFluxTransformer2dModel.from_pretrained(&#xA;    f&#34;mit-han-lab/nunchaku-flux.1-dev/svdq-{precision}_r32-flux.1-dev.safetensors&#34;&#xA;)&#xA;pipeline = FluxPipeline.from_pretrained(&#xA;    &#34;black-forest-labs/FLUX.1-dev&#34;, transformer=transformer, torch_dtype=torch.bfloat16&#xA;).to(&#34;cuda&#34;)&#xA;image = pipeline(&#34;A cat holding a sign that says hello world&#34;, num_inference_steps=50, guidance_scale=3.5).images[0]&#xA;image.save(f&#34;flux.1-dev-{precision}.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you&#39;re using a &lt;strong&gt;Turing GPU (e.g., NVIDIA 20-series)&lt;/strong&gt;, make sure to set &lt;code&gt;torch_dtype=torch.float16&lt;/code&gt; and use our &lt;code&gt;nunchaku-fp16&lt;/code&gt; attention module as below. A complete example is available in &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-turing.py&#34;&gt;&lt;code&gt;examples/flux.1-dev-turing.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;FP16 Attention&lt;/h3&gt; &#xA;&lt;p&gt;In addition to FlashAttention-2, Nunchaku introduces a custom FP16 attention implementation that achieves up to &lt;strong&gt;1.2× faster performance&lt;/strong&gt; on NVIDIA 30-, 40-, and even 50-series GPUs—without loss in precision. To enable it, simply use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transformer.set_attention_impl(&#34;nunchaku-fp16&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-fp16attn.py&#34;&gt;&lt;code&gt;examples/flux.1-dev-fp16attn.py&lt;/code&gt;&lt;/a&gt; for a complete example.&lt;/p&gt; &#xA;&lt;h3&gt;First-Block Cache&lt;/h3&gt; &#xA;&lt;p&gt;Nunchaku supports &lt;a href=&#34;https://github.com/chengzeyi/ParaAttention?tab=readme-ov-file#first-block-cache-our-dynamic-caching&#34;&gt;First-Block Cache&lt;/a&gt; to accelerate long-step denoising. Enable it easily with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;apply_cache_on_pipe(pipeline, residual_diff_threshold=0.12)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can tune the &lt;code&gt;residual_diff_threshold&lt;/code&gt; to balance speed and quality: larger values yield faster inference at the cost of some quality. A recommended value is &lt;code&gt;0.12&lt;/code&gt;, which provides up to &lt;strong&gt;2× speedup&lt;/strong&gt; for 50-step denoising and &lt;strong&gt;1.4× speedup&lt;/strong&gt; for 30-step denoising. See the full example in &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-cache.py&#34;&gt;&lt;code&gt;examples/flux.1-dev-cache.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;CPU Offloading&lt;/h3&gt; &#xA;&lt;p&gt;To minimize GPU memory usage, Nunchaku supports CPU offloading—requiring as little as &lt;strong&gt;4 GiB&lt;/strong&gt; of GPU memory. You can enable it by setting &lt;code&gt;offload=True&lt;/code&gt; when initializing &lt;code&gt;NunchakuFluxTransformer2dModel&lt;/code&gt;, and then calling:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipeline.enable_sequential_cpu_offload()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a complete example, refer to &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-offload.py&#34;&gt;&lt;code&gt;examples/flux.1-dev-offload.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Customized LoRA&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/lora.jpg&#34; alt=&#34;lora&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2411.05007&#34;&gt;SVDQuant&lt;/a&gt; seamlessly integrates with off-the-shelf LoRAs without requiring requantization. You can simply use your LoRA with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transformer.update_lora_params(path_to_your_lora)&#xA;transformer.set_lora_strength(lora_strength)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;path_to_your_lora&lt;/code&gt; can also be a remote HuggingFace path. In &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-lora.py&#34;&gt;&lt;code&gt;examples/flux.1-dev-lora.py&lt;/code&gt;&lt;/a&gt;, we provide a minimal example script for running &lt;a href=&#34;https://huggingface.co/aleksa-codes/flux-ghibsky-illustration&#34;&gt;Ghibsky&lt;/a&gt; LoRA with SVDQuant&#39;s 4-bit FLUX.1-dev:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import FluxPipeline&#xA;&#xA;from nunchaku import NunchakuFluxTransformer2dModel&#xA;from nunchaku.utils import get_precision&#xA;&#xA;precision = get_precision()  # auto-detect your precision is &#39;int4&#39; or &#39;fp4&#39; based on your GPU&#xA;transformer = NunchakuFluxTransformer2dModel.from_pretrained(&#xA;    f&#34;mit-han-lab/nunchaku-flux.1-dev/svdq-{precision}_r32-flux.1-dev.safetensors&#34;&#xA;)&#xA;pipeline = FluxPipeline.from_pretrained(&#xA;    &#34;black-forest-labs/FLUX.1-dev&#34;, transformer=transformer, torch_dtype=torch.bfloat16&#xA;).to(&#34;cuda&#34;)&#xA;&#xA;### LoRA Related Code ###&#xA;transformer.update_lora_params(&#xA;    &#34;aleksa-codes/flux-ghibsky-illustration/lora.safetensors&#34;&#xA;)  # Path to your LoRA safetensors, can also be a remote HuggingFace path&#xA;transformer.set_lora_strength(1)  # Your LoRA strength here&#xA;### End of LoRA Related Code ###&#xA;&#xA;image = pipeline(&#xA;    &#34;GHIBSKY style, cozy mountain cabin covered in snow, with smoke curling from the chimney and a warm, inviting light spilling through the windows&#34;,  # noqa: E501&#xA;    num_inference_steps=25,&#xA;    guidance_scale=3.5,&#xA;).images[0]&#xA;image.save(f&#34;flux.1-dev-ghibsky-{precision}.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compose multiple LoRAs, you can use &lt;code&gt;nunchaku.lora.flux.compose.compose_lora&lt;/code&gt; to compose them. The usage is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;composed_lora = compose_lora(&#xA;    [&#xA;        (&#34;PATH_OR_STATE_DICT_OF_LORA1&#34;, lora_strength1),&#xA;        (&#34;PATH_OR_STATE_DICT_OF_LORA2&#34;, lora_strength2),&#xA;        # Add more LoRAs as needed&#xA;    ]&#xA;)  # set your lora strengths here when using composed lora&#xA;transformer.update_lora_params(composed_lora)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify individual strengths for each LoRA in the list. For a complete example, refer to &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples/flux.1-dev-multiple-lora.py&#34;&gt;&lt;code&gt;examples/flux.1-dev-multiple-lora.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For ComfyUI users, you can directly use our LoRA loader. The converted LoRA is deprecated. Please refer to &lt;a href=&#34;https://github.com/mit-han-lab/ComfyUI-nunchaku&#34;&gt;mit-han-lab/ComfyUI-nunchaku&lt;/a&gt; for more details.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ControlNets&lt;/h2&gt; &#xA;&lt;p&gt;Nunchaku supports both the &lt;a href=&#34;https://blackforestlabs.ai/flux-1-tools/&#34;&gt;FLUX.1-tools&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro&#34;&gt;FLUX.1-dev-ControlNet-Union-Pro&lt;/a&gt; models. Example scripts can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/control.jpg&#34; alt=&#34;control&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ComfyUI&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/mit-han-lab/ComfyUI-nunchaku&#34;&gt;mit-han-lab/ComfyUI-nunchaku&lt;/a&gt; for the usage in &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Gradio Demos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FLUX.1 Models &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text-to-image: see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/flux.1/t2i&#34;&gt;&lt;code&gt;app/flux.1/t2i&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Sketch-to-Image (&lt;a href=&#34;https://github.com/GaParmar/img2img-turbo&#34;&gt;pix2pix-Turbo&lt;/a&gt;): see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/flux.1/sketch&#34;&gt;&lt;code&gt;app/flux.1/sketch&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Depth/Canny-to-Image (&lt;a href=&#34;https://blackforestlabs.ai/flux-1-tools/&#34;&gt;FLUX.1-tools&lt;/a&gt;): see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/flux.1/depth_canny&#34;&gt;&lt;code&gt;app/flux.1/depth_canny&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Inpainting (&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev&#34;&gt;FLUX.1-Fill-dev&lt;/a&gt;): see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/flux.1/fill&#34;&gt;&lt;code&gt;app/flux.1/fill&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Redux (&lt;a href=&#34;https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev&#34;&gt;FLUX.1-Redux-dev&lt;/a&gt;): see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/flux.1/redux&#34;&gt;&lt;code&gt;app/flux.1/redux&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;SANA: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text-to-image: see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/sana/t2i&#34;&gt;&lt;code&gt;app/sana/t2i&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Customized Model Quantization&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/mit-han-lab/deepcompressor/tree/main/examples/diffusion&#34;&gt;mit-han-lab/deepcompressor&lt;/a&gt;. A simpler workflow is coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/app/flux/t2i/README.md&#34;&gt;app/flux/t2i/README.md&lt;/a&gt; for instructions on reproducing our paper&#39;s quality results and benchmarking inference latency on FLUX.1 models.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Please check &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/issues/266&#34;&gt;here&lt;/a&gt; for the roadmap for April.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;We warmly welcome contributions from the community! To get started, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/docs/contribution_guide.md&#34;&gt;contribution guide&lt;/a&gt; for instructions on how to contribute code to Nunchaku.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Encountering issues while using Nunchaku? Start by browsing our &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/nunchaku/main/docs/faq.md&#34;&gt;FAQ&lt;/a&gt; for common solutions. If you still need help, feel free to &lt;a href=&#34;https://github.com/mit-han-lab/nunchaku/issues&#34;&gt;open an issue&lt;/a&gt;. You’re also welcome to join our community discussions on &lt;a href=&#34;https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q&#34;&gt;&lt;strong&gt;Slack&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://discord.gg/Wk6PnwX9Sm&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://huggingface.co/mit-han-lab/nunchaku-artifacts/resolve/main/nunchaku/assets/wechat.jpg&#34;&gt;&lt;strong&gt;WeChat&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;For enterprises interested in adopting SVDQuant or Nunchaku, including technical consulting, sponsorship opportunities, or partnership inquiries, please contact us at &lt;a href=&#34;mailto:muyangli@mit.edu&#34;&gt;muyangli@mit.edu&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.02048&#34;&gt;Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models&lt;/a&gt;, NeurIPS 2022 &amp;amp; T-PAMI 2023&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.10438&#34;&gt;SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models&lt;/a&gt;, ICML 2023&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.04304&#34;&gt;Q-Diffusion: Quantizing Diffusion Models&lt;/a&gt;, ICCV 2023&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration&lt;/a&gt;, MLSys 2024&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.19481&#34;&gt;DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models&lt;/a&gt;, CVPR 2024&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.04532&#34;&gt;QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving&lt;/a&gt;, MLSys 2025&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.10629&#34;&gt;SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers&lt;/a&gt;, ICLR 2025&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;code&gt;nunchaku&lt;/code&gt; useful or relevant to your research, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{&#xA;  li2024svdquant,&#xA;  title={SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},&#xA;  author={Li*, Muyang and Lin*, Yujun and Zhang*, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},&#xA;  booktitle={The Thirteenth International Conference on Learning Representations},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We thank MIT-IBM Watson AI Lab, MIT and Amazon Science Hub, MIT AI Hardware Program, National Science Foundation, Packard Foundation, Dell, LG, Hyundai, and Samsung for supporting this research. We thank NVIDIA for donating the DGX server.&lt;/p&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/GaParmar/img2img-turbo&#34;&gt;img2img-turbo&lt;/a&gt; to train the sketch-to-image LoRA. Our text-to-image and image-to-image UI is built upon &lt;a href=&#34;https://huggingface.co/spaces/playgroundai/playground-v2.5/blob/main/app.py&#34;&gt;playground-v.25&lt;/a&gt; and &lt;a href=&#34;https://github.com/GaParmar/img2img-turbo/raw/main/gradio_sketch2image.py&#34;&gt;img2img-turbo&lt;/a&gt;, respectively. Our safety checker is borrowed from &lt;a href=&#34;https://github.com/mit-han-lab/hart&#34;&gt;hart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Nunchaku is also inspired by many open-source libraries, including (but not limited to) &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/qserve&#34;&gt;QServe&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq&#34;&gt;AWQ&lt;/a&gt;, &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;, and &lt;a href=&#34;https://github.com/efeslab/Atom&#34;&gt;Atom&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#mit-han-lab/nunchaku&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=mit-han-lab/nunchaku&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>