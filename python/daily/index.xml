<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-21T01:36:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>labelmeai/labelme</title>
    <updated>2024-06-21T01:36:07Z</updated>
    <id>tag:github.com,2024-06-21:/labelmeai/labelme</id>
    <link href="https://github.com/labelmeai/labelme" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/labelme/icons/icon.png&#34;&gt;&lt;br&gt;labelme &lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; Image Polygonal Annotation with Python &lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://pypi.python.org/pypi/labelme&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/labelme.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://pypi.org/project/labelme&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/labelme.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/wkentaro/labelme/actions&#34;&gt;&lt;img src=&#34;https://github.com/wkentaro/labelme/workflows/ci/badge.svg?branch=main&amp;amp;event=push&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#starter-guide&#34;&gt;&lt;b&gt;Starter Guide&lt;/b&gt;&lt;/a&gt; | &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#installation&#34;&gt;&lt;b&gt;Installation&lt;/b&gt;&lt;/a&gt; | &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#usage&#34;&gt;&lt;b&gt;Usage&lt;/b&gt;&lt;/a&gt; | &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#examples&#34;&gt;&lt;b&gt;Examples&lt;/b&gt;&lt;/a&gt; &#xA; &lt;!-- | &lt;a href=&#34;https://github.com/wkentaro/labelme/discussions&#34;&gt;&lt;b&gt;Community&lt;/b&gt;&lt;/a&gt; --&gt; &#xA; &lt;!-- | &lt;a href=&#34;https://www.youtube.com/playlist?list=PLI6LvFw0iflh3o33YYnVIfOpaO0hc5Dzw&#34;&gt;&lt;b&gt;Youtube FAQ&lt;/b&gt;&lt;/a&gt; --&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation/.readme/annotation.jpg&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Labelme is a graphical image annotation tool inspired by &lt;a href=&#34;http://labelme.csail.mit.edu&#34;&gt;http://labelme.csail.mit.edu&lt;/a&gt;.&lt;br&gt; It is written in Python and uses Qt for its graphical interface.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation/data_dataset_voc/JPEGImages/2011_000006.jpg&#34; width=&#34;19%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation/data_dataset_voc/SegmentationClass/2011_000006.png&#34; width=&#34;19%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation/data_dataset_voc/SegmentationClassVisualization/2011_000006.jpg&#34; width=&#34;19%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation/data_dataset_voc/SegmentationObject/2011_000006.png&#34; width=&#34;19%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation/data_dataset_voc/SegmentationObjectVisualization/2011_000006.jpg&#34; width=&#34;19%&#34;&gt;&lt;br&gt; &lt;i&gt;VOC dataset example of instance segmentation.&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/semantic_segmentation/.readme/annotation.jpg&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/bbox_detection/.readme/annotation.jpg&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/classification/.readme/annotation_cat.jpg&#34; width=&#34;35%&#34;&gt;&lt;br&gt; &lt;i&gt;Other examples (semantic segmentation, bbox detection, and classification).&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4310419/47907116-85667800-de82-11e8-83d0-b9f4eb33268f.gif&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/4310419/47922172-57972880-deae-11e8-84f8-e4324a7c856a.gif&#34; width=&#34;30%&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/14256482/46932075-92145f00-d080-11e8-8d09-2162070ae57c.png&#34; width=&#34;32%&#34;&gt;&lt;br&gt; &lt;i&gt;Various primitives (polygon, rectangle, circle, line, and point).&lt;/i&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Image annotation for polygon, rectangle, circle, line and point. (&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/tutorial&#34;&gt;tutorial&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Image flag annotation for classification and cleaning. (&lt;a href=&#34;https://github.com/wkentaro/labelme/pull/166&#34;&gt;#166&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Video annotation. (&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/video_annotation&#34;&gt;video annotation&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GUI customization (predefined labels / flags, auto-saving, label validation, etc). (&lt;a href=&#34;https://github.com/wkentaro/labelme/pull/144&#34;&gt;#144&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Exporting VOC-format dataset for semantic/instance segmentation. (&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/semantic_segmentation&#34;&gt;semantic segmentation&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation&#34;&gt;instance segmentation&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Exporting COCO-format dataset for instance segmentation. (&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation&#34;&gt;instance segmentation&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Starter Guide&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re new to Labelme, you can get started with &lt;a href=&#34;https://gumroad.labelme.io/l/starter-guide&#34;&gt;Labelme Starter Guide&lt;/a&gt; (FREE), which contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Installation guides&lt;/strong&gt; for all platforms: Windows, macOS, and Linux üíª&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Step-by-step tutorials&lt;/strong&gt;: first annotation to editing, exporting, and integrating with other programs üìï&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A compilation of valuable resources&lt;/strong&gt; for further exploration üîó.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;There are options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Platform agnostic installation: &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#anaconda&#34;&gt;Anaconda&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Platform specific installation: &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#ubuntu&#34;&gt;Ubuntu&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#macos&#34;&gt;macOS&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/#windows&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pre-build binaries from &lt;a href=&#34;https://github.com/wkentaro/labelme/releases&#34;&gt;the release section&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Anaconda&lt;/h3&gt; &#xA;&lt;p&gt;You need install &lt;a href=&#34;https://www.continuum.io/downloads&#34;&gt;Anaconda&lt;/a&gt;, then run below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# python3&#xA;conda create --name=labelme python=3&#xA;source activate labelme&#xA;# conda install -c conda-forge pyside2&#xA;# conda install pyqt&#xA;# pip install pyqt5  # pyqt5 can be installed via pip on python3&#xA;pip install labelme&#xA;# or you can install everything by conda command&#xA;# conda install labelme -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Ubuntu&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install labelme&#xA;&#xA;# or&#xA;sudo pip3 install labelme&#xA;&#xA;# or install standalone executable from:&#xA;# https://github.com/wkentaro/labelme/releases&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install pyqt  # maybe pyqt5&#xA;pip install labelme&#xA;&#xA;# or&#xA;brew install wkentaro/labelme/labelme  # command line interface&#xA;# brew install --cask wkentaro/labelme/labelme  # app&#xA;&#xA;# or install standalone executable/app from:&#xA;# https://github.com/wkentaro/labelme/releases&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://www.continuum.io/downloads&#34;&gt;Anaconda&lt;/a&gt;, then in an Anaconda Prompt run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name=labelme python=3&#xA;conda activate labelme&#xA;pip install labelme&#xA;&#xA;# or install standalone executable/app from:&#xA;# https://github.com/wkentaro/labelme/releases&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;labelme --help&lt;/code&gt; for detail.&lt;br&gt; The annotations are saved as a &lt;a href=&#34;http://www.json.org/&#34;&gt;JSON&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;labelme  # just open gui&#xA;&#xA;# tutorial (single image example)&#xA;cd examples/tutorial&#xA;labelme apc2016_obj3.jpg  # specify image file&#xA;labelme apc2016_obj3.jpg -O apc2016_obj3.json  # close window after the save&#xA;labelme apc2016_obj3.jpg --nodata  # not include image data but relative image path in JSON file&#xA;labelme apc2016_obj3.jpg \&#xA;  --labels highland_6539_self_stick_notes,mead_index_cards,kong_air_dog_squeakair_tennis_ball  # specify label list&#xA;&#xA;# semantic segmentation example&#xA;cd examples/semantic_segmentation&#xA;labelme data_annotated/  # Open directory to annotate all images in it&#xA;labelme data_annotated/ --labels labels.txt  # specify label list with a file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command Line Arguments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--output&lt;/code&gt; specifies the location that annotations will be written to. If the location ends with .json, a single annotation will be written to this file. Only one image can be annotated if a location is specified with .json. If the location does not end with .json, the program will assume it is a directory. Annotations will be stored in this directory with a name that corresponds to the image that the annotation was made on.&lt;/li&gt; &#xA; &lt;li&gt;The first time you run labelme, it will create a config file in &lt;code&gt;~/.labelmerc&lt;/code&gt;. You can edit this file and the changes will be applied the next time that you launch labelme. If you would prefer to use a config file from another location, you can specify this file with the &lt;code&gt;--config&lt;/code&gt; flag.&lt;/li&gt; &#xA; &lt;li&gt;Without the &lt;code&gt;--nosortlabels&lt;/code&gt; flag, the program will list labels in alphabetical order. When the program is run with this flag, it will display labels in the order that they are provided.&lt;/li&gt; &#xA; &lt;li&gt;Flags are assigned to an entire image. &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/classification&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Labels are assigned to a single polygon. &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/bbox_detection&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;FAQ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to convert JSON file to numpy array?&lt;/strong&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/tutorial#convert-to-dataset&#34;&gt;examples/tutorial&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to load label PNG file?&lt;/strong&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/tutorial#how-to-load-label-png-file&#34;&gt;examples/tutorial&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to get annotations for semantic segmentation?&lt;/strong&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/semantic_segmentation&#34;&gt;examples/semantic_segmentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How to get annotations for instance segmentation?&lt;/strong&gt; See &lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation&#34;&gt;examples/instance_segmentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/classification&#34;&gt;Image Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/bbox_detection&#34;&gt;Bounding Box Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/semantic_segmentation&#34;&gt;Semantic Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/instance_segmentation&#34;&gt;Instance Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/labelmeai/labelme/main/examples/video_annotation&#34;&gt;Video Annotation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to develop&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/wkentaro/labelme.git&#xA;cd labelme&#xA;&#xA;# Install anaconda3 and labelme&#xA;curl -L https://github.com/wkentaro/dotfiles/raw/main/local/bin/install_anaconda3.sh | bash -s .&#xA;source .anaconda3/bin/activate&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to build standalone executable&lt;/h3&gt; &#xA;&lt;p&gt;Below shows how to build the standalone executable on macOS, Linux and Windows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Setup conda&#xA;conda create --name labelme python=3.9&#xA;conda activate labelme&#xA;&#xA;# Build the standalone executable&#xA;pip install .&#xA;pip install &#39;matplotlib&amp;lt;3.3&#39;&#xA;pip install pyinstaller&#xA;pyinstaller labelme.spec&#xA;dist/labelme --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to contribute&lt;/h3&gt; &#xA;&lt;p&gt;Make sure below test passes on your environment.&lt;br&gt; See &lt;code&gt;.github/workflows/ci.yml&lt;/code&gt; for more detail.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements-dev.txt&#xA;&#xA;ruff format --check  # `ruff format` to auto-fix&#xA;ruff check  # `ruff check --fix` to auto-fix&#xA;MPLBACKEND=&#39;agg&#39; pytest -vsx tests/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo is the fork of &lt;a href=&#34;https://github.com/mpitid/pylabelme&#34;&gt;mpitid/pylabelme&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MervinPraison/PraisonAI</title>
    <updated>2024-06-21T01:36:07Z</updated>
    <id>tag:github.com,2024-06-21:/MervinPraison/PraisonAI</id>
    <link href="https://github.com/MervinPraison/PraisonAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PraisonAI application combines AutoGen and CrewAI or similar frameworks into a low-code solution for building and managing multi-agent LLM systems, focusing on simplicity, customisation, and efficient human-agent collaboration.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Praison AI&lt;/h1&gt; &#xA;&lt;p&gt;Praison AI, leveraging both AutoGen and CrewAI or any other agent framework, represents a low-code, centralised framework designed to simplify the creation and orchestration of multi-agent systems for various LLM applications, emphasizing ease of use, customization, and human-agent interaction.&lt;/p&gt; &#xA;&lt;h2&gt;TL;DR&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install praisonai&#xA;export OPENAI_API_KEY=&#34;Enter your API key&#34;&#xA;praisonai --init create a movie script about dog in moon&#xA;praisonai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install praisonai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Initialise&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#34;Enter your API key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate your OPENAI API KEY from here: &lt;a href=&#34;https://platform.openai.com/api-keys&#34;&gt;https://platform.openai.com/api-keys&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: You can use other providers such as Ollama, Mistral ... etc. Details are provided at the bottom.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;praisonai --init create a movie script about dog in moon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will automatically create agents.yaml file in the current directory.&lt;/p&gt; &#xA;&lt;h3&gt;To initialse with a specific agent framework (Optional):&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;praisonai --framework autogen --init create movie script about cat in mars&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;praisonai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m praisonai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Specify the agent framework (Optional):&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;praisonai --framework autogen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Full Automatic Mode&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;praisonai --auto create a movie script about Dog in Moon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Create Custom Tools&lt;/h2&gt; &#xA;&lt;h3&gt;TL;DR to Create a Custom Tool&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install praisonai duckduckgo-search&#xA;export OPENAI_API_KEY=&#34;Enter your API key&#34;&#xA;praisonai --init research about the latest AI News and prepare a detailed report&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;- InternetSearchTool&lt;/code&gt; in the agents.yaml file in the tools section.&lt;/li&gt; &#xA; &lt;li&gt;Create a file called tools.py and add this code &lt;a href=&#34;https://raw.githubusercontent.com/MervinPraison/PraisonAI/develop/tools.py&#34;&gt;tools.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;praisonai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-requisite to Create a Custom Tool&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;agents.yaml&lt;/code&gt; file should be present in the current directory.&lt;/p&gt; &#xA;&lt;p&gt;If it doesn&#39;t exist, create it by running the command &lt;code&gt;praisonai --init research about the latest AI News and prepare a detailed report&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1 to Create a Custom Tool&lt;/h3&gt; &#xA;&lt;p&gt;Create a file called tools.py in the same directory as the agents.yaml file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# example tools.py&#xA;from duckduckgo_search import DDGS&#xA;from praisonai_tools import BaseTool&#xA;&#xA;class InternetSearchTool(BaseTool):&#xA;    name: str = &#34;InternetSearchTool&#34;&#xA;    description: str = &#34;Search Internet for relevant information based on a query or latest news&#34;&#xA;&#xA;    def _run(self, query: str):&#xA;        ddgs = DDGS()&#xA;        results = ddgs.text(keywords=query, region=&#39;wt-wt&#39;, safesearch=&#39;moderate&#39;, max_results=5)&#xA;        return results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2 to Create a Custom Tool&lt;/h3&gt; &#xA;&lt;p&gt;Add the tool to the agents.yaml file as show below under the tools section &lt;code&gt;- InternetSearchTool&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;framework: crewai&#xA;topic: research about the latest AI News and prepare a detailed report&#xA;roles:&#xA;  research_analyst:&#xA;    backstory: Experienced in gathering and analyzing data related to AI news trends.&#xA;    goal: Analyze AI News trends&#xA;    role: Research Analyst&#xA;    tasks:&#xA;      gather_data:&#xA;        description: Conduct in-depth research on the latest AI News trends from reputable&#xA;          sources.&#xA;        expected_output: Comprehensive report on current AI News trends.&#xA;    tools:&#xA;    - InternetSearchTool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Test&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m unittest tests.test &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Agents Playbook&lt;/h2&gt; &#xA;&lt;h3&gt;Simple Playbook Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;framework: crewai&#xA;topic: Artificial Intelligence&#xA;roles:&#xA;  screenwriter:&#xA;    backstory: &#39;Skilled in crafting scripts with engaging dialogue about {topic}.&#39;&#xA;    goal: Create scripts from concepts.&#xA;    role: Screenwriter&#xA;    tasks:&#xA;      scriptwriting_task:&#xA;        description: &#39;Develop scripts with compelling characters and dialogue about {topic}.&#39;&#xA;        expected_output: &#39;Complete script ready for production.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Detailed Playbook Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;framework: crewai&#xA;topic: Artificial Intelligence&#xA;roles:&#xA;  movie_concept_creator:&#xA;    backstory: &#39;Creative thinker with a deep understanding of cinematic storytelling,&#xA;      capable of using AI-generated storylines to create unique and compelling movie&#xA;      ideas.&#39;&#xA;    goal: Generate engaging movie concepts using AI storylines&#xA;    role: Movie Concept Creator&#xA;    tasks:&#xA;      movie_concept_development:&#xA;        description: &#39;Develop movie concepts from AI-generated storylines, ensuring&#xA;          they are engaging and have strong narrative arcs.&#39;&#xA;        expected_output: &#39;Well-structured movie concept document with character&#xA;          bios, settings, and plot outlines.&#39;&#xA;  screenwriter:&#xA;    backstory: &#39;Expert in writing engaging dialogue and script structure, able to&#xA;      turn movie concepts into production-ready scripts.&#39;&#xA;    goal: Write compelling scripts based on movie concepts&#xA;    role: Screenwriter&#xA;    tasks:&#xA;      scriptwriting_task:&#xA;        description: &#39;Turn movie concepts into polished scripts with well-developed&#xA;          characters, strong dialogue, and effective scene transitions.&#39;&#xA;        expected_output: &#39;Production-ready script with a beginning, middle, and&#xA;          end, along with character development and engaging dialogues.&#39;&#xA;  editor:&#xA;    backstory: &#39;Adept at identifying inconsistencies, improving language usage,&#xA;      and maintaining the overall flow of the script.&#39;&#xA;    goal: Refine the scripts and ensure continuity of the movie storyline&#xA;    role: Editor&#xA;    tasks:&#xA;      editing_task:&#xA;        description: &#39;Review, edit, and refine the scripts to ensure they are cohesive&#xA;          and follow a well-structured narrative.&#39;&#xA;        expected_output: &#39;A polished final draft of the script with no inconsistencies,&#xA;          strong character development, and effective dialogue.&#39;&#xA;dependencies: []&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Include praisonai package in your project&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from praisonai import PraisonAI&#xA;&#xA;def basic(): # Basic Mode&#xA;    praison_ai = PraisonAI(agent_file=&#34;agents.yaml&#34;)&#xA;    praison_ai.main()&#xA;    &#xA;def advanced(): # Advanced Mode with options&#xA;    praison_ai = PraisonAI(&#xA;        agent_file=&#34;agents.yaml&#34;,&#xA;        framework=&#34;autogen&#34;,&#xA;    )&#xA;    praison_ai.main()&#xA;    &#xA;def auto(): # Full Automatic Mode&#xA;    praison_ai = PraisonAI(&#xA;        auto=&#34;Create a movie script about car in mars&#34;,&#xA;        framework=&#34;autogen&#34;&#xA;    )&#xA;    print(praison_ai.framework)&#xA;    praison_ai.main()&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    basic()&#xA;    advanced()&#xA;    auto()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Include CrewAI Tools&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;praisonai[crewai-tools]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Deploy&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud init&#xA;gcloud services enable run.googleapis.com&#xA;gcloud services enable containerregistry.googleapis.com&#xA;gcloud services enable cloudbuild.googleapis.com&#xA;&#xA;export OPENAI_MODEL_NAME=&#34;gpt-4o&#34;&#xA;export OPENAI_API_KEY=&#34;Enter your API key&#34;&#xA;export OPENAI_API_BASE=&#34;https://api.openai.com/v1&#34;&#xA;&#xA;yes | gcloud auth configure-docker us-central1-docker.pkg.dev &#xA;gcloud artifacts repositories create praisonai-repository --repository-format=docker --location=us-central1&#xA;&#xA;PROJECT_ID=$(gcloud config get-value project)&#xA;TAG=&#34;latest&#34;&#xA;docker build --platform linux/amd64 -t gcr.io/${PROJECT_ID}/praisonai-app:${TAG} .&#xA;docker tag gcr.io/${PROJECT_ID}/praisonai-app:${TAG} us-central1-docker.pkg.dev/${PROJECT_ID}/praisonai-repository/praisonai-app:${TAG}&#xA;docker push us-central1-docker.pkg.dev/${PROJECT_ID}/praisonai-repository/praisonai-app:${TAG}&#xA;&#xA;gcloud run deploy praisonai-service \&#xA;    --image us-central1-docker.pkg.dev/${PROJECT_ID}/praisonai-repository/praisonai-app:${TAG} \&#xA;    --platform managed \&#xA;    --region us-central1 \&#xA;    --allow-unauthenticated \&#xA;    --set-env-vars OPENAI_MODEL_NAME=${OPENAI_MODEL_NAME},OPENAI_API_KEY=${OPENAI_API_KEY},OPENAI_API_BASE=${OPENAI_API_BASE}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other Models&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Ollama&#xA;OPENAI_API_BASE=&#39;http://localhost:11434/v1&#39;&#xA;OPENAI_MODEL_NAME=&#39;mistral&#39;&#xA;OPENAI_API_KEY=&#39;NA&#39;&#xA;&#xA;FastChat¬∂&#xA;OPENAI_API_BASE=&#34;http://localhost:8001/v1&#34;&#xA;OPENAI_MODEL_NAME=&#39;oh-2.5m7b-q51&#39;&#xA;OPENAI_API_KEY=NA&#xA;&#xA;LM Studio¬∂&#xA;OPENAI_API_BASE=&#34;http://localhost:8000/v1&#34;&#xA;OPENAI_MODEL_NAME=NA&#xA;OPENAI_API_KEY=NA&#xA;&#xA;Mistral API¬∂&#xA;OPENAI_API_BASE=https://api.mistral.ai/v1&#xA;OPENAI_MODEL_NAME=&#34;mistral-small&#34;&#xA;OPENAI_API_KEY=your-mistral-api-key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork on GitHub: Use the &#34;Fork&#34; button on the repository page.&lt;/li&gt; &#xA; &lt;li&gt;Clone your fork: &lt;code&gt;git clone https://github.com/yourusername/praisonAI.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a branch: &lt;code&gt;git checkout -b new-feature&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Make changes and commit: &lt;code&gt;git commit -am &#34;Add some feature&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Push to your fork: &lt;code&gt;git push origin new-feature&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request via GitHub&#39;s web interface.&lt;/li&gt; &#xA; &lt;li&gt;Await feedback from project maintainers.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>XPixelGroup/BasicSR</title>
    <updated>2024-06-21T01:36:07Z</updated>
    <id>tag:github.com,2024-06-21:/XPixelGroup/BasicSR</id>
    <link href="https://github.com/XPixelGroup/BasicSR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source Image and Video Restoration Toolbox for Super-resolution, Denoise, Deblurring, etc. Currently, it includes EDSR, RCAN, SRResNet, SRGAN, ESRGAN, EDVR, BasicVSR, SwinIR, ECBSR, etc. Also support StyleGAN2, DFDNet.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/assets/basicsr_xpixel_logo.png&#34; height=&#34;120&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  &lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/b&gt;&#xA; &lt;/div&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/xinntao/BasicSR/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/xinntao/basicsr.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/basicsr/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/basicsr&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lgtm.com/projects/g/xinntao/BasicSR/context:python&#34;&gt;&lt;img src=&#34;https://img.shields.io/lgtm/grade/python/g/xinntao/BasicSR.svg?logo=lgtm&amp;amp;logoWidth=18&#34; alt=&#34;Language grade: Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/BasicSR/raw/master/.github/workflows/pylint.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/BasicSR/actions/workflows/pylint.yml/badge.svg?sanitize=true&#34; alt=&#34;python lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/BasicSR/raw/master/.github/workflows/publish-pip.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/BasicSR/actions/workflows/publish-pip.yml/badge.svg?sanitize=true&#34; alt=&#34;Publish-pip&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinntao/BasicSR/raw/master/.github/workflows/gitee-mirror.yml&#34;&gt;&lt;img src=&#34;https://github.com/xinntao/BasicSR/actions/workflows/gitee-mirror.yml/badge.svg?sanitize=true&#34; alt=&#34;gitee mirror&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;‚ö°&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/#-HOWTOs&#34;&gt;&lt;strong&gt;HowTo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üîß&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/INSTALL.md&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üíª&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/TrainTest.md&#34;&gt;&lt;strong&gt;Training Commands&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üê¢&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/DatasetPreparation.md&#34;&gt;&lt;strong&gt;DatasetPrepare&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üè∞&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/ModelZoo.md&#34;&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;üìï&lt;a href=&#34;https://github.com/XPixelGroup/BasicSR-docs&#34;&gt;&lt;strong&gt;‰∏≠ÊñáËß£ËØªÊñáÊ°£&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üìä &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/scripts/plot&#34;&gt;&lt;strong&gt;Plot scripts&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; üìù&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/introduction.md&#34;&gt;Introduction&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://github.com/XPixelGroup/BasicSR/tree/master/colab&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; height=&#34;18&#34; alt=&#34;google colab logo&#34;&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚è≥&lt;a href=&#34;https://github.com/xinntao/BasicSR/projects&#34;&gt;TODO List&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; ‚ùì&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/FAQ.md&#34;&gt;FAQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üöÄ We add &lt;a href=&#34;https://github.com/xinntao/BasicSR-examples&#34;&gt;BasicSR-Examples&lt;/a&gt;, which provides guidance and templates of using BasicSR as a python package. üöÄ &lt;br&gt; üì¢ &lt;strong&gt;ÊäÄÊúØ‰∫§ÊµÅQQÁæ§&lt;/strong&gt;Ôºö&lt;strong&gt;320960100&lt;/strong&gt; ‚ÄÉ ÂÖ•Áæ§Á≠îÊ°àÔºö&lt;strong&gt;‰∫íÂ∏Æ‰∫íÂä©ÂÖ±ÂêåËøõÊ≠•&lt;/strong&gt; &lt;br&gt; üß≠ &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/#-contact&#34;&gt;ÂÖ•Áæ§‰∫åÁª¥Á†Å&lt;/a&gt; (QQ„ÄÅÂæÆ‰ø°) ‚ÄÉ‚ÄÉ &lt;a href=&#34;https://docs.qq.com/doc/DYXBSUmxOT0xBZ05u&#34;&gt;ÂÖ•Áæ§ÊåáÂçó (ËÖæËÆØÊñáÊ°£)&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;BasicSR (&lt;strong&gt;Basic&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt;uper &lt;strong&gt;R&lt;/strong&gt;estoration) is an open-source &lt;strong&gt;image and video restoration&lt;/strong&gt; toolbox based on PyTorch, such as super-resolution, denoise, deblurring, JPEG artifacts removal, &lt;em&gt;etc&lt;/em&gt;.&lt;br&gt; BasicSR (&lt;strong&gt;Basic&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt;uper &lt;strong&gt;R&lt;/strong&gt;estoration) ÊòØ‰∏Ä‰∏™Âü∫‰∫é PyTorch ÁöÑÂºÄÊ∫ê ÂõæÂÉèËßÜÈ¢ëÂ§çÂéüÂ∑•ÂÖ∑ÁÆ±, ÊØîÂ¶Ç Ë∂ÖÂàÜËæ®Áéá, ÂéªÂô™, ÂéªÊ®°Á≥ä, Âéª JPEG ÂéãÁº©Âô™Â£∞Á≠â.&lt;/p&gt; &#xA;&lt;p&gt;üö© &lt;strong&gt;New Features/Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ July 26, 2022. Add plot scripts üìä&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/scripts/plot&#34;&gt;Plot&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ May 9, 2022. BasicSR joins &lt;a href=&#34;http://xpixel.group/&#34;&gt;XPixel&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Oct 5, 2021. Add &lt;strong&gt;ECBSR training and testing&lt;/strong&gt; codes: &lt;a href=&#34;https://github.com/xindongzhang/ECBSR&#34;&gt;ECBSR&lt;/a&gt;. &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;ACMMM21: Edge-oriented Convolution Block for Real-time Super Resolution on Mobile Devices&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Sep 2, 2021. Add &lt;strong&gt;SwinIR training and testing&lt;/strong&gt; codes: &lt;a href=&#34;https://github.com/JingyunLiang/SwinIR&#34;&gt;SwinIR&lt;/a&gt; by &lt;a href=&#34;https://github.com/JingyunLiang&#34;&gt;Jingyun Liang&lt;/a&gt;. More details are in &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/HOWTOs.md#how-to-train-swinir-sr&#34;&gt;HOWTOs.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Aug 5, 2021. Add NIQE, which produces the same results as MATLAB (both are 5.7296 for tests/data/baboon.png).&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ July 31, 2021. Add &lt;strong&gt;bi-directional video super-resolution&lt;/strong&gt; codes: &lt;a href=&#34;https://arxiv.org/abs/2012.02181&#34;&gt;&lt;strong&gt;BasicVSR&lt;/strong&gt; and IconVSR&lt;/a&gt;. &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;CVPR21: BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/history_updates.md&#34;&gt;More&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If BasicSR helps your research or work, please help to ‚≠ê this repo or recommend it to your friends. Thanksüòä &lt;br&gt; Other recommended projects:&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;Real-ESRGAN&lt;/a&gt;: A practical algorithm for general image restoration&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt;: A practical algorithm for real-world face restoration &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;facexlib&lt;/a&gt;: A collection that provides useful face-relation functions.&lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyView&#34;&gt;HandyView&lt;/a&gt;: A PyQt5-based image viewer that is handy for view and comparison. &lt;br&gt; ‚ñ∂Ô∏è &lt;a href=&#34;https://github.com/xinntao/HandyFigure&#34;&gt;HandyFigure&lt;/a&gt;: Open source of paper figures &lt;br&gt; &lt;sub&gt;(&lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/xinntao/EDVR&#34;&gt;EDVR&lt;/a&gt;, &lt;a href=&#34;https://github.com/xinntao/DNI&#34;&gt;DNI&lt;/a&gt;, &lt;a href=&#34;https://github.com/xinntao/SFTGAN&#34;&gt;SFTGAN&lt;/a&gt;)&lt;/sub&gt; &lt;sub&gt;(&lt;a href=&#34;https://github.com/xinntao/HandyCrawler&#34;&gt;HandyCrawler&lt;/a&gt;, &lt;a href=&#34;https://github.com/xinntao/HandyWriting&#34;&gt;HandyWriting&lt;/a&gt;)&lt;/sub&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;‚ö° HOWTOs&lt;/h2&gt; &#xA;&lt;p&gt;We provide simple pipelines to train/test/inference models for a quick start. These pipelines/commands cannot cover all the cases and more details are in the following sections.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;GAN&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;StyleGAN2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/HOWTOs.md#How-to-train-StyleGAN2&#34;&gt;Train&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/HOWTOs.md#How-to-inference-StyleGAN2&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Face Restoration&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DFDNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/HOWTOs.md#How-to-inference-DFDNet&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Super Resolution&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ESRGAN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SRGAN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;EDSR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SRResNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;RCAN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SwinIR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/HOWTOs.md#how-to-train-swinir-sr&#34;&gt;Train&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/docs/HOWTOs.md#how-to-inference-swinir-sr&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;EDVR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DUF&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;BasicVSR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;TOF&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Deblurring&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DeblurGANv2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Denoise&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;RIDNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CBDNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;TODO&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;‚ú® &lt;strong&gt;Projects that use BasicSR&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;&lt;strong&gt;Real-ESRGAN&lt;/strong&gt;&lt;/a&gt;: A practical algorithm for general image restoration&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;&lt;strong&gt;GFPGAN&lt;/strong&gt;&lt;/a&gt;: A practical algorithm for real-world face restoration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;BasicSR&lt;/code&gt; in your open-source projects, welcome to contact me (by &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/#-contact&#34;&gt;email&lt;/a&gt; or opening an issue/pull request). I will add your projects to the above list üòä&lt;/p&gt; &#xA;&lt;h2&gt;üìú License and Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/LICENSE.txt&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;br&gt; More details about &lt;strong&gt;license&lt;/strong&gt; and &lt;strong&gt;acknowledgement&lt;/strong&gt; are in &lt;a href=&#34;https://raw.githubusercontent.com/XPixelGroup/BasicSR/master/LICENSE/README.md&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üåè Citations&lt;/h2&gt; &#xA;&lt;p&gt;If BasicSR helps your research or work, please cite BasicSR.&lt;br&gt; The following is a BibTeX reference. The BibTeX entry requires the &lt;code&gt;url&lt;/code&gt; LaTeX package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{basicsr,&#xA;  author =       {Xintao Wang and Liangbin Xie and Ke Yu and Kelvin C.K. Chan and Chen Change Loy and Chao Dong},&#xA;  title =        {{BasicSR}: Open Source Image and Video Restoration Toolbox},&#xA;  howpublished = {\url{https://github.com/XPixelGroup/BasicSR}},&#xA;  year =         {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Xintao Wang, Liangbin Xie, Ke Yu, Kelvin C.K. Chan, Chen Change Loy and Chao Dong. BasicSR: Open Source Image and Video Restoration Toolbox. &lt;a href=&#34;https://github.com/xinntao/BasicSR&#34;&gt;https://github.com/xinntao/BasicSR&lt;/a&gt;, 2022.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üìß Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please email &lt;code&gt;xintao.alpha@gmail.com&lt;/code&gt;, &lt;code&gt;xintao.wang@outlook.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;QQÁæ§&lt;/strong&gt;: Êâ´ÊèèÂ∑¶Ëæπ‰∫åÁª¥Á†Å ÊàñËÄÖ ÊêúÁ¥¢QQÁæ§Âè∑: 320960100 ‚ÄÉ ÂÖ•Áæ§Á≠îÊ°àÔºö‰∫íÂ∏Æ‰∫íÂä©ÂÖ±ÂêåËøõÊ≠•&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ÂæÆ‰ø°Áæ§&lt;/strong&gt;: Êàë‰ª¨ÁöÑ‰∏ÄÁæ§Â∑≤ÁªèÊª°500‰∫∫Âï¶Ôºå‰∫åÁæ§‰πüË∂ÖËøá200‰∫∫‰∫ÜÔºõËøõÁæ§ÂèØ‰ª•Ê∑ªÂä† Liangbin ÁöÑ‰∏™‰∫∫ÂæÆ‰ø° (Âè≥Ëæπ‰∫åÁª¥Á†Å)Ôºå‰ªñ‰ºöÂú®Á©∫Èó≤ÁöÑÊó∂ÂÄôÊãâÂ§ßÂÆ∂ÂÖ•Áæ§~&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/17445847/134879983-6f2d663b-16e7-49f2-97e1-7c53c8a5f71a.jpg&#34; height=&#34;300&#34;&gt; ‚ÄÉ &lt;img src=&#34;https://user-images.githubusercontent.com/17445847/139572512-8e192aac-00fa-432b-ac8e-a33026b019df.png&#34; height=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=XPixelGroup/BasicSR&#34; alt=&#34;visitors&#34;&gt; (start from 2022-11-06)&lt;/p&gt;</summary>
  </entry>
</feed>