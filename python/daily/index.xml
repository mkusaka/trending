<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-12T01:57:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>zhyever/PatchFusion</title>
    <updated>2023-12-12T01:57:32Z</updated>
    <id>tag:github.com,2023-12-12:/zhyever/PatchFusion</id>
    <link href="https://github.com/zhyever/PatchFusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;PatchFusion &lt;/h1&gt; &#xA; &lt;h3&gt;An End-to-End Tile-Based Framework &lt;br&gt; for High-Resolution Monocular Metric Depth Estimation&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://zhyever.github.io/patchfusion/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zhyever/PatchFusion/main/examples/badge-website.svg?sanitize=true&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.02284&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-PDF-b31b1b&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/zhyever/PatchFusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-yellow&#34; alt=&#34;Hugging Face Space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/zhyever/PatchFusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-yellow&#34; alt=&#34;Hugging Face Model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://zhyever.github.io/&#34;&gt;Zhenyu Li&lt;/a&gt;, &lt;a href=&#34;https://shariqfarooq123.github.io/&#34;&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href=&#34;https://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt;. &lt;br&gt;KAUST&lt;/p&gt; &#xA; &lt;center&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/zhyever/PatchFusion/main/examples/showcase_3.gif&#34;&gt; &#xA; &lt;/center&gt; &#xA; &lt;center&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/zhyever/PatchFusion/main/examples/showcase_2.gif&#34;&gt; &#xA; &lt;/center&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;strong&gt;DEMO&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Our official huggingface demo is available &lt;a href=&#34;https://huggingface.co/spaces/zhyever/PatchFusion&#34;&gt;here&lt;/a&gt;! You can have a test with your own high-resolution image, even without a local GPU! It only takes 1min for depth prediction plus controlnet generation!&lt;/p&gt; &#xA;&lt;p&gt;Thanks for the kind support from &lt;a href=&#34;https://github.com/hysts&#34;&gt;hysts&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Environment setup&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The project depends on :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch&lt;/a&gt; (Main framework)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://timm.fast.ai/&#34;&gt;timm&lt;/a&gt; (Backbone helper for MiDaS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/isl-org/ZoeDepth&#34;&gt;ZoeDepth&lt;/a&gt; (Main baseline)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; (For potential application)&lt;/li&gt; &#xA; &lt;li&gt;pillow, matplotlib, scipy, h5py, opencv (utilities)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Install environment using &lt;code&gt;environment.yml&lt;/code&gt; :&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;a href=&#34;https://github.com/mamba-org/mamba&#34;&gt;mamba&lt;/a&gt; (fastest):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mamba (or micromamba) env create -n patchfusion --file environment.yml&#xA;mamba (or micromamba) activate patchfusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using conda :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -n patchfusion --file environment.yml&#xA;conda activate patchfusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Pre-Train Model&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Download our pre-trained model &lt;a href=&#34;https://drive.google.com/file/d/13M_qLOVSANDT1ss59Iebzjj3d0ZeVCi4/view?usp=sharing&#34;&gt;here&lt;/a&gt;, and put this checkpoint at &lt;code&gt;nfs/patchfusion_u4k.pt&lt;/code&gt; as preparation for the following steps.&lt;/p&gt; &#xA;&lt;p&gt;If you want to play the ControlNet demo, please download the pre-trained ControlNet model &lt;a href=&#34;https://huggingface.co/lllyasviel/ControlNet/blob/main/models/control_sd15_depth.pth&#34;&gt;here&lt;/a&gt;, and put this checkpoint at &lt;code&gt;nfs/control_sd15_depth.pth&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Gradio Demo&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We provide a UI demo built using &lt;a href=&#34;https://gradio.app/&#34;&gt;gradio&lt;/a&gt;. To get started, install UI requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r ui_requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch the gradio UI for depth estimation or image to 3D:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./ui_prediction.py --model zoedepth_custom --ckp_path nfs/patchfusion_u4k.pt --model_cfg_path ./zoedepth/models/zoedepth_custom/configs/config_zoedepth_patchfusion.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Launch the gradio UI for depth-guided image generation with ControlNet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./ui_generative.py --model zoedepth_custom --ckp_path nfs/patchfusion_u4k.pt --model_cfg_path ./zoedepth/models/zoedepth_custom/configs/config_zoedepth_patchfusion.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;strong&gt;User Inference&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Put your images in folder &lt;code&gt;path/to/your/folder&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run codes:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./infer_user.py --model zoedepth_custom --ckp_path nfs/patchfusion_u4k.pt --model_cfg_path ./zoedepth/models/zoedepth_custom/configs/config_zoedepth_patchfusion.json --rgb_dir path/to/your/folder --show --show_path path/to/show --save --save_path path/to/save --mode r128 --boundary 0 --blur_mask&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check visualization results in &lt;code&gt;path/to/show&lt;/code&gt; and depth results in &lt;code&gt;path/to/save&lt;/code&gt;, respectively.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We recommand users to use &lt;code&gt;--blur_mask&lt;/code&gt; to reduce patch artifacts, though we didn&#39;t use it in our standard evaluation process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--mode&lt;/code&gt;: select from p16, p49, and r&lt;strong&gt;n&lt;/strong&gt;, where &lt;strong&gt;n&lt;/strong&gt; is the number of random added patches.&lt;/li&gt; &#xA; &lt;li&gt;Please refer to &lt;code&gt;infer_user.py&lt;/code&gt; for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the paper&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{li2023patchfusion,&#xA;    title={PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation}, &#xA;    author={Zhenyu Li and Shariq Farooq Bhat and Peter Wonka},&#xA;    year={2023},&#xA;    eprint={2312.02284},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>LTH14/rcg</title>
    <updated>2023-12-12T01:57:32Z</updated>
    <id>tag:github.com,2023-12-12:/LTH14/rcg</id>
    <link href="https://github.com/LTH14/rcg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch implementation of RCG https://arxiv.org/abs/2312.03701&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RCG PyTorch Implementation&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/figures/method.png&#34; width=&#34;420&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is a PyTorch/GPU implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2312.03701&#34;&gt;Self-conditioned Image Generation via Generating Representations&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Article{RCG2023,&#xA;  author  = {Tianhong Li and Dina Katabi and Kaiming He},&#xA;  journal = {arXiv:2312.03701},&#xA;  title   = {Self-conditioned Image Generation via Generating Representations},&#xA;  year    = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;RCG is a self-conditioned image generation framework that achieves SOTA class-unconditional image generation performance on ImageNet-1K, bridging the long-standing performance gap between class-unconditional and class-conditional image generation.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/figures/result.png&#34; width=&#34;560&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;http://image-net.org/download&#34;&gt;ImageNet&lt;/a&gt; dataset, and place it in your &lt;code&gt;IMAGENET_DIR&lt;/code&gt;. Prepare the ImageNet validation set for FID evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python prepare_imgnet_val.py --data_path ${IMAGENET_DIR} --output_dir imagenet-val&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;A suitable &lt;a href=&#34;https://conda.io/&#34;&gt;conda&lt;/a&gt; environment named &lt;code&gt;rcg&lt;/code&gt; can be created and activated with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate rcg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the code&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/LTH14/rcg.git&#xA;cd rcg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://drive.google.com/file/d/13S_unB87n6KKuuMdyMnyExW0G1kplTbP/view?usp=sharing&#34;&gt;this link&lt;/a&gt; to download the pre-trained VQGAN tokenzier as &lt;code&gt;vqgan_jax_strongaug.ckpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://dl.fbaipublicfiles.com/moco-v3/vit-b-300ep/vit-b-300ep.pth.tar&#34;&gt;this link&lt;/a&gt; to download the pre-trained moco v3 ViT-B encoder and name it as &lt;code&gt;pretrained_enc_ckpts/mocov3/vitb.pth.tar&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://drive.google.com/file/d/1Foa2-FqhwIFYjcAAbY9sXyO-1Vwwx-_9/view?usp=sharing&#34;&gt;this link&lt;/a&gt; to download the pre-trained moco v3 ViT-L encoder and name it as &lt;code&gt;pretrained_enc_ckpts/mocov3/vitl.pth.tar&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;RDM&lt;/h3&gt; &#xA;&lt;p&gt;To train the Moco v3 ViT-B representation diffusion model using 4 V100 GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=4 --nnodes=1 --node_rank=0 \&#xA;main_rdm.py \&#xA;--config config/rdm/mocov3vitb_simplemlp_l12_w1536.yaml \&#xA;--batch_size 128 --input_size 256 \&#xA;--epochs 200 \&#xA;--blr 1e-6 --weight_decay 0.01 \&#xA;--output_dir ${OUTPUT_DIR} \&#xA;--data_path ${IMAGENET_DIR} \&#xA;--dist_url tcp://${MASTER_SERVER_ADDRESS}:2214&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To continue a previously interrupted training session, set &lt;code&gt;--resume&lt;/code&gt; to the &lt;code&gt;OUTPUT_DIR&lt;/code&gt; where &lt;code&gt;checkpoint-last.pth&lt;/code&gt; is stored.&lt;/p&gt; &#xA;&lt;p&gt;The following table provides the pre-trained weights of the Moco v3 ViT-B/ViT-L RDM used in the paper:&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE HEADER --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th valign=&#34;bottom&#34;&gt;&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Moco v3 ViT-B&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Moco v3 ViT-L&lt;/th&gt; &#xA;   &lt;!-- TABLE BODY --&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Class-unconditional RDM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1gdsvzKLmmBWuF4Ymy4rQ_T1t6dDHnTEA/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/config/rdm/mocov3vitb_simplemlp_l12_w1536.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1E5E3i9LRpSy0tVF7NA0bGXEh4CrjHAXz/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/config/rdm/mocov3vitl_simplemlp_l12_w1536.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Class-conditional RDM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1roanmVfg-UaddVehstQErvByqi0OYs2R/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/config/rdm/mocov3vitb_simplemlp_l12_w1536_classcond.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1lZmXOcdHE97Qmn2azNAo2tNVX7dtTAkY/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/config/rdm/mocov3vitl_simplemlp_l12_w1536_classcond.yaml&#34;&gt;config&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pixel Generator: MAGE&lt;/h3&gt; &#xA;&lt;p&gt;To train a MAGE-B conditioned on Moco v3 ViT-B representations, using 64 V100 GPUs for 200 epochs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=8 --nnodes=8 --node_rank=0 \&#xA;main_mage.py \&#xA;--pretrained_enc_arch mocov3_vit_base \&#xA;--pretrained_enc_path pretrained_enc_ckpts/mocov3/vitb.pth.tar --rep_drop_prob 0.1 \&#xA;--use_rep --rep_dim 256 --pretrained_enc_withproj --pretrained_enc_proj_dim 256 \&#xA;--pretrained_rdm_cfg ${RDM_CFG_PATH} --pretrained_rdm_ckpt ${RDM_CKPT_PATH} \&#xA;--rdm_steps 250 --eta 1.0 --temp 6.0 --num_iter 20 --num_images 50000 --cfg 0.0 \&#xA;--batch_size 64 --input_size 256 \&#xA;--model mage_vit_base_patch16 \&#xA;--mask_ratio_min 0.5 --mask_ratio_max 1.0 --mask_ratio_mu 0.75 --mask_ratio_std 0.25 \&#xA;--epochs 200 \&#xA;--warmup_epochs 10 \&#xA;--blr 1.5e-4 --weight_decay 0.05 \&#xA;--output_dir ${OUTPUT_DIR} \&#xA;--data_path ${IMAGENET_DIR} \&#xA;--dist_url tcp://${MASTER_SERVER_ADDRESS}:2214&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train a MAGE-L conditioned on Moco v3 ViT-L representations, change &lt;code&gt;RDM_CFG_PATH&lt;/code&gt; and &lt;code&gt;RDM_CKPT_PATH&lt;/code&gt; for Moco v3 ViT-L RDM, as well as the following arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--pretrained_enc_arch mocov3_vit_large --pretrained_enc_path pretrained_enc_ckpts/mocov3/vitl.pth.tar --temp 11.0 --model mage_vit_large_patch16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Resume&lt;/strong&gt;: set &lt;code&gt;--resume&lt;/code&gt; to the &lt;code&gt;OUTPUT_DIR&lt;/code&gt; where &lt;code&gt;checkpoint-last.pth&lt;/code&gt; is stored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: set &lt;code&gt;--resume&lt;/code&gt; to the pre-trained MAGE checkpoint and include the &lt;code&gt;--evaluate&lt;/code&gt; flag in the above script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-trained Models&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;tbody&gt; &#xA;  &lt;!-- START TABLE --&gt; &#xA;  &lt;!-- TABLE HEADER --&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;th valign=&#34;bottom&#34;&gt;&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Rep. Cond. MAGE-B&lt;/th&gt; &#xA;   &lt;th valign=&#34;bottom&#34;&gt;Rep. Cond. MAGE-L&lt;/th&gt; &#xA;   &lt;!-- TABLE BODY --&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Checkpoint&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1iZY0ujWp5GVochTLj0U6j4HgVTOyWPUI/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1nQh9xCqjQCd78zKwn2L9eLfLyVosb1hp/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Class-unconditional Generation (w/o CFG) &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=4.18, IS=177.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=3.56, IS=186.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Class-unconditional Generation (w/ CFG) &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=4.31, IS=214.9 (cfg=1.0)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=3.31, IS=253.4 (cfg=6.0)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Class-conditional Generation (w/o CFG) &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=4.09, IS=194.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=3.49, IS=215.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Class-conditional Generation (w/ CFG) &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=4.64, IS=242.6 (cfg=1.0)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FID=3.90, IS=300.7 (cfg=6.0)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: use &lt;code&gt;viz_rcg.ipynb&lt;/code&gt; to visualize generation results.&lt;/p&gt; &#xA;&lt;p&gt;Class-unconditional generation examples:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/figures/qualitative-uncond.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Class-conditional generation examples:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LTH14/rcg/main/figures/qualitative-clscond.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Pixel Generator: ADM&lt;/h3&gt; &#xA;&lt;p&gt;To train an ADM conditioned on Moco v3 ViT-B representations, using 128 V100 GPUs for 100 epochs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=8 --nnodes=16 --node_rank=0 \&#xA;main_adm.py \&#xA;--rep_cond --rep_dim 256 \&#xA;--pretrained_enc_arch mocov3_vit_base \&#xA;--pretrained_enc_path pretrained_enc_ckpts/mocov3/vitb.pth.tar \&#xA;--pretrained_rdm_cfg ${RDM_CFG_PATH} \&#xA;--pretrained_rdm_ckpt ${RDM_CKPT_PATH} \&#xA;--batch_size 2 --image_size 256 \&#xA;--epochs 100  \&#xA;--lr 1e-4 --weight_decay 0.0 \&#xA;--attention_resolutions 32,16,8 --diffusion_steps 1000 \&#xA;--learn_sigma --noise_schedule linear \&#xA;--num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown \&#xA;--use_scale_shift_norm \&#xA;--gen_timestep_respacing ddim25 --use_ddim \&#xA;--output_dir ${OUTPUT_DIR} \&#xA;--data_path ${IMAGENET_DIR} \&#xA;--dist_url tcp://${MASTER_SERVER_ADDRESS}:2214&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Resume&lt;/strong&gt;: set &lt;code&gt;--resume&lt;/code&gt; to the &lt;code&gt;OUTPUT_DIR&lt;/code&gt; where &lt;code&gt;checkpoint-last.pth&lt;/code&gt; is stored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: set &lt;code&gt;--resume&lt;/code&gt; to the pre-trained ADM checkpoint and include the &lt;code&gt;--evaluate&lt;/code&gt; flag in the above script. Set &lt;code&gt;--gen_timestep_respacing 250&lt;/code&gt; and disable &lt;code&gt;--use_ddim&lt;/code&gt; for better generation performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-trained ADM&lt;/strong&gt; conditioned on Moco v3 ViT-B representations can be downloaded &lt;a href=&#34;https://drive.google.com/file/d/1aEY8L3BB7QxtFOP-DUB0TGShq9L4gBIP/view?usp=sharing&#34;&gt;here&lt;/a&gt; (FID=7.21, IS=108.9).&lt;/p&gt; &#xA;&lt;h3&gt;Pixel Generator: LDM&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://ommer-lab.com/files/latent-diffusion/vq-f8.zip&#34;&gt;this link&lt;/a&gt; to download the tokenizer and name it as &lt;code&gt;vqgan-ckpts/ldm_vqgan_f8_16384/checkpoints/last.ckpt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To train an LDM-8 conditioned on Moco v3 ViT-B representations, using 64 V100 GPUs for 40 epochs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m torch.distributed.launch --nproc_per_node=8 --nnodes=8 --node_rank=0 \&#xA;main_ldm.py \&#xA;--config config/ldm/cin-ldm-vq-f8-repcond.yaml \&#xA;--batch_size 4 \&#xA;--epochs 40 \&#xA;--blr 2.5e-7 --weight_decay 0.01 \&#xA;--output_dir ${OUTPUT_DIR} \&#xA;--data_path ${IMAGENET_DIR} \&#xA;--dist_url tcp://${MASTER_SERVER_ADDRESS}:2214&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Resume&lt;/strong&gt;: set &lt;code&gt;--resume&lt;/code&gt; to the &lt;code&gt;OUTPUT_DIR&lt;/code&gt; where &lt;code&gt;checkpoint-last.pth&lt;/code&gt; is stored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: set &lt;code&gt;--resume&lt;/code&gt; to the pre-trained LDM checkpoint and include the &lt;code&gt;--evaluate&lt;/code&gt; flag in the above script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-trained LDM&lt;/strong&gt; conditioned on Moco v3 ViT-B representations can be downloaded &lt;a href=&#34;https://drive.google.com/file/d/1-qNGz5biWs9KwsjAMyR_GyciaWmF9axW/view?usp=sharing&#34;&gt;here&lt;/a&gt; (FID=9.08, IS=101.9).&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to contact me through email (&lt;a href=&#34;mailto:tianhong@mit.edu&#34;&gt;tianhong@mit.edu&lt;/a&gt;). Enjoy!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mapup/MapUp-Data-Assessment-F</title>
    <updated>2023-12-12T01:57:32Z</updated>
    <id>tag:github.com,2023-12-12:/mapup/MapUp-Data-Assessment-F</id>
    <link href="https://github.com/mapup/MapUp-Data-Assessment-F" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MapUp - Python Assessment&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This assessment is designed to evaluate your proficiency in Python programming, data manipulation, and analysis, as well as your ability to work with Excel. Below, you&#39;ll find details on each component of the assessment and the tasks you should complete. Best of luck!&lt;/p&gt; &#xA;&lt;h2&gt;Important Points to Note:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The assessment will be tested using our internal set of test cases. Scripts must be developed in accordance with the template shared. Please use the following template to create your scripts: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;📂 templates &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;📄 python_task_1.py&lt;/li&gt; &#xA;     &lt;li&gt;📄 python_task_2.py&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;We&#39;ve clearly outlined the interfaces of our functions, specifying the input and output data types with distinct signatures.&lt;/li&gt; &#xA; &lt;li&gt;Any deviation especially in naming conventions and providing arguments will impact the correct assessment of your work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Submission structure&lt;/h2&gt; &#xA;&lt;p&gt;There should be a folder named &lt;code&gt;submissions&lt;/code&gt; in the root of your repository. This folder should contain the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📂 your_cloned_repo &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;📂 submissions &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;📄 python_task_1.py&lt;/li&gt; &#xA;     &lt;li&gt;📄 python_task_2.py&lt;/li&gt; &#xA;     &lt;li&gt;📄 excel_assessment.xlsm&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;📂 templates&lt;/li&gt; &#xA;   &lt;li&gt;📂 datasets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Result Submission:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data that you need to work with is in the folder &lt;code&gt;datasets&lt;/code&gt;. Store your process outputs in the structure mentioned below&lt;/li&gt; &#xA; &lt;li&gt;Clone the provided GitHub repository.&lt;/li&gt; &#xA; &lt;li&gt;Add the following members as collaborators to your repo &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;venkateshn@mapup.ai&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;namanjeetsingh@mapup.ai&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;saranshj@mapup.ai&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;varuna@mapup.ai&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Submit the link to your repository via the provided Google Form for evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MapUp - Excel Assessment&lt;/h2&gt; &#xA;&lt;p&gt;You have to submit an excel assessment along with your python task. This evaluation tests your proficiency in Conditional Formatting, Excel Formulae, and Data Manipulation&lt;/p&gt; &#xA;&lt;h1&gt;Python Task 1&lt;/h1&gt; &#xA;&lt;h2&gt;Question 1: Car Matrix Generation&lt;/h2&gt; &#xA;&lt;p&gt;Under the function named &lt;code&gt;generate_car_matrix&lt;/code&gt; write a logic that takes the &lt;code&gt;dataset-1.csv&lt;/code&gt; as a DataFrame. Return a new DataFrame that follows the following rules:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;values from &lt;code&gt;id_2&lt;/code&gt; as columns&lt;/li&gt; &#xA; &lt;li&gt;values from &lt;code&gt;id_1&lt;/code&gt; as index&lt;/li&gt; &#xA; &lt;li&gt;dataframe should have values from &lt;code&gt;car&lt;/code&gt; column&lt;/li&gt; &#xA; &lt;li&gt;diagonal values should be 0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sample result dataframe:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mapup/MapUp-Data-Assessment-F/main/readme_images/task1-q1.png&#34; alt=&#34;Task 1 Question 1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Question 2: Car Type Count Calculation&lt;/h2&gt; &#xA;&lt;p&gt;Create a Python function named &lt;code&gt;get_type_count&lt;/code&gt; that takes the &lt;code&gt;dataset-1.csv&lt;/code&gt; as a DataFrame. Add a new categorical column &lt;code&gt;car_type&lt;/code&gt; based on values of the column &lt;code&gt;car&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;low&lt;/code&gt; for values less than or equal to 15,&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;medium&lt;/code&gt; for values greater than 15 and less than or equal to 25,&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;high&lt;/code&gt; for values greater than 25.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Calculate the count of occurrences for each &lt;code&gt;car_type&lt;/code&gt; category and return the result as a dictionary. Sort the dictionary alphabetically based on keys.&lt;/p&gt; &#xA;&lt;h2&gt;Question 3: Bus Count Index Retrieval&lt;/h2&gt; &#xA;&lt;p&gt;Create a Python function named &lt;code&gt;get_bus_indexes&lt;/code&gt; that takes the &lt;code&gt;dataset-1.csv&lt;/code&gt; as a DataFrame. The function should identify and return the indices as a list (sorted in ascending order) where the &lt;code&gt;bus&lt;/code&gt; values are greater than twice the mean value of the &lt;code&gt;bus&lt;/code&gt; column in the DataFrame.&lt;/p&gt; &#xA;&lt;h2&gt;Question 4: Route Filtering&lt;/h2&gt; &#xA;&lt;p&gt;Create a python function &lt;code&gt;filter_routes&lt;/code&gt; that takes the &lt;code&gt;dataset-1.csv&lt;/code&gt; as a DataFrame. The function should return the sorted list of values of column &lt;code&gt;route&lt;/code&gt; for which the average of values of &lt;code&gt;truck&lt;/code&gt; column is greater than 7.&lt;/p&gt; &#xA;&lt;h2&gt;Question 5: Matrix Value Modification&lt;/h2&gt; &#xA;&lt;p&gt;Create a Python function named &lt;code&gt;multiply_matrix&lt;/code&gt; that takes the resulting DataFrame from Question 1, as input and modifies each value according to the following logic:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If a value in the DataFrame is greater than 20, multiply those values by 0.75,&lt;/li&gt; &#xA; &lt;li&gt;If a value is 20 or less, multiply those values by 1.25.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The function should return the modified DataFrame which has values rounded to 1 decimal place.&lt;/p&gt; &#xA;&lt;p&gt;Sample result dataframe:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mapup/MapUp-Data-Assessment-F/main/readme_images/task1-q5.png&#34; alt=&#34;Task 1 Question 5&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Question 6: Time Check&lt;/h2&gt; &#xA;&lt;p&gt;You are given a dataset, &lt;code&gt;dataset-2.csv&lt;/code&gt;, containing columns &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;id_2&lt;/code&gt;, and timestamp (&lt;code&gt;startDay&lt;/code&gt;, &lt;code&gt;startTime&lt;/code&gt;, &lt;code&gt;endDay&lt;/code&gt;, &lt;code&gt;endTime&lt;/code&gt;). The goal is to verify the completeness of the time data by checking whether the timestamps for each unique (&lt;code&gt;id&lt;/code&gt;, &lt;code&gt;id_2&lt;/code&gt;) pair cover a full 24-hour period (from 12:00:00 AM to 11:59:59 PM) and span all 7 days of the week (from Monday to Sunday).&lt;/p&gt; &#xA;&lt;p&gt;Create a function that accepts &lt;code&gt;dataset-2.csv&lt;/code&gt; as a DataFrame and returns a boolean series that indicates if each (&lt;code&gt;id&lt;/code&gt;, &lt;code&gt;id_2&lt;/code&gt;) pair has incorrect timestamps. The boolean series must have multi-index (&lt;code&gt;id&lt;/code&gt;, &lt;code&gt;id_2&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h1&gt;Python Task 2&lt;/h1&gt; &#xA;&lt;h2&gt;Question 1: Distance Matrix Calculation&lt;/h2&gt; &#xA;&lt;p&gt;Create a function named &lt;code&gt;calculate_distance_matrix&lt;/code&gt; that takes the &lt;code&gt;dataset-3.csv&lt;/code&gt; as input and generates a DataFrame representing distances between IDs.&lt;/p&gt; &#xA;&lt;p&gt;The resulting DataFrame should have cumulative distances along known routes, with diagonal values set to 0. If distances between toll locations A to B and B to C are known, then the distance from A to C should be the sum of these distances. Ensure the matrix is symmetric, accounting for bidirectional distances between toll locations (i.e. A to B is equal to B to A).&lt;/p&gt; &#xA;&lt;p&gt;Sample result dataframe:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mapup/MapUp-Data-Assessment-F/main/readme_images/task2-q1.png&#34; alt=&#34;Task 2 Question 1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Question 2: Unroll Distance Matrix&lt;/h2&gt; &#xA;&lt;p&gt;Create a function &lt;code&gt;unroll_distance_matrix&lt;/code&gt; that takes the DataFrame created in Question 1. The resulting DataFrame should have three columns: columns &lt;code&gt;id_start&lt;/code&gt;, &lt;code&gt;id_end&lt;/code&gt;, and &lt;code&gt;distance&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All the combinations except for same &lt;code&gt;id_start&lt;/code&gt; to &lt;code&gt;id_end&lt;/code&gt; must be present in the rows with their distance values from the input DataFrame.&lt;/p&gt; &#xA;&lt;h2&gt;Question 3: Finding IDs within Percentage Threshold&lt;/h2&gt; &#xA;&lt;p&gt;Create a function &lt;code&gt;find_ids_within_ten_percentage_threshold&lt;/code&gt; that takes the DataFrame created in Question 2 and a reference value from the &lt;code&gt;id_start&lt;/code&gt; column as an integer.&lt;/p&gt; &#xA;&lt;p&gt;Calculate average distance for the reference value given as an input and return a sorted list of values from &lt;code&gt;id_start&lt;/code&gt; column which lie within 10% (including ceiling and floor) of the reference value&#39;s average.&lt;/p&gt; &#xA;&lt;h2&gt;Question 4: Calculate Toll Rate&lt;/h2&gt; &#xA;&lt;p&gt;Create a function &lt;code&gt;calculate_toll_rate&lt;/code&gt; that takes the DataFrame created in Question 2 as input and calculates toll rates based on vehicle types.&lt;/p&gt; &#xA;&lt;p&gt;The resulting DataFrame should add 5 columns to the input DataFrame: &lt;code&gt;moto&lt;/code&gt;, &lt;code&gt;car&lt;/code&gt;, &lt;code&gt;rv&lt;/code&gt;, &lt;code&gt;bus&lt;/code&gt;, and &lt;code&gt;truck&lt;/code&gt; with their respective rate coefficients. The toll rates should be calculated by multiplying the distance with the given rate coefficients for each vehicle type:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;0.8 for &lt;code&gt;moto&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;1.2 for &lt;code&gt;car&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;1.5 for &lt;code&gt;rv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;2.2 for &lt;code&gt;bus&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;3.6 for &lt;code&gt;truck&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Sample result dataframe:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mapup/MapUp-Data-Assessment-F/main/readme_images/task2-q4.png&#34; alt=&#34;Task 2 Question 4&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Question 5: Calculate Time-Based Toll Rates&lt;/h2&gt; &#xA;&lt;p&gt;Create a function named &lt;code&gt;calculate_time_based_toll_rates&lt;/code&gt; that takes the DataFrame created in Question 4 as input and calculates toll rates for different time intervals within a day.&lt;/p&gt; &#xA;&lt;p&gt;The resulting DataFrame should have these five columns added to the input: start_day, start_time, end_day, and end_time.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;start_day&lt;/code&gt;, &lt;code&gt;end_day&lt;/code&gt; must be strings with day values (from Monday to Sunday in proper case)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; and &lt;code&gt;end_time&lt;/code&gt; must be of type datetime.time() with the values from time range given below.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Modify the values of vehicle columns according to the following time ranges:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Weekdays (Monday - Friday):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;From 00:00:00 to 10:00:00: Apply a discount factor of 0.8&lt;/li&gt; &#xA; &lt;li&gt;From 10:00:00 to 18:00:00: Apply a discount factor of 1.2&lt;/li&gt; &#xA; &lt;li&gt;From 18:00:00 to 23:59:59: Apply a discount factor of 0.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Weekends (Saturday and Sunday):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apply a constant discount factor of 0.7 for all times.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For each unique (&lt;code&gt;id_start&lt;/code&gt;, &lt;code&gt;id_end&lt;/code&gt;) pair, cover a full 24-hour period (from 12:00:00 AM to 11:59:59 PM) and span all 7 days of the week (from Monday to Sunday).&lt;/p&gt; &#xA;&lt;p&gt;Sample result dataframe:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mapup/MapUp-Data-Assessment-F/main/readme_images/task2-q5.png&#34; alt=&#34;Task 2 Question 5&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>