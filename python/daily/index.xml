<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-31T01:43:17Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Lightning-AI/lit-gpt</title>
    <updated>2023-07-31T01:43:17Z</updated>
    <id>tag:github.com,2023-07-31:/Lightning-AI/lit-gpt</id>
    <link href="https://github.com/Lightning-AI/lit-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hackable implementation of state-of-the-art open-source LLMs based on nanoGPT. Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_lightning/LitStableLM_Badge.png&#34; alt=&#34;Lit-GPT&#34; width=&#34;128&#34;&gt; &#xA; &lt;h1&gt;âš¡ Lit-GPT&lt;/h1&gt; &#xA; &lt;!--&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;  &lt;a href=&#34;https://www.lightning.ai/&#34;&gt;Lightning.ai&lt;/a&gt; â€¢&#xA;  &lt;a href=&#34;https://lightning.ai/docs/pytorch/stable/&#34;&gt;PyTorch Lightning&lt;/a&gt; â€¢&#xA;  &lt;a href=&#34;https://lightning.ai/docs/fabric/stable/&#34;&gt;Fabric&lt;/a&gt;&#xA;&lt;/p&gt;&#xA;--&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/lightning-AI/lit-stablelm/actions/workflows/cpu-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;cpu-tests&#34;&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-stablelm/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1077906959069626439?style=plastic&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_lightning/LitStableLM.gif&#34; alt=&#34;Lit-GPT and pineapple pizza&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;âš¡ Lit-GPT&lt;/h1&gt; &#xA;&lt;p&gt;Hackable &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/lit_gpt/model.py&#34;&gt;implementation&lt;/a&gt; of state-of-the-art open-source large language models released under the &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Supports popular public checkpoints such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Meta AI &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_llama_2.md&#34;&gt;Llama 2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stability AI &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_freewilly_2.md&#34;&gt;FreeWilly2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TII UAE &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_falcon.md&#34;&gt;Falcon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OpenLM Research &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_openllama.md&#34;&gt;OpenLLaMA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LMSYS &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_vicuna.md&#34;&gt;Vicuna&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_longchat.md&#34;&gt;LongChat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Together &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_redpajama_incite.md&#34;&gt;RedPajama-INCITE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;EleutherAI &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_pythia.md&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;StabilityAI &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_stablelm.md&#34;&gt;StableLM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This implementation extends on &lt;a href=&#34;https://github.com/lightning-AI/lit-llama&#34;&gt;Lit-LLaMA&lt;/a&gt; and &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;, and it&#39;s &lt;strong&gt;powered by &lt;a href=&#34;https://lightning.ai/docs/fabric/stable/&#34;&gt;Lightning Fabric&lt;/a&gt; âš¡&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Design principles&lt;/h2&gt; &#xA;&lt;p&gt;This repository follows the main principle of &lt;strong&gt;openness through clarity&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lit-GPT&lt;/strong&gt; is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple:&lt;/strong&gt; Single-file implementation without boilerplate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Correct:&lt;/strong&gt; Numerically equivalent to the original model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Optimized:&lt;/strong&gt; Runs fast on consumer hardware or at scale.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-source:&lt;/strong&gt; No strings attached.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Avoiding code duplication is &lt;strong&gt;not&lt;/strong&gt; a goal. &lt;strong&gt;Readability&lt;/strong&gt; and &lt;strong&gt;hackability&lt;/strong&gt; are.&lt;/p&gt; &#xA;&lt;h2&gt;Get involved!&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;Join our Discord&lt;/a&gt; to build high-performance, truly open-source models for the common benefit of the community.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightning-AI/lit-gpt&#xA;cd lit-gpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Lit-GPT currently relies on flash attention from PyTorch nightly. Until PyTorch 2.1 is released you&#39;ll need to install nightly manually. Luckily that is straightforward:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On CUDA&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre &#39;torch&amp;gt;=2.1.0dev&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;On CPU (incl Macs)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --index-url https://download.pytorch.org/whl/nightly/cpu --pre &#39;torch&amp;gt;=2.1.0dev&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Optional) install Flash Attention 2&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MAX_JOBS=4 pip install &#39;flash-attn&amp;gt;=2.0.0.post1&#39; --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All good, now install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You are all set! ðŸŽ‰&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Use the model&lt;/h2&gt; &#xA;&lt;p&gt;To generate text predictions, you need to download the model weights. &lt;strong&gt;If you don&#39;t have them, check out our &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/download_stablelm.md&#34;&gt;guide&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate/base.py --prompt &#34;Hello, my name is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run the 3B pre-trained model and require ~7 GB of GPU memory using the &lt;code&gt;bfloat16&lt;/code&gt; datatype.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/inference.md&#34;&gt;Full guide for generating samples from the model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also chat with the model interactively:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python chat/base.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run large models on smaller consumer devices&lt;/h3&gt; &#xA;&lt;p&gt;We support 4-bit quantization (as in QLoRA), LLM.int8, and GPTQ.int4 inference by following &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/quantize.md&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Finetune the model&lt;/h2&gt; &#xA;&lt;p&gt;We provide a simple training scripts (&lt;code&gt;finetune/adapter.py&lt;/code&gt;, &lt;code&gt;finetune/adapter_v2.py&lt;/code&gt;, and &lt;code&gt;finetune/lora.py&lt;/code&gt;) that instruction-tunes a pretrained model on the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt; dataset.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the data and generate an instruction tuning dataset:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/prepare_alpaca.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the finetuning script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For example, you can either use&lt;/p&gt; &#xA;&lt;p&gt;Adapter (&lt;a href=&#34;https://arxiv.org/abs/2303.16199&#34;&gt;Zhang et al. 2023&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/adapter.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or Adapter v2 (&lt;a href=&#34;https://arxiv.org/abs/2304.15010&#34;&gt;Gao et al. 2023&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/adapter_v2.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or LoRA (&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;Hu et al. 2021&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/lora.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/finetune_adapter.md&#34;&gt;tutorials/finetune_adapter&lt;/a&gt; for details on the differences between the two adapter methods.)&lt;/p&gt; &#xA;&lt;p&gt;The finetuning requires at least one GPU with ~12 GB memory (RTX 3060).&lt;/p&gt; &#xA;&lt;p&gt;It is expected that you have downloaded the pretrained weights as described above. More details about each finetuning method and how you can apply it to your own data can be found in our technical how-to guides.&lt;/p&gt; &#xA;&lt;h3&gt;Finetuning How-To Guides&lt;/h3&gt; &#xA;&lt;p&gt;These technical tutorials illustrate how to run the finetuning code.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/finetune_adapter.md&#34;&gt;Finetune with Adapters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/lit-gpt/main/tutorials/finetune_lora.md&#34;&gt;Finetune with LoRA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Understanding Finetuning -- Conceptual Tutorials&lt;/h3&gt; &#xA;&lt;p&gt;Looking for conceptual tutorials and explanations? We have some additional articles below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://lightning.ai/pages/community/article/understanding-llama-adapters/&#34;&gt;Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://lightning.ai/pages/community/tutorial/lora-llm/&#34;&gt;Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pre-training&lt;/h2&gt; &#xA;&lt;p&gt;Porting from Lit-LLaMA in progress ðŸ‘·&lt;/p&gt; &#xA;&lt;h2&gt;Get involved!&lt;/h2&gt; &#xA;&lt;p&gt;We are on a quest towards fully open source AI.&lt;/p&gt; &#xA;&lt;img align=&#34;right&#34; src=&#34;https://pl-public-data.s3.amazonaws.com/assets_lightning/LitStableLM_Illustration.png&#34; alt=&#34;Lit-GPT&#34; width=&#34;128&#34;&gt; &#xA;&lt;p&gt;Join us and start contributing, especially on the following areas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt/labels/pre-training&#34;&gt;Pre-training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt/labels/fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt/labels/quantization&#34;&gt;Quantization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt/labels/sparsification&#34;&gt;Sparsification&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We welcome all individual contributors, regardless of their level of experience or hardware. Your contributions are valuable, and we are excited to see what you can accomplish in this collaborative and supportive environment.&lt;/p&gt; &#xA;&lt;p&gt;Unsure about contributing? Check out our &lt;a href=&#34;https://lightning.ai/pages/community/tutorial/contributing-to-lit-llama-a-hitchhikers-guide-to-the-quest-for-fully-open-source-ai/&#34;&gt;Contributing to Lit-LLaMA: A Hitchhikerâ€™s Guide to the Quest for Fully Open-Source AI&lt;/a&gt; guide. The same guidelines apply to Lit-GPT.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t forget to &lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;join our Discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy&#34;&gt;@karpathy&lt;/a&gt; for &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI&#34;&gt;@EleutherAI&lt;/a&gt; for &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers&#34;&gt;@TimDettmers&lt;/a&gt; for &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IST-DASLab&#34;&gt;@IST-DASLab&lt;/a&gt; for &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;GPTQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft&#34;&gt;@Microsoft&lt;/a&gt; for &lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tridao&#34;&gt;@tridao&lt;/a&gt; for &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;Flash Attention 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Lit-GPT is released under the &lt;a href=&#34;https://github.com/Lightning-AI/lit-gpt/raw/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/text-generation-inference</title>
    <updated>2023-07-31T01:43:17Z</updated>
    <id>tag:github.com,2023-07-31:/huggingface/text-generation-inference</id>
    <link href="https://github.com/huggingface/text-generation-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language Model Text Generation Inference&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/huggingface/text-generation-inference/assets/3841370/38ba1531-ea0d-4851-b31a-a6d4ddc944b0&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA; &lt;h1&gt;Text Generation Inference&lt;/h1&gt; &#xA; &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.github.io/text-generation-inference&#34;&gt; &lt;img alt=&#34;Swagger API documentation&#34; src=&#34;https://img.shields.io/badge/API-Swagger-informational&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;A Rust, Python and gRPC server for text generation inference. Used in production at &lt;a href=&#34;https://huggingface.co&#34;&gt;HuggingFace&lt;/a&gt; to power Hugging Chat, the Inference API and Inference Endpoint.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#optimized-architectures&#34;&gt;Optimized Architectures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#get-started&#34;&gt;Get Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#using-a-private-or-gated-model&#34;&gt;Using a private or gated model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#a-note-on-shared-memory-shm&#34;&gt;A note on Shared Memory&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#distributed-tracing&#34;&gt;Distributed Tracing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#local-install&#34;&gt;Local Install&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#cuda-kernels&#34;&gt;CUDA Kernels&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#run-falcon&#34;&gt;Run Falcon&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#run&#34;&gt;Run&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#quantization&#34;&gt;Quantization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#develop&#34;&gt;Develop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#other-supported-hardware&#34;&gt;Other supported hardware&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serve the most popular Large Language Models with a simple launcher&lt;/li&gt; &#xA; &lt;li&gt;Tensor Parallelism for faster inference on multiple GPUs&lt;/li&gt; &#xA; &lt;li&gt;Token streaming using Server-Sent Events (SSE)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/text-generation-inference/tree/main/router&#34;&gt;Continuous batching of incoming requests&lt;/a&gt; for increased total throughput&lt;/li&gt; &#xA; &lt;li&gt;Optimized transformers code for inference using &lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;flash-attention&lt;/a&gt; and &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;Paged Attention&lt;/a&gt; on the most popular architectures&lt;/li&gt; &#xA; &lt;li&gt;Quantization with &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPT-Q&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/safetensors&#34;&gt;Safetensors&lt;/a&gt; weight loading&lt;/li&gt; &#xA; &lt;li&gt;Watermarking with &lt;a href=&#34;https://arxiv.org/abs/2301.10226&#34;&gt;A Watermark for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Logits warper (temperature scaling, top-p, top-k, repetition penalty, more details see &lt;a href=&#34;https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor&#34;&gt;transformers.LogitsProcessor&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Stop sequences&lt;/li&gt; &#xA; &lt;li&gt;Log probabilities&lt;/li&gt; &#xA; &lt;li&gt;Production ready (distributed tracing with Open Telemetry, Prometheus metrics)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Optimized architectures&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/google/flan-t5-xxl&#34;&gt;FLAN-T5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/galactica-120b&#34;&gt;Galactica&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neox-20b&#34;&gt;GPT-Neox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/opt-66b&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigcode/santacoder&#34;&gt;SantaCoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;Starcoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-7b&#34;&gt;Falcon 7B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;Falcon 40B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-30b&#34;&gt;MPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Llama V2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other architectures are supported on a best effort basis using:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;AutoModelForCausalLM.from_pretrained(&amp;lt;model&amp;gt;, device_map=&#34;auto&#34;)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;AutoModelForSeq2SeqLM.from_pretrained(&amp;lt;model&amp;gt;, device_map=&#34;auto&#34;)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way of getting started is using the official Docker container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=tiiuae/falcon-7b-instruct&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;&#xA;docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.0.0 --model-id $model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To use GPUs, you need to install the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;. We also recommend using NVIDIA drivers with CUDA version 11.8 or higher.&lt;/p&gt; &#xA;&lt;p&gt;To see all options to serve your models (in the &lt;a href=&#34;https://github.com/huggingface/text-generation-inference/raw/main/launcher/src/main.rs&#34;&gt;code&lt;/a&gt; or in the cli:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;text-generation-launcher --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then query the model using either the &lt;code&gt;/generate&lt;/code&gt; or &lt;code&gt;/generate_stream&lt;/code&gt; routes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl 127.0.0.1:8080/generate \&#xA;    -X POST \&#xA;    -d &#39;{&#34;inputs&#34;:&#34;What is Deep Learning?&#34;,&#34;parameters&#34;:{&#34;max_new_tokens&#34;:20}}&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl 127.0.0.1:8080/generate_stream \&#xA;    -X POST \&#xA;    -d &#39;{&#34;inputs&#34;:&#34;What is Deep Learning?&#34;,&#34;parameters&#34;:{&#34;max_new_tokens&#34;:20}}&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or from Python:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install text-generation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from text_generation import Client&#xA;&#xA;client = Client(&#34;http://127.0.0.1:8080&#34;)&#xA;print(client.generate(&#34;What is Deep Learning?&#34;, max_new_tokens=20).generated_text)&#xA;&#xA;text = &#34;&#34;&#xA;for response in client.generate_stream(&#34;What is Deep Learning?&#34;, max_new_tokens=20):&#xA;    if not response.token.special:&#xA;        text += response.token.text&#xA;print(text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;API documentation&lt;/h3&gt; &#xA;&lt;p&gt;You can consult the OpenAPI documentation of the &lt;code&gt;text-generation-inference&lt;/code&gt; REST API using the &lt;code&gt;/docs&lt;/code&gt; route. The Swagger UI is also available at: &lt;a href=&#34;https://huggingface.github.io/text-generation-inference&#34;&gt;https://huggingface.github.io/text-generation-inference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using a private or gated model&lt;/h3&gt; &#xA;&lt;p&gt;You have the option to utilize the &lt;code&gt;HUGGING_FACE_HUB_TOKEN&lt;/code&gt; environment variable for configuring the token employed by &lt;code&gt;text-generation-inference&lt;/code&gt;. This allows you to gain access to protected resources.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to serve the gated Llama V2 model variants:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy your cli READ token&lt;/li&gt; &#xA; &lt;li&gt;Export &lt;code&gt;HUGGING_FACE_HUB_TOKEN=&amp;lt;your cli READ token&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;or with Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=meta-llama/Llama-2-7b-chat-hf&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;token=&amp;lt;your cli READ token&amp;gt;&#xA;&#xA;docker run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.0.0 --model-id $model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;A note on Shared Memory (shm)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html&#34;&gt;&lt;code&gt;NCCL&lt;/code&gt;&lt;/a&gt; is a communication framework used by &lt;code&gt;PyTorch&lt;/code&gt; to do distributed training/inference. &lt;code&gt;text-generation-inference&lt;/code&gt; make use of &lt;code&gt;NCCL&lt;/code&gt; to enable Tensor Parallelism to dramatically speed up inference for large language models.&lt;/p&gt; &#xA;&lt;p&gt;In order to share data between the different devices of a &lt;code&gt;NCCL&lt;/code&gt; group, &lt;code&gt;NCCL&lt;/code&gt; might fall back to using the host memory if peer-to-peer using NVLink or PCI is not possible.&lt;/p&gt; &#xA;&lt;p&gt;To allow the container to use 1G of Shared Memory and support SHM sharing, we add &lt;code&gt;--shm-size 1g&lt;/code&gt; on the above command.&lt;/p&gt; &#xA;&lt;p&gt;If you are running &lt;code&gt;text-generation-inference&lt;/code&gt; inside &lt;code&gt;Kubernetes&lt;/code&gt;. You can also add Shared Memory to the container by creating a volume with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- name: shm&#xA;  emptyDir:&#xA;   medium: Memory&#xA;   sizeLimit: 1Gi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and mounting it to &lt;code&gt;/dev/shm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finally, you can also disable SHM sharing by using the &lt;code&gt;NCCL_SHM_DISABLE=1&lt;/code&gt; environment variable. However, note that this will impact performance.&lt;/p&gt; &#xA;&lt;h3&gt;Distributed Tracing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;text-generation-inference&lt;/code&gt; is instrumented with distributed tracing using OpenTelemetry. You can use this feature by setting the address to an OTLP collector with the &lt;code&gt;--otlp-endpoint&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h3&gt;Local install&lt;/h3&gt; &#xA;&lt;p&gt;You can also opt to install &lt;code&gt;text-generation-inference&lt;/code&gt; locally.&lt;/p&gt; &#xA;&lt;p&gt;First &lt;a href=&#34;https://rustup.rs/&#34;&gt;install Rust&lt;/a&gt; and create a Python virtual environment with at least Python 3.9, e.g. using &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh&#xA;&#xA;conda create -n text-generation-inference python=3.9&#xA;conda activate text-generation-inference&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also need to install Protoc.&lt;/p&gt; &#xA;&lt;p&gt;On Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PROTOC_ZIP=protoc-21.12-linux-x86_64.zip&#xA;curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP&#xA;sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc&#xA;sudo unzip -o $PROTOC_ZIP -d /usr/local &#39;include/*&#39;&#xA;rm -f $PROTOC_ZIP&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On MacOS, using Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install protobuf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;BUILD_EXTENSIONS=True make install # Install repository and HF/transformer fork with CUDA kernels&#xA;make run-falcon-7b-instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; on some machines, you may also need the OpenSSL libraries and gcc. On Linux machines, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install libssl-dev gcc -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CUDA Kernels&lt;/h3&gt; &#xA;&lt;p&gt;The custom CUDA kernels are only tested on NVIDIA A100s. If you have any installation or runtime issues, you can remove the kernels by using the &lt;code&gt;DISABLE_CUSTOM_KERNELS=True&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;p&gt;Be aware that the official Docker image has them enabled by default.&lt;/p&gt; &#xA;&lt;h2&gt;Run Falcon&lt;/h2&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make run-falcon-7b-instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can also quantize the weights with bitsandbytes to reduce the VRAM requirement:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make run-falcon-7b-instruct-quantize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Develop&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make server-dev&#xA;make router-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# python&#xA;make python-server-tests&#xA;make python-client-tests&#xA;# or both server and client tests&#xA;make python-tests&#xA;# rust cargo tests&#xA;make rust-tests&#xA;# integration tests&#xA;make integration-tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other supported hardware&lt;/h2&gt; &#xA;&lt;p&gt;TGI is also supported on the following AI hardware accelerators:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Habana first-gen Gaudi and Gaudi2:&lt;/em&gt; checkout &lt;a href=&#34;https://github.com/huggingface/optimum-habana/tree/main/text-generation-inference&#34;&gt;here&lt;/a&gt; how to serve models with TGI on Gaudi and Gaudi2 with &lt;a href=&#34;https://huggingface.co/docs/optimum/habana/index&#34;&gt;Optimum Habana&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>GAIR-NLP/factool</title>
    <updated>2023-07-31T01:43:17Z</updated>
    <id>tag:github.com,2023-07-31:/GAIR-NLP/factool</id>
    <link href="https://github.com/GAIR-NLP/factool" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FacTool: Factuality Detection in Generative AI&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;FacTool: Factuality Detection in Generative AI&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains the source code and plugin configuration for our &lt;a href=&#34;https://arxiv.org/abs/2307.13528&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;Factool is a tool augmented framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Factool now supports 4 tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;knowledge-based QA&lt;/strong&gt;: Factool detects factual errors in knowledge-based QA.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;code generation&lt;/strong&gt;: Factool detects execution errors in code generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;mathematical reasoning&lt;/strong&gt;: Factool detects calculation errors in mathematical reasoning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;scientific literature review&lt;/strong&gt;: Factool detects hallucinated scientific literatures.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/figs/factool.png&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Demo of Knowledge-based QA:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/figs/factool_plugin_kbqa.gif&#34; alt=&#34;Alt Text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/figs/factool_plugin_kbqa2.gif&#34; alt=&#34;Alt Text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;h4&gt;For General User&lt;/h4&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install factool&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;h4&gt;For Developer&lt;/h4&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:GAIR-NLP/factool.git&#xA;cd factool&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;API Key Preparation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;get your OpenAI API key from &lt;a href=&#34;https://beta.openai.com/&#34;&gt;here&lt;/a&gt;. This is used in all scenarios (Knowledge-based QA, Code, Math, Scientific Literature Review).&lt;/li&gt; &#xA; &lt;li&gt;get your Serper API key from &lt;a href=&#34;https://serper.dev/&#34;&gt;here&lt;/a&gt;. This is only used in Knowledge-based QA.&lt;/li&gt; &#xA; &lt;li&gt;get your Scraper API key from &lt;a href=&#34;https://www.scraperapi.com/&#34;&gt;here&lt;/a&gt;. This is only used in Scientific Literature Review.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;General Usage&lt;/h3&gt; &#xA;&lt;p&gt;You could also directly refer to &lt;a href=&#34;https://github.com/GAIR-NLP/factool/raw/main/example/example.py&#34;&gt;./example/example.py&lt;/a&gt; and &lt;a href=&#34;https://github.com/GAIR-NLP/factool/raw/main/example/example_inputs.jsonl&#34;&gt;example_inputs.jsonl&lt;/a&gt; for general usage.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;General Usage (click to toggle the content)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export OPENAI_API_KEY=... # this is required in all tasks&#xA;export SERPER_API_KEY=... # this is required only in knowledge-based QA&#xA;export SCRAPER_API_KEY=... # this is requried only in scientific literature review&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize a list of inputs. &#34;entry_point&#34; is only needed when the task is &#34;code generation&#34;&#xA;# please refer to example_inputs.jsonl for example inputs for each category&#xA;inputs = [&#xA;            {&#34;prompt&#34;: &#34;&amp;lt;prompt1&amp;gt;&#34;, &#34;response&#34;: &#34;&amp;lt;response1&amp;gt;&#34;, &#34;category&#34;: &#34;&amp;lt;category1&amp;gt;&#34;, &#34;entry_point&#34;: &#34;&amp;lt;entry_point_1&amp;gt;&#34;},&#xA;            {&#34;prompt&#34;: &#34;&amp;lt;prompt2&amp;gt;&#34;, &#34;response&#34;: &#34;&amp;lt;response2&amp;gt;&#34;, &#34;category&#34;: &#34;&amp;lt;category2&amp;gt;&#34;, &#34;entry_point&#34;: &#34;&amp;lt;entry_point_2&amp;gt;&#34;},&#xA;          ...&#xA;        ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;where&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: The prompt for the model to generate the response.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;response&lt;/code&gt;: The response generated by the model.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;category&lt;/code&gt;: The category of the task. it could be: &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;code&gt;kbqa&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;code&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;math&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;code&gt;scientific&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;entry_point&lt;/code&gt;: The function name of the code snippet to be fact-checked in the response. Could be &#34;null&#34; if the category of the task is not &lt;code&gt;code&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from factool import Factool&#xA;&#xA;# Initialize a Factool instance with the specified keys. foundation_model could be either &#34;gpt-3.5-turbo&#34; or &#34;gpt-4&#34;&#xA;factool_instance = Factool(&#34;gpt-4&#34;)&#xA;&#xA;inputs = [&#xA;            {&#xA;                &#34;prompt&#34;: &#34;Introduce Graham Neubig&#34;,&#xA;                &#34;response&#34;: &#34;Graham Neubig is a professor at MIT&#34;,&#xA;                &#34;category&#34;: &#34;kbqa&#34;&#xA;            },&#xA;            ...&#xA;]&#xA;response_list = factool_instance.run(inputs)&#xA;&#xA;print(response_list)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Knowledge-based QA&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Detailed usage of factool on knowledge-based QA (click to toggle the content)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export OPENAI_API_KEY=...&#xA;export SERPER_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from factool import Factool&#xA;&#xA;# Initialize a Factool instance with the specified keys. foundation_model could be either &#34;gpt-3.5-turbo&#34; or &#34;gpt-4&#34;&#xA;factool_instance = Factool(&#34;gpt-4&#34;)&#xA;&#xA;inputs = [&#xA;            {&#xA;                &#34;prompt&#34;: &#34;Introduce Graham Neubig&#34;,&#xA;                &#34;response&#34;: &#34;Graham Neubig is a professor at MIT&#34;,&#xA;                &#34;category&#34;: &#34;kbqa&#34;&#xA;            },&#xA;]&#xA;response_list = factool_instance.run(inputs)&#xA;&#xA;print(response_list)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The response_list should follow the following format:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;  &#34;average_claim_level_factuality&#34;: avg_claim_level_factuality&#xA;  &#34;average_response_level_factuality&#34;: avg_response_level_factuality&#xA;  &#34;detailed_information&#34;: [&#xA;    {&#xA;      &#39;prompt&#39;: prompt_1, &#xA;      &#39;response&#39;: response_1, &#xA;      &#39;category&#39;: &#39;kbqa&#39;, &#xA;      &#39;claims&#39;: [claim_11, claim_12, ..., claims_1n], &#xA;      &#39;queries&#39;: [[query_111, query_112], [query_121, query_122], ..[query_1n1, query_1n2]], &#xA;      &#39;evidences&#39;: [[evidences_11], [evidences_12], ..., [evidences_1n]], &#xA;      &#39;claim_level_factuality&#39;: [{claim_11, reasoning_11, error_11, correction_11, factuality_11}, {claim_12, reasoning_12, error_12, correction_12, factuality_12}, ..., {claim_1n, reasoning_1n, error_1n, correction_1n, factuality_1n}], &#xA;      &#39;response_level_factuality&#39;: factuality_1&#xA;    },&#xA;    {&#xA;      &#39;prompt&#39;: prompt_2, &#xA;      &#39;response&#39;: response_2, &#xA;      &#39;category&#39;: &#39;kbqa&#39;,&#xA;      &#39;claims&#39;: [claim_21, claim_22, ..., claims_2n], &#xA;      &#39;queries&#39;: [[query_211, query_212], [query_221, query_222], ..., [query_2n1, query_2n2]], &#xA;      &#39;evidences&#39;: [[evidences_21], [evidences_22], ..., [evidences_2n]], &#xA;      &#39;claim_level_factuality&#39;: [{claim_21, reasoning_21, error_21, correction_21, factuality_21}, {claim_22, reasoning_22, error_22, correction_22, factuality_22}, ..., {claim_2n, reasoning_2n, error_2n, correction_2n, factuality_2n}],&#xA;      &#39;response_level_factuality&#39;: factuality_2,&#xA;    },&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;In this case, you will get:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;average_claim_level_factuality&#34;: avg_claim_level_factuality,&#xA;  &#34;average_response_level_factuality&#34;: avg_response_level_factuality,&#xA;  &#34;detailed_information&#34;: [&#xA;    {&#xA;      &#39;prompt&#39;: &#39;Introduce Graham Neubig&#39;,&#xA;      &#39;response&#39;: &#39;Graham Neubig is a professor at MIT&#39;,&#xA;      &#39;category&#39;: &#39;kbqa&#39;,&#xA;      &#39;entry_point&#39;: &#39;answer_question&#39;,&#xA;      &#39;claims&#39;: [{&#39;claim&#39;: &#39;Graham Neubig is a professor at MIT&#39;}],&#xA;      &#39;queries&#39;: [[&#39;Is Graham Neubig a professor at MIT?&#39;, &#39;Graham Neubig MIT&#39;]],&#xA;      &#39;evidences&#39;: [[&#39;I am an Associate Professor of Computer Science at Carnegie Mellon University and CEO of Inspired Cognition. My research and development focuses on AI and ...&#39;, &#39;Graham Neubig. I am an Associate Professor at the Carnegie Mellon University Language Technology Institute in the School of Computer Science, and work with ...&#39;, &#39;Missing: MIT? | Must include:MIT?.&#39;, &#39;Associate Professor, Language Technology Institute, Carnegie Mellon University Affiliated Faculty, Machine Learning Department, Carnegie Mellon University&#39;, &#39;Missing: MIT? | Must include:MIT?.&#39;, &#39;I am an Associate Professor at the Carnegie Mellon University Language Technology Institute in the School of Computer Science, and work with a bunch of great ...&#39;, &#39;Missing: MIT | Must include:MIT.&#39;, &#39;I am an Associate Professor of Computer Science at Carnegie Mellon University and CEO of Inspired Cognition. My research and development focuses on AI and ...&#39;, &#39;was heavily inspired by an MIT PhD thesis finished 16 years earlier in 1996! ... Episode 22 of The Thesis Review: Graham Neubig (@gneubig), ...&#39;, &#39;Graham Neubig,. Graham Neubig. Graduate School of Information Science Nara Institute of Science and Technology. Search for other works by this author on:.&#39;]],&#xA;      &#39;claim_level_factuality&#39;: [{&#39;reasoning&#39;: &#39;The given text states that Graham Neubig is a professor at MIT. However, the provided evidences consistently mention that Graham Neubig is an Associate Professor at Carnegie Mellon University. There is no mention of Graham Neubig being affiliated with MIT in any of the provided evidences.&#39;, &#39;error&#39;: &#39;The given text Falsely states that Graham Neubig is a professor at MIT.&#39;, &#39;correction&#39;: &#39;Graham Neubig is an Associate Professor at Carnegie Mellon University.&#39;, &#39;factuality&#39;: False, &#39;claim&#39;: &#39;Graham Neubig is a professor at MIT&#39;}],&#xA;      &#39;response_level_factuality&#39;: False&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Detailed usage of factool on code (click to toggle the content)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export OPENAI_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from factool import Factool&#xA;&#xA;# Initialize a Factool instance with the specified keys. foundation_model could be either &#34;gpt-3.5-turbo&#34; or &#34;gpt-4&#34;&#xA;factool_instance = Factool(&#34;gpt-4&#34;)&#xA;&#xA;inputs = [&#xA;        {&#xA;            &#34;prompt&#34;: &#34;def get_max_triples(n): \&#34;\&#34;\&#34; You are given a positive integer n. You have to create an integer array a of length n. For each i (1 \u2264 i \u2264 n), the value of a[i] = i * i - i + 1. Return the number of triples (a[i], a[j], a[k]) of a where i &amp;lt; j &amp;lt; k, and a[i] + a[j] + a[k] is a multiple of 3. Example : Input: n = 5 Output: 1 Explanation: a = [1, 3, 7, 13, 21] The only valid triple is (1, 7, 13). \&#34;\&#34;\&#34; Now implement the function get_max_triples using Python&#34;,&#xA;            &#34;response&#34;: &#34;def get_max_triples(n):\n    a = [i * i - i + 1 for i in range(1, n+1)]\n    count = 0\n    for i in range(n-2):\n        for j in range(i+1, n-1):\n            for k in range(j+1, n):\n                if (a[i] + a[j] + a[k]) % 3 == 0:\n                    count += 1\n    return count\n\nprint(get_max_triples(5)) # Output: 1&#34;,&#xA;            &#34;category&#34;: &#34;code&#34;,&#xA;            &#34;entry_point&#34;: &#34;get_max_triples&#34;&#xA;        }&#xA;]&#xA;&#xA;response_list = factool_instance.run(inputs)&#xA;print(response_list)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The response_list should follow the following format:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response_list = &#xA;{&#xA;    &#34;average_claim_level_factuality&#34;: avg_claim_level_factuality,&#xA;    &#34;average_response_level_factuality&#34;: avg_response_level_factuality,&#xA;    &#34;detailed_information&#34;: [&#xA;      {&#xA;          &#39;prompt&#39;: prompt_1, &#xA;          &#39;response&#39;: response_1, &#xA;          &#39;category&#39;: &#39;code&#39;,&#xA;          &#39;entry_point&#39;: entry_point_1,&#xA;          &#39;claim&#39;: claim_1,&#xA;          &#39;testcases_queries&#39;: [testcase_query_11, testcase_query_12, testcase_query_13], &#xA;          &#39;potential_solutions_queries&#39;: [potential_solution_query_11, potential_solution_query_12, potential_solution_query_13], &#xA;          &#39;exec_results&#39;: [[evidences_111, evidences_112, evidences_113, evidences_114], [evidences_121, evidences_122, evidences_123, evidences_124], [evidences_131, evidences_132, evidences_133, evidences_134]],  # note that evidences_114, evidences_124, evidences_134 are the execution results of response_1 against testcase_query_11, testcase_query_12, and testcase_query_13, respectively.&#xA;          &#39;claim_level_factuality&#39;: factuality_1,&#xA;          &#39;response_level_factuality&#39;: factuality_1,&#xA;      },&#xA;      {&#xA;          &#39;prompt&#39;: prompt_2, &#xA;          &#39;response&#39;: response_2, &#xA;          &#39;category&#39;: &#39;code&#39;,&#xA;          &#39;entry_point&#39;: entry_point_2,&#xA;          &#39;claim&#39;: claim_2,&#xA;          &#39;testcases_queries&#39;: [testcase_query_21, testcase_query_22, testcase_query_23], &#xA;          &#39;potential_solutions_queries&#39;: [potential_solution_query_21, potential_solution_query_22, potential_solution_query_23], &#xA;          &#39;exec_results&#39;: [[evidences_211, evidences_212, evidences_213, evidences_214], [evidences_221, evidences_222, evidences_223, evidences_224], [evidences_231, evidences_232, evidences_233, evidences_234]], # note that evidences_214, evidences_224, evidences_234 are the execution results of response_1 against testcase_query_21, testcase_query_22, and testcase_query_23, respectively.&#xA;          &#39;claim_level_factuality&#39;: factuality_2,&#xA;          &#39;response_level_factuality&#39;: factuality_2,&#xA;      },&#xA;    ]&#xA;    ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;In this case, you will get:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;average_claim_level_factuality&#34;: 1.0,&#xA;  &#34;average_response_level_factuality&#34;: 1.0,&#xA;  &#34;detailed_information&#34;: [&#xA;    {&#xA;      &#39;prompt&#39;: &#39;def get_max_triples(n): &#34;&#34;&#34; You are given a positive integer n. You have to create an integer array a of length n. For each i (1 â‰¤ i â‰¤ n), the value of a[i] = i * i - i + 1. Return the number of triples (a[i], a[j], a[k]) of a where i &amp;lt; j &amp;lt; k, and a[i] + a[j] + a[k] is a multiple of 3. Example : Input: n = 5 Output: 1 Explanation: a = [1, 3, 7, 13, 21] The only valid triple is (1, 7, 13). &#34;&#34;&#34; Now implement the function get_max_triples using Python&#39;,&#xA;      &#39;response&#39;: &#39;def get_max_triples(n):\n    a = [i * i - i + 1 for i in range(1, n+1)]\n    count = 0\n    for i in range(n-2):\n        for j in range(i+1, n-1):\n            for k in range(j+1, n):\n                if (a[i] + a[j] + a[k]) % 3 == 0:\n                    count += 1\n    return count\n\nprint(get_max_triples(5)) # Output: 1&#39;, &#xA;      &#39;category&#39;: &#39;code&#39;,&#xA;      &#39;entry_point&#39;: &#39;get_max_triples&#39;, &#xA;      &#39;claim&#39;: &#39;def get_max_triples(n):\n    a = [i * i - i + 1 for i in range(1, n+1)]\n    count = 0\n    for i in range(n-2):\n        for j in range(i+1, n-1):\n            for k in range(j+1, n):\n                if (a[i] + a[j] + a[k]) % 3 == 0:\n                    count += 1\n    return count\n\nprint(get_max_triples(5)) # Output: 1&#39;, &#xA;      &#39;testcases_queries&#39;: [&#39;get_max_triples(5)&#39;, &#39;get_max_triples(10)&#39;, &#39;get_max_triples(3)&#39;], &#39;potential_solutions_queries&#39;: [&#39;def get_max_triples(n):\n    a = [i * i - i + 1 for i in range(1, n+1)]\n    count = 0\n    for i in range(n-2):\n        for j in range(i+1, n-1):\n            for k in range(j+1, n):\n                if (a[i] + a[j] + a[k]) % 3 == 0:\n                    count += 1\n    return count&#39;, &#39;def get_max_triples(n):\n    a = [i * i - i + 1 for i in range(1, n + 1)]\n    count = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            for k in range(j + 1, n):\n                if (a[i] + a[j] + a[k]) % 3 == 0:\n                    count += 1\n    return count&#39;, &#39;def get_max_triples(n):\n    a = [i * i - i + 1 for i in range(1, n+1)]\n    count = 0\n    for i in range(n):\n        for j in range(i+1, n):\n            for k in range(j+1, n):\n                if (a[i] + a[j] + a[k]) % 3 == 0:\n                    count += 1\n    return count&#39;, &#39;def get_max_triples(n):\n    a = [i * i - i + 1 for i in range(1, n+1)]\n    count = 0\n    for i in range(n-2):\n        for j in range(i+1, n-1):\n            for k in range(j+1, n):\n                if (a[i] + a[j] + a[k]) % 3 == 0:\n                    count += 1\n    return count\n\nprint(get_max_triples(5)) # Output: 1&#39;], &#xA;      &#39;exec_results&#39;: [[1, 1, 1, 1], [36, 36, 36, 36], [0, 0, 0, 0]],&#xA;      &#39;claim_level_factuality&#39;: True,&#xA;      &#39;response_level_factuality&#39;: True&#xA;    }&#xA;  ]&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Math&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Detailed usage of factool on math (click to toggle the content)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export OPENAI_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from factool import Factool&#xA;&#xA;&#xA;# Initialize a Factool instance with the specified keys. foundation_model could be either &#34;gpt-3.5-turbo&#34; or &#34;gpt-4&#34;&#xA;factool_instance = Factool(&#34;gpt-4&#34;)&#xA;&#xA;inputs = [&#xA;        {&#xA;            &#34;prompt&#34;: &#34;Stephen placed an online order for groceries.  His final bill came to $7023116.  Because this was through a delivery vendor, they tacked on a 25% fee to his final total and charged him $3.00 in delivery fees.  Stephen also added a $4.00 tip.  After the extra fees, what was the final price of Stephen&#39;s groceries?&#34;,&#xA;            &#34;category&#34;: &#34;math&#34;,&#xA;            &#34;response&#34;: &#34;Okay, here are the steps to solve this:\n* Stephen&#39;s initial grocery bill was $7023116\n* The delivery vendor charged a 25% fee on top of that. 25% of $7023116 is 0.25 * $7023116 = $1755779\n* They also charged a flat $3 delivery fee\n* Stephen added a $4 tip\n* So the total charges were:\n    * Initial groceries: $7023116\n    * 25% fee: $1755779\n    * Delivery fee: $3 \n    * Tip: $4\n* Adding all that up, the total final price was $7023116 + $1755779 + $3 + $4 = $8779902\n\nSo the final price of Stephen&#39;s groceries was $8779902&#34;&#xA;        },&#xA;]&#xA;&#xA;response_list = factool_instance.run(inputs)&#xA;&#xA;print(response_list)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The response_list should follow the following format:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    &#34;average_claim_level_factuality&#34;: avg_claim_level_factuality,&#xA;    &#34;average_response_level_factuality&#34;: avg_response_level_factuality,&#xA;    &#34;detailed_information&#34;: [&#xA;      {&#xA;          &#39;prompt&#39;: prompt_1, &#xA;          &#39;response&#39;: response_1, &#xA;          &#39;category&#39;: &#39;math&#39;, &#xA;          &#39;claims&#39;: [claim_11, claim_12, ..., claims_1n], &#xA;          &#39;queries&#39;: [query_11, query_12, ..., query_1n], &#xA;          &#39;execution_results&#39;: [exec_result_11, exec_result_12, ..., exec_result_1n],&#xA;          &#39;claim_level_factuality&#39;: [factuality_11, factuality_12, ..., factuality_1n], &#xA;          &#39;response_level_factuality&#39;: factuality_1&#xA;      },&#xA;      {&#xA;          &#39;prompt&#39;: prompt_2, &#xA;          &#39;response&#39;: response_2, &#xA;          &#39;category&#39;: &#39;math&#39;, &#xA;          &#39;claims&#39;: [claim_21, claim_22, ..., claims_2n], &#xA;          &#39;queries&#39;: [query_21, query_22, ..., query_2n], &#xA;          &#39;execution_results&#39;: [exec_result_21, exec_result_22, ..., exec_result_2n],&#xA;          &#39;claim_level_factuality&#39;: [factuality_21, factuality_22, ..., factuality_2n], &#xA;          &#39;response_level_factuality&#39;: factuality_2&#xA;      },&#xA;      ...&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;In this case, you will get:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;average_claim_level_factuality&#34;: 0.5,&#xA;  &#34;average_response_level_factuality&#34;: 0.0,&#xA;  &#34;detailed_information&#34;: [&#xA;    {&#xA;      &#39;prompt&#39;: &#34;Stephen placed an online order for groceries.  His final bill came to $7023116.  Because this was through a delivery vendor, they tacked on a 25% fee to his final total and charged him $3.00 in delivery fees.  Stephen also added a $4.00 tip.  After the extra fees, what was the final price of Stephen&#39;s groceries?&#34;,&#xA;      &#39;category&#39;: &#39;math&#39;,&#xA;      &#39;response&#39;: &#34;Okay, here are the steps to solve this:\n* Stephen&#39;s initial grocery bill was $7023116\n* The delivery vendor charged a 25% fee on top of that. 25% of $7023116 is 0.25 * $7023116 = $1755779\n* They also charged a flat $3 delivery fee\n* Stephen added a $4 tip\n* So the total charges were:\n    * Initial groceries: $7023116\n    * 25% fee: $1755779\n    * Delivery fee: $3 \n    * Tip: $4\n* Adding all that up, the total final price was $7023116 + $1755779 + $3 + $4 = $8779902\n\nSo the final price of Stephen&#39;s groceries was $8779902&#34;, &#39;claims&#39;: [{&#39;math_calculation&#39;: &#39;0.25 * 7023116&#39;, &#39;calculated_answer&#39;: &#39;1755779&#39;}, {&#39;math_calculation&#39;: &#39;7023116 + 1755779 + 3 + 4&#39;, &#39;calculated_answer&#39;: &#39;8779902&#39;}], &#39;queries&#39;: [{&#39;python_snippet&#39;: &#39;print(0.25 * 7023116 == 1755779)&#39;}, {&#39;python_snippet&#39;: &#39;print(7023116 + 1755779 + 3 + 4 == 8779902)&#39;}], &#39;execution_results&#39;: [&#39;True&#39;, &#39;False&#39;],&#xA;      &#39;claim_level_factuality&#39;: [True, False],&#xA;      &#39;response_level_factuality&#39;: False&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Scientific Literature Review&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Detailed usage of factool on scientific literature review (click to toggle the content)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;export OPENAI_API_KEY=...&#xA;export SCRAPER_API_KEY=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from factool import Factool&#xA;&#xA;# Initialize a Factool instance with the specified keys. foundation_model could be either &#34;gpt-3.5-turbo&#34; or &#34;gpt-4&#34;&#xA;factool_instance = Factool(&#34;gpt-4&#34;)&#xA;&#xA;inputs = [&#xA;&#xA;        {&#xA;            &#34;prompt&#34;: &#34;Detail how machine learning algorithms are applied in data&#34;&#xA;                      &#34; analytics and the processes involved in training&#34;&#xA;                      &#34; these algorithms. Please cite relevant research papers&#34;&#xA;                      &#34; or textbooks to support your points, including the author&#39;s&#34;&#xA;                      &#34; name, the title of the work, and the year of publication.&#34;,&#xA;            &#34;category&#34;: &#34;scientific&#34;,&#xA;            &#34;response&#34;: &#34;Machine learning algorithms play a crucial role in data&#34;&#xA;                        &#34; analytics. They are used to extract insights and knowledge&#34;&#xA;                        &#34; from large datasets, which can be used to make informed&#34;&#xA;                        &#34; decisions and predictions. The following are the processes&#34;&#xA;                        &#34; involved in training machine learning algorithms:\n\n1. &#34;&#xA;                        &#34;Data Preprocessing: The first step in training a machine&#34;&#xA;                        &#34; learning algorithm is to preprocess the data. This involves&#34;&#xA;                        &#34; cleaning the data, handling missing values, &#34;&#xA;                        &#34;encoding categorical variables, and scaling the data. &#34;&#xA;                        &#34;The aim of preprocessing is to ensure that the data is in&#34;&#xA;                        &#34; a format that can be used by the machine learning algorithm.\n2. Feature Selection: Once the data has been preprocessed, the next step is to select the relevant features that will be used to train the machine learning algorithm. This involves identifying the features that are most important for predicting the target variable. Feature selection can be done using various techniques, such as filter methods, wrapper methods, and embedded methods.\n3. Model Selection: After feature selection, the next step is to select the appropriate machine learning algorithm to use. There are various types of machine learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning. The choice of algorithm depends on the nature of the problem and the type of data available.\n4. Training the Model: Once the model has been selected, the next step is to train the machine learning algorithm using the preprocessed data. This involves using a training set of data to optimize the parameters of the model so that it can accurately predict the target variable.\n5. Model Evaluation: After the model has been trained, the next step is to evaluate its performance. This involves testing the model on a separate test set of data and measuring its accuracy, precision, recall, and other performance metrics.\n6. Model Deployment: Once the model has been evaluated and fine-tuned, it can be deployed in a production environment. This involves integrating the model into a larger system or workflow, and monitoring its performance over time.\n\nResearch papers and textbooks that support the above points include:\n\n* \&#34;An Introduction to Machine Learning\&#34; by Alpaydin, B., (2010) which provides a comprehensive overview of machine learning algorithms and their applications.\n* \&#34;Data Mining: Concepts and Techniques\&#34; by Han, J., Kamber, B., &amp;amp; Pei, J., (2011) which provides a detailed introduction to data mining and its applications.\n* \&#34;Machine Learning: Trends, Perspectives, and Prospects\&#34; by Kamber, B., &amp;amp; Pei, J., (2012) which provides an overview of the current trends and future prospects of machine learning.\n* \&#34;Machine Learning for Data Mining\&#34; by He, C., &amp;amp; Wang, Y., (2015) which provides an in-depth introduction to machine learning algorithms and their applications in data mining.&#34;&#xA;        },&#xA;]&#xA;&#xA;response_list = factool_instance.run(inputs)&#xA;&#xA;print(response_list)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The response_list should follow the following format:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;  &#34;average_claim_level_factuality&#34;: avg_claim_level_factuality,&#xA;  &#34;average_response_level_factuality&#34;: avg_response_level_factuality,&#xA;  &#34;detailed_information&#34;: [&#xA;    {&#xA;        &#39;prompt&#39;: prompt_1, &#xA;        &#39;response&#39;: response_1, &#xA;        &#39;category&#39;: &#39;scientific&#39;, &#xA;        &#39;claims&#39;: [claim_11, claim_12, ..., claims_1n], &#xA;        &#39;queries&#39;: [query_11, query_12, ..., query_1n], &#xA;        &#39;evidences&#39;: [evidences_11, evidences_12, ..., evidences_1n], &#xA;        &#39;claim_level_factuality&#39;: [{claim_11, evidence_11, error_11, factuality_11}, {claim_12, evidence_12, error_12, factuality_12}, ..., {claim_1n, evidence_1n, error_1n, factuality_1n}], &#xA;        &#39;response_level_factuality&#39;: factuality_1&#xA;    },&#xA;    {&#xA;        &#39;prompt&#39;: prompt_2, &#xA;        &#39;response&#39;: response_2, &#xA;        &#39;category&#39;: &#39;scientific&#39;, &#xA;        &#39;claims&#39;: [claim_21, claim_22, ..., claims_2n], &#xA;        &#39;queries&#39;: [query_21, query_22, ..., query_2n],&#xA;        &#39;evidences&#39;: [evidences_21, evidences_22, ..., evidences_2n], &#xA;        &#39;claim_level_factuality&#39;: [{claim_21, evidence_21, error_21, factuality_21}, {claim_22, evidence_22, error_22, factuality_22}, ..., {claim_2n, evidence_2n, error_2n, factuality_2n}], &#xA;        &#39;response_level_factuality&#39;: factuality_2&#xA;    },&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;In this case, you will get:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;average_claim_level_factuality&#34;: 0.0, &#xA;    &#34;average_response_level_factuality&#34;: 0.0, &#xA;    &#34;detailed_information&#34;: [&#xA;      {&#xA;        &#39;prompt&#39;: &#34;Detail how machine learning algorithms are applied in data analytics and the processes involved in training these algorithms. Please cite relevant research papers or textbooks to support your points, including the author&#39;s name, the title of the work, and the year of publication.&#34;, &#39;response&#39;: &#39;Machine learning algorithms play a crucial role in data analytics. They are used to extract insights and knowledge from large datasets, which can be used to make informed decisions and predictions. The following are the processes involved in training machine learning algorithms:\n\n1. Data Preprocessing: The first step in training a machine learning algorithm is to preprocess the data. This involves cleaning the data, handling missing values, encoding categorical variables, and scaling the data. The aim of preprocessing is to ensure that the data is in a format that can be used by the machine learning algorithm.\n2. Feature Selection: Once the data has been preprocessed, the next step is to select the relevant features that will be used to train the machine learning algorithm. This involves identifying the features that are most important for predicting the target variable. Feature selection can be done using various techniques, such as filter methods, wrapper methods, and embedded methods.\n3. Model Selection: After feature selection, the next step is to select the appropriate machine learning algorithm to use. There are various types of machine learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning. The choice of algorithm depends on the nature of the problem and the type of data available.\n4. Training the Model: Once the model has been selected, the next step is to train the machine learning algorithm using the preprocessed data. This involves using a training set of data to optimize the parameters of the model so that it can accurately predict the target variable.\n5. Model Evaluation: After the model has been trained, the next step is to evaluate its performance. This involves testing the model on a separate test set of data and measuring its accuracy, precision, recall, and other performance metrics.\n6. Model Deployment: Once the model has been evaluated and fine-tuned, it can be deployed in a production environment. This involves integrating the model into a larger system or workflow, and monitoring its performance over time.\n\nResearch papers and textbooks that support the above points include:\n\n* &#34;An Introduction to Machine Learning&#34; by Alpaydin, B., (2010) which provides a comprehensive overview of machine learning algorithms and their applications.\n* &#34;Data Mining: Concepts and Techniques&#34; by Han, J., Kamber, B., &amp;amp; Pei, J., (2011) which provides a detailed introduction to data mining and its applications.\n* &#34;Machine Learning: Trends, Perspectives, and Prospects&#34; by Kamber, B., &amp;amp; Pei, J., (2012) which provides an overview of the current trends and future prospects of machine learning.\n* &#34;Machine Learning for Data Mining&#34; by He, C., &amp;amp; Wang, Y., (2015) which provides an in-depth introduction to machine learning algorithms and their applications in data mining.&#39;, &#xA;        &#39;category&#39;: &#39;scientific&#39;, &#xA;        &#39;claims&#39;: [{&#39;paper_title&#39;: &#39;An Introduction to Machine Learning&#39;, &#39;paper_author(s)&#39;: &#39;Alpaydin, B.&#39;, &#39;paper_pub_year&#39;: &#39;2010&#39;}, {&#39;paper_title&#39;: &#39;Data Mining: Concepts and Techniques&#39;, &#39;paper_author(s)&#39;: &#39;Han, J., Kamber, B., &amp;amp; Pei, J.&#39;, &#39;paper_pub_year&#39;: &#39;2011&#39;}, {&#39;paper_title&#39;: &#39;Machine Learning: Trends, Perspectives, and Prospects&#39;, &#39;paper_author(s)&#39;: &#39;Kamber, B., &amp;amp; Pei, J.&#39;, &#39;paper_pub_year&#39;: &#39;2012&#39;}, {&#39;paper_title&#39;: &#39;Machine Learning for Data Mining&#39;, &#39;paper_author(s)&#39;: &#39;He, C., &amp;amp; Wang, Y.&#39;, &#39;paper_pub_year&#39;: &#39;2015&#39;}], &#xA;        &#39;queries&#39;: [&#39;An Introduction to Machine Learning&#39;, &#39;Data Mining: Concepts and Techniques&#39;, &#39;Machine Learning: Trends, Perspectives, and Prospects&#39;, &#39;Machine Learning for Data Mining&#39;], &#xA;        &#39;evidences&#39;: [{&#39;title&#39;: &#39;Introduction to machine learning&#39;, &#39;author&#39;: [&#39;Y BaÅŸtanlar&#39;, &#39;M Ã–zuysal&#39;], &#39;pub_year&#39;: &#39;2014&#39;}, {&#39;title&#39;: &#39;Data mining: Data mining concepts and techniques&#39;, &#39;author&#39;: [&#39;S Agarwal&#39;], &#39;pub_year&#39;: &#39;2013&#39;}, {&#39;title&#39;: &#39;Machine learning: Trends, perspectives, and prospects&#39;, &#39;author&#39;: [&#39;MI Jordan&#39;, &#39;TM Mitchell&#39;], &#39;pub_year&#39;: &#39;2015&#39;}, {&#39;title&#39;: &#39;Machine learning and data mining&#39;, &#39;author&#39;: [&#39;TM Mitchell&#39;], &#39;pub_year&#39;: &#39;1999&#39;}], &#xA;        &#39;claim_level_factuality&#39;: [{&#39;generated_paper_title&#39;: &#39;An Introduction to Machine Learning&#39;, &#39;generated_paper_author(s)&#39;: &#39;Alpaydin, B.&#39;, &#39;generated_paper_pub_year&#39;: &#39;2010&#39;, &#39;actual_paper_title&#39;: &#39;Introduction to machine learning&#39;, &#39;actual_paper_author(s)&#39;: [&#39;Y BaÅŸtanlar&#39;, &#39;M Ã–zuysal&#39;], &#39;actual_paper_pub_year&#39;: &#39;2014&#39;, &#39;error&#39;: [&#39;wrong_paper_author(s)&#39;, &#39;wrong_paper_pub_year&#39;], &#39;factuality&#39;: False}, {&#39;generated_paper_title&#39;: &#39;Data Mining: Concepts and Techniques&#39;, &#39;generated_paper_author(s)&#39;: &#39;Han, J., Kamber, B., &amp;amp; Pei, J.&#39;, &#39;generated_paper_pub_year&#39;: &#39;2011&#39;, &#39;actual_paper_title&#39;: &#39;Data mining: Data mining concepts and techniques&#39;, &#39;actual_paper_author(s)&#39;: [&#39;S Agarwal&#39;], &#39;actual_paper_pub_year&#39;: &#39;2013&#39;, &#39;error&#39;: [&#39;wrong_paper_title&#39;, &#39;wrong_paper_author(s)&#39;, &#39;wrong_paper_pub_year&#39;], &#39;factuality&#39;: False}, {&#39;generated_paper_title&#39;: &#39;Machine Learning: Trends, Perspectives, and Prospects&#39;, &#39;generated_paper_author(s)&#39;: &#39;Kamber, B., &amp;amp; Pei, J.&#39;, &#39;generated_paper_pub_year&#39;: &#39;2012&#39;, &#39;actual_paper_title&#39;: &#39;Machine learning: Trends, perspectives, and prospects&#39;, &#39;actual_paper_author(s)&#39;: [&#39;MI Jordan&#39;, &#39;TM Mitchell&#39;], &#39;actual_paper_pub_year&#39;: &#39;2015&#39;, &#39;error&#39;: [&#39;wrong_paper_author(s)&#39;, &#39;wrong_paper_pub_year&#39;], &#39;factuality&#39;: False}, {&#39;generated_paper_title&#39;: &#39;Machine Learning for Data Mining&#39;, &#39;generated_paper_author(s)&#39;: &#39;He, C., &amp;amp; Wang, Y.&#39;, &#39;generated_paper_pub_year&#39;: &#39;2015&#39;, &#39;actual_paper_title&#39;: &#39;Machine learning and data mining&#39;, &#39;actual_paper_author(s)&#39;: [&#39;TM Mitchell&#39;], &#39;actual_paper_pub_year&#39;: &#39;1999&#39;, &#39;error&#39;: [&#39;wrong_paper_title&#39;, &#39;wrong_paper_author(s)&#39;, &#39;wrong_paper_pub_year&#39;], &#39;factuality&#39;: False}], &#xA;        &#39;response_level_factuality&#39;: False&#xA;      }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;ChatGPT Plugin with Factool&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;export the API keys&lt;/li&gt; &#xA; &lt;li&gt;Install the package: &lt;a href=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;git clone the repo: &lt;code&gt;git clone https://github.com/GAIR-NLP/factool.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd ./plugin_config&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the API locally: &lt;code&gt;uvicorn main:app --host 0.0.0.0 --port ${PORT:-5003}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter plugin store of &lt;a href=&#34;https://chat.openai.com/?model=gpt-4-plugins&#34;&gt;ChatGPT Website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &#39;develop your own plugin&#39; then enter the website domain &lt;code&gt;localhost:5003&lt;/code&gt; under &#39;domain&#39;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Videos (click to toggle the content)&lt;/summary&gt; &#xA; &lt;p&gt;Knowledge-based QA:&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/figs/factool_plugin_kbqa3.gif&#34; alt=&#34;Alt Text&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Code:&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/figs/factool_plugin_code.gif&#34; alt=&#34;Alt Text&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Math:&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/figs/factool_plugin_math.gif&#34; alt=&#34;Alt Text&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Scientific Literature Review:&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GAIR-NLP/factool/main/figs/factool_plugin_scientific.gif&#34; alt=&#34;Alt Text&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our &lt;a href=&#34;https://arxiv.org/abs/2307.13528&#34;&gt;paper&lt;/a&gt; if you find the repository helpful.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{chern2023factool,&#xA;  title={FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},&#xA;  author={Chern, I-Chun and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei},&#xA;  journal={arXiv preprint arXiv:2307.13528},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>