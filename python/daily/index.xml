<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-30T01:32:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Tele-AI/TeleSpeech-ASR</title>
    <updated>2024-05-30T01:32:33Z</updated>
    <id>tag:github.com,2024-05-30:/Tele-AI/TeleSpeech-ASR</id>
    <link href="https://github.com/Tele-AI/TeleSpeech-ASR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; æ˜Ÿè¾°è¯­éŸ³å¤§æ¨¡å‹-è¶…å¤šæ–¹è¨€ASR &lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ğŸ¤— &lt;a href=&#34;https://huggingface.co/Tele-AI/TeleSpeech-ASR1.0&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¾ &lt;a href=&#34;https://gitee.com/Tele-AI/TeleSpeech-ASR&#34; target=&#34;_blank&#34;&gt;gitee&lt;/a&gt;ï¸ &lt;/p&gt; &#xA;&lt;h1&gt;ç›®å½•&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E7%9B%AE%E5%BD%95&#34;&gt;ç›®å½•&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E6%A8%A1%E5%9E%8B%E5%BC%80%E6%BA%90&#34;&gt;æ¨¡å‹å¼€æº&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE&#34;&gt;ç¯å¢ƒé…ç½®&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&#34;&gt;é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E8%A1%A8%E5%BE%81%E8%AE%AD%E7%BB%83%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1&#34;&gt;è¡¨å¾è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87&#34;&gt;æ•°æ®å‡†å¤‡&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&#34;&gt;ç‰¹å¾æå–&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E5%AD%97%E5%85%B8%E5%87%86%E5%A4%87&#34;&gt;å­—å…¸å‡†å¤‡&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B&#34;&gt;å¾®è°ƒæ¨¡å‹æ¨ç†æµç¨‹ç¤ºä¾‹*&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83-1&#34;&gt;é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E5%BE%AE%E8%B0%83%E9%98%B6%E6%AE%B5&#34;&gt;å¾®è°ƒé˜¶æ®µ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E6%8E%A8%E7%90%86%E4%B8%8E%E8%A7%A3%E7%A0%81%E9%98%B6%E6%AE%B5&#34;&gt;æ¨ç†ä¸è§£ç é˜¶æ®µ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E8%A1%A8%E5%BE%81%E8%AE%AD%E7%BB%83%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1-1&#34;&gt;è¡¨å¾è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E5%BC%80%E6%BA%90%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BB%93%E6%9E%9C&#34;&gt;å¼€æºæ•°æ®é›†ç»“æœ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E5%A3%B0%E6%98%8E%E4%B8%8E%E5%8D%8F%E8%AE%AE&#34;&gt;å£°æ˜ä¸åè®®&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E5%A3%B0%E6%98%8E&#34;&gt;å£°æ˜&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E5%8D%8F%E8%AE%AE&#34;&gt;åè®®&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;æ¨¡å‹å¼€æº&lt;/h1&gt; &#xA;&lt;p&gt;æ˜Ÿè¾°è¶…å¤šæ–¹è¨€è¯­éŸ³è¯†åˆ«å¤§æ¨¡å‹v1.0ï¼Œç”±30ä¸‡å°æ—¶æ— æ ‡æ³¨å¤šæ–¹è¨€è¯­éŸ³æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åˆ©ç”¨å†…éƒ¨30ç§æœ‰æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ‰“ç ´å•ä¸€æ¨¡å‹åªèƒ½è¯†åˆ«ç‰¹å®šå•ä¸€æ–¹è¨€çš„å›°å¢ƒï¼Œå¯æ”¯æŒç†è§£ç²¤è¯­ã€ä¸Šæµ·è¯ã€å››å·è¯ã€æ¸©å·è¯ç­‰30ç§æ–¹è¨€&lt;/p&gt; &#xA;&lt;p&gt;æœ¬æ¬¡å¼€æºä¸‰ä¸ªæ¨¡å‹ï¼šä¸¤ä¸ª30ä¸‡å°æ—¶æ— æ ‡æ³¨è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹å’Œä¸€ä¸ªKeSpeechæ•°æ®é›†8ç§æ–¹è¨€å¾®è°ƒæ¨¡å‹ã€‚å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½é“¾æ¥è§ä¸‹è¡¨&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¨¡å‹ç‰ˆæœ¬&lt;/th&gt; &#xA;   &lt;th&gt;å‚æ•°é‡&lt;/th&gt; &#xA;   &lt;th&gt;ä¸‹è½½é“¾æ¥&lt;/th&gt; &#xA;   &lt;th&gt;å­—å…¸&lt;/th&gt; &#xA;   &lt;th&gt;å¤‡æ³¨&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pretrain_base&lt;/td&gt; &#xA;   &lt;td&gt;0.09 B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI/TeleSpeech-ASR1.0/blob/main/base.pt&#34;&gt;TeleSpeech-ASR1.0-base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ—&lt;/td&gt; &#xA;   &lt;td&gt;30ä¸‡å°æ—¶æ— æ ‡æ³¨è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pretrain_large&lt;/td&gt; &#xA;   &lt;td&gt;0.3 B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI/TeleSpeech-ASR1.0/blob/main/large.pt&#34;&gt;TeleSpeech-ASR1.0-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ—&lt;/td&gt; &#xA;   &lt;td&gt;30ä¸‡å°æ—¶æ— æ ‡æ³¨è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;finetune_large_kespeech&lt;/td&gt; &#xA;   &lt;td&gt;0.3 B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI/TeleSpeech-ASR1.0/blob/main/finetune_large_kespeech.pt&#34;&gt;TeleSpeech-ASR1.0-large-kespeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Tele-AI/TeleSpeech-ASR1.0/blob/main/dict.chr7531.txt&#34;&gt;dict.char7531.txt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäºpretrain_largeï¼Œé‡‡ç”¨KeSpeechæ•°æ®é›†&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#KeSpeech%E5%90%84%E6%96%B9%E8%A8%80%E4%B8%8A%E7%BB%93%E6%9E%9C&#34;&gt;8ç§æ–¹è¨€&lt;/a&gt;å¾®è°ƒè®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;finetuneæ¨¡å‹ä¸ºå·²ç»åœ¨ç‰¹å®šæ•°æ®é›†å¾®è°ƒè¿‡çš„æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨&lt;/li&gt; &#xA; &lt;li&gt;pretrainæ¨¡å‹ä¸ºæ— ç›‘ç£é¢„è®­ç»ƒæ¨¡å‹ï¼Œ&lt;strong&gt;æ— æ³•ç›´æ¥è¿›è¡ŒASRä»»åŠ¡&lt;/strong&gt;ï¼Œéœ€è¦ç”¨å°‘é‡æ ‡æ³¨æ•°æ®è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒåä½¿ç”¨ã€‚ç›¸æ¯”äºç›´æ¥è®­ç»ƒçš„æ–¹è¨€è¯†åˆ«æ¨¡å‹ï¼ŒåŸºäºé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥åˆ©ç”¨æ›´å°‘çš„æœ‰æ ‡æ³¨æ•°æ®è·å¾—æ›´å¥½çš„æ–¹è¨€è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†ä¸¤ç§æœ‰ç›‘ç£è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºä¸‹æ¸¸ASRä»»åŠ¡ï¼š1) åŸºäºfairseqçš„é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼› 2) åŸºäºwenetçš„è¡¨å¾æå–ï¼ˆç‰¹å¾æå–å™¨ï¼‰è®­ç»ƒä¸‹æ¸¸ASRæ¨¡å‹&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ç¯å¢ƒé…ç½®&lt;/h1&gt; &#xA;&lt;p&gt;ç¯å¢ƒä¾èµ–&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch version &amp;gt;= 1.13.0&lt;/li&gt; &#xA; &lt;li&gt;Python version &amp;gt;= 3.8&lt;/li&gt; &#xA; &lt;li&gt;æ•°æ®å‡†å¤‡ã€ç¨‹åºè®­ç»ƒéœ€è¦ä½¿ç”¨kaldiï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…ï¼š&lt;a href=&#34;https://github.com/kaldi-asr/kaldi&#34;&gt;https://github.com/kaldi-asr/kaldi&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;è‹¥å·²æœ‰æå¥½çš„ç‰¹å¾ï¼Œç¨‹åºè¿è¡Œæ—¶å¯ä»¥ä½¿ç”¨wenetå¼€æºæ¡†æ¶ä¸­kaldi_io.pyå®ç°çš„æ–¹æ³•æ›¿æ¢kaldiio.load_matï¼Œä»è€Œæ— éœ€å®‰è£…kaldi&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;fairseqå®‰è£…&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å®‰è£…fairseqåŠå…¶ä¾èµ–&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/pytorch/fairseq&#xA;$ cd fairseq&#xA;$ pip install --editable ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å®‰è£…kaldiio&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install kaldiio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;è¡¨å¾è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ç¡®ä¿fairseqå·²æ­£ç¡®&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#fairseq%E5%AE%89%E8%A3%85&#34;&gt;å®‰è£…&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å®‰è£…è¡¨å¾è®­ç»ƒä»»åŠ¡è¿è¡Œæ‰€éœ€ä¾èµ–&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd wenet_representation&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;æ•°æ®å‡†å¤‡&lt;/h1&gt; &#xA;&lt;h2&gt;ç‰¹å¾æå–&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a id=&#34;ç‰¹å¾æå–&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¨¡å‹è¾“å…¥ä¸º40ç»´mfccç‰¹å¾ï¼Œ&lt;strong&gt;éåŸå§‹éŸ³é¢‘&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;åˆ©ç”¨kaldiæå–40ç»´mfccç‰¹å¾ï¼Œè¿è¡Œè„šæœ¬å‚è€ƒ&lt;code&gt;prepare_kaldi_feats.sh&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;å¯å°†è¿è¡Œè„šæœ¬&lt;code&gt;prepare_kaldi_feats.sh&lt;/code&gt;ä¸å‚æ•°è®¾ç½®&lt;code&gt;mfcc_hires.conf&lt;/code&gt;ç½®äºkaldiä»»ä¸€egsç›®å½•ä¸‹ï¼ˆä¸cmd.shç­‰è„šæœ¬å¹³çº§ï¼Œä¾‹å¦‚/path/to/kaldi/egs/aishell/s5/prepare_kaldi_feats.shï¼‰ï¼Œè¿è¡Œ&lt;code&gt;prepare_kaldi_feats.sh&lt;/code&gt;å³å¯&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ä¸ºå„æ•°æ®é›†å‡†å¤‡è®­ç»ƒç”¨æ–‡ä»¶&lt;code&gt;data.list&lt;/code&gt;ï¼Œå¯å‚è€ƒ&lt;code&gt;make_datalist.py&lt;/code&gt;ï¼Œä»¥&lt;code&gt;\t&lt;/code&gt;åˆ†éš”ï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cat train/data.list&#xA;utt:X0000000000_100638174_S00037&#x9;feat:/data/raw_nnaudio.test.1.ark:2983479385&#x9;feat_shape:363,40&#x9;text:ä¸æƒœåœ¨è¿™ç§è¯•éªŒä¸­æ¯ç­åŒ…æ‹¬è‡ªå·±åœ¨å†…çš„ä¸€åˆ‡&#x9;token:ä¸ æƒœ åœ¨ è¿™ ç§ è¯• éªŒ ä¸­ æ¯ ç­ åŒ… æ‹¬ è‡ª å·± åœ¨ å†… çš„ ä¸€ åˆ‡&#x9;tokenid:[TOKENID]&#x9;token_shape:19,5537&#xA;utt:X0000000001_100849618_S00006&#x9;feat:/data/raw_nnaudio.test.1.ark:2984296665&#x9;feat_shape:345,40&#x9;text:åœ¨ä»–ä»¬æ”¶åˆ°è¶³å¤Ÿå»ºç«‹å¤§ç»Ÿä¸€æ¨¡å‹çš„æ•°æ®å&#x9;token:åœ¨ ä»– ä»¬ æ”¶ åˆ° è¶³ å¤Ÿ å»º ç«‹ å¤§ ç»Ÿ ä¸€ æ¨¡ å‹ çš„ æ•° æ® å&#x9;tokenid:[TOKENID]&#x9;token_shape:18,5537&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;å­—å…¸å‡†å¤‡&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦å‡†å¤‡fairseqæ ¼å¼çš„ &lt;code&gt;dict.${label}.txt&lt;/code&gt;ï¼Œ&lt;code&gt;${label}&lt;/code&gt;ä¸ºå»ºæ¨¡å•å…ƒç±»å‹ï¼Œå¦‚ltr, bpeç­‰ã€‚ä»¥&lt;code&gt;dict.ltr.txt&lt;/code&gt;ä¸ºä¾‹ï¼š&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;æ˜¯ 2&#xA;å¥½ 3&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;é¢„è®­ç»ƒæ¨¡å‹è¡¨å¾è®­ç»ƒASRä»»åŠ¡é˜¶æ®µï¼Œéœ€è¦å‡†å¤‡wenetæ ¼å¼çš„&lt;code&gt;lang_char.txt&lt;/code&gt;ï¼Œç›¸æ¯”äº&lt;code&gt;dict.${label}.txt&lt;/code&gt;é¢å¤–æ·»åŠ &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;sos/eos&amp;gt;&lt;/code&gt;3ä¸ªtokenï¼Œä¾‹å¦‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;blank&amp;gt; 0&#xA;&amp;lt;unk&amp;gt; 1&#xA;æ˜¯ 2&#xA;å¥½ 3&#xA;...&#xA;&amp;lt;sos/eos&amp;gt; 5536&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;å¾®è°ƒæ¨¡å‹æ¨ç†æµç¨‹ç¤ºä¾‹*&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#fairseq%E5%AE%89%E8%A3%85&#34;&gt;fairseqç¯å¢ƒå‡†å¤‡&lt;/a&gt;ï¼Œä¿®æ”¹&lt;code&gt;data2vec_dialect/path.sh&lt;/code&gt;æ–‡ä»¶ä¸­&lt;code&gt;/path/to/fairseq&lt;/code&gt;ä¸ºfairseqå®‰è£…è·¯å¾„&lt;/li&gt; &#xA; &lt;li&gt;åˆ©ç”¨kaldiæå–éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶ä¿å­˜ä¸ºä»¥ .tsv ç»“å°¾çš„æ–‡ä»¶ï¼Œæ ¼å¼å‚è€ƒ&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&#34;&gt;ç‰¹å¾æå–&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;è¿›å…¥data2vec_dialectç›®å½•ï¼Œå¹¶ä¿®æ”¹&lt;code&gt;run_scripts/decode.sh&lt;/code&gt;æ–‡ä»¶ä¸­&lt;code&gt;/path/to&lt;/code&gt;ç›¸å…³è·¯å¾„ä¸ºæœ¬åœ°å­˜å‚¨è·¯å¾„&lt;/li&gt; &#xA; &lt;li&gt;æ‰§è¡Œ&lt;code&gt;run_scripts/decode.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;*ä»…ç»è¿‡å¾®è°ƒåçš„finetuneæ¨¡å‹æ”¯æŒç›´æ¥æ¨ç†ï¼Œæ— ç›‘ç£é¢„è®­ç»ƒæ¨¡å‹&lt;code&gt;pretrain_base&lt;/code&gt;å’Œ&lt;code&gt;pretrain_large&lt;/code&gt;éœ€è¦å…ˆåœ¨æ ‡æ³¨æ•°æ®ä¸Šè®­ç»ƒåï¼Œå†è¿›è¡Œæ¨ç†ï¼Œè¯¦è§&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&#34;&gt;é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ&lt;/a&gt;æˆ–&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/#%E8%A1%A8%E5%BE%81%E8%AE%AD%E7%BB%83%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1&#34;&gt;è¡¨å¾è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a id=&#34;é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å¾®è°ƒé˜¶æ®µ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å‡†å¤‡&lt;code&gt;train.tsv&lt;/code&gt;å’Œ&lt;code&gt;dev.tsv&lt;/code&gt;ï¼Œä¿å­˜äºåŒä¸€è®­ç»ƒç›®å½•ä¸‹ &lt;pre&gt;&lt;code&gt;$ ln -s /path/to/train/data.list /path/to/train/train.tsv&#xA;$ ln -s /path/to/dev/data.list /path/to/train/dev.tsv&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;è¿›å…¥data2vec_dialectè·¯å¾„ï¼Œä¿®æ”¹&lt;code&gt;path.sh&lt;/code&gt;æ–‡ä»¶ä¸­&lt;code&gt;/path/to/fairseq&lt;/code&gt;ä¸ºfairseqå®‰è£…è·¯å¾„&lt;/li&gt; &#xA; &lt;li&gt;å°†&lt;code&gt;run_scripts/run_d2v_finetune.sh&lt;/code&gt;ä¸­&lt;code&gt;/path/to&lt;/code&gt;ç›¸å…³è·¯å¾„æ›¿æ¢&lt;/li&gt; &#xA; &lt;li&gt;ä¿®æ”¹&lt;code&gt;task.data&lt;/code&gt;ä¸º .tsv æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼Œå¦‚&lt;code&gt;task.data=/data/wenetspeech/train&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;æ‰§è¡Œ &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ bash run_scripts/run_d2v_finetune.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ¨ç†ä¸è§£ç é˜¶æ®µ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åŒæ ·ä¿®æ”¹&lt;code&gt;run_scripts/decode.sh&lt;/code&gt;ä¸­çš„æ¨¡å‹è·¯å¾„ã€æµ‹è¯•æ•°æ®è·¯å¾„ç­‰ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;dataset.gen_subset&lt;/code&gt;ä¸ºæµ‹è¯•æ•°æ®è·¯å¾„ä¸‹ .tsv æ–‡ä»¶çš„åç§°ï¼Œå¯é…ç½®å¤šä¸ª&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;æ‰§è¡Œ &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ bash run_scripts/decode.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;è¡¨å¾è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a id=&#34;è¡¨å¾è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;è¿›å…¥wenet_representationè·¯å¾„ï¼Œä¿®æ”¹&lt;code&gt;path.sh&lt;/code&gt;æ–‡ä»¶ä¸­&lt;code&gt;fairseq&lt;/code&gt;, &lt;code&gt;data2vec_dialect&lt;/code&gt;, &lt;code&gt;wenet_representation&lt;/code&gt;ç›¸å…³è·¯å¾„&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;è¿ç»­è¡¨å¾è®­ç»ƒä¸è§£ç ï¼š&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;é…ç½®&lt;code&gt;run_d2v.sh&lt;/code&gt;ä¸­datasetç›¸å…³å†…å®¹ï¼Œæ‰§è¡Œ &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ bash run_d2v.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç¦»æ•£è¡¨å¾è®­ç»ƒä¸è§£ç ï¼š&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;é¦–å…ˆæ ¹æ®&lt;code&gt;data.list&lt;/code&gt;ï¼Œå‡†å¤‡ç¦»æ•£è¡¨å¾å¯¹åº”è®­ç»ƒæ–‡ä»¶&lt;code&gt;data.list.discrete&lt;/code&gt;ï¼Œä¿®æ”¹&lt;code&gt;wenet/discrete_token/kmeans_d2v.yaml&lt;/code&gt;ä¸­&lt;code&gt;model_dir&lt;/code&gt;å’Œ&lt;code&gt;user_dir&lt;/code&gt;ï¼Œæ‰§è¡Œ &lt;pre&gt;&lt;code&gt;$ bash wenet/discrete_token/dump_feat.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;å†é…ç½®&lt;code&gt;run_discrete.sh&lt;/code&gt;ä¸­datasetç›¸å…³å†…å®¹ï¼Œæ‰§è¡Œ &lt;pre&gt;&lt;code&gt;$ bash run_discrete.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;å¼€æºæ•°æ®é›†ç»“æœ&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æˆ‘ä»¬é€‰æ‹©äº†å¤šä¸ªå¼€æºä¸­æ–‡æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œä»¥æµ‹è¯•é›†ä¸Šçš„å­—é”™è¯¯ç‡ (Character Error Rate, CER) ç»“æœä½œä¸ºè¡¡é‡æ ‡å‡†&lt;/li&gt; &#xA; &lt;li&gt;åœ¨Aishell-1ä¸Šæˆ‘ä»¬é€‰æ‹©å…¶Trainé›†ä½œä¸ºæœ‰ç›‘ç£æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåœ¨Testé›†ä¸Šç»Ÿè®¡CER&lt;/li&gt; &#xA; &lt;li&gt;åœ¨WenetSpeechä¸Šï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨100å°æ—¶è®­ç»ƒé›†Train_så’Œ1000å°æ—¶è®­ç»ƒé›†Train_måˆ†åˆ«ä½œä¸ºæœ‰ç›‘ç£æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåœ¨Test_Meetingæµ‹è¯•é›†ä¸Šç»Ÿè®¡CER&lt;/li&gt; &#xA; &lt;li&gt;Babelä¸ºNISTï¼ˆç¾å›½å›½å®¶æ ‡å‡†ä¸æŠ€æœ¯ç ”ç©¶é™¢ï¼‰ä¸¾åŠçš„ä½èµ„æºç²¤è¯­ç”µè¯è¯†åˆ«ä»»åŠ¡æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¶æä¾›çš„è®­ç»ƒé›†ä¸æµ‹è¯•é›†ç»Ÿè®¡CER&lt;/li&gt; &#xA; &lt;li&gt;KeSpeechä¸ºä¸­æ–‡å¤šæ–¹è¨€æµ‹è¯•é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨1396å°æ—¶è®­ç»ƒé›†ä½œä¸ºæœ‰ç›‘ç£æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œé€‰æ‹©æä¾›çš„Testæµ‹è¯•é›†ç»Ÿè®¡CER&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¨¡å‹ç‰ˆæœ¬&lt;/th&gt; &#xA;   &lt;th&gt;Aishell-1 (%)&lt;/th&gt; &#xA;   &lt;th&gt;WenetSpeech* (%)&lt;/th&gt; &#xA;   &lt;th&gt;Babel (%)&lt;/th&gt; &#xA;   &lt;th&gt;KeSpeech (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pretrain_base&lt;/td&gt; &#xA;   &lt;td&gt;4.7&lt;/td&gt; &#xA;   &lt;td&gt;18.3 / 16.4&lt;/td&gt; &#xA;   &lt;td&gt;22.1&lt;/td&gt; &#xA;   &lt;td&gt;10.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pretrain_large&lt;/td&gt; &#xA;   &lt;td&gt;4.0&lt;/td&gt; &#xA;   &lt;td&gt;14.3 / 13.0&lt;/td&gt; &#xA;   &lt;td&gt;19.1&lt;/td&gt; &#xA;   &lt;td&gt;8.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*WenetSpeechä¸­çš„ç»“æœä¸ºåˆ†åˆ«ä½¿ç”¨ &lt;code&gt;train_s/train_m&lt;/code&gt;è®­ç»ƒåï¼Œåœ¨Test_Meetingä¸Šçš„CER&lt;/p&gt; &#xA;&lt;p&gt;&lt;a id=&#34;KeSpeechå„æ–¹è¨€ä¸Šç»“æœ&#34;&gt;&lt;/a&gt; KeSpeechå„æ–¹è¨€ä¸Šç»“æœï¼ˆCER%ï¼‰&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¨¡å‹ç‰ˆæœ¬&lt;/th&gt; &#xA;   &lt;th&gt;æ™®é€šè¯&lt;/th&gt; &#xA;   &lt;th&gt;åŒ—äº¬&lt;/th&gt; &#xA;   &lt;th&gt;è¥¿å—&lt;/th&gt; &#xA;   &lt;th&gt;ä¸­åŸ&lt;/th&gt; &#xA;   &lt;th&gt;ä¸œåŒ—&lt;/th&gt; &#xA;   &lt;th&gt;å…°é“¶&lt;/th&gt; &#xA;   &lt;th&gt;æ±Ÿæ·®&lt;/th&gt; &#xA;   &lt;th&gt;å†€é²&lt;/th&gt; &#xA;   &lt;th&gt;èƒ¶è¾½&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pretrain_large&lt;/td&gt; &#xA;   &lt;td&gt;4.61&lt;/td&gt; &#xA;   &lt;td&gt;8.23&lt;/td&gt; &#xA;   &lt;td&gt;8.74&lt;/td&gt; &#xA;   &lt;td&gt;7.62&lt;/td&gt; &#xA;   &lt;td&gt;7.89&lt;/td&gt; &#xA;   &lt;td&gt;9.72&lt;/td&gt; &#xA;   &lt;td&gt;12.89&lt;/td&gt; &#xA;   &lt;td&gt;8.91&lt;/td&gt; &#xA;   &lt;td&gt;9.30&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;å£°æ˜ä¸åè®®&lt;/h1&gt; &#xA;&lt;h2&gt;å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;æˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œä¸è¦ä½¿ç”¨TeleSpeechæ¨¡å‹åŠå…¶è¡ç”Ÿæ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°†TeleSpeechæ¨¡å‹ç”¨äºæ²¡æœ‰å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰ä½¿ç”¨è€…éµå®ˆä¸Šè¿°åŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€å‘å±•åœ¨åˆæ³•åˆè§„çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚&lt;/p&gt; &#xA;&lt;p&gt;æˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨TeleSpeechå¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;åè®®&lt;/h2&gt; &#xA;&lt;p&gt;ç¤¾åŒºä½¿ç”¨TeleSpeechæ¨¡å‹éœ€è¦éµå¾ªã€Š&lt;a href=&#34;https://raw.githubusercontent.com/Tele-AI/TeleSpeech-ASR/master/TeleSpeech%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;TeleSpeechæ¨¡å‹ç¤¾åŒºè®¸å¯åè®®&lt;/a&gt;ã€‹ã€‚TeleSpeechæ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå¦‚æœæ‚¨è®¡åˆ’å°†TeleSpeechæ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨äºå•†ä¸šç›®çš„ï¼Œæ‚¨éœ€è¦é€šè¿‡ä»¥ä¸‹è”ç³»é‚®ç®± &lt;a href=&#34;mailto:tele_ai@chinatelecom.cn&#34;&gt;tele_ai@chinatelecom.cn&lt;/a&gt;ï¼Œæäº¤ã€ŠTeleSpeechæ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹è¦æ±‚çš„ç”³è¯·ææ–™ã€‚å®¡æ ¸é€šè¿‡åï¼Œå°†ç‰¹æ­¤æˆäºˆæ‚¨ä¸€ä¸ªéæ’ä»–æ€§ã€å…¨çƒæ€§ã€ä¸å¯è½¬è®©ã€ä¸å¯å†è®¸å¯ã€å¯æ’¤é”€çš„å•†ç”¨ç‰ˆæƒè®¸å¯ã€‚&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>vladmandic/automatic</title>
    <updated>2024-05-30T01:32:33Z</updated>
    <id>tag:github.com,2024-05-30:/vladmandic/automatic</id>
    <link href="https://github.com/vladmandic/automatic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SD.Next: Advanced Implementation of Stable Diffusion and other Diffusion-based generative image models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/vladmandic/automatic/raw/dev/html/favicon.png&#34; width=&#34;200&#34; alt=&#34;SD.Next&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;Stable Diffusion implementation with advanced features&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/sponsors/vladmandic&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=%23fe8e86&#34; alt=&#34;Sponsors&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/vladmandic/automatic?svg=true&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/vladmandic/automatic?svg=true&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://discord.gg/VjvR2tabEX&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1101998836328697867?logo=Discord&amp;amp;svg=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki&#34;&gt;Wiki&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/VjvR2tabEX&#34;&gt;Discord&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;Changelog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#sdnext-features&#34;&gt;SD.Next Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#backend-support&#34;&gt;Backend support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#model-support&#34;&gt;Model support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#platform-support&#34;&gt;Platform support&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/#notes&#34;&gt;Notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SD.Next Features&lt;/h2&gt; &#xA;&lt;p&gt;All individual features are not listed here, instead check &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;ChangeLog&lt;/a&gt; for full list of changes&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multiple backends!&lt;br&gt; â–¹ &lt;strong&gt;Diffusers | Original&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiple UIs!&lt;br&gt; â–¹ &lt;strong&gt;Standard | Modern&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiple diffusion models!&lt;br&gt; â–¹ &lt;strong&gt;Stable Diffusion 1.5/2.1 | SD-XL | LCM | Segmind | Kandinsky | Pixart-Î± | Pixart-Î£ | Stable Cascade | WÃ¼rstchen | aMUSEd | DeepFloyd IF | UniDiffusion | SD-Distilled | BLiP Diffusion | KOALA | SDXS | Hyper-SD | etc.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Built-in Control for Text, Image, Batch and video processing!&lt;br&gt; â–¹ &lt;strong&gt;ControlNet | ControlNet XS | Control LLLite | T2I Adapters | IP Adapters&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multiplatform!&lt;br&gt; â–¹ &lt;strong&gt;Windows | Linux | MacOS with CPU | nVidia | AMD | IntelArc/IPEX | DirectML | OpenVINO | ONNX+Olive | ZLUDA&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Platform specific autodetection and tuning performed on install&lt;/li&gt; &#xA; &lt;li&gt;Optimized processing with latest &lt;code&gt;torch&lt;/code&gt; developments with built-in support for &lt;code&gt;torch.compile&lt;/code&gt;&lt;br&gt; and multiple compile backends: &lt;em&gt;Triton, ZLUDA, StableFast, DeepCache, OpenVINO, NNCF, IPEX, OneDiff&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improved prompt parser&lt;/li&gt; &#xA; &lt;li&gt;Enhanced &lt;em&gt;Lora&lt;/em&gt;/&lt;em&gt;LoCon&lt;/em&gt;/&lt;em&gt;Lyco&lt;/em&gt; code supporting latest trends in training&lt;/li&gt; &#xA; &lt;li&gt;Built-in queue management&lt;/li&gt; &#xA; &lt;li&gt;Enterprise level logging and hardened API&lt;/li&gt; &#xA; &lt;li&gt;Built in installer with automatic updates and dependency management&lt;/li&gt; &#xA; &lt;li&gt;Modernized UI with theme support and number of built-in themes &lt;em&gt;(dark and light)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mobile compatible&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;Main interface using &lt;strong&gt;StandardUI&lt;/strong&gt;&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-text2image.jpg&#34; alt=&#34;Screenshot-Dark&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Main interface using &lt;strong&gt;ModernUI&lt;/strong&gt;&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-modernui.jpg&#34; alt=&#34;Screenshot-Dark&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For screenshots and informations on other available themes, see &lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Themes&#34;&gt;Themes Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Backend support&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;SD.Next&lt;/strong&gt; supports two main backends: &lt;em&gt;Diffusers&lt;/em&gt; and &lt;em&gt;Original&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusers&lt;/strong&gt;: Based on new &lt;a href=&#34;https://huggingface.co/docs/diffusers/index&#34;&gt;Huggingface Diffusers&lt;/a&gt; implementation&lt;br&gt; Supports &lt;em&gt;all&lt;/em&gt; models listed below&lt;br&gt; This backend is set as default for new installations&lt;br&gt; See &lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Diffusers&#34;&gt;wiki article&lt;/a&gt; for more information&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Original&lt;/strong&gt;: Based on &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;LDM&lt;/a&gt; reference implementation and significantly expanded on by &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;A1111&lt;/a&gt;&lt;br&gt; This backend and is fully compatible with most existing functionality and extensions written for &lt;em&gt;A1111 SDWebUI&lt;/em&gt;&lt;br&gt; Supports &lt;strong&gt;SD 1.x&lt;/strong&gt; and &lt;strong&gt;SD 2.x&lt;/strong&gt; models&lt;br&gt; All other model types such as &lt;em&gt;SD-XL, LCM, Stable Cascade, PixArt, Playground, Segmind, Kandinsky, etc.&lt;/em&gt; require backend &lt;strong&gt;Diffusers&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model support&lt;/h2&gt; &#xA;&lt;p&gt;Additional models will be added as they become available and there is public interest in them&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/stablediffusion/&#34;&gt;RunwayML Stable Diffusion&lt;/a&gt; 1.x and 2.x &lt;em&gt;(all variants)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;StabilityAI Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid&#34;&gt;StabilityAI Stable Video Diffusion&lt;/a&gt; Base, XT 1.0, XT 1.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/consistency_models&#34;&gt;LCM: Latent Consistency Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/playgroundai/playground-v2-256px-base&#34;&gt;Playground&lt;/a&gt; &lt;em&gt;v1, v2 256, v2 512, v2 1024 and latest v2.5&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableCascade&#34;&gt;Stable Cascade&lt;/a&gt; &lt;em&gt;Full&lt;/em&gt; and &lt;em&gt;Lite&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/amused/amused-256&#34;&gt;aMUSEd 256&lt;/a&gt; 256 and 512&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/segmind/Segmind-Vega&#34;&gt;Segmind Vega&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/segmind/SSD-1B&#34;&gt;Segmind SSD-1B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/segmind/segmoe&#34;&gt;Segmind SegMoE&lt;/a&gt; &lt;em&gt;SD and SD-XL&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ai-forever/Kandinsky-2&#34;&gt;Kandinsky&lt;/a&gt; &lt;em&gt;2.1 and 2.2 and latest 3.0&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Î± XL 2&lt;/a&gt; &lt;em&gt;Medium and Large&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-sigma&#34;&gt;PixArt-Î£&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/wuertschen&#34;&gt;Warp Wuerstchen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thu-ml/unidiffuser&#34;&gt;Tsinghua UniDiffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deep-floyd/IF&#34;&gt;DeepFloyd IF&lt;/a&gt; &lt;em&gt;Medium and Large&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/damo-vilab/text-to-video-ms-1.7b&#34;&gt;ModelScope T2V&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/sd_distillation&#34;&gt;Segmind SD Distilled&lt;/a&gt; &lt;em&gt;(all variants)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dxli94.github.io/BLIP-Diffusion-website/&#34;&gt;BLIP-Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/youngwanLEE/sdxl-koala&#34;&gt;KOALA 700M&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ali-vilab/i2vgen-xl&#34;&gt;VGen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDKiro/sdxs&#34;&gt;SDXS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ByteDance/Hyper-SD&#34;&gt;Hyper-SD&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also supported are modifiers such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LCM&lt;/strong&gt; and &lt;strong&gt;Turbo&lt;/strong&gt; (&lt;em&gt;adversarial diffusion distillation&lt;/em&gt;) networks&lt;/li&gt; &#xA; &lt;li&gt;All &lt;strong&gt;LoRA&lt;/strong&gt; types such as LoCon, LyCORIS, HADA, IA3, Lokr, OFT&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;IP-Adapters&lt;/strong&gt; for SD 1.5 and SD-XL&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;InstantID&lt;/strong&gt;, &lt;strong&gt;FaceSwap&lt;/strong&gt;, &lt;strong&gt;FaceID&lt;/strong&gt;, &lt;strong&gt;PhotoMerge&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AnimateDiff&lt;/strong&gt; for SD 1.5&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Platform support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;nVidia&lt;/em&gt; GPUs using &lt;strong&gt;CUDA&lt;/strong&gt; libraries on both &lt;em&gt;Windows and Linux&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;AMD&lt;/em&gt; GPUs using &lt;strong&gt;ROCm&lt;/strong&gt; libraries on &lt;em&gt;Linux&lt;/em&gt;&lt;br&gt; Support will be extended to &lt;em&gt;Windows&lt;/em&gt; once AMD releases ROCm for Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Intel Arc&lt;/em&gt; GPUs using &lt;strong&gt;OneAPI&lt;/strong&gt; with &lt;em&gt;IPEX XPU&lt;/em&gt; libraries on both &lt;em&gt;Windows and Linux&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Any GPU compatible with &lt;em&gt;DirectX&lt;/em&gt; on &lt;em&gt;Windows&lt;/em&gt; using &lt;strong&gt;DirectML&lt;/strong&gt; libraries&lt;br&gt; This includes support for AMD GPUs that are not supported by native ROCm libraries&lt;/li&gt; &#xA; &lt;li&gt;Any GPU or device compatible with &lt;strong&gt;OpenVINO&lt;/strong&gt; libraries on both &lt;em&gt;Windows and Linux&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Apple M1/M2&lt;/em&gt; on &lt;em&gt;OSX&lt;/em&gt; using built-in support in Torch with &lt;strong&gt;MPS&lt;/strong&gt; optimizations&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;ONNX/Olive&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;IP Adapters&lt;/em&gt;: &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-ipadapter.jpg&#34; alt=&#34;Screenshot-IPAdapter&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Color grading&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-color.jpg&#34; alt=&#34;Screenshot-Color&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;InstantID&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-instantid.jpg&#34; alt=&#34;Screenshot-InstantID&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Loading any model other than standard SD 1.x / SD 2.x requires use of backend &lt;strong&gt;Diffusers&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Loading any other models using &lt;strong&gt;Original&lt;/strong&gt; backend is not supported&lt;/li&gt; &#xA;  &lt;li&gt;Loading manually download model &lt;code&gt;.safetensors&lt;/code&gt; files is supported for specified models only (typically SD 1.x / SD 2.x / SD-XL models only)&lt;/li&gt; &#xA;  &lt;li&gt;For all other model types, use backend &lt;strong&gt;Diffusers&lt;/strong&gt; and use built in Model downloader or&lt;br&gt; select model from Networks -&amp;gt; Models -&amp;gt; Reference list in which case it will be auto-downloaded and loaded&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Installation&#34;&gt;Step-by-step install guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki/Advanced-Install&#34;&gt;Advanced install notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nWTnTyFTuAs&#34;&gt;Video: install and use&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/discussions/1627&#34;&gt;Common installation errors&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/discussions/1011&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;If you can&#39;t run SD.Next locally, try cloud deployment using &lt;a href=&#34;https://rundiffusion.com?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=SDNext&#34;&gt;RunDiffusion&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;Server can run with or without virtual environment,&lt;br&gt; Recommended to use &lt;code&gt;VENV&lt;/code&gt; to avoid library version conflicts with other applications&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;nVidia/CUDA&lt;/strong&gt; / &lt;strong&gt;AMD/ROCm&lt;/strong&gt; / &lt;strong&gt;Intel/OneAPI&lt;/strong&gt; are auto-detected if present and available,&lt;br&gt; For any other use case such as &lt;strong&gt;DirectML&lt;/strong&gt;, &lt;strong&gt;ONNX/Olive&lt;/strong&gt;, &lt;strong&gt;OpenVINO&lt;/strong&gt; specify required parameter explicitly&lt;br&gt; or wrong packages may be installed as installer will assume CPU-only environment&lt;/li&gt; &#xA;  &lt;li&gt;Full startup sequence is logged in &lt;code&gt;sdnext.log&lt;/code&gt;,&lt;br&gt; so if you encounter any issues, please check it first&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;p&gt;Once SD.Next is installed, simply run &lt;code&gt;webui.ps1&lt;/code&gt; or &lt;code&gt;webui.bat&lt;/code&gt; (&lt;em&gt;Windows&lt;/em&gt;) or &lt;code&gt;webui.sh&lt;/code&gt; (&lt;em&gt;Linux or MacOS&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;p&gt;List of available parameters, run &lt;code&gt;webui --help&lt;/code&gt; for the full &amp;amp; up-to-date list:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Server options:&#xA;  --config CONFIG                                    Use specific server configuration file, default: config.json&#xA;  --ui-config UI_CONFIG                              Use specific UI configuration file, default: ui-config.json&#xA;  --medvram                                          Split model stages and keep only active part in VRAM, default: False&#xA;  --lowvram                                          Split model components and keep only active part in VRAM, default: False&#xA;  --ckpt CKPT                                        Path to model checkpoint to load immediately, default: None&#xA;  --vae VAE                                          Path to VAE checkpoint to load immediately, default: None&#xA;  --data-dir DATA_DIR                                Base path where all user data is stored, default:&#xA;  --models-dir MODELS_DIR                            Base path where all models are stored, default: models&#xA;  --allow-code                                       Allow custom script execution, default: False&#xA;  --share                                            Enable UI accessible through Gradio site, default: False&#xA;  --insecure                                         Enable extensions tab regardless of other options, default: False&#xA;  --use-cpu USE_CPU [USE_CPU ...]                    Force use CPU for specified modules, default: []&#xA;  --listen                                           Launch web server using public IP address, default: False&#xA;  --port PORT                                        Launch web server with given server port, default: 7860&#xA;  --freeze                                           Disable editing settings&#xA;  --auth AUTH                                        Set access authentication like &#34;user:pwd,user:pwd&#34;&#34;&#xA;  --auth-file AUTH_FILE                              Set access authentication using file, default: None&#xA;  --autolaunch                                       Open the UI URL in the system&#39;s default browser upon launch&#xA;  --docs                                             Mount API docs, default: False&#xA;  --api-only                                         Run in API only mode without starting UI&#xA;  --api-log                                          Enable logging of all API requests, default: False&#xA;  --device-id DEVICE_ID                              Select the default CUDA device to use, default: None&#xA;  --cors-origins CORS_ORIGINS                        Allowed CORS origins as comma-separated list, default: None&#xA;  --cors-regex CORS_REGEX                            Allowed CORS origins as regular expression, default: None&#xA;  --tls-keyfile TLS_KEYFILE                          Enable TLS and specify key file, default: None&#xA;  --tls-certfile TLS_CERTFILE                        Enable TLS and specify cert file, default: None&#xA;  --tls-selfsign                                     Enable TLS with self-signed certificates, default: False&#xA;  --server-name SERVER_NAME                          Sets hostname of server, default: None&#xA;  --no-hashing                                       Disable hashing of checkpoints, default: False&#xA;  --no-metadata                                      Disable reading of metadata from models, default: False&#xA;  --disable-queue                                    Disable queues, default: False&#xA;  --subpath SUBPATH                                  Customize the URL subpath for usage with reverse proxy&#xA;  --backend {original,diffusers}                     force model pipeline type&#xA;  --allowed-paths ALLOWED_PATHS [ALLOWED_PATHS ...]  add additional paths to paths allowed for web access&#xA;&#xA;Setup options:&#xA;  --reset                                            Reset main repository to latest version, default: False&#xA;  --upgrade                                          Upgrade main repository to latest version, default: False&#xA;  --requirements                                     Force re-check of requirements, default: False&#xA;  --quick                                            Bypass version checks, default: False&#xA;  --use-directml                                     Use DirectML if no compatible GPU is detected, default: False&#xA;  --use-openvino                                     Use Intel OpenVINO backend, default: False&#xA;  --use-ipex                                         Force use Intel OneAPI XPU backend, default: False&#xA;  --use-cuda                                         Force use nVidia CUDA backend, default: False&#xA;  --use-rocm                                         Force use AMD ROCm backend, default: False&#xA;  --use-zluda                                        Force use ZLUDA, AMD GPUs only, default: False&#xA;  --use-xformers                                     Force use xFormers cross-optimization, default: False&#xA;  --skip-requirements                                Skips checking and installing requirements, default: False&#xA;  --skip-extensions                                  Skips running individual extension installers, default: False&#xA;  --skip-git                                         Skips running all GIT operations, default: False&#xA;  --skip-torch                                       Skips running Torch checks, default: False&#xA;  --skip-all                                         Skips running all checks, default: False&#xA;  --skip-env                                         Skips setting of env variables during startup, default: False&#xA;  --experimental                                     Allow unsupported versions of libraries, default: False&#xA;  --reinstall                                        Force reinstallation of all requirements, default: False&#xA;  --test                                             Run test only and exit&#xA;  --version                                          Print version information&#xA;  --ignore                                           Ignore any errors and attempt to continue&#xA;  --safe                                             Run in safe mode with no user extensions&#xA;&#xA;Logging options:&#xA;  --log LOG                                          Set log file, default: None&#xA;  --debug                                            Run installer with debug logging, default: False&#xA;  --profile                                          Run profiler, default: False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you don&#39;t want to use built-in &lt;code&gt;venv&lt;/code&gt; support and prefer to run SD.Next in your own environment such as &lt;em&gt;Docker&lt;/em&gt; container, &lt;em&gt;Conda&lt;/em&gt; environment or any other virtual environment, you can skip &lt;code&gt;venv&lt;/code&gt; create/activate and launch SD.Next directly using &lt;code&gt;python launch.py&lt;/code&gt; (command line flags noted above still apply).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Control&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SD.Next&lt;/strong&gt; comes with built-in control for all types of text2image, image2image, video2video and batch processing&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Control interface&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-control.jpg&#34; alt=&#34;Screenshot-Control&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Control processors&lt;/em&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-processors.jpg&#34; alt=&#34;Screenshot-Process&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Masking&lt;/em&gt;: &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/screenshot-mask.jpg&#34; alt=&#34;Screenshot-Mask&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Extensions&lt;/h3&gt; &#xA;&lt;p&gt;SD.Next comes with several extensions pre-installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/sd-extension-system-info&#34;&gt;System Info&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/sd-extension-chainner&#34;&gt;chaiNNer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/sd-extension-rembg&#34;&gt;RemBg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ArtVentureX/sd-webui-agent-scheduler&#34;&gt;Agent Scheduler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BinaryQuantumSoul/sdnext-modernui&#34;&gt;Modern UI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Collab&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We&#39;d love to have additional maintainers (with comes with full repo rights). If you&#39;re interested, ping us!&lt;/li&gt; &#xA; &lt;li&gt;In addition to general cross-platform code, desire is to have a lead for each of the main platforms&lt;br&gt; This should be fully cross-platform, but we&#39;d really love to have additional contributors and/or maintainers to join and help lead the efforts on different platforms&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Credits&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Main credit goes to &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Automatic1111 WebUI&lt;/a&gt; for original codebase&lt;/li&gt; &#xA; &lt;li&gt;Additional credits are listed in &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/#credits&#34;&gt;Credits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Licenses for modules are listed in &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/html/licenses.html&#34;&gt;Licenses&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evolution&lt;/h3&gt; &#xA;&lt;a href=&#34;https://star-history.com/#vladmandic/automatic&amp;amp;Date&#34;&gt; &#xA; &lt;picture width=&#34;640&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=vladmandic/automatic&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;img src=&#34;https://api.star-history.com/svg?repos=vladmandic/automatic&amp;amp;type=Date&#34; alt=&#34;starts&#34; width=&#34;320&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ossinsight.io/analyze/vladmandic/automatic#overview&#34;&gt;OSS Stats&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docs&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re unsure how to use a feature, best place to start is &lt;a href=&#34;https://github.com/vladmandic/automatic/wiki&#34;&gt;Wiki&lt;/a&gt; and if its not there,&lt;br&gt; check &lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;ChangeLog&lt;/a&gt; for when feature was first introduced as it will always have a short note on how to use it&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/automatic/wiki&#34;&gt;Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/README.md&#34;&gt;ReadMe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/TODO.md&#34;&gt;ToDo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/CHANGELOG.md&#34;&gt;ChangeLog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/automatic/master/cli/README.md&#34;&gt;CLI Tools&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Sponsors&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- sponsors --&gt;&#xA; &lt;a href=&#34;https://github.com/Tillerz&#34;&gt;&lt;img src=&#34;https://github.com/Tillerz.png&#34; width=&#34;60px&#34; alt=&#34;Tillerz&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/allangrant&#34;&gt;&lt;img src=&#34;https://github.com/allangrant.png&#34; width=&#34;60px&#34; alt=&#34;Allan Grant&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/BrentOzar&#34;&gt;&lt;img src=&#34;https://github.com/BrentOzar.png&#34; width=&#34;60px&#34; alt=&#34;Brent Ozar&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/inktomi&#34;&gt;&lt;img src=&#34;https://github.com/inktomi.png&#34; width=&#34;60px&#34; alt=&#34;Matthew Runo&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/4joeknight4&#34;&gt;&lt;img src=&#34;https://github.com/4joeknight4.png&#34; width=&#34;60px&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/SaladTechnologies&#34;&gt;&lt;img src=&#34;https://github.com/SaladTechnologies.png&#34; width=&#34;60px&#34; alt=&#34;Salad Technologies&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/mantzaris&#34;&gt;&lt;img src=&#34;https://github.com/mantzaris.png&#34; width=&#34;60px&#34; alt=&#34;a.v.mantzaris&#34;&gt;&lt;/a&gt;&#xA; &lt;a href=&#34;https://github.com/CurseWave&#34;&gt;&lt;img src=&#34;https://github.com/CurseWave.png&#34; width=&#34;60px&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&#xA; &lt;!-- sponsors --&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>octo-models/octo</title>
    <updated>2024-05-30T01:32:33Z</updated>
    <id>tag:github.com,2024-05-30:/octo-models/octo</id>
    <link href="https://github.com/octo-models/octo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Octo is a transformer-based robot policy trained on a diverse mix of 800k robot trajectories.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Octo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://githubtocolab.com/octo-models/octo/blob/main/examples/01_inference_pretrained.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://octo-models.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-a&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/rail-berkeley/octo/workflows/run-debug/badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://github.com/rail-berkeley/octo/workflows/pre-commit/badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains code for training and finetuning Octo generalist robotic policies (GRPs). Octo models are transformer-based diffusion policies, trained on a diverse mix of 800k robot trajectories.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Follow the installation instructions, then load a pretrained Octo model! See &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/&#34;&gt;examples&lt;/a&gt; for guides to zero-shot evaluation and finetuning and &lt;a href=&#34;https://colab.research.google.com/drive/1z0vELj_lX9OWeoMG_WvXnQs43aPOEAhz?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; for an inference example.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from octo.model.octo_model import OctoModel&#xA;model = OctoModel.load_pretrained(&#34;hf://rail-berkeley/octo-base-1.5&#34;)&#xA;print(model.get_pretty_spec())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/octo-models/octo/main/docs/assets/teaser.jpg&#34; alt=&#34;Octo model&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Out of the box, Octo supports multiple RGB camera inputs, can control various robot arms, and can be instructed via language commands or goal images. Octo uses a modular attention structure in its transformer backbone, allowing it to be effectively finetuned to robot setups with new sensory inputs, action spaces, and morphologies, using only a small target domain dataset and accessible compute budgets.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n octo python=3.10&#xA;conda activate octo&#xA;pip install -e .&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade &#34;jax[cuda11_pip]==0.4.20&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For TPU&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade &#34;jax[tpu]==0.4.20&#34; -f https://storage.googleapis.com/jax-releases/libtpu_releases.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/google/jax&#34;&gt;Jax Github page&lt;/a&gt; for more details on installing Jax.&lt;/p&gt; &#xA;&lt;p&gt;Test the installation by finetuning on the debug dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/finetune.py --config.pretrained_path=hf://rail-berkeley/octo-small-1.5 --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;You can find pretrained Octo checkpoints &lt;a href=&#34;https://huggingface.co/rail-berkeley&#34;&gt;here&lt;/a&gt;. At the moment we provide the following model versions:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Inference on 1x NVIDIA 4090&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/rail-berkeley/octo-base&#34;&gt;Octo-Base&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;13 it/sec&lt;/td&gt; &#xA;   &lt;td&gt;93M Params&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/rail-berkeley/octo-small&#34;&gt;Octo-Small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;17 it/sec&lt;/td&gt; &#xA;   &lt;td&gt;27M Params&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;We provide simple &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples&#34;&gt;example scripts&lt;/a&gt; that demonstrate how to use and finetune Octo models, as well as how to use our data loader independently. We provide the following examples:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/01_inference_pretrained.ipynb&#34;&gt;Octo Inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Minimal example for loading and running a pretrained Octo model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/02_finetune_new_observation_action.py&#34;&gt;Octo Finetuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Minimal example for finetuning a pretrained Octo models on a small dataset with a new observation and action space&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/03_eval_finetuned.py&#34;&gt;Octo Rollout&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Run a rollout of a pretrained Octo policy in a Gym environment&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/04_eval_finetuned_on_robot.py&#34;&gt;Octo Robot Eval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Evaluate a pretrained Octo model on a real WidowX robot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/05_dataloading.ipynb&#34;&gt;OpenX Dataloader Intro&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Walkthrough of the features of our Open X-Embodiment data loader&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/06_pytorch_oxe_dataloader.ipynb&#34;&gt;OpenX PyTorch Dataloader&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Standalone Open X-Embodiment data loader in PyTorch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Octo Pretraining&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce our Octo pretraining on 800k robot trajectories, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/train.py --config scripts/configs/octo_pretrain_config.py:&amp;lt;size&amp;gt; --name=octo --config.dataset_kwargs.oxe_kwargs.data_dir=... --config.dataset_kwargs.oxe_kwargs.data_mix=oxe_magic_soup ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download the pretraining dataset from the &lt;a href=&#34;https://robotics-transformer-x.github.io/&#34;&gt;Open X-Embodiment Dataset&lt;/a&gt;, install the &lt;a href=&#34;https://github.com/kpertsch/rlds_dataset_mod&#34;&gt;rlds_dataset_mod package&lt;/a&gt; and run the &lt;a href=&#34;https://github.com/kpertsch/rlds_dataset_mod/raw/main/prepare_open_x.sh&#34;&gt;prepare_open_x.sh script&lt;/a&gt;. The total size of the pre-processed dataset is ~1.2TB.&lt;/p&gt; &#xA;&lt;p&gt;We run pretraining using a TPUv4-128 pod in 8 hours for the Octo-S model and in 14 hours for Octo-B.&lt;/p&gt; &#xA;&lt;h2&gt;Octo Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/02_finetune_new_observation_action.py&#34;&gt;minimal example&lt;/a&gt; for finetuning with a new observation and action space.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a more advanced finetuning script that allows you to change hyperparameters via a config file and logs finetuning metrics. To run advanced finetuning, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/finetune.py --config.pretrained_path=hf://rail-berkeley/octo-small-1.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We offer three finetuning modes depending on the parts of the model that are kept frozen: &lt;code&gt;head_only&lt;/code&gt;, &lt;code&gt;head_mlp_only&lt;/code&gt;, and &lt;code&gt;full&lt;/code&gt; to finetune the full model. Additionally, one can specify the task type to finetune with: &lt;code&gt;image_conditioned&lt;/code&gt;, &lt;code&gt;language_conditioned&lt;/code&gt; or &lt;code&gt;multimodal&lt;/code&gt; for both. For example, to finetune the full transformer with image inputs only use: &lt;code&gt;--config=finetune_config.py:full,image_conditioned&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Octo Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Loading and running a trained Octo model is as easy as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from octo.model import OctoModel&#xA;&#xA;model = OctoModel.load_pretrained(&#34;hf://rail-berkeley/octo-small-1.5&#34;)&#xA;task = model.create_tasks(texts=[&#34;pick up the spoon&#34;])&#xA;action = model.sample_actions(observation, task, rng=jax.random.PRNGKey(0))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide examples for evaluating Octo &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/03_eval_finetuned.py&#34;&gt;in a simulated Gym environment&lt;/a&gt; as well as &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/04_eval_finetuned_on_robot.py&#34;&gt;on a real WidowX robot&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To evaluate on your own environment, simply wrap it in a Gym interface and follow the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/examples/envs/README.md&#34;&gt;Eval Env README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;File&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hyperparameters&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/scripts/configs/config.py&#34;&gt;config.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Defines all hyperparameters for the training run.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pretraining Loop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/scripts/train.py&#34;&gt;train.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main pretraining script.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Finetuning Loop&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/scripts/finetune.py&#34;&gt;finetune.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main finetuning script.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Datasets&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/octo/data/dataset.py&#34;&gt;dataset.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Functions for creating single / interleaved datasets + data augmentation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tokenizers&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/octo/model/components/tokenizers.py&#34;&gt;tokenizers.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tokenizers that encode image / text inputs into tokens.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Octo Model&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/octo/model/octo_model.py&#34;&gt;octo_model.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main entry point for interacting with Octo models: loading, saving, and inference.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Model Architecture&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/octo/model/octo_module.py&#34;&gt;octo_module.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Combines token sequencing, transformer backbone and readout heads.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Visualization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/octo/utils/visualization_lib.py&#34;&gt;visualization_lib.py&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Utilities for offline qualitative &amp;amp; quantitative eval.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;What is the &lt;code&gt;timestep_pad_mask&lt;/code&gt; in the observation dictionary?&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;timestep_pad_mask&lt;/code&gt; indicates which observations should be attended to, which is important when using multiple timesteps of observation history. Octo was trained with a history window size of 2, meaning the model can predict an action using both the current observation and the previous observation. However, at the very beginning of the trajectory, there is no previous observation, so we need to set &lt;code&gt;timestep_pad_mask=False&lt;/code&gt; at the corresponding index. If you use Octo with a window size of 1, &lt;code&gt;timestep_pad_mask&lt;/code&gt; should always just be &lt;code&gt;[True]&lt;/code&gt;, indicating that the one and only observation in the window should be attended to. Note that if you wrap your robot environment with the &lt;code&gt;HistoryWrapper&lt;/code&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/octo/utils/gym_wrappers.py&#34;&gt;gym_wrappers.py&lt;/a&gt;), the &lt;code&gt;timestep_pad_mask&lt;/code&gt; key will be added to the observation dictionary for you.&lt;/p&gt; &#xA;&lt;h4&gt;What is &lt;code&gt;pad_mask_dict&lt;/code&gt; in the observation dictionary?&lt;/h4&gt; &#xA;&lt;p&gt;While &lt;code&gt;timestep_pad_mask&lt;/code&gt; indicates which observations should be attended to on a timestep level, &lt;code&gt;pad_mask_dict&lt;/code&gt; indicates which elements of the observation should be attended to within a single timestep. For example, for datasets without language labels, &lt;code&gt;pad_mask_dict[&#34;language_instruction&#34;]&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;. For datasets without a wrist camera, &lt;code&gt;pad_mask_dict[&#34;image_wrist&#34;]&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;. For convenience, if a key is missing from the observation dict, it is equivalent to setting &lt;code&gt;pad_mask_dict&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; for that key.&lt;/p&gt; &#xA;&lt;h4&gt;Does &lt;code&gt;model.sample_actions([...])&lt;/code&gt; return the full trajectory to solve a task?&lt;/h4&gt; &#xA;&lt;p&gt;Octo was pretrained with an action chunking size of 4, meaning it predicts the next 4 actions at once. You can choose to execute all these actions before sampling new ones, or only execute the first action before sampling new ones (also known as receding horizon control). You can also do something more advanced like &lt;a href=&#34;https://raw.githubusercontent.com/octo-models/octo/main/octo/utils/gym_wrappers.py&#34;&gt;temporal ensembling&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Updates for Version 1.5&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improved cross-attention between visual and language tokens by repeating language tokens at every timestep in the context window.&lt;/li&gt; &#xA; &lt;li&gt;Augmented the language instructions in the data with rephrasings from GPT-3.5.&lt;/li&gt; &#xA; &lt;li&gt;Bug fixes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Turned off dropout in the diffusion head due to incompatibility with layer norm.&lt;/li&gt; &#xA;   &lt;li&gt;Fixed an off-by-one error with the attention mask.&lt;/li&gt; &#xA;   &lt;li&gt;Fixed an issue where different image augmentations did not get fresh random seeds.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{octo_2023,&#xA;    title={Octo: An Open-Source Generalist Robot Policy},&#xA;    author = {{Octo Model Team} and Dibya Ghosh and Homer Walke and Karl Pertsch and Kevin Black and Oier Mees and Sudeep Dasari and Joey Hejna and Charles Xu and Jianlan Luo and Tobias Kreiman and {You Liang} Tan and Pannag Sanketi and Quan Vuong and Ted Xiao and Dorsa Sadigh and Chelsea Finn and Sergey Levine},&#xA;    booktitle = {Proceedings of Robotics: Science and Systems},&#xA;    address  = {Delft, Netherlands},&#xA;    year = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>