<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-20T01:36:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>qiuyu96/CoDeF</title>
    <updated>2023-08-20T01:36:51Z</updated>
    <id>tag:github.com,2023-08-20:/qiuyu96/CoDeF</id>
    <link href="https://github.com/qiuyu96/CoDeF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official PyTorch implementation of CoDeF: Content Deformation Fields for Temporally Consistent Video Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CoDeF: Content Deformation Fields for Temporally Consistent Video Processing&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/qiuyu96/CoDeF/main/docs/teaser.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ken-ouyang.github.io/&#34;&gt;Hao Ouyang&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qiuyu96/&#34;&gt;Qiuyu Wang&lt;/a&gt;*, &lt;a href=&#34;https://henry123-boy.github.io/&#34;&gt;Yuxi Xiao&lt;/a&gt;*, &lt;a href=&#34;https://scholar.google.com/citations?user=xUMjxi4AAAAJ&amp;amp;hl=en&#34;&gt;Qingyan Bai&lt;/a&gt;, &lt;a href=&#34;https://github.com/JordanZh&#34;&gt;Juntao Zhang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=hMDQifQAAAAJ&#34;&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href=&#34;https://xzhou.me/&#34;&gt;Xiaowei Zhou&lt;/a&gt;, &lt;a href=&#34;https://cqf.io/&#34;&gt;Qifeng Chen&lt;/a&gt;‚Ä†, &lt;a href=&#34;https://shenyujun.github.io/&#34;&gt;Yujun Shen&lt;/a&gt;‚Ä† (*equal contribution, ‚Ä†corresponding author)&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://qiuyu96.github.io/CoDeF/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2308.07926&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://ezioby.github.io/CoDeF_Demo/&#34;&gt;High-Res Translation Demo&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;!-- Abstract: *This work presents the content deformation field **CoDeF** as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We also introduce some decent regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video. With such a design, **CoDeF** naturally supports lifting image algorithms to videos, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field. We experimentally show that **CoDeF** is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in translated videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog.* --&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The codebase is tested on&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 20.04&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; 2.0.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pytorchlightning.ai/index.html&#34;&gt;PyTorch Lightning&lt;/a&gt; 2.0.2&lt;/li&gt; &#xA; &lt;li&gt;1 Nvidia GPU (RTX A6000 48GB) with CUDA version 11.7 (Other GPUs are also suitable, and 10GB of GPU memory is sufficient to run our code.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use video visualizer, please install &lt;code&gt;ffmpeg&lt;/code&gt; by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional Python libraries, please install by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our code also depends on &lt;a href=&#34;https://github.com/NVlabs/tiny-cuda-nn&#34;&gt;tiny-cuda-nn&lt;/a&gt;. See &lt;a href=&#34;https://github.com/NVlabs/tiny-cuda-nn#pytorch-extension&#34;&gt;this&lt;/a&gt; for Pytorch extension install instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;h3&gt;Our data&lt;/h3&gt; &#xA;&lt;p&gt;Download our data from &lt;a href=&#34;https://drive.google.com/file/d/1cKZF6ILeokCjsSAGBmummcQh0uRGaC_F/view?usp=sharing&#34;&gt;this URL&lt;/a&gt;, unzip the file and put it in the current directory. Some additional data can be downloaded from &lt;a href=&#34;https://rec.ustc.edu.cn/share/5d1e0bb0-31d7-11ee-aa60-d1fd6c62dfb4&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Customize your own data&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;To be released.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;And organize files as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CoDeF&#xA;‚îÇ&#xA;‚îî‚îÄ‚îÄ‚îÄ all_sequences&#xA;    ‚îÇ&#xA;    ‚îî‚îÄ‚îÄ‚îÄ NAME1&#xA;           ‚îî‚îÄ NAME1&#xA;           ‚îî‚îÄ NAME1_masks_0 (optional)&#xA;           ‚îî‚îÄ NAME1_masks_1 (optional)&#xA;           ‚îî‚îÄ NAME1_flow (optional)&#xA;           ‚îî‚îÄ NAME1_flow_confidence (optional)&#xA;    ‚îÇ&#xA;    ‚îî‚îÄ‚îÄ‚îÄ NAME2&#xA;           ‚îî‚îÄ NAME2&#xA;           ‚îî‚îÄ NAME2_masks_0 (optional)&#xA;           ‚îî‚îÄ NAME2_masks_1 (optional)&#xA;           ‚îî‚îÄ NAME2_flow (optional)&#xA;           ‚îî‚îÄ NAME2_flow_confidence (optional)&#xA;    ‚îÇ&#xA;    ‚îî‚îÄ‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretrained checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;You can download the pre-trained checkpoints trained with the current codebase as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Sequence Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;beauty_0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;configs/beauty_0/base.yaml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/11SWfnfDct8bE16802PyqYJqsU4x6ACn8/view?usp=sharing&#34;&gt;Google drive link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;beauty_1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;configs/beauty_1/base.yaml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1bSK0ChbPdURWGLdtc9CPLkN4Tfnng51k/view?usp=sharing&#34;&gt;Google drive link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;white_smoke&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;configs/white_smoke/base.yaml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1QOBCDGV2hHwxq4eL1E_45z5zhZ-wTJR7/view?usp=sharing&#34;&gt;Google drive link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;lemon_hit&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;configs/lemon_hit/base.yaml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/140ctcLbv7JTIiy53MuCYtI4_zpIvRXzq/view?usp=sharing&#34;&gt;Google drive link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;scene_0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;configs/scene_0/base.yaml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1abOdREarfw1DGscahOJd2gZf1Xn_zN-F/view?usp=sharing&#34;&gt;Google drive link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;And organize files as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CoDeF&#xA;‚îÇ&#xA;‚îî‚îÄ‚îÄ‚îÄ ckpts/all_sequences&#xA;    ‚îÇ&#xA;    ‚îî‚îÄ‚îÄ‚îÄ NAME1&#xA;        ‚îÇ&#xA;        ‚îî‚îÄ‚îÄ‚îÄ EXP_NAME (base)&#xA;            ‚îÇ&#xA;            ‚îî‚îÄ‚îÄ‚îÄ NAME1.ckpt&#xA;    ‚îÇ&#xA;    ‚îî‚îÄ‚îÄ‚îÄ NAME2&#xA;        ‚îÇ&#xA;        ‚îî‚îÄ‚îÄ‚îÄ EXP_NAME (base)&#xA;            ‚îÇ&#xA;            ‚îî‚îÄ‚îÄ‚îÄ NAME2.ckpt&#xA;    |&#xA;    ‚îî‚îÄ‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train a new model&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/train_multi.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;GPU&lt;/code&gt;: Decide which GPU to train on;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NAME&lt;/code&gt;: Name of the video sequence;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;EXP_NAME&lt;/code&gt;: Name of the experiment;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ROOT_DIRECTORY&lt;/code&gt;: Directory of the input video sequence;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MODEL_SAVE_PATH&lt;/code&gt;: Path to save the checkpoints;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LOG_SAVE_PATH&lt;/code&gt;: Path to save the logs;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MASK_DIRECTORY&lt;/code&gt;: Directory of the preprocessed masks (optional);&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FLOW_DIRECTORY&lt;/code&gt;: Directory of the preprocessed optical flows (optional);&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please check configuration files in &lt;code&gt;configs/&lt;/code&gt;, and you can always add your own model config.&lt;/p&gt; &#xA;&lt;h2&gt;Test reconstruction &lt;a id=&#34;anchor&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/test_multi.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the script, the reconstructed videos can be found in &lt;code&gt;results/all_sequences/{NAME}/{EXP_NAME}&lt;/code&gt;, along with the canonical image.&lt;/p&gt; &#xA;&lt;h2&gt;Test video translation&lt;/h2&gt; &#xA;&lt;p&gt;After obtaining the canonical image through &lt;a href=&#34;https://raw.githubusercontent.com/qiuyu96/CoDeF/main/#anchor&#34;&gt;this step&lt;/a&gt;, use your preferred text prompts to transfer it using &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;. Once you have the transferred canonical image, place it in &lt;code&gt;all_sequences/${NAME}/${EXP_NAME}_control&lt;/code&gt; (i.e. &lt;code&gt;CANONICAL_DIR&lt;/code&gt; in &lt;code&gt;scripts/test_canonical.sh&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/test_canonical.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The transferred results can be seen in &lt;code&gt;results/all_sequences/{NAME}/{EXP_NAME}_transformed&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The &lt;code&gt;canonical_wh&lt;/code&gt; option in the configuration file should be set with caution, usually a little larger than &lt;code&gt;img_wh&lt;/code&gt;, as it determines the field of view of the canonical image.&lt;/p&gt; &#xA;&lt;h3&gt;BibTeX&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{ouyang2023codef,&#xA;      title={CoDeF: Content Deformation Fields for Temporally Consistent Video Processing}, &#xA;      author={Hao Ouyang and Qiuyu Wang and Yuxi Xiao and Qingyan Bai and Juntao Zhang and Kecheng Zheng and Xiaowei Zhou and Qifeng Chen and Yujun Shen},&#xA;      journal={arXiv preprint arXiv:2308.07926},&#xA;      year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>arielnlee/Platypus</title>
    <updated>2023-08-20T01:36:51Z</updated>
    <id>tag:github.com,2023-08-20:/arielnlee/Platypus</id>
    <link href="https://github.com/arielnlee/Platypus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for fine-tuning Platypus fam LLMs using LoRA&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Platypus: Quick, Cheap, and Powerful Refinement of LLMs (&lt;a href=&#34;https://platypus-llm.github.io&#34;&gt;https://platypus-llm.github.io&lt;/a&gt;)&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/arielnlee/Platypus/main/assets/Best_Platty.png&#34; alt=&#34;Platypus&#34; width=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The Platypus models are a series of fine-tuned and merged variants based on the LLaMA and LLaMa-2 transformer architectures. Platypus takes advantage of &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;LoRA&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All models and dataset available via HuggingFace: &lt;a href=&#34;https://huggingface.co/garage-bAInd&#34;&gt;&lt;code&gt;garage-bAInd&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;8/14/23&lt;/strong&gt;: We have cleaned up our pipeline and added data refinement and similarity code. Within in the next few days we&#39;ll have a script to reproduce our exact dataset from 11 open-source datasets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;8/13/23&lt;/strong&gt;: An unquantized GPU chatbot of OpenOrca-Platypus2-13B, our most recent collab, is available via Hugging Face spaces, courtesy of OpenOrca: &lt;a href=&#34;https://huggingface.co/spaces/Open-Orca/OpenOrca-Platypus2-13B&#34;&gt;Chat now!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/arielnlee/Platypus/main/assets/orca_platty.jpeg&#34; alt=&#34;Platypus&#34; width=&#34;120&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;8/11/23&lt;/strong&gt;: Our &lt;a href=&#34;https://arxiv.org/abs/2308.07317&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://platypus-llm.github.io&#34;&gt;project website&lt;/a&gt; have been released!&lt;/p&gt; &#xA;&lt;h2&gt;CLI&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Fastchat&lt;/a&gt; provides a simple setup for those interested in running the model. Afrer downloading the model through HuggingFace, clone the Fastchat repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lm-sys/FastChat.git&#xA;cd FastChat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the required packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install --upgrade pip  # enable PEP 660 support&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m fastchat.serve.cli --model-path garage-bAInd/Platypus-30B --conv_template alpaca&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Local Setup&lt;/h2&gt; &#xA;&lt;p&gt;This repository is multi-GPU friendly, and provides code to use model or data parellelism, depending on your computational resources.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Be sure to use these exact requirements or you may run into model saving or OOM issues.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Fine-tuning (&lt;code&gt;finetune.py&lt;/code&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;fine-tuning.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note: The script above uses &lt;code&gt;torchrun&lt;/code&gt; for data parallelism. PyTorch is not in &lt;code&gt;requirements.txt&lt;/code&gt; since technically you can run fine-tuning without it (after a few minor changes to the .py file). To use &lt;code&gt;fine-tuning.sh&lt;/code&gt;, please install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;. We recommend using &lt;code&gt;torchrun&lt;/code&gt; and PyTorch 2.0+ for speed + &lt;code&gt;torch.compile&lt;/code&gt;. If you do not install pytorch, or use an alternative method like &lt;code&gt;accelerate launch&lt;/code&gt;, please take time to comment out any torch related lines in the scirpts.&lt;/p&gt; &#xA;&lt;p&gt;Hyperparameters used to fine-tune Platypus:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;Value 13B / 70B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;learning rate&lt;/td&gt; &#xA;   &lt;td&gt;4e-4 / 3e-4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;batch size&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;microbatch size&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;warmup steps&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;epochs&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;weight decay&lt;/td&gt; &#xA;   &lt;td&gt;0.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lr scheduler&lt;/td&gt; &#xA;   &lt;td&gt;cosine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lora alpha&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lora rank&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lora dropout&lt;/td&gt; &#xA;   &lt;td&gt;0.05&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lora target modules&lt;/td&gt; &#xA;   &lt;td&gt;gate_proj, up_proj, down_proj&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cutoff length&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;train on inputs&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;group by length&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;add eos token&lt;/td&gt; &#xA;   &lt;td&gt;False&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Example for how to calcualte gradient accumulation steps using 2 GPUs: = global_batch_size / micro_batch_size / num_gpus = 16 / 1 / 2 = 8.&lt;/p&gt; &#xA;&lt;p&gt;If your model &lt;strong&gt;cannot&lt;/strong&gt; fit on the memory of each GPU, please use the alternative fine-tuning option below (or utilize accelerate, FDSP, etc.) to take advantage of model parallelism. A good alternative to torchrun is accelerate.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune.py \&#xA;    --base_model meta-llama/Llama-2-70b-hf \&#xA;    --data-path ./final_data.json \&#xA;    --output_dir ./llama2-platypus-70b \&#xA;    --batch_size 16 \&#xA;    --micro_batch_size 1 \&#xA;    --num_epochs 1 \&#xA;    --learning_rate 0.0003 \&#xA;    --cutoff_len 4096 \&#xA;    --val_set_size 0 \&#xA;    --lora_r 16 \&#xA;    --lora_alpha 16 \&#xA;    --lora_dropout 0.05 \&#xA;    --lora_target_modules &#39;[gate_proj, down_proj, up_proj]&#39; \&#xA;    --train_on_inputs False \&#xA;    --add_eos_token False \&#xA;    --group_by_length False \&#xA;    --prompt_template_name alpaca \&#xA;    --lr_scheduler &#39;cosine&#39; \&#xA;    --warmup_steps 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Merging&lt;/h2&gt; &#xA;&lt;p&gt;Once you&#39;ve completed a fine-tuning, use &lt;code&gt;merge.sh&lt;/code&gt; to merge the LoRA weights back into the base LLaMa model (or base model of your choice) for export to HuggingFace format.&lt;/p&gt; &#xA;&lt;p&gt;While we are experimenting on better and alternative ways to merge (stay tuned!), our current merging process relies on the basic linear merge provided by PEFT. Before we fine-tune, we search for possible models to merge with and the datasets used to create them (to the best of our ability). The success of our LoRA merges stems from using the right data. Our most successful merges have little to no overlap in fine-tuning data. For example, GPlatty-30B is a merge of Platypus-30B and gpt4-alpaca-lora-30b. We saw a 2% jump in accuracy for GPlatty, and the datasets used to fine-tune the aforementioned two LoRA-based models had very low similarity scores. Please see &lt;a href=&#34;https://arxiv.org/abs/2308.07317&#34;&gt;our paper&lt;/a&gt; for additional information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you encounter any errors while merging, please try uninstalling bitsandbytes and peft, then reinstalling with the newest versions (peft should always be installed from source).&lt;/p&gt; &#xA;&lt;h2&gt;Dataset Refinement&lt;/h2&gt; &#xA;&lt;p&gt;We used keyword search to find STEM and logic questions in the 11 open-source datasets that make up &lt;a href=&#34;https://huggingface.co/datasets/garage-bAInd/Open-Platypus&#34;&gt;Open-Platypus&lt;/a&gt;. Then, to remove duplicates and redundancy, we perform a cosine similarity check of the questions using SentenceTransformers embeddings. Lastly, we do a similarity check to remove any questions from our training set that are too similiar to the test set.&lt;/p&gt; &#xA;&lt;p&gt;You can access all of the related code in the &lt;code&gt;data_pipeline&lt;/code&gt; folder of this repo.&lt;/p&gt; &#xA;&lt;h2&gt;Reproducing Benchmark Eval Results&lt;/h2&gt; &#xA;&lt;p&gt;Install LM Evaluation Harness:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/EleutherAI/lm-evaluation-harness&#xA;cd lm-evaluation-harness&#xA;git checkout b281b0921b636bc36ad05c0b0b0763bd6dd43463 # The commit used by the Open LLM Leaderboard&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each task was evaluated on a single A100 80GB GPU for 13B, and 2 A100s for 70B.&lt;/p&gt; &#xA;&lt;p&gt;ARC:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --model hf-causal-experimental --model_args pretrained=garage-bAInd/Platypus-13B,use_accelerate=True --tasks arc_challenge --batch_size 2 --no_cache --write_out --output_path results/Platypus-13B/arc_challenge_25shot.json --device cuda --num_fewshot 25&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;HellaSwag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --model hf-causal-experimental --model_args pretrained=garage-bAInd/Platypus-13B,use_accelerate=True --tasks hellaswag --batch_size 2 --no_cache --write_out --output_path results/Platypus-13B/hellaswag_10shot.json --device cuda --num_fewshot 10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;MMLU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --model hf-causal-experimental --model_args pretrained=garage-bAInd/Platypus-13B,use_accelerate=True --tasks hendrycksTest-* --batch_size 2 --no_cache --write_out --output_path results/Platypus-13B/mmlu_5shot.json --device cuda --num_fewshot 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;TruthfulQA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --model hf-causal-experimental --model_args pretrained=garage-bAInd/Platypus-13B,use_accelerate=True --tasks truthfulqa_mc --batch_size 2 --no_cache --write_out --output_path results/Platypus-13B/truthfulqa_0shot.json --device cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference for Adapters (&lt;code&gt;inference.py&lt;/code&gt;)&lt;/h2&gt; &#xA;&lt;p&gt;This a basic example script for running inference directly using fine-tuned adapters and/or local data. The current version reads data from a csv file. You can easily edit this to pull from HF or use a json file. Please make any necessary edits before using this script (it assumes alpaca formatting).&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{platypus2023,&#xA;    title={Platypus: Quick, Cheap, and Powerful Refinement of LLMs}, &#xA;    author={Ariel N. Lee and Cole J. Hunter and Nataniel Ruiz},&#xA;    booktitle={arXiv preprint arxiv:2308.07317},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>silvanmelchior/IncognitoPilot</title>
    <updated>2023-08-20T01:36:51Z</updated>
    <id>tag:github.com,2023-08-20:/silvanmelchior/IncognitoPilot</id>
    <link href="https://github.com/silvanmelchior/IncognitoPilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI code interpreter for sensitive data, powered by GPT-4 or Llama 2.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/silvanmelchior/IncognitoPilot/raw/main/docs/title.png&#34; alt=&#34;logo&#34; style=&#34;width: 75%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;em&gt;An AI code interpreter for sensitive data, powered by GPT-4 or Llama 2.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Incognito Pilot&lt;/strong&gt; combines a Large Language Model (LLM) with a Python interpreter, so it can run code and execute tasks for you. It is similar to &lt;strong&gt;ChatGPT Code Interpreter&lt;/strong&gt;, but the interpreter runs &lt;strong&gt;locally&lt;/strong&gt; and it can use open-source models like &lt;strong&gt;Llama 2&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Incognito Pilot&lt;/strong&gt; allows you to work with &lt;strong&gt;sensitive data&lt;/strong&gt; without uploading it to the cloud. Either you use a local LLM (like Llama 2), or an API (like GPT-4). For the latter case, there is an &lt;strong&gt;approval mechanism&lt;/strong&gt; in the UI, which separates your local data from the remote services.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;strong&gt;Incognito Pilot&lt;/strong&gt;, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; analyse data and create visualizations&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; convert your files, e.g. a video to a gif&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;‚úÖ&lt;/span&gt; &lt;strong&gt;access the internet&lt;/strong&gt; to e.g. download data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;and much more!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üí°&lt;/span&gt; Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/silvanmelchior/IncognitoPilot/assets/6033305/05b0a874-6f76-4d22-afca-36c11f90b1ff&#34;&gt;https://github.com/silvanmelchior/IncognitoPilot/assets/6033305/05b0a874-6f76-4d22-afca-36c11f90b1ff&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The video shows Incognito Pilot with GPT-4. While your conversation and approved code results are sent to OpenAI&#39;s API, your &lt;strong&gt;data is kept locally&lt;/strong&gt; on your machine. The interpreter is running locally as well and processes your data right there. And you can go even further and use Llama 2 to have everything running on your machine.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üì¶&lt;/span&gt; Installation (GPT via OpenAI API)&lt;/h2&gt; &#xA;&lt;p&gt;This section shows how to install &lt;strong&gt;Incognito Pilot&lt;/strong&gt; using a GPT model via OpenAI&#39;s API. For&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama 2&lt;/strong&gt;, check &lt;a href=&#34;https://raw.githubusercontent.com/silvanmelchior/IncognitoPilot/main/docs/INSTALLATION_LLAMA.md&#34;&gt;Installation for Llama 2&lt;/a&gt; instead, and for&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GPT on Azure&lt;/strong&gt;, check &lt;a href=&#34;https://raw.githubusercontent.com/silvanmelchior/IncognitoPilot/main/docs/INSTALLATION_AZURE.md&#34;&gt;Installation with Azure&lt;/a&gt; instead.&lt;/li&gt; &#xA; &lt;li&gt;If you don&#39;t have docker, you can install &lt;strong&gt;Incognito Pilot&lt;/strong&gt; on your system directly, using the development setup (see below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.docker.com/&#34;&gt;docker&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create an empty folder somewhere on your system. This will be the working directory to which &lt;strong&gt;Incognito Pilot&lt;/strong&gt; has access to. The code interpreter can read your files in this folder and store any results. In the following, we assume it to be &lt;em&gt;/home/user/ipilot&lt;/em&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create an &lt;a href=&#34;https://platform.openai.com&#34;&gt;OpenAI account&lt;/a&gt;, add a &lt;a href=&#34;https://platform.openai.com/account/billing/payment-methods&#34;&gt;credit card&lt;/a&gt; and create an &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;API key&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Now, just run the following command (replace your working directory and API key):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -i -t \&#xA;  -p 3030:80 \&#xA;  -e OPENAI_API_KEY=&#34;sk-your-api-key&#34; \&#xA;  -v /home/user/ipilot:/mnt/data \&#xA;  silvanmelchior/incognito-pilot:latest-slim&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now visit &lt;a href=&#34;http://localhost:3030&#34;&gt;http://localhost:3030&lt;/a&gt; and should see the &lt;strong&gt;Incognito Pilot&lt;/strong&gt; interface.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s also possible to run &lt;strong&gt;Incognito Pilot&lt;/strong&gt; with the free trial credits of OpenAI, without adding a credit card. At the moment, this does not include GPT-4 however, so see below how to change the model to GPT-3.5.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Getting started (GPT)&lt;/h2&gt; &#xA;&lt;p&gt;In the &lt;strong&gt;Incognito Pilot&lt;/strong&gt; interface, you will see a chat interface, with which you can interact with the model. Let&#39;s try it out!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Greetings&lt;/strong&gt;: Type &#34;Hi&#34; and see how the model responds to you.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hello World&lt;/strong&gt;: Type &#34;Print a hello world message for me&#34;. You will see how the &lt;em&gt;Code&lt;/em&gt; part of the UI shows you a Python snippet. As soon as you approve, the code will be executed on your machine (within the docker container). You will see the result in the &lt;em&gt;Result&lt;/em&gt; part of the UI. As soon as you approve it, it will be sent back to the model. In the case of using an API like here OpenAI&#39;s GPT models, this of course also means that this result will be sent to their services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;File Access&lt;/strong&gt;: Type &#34;Create a text file with all numbers from 0 to 100&#34;. After the approval, the model will confirm you the execution. Check your working directory now (e.g. &lt;em&gt;/home/user/ipilot&lt;/em&gt;): You should see the file!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now you should be ready to use &lt;strong&gt;Incognito Pilot&lt;/strong&gt; for your own tasks. Just remember:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Everything you type or every code result you approve is sent to the OpenAI / Azure API&lt;/li&gt; &#xA; &lt;li&gt;Your data stays and is processed locally&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;One more thing: The version you just used has nearly no packages shipped with the Python interpreter. This means, things like reading images or Excel files will not work. To change this, head back to the console and press Ctrl-C to stop the container. Now re-run the command, but remove the &lt;code&gt;-slim&lt;/code&gt; suffix from the image. This will download a much larger version, equipped with &lt;a href=&#34;https://raw.githubusercontent.com/silvanmelchior/IncognitoPilot/main/docker/requirements_full.txt&#34;&gt;many packages&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Change model&lt;/h3&gt; &#xA;&lt;p&gt;To use another model than the default one (GPT-4), set the environment variable &lt;code&gt;LLM&lt;/code&gt;. OpenAI&#39;s GPT models have the prefix &lt;code&gt;gpt:&lt;/code&gt;, so to use GPT-3.5 for example (the original ChatGPT), add the following to the docker run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;-e LLM=&#34;gpt:gpt-3.5-turbo&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that GPT-4 is considerably better in the interpreter setup than GPT-3.5.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚öô&lt;/span&gt; Settings&lt;/h2&gt; &#xA;&lt;h3&gt;Change port&lt;/h3&gt; &#xA;&lt;p&gt;To serve the UI at a different port than 3030, just expose the internal port 80 to a different one, for example 8080:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -i -t \&#xA;  -p 8080:80 \&#xA;  ... \&#xA;  silvanmelchior/incognito-pilot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Timeout&lt;/h3&gt; &#xA;&lt;p&gt;Per default, the Python interpreter stops after 30 seconds. To change this, set the environment variable &lt;code&gt;INTERPRETER_TIMEOUT&lt;/code&gt;. For 2 minutes for example, add the following to the docker run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;-e INTERPRETER_TIMEOUT=&#34;120&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Autostart&lt;/h3&gt; &#xA;&lt;p&gt;To automatically start &lt;strong&gt;Incognito Pilot&lt;/strong&gt; with docker / at startup, remove the &lt;code&gt;-i -t&lt;/code&gt; from the run command and add the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;--restart always&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Together with a bookmark of the UI URL, you&#39;ll have &lt;strong&gt;Incognito Pilot&lt;/strong&gt; at your fingertips whenever you need it.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üß∞&lt;/span&gt; Own dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Not happy with the pre-installed packages of the full (aka non-slim) version? Want to add more Python (or Debian) packages to the interpreter?&lt;/p&gt; &#xA;&lt;p&gt;You can easily containerize your own dependencies with &lt;strong&gt;Incognito Pilot&lt;/strong&gt;. To do so, create a Dockerfile like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;FROM silvanmelchior/incognito-pilot:latest-slim&#xA;SHELL [&#34;/bin/bash&#34;, &#34;-c&#34;]&#xA;&#xA;# uncomment the following line, if you want to install more packages&#xA;# RUN apt update &amp;amp;&amp;amp; apt install -y some-package&#xA;&#xA;WORKDIR /opt/app&#xA;&#xA;COPY requirements.txt .&#xA;&#xA;RUN source venv_interpreter/bin/activate &amp;amp;&amp;amp; \&#xA;    pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Put your dependencies into a &lt;em&gt;requirements.txt&lt;/em&gt; file and run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker build --tag incognito-pilot-custom .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the container like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -i -t \&#xA;  ... \&#xA;  incognito-pilot-custom&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ùì&lt;/span&gt; FAQs&lt;/h2&gt; &#xA;&lt;h3&gt;Is it as good as ChatGPT Code Interpreter?&lt;/h3&gt; &#xA;&lt;p&gt;No, it has its limits. The tradeoff between privacy and capabilities is not an easy one in this case. For things like images, it is as powerful as ChatGPT code interpreter, because it doesn&#39;t need to know about the content of the image to edit it. But for things like spreadsheets, if ChatGPT doesn&#39;t see the content, it has to guess for example the data format from the header, which can go wrong.&lt;/p&gt; &#xA;&lt;p&gt;However, in certain aspects, it&#39;s even better than ChatGPT code interpreter: The interpreter has internet access, allowing for a bunch of new tasks which were not possible before. Also, you can run the interpreter on any machine, including very powerful ones, so you can solve much larger tasks than with ChatGPT code interpreter.&lt;/p&gt; &#xA;&lt;h3&gt;Why not just use ChatGPT to generate the code and run it myself?&lt;/h3&gt; &#xA;&lt;p&gt;You can of course do this. There are quite some advantages of using &lt;strong&gt;Incognito Pilot&lt;/strong&gt; however:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Incognito Pilot can run code in multiple rounds (e.g. first getting the file name of a csv, then the structure, and then analyze the content). It can even correct itself, seeing the stack trace of its failed execution. You can of course also copy back and forth code and result to achieve all of this manually, but it gets cumbersome quite quickly.&lt;/li&gt; &#xA; &lt;li&gt;You have tons of pre-installed dependencies in Incognito Pilot&lt;/li&gt; &#xA; &lt;li&gt;The code runs in a sandbox, protecting your computer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How can it be private if you use public cloud APIs?&lt;/h3&gt; &#xA;&lt;p&gt;Whatever you type and all code results you approve are indeed not private, in the sense that they are sent to the cloud API. Your data however stays local. The interpreter runs locally as well, processing your data right where it is. For certain things, you will have to tell the model something about your data (e.g. the file-name of structure), but it usually is meta-data which you actively approve in the UI and not the actual data. At every step in the execution, you can just reject that something is sent to the API.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üè†&lt;/span&gt; Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/silvanmelchior/IncognitoPilot/main/docs/architecture.png&#34; alt=&#34;Architecture Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîß&lt;/span&gt; Development&lt;/h2&gt; &#xA;&lt;p&gt;Want to contribute to &lt;strong&gt;Incognito Pilot&lt;/strong&gt;? Or just install it without docker? Check out the contribution &lt;a href=&#34;https://raw.githubusercontent.com/silvanmelchior/IncognitoPilot/main/CONTRIBUTING.md&#34;&gt;instruction &amp;amp; guidelines&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>