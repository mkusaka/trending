<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-19T01:46:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iscyy/yoloair</title>
    <updated>2022-08-19T01:46:14Z</updated>
    <id>tag:github.com,2022-08-19:/iscyy/yoloair</id>
    <link href="https://github.com/iscyy/yoloair" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ”¥ğŸ”¥ğŸ”¥YOLOAirï¼šIncluding YOLOv5, YOLOv7, Transformer, YOLOX, YOLOR and other networks... Support to improve backbone, head, loss, IoU, NMS...The original version was created based on YOLOv5&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;YOLOAir: Make the improvement of the YOLO model faster, more convenient and more complete&lt;/h2&gt; &#xA;&lt;p&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&#34;https://raw.githubusercontent.com/iscyy/yoloair/main/README_EN.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;YOLOAirç®—æ³•åº“&amp;nbsp;æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„ä¸€ç³»åˆ— YOLO æ£€æµ‹ç®—æ³•ç»„åˆå·¥å…·ç®±ã€‚ç”¨æ¥&lt;strong&gt;ç»„åˆä¸åŒæ¨¡å—æ„å»ºä¸åŒç½‘ç»œ&lt;/strong&gt;ã€‚&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/iscyy/yoloair/main/docs/image/logo1.png&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;å†…ç½®YOLOv5ã€YOLOv7ã€YOLOXã€YOLORã€Transformerã€Scaled_YOLOv4ã€YOLOv3ã€YOLOv4ã€YOLO-Facev2ã€TPH-YOLOv5ã€YOLOv5Liteã€PicoDetç­‰æ¨¡å‹ç½‘ç»œç»“æ„(æŒç»­æ›´æ–°ä¸­ğŸš€)...&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ¨¡å—ç»„ä»¶åŒ–&lt;/strong&gt;ï¼šå¸®åŠ©ç”¨æˆ·è‡ªå®šä¹‰å¿«é€Ÿç»„åˆBackboneã€Neckã€Headï¼Œä½¿å¾—ç½‘ç»œæ¨¡å‹å¤šæ ·åŒ–ï¼ŒåŠ©åŠ›ç§‘ç ”æ”¹è¿›æ£€æµ‹ç®—æ³•ã€æ¨¡å‹æ”¹è¿›ï¼Œç½‘ç»œæ’åˆ—ç»„åˆğŸ†ã€‚æ„å»ºå¼ºå¤§çš„ç½‘ç»œæ¨¡å‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç»Ÿä¸€æ¨¡å‹ä»£ç æ¡†æ¶ã€ç»Ÿä¸€åº”ç”¨æ–¹å¼ã€ç»Ÿä¸€è°ƒå‚ã€ç»Ÿä¸€æ”¹è¿›ã€æ˜“äºæ¨¡å—ç»„åˆã€æ„å»ºæ›´å¼ºå¤§çš„ç½‘ç»œæ¨¡å‹&lt;/strong&gt;ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— &#xA;â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—&#xA; â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•&#xA;  â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—&#xA;   â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘&#xA;   â•šâ•â•    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•     â•šâ•â•  â•šâ•â•    â•šâ•â•    â•šâ•â•  â•šâ•â•&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;åŸºäº YOLOv5 ä»£ç æ¡†æ¶ï¼Œå¹¶åŒæ­¥é€‚é… &lt;strong&gt;ç¨³å®šçš„YOLOv5_v6.1æ›´æ–°&lt;/strong&gt;, åŒæ­¥v6.1éƒ¨ç½²ç”Ÿæ€ã€‚ä½¿ç”¨è¿™ä¸ªé¡¹ç›®ä¹‹å‰, æ‚¨å¯ä»¥å…ˆäº†è§£YOLOv5åº“ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iscyy/yoloair/main/#Mainfeatures&#34;&gt;ç‰¹æ€§ğŸš€&lt;/a&gt; â€¢ &lt;a href=&#34;https://raw.githubusercontent.com/iscyy/yoloair/main/#Usage&#34;&gt;ä½¿ç”¨ğŸ‰&lt;/a&gt; â€¢ &lt;a href=&#34;https://github.com/iscyy/yoloair&#34;&gt;æ–‡æ¡£ğŸ“’&lt;/a&gt; â€¢ &lt;a href=&#34;https://github.com/iscyy/yoloair/issues/new&#34;&gt;æŠ¥å‘Šé—®é¢˜ğŸŒŸ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/News-2022-red&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Update-YOLOAir-orange&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://visitor-badge.glitch.me/badge?page_id=iscyy.yoloair&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;æ”¯æŒ&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Support-YOLOv5-red&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-YOLOv7-brightgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-YOLOX-yellow&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-YOLOv4-green&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-Scaled_YOLOv4-ff96b4&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-YOLOv3-yellowgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-YOLOR-lightgrey&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-Transformer-9cf&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Support-Attention-green&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;é¡¹ç›®åœ°å€:&amp;nbsp;&lt;a href=&#34;https://github.com/iscyy/yoloair&#34;&gt;https://github.com/iscyy/yoloair&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;éƒ¨åˆ†æ”¹è¿›è¯´æ˜æ¼”ç¤º: &lt;a href=&#34;https://blog.csdn.net/qq_38668236?type=blog&#34;&gt;èŠ’æœæ±æ²¡æœ‰èŠ’æœ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;ä¸»è¦ç‰¹æ€§ğŸš€&lt;/h3&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šçš„YOLOç³»åˆ—ç®—æ³•æ¨¡å‹æ”¹è¿›(æŒç»­æ›´æ–°...)&lt;/p&gt; &#xA;&lt;p&gt;YOLOAir ç®—æ³•åº“æ±‡æ€»äº†å¤šç§ä¸»æµYOLOç³»åˆ—æ£€æµ‹æ¨¡å‹ï¼Œä¸€å¥—ä»£ç é›†æˆå¤šç§æ¨¡å‹:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å†…ç½®é›†æˆ YOLOv5 æ¨¡å‹ç½‘ç»œç»“æ„ã€YOLOv7 æ¨¡å‹ç½‘ç»œç»“æ„ã€ YOLOR æ¨¡å‹ç½‘ç»œç»“æ„ã€YOLOX æ¨¡å‹ç½‘ç»œç»“æ„ã€Scaled_YOLOv4 æ¨¡å‹ç½‘ç»œç»“æ„ã€YOLOv4 æ¨¡å‹ç½‘ç»œç»“æ„ã€YOLOv3 æ¨¡å‹ç½‘ç»œç»“æ„ã€YOLO-FaceV2æ¨¡å‹ç½‘ç»œç»“æ„ã€TPH-YOLOv5æ¨¡å‹ç½‘ç»œç»“æ„ã€YOLOv5-Liteæ¨¡å‹ç½‘ç»œç»“æ„ã€PicoDetæ¨¡å‹ç½‘ç»œç»“æ„ç­‰æŒç»­æ›´æ–°ä¸­...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iscyy/yoloair/main/docs/image/test.jpg&#34; width=&#34;500px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iscyy/yoloair/main/docs/image/zebra.jpg&#34; width=&#34;500px&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä»¥ä¸Šå¤šç§æ£€æµ‹ç®—æ³•ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹ä»£ç æ¡†æ¶ï¼Œ&lt;strong&gt;é›†æˆåœ¨ YOLOAir åº“ä¸­ï¼Œç»Ÿä¸€ä»»åŠ¡å½¢å¼ã€ç»Ÿä¸€åº”ç”¨æ–¹å¼&lt;/strong&gt;ã€‚ğŸŒŸä¾¿äºç§‘ç ”è€…ç”¨äºè®ºæ–‡ç®—æ³•æ¨¡å‹æ”¹è¿›ï¼Œæ¨¡å‹å¯¹æ¯”ï¼Œå®ç°ç½‘ç»œç»„åˆå¤šæ ·åŒ–ã€‚ğŸŒŸå·¥ç¨‹ç®—æ³•éƒ¨ç½²è½åœ°æ›´ä¾¿æ·ï¼ŒåŒ…å«è½»é‡åŒ–æ¨¡å‹å’Œç²¾åº¦æ›´é«˜çš„æ¨¡å‹ï¼Œæ ¹æ®åœºæ™¯åˆç†é€‰æ‹©ï¼Œåœ¨ç²¾åº¦å’Œé€Ÿåº¦ä¿©ä¸ªæ–¹é¢å–å¾—å¹³è¡¡ã€‚åŒæ—¶è¯¥åº“æ”¯æŒè§£è€¦ä¸åŒçš„ç»“æ„å’Œæ¨¡å—ç»„ä»¶ï¼Œè®©æ¨¡å—ç»„ä»¶åŒ–ï¼Œé€šè¿‡ç»„åˆä¸åŒçš„æ¨¡å—ç»„ä»¶ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®ä¸åŒæ•°æ®é›†æˆ–ä¸åŒä¸šåŠ¡åœºæ™¯è‡ªè¡Œå®šåˆ¶åŒ–æ„å»ºä¸åŒæ£€æµ‹æ¨¡å‹ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸ”¥ğŸ”¥ğŸ”¥ é‡ç£…ï¼ï¼ï¼ä½œä¸ºæ³¨æ„åŠ›æœºåˆ¶çš„å¼€æºé¡¹ç›®è¡¥å……ï¼Œå¼ºçƒˆæ¨èä¸€ä¸ª6300+ğŸŒŸStarçš„æ³¨æ„åŠ›æœºåˆ¶ç®—æ³•ä»£ç åº“ğŸ‘‰&lt;a href=&#34;https://github.com/xmu-xiaoma666/External-Attention-pytorch&#34;&gt;External-Attention-pytorch&lt;/a&gt;ï¼Œé‡Œé¢æ±‡æ€»æ•´ç†å¾ˆå…¨é¢ï¼ŒåŒ…å«å„ç§Attentionã€Self-Attentionç­‰ä»£ç ï¼Œä»£ç ç®€æ´æ˜“è¯»ï¼Œä¸€è¡Œä»£ç å®ç°Attentionæœºåˆ¶ã€‚æ¬¢è¿å¤§å®¶æ¥ç©å‘€ï¼&lt;/p&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒåŠ è½½YOLOv3ã€YOLOv4ã€YOLOv5ã€YOLOv7ã€YOLORç­‰ç½‘ç»œçš„å®˜æ–¹é¢„è®­ç»ƒæƒé‡è¿›è¡Œè¿ç§»å­¦ä¹ &lt;/p&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šBackbone&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CSPDarkNetç³»åˆ—&lt;/code&gt;ã€&lt;code&gt;RepBlockç³»åˆ—&lt;/code&gt;ã€ &lt;code&gt;ResNetç³»åˆ—&lt;/code&gt;ã€&lt;code&gt;RegNet ç³»åˆ—&lt;/code&gt;ã€ &lt;code&gt;ShuffleNetç³»åˆ—&lt;/code&gt;ã€&lt;code&gt;Ghostç³»åˆ—&lt;/code&gt;ã€ &lt;code&gt;MobileNetç³»åˆ—&lt;/code&gt;ã€&lt;code&gt;EfficientNetç³»åˆ—&lt;/code&gt;ã€ &lt;code&gt;ConvNextç³»åˆ—&lt;/code&gt;ã€&lt;code&gt;RepLKNetç³»åˆ—&lt;/code&gt;ã€ &lt;code&gt;è‡ªæ³¨æ„åŠ›Transformerç³»åˆ—&lt;/code&gt;ã€&lt;code&gt;CNNå’ŒTransformerç»“åˆ&lt;/code&gt; æŒç»­æ›´æ–°ä¸­ğŸˆ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šNeck&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;neckåŒ…å«&lt;code&gt;FPN&lt;/code&gt;ã€&lt;code&gt;PANet&lt;/code&gt;ã€&lt;code&gt;BiFPN&lt;/code&gt;ç­‰ä¸»æµç»“æ„ã€‚ æŒç»­æ›´æ–°ä¸­ğŸˆ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šæ£€æµ‹å¤´Head&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YOLOv4ã€YOLOv5 Headæ£€æµ‹å¤´ã€&lt;/li&gt; &#xA; &lt;li&gt;YOLOR éšå¼å­¦ä¹ Headæ£€æµ‹å¤´ã€&lt;/li&gt; &#xA; &lt;li&gt;YOLOXçš„è§£è€¦åˆæ£€æµ‹å¤´Decoupled Headã€DetectX Head&lt;/li&gt; &#xA; &lt;li&gt;è‡ªé€‚åº”ç©ºé—´ç‰¹å¾èåˆ æ£€æµ‹å¤´ASFF Headã€&lt;/li&gt; &#xA; &lt;li&gt;YOLOv7æ£€æµ‹å¤´IAuxDetect Head, IDetect Headç­‰ï¼›&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šå³æ’å³ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åœ¨ç½‘ç»œä»»ä½•éƒ¨åˆ†å³æ’å³ç”¨å¼ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ ä¾‹å¦‚ Self Attentionã€Contextual Transformerã€Bottleneck Transformerã€S2-MLP Attentionã€SK Attentionã€CBAM Attentionã€SE Attentionã€Coordinate attentionã€NAM Attentionã€GAM attentionã€ECA Attentionã€Shuffle Attentionã€DANet Attention ç­‰å¤šç§ä¸»æµæ³¨æ„åŠ›æœºåˆ¶&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šIoUæŸå¤±å‡½æ•°&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CIoUã€DIoUã€GIoUã€EIoUã€SIoUã€alpha IOUç­‰æŸå¤±å‡½æ•°;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ›´å¤šç©ºé—´é‡‘å­—å¡”æ± åŒ–ç»“æ„&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SPPã€SPPFã€ASPPã€RFBã€SPPCSPCç­‰;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šNMS&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NMSã€Merge-NMSã€DIoU-NMSã€Soft-NMSã€CIoU_NMSã€DIoU_NMSã€GIoU_NMSã€EIoU_NMSã€SIoU_NMSã€Soft-SIoUNMSã€Soft-CIoUNMSã€Soft-DIoUNMSã€Soft-EIoUNMSã€Soft-GIoUNMSç­‰;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šæ•°æ®å¢å¼º&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mosaicã€Copy pasteã€Random affine(Rotation, Scale, Translation and Shear)ã€MixUpã€Augment HSV(Hue, Saturation, Valueã€Random horizontal flip&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒæ›´å¤šLoss&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ComputeLossã€ComputeNWDLossã€ComputeLoss(X)ã€ComputeLoss(v6)ã€ComputeLossAuxOTA(v7)ã€ComputeLossOTA(v7)ç­‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒAnchor-baseå’ŒAnchor-Free&lt;/p&gt; &#xA;&lt;p&gt;ğŸš€æ”¯æŒåŠ æƒæ¡†èåˆ(WBF)&lt;/p&gt; &#xA;&lt;p&gt;ğŸš€ å†…ç½®å¤šç§ç½‘ç»œæ¨¡å‹æ¨¡å—åŒ–ç»„ä»¶&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP, C3, C3TR, C3SPP, C3Ghostç­‰ è¯¦ç»†ä»£ç  &lt;strong&gt;./models/common.pyæ–‡ä»¶&lt;/strong&gt; å†…&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ‘‰&lt;a href=&#34;https://github.com/iscyy/yoloair/raw/main/docs/document/model_.md&#34;&gt;ç½‘ç»œæ¨¡å‹ç»“æ„å›¾&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;ä»¥ä¸Šç»„ä»¶æ¨¡å—ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹ä»£ç æ¡†æ¶ã€ç»Ÿä¸€ä»»åŠ¡å½¢å¼ã€ç»Ÿä¸€åº”ç”¨æ–¹å¼ï¼Œ&lt;strong&gt;æ¨¡å—ç»„ä»¶åŒ–&lt;/strong&gt;ğŸš€ å¯ä»¥å¸®åŠ©ç”¨æˆ·è‡ªå®šä¹‰å¿«é€Ÿç»„åˆBackboneã€Neckã€Headï¼Œä½¿å¾—ç½‘ç»œæ¨¡å‹å¤šæ ·åŒ–ï¼ŒåŠ©åŠ›ç§‘ç ”æ”¹è¿›æ£€æµ‹ç®—æ³•ï¼Œæ„å»ºæ›´å¼ºå¤§çš„ç½‘ç»œæ¨¡å‹ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;å†…ç½®ç½‘ç»œæ¨¡å‹é…ç½®æ”¯æŒâœ¨&lt;/h3&gt; &#xA;&lt;p&gt;ğŸš€åŒ…æ‹¬YOLOv3ã€YOLOv4ã€Scaled_YOLOv4ã€YOLOv5ã€YOLOv7ã€YOLOXã€YOLORã€Transformerã€YOLO-FaceV2ã€PicoDetã€YOLOv5-Liteã€TPH-YOLOv5ã€&lt;strong&gt;å…¶ä»–å¤šç§æ”¹è¿›ç½‘ç»œç»“æ„ç­‰ç®—æ³•æ¨¡å‹&lt;/strong&gt;çš„yamlé…ç½®æ–‡ä»¶&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;æŠ€æœ¯äº¤æµ &lt;img title=&#34;&#34; src=&#34;https://user-images.githubusercontent.com/48054808/157800467-2a9946ad-30d1-49a9-b9db-ba33413d9c90.png&#34; alt=&#34;&#34; width=&#34;20&#34;&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æœ€æ–°ç§‘ç ”è®ºæ–‡ æ¨èFightingCVå…¬ä¼—å·ï¼Œåˆ†äº«æœ€æ–°è®ºæ–‡è§£æå’Œç§‘æŠ€å‰æ²¿åŠ¨æ€ï¼&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/iscyy/yoloair/main/docs/image/fightingcv.jpg&#34; width=&#34;200&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ¬¢è¿åŠ å…¥FightingCVå¾®ä¿¡äº¤æµç¾¤ï¼ˆå…³æ³¨å¹¶å›å¤å°åŠ©æ‰‹â€œåŠ ç¾¤â€ï¼‰ å…¬ä¼—å·æ¯å¤©éƒ½ä¼šè¿›è¡Œè®ºæ–‡ã€ç®—æ³•å’Œä»£ç çš„å¹²è´§åˆ†äº«å“¦~&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ä½¿ç”¨ğŸ‰&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;About the code.&lt;/strong&gt; Follow the design principle of &lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;.&lt;br&gt; The original version was created based on YOLOv5(v6.1)&lt;/p&gt; &#xA;&lt;h3&gt;å®‰è£…&lt;/h3&gt; &#xA;&lt;p&gt;åœ¨&lt;strong&gt;Python&amp;gt;=3.7.0&lt;/strong&gt;&amp;nbsp;çš„ç¯å¢ƒä¸­å…‹éš†ç‰ˆæœ¬ä»“å¹¶å®‰è£…&amp;nbsp;requirements.txtï¼ŒåŒ…æ‹¬&lt;strong&gt;PyTorch&amp;gt;=1.7&lt;/strong&gt;ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git&amp;nbsp;clone&amp;nbsp;https://github.com/iscyy/yoloair.git&amp;nbsp;&amp;nbsp;#&amp;nbsp;å…‹éš†&#xA;$ cd&amp;nbsp;YOLOAir&#xA;$ pip&amp;nbsp;install&amp;nbsp;-r&amp;nbsp;requirements.txt&amp;nbsp;&amp;nbsp;#&amp;nbsp;å®‰è£…&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;è®­ç»ƒ&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python&amp;nbsp;train.py&amp;nbsp;--data&amp;nbsp;coco128.yaml&amp;nbsp;--cfg&amp;nbsp;configs/yolov5/yolov5s.yaml #é»˜è®¤ä¸ºyolo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;æ¨ç†&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt;&amp;nbsp;åœ¨å„ç§æ•°æ®æºä¸Šè¿è¡Œæ¨ç†, å¹¶å°†æ£€æµ‹ç»“æœä¿å­˜åˆ°&amp;nbsp;&lt;code&gt;runs/detect&lt;/code&gt;&amp;nbsp;ç›®å½•ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python&amp;nbsp;detect.py&amp;nbsp;--source&amp;nbsp;0&amp;nbsp;&amp;nbsp;#&amp;nbsp;ç½‘ç»œæ‘„åƒå¤´&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;img.jpg&amp;nbsp;&amp;nbsp;#&amp;nbsp;å›¾åƒ&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;vid.mp4&amp;nbsp;&amp;nbsp;#&amp;nbsp;è§†é¢‘&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;path/&amp;nbsp;&amp;nbsp;#&amp;nbsp;æ–‡ä»¶å¤¹&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;path/*.jpg&amp;nbsp;&amp;nbsp;#&amp;nbsp;glob&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;èåˆ&lt;/h3&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨ä½¿ç”¨ä¸åŒæ¨¡å‹æ¥æ¨ç†æ•°æ®é›†ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ wbf.pyæ–‡ä»¶ é€šè¿‡åŠ æƒæ¡†èåˆæ¥é›†æˆç»“æœã€‚ æ‚¨åªéœ€è¦åœ¨ wbf.pyæ–‡ä»¶ ä¸­è®¾ç½® img è·¯å¾„å’Œ txt è·¯å¾„ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python wbf.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;Updating...&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;YOLOç½‘ç»œæ¨¡å‹å…·ä½“æ”¹è¿›æ–¹å¼æ•™ç¨‹åŠåŸç†å‚è€ƒ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;9.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126333061&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š9.BoTNet Transformerç»“æ„çš„ä¿®æ”¹&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;8.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126302599&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š8.å¢åŠ ACmixç»“æ„çš„ä¿®æ”¹,è‡ªæ³¨æ„åŠ›å’Œå·ç§¯é›†æˆ&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;7.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126243834&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š7.ä¿®æ”¹DIoU-NMS,SIoU-NMS,EIoU-NMS,CIoU-NMS,GIoU-NMS&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;6.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126245080&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š6.ä¿®æ”¹Soft-NMS,Soft-CIoUNMS,Soft-SIoUNMS&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;5.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126226726&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š5.CotNet Transformerç»“æ„çš„ä¿®æ”¹&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;4.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126157859&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š4.YOLOv5_æœ€æ–°MobileOneç»“æ„æ¢Backboneä¿®æ”¹&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;3.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126122888?spm=1001.2014.3001.5502&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š3.Swin Transformerç»“æ„çš„ä¿®æ”¹&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126087343?spm=1001.2014.3001.5502&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š2.PicoDetç»“æ„çš„ä¿®æ”¹&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;1.&lt;a href=&#34;https://blog.csdn.net/qq_38668236/article/details/126086716&#34;&gt;æ”¹è¿›YOLOv5ç³»åˆ—ï¼š1.å¤šç§æ³¨æ„åŠ›æœºåˆ¶ä¿®æ”¹&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43694096/article/details/126354660&#34;&gt;1.ç©ºé—´é‡‘å­—å¡”æ± åŒ–æ”¹è¿› SPP / SPPF / ASPP / RFB / SPPCSPC&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43694096/article/details/124413941&#34;&gt;2.Yolov5æ›´æ¢æ¿€æ´»å‡½æ•°&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43694096/article/details/125416120&#34;&gt;3.Yolov5æ›´æ¢ä¸Šé‡‡æ ·æ–¹å¼&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ›´å¤šæ¨¡å—è¯¦ç»†è§£é‡Šæ•™ç¨‹æŒç»­æ›´æ–°ä¸­...&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;YOLOv5å®˜æ–¹æ•™ç¨‹âœ¨&lt;/h3&gt; &#xA;&lt;p&gt;ä¸YOLOv5æ¡†æ¶åŒæ­¥&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&#34;&gt;è®­ç»ƒè‡ªå®šä¹‰æ•°æ®&lt;/a&gt; ğŸš€ æ¨è&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results&#34;&gt;è·å¾—æœ€ä½³è®­ç»ƒæ•ˆæœçš„æŠ€å·§&lt;/a&gt; â˜˜ï¸ æ¨è&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/1289&#34;&gt;ä½¿ç”¨ Weights &amp;amp; Biases è®°å½•å®éªŒ&lt;/a&gt; ğŸŒŸ æ–°&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/4975&#34;&gt;Roboflowï¼šæ•°æ®é›†ã€æ ‡ç­¾å’Œä¸»åŠ¨å­¦ä¹ &lt;/a&gt; ğŸŒŸ æ–°&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/475&#34;&gt;å¤šGPUè®­ç»ƒ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/36&#34;&gt;PyTorch Hub&lt;/a&gt; â­ æ–°&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/251&#34;&gt;TFLite, ONNX, CoreML, TensorRT å¯¼å‡º&lt;/a&gt; ğŸš€&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/303&#34;&gt;æµ‹è¯•æ—¶æ•°æ®å¢å¼º (TTA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/318&#34;&gt;æ¨¡å‹é›†æˆ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/304&#34;&gt;æ¨¡å‹å‰ªæ/ç¨€ç–æ€§&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/607&#34;&gt;è¶…å‚æ•°è¿›åŒ–&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/1314&#34;&gt;å¸¦æœ‰å†»ç»“å±‚çš„è¿ç§»å­¦ä¹ &lt;/a&gt; â­ æ–°&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5/issues/6998&#34;&gt;æ¶æ„æ¦‚è¦&lt;/a&gt; â­ æ–°&lt;/li&gt; &#xA;&lt;/ul&gt;  &#xA;&lt;h3&gt;æœªæ¥å¢å¼ºâœ¨&lt;/h3&gt; &#xA;&lt;p&gt;åç»­ä¼šæŒç»­å»ºè®¾å’Œå®Œå–„ YOLOAir ç”Ÿæ€&lt;br&gt; å®Œå–„é›†æˆæ›´å¤š YOLO ç³»åˆ—æ¨¡å‹ï¼ŒæŒç»­ç»“åˆä¸åŒæ¨¡å—ï¼Œæ„å»ºæ›´å¤šä¸åŒç½‘ç»œæ¨¡å‹&lt;br&gt; æ¨ªå‘æ‹“å±•å’Œå¼•å…¥å…³è”æŠ€æœ¯ï¼Œå¦‚åŠç›‘ç£å­¦ä¹ ç­‰ç­‰&lt;br&gt; è·Ÿè¿›ï¼šYOLO-mask &amp;amp; YOLO-pose&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Statement&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; &lt;b&gt;Expand&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;The content of this site is only for sharing notes. If some content is infringing, please sending email.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;If you have any question, please discuss with me by sending email.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; &lt;b&gt;Expand&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;https://github.com/ultralytics/yolov5&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/AlexeyAB/darknet&#34;&gt;https://github.com/AlexeyAB/darknet&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/ultralytics/yolov3&#34;&gt;https://github.com/ultralytics/yolov3&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/WongKinYiu/PyTorch_YOLOv4&#34;&gt;https://github.com/WongKinYiu/PyTorch_YOLOv4&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/WongKinYiu/ScaledYOLOv4&#34;&gt;https://github.com/WongKinYiu/ScaledYOLOv4&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/meituan/YOLOv6&#34;&gt;https://github.com/meituan/YOLOv6&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolov7&#34;&gt;https://github.com/WongKinYiu/yolov7&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/WongKinYiu/yolor&#34;&gt;https://github.com/WongKinYiu/yolor&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/xmu-xiaoma666/External-Attention-pytorch&#34;&gt;https://github.com/xmu-xiaoma666/External-Attention-pytorch&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://gitee.com/SearchSource/yolov5_yolox&#34;&gt;https://gitee.com/SearchSource/yolov5_yolox&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/Krasjet-Yu/YOLO-FaceV2&#34;&gt;https://github.com/Krasjet-Yu/YOLO-FaceV2&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/positive666/yolov5_research&#34;&gt;https://github.com/positive666/yolov5_research/&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/ppogg/YOLOv5-Lite&#34;&gt;https://github.com/ppogg/YOLOv5-Lite&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/Gumpest/YOLOv5-Multibackbone-Compression&#34;&gt;https://github.com/Gumpest/YOLOv5-Multibackbone-Compression&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/cv516Buaa/tph-yolov5&#34;&gt;https://github.com/cv516Buaa/tph-yolov5&lt;/a&gt; Paper:&lt;a href=&#34;https://arxiv.org/abs/2208.02019&#34;&gt;https://arxiv.org/abs/2208.02019&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>archinetai/audio-diffusion-pytorch</title>
    <updated>2022-08-19T01:46:14Z</updated>
    <id>tag:github.com,2022-08-19:/archinetai/audio-diffusion-pytorch</id>
    <link href="https://github.com/archinetai/audio-diffusion-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Audio generation using diffusion models, in PyTorch.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/archinetai/audio-diffusion-pytorch/main/LOGO.png&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unconditional audio generation using diffusion models, in PyTorch. The goal of this repository is to explore different architectures and diffusion models to generate audio (speech and music) directly from/to the waveform. Progress will be documented in the &lt;a href=&#34;https://raw.githubusercontent.com/archinetai/audio-diffusion-pytorch/main/#experiments&#34;&gt;experiments&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install audio-diffusion-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/audio-diffusion-pytorch/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/audio-diffusion-pytorch?style=flat&amp;amp;colorA=0f0f0f&amp;amp;colorB=0f0f0f&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from audio_diffusion_pytorch import AudioDiffusionModel&#xA;&#xA;model = AudioDiffusionModel()&#xA;&#xA;# Train model with audio sources&#xA;x = torch.randn(2, 1, 2 ** 18) # [batch, channels, samples], 2**18 â‰ˆ 12s of audio at a frequency of 22050&#xA;loss = model(x)&#xA;loss.backward() # Do this many times&#xA;&#xA;# Sample 2 sources given start noise&#xA;noise = torch.randn(2, 1, 2 ** 18)&#xA;sampled = model.sample(&#xA;    noise=noise,&#xA;    num_steps=5 # Suggested range: 2-100&#xA;) # [2, 1, 262144]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage with Components&lt;/h2&gt; &#xA;&lt;h3&gt;UNet1d&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from audio_diffusion_pytorch import UNet1d&#xA;&#xA;# UNet used to denoise our 1D (audio) data&#xA;unet = UNet1d(&#xA;    in_channels=1,&#xA;    patch_size=16,&#xA;    channels=128,&#xA;    multipliers=[1, 2, 4, 4, 4, 4, 4],&#xA;    factors=[4, 4, 4, 2, 2, 2],&#xA;    attentions=[False, False, False, True, True, True],&#xA;    num_blocks=[2, 2, 2, 2, 2, 2],&#xA;    attention_heads=8,&#xA;    attention_features=64,&#xA;    attention_multiplier=2,&#xA;    resnet_groups=8,&#xA;    kernel_multiplier_downsample=2,&#xA;    kernel_sizes_init=[1, 3, 7],&#xA;    use_nearest_upsample=False,&#xA;    use_skip_scale=True,&#xA;    use_attention_bottleneck=True,&#xA;    use_learned_time_embedding=True,&#xA;)&#xA;&#xA;x = torch.randn(3, 1, 2 ** 16)&#xA;t = torch.tensor([0.2, 0.8, 0.3])&#xA;&#xA;y = unet(x, t) # [3, 1, 32768], compute 3 samples of ~1.5 seconds at 22050Hz with the given noise levels t&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Diffusion&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from audio_diffusion_pytorch import Diffusion, LogNormalDistribution&#xA;&#xA;diffusion = Diffusion(&#xA;    net=unet,&#xA;    sigma_distribution=LogNormalDistribution(mean = -3.0, std = 1.0),&#xA;    sigma_data=0.1,&#xA;    dynamic_threshold=0.95&#xA;)&#xA;&#xA;x = torch.randn(3, 1, 2 ** 18) # Batch of training audio samples&#xA;loss = diffusion(x)&#xA;loss.backward() # Do this many times&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Sampling&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from audio_diffusion_pytorch import DiffusionSampler, KarrasSchedule&#xA;&#xA;sampler = DiffusionSampler(&#xA;    diffusion,&#xA;    num_steps=5, # Suggested range 2-100, higher better quality but takes longer&#xA;    sampler=ADPM2Sampler(rho=1),&#xA;    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0)&#xA;)&#xA;# Generate a sample starting from the provided noise&#xA;y = sampler(noise = torch.randn(1,1,2 ** 18))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Inpainting&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from audio_diffusion_pytorch import DiffusionInpainter, KarrasSchedule, ADPM2Sampler&#xA;&#xA;inpainter = DiffusionInpainter(&#xA;    diffusion,&#xA;    num_steps=5, # Suggested range 2-100, higher for better quality&#xA;    num_resamples=1, # Suggested range 1-10, higher for better quality&#xA;    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0),&#xA;    sampler=ADPM2Sampler(rho=1.0),&#xA;)&#xA;&#xA;inpaint = torch.randn(1,1,2 ** 18) # Start track, e.g. one sampled with DiffusionSampler&#xA;inpaint_mask = torch.randint(0,2, (1,1,2 ** 18), dtype=torch.bool) # Set to `True` the parts you want to keep&#xA;y = inpainter(inpaint = inpaint, inpaint_mask = inpaint_mask)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Infinite Generation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from audio_diffusion_pytorch import SpanBySpanComposer&#xA;&#xA;composer = SpanBySpanComposer(&#xA;    inpainter,&#xA;    num_spans=4 # Number of spans to inpaint after provided input&#xA;)&#xA;y_long = composer(y, keep_start=True) # [1, 1, 98304]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Experiments&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Report&lt;/th&gt; &#xA;   &lt;th&gt;Snapshot&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/schneider/audio/reports/Audio-Diffusion-UNet-Alpha---VmlldzoyMjk3MzIz?accessToken=y0l3igdvnm4ogn4d3ph3b0i8twwcf7meufbviwt15f0qtasyn1i14hg340bkk1te&#34;&gt;Alpha&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/archinetai/audio-diffusion-pytorch/tree/6bd9279f192fc0c11eb8a21cd919d9c41181bf35&#34;&gt;6bd9279f19&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Initial tests on LJSpeech dataset with new architecture and basic DDPM diffusion model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/schneider/audio/reports/Audio-Diffusion-Bravo---VmlldzoyMzE4NjIx?accessToken=qt2w1jeqch9l5v3ffjns99p69jsmexk849dszyiennfbivgg396378u6ken2fm2d&#34;&gt;Bravo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/archinetai/audio-diffusion-pytorch/tree/a05f30aa94e07600038d36cfb96f8492ef735a99&#34;&gt;a05f30aa94&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Elucidated diffusion, improved architecture with patching, longer duration, initial good (unsupervised) results on LJSpeech.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/schneider/audio/reports/Audio-Diffusion-Charlie---VmlldzoyMzYyNDA1?accessToken=71gmurcwndv5e2abqrjnlh3n74j5555j3tycpd7h40tnv8fvb17k5pjkb57j9xxa&#34;&gt;Charlie&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/archinetai/audio-diffusion-pytorch/tree/50ecc30d70a211b92cb9c38d4b0250d7cc30533f&#34;&gt;50ecc30d70&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Train on music with &lt;a href=&#34;https://github.com/archinetai/audio-data-pytorch&#34;&gt;YoutubeDataset&lt;/a&gt;, larger patch tests for longer tracks, inpainting tests, initial test with infinite generation using SpanBySpanComposer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/schneider/audio/reports/Audio-Diffusion-Delta---VmlldzoyNDYyMzk1?accessToken=n1d34n35qserpx7nhskkfdm1q12hlcxx1qcmfw5ypz53kjkzoh0ge2uvhshiseqx&#34;&gt;Delta&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;(current)&lt;/td&gt; &#xA;   &lt;td&gt;Test model with the faster &lt;code&gt;ADPM2&lt;/code&gt; sampler and dynamic thresholding.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add elucidated diffusion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add ancestral DPM2 sampler.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add dynamic thresholding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add (variational) autoencoder option to compress audio before diffusion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fix inpainting and make it work with ADPM2 sampler.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://inf.ethz.ch/&#34;&gt;ETH Zurich&lt;/a&gt; for the compute, &lt;a href=&#34;https://zhijing-jin.com/&#34;&gt;Zhijing Jin&lt;/a&gt;, &lt;a href=&#34;http://www.mrinmaya.io/&#34;&gt;Mrinmaya Sachan&lt;/a&gt;, and &lt;a href=&#34;https://is.mpg.de/~bs&#34;&gt;Bernhard Schoelkopf&lt;/a&gt; for supervising this Thesis.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lucidrains&#34;&gt;Phil Wang&lt;/a&gt; for the beautiful open source contributions on &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;diffusion&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/imagen-pytorch&#34;&gt;Imagen&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;Katherine Crowson&lt;/a&gt; for the experiments with &lt;a href=&#34;https://github.com/crowsonkb/k-diffusion&#34;&gt;k-diffusion&lt;/a&gt; and discovering the insane &lt;code&gt;ADPM2&lt;/code&gt; sampler.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;DDPM&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2006.11239,&#xA;Author = {Jonathan Ho and Ajay Jain and Pieter Abbeel},&#xA;Title = {Denoising Diffusion Probabilistic Models},&#xA;Year = {2020},&#xA;Eprint = {arXiv:2006.11239},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Diffusion inpainting&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2201.09865,&#xA;Author = {Andreas Lugmayr and Martin Danelljan and Andres Romero and Fisher Yu and Radu Timofte and Luc Van Gool},&#xA;Title = {RePaint: Inpainting using Denoising Diffusion Probabilistic Models},&#xA;Year = {2022},&#xA;Eprint = {arXiv:2201.09865},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Diffusion weighted loss&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2204.00227,&#xA;Author = {Jooyoung Choi and Jungbeom Lee and Chaehun Shin and Sungwon Kim and Hyunwoo Kim and Sungroh Yoon},&#xA;Title = {Perception Prioritized Training of Diffusion Models},&#xA;Year = {2022},&#xA;Eprint = {arXiv:2204.00227},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Improved UNet architecture&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2205.11487,&#xA;Author = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S. Sara Mahdavi and Rapha Gontijo Lopes and Tim Salimans and Jonathan Ho and David J Fleet and Mohammad Norouzi},&#xA;Title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},&#xA;Year = {2022},&#xA;Eprint = {arXiv:2205.11487},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Elucidated diffusion&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{2206.00364,&#xA;Author = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},&#xA;Title = {Elucidating the Design Space of Diffusion-Based Generative Models},&#xA;Year = {2022},&#xA;Eprint = {arXiv:2206.00364},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>TimDettmers/bitsandbytes</title>
    <updated>2022-08-19T01:46:14Z</updated>
    <id>tag:github.com,2022-08-19:/TimDettmers/bitsandbytes</id>
    <link href="https://github.com/TimDettmers/bitsandbytes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;8-bit CUDA functions for PyTorch&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitsandbytes&lt;/h1&gt; &#xA;&lt;p&gt;The bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.&lt;/p&gt; &#xA;&lt;p&gt;Resources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.02861&#34;&gt;8-bit Optimizer Paper&lt;/a&gt; -- &lt;a href=&#34;https://www.youtube.com/watch?v=IxrlHAJtqKE&#34;&gt;Video&lt;/a&gt; -- &lt;a href=&#34;https://bitsandbytes.readthedocs.io/en/latest/&#34;&gt;Docs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.07339&#34;&gt;LLM.int8() Paper&lt;/a&gt; -- &lt;a href=&#34;https://huggingface.co/blog/hf-bitsandbytes-integration&#34;&gt;LLM.int8() Software Blog Post&lt;/a&gt; -- &lt;a href=&#34;https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/&#34;&gt;LLM.int8() Emergent Features Blog Post&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TL;DR&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation&lt;/strong&gt;: &lt;code&gt;pip install bitsandbytes&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using 8-bit optimizer&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Comment out optimizer: &lt;code&gt;#torch.optim.Adam(....)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add 8-bit optimizer of your choice &lt;code&gt;bnb.optim.Adam8bit(....)&lt;/code&gt; (arguments stay the same)&lt;/li&gt; &#xA; &lt;li&gt;Replace embedding layer if necessary: &lt;code&gt;torch.nn.Embedding(..) -&amp;gt; bnb.nn.Embedding(..)&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using 8-bit Inference&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Comment out torch.nn.Linear: &lt;code&gt;#linear = torch.nn.Linear(...)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add bnb 8-bit linear light module: &lt;code&gt;linear = bnb.nn.Linear8bitLt(...)&lt;/code&gt; (base arguments stay the same)&lt;/li&gt; &#xA; &lt;li&gt;There are two modes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Mixed 8-bit training with 16-bit main weights. Pass the argument &lt;code&gt;use_fp16_weights=True&lt;/code&gt; (default)&lt;/li&gt; &#xA;   &lt;li&gt;Int8 inference. Pass the argument &lt;code&gt;use_fp16_weights=False&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;To use the full LLM.int8() method, use the &lt;code&gt;threshold=k&lt;/code&gt; argument. We recommend &lt;code&gt;k=6.0&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# LLM.int8()&#xA;linear = bnb.nn.Linear8bitLt(dim1, dim2, bias=True, use_fp16_weights=False, threshold=6.0)&#xA;# inputs need to be fp16&#xA;out = linear(x.to(torch.float16))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8-bit Matrix multiplication with mixed precision decomposition&lt;/li&gt; &#xA; &lt;li&gt;LLM.int8() inference&lt;/li&gt; &#xA; &lt;li&gt;8-bit Optimizers: Adam, AdamW, RMSProp, LARS, LAMB (saves 75% memory)&lt;/li&gt; &#xA; &lt;li&gt;Stable Embedding Layer: Improved stability through better initialization, and normalization&lt;/li&gt; &#xA; &lt;li&gt;8-bit quantization: Quantile, Linear, and Dynamic quantization&lt;/li&gt; &#xA; &lt;li&gt;Fast quantile estimation: Up to 100x faster than other algorithms&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements &amp;amp; Installation&lt;/h2&gt; &#xA;&lt;p&gt;Requirements: anaconda, cudatoolkit, pytorch&lt;/p&gt; &#xA;&lt;p&gt;Hardware requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM.int8(): NVIDIA Turing (RTX 20xx; T4) or Ampere GPU (RTX 30xx; A4-A100); (a GPU from 2018 or older).&lt;/li&gt; &#xA; &lt;li&gt;8-bit optimizers and quantization: NVIDIA Maxwell GPU or newer (&amp;gt;=GTX 9XX).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Supported CUDA versions: 10.2 - 11.7&lt;/p&gt; &#xA;&lt;p&gt;The requirements can best be fulfilled by installing pytorch via anaconda. You can install PyTorch by following the &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;&#34;Get Started&#34;&lt;/a&gt; instructions on the official website.&lt;/p&gt; &#xA;&lt;h2&gt;Using bitsandbytes&lt;/h2&gt; &#xA;&lt;h3&gt;Using Int8 Matrix Multiplication&lt;/h3&gt; &#xA;&lt;p&gt;For straight Int8 matrix multiplication with mixed precision decomposition you can use &lt;code&gt;bnb.matmul(...)&lt;/code&gt;. To enable mixed precision decomposition, use the threshold parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bnb.matmul(..., threshold=6.0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instructions how to use LLM.int8() inference layers in your own code, see the TL;DR above or for extended instruction see &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;this blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using the 8-bit Optimizers&lt;/h3&gt; &#xA;&lt;p&gt;With bitsandbytes 8-bit optimizers can be used by changing a single line of code in your codebase. For NLP models we recommend also to use the StableEmbedding layers (see below) which improves results and helps with stable 8-bit optimization. To get started with 8-bit optimizers, it is sufficient to replace your old optimizer with the 8-bit optimizer in the following way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import bitsandbytes as bnb&#xA;&#xA;# adam = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.995)) # comment out old optimizer&#xA;adam = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995)) # add bnb optimizer&#xA;adam = bnb.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.995), optim_bits=8) # equivalent&#xA;&#xA;&#xA;torch.nn.Embedding(...) -&amp;gt;  bnb.nn.StableEmbedding(...) # recommended for NLP models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that by default all parameter tensors with less than 4096 elements are kept at 32-bit even if you initialize those parameters with 8-bit optimizers. This is done since such small tensors do not save much memory and often contain highly variable parameters (biases) or parameters that require high precision (batch norm, layer norm). You can change this behavior like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# parameter tensors with less than 16384 values are optimized in 32-bit&#xA;# it is recommended to use multiplies of 4096&#xA;adam = bnb.optim.Adam8bit(model.parameters(), min_8bit_size=16384) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Change Bits and other Hyperparameters for Individual Parameters&lt;/h3&gt; &#xA;&lt;p&gt;If you want to optimize some unstable parameters with 32-bit Adam and others with 8-bit Adam, you can use the &lt;code&gt;GlobalOptimManager&lt;/code&gt;. With this, we can also configure specific hyperparameters for particular layers, such as embedding layers. To do that, we need two things: (1) register the parameter while they are still on the CPU, (2) override the config with the new desired hyperparameters (anytime, anywhere). See our &lt;a href=&#34;https://raw.githubusercontent.com/TimDettmers/bitsandbytes/main/howto_config_override.md&#34;&gt;guide&lt;/a&gt; for more details&lt;/p&gt; &#xA;&lt;h3&gt;Fairseq Users&lt;/h3&gt; &#xA;&lt;p&gt;To use the Stable Embedding Layer, override the respective &lt;code&gt;build_embedding(...)&lt;/code&gt; function of your model. Make sure to also use the &lt;code&gt;--no-scale-embedding&lt;/code&gt; flag to disable scaling of the word embedding layer (nor replaced with layer norm). You can use the optimizers by replacing the optimizer in the respective file (&lt;code&gt;adam.py&lt;/code&gt; etc.).&lt;/p&gt; &#xA;&lt;h2&gt;Release and Feature History&lt;/h2&gt; &#xA;&lt;p&gt;For upcoming features and changes and full history see &lt;a href=&#34;https://raw.githubusercontent.com/TimDettmers/bitsandbytes/main/CHANGELOG.md&#34;&gt;Patch Notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Errors&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device. &lt;a href=&#34;https://raw.githubusercontent.com/TimDettmers/bitsandbytes/main/errors_and_solutions.md#No-kernel-image-available&#34;&gt;Solution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;_&lt;em&gt;fatbinwrap&lt;/em&gt;.. &lt;a href=&#34;https://raw.githubusercontent.com/TimDettmers/bitsandbytes/main/errors_and_solutions.md#fatbinwrap_&#34;&gt;Solution&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Compile from source&lt;/h2&gt; &#xA;&lt;p&gt;To compile from source, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/TimDettmers/bitsandbytes/main/compile_from_source.md&#34;&gt;compile_from_source.md&lt;/a&gt; instructions.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of bitsandbytes is licensed under MIT, however portions of the project are available under separate license terms: Pytorch is licensed under the BSD license.&lt;/p&gt; &#xA;&lt;p&gt;We thank Fabio Cannizzo for his work on &lt;a href=&#34;https://github.com/fabiocannizzo/FastBinarySearch&#34;&gt;FastBinarySearch&lt;/a&gt; which we use for CPU quantization.&lt;/p&gt; &#xA;&lt;h2&gt;How to cite us&lt;/h2&gt; &#xA;&lt;p&gt;If you found this library and found LLM.int8() useful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{dettmers2022llmint8,&#xA;  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},&#xA;  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},&#xA;  journal={arXiv preprint arXiv:2208.07339},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For 8-bit optimizers or quantization routines please consider citing the following work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{dettmers2022optimizers,&#xA;  title={8-bit Optimizers via Block-wise Quantization},&#xA;  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},&#xA;  journal={9th International Conference on Learning Representations, ICLR},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>