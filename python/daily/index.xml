<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-01T01:32:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jinfagang/yolov7</title>
    <updated>2022-07-01T01:32:35Z</updated>
    <id>tag:github.com,2022-07-01:/jinfagang/yolov7</id>
    <link href="https://github.com/jinfagang/yolov7" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üî•üî•üî•üî• YOLO with Transformers and Instance Segmentation, with TensorRT acceleration! üî•üî•üî•&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://s4.ax1x.com/2022/02/01/Hk2dtP.png&#34;&gt; &#xA; &lt;h1&gt;YOLOv7 - Make YOLO Great Again&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/jinfagang/yolov7&#34;&gt;Documentation&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/jinfagang/yolov7&#34;&gt;Installation Instructions&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/#deploy&#34;&gt;Deployment&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/.github/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/jinfagang/yolov7/issues/new?assignees=&amp;amp;labels=&amp;amp;template=bug-report.yml&#34;&gt;Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/alfred-py/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/yolort&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/yolort&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/alfred-py?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=blue&amp;amp;left_text=pypi%20downloads&#34; alt=&#34;PyPI downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://img.shields.io/github/downloads/jinfagang/yolov7/total?color=blue&amp;amp;label=Downloads&amp;amp;logo=github&amp;amp;logoColor=lightgrey&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/jinfagang/yolov7/total?color=blue&amp;amp;label=downloads&amp;amp;logo=github&amp;amp;logoColor=lightgrey&#34; alt=&#34;Github downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/zhiqwang/yolov5-rt-stack&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/zhiqwang/yolov5-rt-stack/branch/main/graph/badge.svg?token=1GX96EA72Y&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/zhiqwang/yolov5-rt-stack?color=dfd&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/yolort/shared_invite/zt-mqwc7235-940aAh8IaKYeWclrJx10SA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-chat-aff.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jinfagang/yolov7/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-pink.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In short: &lt;strong&gt;YOLOv7 added instance segmentation to YOLO arch&lt;/strong&gt;. Also many transformer backbones, archs included. If you look carefully, you&#39;ll find our ultimate vision is to &lt;strong&gt;make YOLO great again&lt;/strong&gt; by the power of &lt;strong&gt;transformers&lt;/strong&gt;, as well as &lt;strong&gt;multi-tasks training&lt;/strong&gt;. YOLOv7 achieves mAP 43, AP-s exceed MaskRCNN by 10 with a convnext-tiny backbone while simillar speed with YOLOX-s, more models listed below, it&#39;s more accurate and even more lighter!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;üî•üî•üî• Just another yolo variant implemented based on &lt;strong&gt;&lt;code&gt;detectron2&lt;/code&gt;&lt;/strong&gt;. But note that &lt;strong&gt;YOLOv7 doesn&#39;t meant to be a successor of yolo family, 7 is just a magic and lucky number. Instead, YOLOv7 extend yolo into many other vision tasks, such as instance segmentation, one-stage keypoints detection etc.&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The supported matrix in YOLOv7 are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOv4 contained with CSP-Darknet53;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOv7 arch with resnets backbone;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOv7 arch with resnet-vd backbone (likely as PP-YOLO), deformable conv, Mish etc;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GridMask augmentation from PP-YOLO included;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mosiac transform supported with a custom datasetmapper;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOv7 arch Swin-Transformer support (higher accuracy but lower speed);&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOv7 arch Efficientnet + BiFPN;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOv5 style positive samples selection, new coordinates coding style;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; RandomColorDistortion, RandomExpand, RandomCrop, RandomFlip;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CIoU loss (DIoU, GIoU) and label smoothing (from YOLOv5 &amp;amp; YOLOv4);&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOF also included;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOv7 Res2net + FPN supported;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pyramid Vision Transformer v2 (PVTv2) supported;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; WBF (Weighted Box Fusion), this works better than NMS, &lt;a href=&#34;https://github.com/ZFTurbo/Weighted-Boxes-Fusion&#34;&gt;link&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOX like head design and anchor design, also training support;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOX s,m,l backbone and PAFPN added, we have a new combination of YOLOX backbone and pafpn;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOv7 with Res2Net-v1d backbone, we &lt;strong&gt;found res2net-v1d&lt;/strong&gt; have a better accuracy then darknet53;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added PPYOLOv2 PAN neck with SPP and dropblock;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; YOLOX arch added, now you can train YOLOX model (&lt;strong&gt;anchor free yolo&lt;/strong&gt;) as well;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; DETR: transformer based detection model and &lt;strong&gt;onnx export supported, as well as TensorRT acceleration&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AnchorDETR: Faster converge version of detr, now supported!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Almost all models can export to onnx;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supports TensorRT deployment for DETR and other transformer models;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; It will integrate with &lt;a href=&#34;https://github.com/jinfagang/wanwu_release&#34;&gt;wanwu&lt;/a&gt;, a torch-free deploy framework run fastest on your target platform.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è Important note: &lt;strong&gt;YOLOv7 on Github not the latest version, many features are closed-source but you can get it from &lt;a href=&#34;https://manaai.cn&#34;&gt;https://manaai.cn&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Features are ready but not opensource yet:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Convnext training on YOLOX, higher accuracy than original YOLOX;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GFL loss support;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;MobileVit-V2&lt;/strong&gt; backbone available;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; CSPRep-Resnet: a repvgg style resnet used in PP-YOLOE but in pytorch rather than paddle;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; VitDet support;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Simple-FPN support from VitDet;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; PP-YOLOE head supported;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want get full version YOLOv7, either &lt;strong&gt;become a contributor&lt;/strong&gt; or get from &lt;a href=&#34;https://manaai.cn&#34;&gt;https://manaai.cn&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;üÜï News!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.06.25&lt;/strong&gt;&lt;/em&gt;: Meituan&#39;s YOLOv6 training has been supported in YOLOv7!&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.06.13&lt;/strong&gt;&lt;/em&gt;: New model &lt;strong&gt;YOLOX-Convnext-tiny&lt;/strong&gt; got a &lt;del&gt;41.3&lt;/del&gt; 43 mAP beats yolox-s, AP-small even higher!;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.06.09&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;GFL&lt;/strong&gt;, general focal loss supported;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.05.26&lt;/strong&gt;&lt;/em&gt;: Added &lt;strong&gt;YOLOX-ConvNext&lt;/strong&gt; config;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.05.18&lt;/strong&gt;&lt;/em&gt;: DINO and DABDetr are about added, new records on coco up to 63.3 AP!&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.05.09&lt;/strong&gt;&lt;/em&gt;: Big new function added! &lt;strong&gt;We adopt YOLOX with Keypoints Head!&lt;/strong&gt;, model still under train, but you can check at code already;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.04.23&lt;/strong&gt;&lt;/em&gt;: We finished the int8 quantization on SparseInst! It works perfect! Download the onnx try it our by your self.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.04.15&lt;/strong&gt;&lt;/em&gt;: Now, we support the &lt;code&gt;SparseInst&lt;/code&gt; onnx expport!&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2022.03.25&lt;/strong&gt;&lt;/em&gt;: New instance seg supported! 40 FPS @ 37 mAP!! Which is fast;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2021.09.16&lt;/strong&gt;&lt;/em&gt;: First transformer based DETR model added, will explore more DETR series models;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2021.08.02&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;YOLOX&lt;/strong&gt; arch added, you can train YOLOX as well in this repo;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2021.07.25&lt;/strong&gt;&lt;/em&gt;: We found &lt;strong&gt;YOLOv7-Res2net50&lt;/strong&gt; beat res50 and darknet53 at same speed level! 5% AP boost on custom dataset;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2021.07.04&lt;/strong&gt;&lt;/em&gt;: Added YOLOF and we can have a anchor free support as well, YOLOF achieves a better trade off on speed and accuracy;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;2021.06.25&lt;/strong&gt;&lt;/em&gt;: this project first started.&lt;/li&gt; &#xA; &lt;li&gt;more&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåπ Contribution Wanted&lt;/h2&gt; &#xA;&lt;p&gt;If you have spare time or if you have GPU card, then help YOLOv7 become more stronger! Here is the guidance of contribute:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Claim task&lt;/code&gt;&lt;/strong&gt;: I have some ideas but do not have enough time to do it, if you want implement it, claim the task, &lt;strong&gt;I will give u fully advise on how to do, and you can learn a lot from it&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Test mAP&lt;/code&gt;&lt;/strong&gt;: When you finished new idea implementation, create a thread to report experiment mAP, if it work, then merge into our main master branch;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Pull request&lt;/code&gt;&lt;/strong&gt;: YOLOv7 is open and always tracking on SOTA and &lt;strong&gt;light&lt;/strong&gt; models, if a model is useful, we will merge it and deploy it, distribute to all users want to try.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here are some tasks need to be claimed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; VAN: Visual Attention Network, &lt;a href=&#34;https://arxiv.org/abs/2202.09741&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/Visual-Attention-Network/VAN-Segmentation&#34;&gt;VAN-Segmentation&lt;/a&gt;, it was better than Swin and PVT and DeiT: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; D2 VAN backbone integration;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test with YOLOv7 arch;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ViDet: &lt;a href=&#34;https://github.com/naver-ai/vidt&#34;&gt;code&lt;/a&gt;, this provides a realtime detector based on transformer, Swin-Nano mAP: 40, while 20 FPS, it can be integrated into YOLOv7; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integrate into D2 backbone, remove MSAtten deps;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test with YOLOv7 or DETR arch;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; DINO: 63.3mAP highest in 2022 on coco. &lt;a href=&#34;https://github.com/IDEACVR/DINO&#34;&gt;https://github.com/IDEACVR/DINO&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; waiting for DINO opensource code.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ConvNext: &lt;a href=&#34;https://github.com/facebookresearch/ConvNeXt&#34;&gt;https://github.com/facebookresearch/ConvNeXt&lt;/a&gt;, combined convolution and transformer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; NASVit: &lt;a href=&#34;https://github.com/facebookresearch/NASViT&#34;&gt;https://github.com/facebookresearch/NASViT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; MobileVIT: &lt;a href=&#34;https://github.com/apple/ml-cvnets/raw/main/cvnets/models/classification/mobilevit.py&#34;&gt;https://github.com/apple/ml-cvnets/blob/main/cvnets/models/classification/mobilevit.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; DAB-DETR: &lt;a href=&#34;https://github.com/IDEA-opensource/DAB-DETR&#34;&gt;https://github.com/IDEA-opensource/DAB-DETR&lt;/a&gt;, WIP&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/jahongir7174/EfficientNetV2&#34;&gt;https://github.com/jahongir7174/EfficientNetV2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Just join our in-house contributor plan, you can share our newest code with your contribution!&lt;/p&gt; &#xA;&lt;h2&gt;üíÅ‚Äç‚ôÇÔ∏è Results&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;YOLOv7 Instance&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Face &amp;amp; Detection&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/09/08/hHPhUx.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/19/WGVhlj.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/09/08/hHP7xe.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/22/WDr5V0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://s1.ax1x.com/2022/03/25/qN5zp6.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/03/25/MBwq9YT7zC5Sd1A.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://s1.ax1x.com/2022/05/09/OJnXjI.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://s1.ax1x.com/2022/05/09/OJuuUU.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jinfagang/public_images/master/20220613110908.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jinfagang/public_images/master/20220613111122.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jinfagang/public_images/master/20220613111139.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jinfagang/public_images/master/20220613111239.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üßë‚Äçü¶Ø Installation &amp;amp;&amp;amp; Quick Start&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/docs/install.md&#34;&gt;docs/install.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Special requirements (other version may also work, but these are tested, with best performance, including ONNX export best support):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;torch 1.11 (stable version)&lt;/li&gt; &#xA; &lt;li&gt;onnx&lt;/li&gt; &#xA; &lt;li&gt;onnx-simplifier 0.3.7&lt;/li&gt; &#xA; &lt;li&gt;alfred-py latest&lt;/li&gt; &#xA; &lt;li&gt;detectron2 latest&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you using lower version torch, onnx exportation might not work as our expected.&lt;/p&gt; &#xA;&lt;h2&gt;ü§î Features&lt;/h2&gt; &#xA;&lt;p&gt;Some highlights of YOLOv7 are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A simple and standard training framework for any detection &amp;amp;&amp;amp; instance segmentation tasks, based on detectron2;&lt;/li&gt; &#xA; &lt;li&gt;Supports DETR and many transformer based detection framework out-of-box;&lt;/li&gt; &#xA; &lt;li&gt;Supports easy to deploy pipeline thought onnx.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;This is the only framework support YOLOv4 + InstanceSegmentation&lt;/strong&gt; in single stage style;&lt;/li&gt; &#xA; &lt;li&gt;Easily plugin into transformers based detector;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are strongly recommend you send PR if you have any further development on this project, &lt;strong&gt;the only reason for opensource it is just for using community power to make it stronger and further&lt;/strong&gt;. It&#39;s very welcome for anyone contribute on any features!&lt;/p&gt; &#xA;&lt;h2&gt;üßô‚Äç‚ôÇÔ∏è Pretrained Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;input&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;aug&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/sparse_inst_r50_base.yaml&#34;&gt;SparseInst&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;R-50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/12RQLHD5EZKIOvlqW3avUCeYjFG1NPKDy/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/sparse_inst_r50vd_base.yaml&#34;&gt;SparseInst&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth&#34;&gt;R-50-vd&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/sparse_inst_r50_giam.yaml&#34;&gt;SparseInst (G-IAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;R-50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úò&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1pXU7Dsa1L7nUiLU9ULG2F6Pl5m5NEguL/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/sparse_inst_r50_giam_aug.yaml&#34;&gt;SparseInst (G-IAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;R-50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1MK8rO3qtA7vN9KVSBdp0VvZHCNq8-bvz/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/sparse_inst_r50_dcn_giam_aug.yaml&#34;&gt;SparseInst (G-IAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;&#34;&gt;R-50-DCN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1qxdLRRHbIWEwRYn-NPPeCCk6fhBjc946/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/sparse_inst_r50vd_giam_aug.yaml&#34;&gt;SparseInst (G-IAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth&#34;&gt;R-50-vd&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1dlamg7ych_BdWpPUCuiBXbwE0SXpsfGx/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/sparse_inst_r50vd_dcn_giam_aug.yaml&#34;&gt;SparseInst (G-IAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth&#34;&gt;R-50-vd-DCN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;608&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1clYPdCNrDNZLbmlAEJ7wjsrOLn1igOpT/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/sparse_inst_r50vd_dcn_giam_aug.yaml&#34;&gt;SparseInst (G-IAM)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth&#34;&gt;R-50-vd-DCN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1clYPdCNrDNZLbmlAEJ7wjsrOLn1igOpT/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;SparseInst Int8 onnx&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1FYn_0p3RXzKaTGzTfdiJI1YhAexA_V3s/view?usp=sharing&#34;&gt;google drive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üßô‚Äç‚ôÇÔ∏è Models trained in YOLOv7&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;backbone&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;input&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;aug&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP50&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;APs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FPS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/coco/yolotr/yolotr_convnext.yaml&#34;&gt;YoloFormer-Convnext-tiny&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth&#34;&gt;Convnext-tiny&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;800&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1bTedWQaENvlFknqyQreBKA1HoAMOtHkn/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jinfagang/yolov7/main/configs/coco/yolox_s.yaml&#34;&gt;YOLOX-s&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;800&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1clYPdCNrDNZLbmlAEJ7wjsrOLn1igOpT/view?usp=sharing&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;note: We post AP-s here because we want to know how does small object performance in related model, it was notablely higher small-APs for transformer backbone based model! &lt;strong&gt;Some of above model might not opensourced but we provide weights&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ü•∞ Demo&lt;/h2&gt; &#xA;&lt;p&gt;Run a quick demo would be like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 demo.py --config-file configs/wearmask/darknet53.yaml --input ./datasets/wearmask/images/val2017 --opts MODEL.WEIGHTS output/model_0009999.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run SparseInst:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py --config-file configs/coco/sparseinst/sparse_inst_r50vd_giam_aug.yaml --video-input ~/Movies/Videos/86277963_nb2-1-80.flv -c 0.4 --opts MODEL.WEIGHTS weights/sparse_inst_r50vd_giam_aug_8bc5b3.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;an update based on detectron2 newly introduced LazyConfig system, run with a LazyConfig model using&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 demo_lazyconfig.py --config-file configs/new_baselines/panoptic_fpn_regnetx_0.4g.py --opts train.init_checkpoint=output/model_0004999.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üòé Train&lt;/h2&gt; &#xA;&lt;p&gt;For training, quite simple, same as detectron2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py --config-file configs/coco/darknet53.yaml --num-gpus 8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want train YOLOX, you can using config file &lt;code&gt;configs/coco/yolox_s.yaml&lt;/code&gt;. All support arch are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;YOLOX&lt;/strong&gt;: anchor free yolo;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;YOLOv7&lt;/strong&gt;: traditional yolo with some explorations, mainly focus on loss experiments;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;YOLOv7P&lt;/strong&gt;: traditional yolo merged with decent arch from YOLOX;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;YOLOMask&lt;/strong&gt;: arch do detection and segmentation at the same time (tbd);&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;YOLOInsSeg&lt;/strong&gt;: instance segmentation based on YOLO detection (tbd);&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üòé Rules&lt;/h2&gt; &#xA;&lt;p&gt;There are some rules you must follow to if you want train on your own dataset:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rule No.1: Always set your own anchors on your dataset, using &lt;code&gt;tools/compute_anchors.py&lt;/code&gt;, this applys to any other anchor-based detection methods as well (EfficientDet etc.);&lt;/li&gt; &#xA; &lt;li&gt;Rule No.2: Keep a faith on your loss will goes down eventually, if not, dig deeper to find out why (but do not post issues repeated caused I might don&#39;t know either.).&lt;/li&gt; &#xA; &lt;li&gt;Rule No.3: No one will tells u but it&#39;s real: &lt;em&gt;do not change backbone easily, whole params coupled with your backbone, dont think its simple as you think it should be&lt;/em&gt;, also a Deeplearning engineer &lt;strong&gt;is not an easy work as you think&lt;/strong&gt;, the whole knowledge like an ocean, and your knowledge is just a tiny drop of water...&lt;/li&gt; &#xA; &lt;li&gt;Rule No.4: &lt;strong&gt;must&lt;/strong&gt; using pretrain weights for &lt;strong&gt;transoformer based backbone&lt;/strong&gt;, otherwise your loss will bump;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure you have read &lt;strong&gt;rules&lt;/strong&gt; before ask me any questions.&lt;/p&gt; &#xA;&lt;h2&gt;üî® Export ONNX &amp;amp;&amp;amp; TensorRTT &amp;amp;&amp;amp; TVM&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;detr&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python export_onnx.py --config-file detr/config/file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;this works has been done, inference script included inside &lt;code&gt;tools&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;code&gt;AnchorDETR&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;anchorDETR also supported training and exporting to ONNX.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;code&gt;SparseInst&lt;/code&gt;: Sparsinst already supported exporting to onnx!!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python export_onnx.py --config-file configs/coco/sparseinst/sparse_inst_r50_giam_aug.yaml --video-input ~/Videos/a.flv  --opts MODEL.WEIGHTS weights/sparse_inst_r50_giam_aug_2b7d68.pth INPUT.MIN_SIZE_TEST 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on a CPU device, please using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python export_onnx.py --config-file configs/coco/sparseinst/sparse_inst_r50_giam_aug.yaml --input images/COCO_val2014_000000002153.jpg --verbose  --opts MODEL.WEIGHTS weights/sparse_inst_r50_giam_aug_2b7d68.pth MODEL.DEVICE &#39;cpu&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can have &lt;code&gt;weights/sparse_inst_r50_giam_aug_2b7d68_sim.onnx&lt;/code&gt; generated, this onnx can be inference using ORT without any unsupported ops.&lt;/p&gt; &#xA;&lt;h2&gt;ü§íÔ∏è Performance&lt;/h2&gt; &#xA;&lt;p&gt;Here is a dedicated performance compare with other packages.&lt;/p&gt; &#xA;&lt;p&gt;tbd.&lt;/p&gt; &#xA;&lt;h2&gt;ü™ú Some Tiny Object Datasets supported&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wearmask&lt;/strong&gt;: support VOC, Yolo, coco 3 format. You can using coco format here. Download from: ÈìæÊé•: &lt;a href=&#34;https://pan.baidu.com/s/1ozAgUFLqfTXLp-iOecddqQ&#34;&gt;https://pan.baidu.com/s/1ozAgUFLqfTXLp-iOecddqQ&lt;/a&gt; ÊèêÂèñÁ†Å: xgep . Using &lt;code&gt;configs/wearmask&lt;/code&gt; to train this dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;more&lt;/strong&gt;: to go.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üëã Detection Results&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Image&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Detections&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/22/WDs9PO.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/22/WDr5V0.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/19/WGVhlj.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/26/WWBxi9.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üòØ Dicussion Group&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Wechat&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;QQ&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/03/14/9uxaEnDA6vdByr2.png&#34; alt=&#34;image.png&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://s2.loli.net/2022/02/28/C4gjf6DcwdHvnO8.png&#34; alt=&#34;image.png&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;if wechat expired, please contact me update via github issue. group for general discussion, not only for yolov7.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üÄÑÔ∏è Some Exp Visualizations&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GridMask&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Mosaic&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/06/27/RYeJkd.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/06/RIX1iR.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/06/Roj5dg.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/07/06/Roq97d.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/08/06/futTte.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://z3.ax1x.com/2021/08/06/futv0f.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;¬©Ô∏è License&lt;/h2&gt; &#xA;&lt;p&gt;Code released under GPL license. Please pull request to this source repo before you make your changes public or commercial usage. All rights reserved by Lucas Jin.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>aolofsson/awesome-opensource-hardware</title>
    <updated>2022-07-01T01:32:35Z</updated>
    <id>tag:github.com,2022-07-01:/aolofsson/awesome-opensource-hardware</id>
    <link href="https://github.com/aolofsson/awesome-opensource-hardware" rel="alternate"></link>
    <summary type="html">&lt;p&gt;List of awesome open source hardware projects&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;awesome-opensource-hardware&lt;/h1&gt; &#xA;&lt;p&gt;A curated list of awesome open source hardware tools.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Categorized&lt;/li&gt; &#xA; &lt;li&gt;Alphabetical (per category)&lt;/li&gt; &#xA; &lt;li&gt;Requirements &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;link should be to source code repository&lt;/li&gt; &#xA;   &lt;li&gt;open source projects only&lt;/li&gt; &#xA;   &lt;li&gt;working projects only (not WIP/rusty)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;One tag line sentence per project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Accelerators&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/secworks/aes&#34;&gt;aes&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Symmetric block cipher AES (Advanced Encryption Standard)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pulp-platform/ara&#34;&gt;ara&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Vector Unit, compatible with the RISC-V Vector Extension&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ucb-bar/FFTGenerator&#34;&gt;FFTGenerator&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;MMIO-Based FFT Generator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dawsonjon/fpu&#34;&gt;fpu&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Synthesizable ieee 754 floating point library in verilog&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ucb-bar/gemmini&#34;&gt;gemmini&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Berkeley Spatial Array Generator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/asicguy/gplgpu&#34;&gt;gplgpu&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPL v3 2D/3D graphics engine in verilog&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultraembedded/core_jpeg&#34;&gt;core_jpeg&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;High throughput JPEG decoder in Verilog for FPGA&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openasic-org/h265-encoder-rtl&#34;&gt;h265-encoder-rtl&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;H.265 Video Encoder IP Core&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nvdla/hw&#34;&gt;nvdla&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NVIDIA Deep Learning Accelerator (NVDLA)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jbush001/NyuziProcessor&#34;&gt;NyuziProcessor&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPGPU microprocessor architecture&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jhshi/openofdm&#34;&gt;openofdm&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;802.11 OFDM PHY decoder&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ucb-bar/sha3&#34;&gt;sha3&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Berkeley SHAR3 ROCC Accelerator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/apache/tvm-vta&#34;&gt;tvm-vta&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Opwn, modular, deep learning accelerator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/VeriGOOD-ML/public&#34;&gt;VeriGOOD-ML&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verilog Generator, Optimized for Designs for Machine Learning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hughperkins/VeriGPU&#34;&gt;VeriGPU&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenSource GPU, loosely based on RISC-V ISA&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexforencich/verilog-lfsr&#34;&gt;verilog-lfsr&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parametrizable combinatorial parallel LFSR/CRC module&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vortexgpgpu/vortex&#34;&gt;vortex&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Full-system RISCV-based GPGPU processor&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Analog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/USCPOSH/AMS_KGD&#34;&gt;AMS_KGD&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Repository for Known Good Analog Designs (KGDs)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/westonb/open-pmic&#34;&gt;open-pmic&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Current mode buck converter on the SKY130 PDK&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mabrains/Analog_blocks&#34;&gt;Analog Basic Blocks/LDO&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Repo that has designs with the following: OTA, BandGap and LDO design on Skywaters 130nm.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Boards&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/parallella/parallella-hw&#34;&gt;parallella-hw&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parallella board design files&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Connectivity&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chipsalliance/aib-phy-hardware&#34;&gt;aib&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Advanced Interface Bus (AIB) die to die hardware&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chipsalliance/aib-protocols&#34;&gt;aib-protocols&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Advanced Interface Bus (AIB) Protocol IP&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pulp-platform/axi&#34;&gt;axi&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;AXI SystemVerilog synthesizable IP&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lmco/axi4_aib_bridge&#34;&gt;axi4_aib_bridge&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;AXI4/AIB Bridge RTL&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultraembedded/core_ddr3_controller&#34;&gt;core_ddr3_controller&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DDR3 memory controller in Verilog for various FPGAs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hdl-util/hdmi&#34;&gt;hdmi&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Send video/audio over HDMI on an FPGA&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hdl-util/i2c&#34;&gt;i2c&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fully featured implementation of Inter-IC (I2C) bus master&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enjoy-digital/litedram&#34;&gt;litedram&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Small footprint and configurable DRAM (litex)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enjoy-digital/liteeth&#34;&gt;liteeth&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Small footprint and configurable Ethernet core&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enjoy-digital/litescope&#34;&gt;litescope&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Small footprint and configurable embedded FPGA logic analyzer&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enjoy-digital/litepcie&#34;&gt;litepice&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Small footprint and configurable PCIe core&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SparcLab/OpenSERDES&#34;&gt;OpenSERDES&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Digitally synthesizable architecture for SerDes using Skywater130&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aignacio/ravenoc&#34;&gt;ravenoc&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Configurable HDL NoC (Network-On-Chip)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/taichi-ishitani/tnoc&#34;&gt;tnoc&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Network on Chip Implementation written in SytemVerilog&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexforencich/verilog-axis&#34;&gt;verilog-axis&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verilog AXI stream components for FPGA implementation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexforencich/verilog-ethernet&#34;&gt;verilog-ethernet&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verilog Ethernet components for FPGA implementation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexforencich/verilog-i2c&#34;&gt;verilog-i2c&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verilog I2C interface for FPGA implementation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexforencich/verilog-uart&#34;&gt;verilog-uart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verilog UART&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexforencich/verilog-pcie&#34;&gt;verilog-pcie&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verilog PCI express components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexforencich/verilog-wishbone&#34;&gt;verilog-wishbone&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Verilog wishbone components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/waviousllc/wav-d2d-hw&#34;&gt;wav-d2d-hw&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;8lane Wlink with D2D and a single AXI Target/Initiator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/waviousllc/wav-lpddr-hw&#34;&gt;wav-lpddr-hw&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DDR (WDDR) Physical interface (PHY) Hardware&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/waviousllc/wav-slink-hw&#34;&gt;wav-slink-hw&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Chiplet link&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/waviousllc/wav-wlink-hw&#34;&gt;wav-wlink-hw&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Chiplet link&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;CPU cores&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openpower-cores/a2i&#34;&gt;a2i&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A2I POWER processor core RTL (VHDL)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/black-parrot/black-parrot&#34;&gt;black-parrot&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux-capable RISC-V multicore&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chipsalliance/Cores-SweRV&#34;&gt;Cores-SweRV&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SweRV EH1 RISC-Vcore&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chipsalliance/Cores-SweRV-EL2&#34;&gt;Cores-SweRV-EL2&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SweRV EL2 RISC-V Core&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openhwgroup/core-v-verif&#34;&gt;core-v-verif&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Functional verification project for the CORE-V family of RISC-V cores&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openhwgroup/cva6&#34;&gt;cva6&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux capable RISC-V CPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openhwgroup/cv32e40p&#34;&gt;cv32e40p&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RV32IMFCX RISC-V 4-stage RISC-V CPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lowRISC/ibex&#34;&gt;ibex&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Small 32 bit RISC-V CPU core&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenXiangShan/HuanCun&#34;&gt;HuanCun&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open-source high-performance non-blocking cache&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antonblanchard/microwatt&#34;&gt;microwatt&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open POWER ISA softcore written in VHDL 2008&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stnolting/neorv32&#34;&gt;neorv32&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Customizable and highly extensible MCU-class 32-bit RISC-V (VHDL)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenXiangShan/XiangShan&#34;&gt;OpenXiangShan&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open-source high-performance RISC-V processor&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/YosysHQ/picorv32&#34;&gt;picorv32&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Size-Optimized RISC-V CPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chipsalliance/rocket-chip&#34;&gt;rocket-chip&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux capable RISC-V Rocket Chip Generator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/olofk/serv&#34;&gt;serv&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SErial RISC-V CPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pulp-platform/snitch&#34;&gt;snitch&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Lean but mean RISC-V system&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FPGAs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FPGA-Research-Manchester/FABulous&#34;&gt;FABulous&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fabric generator and CAD tools&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ucb-cs250/fabric_team&#34;&gt;fabric_team&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ucb-cs250 FPGA class project&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lnis-uofu/OpenFPGA&#34;&gt;OpenFPGA&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FPGA IP Generator&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PrincetonUniversity/prga&#34;&gt;prga&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open-source FPGA research and prototyping framework&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Libraries&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pConst/basic_verilog&#34;&gt;basic_verilog&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Library of SystemVerilog components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pulp-platform/common_cells&#34;&gt;common_cells&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Library of SystemVerilog components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/analogdevicesinc/hdl&#34;&gt;hdl&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Library of Analog Deveices specific components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aolofsson/oh&#34;&gt;oh&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Library of Verilog components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bespoke-silicon-group/basejump_stl&#34;&gt;basejump_stl&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Library of SystemVerilog components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Memory&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultraembedded/core_axi_cache&#34;&gt;core_axi_cache&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;128KB AXI cache (32-bit in, 256-bit out)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Packaging&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bespoke-silicon-group/bsg_packaging&#34;&gt;bsg_packaging&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open-Source Hardware Accelerator Packages and Sockets&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Retro&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zephray/VerilogBoy&#34;&gt;VerilogBoy&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Game Boy compatible machine with Verilog&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Systems&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jks-prv/Beagle_SDR_GPS&#34;&gt;Beagle_SDR_GPS&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;KiwiSDR: BeagleBone web-accessible GPS/SDR&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bespoke-silicon-group/bsg_manycore&#34;&gt;bsg_manycore&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tile based architecture designed for computing efficiency, scalability&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sld-columbia/esp&#34;&gt;esp&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Heterogeneous SoC architecture and IP design platform&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pulp-platform/hero&#34;&gt;hero&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;FPGA-based research platform for heterogeneous design&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/enjoy-digital/litex&#34;&gt;litex&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;SoC builder framework&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/idea-fasoc/OpenFASOC&#34;&gt;openFASOC&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open Source FASOC generators&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PrincetonUniversity/openpiton&#34;&gt;openpiton&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;General purpose, multithreaded manycore processor&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lowRISC/opentitan&#34;&gt;opentitan&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Open source silicon root of trust&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-sdr/openwifi-hw&#34;&gt;openwifi-hw&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;IEEE 802.11 WiFi baseband FPGA (chip) design&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pulp-platform/pulp&#34;&gt;pulp&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multicore RISC-V based SoC&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pulp-platform/pulpissimo&#34;&gt;pulpissimo&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single core RISC-V based SoC&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>kuprel/min-dalle</title>
    <updated>2022-07-01T01:32:35Z</updated>
    <id>tag:github.com,2022-07-01:/kuprel/min-dalle</id>
    <link href="https://github.com/kuprel/min-dalle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;min(DALL¬∑E) is a minimal implementation of DALL¬∑E Mini in PyTorch&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;min(DALL¬∑E)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/kuprel/min-dalle/blob/main/min_dalle.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://replicate.com/kuprel/min-dalle&#34;&gt;&lt;img src=&#34;https://replicate.com/kuprel/min-dalle/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a minimal implementation of Boris Dayma&#39;s &lt;a href=&#34;https://github.com/borisdayma/dalle-mini&#34;&gt;DALL¬∑E Mini&lt;/a&gt;. It has been stripped to the bare essentials necessary for doing inference, and converted to PyTorch. To run the torch model, the only third party dependencies are numpy and torch. Flax is used to convert the weights (which are saved with &lt;code&gt;torch.save&lt;/code&gt; the first time the model is loaded), and wandb is only used to download the models.&lt;/p&gt; &#xA;&lt;p&gt;It currently takes &lt;strong&gt;7.4 seconds&lt;/strong&gt; to generate an image with DALL¬∑E Mega with PyTorch on a standard GPU runtime in Colab&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;Run &lt;code&gt;sh setup.sh&lt;/code&gt; to install dependencies and download pretrained models. The models can also be downloaded manually here: &lt;a href=&#34;https://huggingface.co/dalle-mini/vqgan_imagenet_f16_16384&#34;&gt;VQGan&lt;/a&gt;, &lt;a href=&#34;https://wandb.ai/dalle-mini/dalle-mini/artifacts/DalleBart_model/mini-1/v0/files&#34;&gt;DALL¬∑E Mini&lt;/a&gt;, &lt;a href=&#34;https://wandb.ai/dalle-mini/dalle-mini/artifacts/DalleBart_model/mega-1-fp16/v14/files&#34;&gt;DALL¬∑E Mega&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Use the python script &lt;code&gt;image_from_text.py&lt;/code&gt; to generate images from the command line. Note: the command line script loads the models and parameters each time. To load a model once and generate multiple times, initialize either &lt;code&gt;MinDalleTorch&lt;/code&gt; or &lt;code&gt;MinDalleFlax&lt;/code&gt;, then call &lt;code&gt;generate_image&lt;/code&gt; with some text and a seed. See the colab for an example.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python image_from_text.py --text=&#39;artificial intelligence&#39; --torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuprel/min-dalle/main/examples/artificial_intelligence.png&#34; alt=&#34;Alien&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python image_from_text.py --text=&#39;a comfy chair that looks like an avocado&#39; --torch --mega --seed=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuprel/min-dalle/main/examples/avocado_armchair.png&#34; alt=&#34;Avocado Armchair&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python image_from_text.py --text=&#39;court sketch of godzilla on trial&#39; --mega --seed=100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kuprel/min-dalle/main/examples/godzilla_trial.png&#34; alt=&#34;Godzilla Trial&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>