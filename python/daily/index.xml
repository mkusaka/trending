<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-17T01:33:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/UFO</title>
    <updated>2024-02-17T01:33:05Z</updated>
    <id>tag:github.com,2024-02-17:/microsoft/UFO</id>
    <link href="https://github.com/microsoft/UFO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A UI-Focused Agent for Windows OS Interaction.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;b&gt;UFO&lt;/b&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/ufo_blue.png&#34; alt=&#34;UFO Image&#34; width=&#34;40&#34;&gt;: A &lt;b&gt;U&lt;/b&gt;I-&lt;b&gt;F&lt;/b&gt;ocused Agent for Windows &lt;b&gt;O&lt;/b&gt;S Interaction &lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3776AB?&amp;amp;logo=python&amp;amp;logoColor=white-blue&amp;amp;label=3.10%20%7C%203.11&#34; alt=&#34;Python Version&#34;&gt;‚ÄÇ &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;‚ÄÇ &lt;img src=&#34;https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&#34; alt=&#34;Welcome&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;UFO&lt;/strong&gt; is a &lt;strong&gt;UI-Focused&lt;/strong&gt; dual-agent framework to fulfill user requests on &lt;strong&gt;Windows OS&lt;/strong&gt; by seamlessly navigating and operating within individual or spanning multiple applications.&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/overview_n.png&#34;&gt; &lt;/h1&gt; &#xA;&lt;h2&gt;üïå Framework&lt;/h2&gt; &#xA;&lt;p&gt;&lt;b&gt;UFO&lt;/b&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/ufo_blue.png&#34; alt=&#34;UFO Image&#34; width=&#34;24&#34;&gt; operates as a dual-agent framework, encompassing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;b&gt;AppAgent ü§ñ&lt;/b&gt;, tasked with choosing an application for fulfilling user requests. This agent may also switch to a different application when a request spans multiple applications, and the task is partially completed in the preceding application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;ActAgent üëæ&lt;/b&gt;, responsible for iteratively executing actions on the selected applications until the task is successfully concluded within a specific application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;b&gt;Control Interaction üéÆ&lt;/b&gt;, is tasked with translating actions from AppAgent and ActAgent into interactions with the application and its UI controls. It&#39;s essential that the targeted controls are compatible with the Windows &lt;strong&gt;UI Automation&lt;/strong&gt; API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Both agents leverage the multi-modal capabilities of GPT-Vision to comprehend the application UI and fulfill the user&#39;s request. For more details, please consult our &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/framework.png&#34;&gt; &lt;/h1&gt; &#xA;&lt;h2&gt;üÜï News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìÖ 2024-02-14: Our &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;technical report&lt;/a&gt; is online!&lt;/li&gt; &#xA; &lt;li&gt;üìÖ 2024-02-10: UFO is released on GitHubüéà. Happy Chinese New yearüêâ!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí• Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;First Windows Agent&lt;/strong&gt; - UFO is the pioneering agent framework capable of translating user requests in natural language into actionable operations on Windows OS.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Interactive Mode&lt;/strong&gt; - UFO facilitates multiple sub-requests from users within the same session, enabling the completion of complex tasks seamlessly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Action Safeguard&lt;/strong&gt; - UFO incorporates safeguards to prompt user confirmation for sensitive actions, enhancing security and preventing inadvertent operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Easy Extension&lt;/strong&gt; - UFO offers extensibility, allowing for the integration of additional functionalities and control types to tackle diverse and intricate tasks with ease.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ú® Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;üõ†Ô∏è Step 1: Installation&lt;/h3&gt; &#xA;&lt;p&gt;UFO requires &lt;strong&gt;Python &amp;gt;= 3.10&lt;/strong&gt; running on &lt;strong&gt;Windows OS &amp;gt;= 10&lt;/strong&gt;. It can be installed by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# [optional to create conda environment]&#xA;# conda create -n ufo python=3.10&#xA;# conda activate ufo&#xA;&#xA;# clone the repository&#xA;git clone https://github.com/microsoft/UFO.git&#xA;cd UFO&#xA;# install the requirements&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;‚öôÔ∏è Step 2: Configure the LLMs&lt;/h3&gt; &#xA;&lt;p&gt;Before running UFO, you need to provide your LLM configurations. Taking OpenAI as an example, you can configure &lt;code&gt;ufo/config/config.yaml&lt;/code&gt; file as follows.&lt;/p&gt; &#xA;&lt;h4&gt;OpenAI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;API_TYPE: &#34;openai&#34; &#xA;OPENAI_API_BASE: &#34;https://api.openai.com/v1/chat/completions&#34; # The base URL for the OpenAI API&#xA;OPENAI_API_KEY: &#34;YOUR_API_KEY&#34;  # Set the value to the openai key for the llm model&#xA;OPENAI_API_MODEL: &#34;GPTV_MODEL_NAME&#34;  # The only OpenAI model by now that accepts visual input&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Azure OpenAI (AOAI)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;API_TYPE: &#34;aoai&#34; &#xA;OPENAI_API_BASE: &#34;YOUR_ENDPOINT&#34; # The AOAI API address. Format: https://{your-resource-name}.openai.azure.com/openai/deployments/{deployment-id}/completions?api-version={api-version}&#xA;OPENAI_API_KEY: &#34;YOUR_API_KEY&#34;  # Set the value to the openai key for the llm model&#xA;OPENAI_API_MODEL: &#34;GPTV_MODEL_NAME&#34;  # The only OpenAI model by now that accepts visual input&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üéâ Step 3: Start UFO&lt;/h3&gt; &#xA;&lt;h4&gt;‚å®Ô∏è You can execute the following on your Windows command Line (CLI):&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# assume you are in the cloned UFO folder&#xA;python -m ufo --task &amp;lt;your_task_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start the UFO process and you can interact with it through the command line interface. If everything goes well, you will see the following message:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Welcome to use UFOüõ∏, A UI-focused Agent for Windows OS Interaction. &#xA; _   _  _____   ___&#xA;| | | ||  ___| / _ \&#xA;| | | || |_   | | | |&#xA;| |_| ||  _|  | |_| |&#xA; \___/ |_|     \___/&#xA;Please enter your request to be completedüõ∏:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;‚ö†Ô∏èReminder:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before UFO executing your request, please make sure the targeted applications are active on the system.&lt;/li&gt; &#xA; &lt;li&gt;The GPT-V accepts screenshots of your desktop and application GUI as input. Please ensure that no sensitive or confidential information is visible or captured during the execution process. For further information, refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/DISCLAIMER.md&#34;&gt;DISCLAIMER.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 4 üé•: Execution Logs&lt;/h3&gt; &#xA;&lt;p&gt;You can find the screenshots taken and request &amp;amp; response logs in the following folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./ufo/logs/&amp;lt;your_task_name&amp;gt;/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may use them to debug, replay, or analyze the agent output.&lt;/p&gt; &#xA;&lt;h2&gt;‚ùìGet help&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ùîGitHub Issues (prefered)&lt;/li&gt; &#xA; &lt;li&gt;For other communications, please contact &lt;a href=&#34;mailto:ufo-agent@microsoft.com&#34;&gt;ufo-agent@microsoft.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üé¨ Demo Examples&lt;/h2&gt; &#xA;&lt;p&gt;We present two demo videos that complete user request on Windows OS using UFO. For more case study, please consult our &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;technical report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;1Ô∏è‚É£üóëÔ∏è Example 1: Deleting all notes on a PowerPoint presentation.&lt;/h4&gt; &#xA;&lt;p&gt;In this example, we will demonstrate how to efficiently use UFO to delete all notes on a PowerPoint presentation with just a few simple steps. Explore this functionality to enhance your productivity and work smarter, not harder!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/UFO/assets/11352048/cf60c643-04f7-4180-9a55-5fb240627834&#34;&gt;https://github.com/microsoft/UFO/assets/11352048/cf60c643-04f7-4180-9a55-5fb240627834&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2Ô∏è‚É£üìß Example 2: Composing an email using text from multiple sources.&lt;/h4&gt; &#xA;&lt;p&gt;In this example, we will demonstrate how to utilize UFO to extract text from Word documents, describe an image, compose an email, and send it seamlessly. Enjoy the versatility and efficiency of cross-application experiences with UFO!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/UFO/assets/11352048/aa41ad47-fae7-4334-8e0b-ba71c4fc32e0&#34;&gt;https://github.com/microsoft/UFO/assets/11352048/aa41ad47-fae7-4334-8e0b-ba71c4fc32e0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìä Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please consult the &lt;a href=&#34;https://arxiv.org/pdf/2402.07939.pdf&#34;&gt;WindowsBench&lt;/a&gt; provided in Section A of the Appendix within our technical report. Here are some tips (and requirements) to aid in completing your request:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prior to UFO execution of your request, ensure that the targeted application is active (though it may be minimized).&lt;/li&gt; &#xA; &lt;li&gt;Occasionally, requests to GPT-V may trigger content safety measures. UFO will attempt to retry regardless, but adjusting the size or scale of the application window may prove helpful. We are actively solving this issue.&lt;/li&gt; &#xA; &lt;li&gt;Currently, UFO supports a limited set of applications and UI controls that are compatible with the Windows &lt;strong&gt;UI Automation&lt;/strong&gt; API. Our future plans include extending support to the Win32 API to enhance its capabilities.&lt;/li&gt; &#xA; &lt;li&gt;Please note that the output of GPT-V may not consistently align with the same request. If unsuccessful with your initial attempt, consider trying again.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Citation&lt;/h2&gt; &#xA;&lt;p&gt;Our technical report paper can be found &lt;a href=&#34;https://arxiv.org/abs/2402.07939&#34;&gt;here&lt;/a&gt;. If you use UFO in your research, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{ufo,&#xA;  title={{UFO: A UI-Focused Agent for Windows OS Interaction}},&#xA;  author={Zhang, Chaoyun and Li, Liqun and He, Shilin and  Zhang, Xu and Qiao, Bo and  Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and  Zhang, Qi},&#xA;  journal={arXiv preprint arXiv:2402.07939},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üé® Related Project&lt;/h2&gt; &#xA;&lt;p&gt;You may also find &lt;a href=&#34;https://github.com/microsoft/TaskWeaver?tab=readme-ov-file&#34;&gt;TaskWeaver&lt;/a&gt; useful, a code-first LLM agent framework for seamlessly planning and executing data analytics tasks.&lt;/p&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;By choosing to run the provided code, you acknowledge and agree to the following terms and conditions regarding the functionality and data handling practices in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/DISCLAIMER.md&#34;&gt;DISCLAIMER.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/UFO/main/assets/ufo_blue.png&#34; alt=&#34;logo&#34; width=&#34;30&#34;&gt; Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LargeWorldModel/LWM</title>
    <updated>2024-02-17T01:33:05Z</updated>
    <id>tag:github.com,2024-02-17:/LargeWorldModel/LWM</id>
    <link href="https://github.com/LargeWorldModel/LWM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Large World Model (LWM)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://largeworldmodel.github.io/&#34;&gt;[Project]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.08268&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/LargeWorldModel&#34;&gt;[Models]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large World Model (LWM)&lt;/strong&gt; is a general-purpose large-context multimodal autoregressive model. It is trained on a large dataset of diverse long videos and books using RingAttention, and can perform language, image, and video understanding and generation.&lt;/p&gt; &#xA;&lt;h2&gt;Approach&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/LargeWorldModel/LWM/main/imgs/data.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.&lt;/p&gt; &#xA;&lt;h2&gt;LWM Capabilities&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/LargeWorldModel/LWM/main/imgs/single_needle_1M.png&#34;&gt; &#xA; &lt;p&gt; LWM can retrieval facts across 1M context with high accuracy. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/LargeWorldModel/LWM/main/imgs/long_video_chat_main.png&#34;&gt; &#xA; &lt;p&gt; LWM can answer questions over 1 hour YouTube video. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/LargeWorldModel/LWM/main/imgs/image_chat.png&#34;&gt; &#xA; &lt;p&gt; LWM can chat with images. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/LargeWorldModel/LWM/main/imgs/image_video_gen.png&#34;&gt; &#xA; &lt;p&gt; LWM can generate videos and images from text. &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Install the requirements with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n lwm python=3.10&#xA;pip install -U &#34;jax[cuda12_pip]==0.4.23&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or set up TPU VM with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh tpu_requirements.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Available models&lt;/h2&gt; &#xA;&lt;p&gt;There are language-only and video-language versions, offering context sizes from 32K, to 128K, 256K and 1M tokens. The vision-language models are available only in Jax, and the language-only models are available in both PyTorch and Jax. Below are the names of the available models and their corresponding context sizes and capabilities:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;Context Size&lt;/th&gt; &#xA;   &lt;th&gt;Language or Vision-Language&lt;/th&gt; &#xA;   &lt;th&gt;Chat or Base&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-Chat-128K&lt;/td&gt; &#xA;   &lt;td&gt;128K&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-128K&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-128K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-Chat-256K&lt;/td&gt; &#xA;   &lt;td&gt;256K&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-256K&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-256K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-Chat-512K&lt;/td&gt; &#xA;   &lt;td&gt;512K&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-512K&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-512K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-Chat-1M&lt;/td&gt; &#xA;   &lt;td&gt;1M&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-128K&lt;/td&gt; &#xA;   &lt;td&gt;128K&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-128K&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-128K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-256K&lt;/td&gt; &#xA;   &lt;td&gt;256K&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-256K&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-256K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-512K&lt;/td&gt; &#xA;   &lt;td&gt;512K&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-512K&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-512K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Text-1M&lt;/td&gt; &#xA;   &lt;td&gt;1M&lt;/td&gt; &#xA;   &lt;td&gt;Language&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-1M&#34;&gt;Pytorch&lt;/a&gt;][&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-Text-1M-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Chat-32K&lt;/td&gt; &#xA;   &lt;td&gt;32K&lt;/td&gt; &#xA;   &lt;td&gt;Vision-Language&lt;/td&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-32K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Chat-128K&lt;/td&gt; &#xA;   &lt;td&gt;128K&lt;/td&gt; &#xA;   &lt;td&gt;Vision-Language&lt;/td&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-128K-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LWM-Chat-1M&lt;/td&gt; &#xA;   &lt;td&gt;1M&lt;/td&gt; &#xA;   &lt;td&gt;Vision-Language&lt;/td&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://huggingface.co/LargeWorldModel/LWM-1M-Jax&#34;&gt;Jax&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Code structure&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;scan_query_chunk_size&lt;/code&gt; and &lt;code&gt;scan_key_chunk_size&lt;/code&gt; to control the block size in blockwise compute of the self-attention. Use &lt;code&gt;scan_mlp_chunk_size&lt;/code&gt; to control the block size in blockwise compute of the feedforward network. Use &lt;code&gt;scan_attention=True&lt;/code&gt; and &lt;code&gt;scan_mlp=True&lt;/code&gt; to enable/disable blockwise compute in the self-attention and feed-forward network. Use &lt;code&gt;remat_attention&lt;/code&gt; and &lt;code&gt;remat_mlp&lt;/code&gt; to control the rematerialization policy with &lt;code&gt;nothing_saveable&lt;/code&gt; recommended.&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;mesh_dim=dp, fsdp, tp, sp&lt;/code&gt; to control the degree of parallelism and RingAttention. It is a string of 4 integers separated by commas, representing the number of data parallelism, fully sharded data parallelism, tensor parallelism, and sequence parallelism. For example, &lt;code&gt;mesh_dim=&#39;1,64,4,1&#39;&lt;/code&gt; means 1 data parallelism, 64 fully sharded data parallelism, 4 tensor parallelism, and 1 sequence parallelism. &lt;code&gt;mesh_dim=&#39;1,1,4,64&#39;&lt;/code&gt; means 1 data parallelism, 1 fully sharded data parallelism, 4 tensor parallelism, and 64 sequence parallelism for RingAttention.&lt;/p&gt; &#xA;&lt;h2&gt;Command-line usage&lt;/h2&gt; &#xA;&lt;p&gt;In this section, we provide instructions on how to run each of the provided scripts. For each script, you may need to fill in your own paths and values in the variables described in the beginning of each script.&lt;/p&gt; &#xA;&lt;p&gt;To run each of the following scripts, use &lt;code&gt;bash &amp;lt;script_name&amp;gt;.sh&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Language model training: &lt;code&gt;bash scripts/run_train_text.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vision-Language model training: &lt;code&gt;bash scripts/run_train_vision_text.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Single Needle Evals (Language Model): &lt;code&gt;bash scripts/run_eval_needle.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multi Needle Evals (Language Model): &lt;code&gt;bash scripts/run_eval_needle_multi.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sampling images (Vision-Language Model): &lt;code&gt;bash scripts/run_sample_image.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sampling videos (Vision-LanguageModel): &lt;code&gt;bash scripts/run_sample_video.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image / Video understanding (Vision-Language Model): &lt;code&gt;bash scripts/run_vision_chat.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;If you have issues&lt;/h2&gt; &#xA;&lt;p&gt;This is based on the &lt;a href=&#34;https://github.com/lhao499/ring-attention&#34;&gt;codebase&lt;/a&gt; of BPT and RingAttention, with the necessary features for vision-language training. The training and inference have been tested on both TPUv3 and TPUv4.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter bugs, please open a GitHub issue!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2023world,&#xA;    title={World Model on Million-Length Video and Language with RingAttention},&#xA;    author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},&#xA;    journal={arXiv preprint},&#xA;    year={2024},&#xA;}&#xA;@article{liu2023ring,&#xA;    title={Ring Attention with Blockwise Transformers for Near-Infinite Context},&#xA;    author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},&#xA;    journal={International Conference on Learning Representations},&#xA;    year={2024}&#xA;}&#xA;@article{liu2023blockwise,&#xA;    title={Blockwise Parallel Transformer for Large Context Models},&#xA;    author={Liu, Hao and Abbeel, Pieter},&#xA;    journal={Advances in neural information processing systems},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;LWM&#39;s code and model weights are released under the Apache 2.0 License. See &lt;a href=&#34;https://github.com/LargeWorldModel/lwm/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for further details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/trt-llm-rag-windows</title>
    <updated>2024-02-17T01:33:05Z</updated>
    <id>tag:github.com,2024-02-17:/NVIDIA/trt-llm-rag-windows</id>
    <link href="https://github.com/NVIDIA/trt-llm-rag-windows" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A developer reference project for creating Retrieval Augmented Generation (RAG) chatbots on Windows using TensorRT-LLM&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄ RAG on Windows using TensorRT-LLM and LlamaIndex ü¶ô&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/NVIDIA/trt-llm-rag-windows/raw/release/1.0/media/rag-demo.gif?raw=true&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This repository showcases a Retrieval-augmented Generation (RAG) pipeline implemented using the &lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;llama_index&lt;/a&gt; library for Windows. The pipeline incorporates the LLaMa 2 13B model, &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/&#34;&gt;TensorRT-LLM&lt;/a&gt;, and the &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;FAISS&lt;/a&gt; vector search library. For demonstration, the dataset consists of thirty recent articles sourced from &lt;a href=&#34;https://www.nvidia.com/en-us/geforce/news/&#34;&gt;NVIDIA Geforce News&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What is RAG? üîç&lt;/h3&gt; &#xA;&lt;p&gt;Retrieval-augmented generation (RAG) for large language models (LLMs) seeks to enhance prediction accuracy by leveraging an external datastore during inference. This approach constructs a comprehensive prompt enriched with context, historical data, and recent or relevant knowledge.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you have the pre-requisites in place:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/&#34;&gt;TensorRT-LLM&lt;/a&gt; for Windows using the instructions &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/raw/release/0.5.0/windows/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you have access to the Llama 2 &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&#34;&gt;repository on Huggingface&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In this project, the LLaMa 2 13B AWQ 4bit quantized model is employed for inference. Before using it, you&#39;ll need to compile a TensorRT Engine specific to your GPU. If you&#39;re using the GeForce RTX 4090 (TensorRT 9.1.0.4 and TensorRT-LLM release 0.5.0), the compiled TRT Engine is available for download &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/models/llama2-13b/files?version=1.2&#34;&gt;here&lt;/a&gt;. For other NVIDIA GPUs or TensorRT versions, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/trt-llm-rag-windows/release/1.0/#building-trt-engine&#34;&gt;instructions&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3 id=&#34;setup&#34;&gt; Setup Steps &lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NVIDIA/trt-llm-rag-windows.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Place the TensorRT engine for LLaMa 2 13B model in the model/ directory&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For GeForce RTX 4090 users: Download the pre-built TRT engine &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/models/llama2-13b/files?version=1.2&#34;&gt;here&lt;/a&gt; and place it in the model/ directory.&lt;/li&gt; &#xA; &lt;li&gt;For other NVIDIA GPU users: Build the TRT engine by following the instructions provided &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/trt-llm-rag-windows/release/1.0/#building-trt-engine&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Acquire the llama tokenizer &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf/tree/main&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download AWQ weights for building the TensorRT engine model.pt &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/models/llama2-13b/files?version=1.2&#34;&gt;here&lt;/a&gt;. (For RTX 4090, use the pregenerated engine provided earlier.)&lt;/li&gt; &#xA; &lt;li&gt;Install the necessary libraries:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Launch the application using the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py --trt_engine_path &amp;lt;TRT Engine folder&amp;gt; --trt_engine_name &amp;lt;TRT Engine file&amp;gt;.engine --tokenizer_dir_path &amp;lt;tokernizer folder&amp;gt; --data_dir &amp;lt;Data folder&amp;gt;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In our case, that will be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py --trt_engine_path model/ --trt_engine_name llama_float16_tp1_rank0.engine --tokenizer_dir_path model/ --data_dir dataset/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: On its first run, this example will persist/cache the data folder in vector library. Any modifications in the data folder won&#39;t take effect until the &#34;storage-default&#34; cache directory is removed from the application directory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Detailed Command References&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py --trt_engine_path &amp;lt;TRT Engine folder&amp;gt; --trt_engine_name &amp;lt;TRT Engine file&amp;gt;.engine --tokenizer_dir_path &amp;lt;tokernizer folder&amp;gt; --data_dir &amp;lt;Data folder&amp;gt;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Arguments&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Details&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--trt_engine_path &amp;lt;&amp;gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory of TensorRT engine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--trt_engine_name &amp;lt;&amp;gt;&lt;/td&gt; &#xA;   &lt;td&gt;Engine file name (e.g. llama_float16_tp1_rank0.engine)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--tokenizer_dir_path &amp;lt;&amp;gt;&lt;/td&gt; &#xA;   &lt;td&gt;HF downloaded model files for tokenizer e.g. &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&#34;&gt;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;--data_dir &amp;lt;&amp;gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory with context data (pdf, txt etc.) e.g. &#34;.\dataset&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3 id=&#34;building-trt-engine&#34;&gt;Building TRT Engine&lt;/h3&gt; &#xA;&lt;p&gt;For RTX 4090 (TensorRT 9.1.0.4 &amp;amp; TensorRT-LLM 0.5.0), a prebuilt TRT engine is provided. For other RTX GPUs or TensorRT versions, follow these steps to build your TRT engine:&lt;/p&gt; &#xA;&lt;p&gt;Download LLaMa 2 13B chat model from &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&#34;&gt;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download LLaMa 2 13B AWQ int4 checkpoints &lt;strong&gt;model.pt&lt;/strong&gt; from &lt;a href=&#34;https://catalog.ngc.nvidia.com/orgs/nvidia/models/llama2-13b/files?version=1.2&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clone the &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/&#34;&gt;TensorRT LLM&lt;/a&gt; repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/NVIDIA/TensorRT-LLM.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to the examples\llama directory and run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python build.py --model_dir &amp;lt;path to llama13_chat model&amp;gt; --quant_ckpt_path &amp;lt;path to model.pt&amp;gt; --dtype float16 --use_gpt_attention_plugin float16 --use_gemm_plugin float16 --use_weight_only --weight_only_precision int4_awq --per_group --enable_context_fmha --max_batch_size 1 --max_input_len 3000 --max_output_len 1024 --output_dir &amp;lt;TRT engine folder&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Adding your own data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This app loads data from the dataset/ directory into the vector store. To add support for your own data, replace the files in the dataset/ directory with your own data. By default, the script uses llamaindex&#39;s SimpleDirectoryLoader which supports text files in several platforms such as .txt, PDF, and so on.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This project requires additional third-party open source software projects as specified in the documentation. Review the license terms of these open source projects before use.&lt;/p&gt;</summary>
  </entry>
</feed>