<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-29T01:31:49Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xinghaochen/TinySAM</title>
    <updated>2023-12-29T01:31:49Z</updated>
    <id>tag:github.com,2023-12-29:/xinghaochen/TinySAM</id>
    <link href="https://github.com/xinghaochen/TinySAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official PyTorch implementation of &#34;TinySAM: Pushing the Envelope for Efficient Segment Anything Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinySAM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;TinySAM: Pushing the Envelope for Efficient Segment Anything Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yihao Chen, Houqiang Li, Yunhe Wang, Xinghao Chen&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;arXiv 2023&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2312.13789&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#citation&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/merve/tinysam&#34;&gt;&lt;code&gt;Hugging Face Demo&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img width=&#34;300&#34; alt=&#34;compare&#34; src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/tinysam_point.gif&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img width=&#34;300&#34; alt=&#34;compare&#34; src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/tinysam_box.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/27&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/merve/tinysam&#34;&gt;Models&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/merve/tinysam&#34;&gt;demo&lt;/a&gt; of TinySAM are now available in Hugging Face. Thanks for &lt;a href=&#34;https://github.com/merveenoyan&#34;&gt;merveenoyan&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/27&lt;/strong&gt;: Pre-trained models and codes of &lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#usage&#34;&gt;Q-TinySAM&lt;/a&gt; (quantized variant) are released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/27&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt; codes for zero-shot instance segmentation task on COCO are released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/22&lt;/strong&gt;: Pre-trained models and codes of TinySAM are released both in &lt;a href=&#34;https://github.com/xinghaochen/TinySAM&#34;&gt;Pytorch&lt;/a&gt; and &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv/TinySAM&#34;&gt;Mindspore&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;We propose a framework to obtain a tiny segment anything model (&lt;strong&gt;TinySAM&lt;/strong&gt;) while maintaining the strong zero-shot performance. We first propose a full-stage knowledge distillation method with online hard prompt sampling strategy to distill a lightweight student model. We also adapt the post-training quantization to the promptable segmentation task and further reducing the computational cost. Moreover, a hierarchical segmenting everything strategy is proposed to accelerate the everything inference by with almost no performance degradation. With all these proposed methods, our TinySAM leads to orders of magnitude computational reduction and pushes the envelope for efficient segment anything task. Extensive experiments on various zero-shot transfer tasks demonstrate the significantly advantageous performance of our TinySAM against counterpart methods.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/framework.png&#34; alt=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;Figure 1: Overall framework and zero-shot results of TinySAM.&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/everything.png&#34; alt=&#34;everything&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;Figure 2: Our hierarchical strategy for everything mode.&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/vis.png&#34; alt=&#34;vis&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;Figure 3: Visualization results of TinySAM.&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.7&lt;/code&gt; and we use &lt;code&gt;torch==1.10.2&lt;/code&gt; and &lt;code&gt;torchvision==0.11.3&lt;/code&gt;. To visualize the results, &lt;code&gt;matplotlib&amp;gt;=3.5.1&lt;/code&gt; is also required.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.7&lt;/li&gt; &#xA; &lt;li&gt;pytorch == 1.10.2&lt;/li&gt; &#xA; &lt;li&gt;torchvision == 0.11.3&lt;/li&gt; &#xA; &lt;li&gt;matplotlib==3.5.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#evaluation&#34;&gt;checkpoints&lt;/a&gt; into the directory of &lt;em&gt;weights&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the demo code for single prompt of point or box.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the demo code for hierarchical segment everything strategy.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo_hierachical_everything.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Run the demo code for quantization inference.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo_quant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We follow the setting of original &lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;SAM&lt;/a&gt; paper and evaluate the zero-shot instance segmentaion on COCO and LVIS dataset. The experiment results are described as followed.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;FLOPs (G)&lt;/th&gt; &#xA;   &lt;th&gt;COCO AP (%)&lt;/th&gt; &#xA;   &lt;th&gt;LVIS AP (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-H&lt;/td&gt; &#xA;   &lt;td&gt;3166&lt;/td&gt; &#xA;   &lt;td&gt;46.5&lt;/td&gt; &#xA;   &lt;td&gt;44.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-L&lt;/td&gt; &#xA;   &lt;td&gt;1681&lt;/td&gt; &#xA;   &lt;td&gt;45.5&lt;/td&gt; &#xA;   &lt;td&gt;43.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-B&lt;/td&gt; &#xA;   &lt;td&gt;677&lt;/td&gt; &#xA;   &lt;td&gt;41.0&lt;/td&gt; &#xA;   &lt;td&gt;40.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td&gt;344&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;   &lt;td&gt;34.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobileSAM&lt;/td&gt; &#xA;   &lt;td&gt;232&lt;/td&gt; &#xA;   &lt;td&gt;41.0&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;TinySAM&lt;/strong&gt; &lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/1.0/tinysam.pth&#34;&gt;[ckpt]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;232&lt;/td&gt; &#xA;   &lt;td&gt;41.9&lt;/td&gt; &#xA;   &lt;td&gt;38.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Q-TinySAM&lt;/strong&gt; &lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/2.0/tinysam_w8a8.pth&#34;&gt;[ckpt]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;   &lt;td&gt;41.3&lt;/td&gt; &#xA;   &lt;td&gt;37.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;First download the detection boxes (&lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/2.0/coco_instances_results_vitdet.json&#34;&gt;&lt;code&gt;coco_instances_results_vitdet.json&lt;/code&gt;&lt;/a&gt;) produced by ViTDet model, as well as the ground-truth instance segmentation labels(&lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/2.0/instances_val2017.json&#34;&gt;&lt;code&gt;instances_val2017.json&lt;/code&gt;&lt;/a&gt;) and put them into &lt;code&gt;eval/json_files&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run the following code to perform evaluation for zero-shot instance segmentation on COCO dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd eval; sh eval_coco.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.419&#xA;Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.683&#xA;Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.436&#xA;Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.260&#xA;Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.456&#xA;Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.583&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.325&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.511&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.532&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.390&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.577&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.671&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank the following projects: &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;MobileSAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/Cream&#34;&gt;TinyViT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{tinysam,&#xA;  title={TinySAM: Pushing the Envelope for Efficient Segment Anything Model},&#xA;  author={Shu, Han and Li, Wenshuo and Tang, Yehui and Zhang, Yiman and Chen, Yihao and Li, Houqiang and Wang, Yunhe and Chen, Xinghao},&#xA;  journal={arXiv preprint arXiv:2312.13789},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under &lt;a rel=&#34;license&#34; href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/License.txt&#34;&gt; Apache License 2.0&lt;/a&gt;. Redistribution and use should follow this license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yangxy/PASD</title>
    <updated>2023-12-29T01:31:49Z</updated>
    <id>tag:github.com,2023-12-29:/yangxy/PASD</id>
    <link href="https://github.com/yangxy/PASD" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.14469&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cg.cs.tsinghua.edu.cn/people/~tyang&#34;&gt;Tao Yang&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, Peiran Ren&lt;sup&gt;1&lt;/sup&gt;, Xuansong Xie&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://www4.comp.polyu.edu.hk/~cslzhang&#34;&gt;Lei Zhang&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;br&gt; &lt;em&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;a href=&#34;https://damo.alibaba.com&#34;&gt;DAMO Academy, Alibaba Group&lt;/a&gt;, Hangzhou, China&lt;/em&gt;&lt;br&gt; &lt;em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;a href=&#34;http://www.comp.polyu.edu.hk&#34;&gt;Department of Computing, The Hong Kong Polytechnic University&lt;/a&gt;, Hong Kong, China&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Our model can do various tasks. Hope you can enjoy it.&lt;/h2&gt; &#xA;&lt;h2&gt;Realistic Image SR&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/frog.gif&#34; width=&#34;390px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/house.gif&#34; width=&#34;390px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Old photo restoration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/629e4da70703193b.gif&#34; width=&#34;390px&#34; height=&#34;520&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/27d38eeb2dbbe7c9.gif&#34; width=&#34;390px&#34; height=&#34;520&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Personalized Stylization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000020x2.gif&#34; width=&#34;390px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000067x2.gif&#34; width=&#34;390px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Colorization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000004x2.gif&#34; width=&#34;390px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000080x2.gif&#34; width=&#34;390px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;(2023-10-20) Add additional noise level via &lt;code&gt;--added_noise_level&lt;/code&gt; and the SR result achieves a great balance between &#34;extremely-detailed&#34; and &#34;over-smoothed&#34;. Very interesting!. You can control the SR&#39;s detail level freely.&lt;/p&gt; &#xA;&lt;p&gt;(2023-10-18) Completely solved the &lt;a href=&#34;https://github.com/yangxy/PASD/issues/16&#34;&gt;issues&lt;/a&gt; by initializing latents with input LR images. Interestingly, the SR results also become much more stable.&lt;/p&gt; &#xA;&lt;p&gt;(2023-10-11) &lt;a href=&#34;https://colab.research.google.com/drive/1lZ_-rSGcmreLCiRniVT973x6JLjFiC-b?usp=sharing&#34;&gt;Colab demo&lt;/a&gt; is now available. Credits to &lt;a href=&#34;https://github.com/MasahideOkada&#34;&gt;Masahide Okada&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;(2023-10-09) Add training dataset.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-28) Add tiled latent to allow upscaling ultra high-resolution images. Please carefully set &lt;code&gt;latent_tiled_size&lt;/code&gt; as well as &lt;code&gt;--decoder_tiled_size&lt;/code&gt; when upscaling large images.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-12) Add Gradio demo.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-11) Upload pre-trained models.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-07) Upload source codes.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/yangxy/PASD.git&#xA;cd PASD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download SD1.5 models from &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;huggingface&lt;/a&gt; and put them into &lt;code&gt;checkpoints/stable-diffusion-v1-5&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prepare training datasets. Please check &lt;code&gt;dataloader/localdataset.py&lt;/code&gt; and &lt;code&gt;dataloader/webdataset.py&lt;/code&gt; carefully and set the paths correctly. We highly recommend to use &lt;code&gt;dataloader/webdataset.py&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our training dataset. &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV2K_train_HR.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200038&amp;amp;Signature=pS4wwrAMm3wdlpU%2BxYKUsOkrgjA%3D&#34;&gt;DIV2K_train_HR&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-0.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200091&amp;amp;Signature=JJBUqbfNoOLnzGp9mFNDFJsUh%2Fk%3D&#34;&gt;DIV8K-0&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-1.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200121&amp;amp;Signature=ou0ooaSaGVtMx0tFN3rZEx236s8%3D&#34;&gt;DIV8K-1&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-2.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200150&amp;amp;Signature=5FQeeqxX%2Fzb9%2FhnwTklz8N34hKI%3D&#34;&gt;DIV8K-2&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-3.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200175&amp;amp;Signature=h08kXBnZ9%2FTFpxU%2F5apBQvVMuic%3D&#34;&gt;DIV8K-3&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-4.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200201&amp;amp;Signature=6wz8PREaNkykhZ%2BAZeoeGO3Jm4Y%3D&#34;&gt;DIV8K-4&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-5.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200226&amp;amp;Signature=uCWbd2jaJsAoYFKtr7nlWQ3WiOY%3D&#34;&gt;DIV8K-5&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/FFHQ_5K.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200266&amp;amp;Signature=GBDovtNCqRR2nNz%2B4UKlmpVfJtE%3D&#34;&gt;FFHQ_5K&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Flickr2K_HR-0.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200304&amp;amp;Signature=vu7uOdZdLB5uSbdBKbsBMnmnjvQ%3D&#34;&gt;Flickr2K_HR-0&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Flickr2K_HR-1.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200331&amp;amp;Signature=9ID3bgjXfR7xt5zUDfNkRG50DQc%3D&#34;&gt;Flickr2K_HR-1&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Flickr2K_HR-2.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200371&amp;amp;Signature=Zww60FOUIX%2Bysg%2FJaaM%2BvdK5ePk%3D&#34;&gt;Flickr2K_HR-2&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_animal.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200402&amp;amp;Signature=QnIJVqzwBITZW%2FNTatRxaKiyjaY%3D&#34;&gt;OST_animal&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_building.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200430&amp;amp;Signature=W9AYnjnoftY8YF2NII6hx9Xf%2B0o%3D&#34;&gt;OST_building&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_grass.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200460&amp;amp;Signature=qoy%2FNAJUxOdVqYb6CpL4gt9aYxo%3D&#34;&gt;OST_grass&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_mountain.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200477&amp;amp;Signature=si3mNDvz2ZxIyoLbyuZmIOvzctE%3D&#34;&gt;OST_mountain&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_plant.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200510&amp;amp;Signature=jz%2BZeVmeoi6Lu9CQHId6XZhyyk8%3D&#34;&gt;OST_plant&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_sky.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200533&amp;amp;Signature=YWdVSXe9gKAVSm2pZgtBr1NQyp4%3D&#34;&gt;OST_sky&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_water.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200558&amp;amp;Signature=%2FGtrRmYFt6xOURPFR836IqXO7Q0%3D&#34;&gt;OST_water&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Unsplash2K.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200614&amp;amp;Signature=SF1XLJp8swA3O2Rr1eYI%2FLCNg2U%3D&#34;&gt;Unsplash2K&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train a PASD.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./train_pasd.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you want to train pasd_light, use &lt;code&gt;--use_pasd_light&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Test PASD.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Download our pre-trained models &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd.zip&#34;&gt;pasd&lt;/a&gt; | &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd_rrdb.zip&#34;&gt;pasd_rrdb&lt;/a&gt; | &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd_light.zip&#34;&gt;pasd_light&lt;/a&gt; | &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd_light_rrdb.zip&#34;&gt;pasd_light_rrdb&lt;/a&gt;, and put them into &lt;code&gt;runs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python test_pasd.py # --use_pasd_light --use_personalized_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please read the arguments in &lt;code&gt;test_pasd.py&lt;/code&gt; carefully. We adopt the tiled vae method proposed by &lt;a href=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111&#34;&gt;multidiffusion-upscaler-for-automatic1111&lt;/a&gt; to save GPU memory.&lt;/p&gt; &#xA;&lt;p&gt;Please try &lt;code&gt;--use_personalized_model&lt;/code&gt; for personalized stylizetion, old photo restoration and real-world SR. Set &lt;code&gt;--conditioning_scale&lt;/code&gt; for different stylized strength.&lt;/p&gt; &#xA;&lt;p&gt;We use personalized models including &lt;a href=&#34;https://civitai.com/models/43331/&#34;&gt;majicMIX realistic&lt;/a&gt;(for SR and restoration), &lt;a href=&#34;https://civitai.com/models/30240/&#34;&gt;ToonYou&lt;/a&gt;(for stylization) and &lt;a href=&#34;https://huggingface.co/nitrosocke/mo-di-diffusion&#34;&gt;modern disney style&lt;/a&gt;(&lt;code&gt;unet&lt;/code&gt; only, for stylization). You can download more from communities and put them into &lt;code&gt;checkpoints/personalized_models&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If the default setting does not yield good results, try different &lt;code&gt;--pasd_model_path&lt;/code&gt;, &lt;code&gt;--seed&lt;/code&gt;, &lt;code&gt;--prompt&lt;/code&gt;, &lt;code&gt;--upscale&lt;/code&gt;, or &lt;code&gt;--high_level_info&lt;/code&gt; to get better performance.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio Demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_pasd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work is useful for your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{yang2023pasd,&#xA;    title={Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization},&#xA;    author={Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang},&#xA;    booktitle={Arxiv},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;© Alibaba, 2023. For academic and non-commercial use only.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Our project is based on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions about this paper, feel free to reach me at &lt;a href=&#34;mailto:yangtao9009@gmail.com&#34;&gt;yangtao9009@gmail.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mouredev/roadmap-retos-programacion</title>
    <updated>2023-12-29T01:31:49Z</updated>
    <id>tag:github.com,2023-12-29:/mouredev/roadmap-retos-programacion</id>
    <link href="https://github.com/mouredev/roadmap-retos-programacion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ruta de estudio basada en ejercicios de código semanales en 2024 de la comunidad MoureDev para aprender y practicar lógica usando cualquier lenguaje de programación.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Images/header.jpg&#34; alt=&#34;https://retosdeprogramacion.com&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap retos de programación semanales 2024&lt;/h1&gt; &#xA;&lt;h3&gt;Ruta de estudio con ejercicios para mejorar tu lógica de programación y aprender cualquier lenguaje. Gratis, a tu ritmo y en comunidad.&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://retosdeprogramacion.com/roadmap&#34;&gt;https://retosdeprogramacion.com/roadmap&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/retos-programacion-web&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/retos-programacion-web?label=Web%20Retos%20Programaci%C3%B3n&amp;amp;style=social&#34; alt=&#34;Retos programación web&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Información importante&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cada semana se publica un nuevo reto y se corrige en directo desde &lt;strong&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/strong&gt; el ejercicio de la semana pasada.&lt;/li&gt; &#xA; &lt;li&gt;En la sección &#34;Eventos&#34; de nuestro servidor de &lt;strong&gt;&lt;a href=&#34;https://discord.gg/mouredev&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt; encontrarás el día y horario por país de los directos.&lt;/li&gt; &#xA; &lt;li&gt;Puedes utilizar &lt;strong&gt;cualquier lenguaje de programación&lt;/strong&gt;, y encontrar tanto mis correcciones como las de la comunidad en el directorio de cada reto.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;¿Quieres participar?&lt;/strong&gt; Te lo explico en la sección &lt;strong&gt;&lt;a href=&#34;https://github.com/mouredev/roadmap-retos-programacion#instrucciones&#34;&gt;Instrucciones&lt;/a&gt;&lt;/strong&gt; en este mismo documento.&lt;/li&gt; &#xA; &lt;li&gt;Los retos siguen un orden basado en su ruta de estudio pero si ya tienes conocimientos puedes resolverlos de manera totalmente independiente. Simplemente revisa su nivel de dificultad.&lt;/li&gt; &#xA; &lt;li&gt;Una vez se haya cumplido la semana de publicación del reto, podrás consultar mi corrección y las de la comunidad en cualquier lenguaje de programación.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Consulta la &lt;a href=&#34;https://retosdeprogramacion.com/roadmap&#34;&gt;web&lt;/a&gt; para más información.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Corrección y próximo ejercicio&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Martes 2 de Enero de 2024 a las 20:00 (hora España) desde &lt;strong&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; &#xA; &lt;h4&gt;Consulta el &lt;strong&gt;&lt;a href=&#34;https://discord.gg/C83vqurv?event=1189147970021642271&#34;&gt;horario&lt;/a&gt;&lt;/strong&gt; por país y crea un &lt;strong&gt;&lt;a href=&#34;https://discord.gg/C83vqurv?event=1189147970021642271&#34;&gt;recordatorio&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Ejercicio&lt;/th&gt; &#xA;   &lt;th&gt;Corrección&lt;/th&gt; &#xA;   &lt;th&gt;Vídeo&lt;/th&gt; &#xA;   &lt;th&gt;Comunidad&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;00&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Roadmap/00%20-%20SINTAXIS,%20VARIABLES,%20TIPOS%20DE%20DATOS%20Y%20HOLA%20MUNDO/ejercicio.md&#34;&gt;SINTAXIS, VARIABLES, TIPOS DE DATOS Y HOLA MUNDO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/C83vqurv?event=1189147970021642271&#34;&gt;🗓️ 02/01/24&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Roadmap/00%20-%20SINTAXIS,%20VARIABLES,%20TIPOS%20DE%20DATOS%20Y%20HOLA%20MUNDO/&#34;&gt;👥&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Instrucciones&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Haz un &lt;a href=&#34;https://github.com/mouredev/roadmap-retos-programacion/fork&#34;&gt;FORK&lt;/a&gt; del proyecto y trabaja con Git para ir sincronizando las actualizaciones.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;En el proyecto tienes un directorio para cada ejercicio en la carpeta &lt;a href=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Roadmap&#34;&gt;Roadmap&lt;/a&gt;. Dentro de cada directorio encontrarás un fichero llamado &lt;strong&gt;ejercicio.md&lt;/strong&gt; con el enunciado de cada reto.&lt;/li&gt; &#xA; &lt;li&gt;Si quieres compartir tu propia solución de un ejercicio con la comunidad, crea un fichero de código con tu nombre y extensión, y realiza una &lt;a href=&#34;https://docs.github.com/es/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request&#34;&gt;&lt;strong&gt;PULL REQUEST&lt;/strong&gt;&lt;/a&gt; contra el repositorio.&lt;/li&gt; &#xA; &lt;li&gt;El fichero de código debe situarse dentro del directorio del reto, en la carpeta correspondiente al lenguaje de programación utilizado (si no existe la carpeta del lenguaje, créala con todas sus letras en minúsculas). Por ejemplo, si has resuelto el reto #00 utilizando el lenguaje de programación Python y tu usuario de GitHub se llama &#34;mouredev&#34;, tu corrección deberá estar en &lt;strong&gt;&#34;Roadmap/#00/python/mouredev.py&#34;&lt;/strong&gt;. El título de la Pull Request también debe seguir este formato: &lt;strong&gt;&#34;#[número] - [lenguaje_utilizado]&#34;&lt;/strong&gt;. En el ejemplo anterior sería &lt;strong&gt;&#34;#00 - Python&#34;&lt;/strong&gt;. Se rechazarán las Pull Request que no sigan este formato o contengan ficheros adicionales.&lt;/li&gt; &#xA; &lt;li&gt;Cada &lt;strong&gt;SEMANA&lt;/strong&gt; (consulta el día en el ejercicio correspondiente) realizaré una transmisión en directo desde &lt;strong&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/strong&gt; corrigiendo el reto, revisando soluciones de la comunidad y publicando un nuevo ejercicio.&lt;/li&gt; &#xA; &lt;li&gt;Si necesitas ayuda o quieres comentar cualquier cosa sobre los retos semanales, tienes el canal &#34;reto-semanal” en nuestro servidor de &lt;strong&gt;&lt;a href=&#34;https://discord.gg/mouredev&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt; (también el día y horario de corrección en la sección &#34;Eventos&#34;).&lt;/li&gt; &#xA; &lt;li&gt;Puedes proponer Pull Request con propuestas o correcciones sobre ejercicios del resto de la comunidad si estos poseen errores. De esta manera colaboraremos para crear un repositorio cada vez más valioso.&lt;/li&gt; &#xA; &lt;li&gt;Si se te solicita un cambio/corrección en una Pull Request, y al cabo de 2 semanas no se muestra nueva actividad, se cerrará esa petición para mantener el repositorio limpio. Por supuesto, puedes volver a enviar la Pull Request cuando quieras.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Aclaraciones&lt;/h2&gt; &#xA;&lt;p&gt;Si tienes dudas con el nombre del directorio de algún lenguaje, intenta consultar el nombre que se ha empleado en ejercicios anteriores. Algunos ejemplos que puedes llegar a dudar:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;c#, no csharp&lt;/li&gt; &#xA; &lt;li&gt;c++, no cplusplus&lt;/li&gt; &#xA; &lt;li&gt;go, no golang&lt;/li&gt; &#xA; &lt;li&gt;javascript, no js&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Guía rápida Git y GitHub&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Realiza un &lt;a href=&#34;https://github.com/mouredev/roadmap-retos-programacion/fork&#34;&gt;FORK&lt;/a&gt; del repositorio de retos semanales desde GitHub.&lt;/li&gt; &#xA; &lt;li&gt;CLONA ese repositorio a tu máquina local &lt;code&gt;git clone [TU-REPOSITORIO]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Crea una RAMA para la solución y desplázate a ella &lt;code&gt;git checkout -b [EL-NOMBRE-DE-TU-RAMA]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Añade el fichero de tu solución al STAGE &lt;code&gt;git add [FICHERO-DE-TU-RETO]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Haz COMMIT con el mensaje de la solución &lt;code&gt;git commit -m &#34;#[NÚMERO-RETO] - [LENGUAJE-UTILIZADO]&#34;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Haz PUSH &lt;code&gt;git push [EL-NOMBRE-DE-TU-RAMA]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;En el repositorio principal debes ir a la rama y hacer &lt;a href=&#34;https://docs.github.com/es/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request&#34;&gt;PULL REQUEST&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;CONTRIBUTE.&lt;/li&gt; &#xA; &lt;li&gt;CREATE PULL REQUEST (cubre la plantilla que te aparecerá).&lt;/li&gt; &#xA; &lt;li&gt;Si el proceso de entrega se ha realizado de forma correcta, se añadirá tu corrección al repositorio. En caso contrario, se te notificarán los cambios a realizar o los motivos del rechazo.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;He creado un curso completo gratis para aprender a trabajar con Git y GitHub desde cero.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/hello-git&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/hello-git?label=Curso%20Git%20GitHub&amp;amp;style=social&#34; alt=&#34;Curso Git y GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Más retos de programación&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consulta los 101 retos de programación resueltos y las 12 aplicaciones para tu portfolio que ya hemos desarrollado.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/retos-programacion-2023&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/retos-programacion-2023?label=Retos%20Programaci%C3%B3n%202023&amp;amp;style=social&#34; alt=&#34;Retos programación 2023&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mouredev/Weekly-Challenge-2022-Kotlin&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/Weekly-Challenge-2022-Kotlin?label=Retos%20Semanales%202022&amp;amp;style=social&#34; alt=&#34;Retos programación 2022&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mouredev/Monthly-App-Challenge-2022&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/Monthly-App-Challenge-2022?label=Aplicaciones%20portafolio&amp;amp;style=social&#34; alt=&#34;Aplicaciones portafolio&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png&#34; alt=&#34;https://mouredev.com&#34;&gt; Hola, mi nombre es Brais Moure.&lt;/h2&gt; &#xA;&lt;h3&gt;Freelance full-stack iOS &amp;amp; Android engineer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitch-Retos_en_directo-9146FF?style=for-the-badge&amp;amp;logo=twitch&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Twitch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Chat_comunidad-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://moure.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Links_de_inter%C3%A9s-moure.dev-39E09B?style=for-the-badge&amp;amp;logo=Linktree&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtube.com/mouredevapps?sub_confirmation=1&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/subscribers/UCxPD7bsocoAMq8Dj18kmGyQ?style=social&#34; alt=&#34;YouTube Channel Subscribers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitch.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitch/status/mouredev?style=social&#34; alt=&#34;Twitch Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/729672926432985098?style=social&amp;amp;label=Discord&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mouredev?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/followers/mouredev?style=social&#34; alt=&#34;GitHub Followers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/mouredev?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Soy ingeniero de software desde 2010. Desde 2018 años combino mi trabajo desarrollando Apps con la creación de contenido formativo sobre programación y tecnología en diferentes redes sociales como &lt;strong&gt;&lt;a href=&#34;https://moure.dev&#34;&gt;@mouredev&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;En mi perfil de GitHub tienes más información&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-MoureDev-14a1f0?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Web&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>