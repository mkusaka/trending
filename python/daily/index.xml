<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-16T01:31:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nkasmanoff/pi-card</title>
    <updated>2024-05-16T01:31:54Z</updated>
    <id>tag:github.com,2024-05-16:/nkasmanoff/pi-card</id>
    <link href="https://github.com/nkasmanoff/pi-card" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Raspberry Pi Voice Assistant&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pi-C.A.R.D&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/assets/assistant.png&#34; height=&#34;300&#34;&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/#demos&#34;&gt;Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/#hardware&#34;&gt;Hardware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OryGVbh5JZE&#34;&gt;Talking to Llama 3 on a Raspberry Pi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Pi-card is an AI powered voice assistant running entirely on a Raspberry Pi. It is capable of doing anything a standard LLM (like ChatGPT) can do in a conversational setting. In addition, if there is a camera equipped, you can also ask Pi-card to take a photo, describe what it sees, and then ask questions about that image.&lt;/p&gt; &#xA;&lt;h3&gt;Why Pi-card?&lt;/h3&gt; &#xA;&lt;p&gt;Raspberry &lt;strong&gt;Pi&lt;/strong&gt; - &lt;strong&gt;C&lt;/strong&gt;amera &lt;strong&gt;A&lt;/strong&gt;udio &lt;strong&gt;R&lt;/strong&gt;ecognition &lt;strong&gt;D&lt;/strong&gt;evice.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/nkasmanoff/pi-card/main/assets/picard-facepalm.jpg&#34; height=&#34;300&#34;&gt; &#xA;&lt;p&gt;Please submit an issue or pull request if you can think of a better way to force this acronym.&lt;/p&gt; &#xA;&lt;h3&gt;How does it work?&lt;/h3&gt; &#xA;&lt;p&gt;Pi-card runs entirely on your Raspberry Pi. Once the main program is run, the system will listen for your wake word. Once your wake word has been said, you are officially in a conversation. Within this conversation you do not need to constantly repeat the wake word. The system will continue to listen for your commands until you say something like &#34;stop&#34;, &#34;exit&#34;, or &#34;goodbye&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The system has a memory of the conversation while you have it, meaning if you want the assistant to repeat something it said, or elaborate on a previous topic, you can do so.&lt;/p&gt; &#xA;&lt;p&gt;While the system is designed to be entirely local, it is also possible to easily connect it to some external APIs or services if you want to enhance the conversation, or give it control to some external devices. To do so is something I am open to improving, but for now it will be done based on specific keywords to trigger the external service. A good example of this is that for camera purposes, the system will activate the camera if you say &#34;take a photo&#34; or &#34;what do you see&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;How useful is it?&lt;/h3&gt; &#xA;&lt;p&gt;The system is designed to be a fun project that can be a &lt;em&gt;somewhat&lt;/em&gt; helpful AI assistant. Since everything is done locally, the system will not be as capable, or as fast, as cloud based systems. However, the system is still capable of a lot of improvements to be made.&lt;/p&gt; &#xA;&lt;h3&gt;Why isn&#39;t this an app?&lt;/h3&gt; &#xA;&lt;p&gt;The main reason for this is that I wanted to create a voice assistant that is completely offline and doesn&#39;t require any internet connection. This is because I wanted to ensure that the user&#39;s privacy is protected and that the user&#39;s data is not being sent to any third party servers.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;After downloading the repository, installing the requirements, and following the other setup instructions, you can run the main program by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the program is running, you can start a conversation with the assistant by saying the wake word. The default wake word is &#34;hey assistant&#34;, but you can change this in the &lt;code&gt;config.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Software&lt;/h3&gt; &#xA;&lt;p&gt;To keep this system as fast and lean as possible, we use cpp implementations of the audio transcription and vision language models. These are done with the wonderful libraries &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whipser.cpp&lt;/a&gt; for the audio transcription and &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; for the vision language model.&lt;/p&gt; &#xA;&lt;p&gt;In both cases, please clone these repositories wherever you like, and add their paths to the &lt;code&gt;config.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Once cloned, please go to each repository, and follow the setup instructions to get the models running. Some pointers are given below:&lt;/p&gt; &#xA;&lt;p&gt;For llama.cpp, we are using the vision language model capabilities, which are slightly different from the standard setup. You will need to follow the setup instructions for &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/master/examples/llava/README.md&#34;&gt;LlaVA&lt;/a&gt;, but update the model to be used to be one better suited for this device, &lt;a href=&#34;https://moondream.ai&#34;&gt;Moondream2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To install Moondream, you&#39;ll need to go to HuggingFace model hub, and download the model. I did so using python, with the following commands. Once again, make sure the vision model path is added to the &lt;code&gt;config.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import snapshot_download&#xA;model_id=&#34;vikhyatk/moondream2&#34;&#xA;snapshot_download(repo_id=model_id, local_dir=your/local/path, local_dir_use_symlinks=False, revision=&#34;main&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For whisper.cpp, you will need to follow the quick-start guide in the &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp?tab=readme-ov-file#quick-start&#34;&gt;README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Since this project is depending on openly available models, depending on the ones used, the limitations of this assistant will be the same as limitations of the models.&lt;/p&gt; &#xA;&lt;h3&gt;Hardware&lt;/h3&gt; &#xA;&lt;p&gt;The hardware setup is quite simple. You will need a Raspberry Pi 5 Model B, a USB microphone, a speaker, and a camera.&lt;/p&gt; &#xA;&lt;p&gt;The USB microphone and speaker can be plugged into the Raspberry Pi&#39;s USB ports. The camera can be connected to the camera port on the Raspberry Pi.&lt;/p&gt; &#xA;&lt;p&gt;I used the following hardware for my setup:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B0CRSNCJ6Y?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&#34;&gt;Raspberry Pi 5 Kit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B087PTH787?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&#34;&gt;USB Microphone&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B075M7FHM1?ref=ppx_yo2ov_dt_b_product_details&amp;amp;th=1&#34;&gt;Speaker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B012V1HEP4?ref=ppx_yo2ov_dt_b_product_details&amp;amp;th=1&#34;&gt;Camera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.amazon.com/dp/B0716TB6X3?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&#34;&gt;Camera Connector&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note Pi 5&#39;s have a new camera port, hence the new camera connector. At the same time, while this project is focused on making this work on a Raspberry Pi 5, it should work on other devices as well.&lt;/p&gt; &#xA;&lt;p&gt;Feel free to use your own, this is what worked for me.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Coming soon, but I plan to add notes here on things currently implemented, and what can be done in the future. Some quick notes on it are below&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Basic conversation capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Camera capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Benchmark response times&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test overclocking&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Figure out how to speed up whisper times&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add more external services&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add ability to interrupt assistant, and ask new question&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test combining with smart home or other devices?&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Chaphlagical/Deblur-GS</title>
    <updated>2024-05-16T01:31:54Z</updated>
    <id>tag:github.com,2024-05-16:/Chaphlagical/Deblur-GS</id>
    <link href="https://github.com/Chaphlagical/Deblur-GS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[I3D 2024] Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation of paper &#34;Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images&#34;, I3D 2024&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images&lt;/p&gt; &#xA; &lt;p&gt;Wenbo Chen, Ligang Liu&lt;/p&gt; &#xA; &lt;p&gt;I3D 2024&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://chaphlagical.icu/Deblur-GS/&#34;&gt;project page&lt;/a&gt; &lt;a href=&#34;http://doi.acm.org/10.1145/3651301&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1d8hAA-Wi3UoInQZCv-wc4A2efjuopqsQ?usp=sharing&#34;&gt;dataset&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Chaphlagical/Deblur-GS/main/asset/teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Novel view synthesis has undergone a revolution thanks to the radiance field method. The introduction of 3D Gaussian splatting (3DGS) has successfully addressed the issues of prolonged training times and slow rendering speeds associated with the Neural Radiance Field (NeRF), all while preserving the quality of reconstructions. However, 3DGS remains heavily reliant on the quality of input images and their initial camera pose initialization. In cases where input images are blurred, the reconstruction results suffer from blurriness and artifacts. In this paper, we propose the Deblur-GS method for reconstructing 3D Gaussian points to create a sharp radiance field from a camera motion blurred image set. We model the problem of motion blur as a joint optimization challenge involving camera trajectory estimation and time sampling. We cohesively optimize the parameters of the Gaussian points and the camera trajectory during the shutter time. Deblur-GS consistently achieves superior performance and rendering quality when compared to previous methods, as demonstrated in evaluations conducted on both synthetic and real datasets&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;SET DISTUTILS_USE_SDK=1 # Windows only&#xA;conda env create --file environment.yml&#xA;conda activate deblur_gs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py -s &amp;lt;path to dataset&amp;gt; --eval --deblur # Train with train/test split&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional Command Line Arguments for &lt;code&gt;train.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;blur_sample_num&lt;/code&gt;: number of key frames for trajectory time sampling&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;deblur&lt;/code&gt;: switch the deblur mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mode&lt;/code&gt;: models of camera motion trajectory (i.e. Linear, Spline, Bezier)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bezier_order&lt;/code&gt;: order of the B√©zier curve when use B√©zier curve for trajectory modeling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python train.py -s &amp;lt;path to dataset&amp;gt; --eval # Train with train/test split&#xA;python render.py -m &amp;lt;path to trained model&amp;gt; # Generate renderings&#xA;python metrics.py -m &amp;lt;path to trained model&amp;gt; # Compute error metrics on renderings&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional Command Line Arguments for &lt;code&gt;render.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;optim_pose&lt;/code&gt;: optimize the camera pose to align with the dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Render Video&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python render_video.py -m &amp;lt;path to trained model&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;BibTex&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@article{Chen_deblurgs2024,&#xA;   author       = {Wenbo, Chen and Ligang, Liu},&#xA;   title        = {Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images},&#xA;   journal      = {Proc. ACM Comput. Graph. Interact. Tech. (Proceedings of I3D 2024)},&#xA;   year         = {2024},&#xA;   volume       = {7},&#xA;   number       = {1},&#xA;   numpages     = {13},&#xA;   location     = {Philadelphia, PA, USA},&#xA;   url          = {http://doi.acm.org/10.1145/3651301},&#xA;   doi          = {10.1145/3651301},&#xA;   publisher    = {ACM Press},&#xA;   address      = {New York, NY, USA},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Chainlit/cookbook</title>
    <updated>2024-05-16T01:31:54Z</updated>
    <id>tag:github.com,2024-05-16:/Chainlit/cookbook</id>
    <link href="https://github.com/Chainlit/cookbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chainlit&#39;s cookbook repo&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chainlit Cookbook&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Chainlit Demos repository! Here you&#39;ll find a collection of example projects demonstrating how to use Chainlit to create amazing chatbot UIs with ease. Each folder in this repository represents a separate demo project.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To run a demo, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/Chainlit/cookbook.git chainlit-cookbook&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Navigate to the desired demo folder: &lt;pre&gt;&lt;code&gt;cd chainlit-cookbook/demo-folder-name&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install the required dependencies: &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file based on the provided &lt;code&gt;.env.example&lt;/code&gt; file: &lt;pre&gt;&lt;code&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; Modify the &lt;code&gt;.env&lt;/code&gt; file as needed to include any necessary API keys or configuration settings.&lt;/li&gt; &#xA; &lt;li&gt;Run the Chainlit app in watch mode: &lt;pre&gt;&lt;code&gt;chainlit run app.py -w&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Your demo chatbot UI should now be up and running in your browser!&lt;/p&gt; &#xA;&lt;h2&gt;üíÅ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;d love to see more demos showcasing the power of Chainlit. If you have an idea for a demo or want to contribute one, please feel free to open an issue or create a pull request. Your contributions are highly appreciated!&lt;/p&gt;</summary>
  </entry>
</feed>