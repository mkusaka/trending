<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-26T01:37:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jina-ai/docarray</title>
    <updated>2022-09-26T01:37:05Z</updated>
    <id>tag:github.com,2022-09-26:/jina-ai/docarray</id>
    <link href="https://github.com/jina-ai/docarray" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üß¨ The data structure for unstructured multimodal data ¬∑ Neural Search ¬∑ Vector Search ¬∑ Document Store&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/docs/_static/logo-light.svg?raw=true&#34; alt=&#34;DocArray logo: The data structure for unstructured data&#34; width=&#34;150px&#34;&gt; &lt;br&gt; &lt;b&gt;The data structure for unstructured multimodal data&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/docarray/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/docarray?style=flat-square&amp;amp;label=Release&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/jina-ai/docarray&#34;&gt;&lt;img alt=&#34;Codecov branch&#34; src=&#34;https://img.shields.io/codecov/c/github/jina-ai/docarray/main?logo=Codecov&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/docarray&#34;&gt;&lt;img alt=&#34;PyPI - Downloads from official pypistats&#34; src=&#34;https://img.shields.io/pypi/dm/docarray?style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://slack.jina.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-3.6k-blueviolet?logo=slack&amp;amp;logoColor=white&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- start elevator-pitch --&gt; &#xA;&lt;p&gt;DocArray is a library for nested, unstructured, multimodal data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the multi-modal data with a Pythonic API.&lt;/p&gt; &#xA;&lt;p&gt;üö™ &lt;strong&gt;Door to cross-/multi-modal world&lt;/strong&gt;: super-expressive data structure for representing complicated/mixed/nested text, image, video, audio, 3D mesh data. The foundation data structure of &lt;a href=&#34;https://github.com/jina-ai/jina&#34;&gt;Jina&lt;/a&gt;, &lt;a href=&#34;https://github.com/jina-ai/clip-as-service&#34;&gt;CLIP-as-service&lt;/a&gt;, &lt;a href=&#34;https://github.com/jina-ai/dalle-flow&#34;&gt;DALL¬∑E Flow&lt;/a&gt;, &lt;a href=&#34;https://github.com/jina-ai/discoart&#34;&gt;DiscoArt&lt;/a&gt; etc.&lt;/p&gt; &#xA;&lt;p&gt;üßë‚Äçüî¨ &lt;strong&gt;Data science powerhouse&lt;/strong&gt;: greatly accelerate data scientists&#39; work on embedding, k-NN matching, querying, visualizing, evaluating via Torch/TensorFlow/ONNX/PaddlePaddle on CPU/GPU.&lt;/p&gt; &#xA;&lt;p&gt;üö° &lt;strong&gt;Data in transit&lt;/strong&gt;: optimized for network communication, ready-to-wire at anytime with fast and compressed serialization in Protobuf, bytes, base64, JSON, CSV, DataFrame. Perfect for streaming and out-of-memory data.&lt;/p&gt; &#xA;&lt;p&gt;üîé &lt;strong&gt;One-stop k-NN&lt;/strong&gt;: Unified and consistent API for mainstream vector databases that allows nearest neighboour search including Elasticsearch, Redis, ANNLite, Qdrant, Weaviate.&lt;/p&gt; &#xA;&lt;p&gt;üëí &lt;strong&gt;For modern apps&lt;/strong&gt;: GraphQL support makes your server versatile on request and response; built-in data validation and JSON Schema (OpenAPI) help you build reliable webservices.&lt;/p&gt; &#xA;&lt;p&gt;üêç &lt;strong&gt;Pythonic experience&lt;/strong&gt;: designed to be as easy as a Python list. If you know how to Python, you know how to DocArray. Intuitive idioms and type annotation simplify the code you write.&lt;/p&gt; &#xA;&lt;p&gt;üõ∏ &lt;strong&gt;Integrate with IDE&lt;/strong&gt;: pretty-print and visualization on Jupyter notebook &amp;amp; Google Colab; comprehensive auto-complete and type hint in PyCharm &amp;amp; VS Code.&lt;/p&gt; &#xA;&lt;p&gt;Read more on &lt;a href=&#34;https://docarray.jina.ai/get-started/what-is/&#34;&gt;why should you use DocArray&lt;/a&gt; and &lt;a href=&#34;https://docarray.jina.ai/get-started/what-is/#comparing-to-alternatives&#34;&gt;comparison to alternatives&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/docarray/main/#&#34;&gt;&lt;img src=&#34;https://github.com/jina-ai/jina/raw/master/.github/readme/core-tree-graph.svg?raw=true&#34; alt=&#34;Jina in Jina AI neural search ecosystem&#34; width=&#34;100%&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- end elevator-pitch --&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://docarray.jina.ai&#34;&gt;Documentation&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;Requires Python 3.7+ and &lt;code&gt;numpy&lt;/code&gt; only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install docarray&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or via Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install -c conda-forge docarray&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docarray.jina.ai/#install&#34;&gt;Commonly used features&lt;/a&gt; can be enabled via &lt;code&gt;pip install &#34;docarray[common]&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;DocArray consists of three simple concepts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document&lt;/strong&gt;: a data structure for easily representing nested, unstructured data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;DocumentArray&lt;/strong&gt;: a container for efficiently accessing, manipulating, and understanding multiple Documents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dataclass&lt;/strong&gt;: a high-level API for intuitively representing multimodal data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s see DocArray in action with some examples.&lt;/p&gt; &#xA;&lt;h3&gt;Example 1: represent multimodal data in dataclass&lt;/h3&gt; &#xA;&lt;p&gt;The following news article card can be easily represented via &lt;code&gt;docarray.dataclass&lt;/code&gt; and type annotation:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/docs/fundamentals/dataclass/img/image-mmdoc-example.png?raw=true&#34; alt=&#34;A example multimodal document&#34; width=&#34;300px&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import dataclass, Document&#xA;from docarray.typing import Image, Text, JSON&#xA;&#xA;&#xA;@dataclass&#xA;class WPArticle:&#xA;    banner: Image&#xA;    headline: Text&#xA;    meta: JSON&#xA;&#xA;&#xA;a = WPArticle(&#xA;    banner=&#39;https://.../cat-dog-flight.png&#39;,&#xA;    headline=&#39;Everything to know about flying with pets, ...&#39;,&#xA;    meta={&#xA;        &#39;author&#39;: &#39;Nathan Diller&#39;,&#xA;        &#39;Column&#39;: &#39;By the Way - A Post Travel Destination&#39;,&#xA;    },&#xA;)&#xA;&#xA;d = Document(a)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Example 2: a 10-liners text matching&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s search for top-5 similar sentences of &lt;kbd&gt;she smiled too much&lt;/kbd&gt; in &#34;Pride and Prejudice&#34;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import Document, DocumentArray&#xA;&#xA;d = Document(uri=&#39;https://www.gutenberg.org/files/1342/1342-0.txt&#39;).load_uri_to_text()&#xA;da = DocumentArray(Document(text=s.strip()) for s in d.text.split(&#39;\n&#39;) if s.strip())&#xA;da.apply(Document.embed_feature_hashing, backend=&#39;process&#39;)&#xA;&#xA;q = (&#xA;    Document(text=&#39;she smiled too much&#39;)&#xA;    .embed_feature_hashing()&#xA;    .match(da, metric=&#39;jaccard&#39;, use_scipy=True)&#xA;)&#xA;&#xA;print(q.matches[:5, (&#39;text&#39;, &#39;scores__jaccard__value&#39;)])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;[[&#39;but she smiled too much.&#39;, &#xA;  &#39;_little_, she might have fancied too _much_.&#39;, &#xA;  &#39;She perfectly remembered everything that had passed in&#39;, &#xA;  &#39;tolerably detached tone. While she spoke, an involuntary glance&#39;, &#xA;  &#39;much as she chooses.‚Äù&#39;], &#xA;  [0.3333333333333333, 0.6666666666666666, 0.7, 0.7272727272727273, 0.75]]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here the feature embedding is done by simple &lt;a href=&#34;https://en.wikipedia.org/wiki/Feature_hashing&#34;&gt;feature hashing&lt;/a&gt; and distance metric is &lt;a href=&#34;https://en.wikipedia.org/wiki/Jaccard_index&#34;&gt;Jaccard distance&lt;/a&gt;. You have better embeddings? Of course you do! We look forward to seeing your results!&lt;/p&gt; &#xA;&lt;h3&gt;Example 3: external storage for out-of-memory data&lt;/h3&gt; &#xA;&lt;p&gt;When your data is too big, storing in memory is probably not a good idea. DocArray supports &lt;a href=&#34;https://docarray.jina.ai/advanced/document-store/&#34;&gt;multiple storage backends&lt;/a&gt; such as SQLite, Weaviate, Qdrant and ANNLite. They are all unified under &lt;strong&gt;the exact same user experience and API&lt;/strong&gt;. Take the above snippet as an example, you only need to change one line to use SQLite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;da = DocumentArray(&#xA;    (Document(text=s.strip()) for s in d.text.split(&#39;\n&#39;) if s.strip()),&#xA;    storage=&#39;sqlite&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The code snippet can still run &lt;strong&gt;as-is&lt;/strong&gt;. All APIs remain the same, the code after are then running in a &#34;in-database&#34; manner.&lt;/p&gt; &#xA;&lt;p&gt;Besides saving memory, one can leverage storage backends for persistence, faster retrieval (e.g. on nearest-neighbour queries).&lt;/p&gt; &#xA;&lt;h3&gt;Example 4: a complete workflow of visual search&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s use DocArray and the &lt;a href=&#34;https://sites.google.com/view/totally-looks-like-dataset&#34;&gt;Totally Looks Like&lt;/a&gt; dataset to build a simple meme image search. The dataset contains 6,016 image-pairs stored in &lt;code&gt;/left&lt;/code&gt; and &lt;code&gt;/right&lt;/code&gt;. Images that share the same filename are perceptually similar. For example:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;left/00018.jpg&lt;/th&gt; &#xA;   &lt;th&gt;right/00018.jpg&lt;/th&gt; &#xA;   &lt;th&gt;left/00131.jpg&lt;/th&gt; &#xA;   &lt;th&gt;right/00131.jpg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/left-00018.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/right-00018.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/left-00131.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/right-00131.jpg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; width=&#34;50%&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Our problem is given an image from &lt;code&gt;/left&lt;/code&gt;, can we find its most-similar image in &lt;code&gt;/right&lt;/code&gt;? (without looking at the filename of course).&lt;/p&gt; &#xA;&lt;h3&gt;Load images&lt;/h3&gt; &#xA;&lt;p&gt;First we load images. You &lt;em&gt;can&lt;/em&gt; go to &lt;a href=&#34;https://sites.google.com/view/totally-looks-like-dataset&#34;&gt;Totally Looks Like&lt;/a&gt; website, unzip and load images as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import DocumentArray&#xA;&#xA;left_da = DocumentArray.from_files(&#39;left/*.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can simply pull it from Jina Cloud:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da = DocumentArray.pull(&#39;demo-leftda&#39;, show_progress=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will see a running progress bar to indicate the downloading process.&lt;/p&gt; &#xA;&lt;p&gt;To get a feeling of the data you will handle, plot them in one sprite image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.plot_image_sprites()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docarray.jina.ai&#34;&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/sprite.png?raw=true&#34; alt=&#34;Load totally looks like dataset with docarray API&#34; width=&#34;60%&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Apply preprocessing&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s do some standard computer vision pre-processing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from docarray import Document&#xA;&#xA;&#xA;def preproc(d: Document):&#xA;    return (&#xA;        d.load_uri_to_image_tensor()  # load&#xA;        .set_image_tensor_normalization()  # normalize color&#xA;        .set_image_tensor_channel_axis(-1, 0)&#xA;    )  # switch color axis for the PyTorch model later&#xA;&#xA;&#xA;left_da.apply(preproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Did I mention &lt;code&gt;apply&lt;/code&gt; works in parallel?&lt;/p&gt; &#xA;&lt;h3&gt;Embed images&lt;/h3&gt; &#xA;&lt;p&gt;Now convert images into embeddings using a pretrained ResNet50:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torchvision&#xA;&#xA;model = torchvision.models.resnet50(pretrained=True)  # load ResNet50&#xA;left_da.embed(model, device=&#39;cuda&#39;)  # embed via GPU to speed up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This step takes ~30 seconds on GPU. Beside PyTorch, you can also use TensorFlow, PaddlePaddle, or ONNX models in &lt;code&gt;.embed(...)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Visualize embeddings&lt;/h3&gt; &#xA;&lt;p&gt;You can visualize the embeddings via tSNE in an interactive embedding projector:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.plot_embeddings()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docarray.jina.ai&#34;&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/tsne.gif?raw=true&#34; alt=&#34;Visualizing embedding via tSNE and embedding projector&#34; width=&#34;90%&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Fun is fun, but recall our goal is to match left images against right images and so far we have only handled the left. Let&#39;s repeat the same procedure for the right:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Pull from Cloud &lt;/th&gt; &#xA;   &lt;th&gt; Download, unzip, load from local &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;right_da = (&#xA;    DocumentArray.pull(&#39;demo-rightda&#39;, show_progress=True)&#xA;    .apply(preproc)&#xA;    .embed(model, device=&#39;cuda&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;right_da = (&#xA;    DocumentArray.from_files(&#39;right/*.jpg&#39;).apply(preproc).embed(model, device=&#39;cuda&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Match nearest neighbours&lt;/h3&gt; &#xA;&lt;p&gt;We can now match the left to the right and take the top-9 results.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.match(right_da, limit=9)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s inspect what&#39;s inside &lt;code&gt;left_da&lt;/code&gt; matches now:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for d in left_da:&#xA;    for m in d.matches:&#xA;        print(d.uri, m.uri, m.scores[&#39;cosine&#39;].value)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;left/02262.jpg right/03459.jpg 0.21102&#xA;left/02262.jpg right/02964.jpg 0.13871843&#xA;left/02262.jpg right/02103.jpg 0.18265384&#xA;left/02262.jpg right/04520.jpg 0.16477376&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or shorten the loop as one-liner using the element &amp;amp; attribute selector:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(left_da[&#39;@m&#39;, (&#39;uri&#39;, &#39;scores__cosine__value&#39;)])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Better see it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;(&#xA;    DocumentArray(left_da[8].matches, copy=True)&#xA;    .apply(&#xA;        lambda d: d.set_image_tensor_channel_axis(&#xA;            0, -1&#xA;        ).set_image_tensor_inv_normalization()&#xA;    )&#xA;    .plot_image_sprites()&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docarray.jina.ai&#34;&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/9nn-left.jpeg?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; height=&#34;250px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docarray.jina.ai&#34;&gt;&lt;img src=&#34;https://github.com/jina-ai/docarray/raw/main/.github/README-img/9nn.png?raw=true&#34; alt=&#34;Visualizing top-9 matches using DocArray API&#34; height=&#34;250px&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;What we did here is revert the preprocessing steps (i.e. switching axis and normalizing) on the copied matches, so that you can visualize them using image sprites.&lt;/p&gt; &#xA;&lt;h3&gt;Quantitative evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Serious as you are, visual inspection is surely not enough. Let&#39;s calculate the recall@K. First we construct the groundtruth matches:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;groundtruth = DocumentArray(&#xA;    Document(uri=d.uri, matches=[Document(uri=d.uri.replace(&#39;left&#39;, &#39;right&#39;))])&#xA;    for d in left_da&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we create a new DocumentArray with real matches by simply replacing the filename, e.g. &lt;code&gt;left/00001.jpg&lt;/code&gt; to &lt;code&gt;right/00001.jpg&lt;/code&gt;. That&#39;s all we need: if the predicted match has the identical &lt;code&gt;uri&lt;/code&gt; as the groundtruth match, then it is correct.&lt;/p&gt; &#xA;&lt;p&gt;Now let&#39;s check recall rate from 1 to 5 over the full dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for k in range(1, 6):&#xA;    print(&#xA;        f&#39;recall@{k}&#39;,&#xA;        left_da.evaluate(&#xA;            groundtruth, hash_fn=lambda d: d.uri, metric=&#39;recall_at_k&#39;, k=k, max_rel=1&#xA;        ),&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;recall@1 0.02726063829787234&#xA;recall@2 0.03873005319148936&#xA;recall@3 0.04670877659574468&#xA;recall@4 0.052194148936170214&#xA;recall@5 0.0573470744680851&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More metrics can be used such as &lt;code&gt;precision_at_k&lt;/code&gt;, &lt;code&gt;ndcg_at_k&lt;/code&gt;, &lt;code&gt;hit_at_k&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you think a pretrained ResNet50 is good enough, let me tell you with &lt;a href=&#34;https://github.com/jina-ai/finetuner&#34;&gt;Finetuner&lt;/a&gt; you could do much better in just 10 extra lines of code. &lt;a href=&#34;https://finetuner.jina.ai/tasks/image-to-image/&#34;&gt;Here is how&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Save results&lt;/h3&gt; &#xA;&lt;p&gt;You can save a DocumentArray to binary, JSON, dict, DataFrame, CSV or Protobuf message with/without compression. In its simplest form,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.save(&#39;left_da.bin&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reuse it, do &lt;code&gt;left_da = DocumentArray.load(&#39;left_da.bin&#39;)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to transfer a DocumentArray from one machine to another or share it with your colleagues, you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da.push(&#39;my_shared_da&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now anyone who knows the token &lt;code&gt;my_shared_da&lt;/code&gt; can pull and work on it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left_da = DocumentArray.pull(&#39;my_shared_da&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Intrigued? That&#39;s only scratching the surface of what DocArray is capable of. &lt;a href=&#34;https://docarray.jina.ai&#34;&gt;Read our docs to learn more&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- start support-pitch --&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out the &lt;a href=&#34;https://learn.jina.ai&#34;&gt;Learning Bootcamp&lt;/a&gt; to get started with DocArray.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://slack.jina.ai&#34;&gt;Slack community&lt;/a&gt; and chat with other community members about ideas.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne&#34;&gt;Engineering All Hands&lt;/a&gt; meet-up to discuss your use case and learn Jina&#39;s new features. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;When?&lt;/strong&gt; The second Tuesday of every month&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Where?&lt;/strong&gt; Zoom (&lt;a href=&#34;https://calendar.google.com/calendar/embed?src=c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com&amp;amp;ctz=Europe%2FBerlin&#34;&gt;see our public events calendar&lt;/a&gt;/&lt;a href=&#34;https://calendar.google.com/calendar/ical/c_1t5ogfp2d45v8fit981j08mcm4%40group.calendar.google.com/public/basic.ics&#34;&gt;.ical&lt;/a&gt;) and &lt;a href=&#34;https://youtube.com/c/jina-ai&#34;&gt;live stream on YouTube&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Subscribe to the latest video tutorials on our &lt;a href=&#34;https://youtube.com/c/jina-ai&#34;&gt;YouTube channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Join Us&lt;/h2&gt; &#xA;&lt;p&gt;DocArray is backed by &lt;a href=&#34;https://jina.ai&#34;&gt;Jina AI&lt;/a&gt; and licensed under &lt;a href=&#34;https://raw.githubusercontent.com/jina-ai/docarray/main/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt;. &lt;a href=&#34;https://jobs.jina.ai&#34;&gt;We are actively hiring&lt;/a&gt; AI engineers, solution engineers to build the next neural search ecosystem in open-source.&lt;/p&gt; &#xA;&lt;!-- end support-pitch --&gt;</summary>
  </entry>
  <entry>
    <title>towhee-io/towhee</title>
    <updated>2022-09-26T01:37:05Z</updated>
    <id>tag:github.com,2022-09-26:/towhee-io/towhee</id>
    <link href="https://github.com/towhee-io/towhee" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Towhee is a framework that is dedicated to making neural data processing pipelines simple and fast.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/towhee-io/towhee/main/towhee_logo.png#gh-light-mode-only&#34; width=&#34;60%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/towhee-io/towhee/main/assets/towhee_logo_dark.png#gh-dark-mode-only&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;span style=&#34;font-weight: bold; font: Arial, sans-serif;&#34;&gt;x&lt;/span&gt;2vec, Towhee is all you need! &lt;/p&gt; &lt;/h3&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/towhee-io/towhee/main/README.md&#34; target=&#34;_blank&#34;&gt;ENGLISH&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/towhee-io/towhee/main/README_CN.md&#34;&gt;‰∏≠ÊñáÊñáÊ°£&lt;/a&gt; &lt;/p&gt; &lt;/h3&gt; &#xA;&lt;div class=&#34;column&#34; align=&#34;middle&#34;&gt; &#xA; &lt;a href=&#34;https://slack.towhee.io&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/join-slack-orange?style=flat&#34; alt=&#34;join-slack&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/towheeio&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/follow-twitter-blue?style=flat&#34; alt=&#34;twitter&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-apache2.0-green?style=flat&#34; alt=&#34;license&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/towhee-io/towhee/actions/workflows/pylint.yml&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/workflow/status/towhee-io/towhee/Workflow%20for%20pylint/main?label=pylint&amp;amp;style=flat&#34; alt=&#34;github actions&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://app.codecov.io/gh/towhee-io/towhee&#34;&gt; &lt;img src=&#34;https://img.shields.io/codecov/c/github/towhee-io/towhee?style=flat&#34; alt=&#34;coverage&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://towhee.io&#34;&gt;Towhee&lt;/a&gt; makes it easy to build neural data processing pipelines for AI applications. We provide hundreds of models, algorithms, and transformations that can be used as standard pipeline building blocks. You can use Towhee&#39;s Pythonic API to build a prototype of your pipeline and automatically optimize it for production-ready environments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üé®&lt;/span&gt;‚ÄÉ&lt;strong&gt;Various Modalities:&lt;/strong&gt; Towhee supports data processing on a variety of modalities, including images, videos, text, audio, molecular structures, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üéì&lt;/span&gt;‚ÄÉ&lt;strong&gt;SOTA Models:&lt;/strong&gt; Towhee provides SOTA models across 5 fields (CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures. These include BERT, CLIP, ViT, SwinTransformer, MAE, and data2vec, all pretrained and ready to use.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üì¶&lt;/span&gt;‚ÄÉ&lt;strong&gt;Data Processing:&lt;/strong&gt; Towhee also provides traditional methods alongside neural network models to help you build practical data processing pipelines. We have a rich pool of operators available, such as video decoding, audio slicing, frame sampling, feature vector dimension reduction, ensembling, and database operations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üêç&lt;/span&gt;‚ÄÉ&lt;strong&gt;Pythonic API:&lt;/strong&gt; Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.8.0 Aug. 16, 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Towhee now supports generating an Nvidia Triton Server from a Towhee pipeline, with aditional support for GPU image decoding.&lt;/li&gt; &#xA; &lt;li&gt;Added one audio fingerprinting model: &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/nnfp&#34;&gt;&lt;strong&gt;nnfp&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Added two image embedding models: &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp&#34;&gt;&lt;strong&gt;RepMLP&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/wave_vit&#34;&gt;&lt;strong&gt;WaveViT&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.7.3 Jul. 27, 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added one multimodal (text/image) model: &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coca&#34;&gt;&lt;em&gt;CoCa&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added two video models for grounded situation recognition &amp;amp; repetitive action counting: &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coformer&#34;&gt;&lt;em&gt;CoFormer&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac&#34;&gt;&lt;em&gt;TransRAC&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added two SoTA models for image tasks (image retrieval, image classification, etc.): &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet&#34;&gt;&lt;em&gt;CVNet&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/max_vit&#34;&gt;&lt;em&gt;MaxViT&lt;/em&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.7.1 Jul. 1, 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added one image embedding model: &lt;a href=&#34;https://towhee.io/image-embedding/mpvit&#34;&gt;&lt;em&gt;MPViT&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added two video retrieval models: &lt;a href=&#34;https://towhee.io/video-text-embedding/bridge-former&#34;&gt;&lt;em&gt;BridgeFormer&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/video-text-embedding/collaborative-experts&#34;&gt;&lt;em&gt;collaborative-experts&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added FAISS-based ANNSearch operators: &lt;em&gt;to_faiss&lt;/em&gt;, &lt;em&gt;faiss_search&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.7.0 Jun. 24, 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added six video understanding/classification models: &lt;a href=&#34;https://towhee.io/action-classification/video-swin-transformer&#34;&gt;&lt;em&gt;Video Swin Transformer&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/tsm&#34;&gt;&lt;em&gt;TSM&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/uniformer&#34;&gt;&lt;em&gt;Uniformer&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/omnivore&#34;&gt;&lt;em&gt;OMNIVORE&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/timesformer&#34;&gt;&lt;em&gt;TimeSformer&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/movinet&#34;&gt;&lt;em&gt;MoViNets&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added four video retrieval models: &lt;a href=&#34;https://towhee.io/video-text-embedding/clip4clip&#34;&gt;&lt;em&gt;CLIP4Clip&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/video-text-embedding/drl&#34;&gt;&lt;em&gt;DRL&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/video-text-embedding/frozen-in-time&#34;&gt;&lt;em&gt;Frozen in Time&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/video-text-embedding/mdmmt&#34;&gt;&lt;em&gt;MDMMT&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;v0.6.1 May. 13, 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added three text-image retrieval models: &lt;a href=&#34;https://towhee.io/image-text-embedding/clip&#34;&gt;&lt;em&gt;CLIP&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/image-text-embedding/blip&#34;&gt;&lt;em&gt;BLIP&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/image-text-embedding/lightningdot&#34;&gt;&lt;em&gt;LightningDOT&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added six video understanding/classification models from PyTorchVideo: &lt;a href=&#34;https://towhee.io/action-classification/pytorchvideo&#34;&gt;&lt;em&gt;I3D&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/pytorchvideo&#34;&gt;&lt;em&gt;C2D&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/pytorchvideo&#34;&gt;&lt;em&gt;Slow&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/pytorchvideo&#34;&gt;&lt;em&gt;SlowFast&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/pytorchvideo&#34;&gt;&lt;em&gt;X3D&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://towhee.io/action-classification/pytorchvideo&#34;&gt;&lt;em&gt;MViT&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Towhee requires Python 3.6+. You can install Towhee via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install towhee towhee.models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you run into any pip-related install problems, please try to upgrade pip with &lt;code&gt;pip install -U pip&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s try your first Towhee pipeline. Below is an example for how to create a CLIP-based cross modal retrieval pipeline with only 15 lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import towhee&#xA;&#xA;# create image embeddings and build index&#xA;(&#xA;    towhee.glob[&#39;file_name&#39;](&#39;./*.png&#39;)&#xA;          .image_decode[&#39;file_name&#39;, &#39;img&#39;]()&#xA;          .image_text_embedding.clip[&#39;img&#39;, &#39;vec&#39;](model_name=&#39;clip_vit_b32&#39;, modality=&#39;image&#39;)&#xA;          .tensor_normalize[&#39;vec&#39;,&#39;vec&#39;]()&#xA;          .to_faiss[(&#39;file_name&#39;, &#39;vec&#39;)](findex=&#39;./index.bin&#39;)&#xA;)&#xA;&#xA;# search image by text&#xA;results = (&#xA;    towhee.dc[&#39;text&#39;]([&#39;puppy Corgi&#39;])&#xA;          .image_text_embedding.clip[&#39;text&#39;, &#39;vec&#39;](model_name=&#39;clip_vit_b32&#39;, modality=&#39;text&#39;)&#xA;          .tensor_normalize[&#39;vec&#39;, &#39;vec&#39;]()&#xA;          .faiss_search[&#39;vec&#39;, &#39;results&#39;](findex=&#39;./index.bin&#39;, k=3)&#xA;          .select[&#39;text&#39;, &#39;results&#39;]()&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/towhee-io/towhee/main/assets/towhee_example.png&#34; style=&#34;width: 60%; height: 60%&#34;&gt; &#xA;&lt;p&gt;Learn more examples from the &lt;a href=&#34;https://codelabs.towhee.io/&#34;&gt;Towhee Bootcamp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Core Concepts&lt;/h2&gt; &#xA;&lt;p&gt;Towhee is composed of four main building blocks - &lt;code&gt;Operators&lt;/code&gt;, &lt;code&gt;Pipelines&lt;/code&gt;, &lt;code&gt;DataCollection API&lt;/code&gt; and &lt;code&gt;Engine&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Operators&lt;/strong&gt;: An operator is a single building block of a neural data processing pipeline. Different implementations of operators are categorized by tasks, with each task having a standard interface. An operator can be a deep learning model, a data processing method, or a Python function.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pipelines&lt;/strong&gt;: A pipeline is composed of several operators interconnected in the form of a DAG (directed acyclic graph). This DAG can direct complex functionalities, such as embedding feature extraction, data tagging, and cross modal data analysis.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DataCollection API&lt;/strong&gt;: A Pythonic and method-chaining style API for building custom pipelines. A pipeline defined by the DataColltion API can be run locally on a laptop for fast prototyping and then be converted to a docker image, with end-to-end optimizations, for production-ready environments.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Engine&lt;/strong&gt;: The engine sits at Towhee&#39;s core. Given a pipeline, the engine will drive dataflow among individual operators, schedule tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within Towhee to run pipelines on a single-instance machine and a Triton-based engine for docker containers.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Writing code is not the only way to contribute! Submitting issues, answering questions, and improving documentation are just some of the many ways you can help our growing community. Check out our &lt;a href=&#34;https://github.com/towhee-io/towhee/raw/main/CONTRIBUTING.md&#34;&gt;contributing page&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks goes to these folks for contributing to Towhee, either on Github, our Towhee Hub, or elsewhere: &lt;br&gt;&#xA; &lt;!-- Do not remove start of hero-bot --&gt;&lt;br&gt; &lt;img src=&#34;https://img.shields.io/badge/all--contributors-33-orange&#34;&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/AniTho&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/34787227?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Chiiizzzy&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/72550076?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/GuoRentong&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/57477222?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Tumao727&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/20420181?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YuDongPan&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/88148730?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binbinlv&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/83755740?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/derekdqc&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/11754703?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dreamfireyu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/47691077?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/filip-halt&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/81822489?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/fzliu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/6334158?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gexy185&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/103474331?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hyf3513OneGO&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/67197231?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jaelgu&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/86251631?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jeffoverflow&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/24581746?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jennyli-z&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/93511422?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jingkl&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/34296482?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jinlingxu06&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/106302799?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/junjiejiangjjj&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/14136703?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/krishnakatyal&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/37455387?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/omartarek206&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/40853054?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/oneseer&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/28955741?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pravee42&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/65100038?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/reiase&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5417329?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shiyu22&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/53459423?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/songxianj&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/107831450?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/soulteary&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/1500781?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sre-ci-robot&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/56469371?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sutcalag&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/83750738?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wxywb&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/5432721?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zc277584121&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/17022025?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zengxiang68&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/68835157?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhousicong&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/7541863?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhujiming&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/u/18031320?v=4&#34; width=&#34;30px&#34;&gt;&lt;/a&gt; &lt;br&gt;&#xA; &lt;!-- Do not remove end of hero-bot --&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Looking for a database to store and index your embedding vectors? Check out &lt;a href=&#34;https://github.com/milvus-io/milvus&#34;&gt;Milvus&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jaywalnut310/vits</title>
    <updated>2022-09-26T01:37:05Z</updated>
    <id>tag:github.com,2022-09-26:/jaywalnut310/vits</id>
    <link href="https://github.com/jaywalnut310/vits" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/h1&gt; &#xA;&lt;h3&gt;Jaehyeon Kim, Jungil Kong, and Juhee Son&lt;/h3&gt; &#xA;&lt;p&gt;In our recent &lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;paper&lt;/a&gt;, we propose VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech.&lt;/p&gt; &#xA;&lt;p&gt;Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.&lt;/p&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://jaywalnut310.github.io/vits-demo/index.html&#34;&gt;demo&lt;/a&gt; for audio samples.&lt;/p&gt; &#xA;&lt;p&gt;We also provide the &lt;a href=&#34;https://drive.google.com/drive/folders/1ksarh-cJf3F5eKJjLVWY0X1j1qsQqiS2?usp=sharing&#34;&gt;pretrained models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;** Update note: Thanks to &lt;a href=&#34;https://github.com/jaywalnut310/vits/issues/1&#34;&gt;Rishikesh (‡§ã‡§∑‡§ø‡§ï‡•á‡§∂)&lt;/a&gt;, our interactive TTS demo is now available on &lt;a href=&#34;https://colab.research.google.com/drive/1CO61pZizDj7en71NQG_aqqKdGaA_SaBf?usp=sharing&#34;&gt;Colab Notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table style=&#34;width:100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;VITS at training&lt;/th&gt; &#xA;   &lt;th&gt;VITS at inference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jaywalnut310/vits/main/resources/fig_1a.png&#34; alt=&#34;VITS at training&#34; height=&#34;400&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jaywalnut310/vits/main/resources/fig_1b.png&#34; alt=&#34;VITS at inference&#34; height=&#34;400&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA; &lt;li&gt;Install python requirements. Please refer &lt;a href=&#34;https://raw.githubusercontent.com/jaywalnut310/vits/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;You may need to install espeak first: &lt;code&gt;apt-get install espeak&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Download datasets &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Download and extract the LJ Speech dataset, then rename or create a link to the dataset folder: &lt;code&gt;ln -s /path/to/LJSpeech-1.1/wavs DUMMY1&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For mult-speaker setting, download and extract the VCTK dataset, and downsample wav files to 22050 Hz. Then rename or create a link to the dataset folder: &lt;code&gt;ln -s /path/to/VCTK-Corpus/downsampled_wavs DUMMY2&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build Monotonic Alignment Search and run preprocessing if you use your own datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Cython-version Monotonoic Alignment Search&#xA;cd monotonic_align&#xA;python setup.py build_ext --inplace&#xA;&#xA;# Preprocessing (g2p) for your own datasets. Preprocessed phonemes for LJ Speech and VCTK have been already provided.&#xA;# python preprocess.py --text_index 1 --filelists filelists/ljs_audio_text_train_filelist.txt filelists/ljs_audio_text_val_filelist.txt filelists/ljs_audio_text_test_filelist.txt &#xA;# python preprocess.py --text_index 2 --filelists filelists/vctk_audio_sid_text_train_filelist.txt filelists/vctk_audio_sid_text_val_filelist.txt filelists/vctk_audio_sid_text_test_filelist.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training Exmaple&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# LJ Speech&#xA;python train.py -c configs/ljs_base.json -m ljs_base&#xA;&#xA;# VCTK&#xA;python train_ms.py -c configs/vctk_base.json -m vctk_base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference Example&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/jaywalnut310/vits/main/inference.ipynb&#34;&gt;inference.ipynb&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>