<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-23T01:35:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/presidio</title>
    <updated>2025-08-23T01:35:54Z</updated>
    <id>tag:github.com,2025-08-23:/microsoft/presidio</id>
    <link href="https://github.com/microsoft/presidio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source framework for detecting, redacting, masking, and anonymizing sensitive data (PII) across text, images, and structured data. Supports NLP, pattern matching, and customizable pipelines.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Presidio - Data Protection and De-identification SDK&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Context aware, pluggable and customizable PII de-identification service for text and images.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/csedevil/Presidio/_build/latest?definitionId=212&amp;amp;branchName=main&#34;&gt;&lt;img src=&#34;https://dev.azure.com/csedevil/Presidio/_apis/build/status/Presidio-CI%20V2?branchName=main&#34; alt=&#34;Build Status&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;http://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-brightgreen.svg?sanitize=true&#34; alt=&#34;MIT license&#34; /&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/release/Microsoft/presidio.svg?sanitize=true&#34; alt=&#34;Release&#34; /&gt; &lt;a href=&#34;https://www.bestpractices.dev/projects/6076&#34;&gt;&lt;img src=&#34;https://www.bestpractices.dev/projects/6076/badge&#34; alt=&#34;OpenSSF Best Practices&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/presidio-analyzer/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/presidio-analyzer.svg?sanitize=true&#34; alt=&#34;PyPI pyversions&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Presidio Analyzer &lt;a href=&#34;https://img.shields.io/pypi/dm/presidio-analyzer.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/presidio-analyzer.svg?sanitize=true&#34; alt=&#34;Pypi Downloads&#34; /&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Presidio Anonymizer &lt;a href=&#34;https://img.shields.io/pypi/dm/presidio-anonymizer.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/presidio-anonymizer.svg?sanitize=true&#34; alt=&#34;Pypi Downloads&#34; /&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Presidio Image-Redactor &lt;a href=&#34;https://img.shields.io/pypi/dm/presidio-image-redactor.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/presidio-image-redactor.svg?sanitize=true&#34; alt=&#34;Pypi Downloads&#34; /&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Presidio Structured &lt;a href=&#34;https://img.shields.io/pypi/dm/presidio-structured.svg&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/presidio-structured.svg?sanitize=true&#34; alt=&#34;Pypi Downloads&#34; /&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is Presidio&lt;/h2&gt; &#xA;&lt;p&gt;Presidio &lt;em&gt;(Origin from Latin praesidium ‚Äòprotection, garrison‚Äô)&lt;/em&gt; helps to ensure sensitive data is properly managed and governed. It provides fast &lt;strong&gt;&lt;em&gt;identification&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;anonymization&lt;/em&gt;&lt;/strong&gt; modules for private entities in text such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/presidio/main/docs/assets/changing_text.gif&#34; alt=&#34;Presidio demo gif&#34; /&gt;&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h3&gt;&lt;span&gt;üìò&lt;/span&gt; &lt;a href=&#34;https://microsoft.github.io/presidio&#34;&gt;Full documentation&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;span&gt;‚ùì&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/presidio/main/docs/faq.md&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;span&gt;üí≠&lt;/span&gt; &lt;a href=&#34;https://aka.ms/presidio-demo&#34;&gt;Demo&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;span&gt;üõ´&lt;/span&gt; &lt;a href=&#34;https://microsoft.github.io/presidio/samples/&#34;&gt;Examples&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Are you using Presidio? We&#39;d love to know how&lt;/h2&gt; &#xA;&lt;p&gt;Please help us improve by taking &lt;a href=&#34;https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR9LagCGNW01LpMix2pnFWFJUQjJDTVkwSlJYRkFPSUNNVlVRRVRWVDVNSy4u&#34;&gt;this short anonymous survey&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h3&gt;Goals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions.&lt;/li&gt; &#xA; &lt;li&gt;Embrace extensibility and customizability to a specific business need.&lt;/li&gt; &#xA; &lt;li&gt;Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Main features&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Predefined&lt;/strong&gt; or &lt;strong&gt;custom PII recognizers&lt;/strong&gt; leveraging &lt;em&gt;Named Entity Recognition&lt;/em&gt;, &lt;em&gt;regular expressions&lt;/em&gt;, &lt;em&gt;rule based logic&lt;/em&gt; and &lt;em&gt;checksum&lt;/em&gt; with relevant context in multiple languages.&lt;/li&gt; &#xA; &lt;li&gt;Options for connecting to external PII detection models.&lt;/li&gt; &#xA; &lt;li&gt;Multiple usage options, &lt;strong&gt;from Python or PySpark workloads through Docker to Kubernetes&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizability&lt;/strong&gt; in PII identification and de-identification.&lt;/li&gt; &#xA; &lt;li&gt;Module for &lt;strong&gt;redacting PII text in images&lt;/strong&gt; (standard image types and DICOM medical images).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Presidio can help identify sensitive/PII data in un/structured text. However, because it is using automated detection mechanisms, there is no guarantee that Presidio will find all sensitive information. Consequently, additional systems and protections should be employed.&lt;/p&gt; &#xA;&lt;h2&gt;Installing Presidio&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/installation/#using-pip&#34;&gt;Using pip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/installation/#using-docker&#34;&gt;Using Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/installation/#install-from-source&#34;&gt;From source&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/presidio/main/docs/presidio_V2.md&#34;&gt;Migrating from V1 to V2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Running Presidio&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/getting_started&#34;&gt;Getting started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/development&#34;&gt;Setting up a development environment&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/text_anonymization&#34;&gt;PII de-identification in text&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/image-redactor&#34;&gt;PII de-identification in images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://microsoft.github.io/presidio/samples&#34;&gt;Usage samples and example deployments&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before you submit an issue, please go over the &lt;a href=&#34;https://microsoft.github.io/presidio/&#34;&gt;documentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For general discussions, please use the &lt;a href=&#34;https://github.com/microsoft/presidio/discussions&#34;&gt;GitHub repo&#39;s discussion board&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you have a usage question, found a bug or have a suggestion for improvement, please file a &lt;a href=&#34;https://github.com/microsoft/presidio/issues&#34;&gt;GitHub issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For other matters, please email &lt;a href=&#34;mailto:presidio@microsoft.com&#34;&gt;presidio@microsoft.com&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;For details on contributing to this repository, see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/presidio/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;a href=&#34;https://github.com/microsoft/presidio/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=microsoft/presidio&#34; /&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>codelion/openevolve</title>
    <updated>2025-08-23T01:35:54Z</updated>
    <id>tag:github.com,2025-08-23:/codelion/openevolve</id>
    <link href="https://github.com/codelion/openevolve" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source implementation of AlphaEvolve&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenEvolve&lt;/h1&gt; &#xA;&lt;p&gt;An open-source evolutionary coding agent that began as a faithful implementation of AlphaEvolve and has evolved far beyond it, enabling automated scientific and algorithmic discovery.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/openevolve-logo.png&#34; alt=&#34;OpenEvolve Logo&#34; /&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;OpenEvolve is an evolutionary coding agent that uses Large Language Models to automatically optimize and discover algorithms through iterative improvement. Starting from the AlphaEvolve research, it incorporates advanced features for reproducibility, multi-language support, sophisticated evaluation pipelines, and integration with cutting-edge LLM optimization techniques. It serves as both a research platform for evolutionary AI and a practical tool for automated code optimization.&lt;/p&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;p&gt;OpenEvolve implements a comprehensive evolutionary coding system with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evolutionary Coding Agent&lt;/strong&gt;: LLM-guided evolution of entire code files (not just functions)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed Controller Loop&lt;/strong&gt;: Asynchronous pipeline coordinating LLMs, evaluators, and databases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Program Database&lt;/strong&gt;: Storage and sampling of evolved programs with evaluation metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Sampling&lt;/strong&gt;: Context-rich prompts with past programs, scores, and problem descriptions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM Ensemble&lt;/strong&gt;: Multiple language models working together for code generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-objective Optimization&lt;/strong&gt;: Simultaneous optimization of multiple evaluation metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Checkpoint System&lt;/strong&gt;: Automatic saving and resuming of evolution state&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;üî¨ &lt;strong&gt;Scientific Reproducibility&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Seeding&lt;/strong&gt;: Full deterministic reproduction with hash-based component isolation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Default Reproducibility&lt;/strong&gt;: Seed=42 by default for immediate reproducible results&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Granular Control&lt;/strong&gt;: Per-component seeding for LLMs, database, and evaluation pipeline&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;ü§ñ &lt;strong&gt;Advanced LLM Integration&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ensemble Sophistication&lt;/strong&gt;: Weighted model combinations with intelligent fallback strategies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Test-Time Compute&lt;/strong&gt;: Integration with &lt;a href=&#34;https://github.com/codelion/optillm&#34;&gt;optillm&lt;/a&gt; for Mixture of Agents (MoA) and enhanced reasoning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Universal API Support&lt;/strong&gt;: Works with any OpenAI-compatible endpoint (Anthropic, Google, local models)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plugin Ecosystem&lt;/strong&gt;: Support for optillm plugins (readurls, executecode, z3_solver, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;üß¨ &lt;strong&gt;Evolution Algorithm Innovations&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MAP-Elites Implementation&lt;/strong&gt;: Quality-diversity algorithm for balanced exploration/exploitation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Island-Based Evolution&lt;/strong&gt;: Multiple populations with periodic migration for diversity maintenance&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inspiration vs Performance&lt;/strong&gt;: Sophisticated prompt engineering separating top performers from diverse inspirations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Strategy Selection&lt;/strong&gt;: Elite, diverse, and exploratory program sampling strategies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adaptive Feature Dimensions&lt;/strong&gt;: Default features (complexity &amp;amp; diversity) with customizable multi-dimensional search spaces&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;üìä &lt;strong&gt;Evaluation &amp;amp; Feedback Systems&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Artifacts Side-Channel&lt;/strong&gt;: Capture build errors, profiling data, and execution feedback for LLM improvement&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cascade Evaluation&lt;/strong&gt;: Multi-stage testing with progressive complexity for efficient resource usage&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM-Based Feedback&lt;/strong&gt;: Automated code quality assessment and reasoning capture&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Error Handling&lt;/strong&gt;: Graceful recovery from evaluation failures with detailed diagnostics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;üåê &lt;strong&gt;Multi-Language &amp;amp; Platform Support&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Language Agnostic&lt;/strong&gt;: Python, Rust, R, Metal shaders, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Platform Optimization&lt;/strong&gt;: Apple Silicon GPU kernels, CUDA optimization, CPU-specific tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Framework Integration&lt;/strong&gt;: MLX, PyTorch, scientific computing libraries&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;üîß &lt;strong&gt;Developer Experience &amp;amp; Tooling&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-Time Visualization&lt;/strong&gt;: Interactive web-based evolution tree viewer with performance analytics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced CLI&lt;/strong&gt;: Rich command-line interface with checkpoint management and configuration override&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Examples&lt;/strong&gt;: 12+ diverse examples spanning optimization, ML, systems programming, and scientific computing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Error Recovery&lt;/strong&gt;: Robust checkpoint loading with automatic fix for common serialization issues&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;üöÄ &lt;strong&gt;Performance &amp;amp; Scalability&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Process-Based Parallelism&lt;/strong&gt;: True parallel execution bypassing Python&#39;s GIL for CPU-bound tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resource Management&lt;/strong&gt;: Memory limits, timeouts, and resource monitoring&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient Storage&lt;/strong&gt;: Optimized database with artifact management and cleanup policies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;OpenEvolve orchestrates a sophisticated evolutionary pipeline:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/openevolve-architecture.png&#34; alt=&#34;OpenEvolve Architecture&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Core Evolution Loop&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Prompt Sampler&lt;/strong&gt;: Creates rich prompts containing:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Top-performing programs (for optimization guidance)&lt;/li&gt; &#xA;   &lt;li&gt;Diverse inspiration programs (for creative exploration)&lt;/li&gt; &#xA;   &lt;li&gt;Execution artifacts and error feedback&lt;/li&gt; &#xA;   &lt;li&gt;Dynamic documentation fetching (via optillm plugins)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Intelligent LLM Ensemble&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Weighted model combinations for quality/speed tradeoffs&lt;/li&gt; &#xA;   &lt;li&gt;Test-time compute techniques (MoA, chain-of-thought, reflection)&lt;/li&gt; &#xA;   &lt;li&gt;Deterministic selection with comprehensive seeding&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Evaluator Pool&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi-stage cascade evaluation&lt;/li&gt; &#xA;   &lt;li&gt;Artifact collection for detailed feedback&lt;/li&gt; &#xA;   &lt;li&gt;LLM-based code quality assessment&lt;/li&gt; &#xA;   &lt;li&gt;Parallel execution with resource limits&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sophisticated Program Database&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;MAP-Elites algorithm for quality-diversity balance&lt;/li&gt; &#xA;   &lt;li&gt;Island-based populations with migration&lt;/li&gt; &#xA;   &lt;li&gt;Feature map clustering and archive management&lt;/li&gt; &#xA;   &lt;li&gt;Comprehensive metadata and lineage tracking&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Island-Based Evolution with Worker Pinning&lt;/h3&gt; &#xA;&lt;p&gt;OpenEvolve implements a sophisticated island-based evolutionary architecture that maintains multiple isolated populations to prevent premature convergence and preserve genetic diversity.&lt;/p&gt; &#xA;&lt;h4&gt;How Islands Work&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Isolated Populations&lt;/strong&gt;: Each island maintains its own population of programs that evolve independently&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Periodic Migration&lt;/strong&gt;: Top-performing programs periodically migrate between adjacent islands (ring topology) to share beneficial mutations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;True Population Isolation&lt;/strong&gt;: Worker processes are deterministically pinned to specific islands to ensure no cross-contamination during parallel evolution&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Worker-to-Island Pinning&lt;/h4&gt; &#xA;&lt;p&gt;To ensure true island isolation during parallel execution, OpenEvolve implements automatic worker-to-island pinning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Workers are distributed across islands using modulo arithmetic&#xA;worker_id = 0, 1, 2, 3, 4, 5, ...&#xA;island_id = worker_id % num_islands&#xA;&#xA;# Example with 3 islands and 6 workers:&#xA;# Worker 0, 3 ‚Üí Island 0  &#xA;# Worker 1, 4 ‚Üí Island 1&#xA;# Worker 2, 5 ‚Üí Island 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Benefits of Worker Pinning&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Genetic Isolation&lt;/strong&gt;: Prevents accidental population mixing between islands during parallel sampling&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Consistent Evolution&lt;/strong&gt;: Each island maintains its distinct evolutionary trajectory&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Balanced Load&lt;/strong&gt;: Workers are evenly distributed across islands automatically&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Migration Integrity&lt;/strong&gt;: Controlled migration happens only at designated intervals, not due to race conditions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic Distribution&lt;/strong&gt;: The system handles all edge cases automatically:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;More workers than islands&lt;/strong&gt;: Multiple workers per island with balanced distribution&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fewer workers than islands&lt;/strong&gt;: Some islands may not have dedicated workers but still participate in migration&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Single island&lt;/strong&gt;: All workers sample from the same population (degrades to standard evolution)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This architecture ensures that each island develops unique evolutionary pressures and solutions, while periodic migration allows successful innovations to spread across the population without destroying diversity.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;To install natively, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/codelion/openevolve.git&#xA;cd openevolve&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;h4&gt;Setting up LLM Access&lt;/h4&gt; &#xA;&lt;p&gt;OpenEvolve uses the OpenAI SDK, which means it works with any LLM provider that supports an OpenAI-compatible API:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set the API Key&lt;/strong&gt;: Export the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=your-api-key-here&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using Alternative LLM Providers&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For providers other than OpenAI (e.g., Anthropic, Cohere, local models), update the &lt;code&gt;api_base&lt;/code&gt; in your config.yaml:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;llm:&#xA;  api_base: &#34;https://your-provider-endpoint.com/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Maximum Flexibility with optillm&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For advanced routing, rate limiting, or using multiple providers, we recommend &lt;a href=&#34;https://github.com/codelion/optillm&#34;&gt;optillm&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;optillm acts as a proxy that can route requests to different LLMs based on your rules&lt;/li&gt; &#xA;   &lt;li&gt;Simply point &lt;code&gt;api_base&lt;/code&gt; to your optillm instance:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;llm:&#xA;  api_base: &#34;http://localhost:8000/v1&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This setup ensures OpenEvolve can work with any LLM provider - OpenAI, Anthropic, Google, Cohere, local models via Ollama/vLLM, or any OpenAI-compatible endpoint.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from openevolve import OpenEvolve&#xA;&#xA;# Ensure API key is set&#xA;if not os.environ.get(&#34;OPENAI_API_KEY&#34;):&#xA;    raise ValueError(&#34;Please set OPENAI_API_KEY environment variable&#34;)&#xA;&#xA;# Initialize the system&#xA;evolve = OpenEvolve(&#xA;    initial_program_path=&#34;path/to/initial_program.py&#34;,&#xA;    evaluation_file=&#34;path/to/evaluator.py&#34;,&#xA;    config_path=&#34;path/to/config.yaml&#34;&#xA;)&#xA;&#xA;# Run the evolution&#xA;best_program = await evolve.run(iterations=1000)&#xA;print(f&#34;Best program metrics:&#34;)&#xA;for name, value in best_program.metrics.items():&#xA;    print(f&#34;  {name}: {value:.4f}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Command-Line Usage&lt;/h3&gt; &#xA;&lt;p&gt;OpenEvolve can also be run from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python openevolve-run.py path/to/initial_program.py path/to/evaluator.py --config path/to/config.yaml --iterations 1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Resuming from Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;OpenEvolve automatically saves checkpoints at intervals specified by the &lt;code&gt;checkpoint_interval&lt;/code&gt; config parameter (default is 10 iterations). You can resume an evolution run from a saved checkpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python openevolve-run.py path/to/initial_program.py path/to/evaluator.py \&#xA;  --config path/to/config.yaml \&#xA;  --checkpoint path/to/checkpoint_directory \&#xA;  --iterations 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When resuming from a checkpoint:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The system loads all previously evolved programs and their metrics&lt;/li&gt; &#xA; &lt;li&gt;Checkpoint numbering continues from where it left off (e.g., if loaded from checkpoint_50, the next checkpoint will be checkpoint_60)&lt;/li&gt; &#xA; &lt;li&gt;All evolution state is preserved (best programs, feature maps, archives, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Each checkpoint directory contains a copy of the best program at that point in time&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example workflow with checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run for 50 iterations (creates checkpoints at iterations 10, 20, 30, 40, 50)&#xA;python openevolve-run.py examples/function_minimization/initial_program.py \&#xA;  examples/function_minimization/evaluator.py \&#xA;  --iterations 50&#xA;&#xA;# Resume from checkpoint 50 for another 50 iterations (creates checkpoints at 60, 70, 80, 90, 100)&#xA;python openevolve-run.py examples/function_minimization/initial_program.py \&#xA;  examples/function_minimization/evaluator.py \&#xA;  --checkpoint examples/function_minimization/openevolve_output/checkpoints/checkpoint_50 \&#xA;  --iterations 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Comparing Results Across Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;Each checkpoint directory contains the best program found up to that point, making it easy to compare solutions over time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;checkpoints/&#xA;  checkpoint_10/&#xA;    best_program.py         # Best program at iteration 10&#xA;    best_program_info.json  # Metrics and details&#xA;    programs/               # All programs evaluated so far&#xA;    metadata.json           # Database state&#xA;  checkpoint_20/&#xA;    best_program.py         # Best program at iteration 20&#xA;    ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can compare the evolution of solutions by examining the best programs at different checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Compare best programs at different checkpoints&#xA;diff -u checkpoints/checkpoint_10/best_program.py checkpoints/checkpoint_20/best_program.py&#xA;&#xA;# Compare metrics&#xA;cat checkpoints/checkpoint_*/best_program_info.json | grep -A 10 metrics&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualizing the evolution tree&lt;/h3&gt; &#xA;&lt;p&gt;The script in &lt;code&gt;scripts/visualize.py&lt;/code&gt; allows you to visualize the evolution tree and display it in your webbrowser. The script watches live for the newest checkpoint directory in the examples/ folder structure and updates the graph. Alternatively, you can also provide a specific checkpoint folder with the &lt;code&gt;--path&lt;/code&gt; parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install requirements&#xA;pip install -r scripts/requirements.txt&#xA;&#xA;# Start the visualization web server and have it watch the examples/ folder&#xA;python scripts/visualizer.py&#xA;&#xA;# Start the visualization web server with a specific checkpoint&#xA;python scripts/visualizer.py --path examples/function_minimization/openevolve_output/checkpoints/checkpoint_100/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the visualization UI, you can&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;see the branching of your program evolution in a network visualization, with node radius chosen by the program fitness (= the currently selected metric),&lt;/li&gt; &#xA; &lt;li&gt;see the parent-child relationship of nodes and click through them in the sidebar (use the yellow locator icon in the sidebar to center the node in the graph),&lt;/li&gt; &#xA; &lt;li&gt;select the metric of interest (with the available metric choices depending on your data set),&lt;/li&gt; &#xA; &lt;li&gt;highlight nodes, for example the top score (for the chosen metric) or the MAP-elites members,&lt;/li&gt; &#xA; &lt;li&gt;click nodes to see their code and prompts (if available from the checkpoint data) in a sidebar,&lt;/li&gt; &#xA; &lt;li&gt;in the &#34;Performance&#34; tab, see their selected metric score vs generation in a graph&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/openevolve-visualizer.png&#34; alt=&#34;OpenEvolve Visualizer&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can also install and execute via Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t openevolve .&#xA;docker run --rm -v $(pwd):/app --network=&#34;host&#34; openevolve examples/function_minimization/initial_program.py examples/function_minimization/evaluator.py --config examples/function_minimization/config.yaml --iterations 1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;OpenEvolve is highly configurable with advanced options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Example configuration showcasing advanced features&#xA;max_iterations: 1000&#xA;random_seed: 42  # Full reproducibility by default&#xA;&#xA;llm:&#xA;  # Advanced ensemble configuration&#xA;  models:&#xA;    - name: &#34;gemini-2.0-flash-lite&#34;&#xA;      weight: 0.7&#xA;    - name: &#34;moa&amp;amp;readurls-gemini-2.0-flash&#34;  # optillm test-time compute&#xA;      weight: 0.3&#xA;  temperature: 0.7&#xA;  &#xA;database:&#xA;  # MAP-Elites configuration&#xA;  population_size: 500&#xA;  num_islands: 5  # Island-based evolution&#xA;  migration_interval: 20&#xA;  feature_dimensions: [&#34;complexity&#34;, &#34;diversity&#34;]  # Default quality-diversity features&#xA;  &#xA;evaluator:&#xA;  # Advanced evaluation features&#xA;  enable_artifacts: true  # Capture execution feedback&#xA;  cascade_evaluation: true  # Multi-stage testing&#xA;  use_llm_feedback: true  # AI-based code quality assessment&#xA;  &#xA;prompt:&#xA;  # Sophisticated prompt engineering&#xA;  num_top_programs: 3      # Performance examples&#xA;  num_diverse_programs: 2  # Creative inspiration&#xA;  include_artifacts: true  # Execution feedback&#xA;  &#xA;  # Template customization&#xA;  template_dir: null               # Directory for custom prompt templates&#xA;  use_template_stochasticity: true # Enable random variations in prompts&#xA;  template_variations: {}          # Define variation placeholders&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sample configuration files are available in the &lt;code&gt;configs/&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;default_config.yaml&lt;/code&gt;: Comprehensive configuration with all available options&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;island_config_example.yaml&lt;/code&gt;: Advanced island-based evolution setup&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prompt Engineering Design&lt;/h3&gt; &#xA;&lt;p&gt;OpenEvolve uses a sophisticated prompt engineering approach that separates different types of program examples to optimize LLM learning:&lt;/p&gt; &#xA;&lt;h4&gt;Program Selection Strategy&lt;/h4&gt; &#xA;&lt;p&gt;The system distinguishes between three types of program examples shown to the LLM:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Previous Attempts&lt;/strong&gt; (&lt;code&gt;num_top_programs&lt;/code&gt;): Shows only the best performing programs to demonstrate high-quality approaches&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Used for the &#34;Previous Attempts&#34; section in prompts&lt;/li&gt; &#xA;   &lt;li&gt;Focused on proven successful patterns&lt;/li&gt; &#xA;   &lt;li&gt;Helps LLM understand what constitutes good performance&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Top Programs&lt;/strong&gt; (&lt;code&gt;num_top_programs + num_diverse_programs&lt;/code&gt;): Broader selection including both top performers and diverse approaches&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Used for the &#34;Top Performing Programs&#34; section&lt;/li&gt; &#xA;   &lt;li&gt;Includes diverse programs to prevent local optima&lt;/li&gt; &#xA;   &lt;li&gt;Balances exploitation of known good solutions with exploration of novel approaches&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inspirations&lt;/strong&gt; (&lt;code&gt;num_top_programs&lt;/code&gt;): Cross-island program samples for creative inspiration&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Derived from other evolution islands to maintain diversity&lt;/li&gt; &#xA;   &lt;li&gt;Count automatically configures based on &lt;code&gt;num_top_programs&lt;/code&gt; setting&lt;/li&gt; &#xA;   &lt;li&gt;Prevents convergence by exposing LLM to different evolutionary trajectories&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Design Rationale&lt;/h4&gt; &#xA;&lt;p&gt;This separation is intentional and serves multiple purposes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Focused Learning&lt;/strong&gt;: Previous attempts show only the best patterns, helping LLM understand quality standards&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diversity Maintenance&lt;/strong&gt;: Top programs include diverse solutions to encourage exploration beyond local optima&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Pollination&lt;/strong&gt;: Inspirations from other islands introduce novel approaches and prevent stagnation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configurable Balance&lt;/strong&gt;: Adjust &lt;code&gt;num_top_programs&lt;/code&gt; and &lt;code&gt;num_diverse_programs&lt;/code&gt; to control exploration vs exploitation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The inspiration count automatically scales with &lt;code&gt;num_top_programs&lt;/code&gt; to maintain consistency across different configuration sizes, eliminating the need for a separate configuration parameter.&lt;/p&gt; &#xA;&lt;h3&gt;Template Customization&lt;/h3&gt; &#xA;&lt;p&gt;OpenEvolve supports advanced prompt template customization to increase diversity in code evolution:&lt;/p&gt; &#xA;&lt;h4&gt;Custom Templates with &lt;code&gt;template_dir&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;You can override the default prompt templates by providing custom ones:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;prompt:&#xA;  template_dir: &#34;path/to/your/templates&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create &lt;code&gt;.txt&lt;/code&gt; files in your template directory with these names:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;diff_user.txt&lt;/code&gt; - Template for diff-based evolution&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;full_rewrite_user.txt&lt;/code&gt; - Template for full code rewrites&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;evolution_history.txt&lt;/code&gt; - Format for presenting evolution history&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_program.txt&lt;/code&gt; - Format for top-performing programs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;previous_attempt.txt&lt;/code&gt; - Format for previous attempts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See these directories for complete examples of custom templates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/lm_eval/prompts/&lt;/code&gt; - Custom templates for evaluation tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;examples/llm_prompt_optimization/templates/&lt;/code&gt; - Templates for evolving prompts instead of code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Template Variations with Stochasticity&lt;/h4&gt; &#xA;&lt;p&gt;To add randomness to your prompts and prevent getting stuck in local optima:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enable stochasticity&lt;/strong&gt; in your config:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;prompt:&#xA;  use_template_stochasticity: true&#xA;  template_variations:&#xA;    greeting:&#xA;      - &#34;Let&#39;s improve this code.&#34;&#xA;      - &#34;Time to enhance this program.&#34;&#xA;      - &#34;Here&#39;s how we can optimize:&#34;&#xA;    analysis_intro:&#xA;      - &#34;Current metrics show&#34;&#xA;      - &#34;Performance analysis indicates&#34;&#xA;      - &#34;The evaluation reveals&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Use variation placeholders&lt;/strong&gt; in your custom templates:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# custom_template.txt&#xA;{greeting}&#xA;{analysis_intro} the following results:&#xA;{metrics}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The system will randomly select one variation for each placeholder during prompt generation, creating diverse prompts that can lead to more creative code evolutions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The default templates don&#39;t include variation placeholders, so you&#39;ll need to create custom templates to use this feature effectively.&lt;/p&gt; &#xA;&lt;h3&gt;Feature Dimensions in MAP-Elites&lt;/h3&gt; &#xA;&lt;p&gt;Feature dimensions control how programs are organized in the MAP-Elites quality-diversity grid:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Default Features&lt;/strong&gt;: If &lt;code&gt;feature_dimensions&lt;/code&gt; is NOT specified in your config, OpenEvolve uses &lt;code&gt;[&#34;complexity&#34;, &#34;diversity&#34;]&lt;/code&gt; as defaults.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Built-in Features&lt;/strong&gt; (always computed internally by OpenEvolve):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;complexity&lt;/strong&gt;: Code length (recommended default)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;diversity&lt;/strong&gt;: Code structure diversity compared to other programs (recommended default)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Only &lt;code&gt;complexity&lt;/code&gt; and &lt;code&gt;diversity&lt;/code&gt; are used as defaults because they work well across all program types.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom Features&lt;/strong&gt;: You can mix built-in features with metrics from your evaluator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;database:&#xA;  feature_dimensions: [&#34;complexity&#34;, &#34;performance&#34;, &#34;correctness&#34;]  # Mix of built-in and custom&#xA;  # Per-dimension bin configuration (optional)&#xA;  feature_bins: &#xA;    complexity: 10        # 10 bins for complexity&#xA;    performance: 20       # 20 bins for performance (from YOUR evaluator)&#xA;    correctness: 15       # 15 bins for correctness (from YOUR evaluator)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;CRITICAL: Return Raw Values, Not Bin Indices&lt;/strong&gt;: For custom feature dimensions, your evaluator must return &lt;strong&gt;raw continuous values&lt;/strong&gt;, not pre-computed bin indices. OpenEvolve handles all scaling and binning internally.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ‚úÖ CORRECT: Return raw values&#xA;return {&#xA;    &#34;combined_score&#34;: 0.85,&#xA;    &#34;prompt_length&#34;: 1247,     # Actual character count&#xA;    &#34;execution_time&#34;: 0.234    # Raw time in seconds&#xA;}&#xA;&#xA;# ‚ùå WRONG: Don&#39;t return bin indices&#xA;return {&#xA;    &#34;combined_score&#34;: 0.85,&#xA;    &#34;prompt_length&#34;: 7,        # Pre-computed bin index&#xA;    &#34;execution_time&#34;: 3        # Pre-computed bin index&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OpenEvolve automatically handles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Min-max scaling to [0,1] range&lt;/li&gt; &#xA; &lt;li&gt;Binning into the specified number of bins&lt;/li&gt; &#xA; &lt;li&gt;Adaptive scaling as the value range expands during evolution&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: OpenEvolve will raise an error if a specified feature is not found in the evaluator&#39;s metrics. This ensures your configuration is correct. The error message will show available metrics to help you fix the configuration.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/configs/default_config.yaml&#34;&gt;Configuration Guide&lt;/a&gt; for a full list of options.&lt;/p&gt; &#xA;&lt;h3&gt;Default Metric for Program Selection&lt;/h3&gt; &#xA;&lt;p&gt;When comparing and selecting programs, OpenEvolve uses the following priority:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;combined_score&lt;/strong&gt;: If your evaluator returns a &lt;code&gt;combined_score&lt;/code&gt; metric, it will be used as the primary fitness measure&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Average of all metrics&lt;/strong&gt;: If no &lt;code&gt;combined_score&lt;/code&gt; is provided, OpenEvolve calculates the average of all numeric metrics returned by your evaluator&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This ensures programs can always be compared even without explicit fitness definitions. For best results, consider having your evaluator return a &lt;code&gt;combined_score&lt;/code&gt; that represents overall program fitness.&lt;/p&gt; &#xA;&lt;h2&gt;Artifacts Channel&lt;/h2&gt; &#xA;&lt;p&gt;OpenEvolve includes an &lt;strong&gt;artifacts side-channel&lt;/strong&gt; that allows evaluators to capture build errors, profiling results, etc. to provide better feedback to the LLM in subsequent generations. This feature enhances the evolution process by giving the LLM context about what went wrong and how to fix it.&lt;/p&gt; &#xA;&lt;p&gt;The artifacts channel operates alongside the traditional fitness metrics.&lt;/p&gt; &#xA;&lt;h3&gt;Example: Compilation Failure Feedback&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openevolve.evaluation_result import EvaluationResult&#xA;&#xA;return EvaluationResult(&#xA;    metrics={&#34;compile_ok&#34;: 0.0, &#34;score&#34;: 0.0},&#xA;    artifacts={&#xA;        &#34;stderr&#34;: &#34;SyntaxError: invalid syntax (line 15)&#34;,&#xA;        &#34;traceback&#34;: &#34;...&#34;,&#xA;        &#34;failure_stage&#34;: &#34;compilation&#34;&#xA;    }&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The next generation prompt will include:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## Last Execution Output&#xA;### Stderr&#xA;SyntaxError: invalid syntax (line 15)&#xA;&#xA;### Traceback&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example: LLM Feedback&lt;/h2&gt; &#xA;&lt;p&gt;An example for an LLM artifact side channel is part of the default evaluation template, which ends with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;Return your evaluation as a JSON object with the following format:&#xA;{{&#xA;    &#34;readability&#34;: [score],&#xA;    &#34;maintainability&#34;: [score],&#xA;    &#34;efficiency&#34;: [score],&#xA;    &#34;reasoning&#34;: &#34;[brief explanation of scores]&#34;&#xA;}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The non-float values, in this case the &#34;reasoning&#34; key of the json response that the evaluator LLM generates, will be available within the next generation prompt.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Artifacts can be controlled via configuration and environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# config.yaml&#xA;evaluator:&#xA;  enable_artifacts: true&#xA;&#xA;prompt:&#xA;  include_artifacts: true&#xA;  max_artifact_bytes: 4096  # 4KB limit in prompts&#xA;  artifact_security_filter: true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Environment variable to disable artifacts&#xA;export ENABLE_ARTIFACTS=false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Benefits&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Faster convergence&lt;/strong&gt; - LLMs can see what went wrong and fix it directly&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Better error handling&lt;/strong&gt; - Compilation and runtime failures become learning opportunities&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich debugging context&lt;/strong&gt; - Full stack traces and error messages guide improvements&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero overhead&lt;/strong&gt; - When disabled, no performance impact on evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;code&gt;examples/&lt;/code&gt; directory for complete examples of using OpenEvolve on various problems:&lt;/p&gt; &#xA;&lt;h3&gt;Mathematical Optimization&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/function_minimization/&#34;&gt;Function Minimization&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;A comprehensive example demonstrating evolution from random search to sophisticated simulated annealing.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/circle_packing/&#34;&gt;Circle Packing&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Our implementation of the circle packing problem. For the n=26 case, we achieve state-of-the-art results matching published benchmarks.&lt;/p&gt; &#xA;&lt;p&gt;Below is the optimal packing found by OpenEvolve after 800 iterations:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/00100f9e-2ac3-445b-9266-0398b7174193&#34; alt=&#34;circle-packing-result&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Advanced AI &amp;amp; LLM Integration&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/web_scraper_optillm/&#34;&gt;Web Scraper with optillm&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Demonstrates integration with &lt;a href=&#34;https://github.com/codelion/optillm&#34;&gt;optillm&lt;/a&gt; for test-time compute optimization, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;readurls plugin&lt;/strong&gt;: Automatic documentation fetching&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mixture of Agents (MoA)&lt;/strong&gt;: Multi-response synthesis for improved accuracy&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local model optimization&lt;/strong&gt;: Enhanced reasoning with smaller models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/llm_prompt_optimization/&#34;&gt;LLM Prompt Optimization&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Evolving prompts for better LLM performance on HuggingFace datasets. Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Custom templates for evolving prompts instead of code&lt;/li&gt; &#xA; &lt;li&gt;Two-stage cascading evaluation for efficiency&lt;/li&gt; &#xA; &lt;li&gt;Support for any HuggingFace dataset&lt;/li&gt; &#xA; &lt;li&gt;Automatic prompt improvement through evolution&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Systems &amp;amp; Performance Optimization&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/mlx_metal_kernel_opt/&#34;&gt;MLX Metal Kernel Optimization&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Automated discovery of custom GPU kernels for Apple Silicon, achieving:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2-3x speedup&lt;/strong&gt; over baseline attention implementations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware-aware optimizations&lt;/strong&gt; for unified memory architecture&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Metal shader evolution&lt;/strong&gt; with numerical correctness validation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/rust_adaptive_sort/&#34;&gt;Rust Adaptive Sort&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Evolution of sorting algorithms that adapt to data patterns, showcasing OpenEvolve&#39;s language-agnostic capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Scientific Computing &amp;amp; Discovery&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/symbolic_regression/&#34;&gt;Symbolic Regression&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;A comprehensive example demonstrating automated discovery of mathematical expressions from scientific datasets using the LLM-SRBench benchmark.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/r_robust_regression/&#34;&gt;R Robust Regression&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Developing robust regression methods resistant to outliers using R language support.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/signal_processing/&#34;&gt;Signal Processing&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Automated design of digital filters with superior performance characteristics.&lt;/p&gt; &#xA;&lt;h3&gt;Web and Integration Examples&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/online_judge_programming/&#34;&gt;Online Judge Programming&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Automated competitive programming solution generation with external evaluation systems.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://raw.githubusercontent.com/codelion/openevolve/main/examples/lm_eval/&#34;&gt;LM-Eval Integration&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Working with standard ML evaluation harnesses for automated benchmark improvement.&lt;/p&gt; &#xA;&lt;h2&gt;Preparing Your Own Problems&lt;/h2&gt; &#xA;&lt;p&gt;To use OpenEvolve for your own problems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mark code sections&lt;/strong&gt; to evolve with &lt;code&gt;# EVOLVE-BLOCK-START&lt;/code&gt; and &lt;code&gt;# EVOLVE-BLOCK-END&lt;/code&gt; comments&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create an evaluation function&lt;/strong&gt; that returns a dictionary of metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configure OpenEvolve&lt;/strong&gt; with appropriate parameters&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run the evolution&lt;/strong&gt; process&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use OpenEvolve in your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{openevolve,&#xA;  title = {OpenEvolve: an open-source evolutionary coding agent},&#xA;  author = {Asankhaya Sharma},&#xA;  year = {2025},&#xA;  publisher = {GitHub},&#xA;  url = {https://github.com/codelion/openevolve}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>KoljaB/RealtimeVoiceChat</title>
    <updated>2025-08-23T01:35:54Z</updated>
    <id>tag:github.com,2025-08-23:/KoljaB/RealtimeVoiceChat</id>
    <link href="https://github.com/KoljaB/RealtimeVoiceChat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Have a natural, spoken conversation with AI!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time AI Voice Chat üé§üí¨üß†üîä&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Have a natural, spoken conversation with an AI!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/16cc29a7-bec2-4dd0-a056-d213db798d8f&#34;&gt;https://github.com/user-attachments/assets/16cc29a7-bec2-4dd0-a056-d213db798d8f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(early preview - first reasonably stable version)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùó &lt;strong&gt;Project Status: Community-Driven&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;This project is no longer being actively maintained by me due to time constraints. I&#39;ve taken on too many projects and I have to step back. I will no longer be implementing new features or providing user support.&lt;/p&gt; &#xA; &lt;p&gt;I will continue to review and merge high-quality, well-written Pull Requests from the community from time to time. Your contributions are welcome and appreciated!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What&#39;s Under the Hood?&lt;/h2&gt; &#xA;&lt;p&gt;A sophisticated client-server system built for low-latency interaction:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üéôÔ∏è &lt;strong&gt;Capture:&lt;/strong&gt; Your voice is captured by your browser.&lt;/li&gt; &#xA; &lt;li&gt;‚û°Ô∏è &lt;strong&gt;Stream:&lt;/strong&gt; Audio chunks are whisked away via WebSockets to a Python backend.&lt;/li&gt; &#xA; &lt;li&gt;‚úçÔ∏è &lt;strong&gt;Transcribe:&lt;/strong&gt; &lt;code&gt;RealtimeSTT&lt;/code&gt; rapidly converts your speech to text.&lt;/li&gt; &#xA; &lt;li&gt;ü§î &lt;strong&gt;Think:&lt;/strong&gt; The text is sent to an LLM (like Ollama or OpenAI) for processing.&lt;/li&gt; &#xA; &lt;li&gt;üó£Ô∏è &lt;strong&gt;Synthesize:&lt;/strong&gt; The AI&#39;s text response is turned back into speech using &lt;code&gt;RealtimeTTS&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚¨ÖÔ∏è &lt;strong&gt;Return:&lt;/strong&gt; The generated audio is streamed back to your browser for playback.&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Interrupt:&lt;/strong&gt; Jump in anytime! The system handles interruptions gracefully.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Key Features ‚ú®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fluid Conversation:&lt;/strong&gt; Speak and listen, just like a real chat.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-Time Feedback:&lt;/strong&gt; See partial transcriptions and AI responses as they happen.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low Latency Focus:&lt;/strong&gt; Optimized architecture using audio chunk streaming.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart Turn-Taking:&lt;/strong&gt; Dynamic silence detection (&lt;code&gt;turndetect.py&lt;/code&gt;) adapts to the conversation pace.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible AI Brains:&lt;/strong&gt; Pluggable LLM backends (Ollama default, OpenAI support via &lt;code&gt;llm_module.py&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable Voices:&lt;/strong&gt; Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via &lt;code&gt;audio_module.py&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Web Interface:&lt;/strong&gt; Clean and simple UI using Vanilla JS and the Web Audio API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dockerized Deployment:&lt;/strong&gt; Recommended setup using Docker Compose for easier dependency management.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Technology Stack üõ†Ô∏è&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Python &amp;lt; 3.13, FastAPI&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Communication:&lt;/strong&gt; WebSockets&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Containerization:&lt;/strong&gt; Docker, Docker Compose&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Core AI/ML Libraries:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;RealtimeSTT&lt;/code&gt; (Speech-to-Text)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;RealtimeTTS&lt;/code&gt; (Text-to-Speech)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; (Turn detection, Tokenization)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;torch&lt;/code&gt; / &lt;code&gt;torchaudio&lt;/code&gt; (ML Framework)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ollama&lt;/code&gt; / &lt;code&gt;openai&lt;/code&gt; (LLM Clients)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Audio Processing:&lt;/strong&gt; &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Before You Dive In: Prerequisites üèä‚Äç‚ôÄÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;This project leverages powerful AI models, which have some requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Operating System:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Docker:&lt;/strong&gt; Linux is recommended for the best GPU integration with Docker.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Manual:&lt;/strong&gt; The provided script (&lt;code&gt;install.bat&lt;/code&gt;) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üêç Python:&lt;/strong&gt; 3.9 or higher (if setting up manually).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üöÄ GPU:&lt;/strong&gt; &lt;strong&gt;A powerful CUDA-enabled NVIDIA GPU is &lt;em&gt;highly recommended&lt;/em&gt;&lt;/strong&gt;, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The setup assumes &lt;strong&gt;CUDA 12.1&lt;/strong&gt;. Adjust PyTorch installation if you have a different CUDA version.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Docker (Linux):&lt;/strong&gt; Requires &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üê≥ Docker (Optional but Recommended):&lt;/strong&gt; Docker Engine and Docker Compose v2+ for the containerized setup.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üß† Ollama (Optional):&lt;/strong&gt; If using the Ollama backend &lt;em&gt;without&lt;/em&gt; Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîë OpenAI API Key (Optional):&lt;/strong&gt; If using the OpenAI backend, set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable (e.g., in a &lt;code&gt;.env&lt;/code&gt; file or passed to Docker).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Getting Started: Installation &amp;amp; Setup ‚öôÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Clone the repository first:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/KoljaB/RealtimeVoiceChat.git&#xA;cd RealtimeVoiceChat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, choose your adventure:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;üöÄ Option A: Docker Installation (Recommended for Linux/GPU)&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build the Docker images:&lt;/strong&gt; &lt;em&gt;(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(If you want to customize models/settings in &lt;code&gt;code/*.py&lt;/code&gt;, do it &lt;strong&gt;before&lt;/strong&gt; this step!)&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the services (App &amp;amp; Ollama):&lt;/strong&gt; &lt;em&gt;(Runs containers in the background. GPU access is configured in &lt;code&gt;docker-compose.yml&lt;/code&gt;.)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Give them a minute to initialize.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;(Crucial!) Pull your desired Ollama Model:&lt;/strong&gt; &lt;em&gt;(This is done &lt;em&gt;after&lt;/em&gt; startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Pull the default model (adjust if you configured a different one in server.py)&#xA;docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M&#xA;&#xA;# (Optional) Verify the model is available&#xA;docker compose exec ollama ollama list&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stopping the Services:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose down&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restarting:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Viewing Logs / Debugging:&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Follow app logs: &lt;code&gt;docker compose logs -f app&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Follow Ollama logs: &lt;code&gt;docker compose logs -f ollama&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Save logs to file: &lt;code&gt;docker compose logs app &amp;gt; app_logs.txt&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Option B: Manual Installation (Windows Script / venv)&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;B1) Using the Windows Install Script:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Ensure you meet the prerequisites (Python, potentially CUDA drivers).&lt;/li&gt; &#xA;  &lt;li&gt;Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements. &lt;pre&gt;&lt;code class=&#34;language-batch&#34;&gt;install.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;em&gt;(This opens a new command prompt within the activated virtual environment.)&lt;/em&gt; Proceed to the &lt;strong&gt;&#34;Running the Application&#34;&lt;/strong&gt; section.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;strong&gt;B2) Manual Steps (Linux/macOS/Windows):&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create &amp;amp; Activate Virtual Environment:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;# Linux/macOS:&#xA;source venv/bin/activate&#xA;# Windows:&#xA;.\venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgrade Pip:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install --upgrade pip&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to Code Directory:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd code&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch (Crucial Step - Match Your Hardware!):&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;With NVIDIA GPU (CUDA 12.1 Example):&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Verify your CUDA version! Adjust &#39;cu121&#39; and the URL if needed.&#xA;pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;CPU Only (Expect Slow Performance):&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# pip install torch torchaudio torchvision&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;em&gt;Find other PyTorch versions:&lt;/em&gt; &lt;a href=&#34;https://pytorch.org/get-started/previous-versions/&#34;&gt;https://pytorch.org/get-started/previous-versions/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Other Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Note on DeepSpeed:&lt;/strong&gt; The &lt;code&gt;requirements.txt&lt;/code&gt; may include DeepSpeed. Installation can be complex, especially on Windows. The &lt;code&gt;install.bat&lt;/code&gt; tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like &lt;a href=&#34;https://github.com/erew123/deepspeedpatcher&#34;&gt;deepspeedpatcher&lt;/a&gt; (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Running the Application ‚ñ∂Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If using Docker:&lt;/strong&gt; Your application is already running via &lt;code&gt;docker compose up -d&lt;/code&gt;! Check logs using &lt;code&gt;docker compose logs -f app&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If using Manual/Script Installation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Activate your virtual environment&lt;/strong&gt; (if not already active): &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Linux/macOS: source ../venv/bin/activate&#xA;# Windows: ..\venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Navigate to the &lt;code&gt;code&lt;/code&gt; directory&lt;/strong&gt; (if not already there): &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd code&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Start the FastAPI server:&lt;/strong&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accessing the Client (Both Methods):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open your web browser to &lt;code&gt;http://localhost:8000&lt;/code&gt; (or your server&#39;s IP if running remotely/in Docker on another machine).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Grant microphone permissions&lt;/strong&gt; when prompted.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;&#34;Start&#34;&lt;/strong&gt; to begin chatting! Use &#34;Stop&#34; to end and &#34;Reset&#34; to clear the conversation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Configuration Deep Dive üîß&lt;/h2&gt; &#xA;&lt;p&gt;Want to tweak the AI&#39;s voice, brain, or how it listens? Modify the Python files in the &lt;code&gt;code/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important Docker Note:&lt;/strong&gt; If using Docker, make any configuration changes &lt;em&gt;before&lt;/em&gt; running &lt;code&gt;docker compose build&lt;/code&gt; to ensure they are included in the image.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TTS Engine &amp;amp; Voice (&lt;code&gt;server.py&lt;/code&gt;, &lt;code&gt;audio_module.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Change &lt;code&gt;START_ENGINE&lt;/code&gt; in &lt;code&gt;server.py&lt;/code&gt; to &lt;code&gt;&#34;coqui&#34;&lt;/code&gt;, &lt;code&gt;&#34;kokoro&#34;&lt;/code&gt;, or &lt;code&gt;&#34;orpheus&#34;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within &lt;code&gt;AudioProcessor.__init__&lt;/code&gt; in &lt;code&gt;audio_module.py&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Backend &amp;amp; Model (&lt;code&gt;server.py&lt;/code&gt;, &lt;code&gt;llm_module.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set &lt;code&gt;LLM_START_PROVIDER&lt;/code&gt; (&lt;code&gt;&#34;ollama&#34;&lt;/code&gt; or &lt;code&gt;&#34;openai&#34;&lt;/code&gt;) and &lt;code&gt;LLM_START_MODEL&lt;/code&gt; (e.g., &lt;code&gt;&#34;hf.co/...&#34;&lt;/code&gt; for Ollama, model name for OpenAI) in &lt;code&gt;server.py&lt;/code&gt;. Remember to pull the Ollama model if using Docker (see Installation Step A3).&lt;/li&gt; &#xA;   &lt;li&gt;Customize the AI&#39;s personality by editing &lt;code&gt;system_prompt.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;STT Settings (&lt;code&gt;transcribe.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Modify &lt;code&gt;DEFAULT_RECORDER_CONFIG&lt;/code&gt; to change the Whisper model (&lt;code&gt;model&lt;/code&gt;), language (&lt;code&gt;language&lt;/code&gt;), silence thresholds (&lt;code&gt;silence_limit_seconds&lt;/code&gt;), etc. The default &lt;code&gt;base.en&lt;/code&gt; model is pre-downloaded during the Docker build.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Turn Detection Sensitivity (&lt;code&gt;turndetect.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Adjust pause duration constants within the &lt;code&gt;TurnDetector.update_settings&lt;/code&gt; method.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SSL/HTTPS (&lt;code&gt;server.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set &lt;code&gt;USE_SSL = True&lt;/code&gt; and provide paths to your certificate (&lt;code&gt;SSL_CERT_PATH&lt;/code&gt;) and key (&lt;code&gt;SSL_KEY_PATH&lt;/code&gt;) files.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Docker Users:&lt;/strong&gt; You&#39;ll need to adjust &lt;code&gt;docker-compose.yml&lt;/code&gt; to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;&lt;strong&gt;Generating Local SSL Certificates (Windows Example w/ mkcert)&lt;/strong&gt;&lt;/summary&gt; &#xA;   &lt;ol&gt; &#xA;    &lt;li&gt;Install Chocolatey package manager if you haven&#39;t already.&lt;/li&gt; &#xA;    &lt;li&gt;Install mkcert: &lt;code&gt;choco install mkcert&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Run Command Prompt &lt;em&gt;as Administrator&lt;/em&gt;.&lt;/li&gt; &#xA;    &lt;li&gt;Install a local Certificate Authority: &lt;code&gt;mkcert -install&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Generate certs (replace &lt;code&gt;your.local.ip&lt;/code&gt;): &lt;code&gt;mkcert localhost 127.0.0.1 ::1 your.local.ip&lt;/code&gt; &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;This creates &lt;code&gt;.pem&lt;/code&gt; files (e.g., &lt;code&gt;localhost+3.pem&lt;/code&gt; and &lt;code&gt;localhost+3-key.pem&lt;/code&gt;) in the current directory. Update &lt;code&gt;SSL_CERT_PATH&lt;/code&gt; and &lt;code&gt;SSL_KEY_PATH&lt;/code&gt; in &lt;code&gt;server.py&lt;/code&gt; accordingly. Remember to potentially mount these into your Docker container.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ol&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;Contributing ü§ù&lt;/h2&gt; &#xA;&lt;p&gt;Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.&lt;/p&gt; &#xA;&lt;h2&gt;License üìú&lt;/h2&gt; &#xA;&lt;p&gt;The core codebase of this project is released under the &lt;strong&gt;MIT License&lt;/strong&gt; (see the &lt;a href=&#34;https://raw.githubusercontent.com/KoljaB/RealtimeVoiceChat/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details).&lt;/p&gt; &#xA;&lt;p&gt;This project relies on external specific TTS engines (like &lt;code&gt;Coqui XTTSv2&lt;/code&gt;) and LLM providers which have their &lt;strong&gt;own licensing terms&lt;/strong&gt;. Please ensure you comply with the licenses of all components you use.&lt;/p&gt;</summary>
  </entry>
</feed>