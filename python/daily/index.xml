<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-08T01:40:59Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tonyzhaozh/aloha</title>
    <updated>2024-01-08T01:40:59Z</updated>
    <id>tag:github.com,2024-01-08:/tonyzhaozh/aloha</id>
    <link href="https://github.com/tonyzhaozh/aloha" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ALOHA: A Low-cost Open-source Hardware System for Bimanual Teleoperation&lt;/h1&gt; &#xA;&lt;h4&gt;Project Website: &lt;a href=&#34;https://tonyzhaozh.github.io/aloha/&#34;&gt;https://tonyzhaozh.github.io/aloha/&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This codebase contains implementation for teleoperation and data collection with the ALOHA hardware. To build ALOHA, follow the &lt;a href=&#34;https://docs.google.com/document/d/1sgRZmpS7HMcZTPfGy3kAxDrqFMtNNzmK-yVtX5cKYME/edit?usp=sharing&#34;&gt;Hardware Assembly Tutorial&lt;/a&gt; and the quick start guide below. To train imitation learning algorithms, you would also need to install &lt;a href=&#34;https://github.com/tonyzhaozh/act&#34;&gt;ACT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Repo Structure&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config&lt;/code&gt;: a config for each robot, designating the port they should bind to, more details in quick start guide.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;launch&lt;/code&gt;: a ROS launch file for all 4 cameras and all 4 robots.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;aloha_scripts&lt;/code&gt;: python code for teleop and data collection&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start guide&lt;/h2&gt; &#xA;&lt;h3&gt;Hardware selection&lt;/h3&gt; &#xA;&lt;p&gt;We suggest using a &#34;heavy-duty&#34; computer if possible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;In particular, at least 6 USB3 ports are needed. 4 ports for robot connections and 2 ports for cameras.&lt;/em&gt; We have seen cases that a machine was not able to stably connect to all 4 robot arms simultaneously over USB, especially when USB hubs are used.&lt;/p&gt; &#xA;&lt;h3&gt;Software selection -- OS:&lt;/h3&gt; &#xA;&lt;p&gt;Currently tested and working configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;âœ…&lt;/span&gt; Ubuntu 18.04 + ROS 1 noetic&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;âœ…&lt;/span&gt; Ubuntu 20.04 + ROS 1 noetic&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Ongoing testing (compatibility effort underway):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;ðŸš§&lt;/span&gt; ROS 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Software installation - ROS:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install ROS and interbotix software following &lt;a href=&#34;https://docs.trossenrobotics.com/interbotix_xsarms_docs/&#34;&gt;https://docs.trossenrobotics.com/interbotix_xsarms_docs/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;This will create the directory &lt;code&gt;~/interbotix_ws&lt;/code&gt; which contains &lt;code&gt;src&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;git clone this repo inside &lt;code&gt;~/interbotix_ws/src&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;source /opt/ros/noetic/setup.sh &amp;amp;&amp;amp; source ~/interbotix_ws/devel/setup.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sudo apt-get install ros-noetic-usb-cam &amp;amp;&amp;amp; sudo apt-get install ros-noetic-cv-bridge&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;catkin_make&lt;/code&gt; inside &lt;code&gt;~/interbotix_ws&lt;/code&gt;, make sure the build is successful&lt;/li&gt; &#xA; &lt;li&gt;go to &lt;code&gt;~/interbotix_ws/src/interbotix_ros_toolboxes/interbotix_xs_toolbox/interbotix_xs_modules/src/interbotix_xs_modules/arm.py&lt;/code&gt;, find function &lt;code&gt;publish_positions&lt;/code&gt;. Change &lt;code&gt;self.T_sb = mr.FKinSpace(self.robot_des.M, self.robot_des.Slist, self.joint_commands)&lt;/code&gt; to &lt;code&gt;self.T_sb = None&lt;/code&gt;. This prevents the code from calculating FK at every step which delays teleoperation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Hardware installation:&lt;/h3&gt; &#xA;&lt;p&gt;The goal of this section is to run &lt;code&gt;roslaunch aloha 4arms_teleop.launch&lt;/code&gt;, which starts communication with 4 robots and 4 cameras. It should work after finishing the following steps:&lt;/p&gt; &#xA;&lt;p&gt;Step 1: Connect 4 robots to the computer via USB, and power on. &lt;em&gt;Do not use extension cable or usb hub&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To check if the robot is connected, install dynamixel wizard &lt;a href=&#34;https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_wizard2/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dynamixel wizard is a very helpful debugging tool that connects to individual motors of the robot. It allows things such as rebooting the motor (very useful!), torque on/off, and sending commands. However, it has no knowledge about the kinematics of the robot, so be careful about collisions. The robot &lt;em&gt;will&lt;/em&gt; collapse if motors are torque off i.e. there is no automatically engaged brakes in joints.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open Dynamixel wizard, go into &lt;code&gt;options&lt;/code&gt; and select:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Protocal 2.0&lt;/li&gt; &#xA;   &lt;li&gt;All ports&lt;/li&gt; &#xA;   &lt;li&gt;1000000 bps&lt;/li&gt; &#xA;   &lt;li&gt;ID range from 0-10&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Note: repeat above everytime before you scan.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then hit &lt;code&gt;Scan&lt;/code&gt;. There should be 4 devices showing up, each with 9 motors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;One issue that arises is the port each robot binds to can change over time, e.g. a robot that is initially &lt;code&gt;ttyUSB0&lt;/code&gt; might suddenly become &lt;code&gt;ttyUSB5&lt;/code&gt;. To resolve this, we bind each robot to a fixed symlink port with the following mapping:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;ttyDXL_master_right&lt;/code&gt;: right master robot (master: the robot that the operator would be holding)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ttyDXL_puppet_right&lt;/code&gt;: right puppet robot (puppet: the robot that performs the task)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ttyDXL_master_left&lt;/code&gt;: left master robot&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ttyDXL_puppet_left&lt;/code&gt;: left puppet robot&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Take &lt;code&gt;ttyDXL_master_right&lt;/code&gt;: right master robot as an example:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt; &lt;p&gt;Find the port that the right master robot is currently binding to, e.g. &lt;code&gt;ttyUSB0&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;run &lt;code&gt;udevadm info --name=/dev/ttyUSB0 --attribute-walk | grep serial&lt;/code&gt; to obtain the serial number. Use the first one that shows up, the format should look similar to &lt;code&gt;FT6S4DSP&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;sudo vim /etc/udev/rules.d/99-fixed-interbotix-udev.rules&lt;/code&gt; and add the following line:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;SUBSYSTEM==&#34;tty&#34;, ATTRS{serial}==&#34;&amp;lt;serial number here&amp;gt;&#34;, ENV{ID_MM_DEVICE_IGNORE}=&#34;1&#34;, ATTR{device/latency_timer}=&#34;1&#34;, SYMLINK+=&#34;ttyDXL_master_right&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;This will make sure the right master robot is &lt;em&gt;always&lt;/em&gt; binding to &lt;code&gt;ttyDXL_master_right&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Repeat with the rest of 3 arms.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To apply the changes, run &lt;code&gt;sudo udevadm control --reload &amp;amp;&amp;amp; sudo udevadm trigger&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If successful, you should be able to find &lt;code&gt;ttyDXL*&lt;/code&gt; in your &lt;code&gt;/dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Step 2: Set max current for gripper motors&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open Dynamixel Wizard, and select the wrist motor for puppet arms. The name of it should be &lt;code&gt;[ID:009] XM430-W350&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tip: the LED on the base of robot will flash when it is talking to Dynamixel Wizard. This will help determine which robot is selected.&lt;/li&gt; &#xA; &lt;li&gt;Find &lt;code&gt;38 Current Limit&lt;/code&gt;, enter &lt;code&gt;300&lt;/code&gt;, then hit &lt;code&gt;save&lt;/code&gt; at the bottom.&lt;/li&gt; &#xA; &lt;li&gt;Repeat this for both puppet robots.&lt;/li&gt; &#xA; &lt;li&gt;This limits the max current through gripper motors, to prevent overloading errors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Step 3: Setup 4 cameras&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;You may use usb hub here, but &lt;em&gt;maximum 2 cameras per hub for reasonable latency&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To make sure all 4 cameras are binding to a consistent port, similar steps are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Cameras are by default binding to &lt;code&gt;/dev/video{0, 1, 2...}&lt;/code&gt;, while we want to have symlinks &lt;code&gt;{CAM_RIGHT_WRIST, CAM_LEFT_WRIST, CAM_LOW, CAM_HIGH}&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Take &lt;code&gt;CAM_RIGHT_WRIST&lt;/code&gt; as an example, and let&#39;s say it is now binding to &lt;code&gt;/dev/video0&lt;/code&gt;. run &lt;code&gt;udevadm info --name=/dev/video0 --attribute-walk | grep serial&lt;/code&gt; to obtain it&#39;s serial. Use the first one that shows up, the format should look similar to &lt;code&gt;0E1A2B2F&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then &lt;code&gt;sudo vim /etc/udev/rules.d/99-fixed-interbotix-udev.rules&lt;/code&gt; and add the following line&lt;/p&gt; &lt;pre&gt;&lt;code&gt;SUBSYSTEM==&#34;video4linux&#34;, ATTRS{serial}==&#34;&amp;lt;serial number here&amp;gt;&#34;, ATTR{index}==&#34;0&#34;, ATTRS{idProduct}==&#34;085c&#34;, ATTR{device/latency_timer}=&#34;1&#34;, SYMLINK+=&#34;CAM_RIGHT_WRIST&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Repeat this for &lt;code&gt;{CAM_LEFT_WRIST, CAM_LOW, CAM_HIGH}&lt;/code&gt; in additional to &lt;code&gt;CAM_RIGHT_WRIST&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To apply the changes, run &lt;code&gt;sudo udevadm control --reload &amp;amp;&amp;amp; sudo udevadm trigger&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If successful, you should be able to find &lt;code&gt;{CAM_RIGHT_WRIST, CAM_LEFT_WRIST, CAM_LOW, CAM_HIGH}&lt;/code&gt; in your &lt;code&gt;/dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;At this point, have a new terminal&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda deactivate # if conda shows up by default&#xA;source /opt/ros/noetic/setup.sh &amp;amp;&amp;amp; source ~/interbotix_ws/devel/setup.sh&#xA;roslaunch aloha 4arms_teleop.launch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If no error message is showing up, the computer should be successfully connected to all 4 cameras and all 4 robots.&lt;/p&gt; &#xA;&lt;h4&gt;Trouble shooting&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure Dynamixel Wizard is disconnected, and no app is using webcam&#39;s stream. It will prevent ROS from connecting to these devices.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Software installation - Conda:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n aloha python=3.8.10&#xA;conda activate aloha&#xA;pip install torchvision&#xA;pip install torch&#xA;pip install pyquaternion&#xA;pip install pyyaml&#xA;pip install rospkg&#xA;pip install pexpect&#xA;pip install mujoco&#xA;pip install dm_control&#xA;pip install opencv-python&#xA;pip install matplotlib&#xA;pip install einops&#xA;pip install packaging&#xA;pip install h5py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing teleoperation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;: Before running the commands below, be sure to place all 4 robots in their sleep positions, and open master robot&#39;s gripper. All robots will rise to a height that is easy for teleoperation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# ROS terminal&#xA;conda deactivate&#xA;source /opt/ros/noetic/setup.sh &amp;amp;&amp;amp; source ~/interbotix_ws/devel/setup.sh&#xA;roslaunch aloha 4arms_teleop.launch&#xA;&#xA;# Right hand terminal&#xA;conda activate aloha&#xA;cd ~/interbotix_ws/src/aloha/aloha_scripts&#xA;python3 one_side_teleop.py right&#xA;&#xA;# Left hand terminal&#xA;conda activate aloha&#xA;cd ~/interbotix_ws/src/aloha/aloha_scripts&#xA;python3 one_side_teleop.py left&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The teleoperation will start when the master side gripper is closed.&lt;/p&gt; &#xA;&lt;h2&gt;Example Usages&lt;/h2&gt; &#xA;&lt;p&gt;To set up a new terminal, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate aloha&#xA;cd ~/interbotix_ws/src/aloha/aloha_scripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;one_side_teleop.py&lt;/code&gt; we ran is for testing teleoperation and has no data collection. To collect data for an episode, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 record_episodes.py --dataset_dir &amp;lt;data save dir&amp;gt; --episode_idx 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will store a hdf5 file at &lt;code&gt;&amp;lt;data save dir&amp;gt;&lt;/code&gt;. To change episode length and other params, edit &lt;code&gt;constants.py&lt;/code&gt; directly.&lt;/p&gt; &#xA;&lt;p&gt;To visualize the episode collected, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 visualize_episodes.py --dataset_dir &amp;lt;data save dir&amp;gt; --episode_idx 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To replay the episode collected with real robot, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 replay_episodes.py --dataset_dir &amp;lt;data save dir&amp;gt; --episode_idx 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To lower 4 robots before e.g. cutting off power, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 sleep.py&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tconbeer/harlequin</title>
    <updated>2024-01-08T01:40:59Z</updated>
    <id>tag:github.com,2024-01-08:/tconbeer/harlequin</id>
    <link href="https://github.com/tconbeer/harlequin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The SQL IDE for Your Terminal.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Harlequin&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/harlequin/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/harlequin&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/harlequin&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/runs%20on-Linux%20%7C%20MacOS%20%7C%20Windows-blue&#34; alt=&#34;Runs on Linux | MacOS | Windows&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The SQL IDE for Your Terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tconbeer/harlequin/main/harlequin.svg?sanitize=true&#34; alt=&#34;Harlequin&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installing Harlequin&lt;/h2&gt; &#xA;&lt;p&gt;After installing Python 3.8 or above, install Harlequin using &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;pipx&lt;/code&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx install harlequin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using Harlequin with DuckDB&lt;/h2&gt; &#xA;&lt;p&gt;From any shell, to open one or more DuckDB database files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;harlequin &#34;path/to/duck.db&#34; &#34;another_duck.db&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To open an in-memory DuckDB session, run Harlequin with no arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;harlequin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to control the version of DuckDB that Harlequin uses, see the &lt;a href=&#34;https://raw.githubusercontent.com/tconbeer/harlequin/main/troubleshooting/duckdb-version-mismatch&#34;&gt;Troubleshooting&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Using Harlequin with SQLite and Other Adapters&lt;/h2&gt; &#xA;&lt;p&gt;Harlequin also ships with a SQLite3 adapter. You can open one or more SQLite database files with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;harlequin -a sqlite &#34;path/to/sqlite.db&#34; &#34;another_sqlite.db&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Like DuckDB, you can also open an in-memory database by omitting the paths:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;harlequin -a sqlite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other adapters can be installed using &lt;code&gt;pip install &amp;lt;adapter package&amp;gt;&lt;/code&gt; or &lt;code&gt;pipx inject harlequin &amp;lt;adapter package&amp;gt;&lt;/code&gt;, depending on how you installed Harlequin. For a list of known adapters provided either by the Harlequin maintainers or the broader community, see the &lt;a href=&#34;https://harlequin.sh/docs/adapters&#34;&gt;adapters&lt;/a&gt; page in the docs.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;p&gt;To view all command-line options for Harlequin and all installed adapters, after installation, simply type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;harlequin --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To view a list of all key bindings (keyboard shortcuts) within the app, press &#xA; &lt;key&gt;&#xA;  F1&#xA; &lt;/key&gt;. You can also view this list outside the app &lt;a href=&#34;https://harlequin.sh/docs/bindings&#34;&gt;in the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;More info at &lt;a href=&#34;https://harlequin.sh&#34;&gt;harlequin.sh&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://harlequin.sh&#34;&gt;harlequin.sh&lt;/a&gt; for an overview of features and full documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for your interest in Harlequin! Harlequin is primarily maintained by &lt;a href=&#34;https://github.com/tconbeer&#34;&gt;Ted Conbeer&lt;/a&gt;, but he welcomes all contributions and is looking for additional maintainers!&lt;/p&gt; &#xA;&lt;h3&gt;Providing Feedback&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;d love to hear from you! &lt;a href=&#34;https://github.com/tconbeer/harlequin/issues/new&#34;&gt;Open an Issue&lt;/a&gt; to request new features, report bugs, or say hello.&lt;/p&gt; &#xA;&lt;h3&gt;Setting up Your Dev Environment and Running Tests&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Poetry v1.2 or higher if you don&#39;t have it already. You may also need or want pyenv, make, and gcc.&lt;/li&gt; &#xA; &lt;li&gt;Fork this repo, and then clone the fork into a directory (let&#39;s call it &lt;code&gt;harlequin&lt;/code&gt;), then &lt;code&gt;cd harlequin&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;poetry install --sync&lt;/code&gt; to install the project (editable) and its dependencies (including all test and dev dependencies) into a new virtual env.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;poetry shell&lt;/code&gt; to spawn a subshell.&lt;/li&gt; &#xA; &lt;li&gt;Type &lt;code&gt;make&lt;/code&gt; to run all tests and linters, or run &lt;code&gt;pytest&lt;/code&gt;, &lt;code&gt;black .&lt;/code&gt;, &lt;code&gt;ruff . --fix&lt;/code&gt;, and &lt;code&gt;mypy&lt;/code&gt; individually.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Opening PRs&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;PRs should be motivated by an open issue. If there isn&#39;t already an issue describing the feature or bug, &lt;a href=&#34;https://github.com/tconbeer/harlequin/issues/new&#34;&gt;open one&lt;/a&gt;. Do this before you write code, so you don&#39;t waste time on something that won&#39;t get merged.&lt;/li&gt; &#xA; &lt;li&gt;Ideally new features and bug fixes would be tested, to prevent future regressions. Textual provides a test harness that we use to test features of Harlequin. You can find some examples in the &lt;code&gt;tests&lt;/code&gt; directory of this project. Please include a test in your PR, but if you can&#39;t figure it out, open a PR to ask for help.&lt;/li&gt; &#xA; &lt;li&gt;Open a PR from your fork to the &lt;code&gt;main&lt;/code&gt; branch of &lt;code&gt;tconbeer/harlequin&lt;/code&gt;. In the PR description, link to the open issue, and then write a few sentences about &lt;strong&gt;why&lt;/strong&gt; you wrote the code you did: explain your design, etc.&lt;/li&gt; &#xA; &lt;li&gt;Ted may ask you to make changes, or he may make them for you. Don&#39;t take this the wrong way -- he values your contributions, but he knows this isn&#39;t your job, either, so if it&#39;s faster for him, he may push a commit to your branch or create a new branch from your commits.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>ali-vilab/i2vgen-xl</title>
    <updated>2024-01-08T01:40:59Z</updated>
    <id>tag:github.com,2024-01-08:/ali-vilab/i2vgen-xl</id>
    <link href="https://github.com/ali-vilab/i2vgen-xl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repo for VGen: a holistic video generation ecosystem for video generation building on diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VGen&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ali-vilab/i2vgen-xl/main/source/VGen.jpg&#34; alt=&#34;figure1&#34; title=&#34;figure1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;VGen is an open-source video synthesis codebase developed by the Tongyi Lab of Alibaba Group, featuring state-of-the-art video generative models. This repository includes implementations of the following methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://i2vgen-xl.github.io&#34;&gt;I2VGen-xl: High-quality image-to-video synthesis via cascaded diffusion models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://videocomposer.github.io&#34;&gt;VideoComposer: Compositional Video Synthesis with Motion Controllability&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://higen-t2v.github.io&#34;&gt;Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tf-t2v.github.io&#34;&gt;A Recipe for Scaling up Text-to-Video Generation with Text-free Videos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://instructvideo.github.io&#34;&gt;InstructVideo: Instructing Video Diffusion Models with Human Feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dreamvideo-t2v.github.io&#34;&gt;DreamVideo: Composing Your Dream Videos with Customized Subject and Motion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.09109&#34;&gt;VideoLCM: Video Latent Consistency Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.06571&#34;&gt;Modelscope text-to-video technical report&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;VGen can produce high-quality videos from the input text, images, desired motion, desired subjects, and even the feedback signals provided. It also offers a variety of commonly used video generation tools such as visualization, sampling, training, inference, join training using images and videos, acceleration, and more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://i2vgen-xl.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.04145&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/XUi0y7dxqEQ&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441039979087.mp4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ali-vilab/i2vgen-xl/main/source/logo.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/cjwbw/i2vgen-xl/&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/i2vgen-xl/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ðŸ”¥News!!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.01]&lt;/strong&gt; Thanks @&lt;a href=&#34;https://chenxwh.github.io&#34;&gt;Chenxi&lt;/a&gt; for supporting the running of i2vgen-xl on &lt;a href=&#34;https://replicate.com/cjwbw/i2vgen-xl/&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/i2vgen-xl/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;. Feel free to give it a try.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.01]&lt;/strong&gt; The gradio demo of I2VGen-XL has been completed in &lt;a href=&#34;https://modelscope.cn/studios/damo/I2VGen-XL/summary&#34;&gt;Modelscope&lt;/a&gt;, and welcome to try it out.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We have open-sourced the code and models for &lt;a href=&#34;https://github.com/ali-vilab/dreamtalk&#34;&gt;DreamTalk&lt;/a&gt;, which can produce high-quality talking head videos across diverse speaking styles using diffusion models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release &lt;a href=&#34;https://tf-t2v.github.io&#34;&gt;TF-T2V&lt;/a&gt; that can scale up existing video generation techniques using text-free videos, significantly enhancing the performance of both &lt;a href=&#34;https://arxiv.org/abs/2308.06571&#34;&gt;Modelscope-T2V&lt;/a&gt; and &lt;a href=&#34;https://videocomposer.github.io&#34;&gt;VideoComposer&lt;/a&gt; at the same time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We updated the codebase to support higher versions of xformer (0.0.22), torch2.0+, and removed the dependency on flash_attn.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release &lt;a href=&#34;https://instructvideo.github.io/&#34;&gt;InstructVideo&lt;/a&gt; that can accept human feedback signals to improve VLDM&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the diffusion based expressive talking head generation &lt;a href=&#34;https://dreamtalk-project.github.io&#34;&gt;DreamTalk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the high-efficiency video generation method &lt;a href=&#34;https://arxiv.org/abs/2312.09109&#34;&gt;VideoLCM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the code and model of &lt;a href=&#34;https://i2vgen-xl.github.io&#34;&gt;I2VGen-XL&lt;/a&gt; and the &lt;a href=&#34;https://arxiv.org/abs/2308.06571&#34;&gt;ModelScope T2V&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We release the T2V method &lt;a href=&#34;https://higen-t2v.github.io&#34;&gt;HiGen&lt;/a&gt; and customizing T2V method &lt;a href=&#34;https://dreamvideo-t2v.github.io&#34;&gt;DreamVideo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.12]&lt;/strong&gt; We write an &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/i2vgen-xl/main/doc/introduction.pdf&#34;&gt;introduction document&lt;/a&gt; for VGen and compare I2VGen-XL with SVD.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.11]&lt;/strong&gt; We release a high-quality I2VGen-XL model, please refer to the &lt;a href=&#34;https://i2vgen-xl.github.io&#34;&gt;Webpage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the technical papers and webpage of &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/i2vgen-xl/main/doc/i2vgen-xl.md&#34;&gt;I2VGen-XL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the code and pretrained models that can generate 1280x720 videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the code and models of &lt;a href=&#34;https://github.com/ali-vilab/dreamtalk&#34;&gt;DreamTalk&lt;/a&gt; that can generate expressive talking head&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release the code and pretrained models of &lt;a href=&#34;&#34;&gt;HumanDiff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release models optimized specifically for the human body and faces&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Updated version can fully maintain the ID and capture large and accurate motions simultaneously&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release other methods and the corresponding models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;p&gt;The main features of VGen are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Expandability, allowing for easy management of your own experiments.&lt;/li&gt; &#xA; &lt;li&gt;Completeness, encompassing all common components for video generation.&lt;/li&gt; &#xA; &lt;li&gt;Excellent performance, featuring powerful pre-trained models in multiple tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n vgen python=3.8&#xA;conda activate vgen&#xA;pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You also need to ensure that your system has installed the &lt;code&gt;ffmpeg&lt;/code&gt; command. If it is not installed, you can install it using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get update &amp;amp;&amp;amp; apt-get install ffmpeg libsm6 libxext6  -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We have provided a &lt;strong&gt;demo dataset&lt;/strong&gt; that includes images and videos, along with their lists in &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Please note that the demo images used here are for testing purposes and were not included in the training.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Clone the code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/damo-vilab/i2vgen-xl.git&#xA;cd i2vgen-xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started with VGen&lt;/h2&gt; &#xA;&lt;h3&gt;(1) Train your text-to-video model&lt;/h3&gt; &#xA;&lt;p&gt;Executing the following command to enable distributed training is as easy as that.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_net.py --cfg configs/t2v_train.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the &lt;code&gt;t2v_train.yaml&lt;/code&gt; configuration file, you can specify the data, adjust the video-to-image ratio using &lt;code&gt;frame_lens&lt;/code&gt;, and validate your ideas with different Diffusion settings, and so on.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before the training, you can download any of our open-source models for initialization. Our codebase supports custom initialization and &lt;code&gt;grad_scale&lt;/code&gt; settings, all of which are included in the &lt;code&gt;Pretrain&lt;/code&gt; item in yaml file.&lt;/li&gt; &#xA; &lt;li&gt;During the training, you can view the saved models and intermediate inference results in the &lt;code&gt;workspace/experiments/t2v_train&lt;/code&gt;directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After the training is completed, you can perform inference on the model using the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/t2v_infer.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can find the videos you generated in the &lt;code&gt;workspace/experiments/test_img_01&lt;/code&gt; directory. For specific configurations such as data, models, seed, etc., please refer to the &lt;code&gt;t2v_infer.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you want to directly load our previously open-sourced &lt;a href=&#34;https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/tree/main&#34;&gt;Modelscope T2V model&lt;/a&gt;, please refer to &lt;a href=&#34;https://github.com/damo-vilab/i2vgen-xl/issues/31&#34;&gt;this link&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;!-- &lt;table&gt;&#xA;&lt;center&gt;&#xA;  &lt;tr&gt;&#xA;    &lt;td &gt;&lt;center&gt;&#xA;      &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441754174077.mp4&#34;&gt;&lt;/video&gt;&#x9;&#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;    &lt;td &gt;&lt;center&gt;&#xA;      &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441138824052.mp4&#34;&gt;&lt;/video&gt;&#x9;&#xA;    &lt;/center&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;&lt;/center&gt;&#xA;&lt;/table&gt;&#xA;&lt;/center&gt; --&gt; &#xA;&lt;center&gt; &#xA;&lt;/center&gt;&#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01Ya2I5I25utrJwJ9Jf_!!6000000007587-2-tps-1280-720.png&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01CrmYaz1zXBetmg3dd_!!6000000006723-2-tps-1280-720.png&#34;&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441754174077.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;center&gt; &#xA;     &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441138824052.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;  &#xA;&lt;h3&gt;(2) Run the I2VGen-XL model&lt;/h3&gt; &#xA;&lt;p&gt;(i) Download model and test data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;!pip install modelscope&#xA;from modelscope.hub.snapshot_download import snapshot_download&#xA;model_dir = snapshot_download(&#39;damo/I2VGen-XL&#39;, cache_dir=&#39;models/&#39;, revision=&#39;v1.0.0&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can also download it through HuggingFace (&lt;a href=&#34;https://huggingface.co/damo-vilab/i2vgen-xl&#34;&gt;https://huggingface.co/damo-vilab/i2vgen-xl&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make sure you have git-lfs installed (https://git-lfs.com)&#xA;git lfs install&#xA;git clone https://huggingface.co/damo-vilab/i2vgen-xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(ii) Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/i2vgen_xl_infer.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python inference.py --cfg configs/i2vgen_xl_infer.yaml  test_list_path data/test_list_for_i2vgen.txt test_model models/i2vgen_xl_00854500.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;test_list_path&lt;/code&gt; represents the input image path and its corresponding caption. Please refer to the specific format and suggestions within demo file &lt;code&gt;data/test_list_for_i2vgen.txt&lt;/code&gt;. &lt;code&gt;test_model&lt;/code&gt; is the path for loading the model. In a few minutes, you can retrieve the high-definition video you wish to create from the &lt;code&gt;workspace/experiments/test_list_for_i2vgen&lt;/code&gt; directory. At present, we find that the current model performs inadequately on &lt;strong&gt;anime images&lt;/strong&gt; and &lt;strong&gt;images with a black background&lt;/strong&gt; due to the lack of relevant training data. We are consistently working to optimize it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;Due to the compression of our video quality in GIF format, please click &#39;HERE&#39; below to view the original video.&lt;/span&gt;&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;center&gt; &#xA; &lt;/center&gt;&#xA; &lt;table&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01CCEq7K1ZeLpNQqrWu_!!6000000003219-0-tps-1280-720.jpg&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442125067544.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01hIQcvG1spmQMLqBo0_!!6000000005816-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442125067544.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01ZXY7UN23K8q4oQ3uG_!!6000000007236-2-tps-1280-720.png&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441385957074.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01iaSiiv1aJZURUEY53_!!6000000003309-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/441385957074.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01NHpVGl1oat4H54Hjf_!!6000000005242-2-tps-1280-720.png&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442102706767.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;!-- &lt;image muted=&#34;true&#34; height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01DgLj1T240jfpzKoaQ_!!6000000007329-1-tps-1280-704.gif&#34;&gt;&lt;/image&gt;&#x9;&#xA;       --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i4/O1CN01DgLj1T240jfpzKoaQ_!!6000000007329-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442102706767.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01odS61s1WW9tXen21S_!!6000000002795-0-tps-1280-720.jpg&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;!-- &lt;video muted=&#34;true&#34; autoplay=&#34;true&#34; loop=&#34;true&#34; height=&#34;260&#34; src=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442163934688.mp4&#34;&gt;&lt;/video&gt;&#x9; --&gt; &#xA;      &lt;img height=&#34;260&#34; src=&#34;https://img.alicdn.com/imgextra/i3/O1CN01Jyk1HT28JkZtpAtY6_!!6000000007912-1-tps-1280-704.gif&#34;&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Input Image&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&#xA;     &lt;center&gt; &#xA;      &lt;p&gt;Click &lt;a href=&#34;https://cloud.video.taobao.com/play/u/null/p/1/e/6/t/1/442163934688.mp4&#34;&gt;HERE&lt;/a&gt; to view the generated video.&lt;/p&gt; &#xA;     &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt;  &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/center&gt; &#xA;&lt;h3&gt;(3) Other methods&lt;/h3&gt; &#xA;&lt;p&gt;In preparation.&lt;/p&gt; &#xA;&lt;h2&gt;Customize your own approach&lt;/h2&gt; &#xA;&lt;p&gt;Our codebase essentially supports all the commonly used components in video generation. You can manage your experiments flexibly by adding corresponding registration classes, including &lt;code&gt;ENGINE, MODEL, DATASETS, EMBEDDER, AUTO_ENCODER, VISUAL, DIFFUSION, PRETRAIN&lt;/code&gt;, and can be compatible with all our open-source algorithms according to your own needs. If you have any questions, feel free to give us your feedback at any time.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If this repo is useful to you, please cite our corresponding technical paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{2023videocomposer,&#xA;  title={VideoComposer: Compositional Video Synthesis with Motion Controllability},&#xA;  author={Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu, and Zhang, Yingya, and Shen, Yujun, and Zhao, Deli and Zhou, Jingren},&#xA;  booktitle={arXiv preprint arXiv:2306.02018},&#xA;  year={2023}&#xA;}&#xA;@article{2023i2vgenxl,&#xA;  title={I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models},&#xA;  author={Zhang, Shiwei and Wang, Jiayu and Zhang, Yingya and Zhao, Kang and Yuan, Hangjie and Qing, Zhiwu and Wang, Xiang  and Zhao, Deli and Zhou, Jingren},&#xA;  booktitle={arXiv preprint arXiv:2311.04145},&#xA;  year={2023}&#xA;}&#xA;@article{wang2023modelscope,&#xA;  title={Modelscope text-to-video technical report},&#xA;  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},&#xA;  journal={arXiv preprint arXiv:2308.06571},&#xA;  year={2023}&#xA;}&#xA;@article{dreamvideo,&#xA;  title={DreamVideo: Composing Your Dream Videos with Customized Subject and Motion},&#xA;  author={Wei, Yujie and Zhang, Shiwei and Qing, Zhiwu and Yuan, Hangjie and Liu, Zhiheng and Liu, Yu and Zhang, Yingya and Zhou, Jingren and Shan, Hongming},&#xA;  journal={arXiv preprint arXiv:2312.04433},&#xA;  year={2023}&#xA;}&#xA;@article{qing2023higen,&#xA;  title={Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation},&#xA;  author={Qing, Zhiwu and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Wei, Yujie and Zhang, Yingya and Gao, Changxin and Sang, Nong },&#xA;  journal={arXiv preprint arXiv:2312.04483},&#xA;  year={2023}&#xA;}&#xA;@article{wang2023videolcm,&#xA;  title={VideoLCM: Video Latent Consistency Model},&#xA;  author={Wang, Xiang and Zhang, Shiwei and Zhang, Han and Liu, Yu and Zhang, Yingya and Gao, Changxin and Sang, Nong },&#xA;  journal={arXiv preprint arXiv:2312.09109},&#xA;  year={2023}&#xA;}&#xA;@article{ma2023dreamtalk,&#xA;  title={DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models},&#xA;  author={Ma, Yifeng and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Zhang, Yingya and Deng Zhidong},&#xA;  journal={arXiv preprint arXiv:2312.09767},&#xA;  year={2023}&#xA;}&#xA;@article{2023InstructVideo,&#xA;  title={InstructVideo: Instructing Video Diffusion Models with Human Feedback},&#xA;  author={Yuan, Hangjie and Zhang, Shiwei and Wang, Xiang and Wei, Yujie and Feng, Tao and Pan, Yining and Zhang, Yingya and Liu, Ziwei and Albanie, Samuel and Ni, Dong},&#xA;  booktitle={arXiv preprint arXiv:2312.12490},&#xA;  year={2023}&#xA;}&#xA;@article{TFT2V,&#xA; title={A Recipe for Scaling up Text-to-Video Generation with Text-free Videos},&#xA; author={Wang, Xiang and Zhang, Shiwei and Yuan, Hangjie and Qing, Zhiwu and Gong, Biao and Zhang, Yingya and Shen, Yujun and Gao, Changxin and Sang, Nong},&#xA; journal={arXiv preprint arXiv:2312.15770},&#xA; year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to express our gratitude for the contributions of several previous works to the development of VGen. This includes, but is not limited to &lt;a href=&#34;https://arxiv.org/abs/2302.09778&#34;&gt;Composer&lt;/a&gt;, &lt;a href=&#34;https://modelscope.cn/models/damo/text-to-video-synthesis/summary&#34;&gt;ModelScopeT2V&lt;/a&gt;, &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP&lt;/a&gt;, &lt;a href=&#34;https://m-bain.github.io/webvid-dataset/&#34;&gt;WebVid-10M&lt;/a&gt;, &lt;a href=&#34;https://laion.ai/blog/laion-400-open-dataset/&#34;&gt;LAION-400M&lt;/a&gt;, &lt;a href=&#34;https://github.com/zhuoinoulu/pidinet&#34;&gt;Pidinet&lt;/a&gt; and &lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS&lt;/a&gt;. We are committed to building upon these foundations in a way that respects their original contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This open-source model is trained with using &lt;a href=&#34;https://m-bain.github.io/webvid-dataset/&#34;&gt;WebVid-10M&lt;/a&gt; and &lt;a href=&#34;https://laion.ai/blog/laion-400-open-dataset/&#34;&gt;LAION-400M&lt;/a&gt; datasets and is intended for &lt;strong&gt;RESEARCH/NON-COMMERCIAL USE ONLY&lt;/strong&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>