<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-26T01:36:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Hillobar/Rope</title>
    <updated>2023-12-26T01:36:11Z</updated>
    <id>tag:github.com,2023-12-26:/Hillobar/Rope</id>
    <link href="https://github.com/Hillobar/Rope" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GUI-focused roop&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/Hillobar/Rope/assets/63615199/97a57957-fb30-4329-b8f6-adbfa96203ab&#34; alt=&#34;Screenshot 2023-12-22 211725&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rope implements the insightface inswapper_128 model with a helpful GUI.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://discord.gg/EcdVAFJzqp&#34;&gt;Discord&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.paypal.com/donate/?hosted_button_id=Y5SB9LSXFGRF2&#34;&gt;Donate&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/Hillobar/Rope/wiki&#34;&gt;Wiki&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4Y4U0TZ8cWY&#34;&gt;Demo Video&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;${{\color{Goldenrod}{\textsf{Last Updated 2023-12-25 11:02 PST}}}}$&lt;/h3&gt; &#xA;&lt;p&gt;note: you need to download the latest GFPGAN model for Ruby. Link is in the wiki&lt;/p&gt; &#xA;&lt;h3&gt;Features:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightning speed face swapping with all the features&lt;/li&gt; &#xA; &lt;li&gt;Upscalers&lt;/li&gt; &#xA; &lt;li&gt;Likeness modifiers&lt;/li&gt; &#xA; &lt;li&gt;Orientation management&lt;/li&gt; &#xA; &lt;li&gt;Masks: borders, differentials, auto occlusion, face parsers, text-based masking - all with strength adjustments and blending settings&lt;/li&gt; &#xA; &lt;li&gt;Mask view to evaluate masks directly&lt;/li&gt; &#xA; &lt;li&gt;Source face merging and saving&lt;/li&gt; &#xA; &lt;li&gt;Swap images or videos&lt;/li&gt; &#xA; &lt;li&gt;Auto save filename generation&lt;/li&gt; &#xA; &lt;li&gt;Dock/Undock the video player&lt;/li&gt; &#xA; &lt;li&gt;Real-time player&lt;/li&gt; &#xA; &lt;li&gt;Segment recording&lt;/li&gt; &#xA; &lt;li&gt;Fine tune your video ahead of time by creating image setting markers at specific frames.&lt;/li&gt; &#xA; &lt;li&gt;Lightening fast!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updates for Rope-Ruby:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Almost double the performance of previous Rope! Twice as fast! Half the time! Most of the effort for Ruby focuses on huge performance gains. Enjoy the speed!&lt;/li&gt; &#xA; &lt;li&gt;Much faster GFPGAN&lt;/li&gt; &#xA; &lt;li&gt;Occluder mask size can now be adjusted&lt;/li&gt; &#xA; &lt;li&gt;Experimental features added to make adjustments to face swap region placement and face scale.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Changelog for 2023-12-25 08:56 PST:&lt;/h3&gt; &#xA;&lt;p&gt;Files changed: Coordinator.py, Dicts.py, GUI.py, VideoManager.py. DL new GPEN models&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(fixed) Adjusted the Brdr default settings to fix some blending lines&lt;/li&gt; &#xA; &lt;li&gt;(fixed) Video loading errors addressed&lt;/li&gt; &#xA; &lt;li&gt;(feature) Read target videos and images, and source faces from subfolders&lt;/li&gt; &#xA; &lt;li&gt;(fixed) Low resolution videos no longer results in partial face swaps&lt;/li&gt; &#xA; &lt;li&gt;(feature) Added GPEN 256 and GPEN 512&lt;/li&gt; &#xA; &lt;li&gt;(feature) Added manual color correction&lt;/li&gt; &#xA; &lt;li&gt;(fixed) Using &#39;wasd&#39; can no longer go out of bounds&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Changelog for 12/25 11:02 PST:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(fixed) Couple of bugs related to GPEN. Redownload Coordinator and VideoManger&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known Bugs:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When using Markers, the frames before the first marker will use parameters from the the last settings in your options. Not sure if it is a true bug, but best way to deal with this is to create a marker at the first frame.&lt;/li&gt; &#xA; &lt;li&gt;Starting a mode for the first time while playing will crash Rope. Due to the new performance architecture, this will not be possible. The first time you turn on a model, the video should not be playing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Performance:&lt;/h3&gt; &#xA;&lt;p&gt;Machine: 3090Ti (24GB), i5-13600K&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/Hillobar/Rope/assets/63615199/3e3505db-bc76-48df-b8ac-1e7e86c8d751&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;File: benchmark/target-1080p.mp4, 2048x1080, 269 frames, 25 fps, 10s Rendering time in seconds:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Option&lt;/th&gt; &#xA;   &lt;th&gt;Crystal&lt;/th&gt; &#xA;   &lt;th&gt;Sapphire&lt;/th&gt; &#xA;   &lt;th&gt;Ruby&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Only Swap&lt;/td&gt; &#xA;   &lt;td&gt;7.3&lt;/td&gt; &#xA;   &lt;td&gt;7.5&lt;/td&gt; &#xA;   &lt;td&gt;4.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+GFPGAN&lt;/td&gt; &#xA;   &lt;td&gt;10.7&lt;/td&gt; &#xA;   &lt;td&gt;11.0&lt;/td&gt; &#xA;   &lt;td&gt;9.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+Codeformer&lt;/td&gt; &#xA;   &lt;td&gt;12.4&lt;/td&gt; &#xA;   &lt;td&gt;13.5&lt;/td&gt; &#xA;   &lt;td&gt;11.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+one word CLIP&lt;/td&gt; &#xA;   &lt;td&gt;10.4&lt;/td&gt; &#xA;   &lt;td&gt;11.2&lt;/td&gt; &#xA;   &lt;td&gt;9.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+Occluder&lt;/td&gt; &#xA;   &lt;td&gt;7.8&lt;/td&gt; &#xA;   &lt;td&gt;7.8&lt;/td&gt; &#xA;   &lt;td&gt;4.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swap+MouthParser&lt;/td&gt; &#xA;   &lt;td&gt;13.9&lt;/td&gt; &#xA;   &lt;td&gt;12.1&lt;/td&gt; &#xA;   &lt;td&gt;5.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Preview:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Hillobar/Rope/assets/63615199/384fd63a-b870-4714-a137-d27e31560433&#34; alt=&#34;Screenshot 2023-12-22 212639&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Disclaimer:&lt;/h3&gt; &#xA;&lt;p&gt;Rope is a personal project that I&#39;m making available to the community as a thank you for all of the contributors ahead of me. I&#39;ve copied the disclaimer from &lt;a href=&#34;https://github.com/harisreedhar/Swap-Mukham&#34;&gt;Swap-Mukham&lt;/a&gt; here since it is well-written and applies 100% to this repo.&lt;/p&gt; &#xA;&lt;p&gt;I would like to emphasize that our swapping software is intended for responsible and ethical use only. I must stress that users are solely responsible for their actions when using our software.&lt;/p&gt; &#xA;&lt;p&gt;Intended Usage: This software is designed to assist users in creating realistic and entertaining content, such as movies, visual effects, virtual reality experiences, and other creative applications. I encourage users to explore these possibilities within the boundaries of legality, ethical considerations, and respect for others&#39; privacy.&lt;/p&gt; &#xA;&lt;p&gt;Ethical Guidelines: Users are expected to adhere to a set of ethical guidelines when using our software. These guidelines include, but are not limited to:&lt;/p&gt; &#xA;&lt;p&gt;Not creating or sharing content that could harm, defame, or harass individuals. Obtaining proper consent and permissions from individuals featured in the content before using their likeness. Avoiding the use of this technology for deceptive purposes, including misinformation or malicious intent. Respecting and abiding by applicable laws, regulations, and copyright restrictions.&lt;/p&gt; &#xA;&lt;p&gt;Privacy and Consent: Users are responsible for ensuring that they have the necessary permissions and consents from individuals whose likeness they intend to use in their creations. We strongly discourage the creation of content without explicit consent, particularly if it involves non-consensual or private content. It is essential to respect the privacy and dignity of all individuals involved.&lt;/p&gt; &#xA;&lt;p&gt;Legal Considerations: Users must understand and comply with all relevant local, regional, and international laws pertaining to this technology. This includes laws related to privacy, defamation, intellectual property rights, and other relevant legislation. Users should consult legal professionals if they have any doubts regarding the legal implications of their creations.&lt;/p&gt; &#xA;&lt;p&gt;Liability and Responsibility: We, as the creators and providers of the deep fake software, cannot be held responsible for the actions or consequences resulting from the usage of our software. Users assume full liability and responsibility for any misuse, unintended effects, or abusive behavior associated with the content they create.&lt;/p&gt; &#xA;&lt;p&gt;By using this software, users acknowledge that they have read, understood, and agreed to abide by the above guidelines and disclaimers. We strongly encourage users to approach this technology with caution, integrity, and respect for the well-being and rights of others.&lt;/p&gt; &#xA;&lt;p&gt;Remember, technology should be used to empower and inspire, not to harm or deceive. Let&#39;s strive for ethical and responsible use of deep fake technology for the betterment of society.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>atomicals/atomicals-electrumx</title>
    <updated>2023-12-26T01:36:11Z</updated>
    <id>tag:github.com,2023-12-26:/atomicals/atomicals-electrumx</id>
    <link href="https://github.com/atomicals/atomicals-electrumx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Electrumx Atomicals Indexer Server&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: &lt;a href=&#34;https://api.cirrus-ci.com/github/spesmilo/electrumx.svg?branch=master&#34;&gt;https://api.cirrus-ci.com/github/spesmilo/electrumx.svg?branch=master&lt;/a&gt; :target: &lt;a href=&#34;https://cirrus-ci.com/github/spesmilo/electrumx&#34;&gt;https://cirrus-ci.com/github/spesmilo/electrumx&lt;/a&gt; .. image:: &lt;a href=&#34;https://coveralls.io/repos/github/spesmilo/electrumx/badge.svg&#34;&gt;https://coveralls.io/repos/github/spesmilo/electrumx/badge.svg&lt;/a&gt; :target: &lt;a href=&#34;https://coveralls.io/github/spesmilo/electrumx&#34;&gt;https://coveralls.io/github/spesmilo/electrumx&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;=============================================== ElectrumX - Reimplementation of electrum-server&lt;/h1&gt; &#xA;&lt;p&gt;:Licence: MIT :Language: Python (&amp;gt;= 3.8) :Original Author: Neil Booth&lt;/p&gt; &#xA;&lt;p&gt;This project is a fork of &lt;code&gt;kyuupichan/electrumx &amp;lt;https://github.com/kyuupichan/electrumx&amp;gt;&lt;/code&gt;_. The original author dropped support for Bitcoin, which we intend to keep.&lt;/p&gt; &#xA;&lt;p&gt;ElectrumX allows users to run their own Electrum server. It connects to your full node and indexes the blockchain, allowing efficient querying of the history of arbitrary addresses. The server can be exposed publicly, and joined to the public network of servers via peer discovery. As of May 2020, a significant chunk of the public Electrum server network runs ElectrumX.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;code&gt;readthedocs &amp;lt;https://electrumx-spesmilo.readthedocs.io/&amp;gt;&lt;/code&gt;_.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/promptbench</title>
    <updated>2023-12-26T01:36:11Z</updated>
    <id>tag:github.com,2023-12-26:/microsoft/promptbench</id>
    <link href="https://github.com/microsoft/promptbench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A unified evaluation framework for large language models&lt;/p&gt;&lt;hr&gt;&lt;div id=&#34;top&#34;&gt;&lt;/div&gt; &#xA;&lt;!--&#xA;*** Thanks for checking out the Best-README-Template. If you have a suggestion&#xA;*** that would make this better, please fork the repo and create a pull request&#xA;*** or simply open an issue with the tag &#34;enhancement&#34;.&#xA;*** Don&#39;t forget to give the project a star!&#xA;*** Thanks again! Now go create something AMAZING! :D&#xA;--&gt; &#xA;&lt;!-- PROJECT SHIELDS --&gt; &#xA;&lt;!--&#xA;*** I&#39;m using markdown &#34;reference style&#34; links for readability.&#xA;*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).&#xA;*** See the bottom of this document for the declaration of the reference variables&#xA;*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.&#xA;*** https://www.markdownguide.org/basic-syntax/#reference-style-links&#xA;--&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/promptbench/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/promptbench.svg?style=for-the-badge&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/promptbench/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/promptbench.svg?style=for-the-badge&#34; alt=&#34;Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/promptbench/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/promptbench.svg?style=for-the-badge&#34; alt=&#34;Stargazers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/promptbench/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/promptbench.svg?style=for-the-badge&#34; alt=&#34;Issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &#xA;***[![MIT License][license-shield]][license-url]&#xA;--&gt; &#xA;&lt;!-- PROJECT LOGO --&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/microsoft/promptbench&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/imgs/promptbench_logo.png&#34; alt=&#34;Logo&#34; width=&#34;300&#34;&gt; &lt;/a&gt; &#xA; &lt;!-- &lt;h3 align=&#34;center&#34;&gt;USB&lt;/h3&gt; --&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;strong&gt;PromptBench&lt;/strong&gt;: A Unified Library for Evaluating and Understanding Large Language Models. &#xA;  &lt;!-- &lt;br /&gt;&#xA;    &lt;a href=&#34;https://github.com/microsoft/promptbench&#34;&gt;&lt;strong&gt;Explore the docs »&lt;/strong&gt;&lt;/a&gt;&#xA;    &lt;br /&gt; --&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.07910&#34;&gt;Paper&lt;/a&gt; · &lt;a href=&#34;https://promptbench.readthedocs.io/en/latest/&#34;&gt;Documentation&lt;/a&gt; · &lt;a href=&#34;https://llm-eval.github.io/pages/leaderboard.html&#34;&gt;Leaderboard&lt;/a&gt; · &lt;a href=&#34;https://llm-eval.github.io/pages/papers.html&#34;&gt;More papers&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- TABLE OF CONTENTS --&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#news-and-updates&#34;&gt;News and Updates&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#supported-datasets-and-models&#34;&gt;Datasets and Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#benchmark-results&#34;&gt;Benchmark Results&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- News and Updates --&gt; &#xA;&lt;h2&gt;News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[16/12/2023] Add support for Gemini, Mistral, Mixtral, Baichuan, Yi models.&lt;/li&gt; &#xA; &lt;li&gt;[15/12/2023] Add detailed instructions for users to add new modules (models, datasets, etc.) &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/examples/add_new_modules.md&#34;&gt;examples/add_new_modules.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[05/12/2023] Published promptbench 0.0.1.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Introduction --&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;PromptBench&lt;/strong&gt; is a Pytorch-based Python package for Evaluation of Large Language Models (LLMs). It provides user-friendly APIs for researchers to conduct evaluation on LLMs. Check the technical report: &lt;a href=&#34;https://arxiv.org/abs/2312.07910&#34;&gt;https://arxiv.org/abs/2312.07910&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/imgs/promptbench.png&#34; alt=&#34;Code Structure&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What does promptbench currently provide?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quick model performance assessment:&lt;/strong&gt; We offer a user-friendly interface that allows for quick model building, dataset loading, and evaluation of model performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Engineering:&lt;/strong&gt; We implemented several prompt engineering methods. For example: &lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;Few-shot Chain-of-Thought&lt;/a&gt; [1], &lt;a href=&#34;https://arxiv.org/abs/2307.11760&#34;&gt;Emotion Prompt&lt;/a&gt; [2], &lt;a href=&#34;https://arxiv.org/abs/2305.14688&#34;&gt;Expert Prompting&lt;/a&gt; [3] and so on.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluating adversarial prompts:&lt;/strong&gt; promptbench integrated &lt;a href=&#34;https://arxiv.org/abs/2306.04528&#34;&gt;prompt attacks&lt;/a&gt; [4], enabling researchers to simulate black-box adversarial prompt attacks on models and evaluate their robustness (see details &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/promptbench/prompt_attack/README.md&#34;&gt;here&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic evaluation to mitigate potential test data contamination:&lt;/strong&gt; we integrated the dynamic evaluation framework &lt;a href=&#34;https://arxiv.org/pdf/2309.17167&#34;&gt;DyVal&lt;/a&gt; [5], which generates evaluation samples on-the-fly with controlled complexity.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- GETTING STARTED --&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install via &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We provide a Python package &lt;em&gt;promptbench&lt;/em&gt; for users who want to start evaluation quickly. Simply run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install promptbench&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the pip installation could be behind the recent updates. So, if you want to use the latest features or develop based on our code, you should intall via Github.&lt;/p&gt; &#xA;&lt;h3&gt;Install via GitHub&lt;/h3&gt; &#xA;&lt;p&gt;First, clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone git@github.com:microsoft/promptbench.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd promptbench&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install the required packages, you can create a conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create --name promptbench python=3.9&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then use pip to install required packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this only installed basic python packages. For Prompt Attacks, it requires to install textattacks.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;promptbench is easy to use and extend. Going through the bellowing examples will help you familiar with promptbench for quick use, evaluate an existing datasets and LLMs, or creating your own datasets and models.&lt;/p&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/#installation&#34;&gt;Installation&lt;/a&gt; to install promptbench first.&lt;/p&gt; &#xA;&lt;p&gt;If promptbench is installed via &lt;code&gt;pip&lt;/code&gt;, you can simply do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import promptbench as pb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you installed promptbench from &lt;code&gt;git&lt;/code&gt; and want to use it in other projects:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sys&#xA;&#xA;# Add the directory of promptbench to the Python path&#xA;sys.path.append(&#39;/home/xxx/promptbench&#39;)&#xA;&#xA;# Now you can import promptbench by name&#xA;import promptbench as pb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide tutorials for:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;evaluate models on existing benchmarks:&lt;/strong&gt; please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/examples/basic.ipynb&#34;&gt;examples/basic.ipynb&lt;/a&gt; for constructing your evaluation pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;test the effects of different prompting techniques:&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;examine the robustness for prompt attacks&lt;/strong&gt;, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/examples/prompt_attack.ipynb&#34;&gt;examples/prompt_attack.ipynb&lt;/a&gt; to construct the attacks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;use DyVal for evaluation:&lt;/strong&gt; please refer to &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/promptbench/main/examples/dyval.ipynb&#34;&gt;examples/dyval.ipynb&lt;/a&gt; to construct DyVal datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Supported Datasets and Models&lt;/h2&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We support a range of datasets to facilitate comprehensive analysis, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GLUE: SST-2, CoLA, QQP, MRPC, MNLI, QNLI, RTE, WNLI&lt;/li&gt; &#xA; &lt;li&gt;MMLU&lt;/li&gt; &#xA; &lt;li&gt;SQuAD V2&lt;/li&gt; &#xA; &lt;li&gt;IWSLT 2017&lt;/li&gt; &#xA; &lt;li&gt;UN Multi&lt;/li&gt; &#xA; &lt;li&gt;Math&lt;/li&gt; &#xA; &lt;li&gt;Bool Logic (BigBench)&lt;/li&gt; &#xA; &lt;li&gt;Valid Parentheses (BigBench)&lt;/li&gt; &#xA; &lt;li&gt;Object Tracking (BigBench)&lt;/li&gt; &#xA; &lt;li&gt;Date (BigBench)&lt;/li&gt; &#xA; &lt;li&gt;GSM8K&lt;/li&gt; &#xA; &lt;li&gt;CSQA (CommonSense QA)&lt;/li&gt; &#xA; &lt;li&gt;Numersense&lt;/li&gt; &#xA; &lt;li&gt;QASC&lt;/li&gt; &#xA; &lt;li&gt;Last Letter Concatenate&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;google/flan-t5-large&lt;/li&gt; &#xA; &lt;li&gt;databricks/dolly-v1-6b&lt;/li&gt; &#xA; &lt;li&gt;Llama2 series&lt;/li&gt; &#xA; &lt;li&gt;vicuna-13b, vicuna-13b-v1.3&lt;/li&gt; &#xA; &lt;li&gt;Cerebras/Cerebras-GPT-13B&lt;/li&gt; &#xA; &lt;li&gt;EleutherAI/gpt-neox-20b&lt;/li&gt; &#xA; &lt;li&gt;Google/flan-ul2&lt;/li&gt; &#xA; &lt;li&gt;PaLM 2&lt;/li&gt; &#xA; &lt;li&gt;ChatGPT&lt;/li&gt; &#xA; &lt;li&gt;GPT-4&lt;/li&gt; &#xA; &lt;li&gt;phi-1.5, phi-2&lt;/li&gt; &#xA; &lt;li&gt;Gemini Pro&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark Results&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://llm-eval.github.io/&#34;&gt;benchmark website&lt;/a&gt; for benchmark results on Prompt Attacks, Prompt Engineering and Dynamic Evaluation DyVal.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for multi-modal models such as LlaVa and BLIP2.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/textattacks&#34;&gt;textattacks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/othneildrew/Best-README-Template&#34;&gt;README Template&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;We thank the volunteers: Hanyuan Zhang, Lingrui Li, Yating Zhou for conducting the semantic preserving experiment in Prompt Attack benchmark.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;[1] Jason Wei, et al. &#34;Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.&#34; arXiv preprint arXiv:2201.11903 (2022).&lt;/p&gt; &#xA;&lt;p&gt;[2] Cheng Li, et al. &#34;Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus.&#34; arXiv preprint arXiv:2307.11760 (2023).&lt;/p&gt; &#xA;&lt;p&gt;[3] BenFeng Xu, et al. &#34;ExpertPrompting: Instructing Large Language Models to be Distinguished Experts&#34; arXiv preprint arXiv:2305.14688 (2023).&lt;/p&gt; &#xA;&lt;p&gt;[4] Zhu, Kaijie, et al. &#34;PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts.&#34; arXiv preprint arXiv:2306.04528 (2023).&lt;/p&gt; &#xA;&lt;p&gt;[5] Zhu, Kaijie, et al. &#34;DyVal: Graph-informed Dynamic Evaluation of Large Language Models.&#34; arXiv preprint arXiv:2309.17167 (2023).&lt;/p&gt; &#xA;&lt;!-- CITE --&gt; &#xA;&lt;h2&gt;Citing promptbench and other research papers&lt;/h2&gt; &#xA;&lt;p&gt;Please cite us if you fine this project helpful for your project/paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhu2023promptbench2,&#xA;  title={PromptBench: A Unified Library for Evaluation of Large Language Models},&#xA;  author={Zhu, Kaijie and Zhao, Qinlin and Chen, Hao and Wang, Jindong and Xie, Xing},&#xA;  journal={arXiv preprint arXiv:2312.07910},&#xA;  year={2023}&#xA;}&#xA;&#xA;@article{zhu2023promptbench,&#xA;  title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},&#xA;  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and others},&#xA;  journal={arXiv preprint arXiv:2306.04528},&#xA;  year={2023}&#xA;}&#xA;&#xA;@article{zhu2023dyval,&#xA;  title={DyVal: Graph-informed Dynamic Evaluation of Large Language Models},&#xA;  author={Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing},&#xA;  journal={arXiv preprint arXiv:2309.17167},&#xA;  year={2023}&#xA;}&#xA;&#xA;@article{chang2023survey,&#xA;  title={A survey on evaluation of large language models},&#xA;  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},&#xA;  journal={arXiv preprint arXiv:2307.03109},&#xA;  year={2023}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- CONTRIBUTING --&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;p&gt;If you have a suggestion that would make promptbench better, please fork the repo and create a pull request. You can also simply open an issue with the tag &#34;enhancement&#34;. Don&#39;t forget to give the project a star! Thanks again!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the project&lt;/li&gt; &#xA; &lt;li&gt;Create your branch (&lt;code&gt;git checkout -b your_name/your_branch&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Commit your changes (&lt;code&gt;git commit -m &#39;Add some features&#39;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch (&lt;code&gt;git push origin your_name/your_branch&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Open a Pull Request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- TRADEMARKS --&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt; &#xA;&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; &#xA;&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</summary>
  </entry>
</feed>