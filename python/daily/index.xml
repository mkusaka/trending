<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-25T01:32:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xmu-xiaoma666/External-Attention-pytorch</title>
    <updated>2022-06-25T01:32:05Z</updated>
    <id>tag:github.com,2022-06-25:/xmu-xiaoma666/External-Attention-pytorch</id>
    <link href="https://github.com/xmu-xiaoma666/External-Attention-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ€ Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.â­â­â­&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/FightingCVimg/LOGO.gif&#34; height=&#34;200&#34; width=&#34;400&#34;&gt; &#xA;&lt;h1&gt;FightingCV Codebase For &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#attention-series&#34;&gt;&lt;em&gt;&lt;strong&gt;Attention&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;,&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#backbone-series&#34;&gt;&lt;em&gt;&lt;strong&gt;Backbone&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#mlp-series&#34;&gt;&lt;em&gt;&lt;strong&gt;MLP&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#re-parameter-series&#34;&gt;&lt;em&gt;&lt;strong&gt;Re-parameter&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#convolution-series&#34;&gt;&lt;strong&gt;Convolution&lt;/strong&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-%3E=v3.0-blue&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/pytorch-%3E=v1.4-red&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;If this project is helpful to you, welcome to give a &lt;em&gt;&lt;strong&gt;star&lt;/strong&gt;&lt;/em&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Don&#39;t forget to &lt;em&gt;&lt;strong&gt;follow&lt;/strong&gt;&lt;/em&gt; me to learn about project updates.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- - **ğŸ˜„ ğŸ˜„ ğŸ˜„ I am looking for jobs for 2023, my interest is multi-modal pretraining, video-text retrieval, computer vision or other feilds about multi modality!!! Welcome to chat with me by wechat(id:mayiwei1998)**&#xA;- **ğŸ˜„ ğŸ˜„ ğŸ˜„ æˆ‘æ­£åœ¨å¯»æ‰¾2023å¹´çš„å·¥ä½œï¼Œæˆ‘çš„å…´è¶£æ˜¯å¤šæ¨¡æ€é¢„è®­ç»ƒã€è§†é¢‘æ–‡æœ¬æ£€ç´¢ã€è®¡ç®—æœºè§†è§‰æˆ–å…¶ä»–å…³äºå¤šæ¨¡æ€çš„é¢†åŸŸï¼ï¼ï¼æ¬¢è¿å¤§å®¶é€šè¿‡å¾®ä¿¡ä¸æˆ‘èŠå¤©ï¼ˆid:mayiwei1998ï¼‰** --&gt; &#xA;&lt;p&gt;Helloï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯å°é©¬ğŸš€ğŸš€ğŸš€&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For å°ç™½ï¼ˆLike Meï¼‰ï¼š&lt;/strong&gt;&lt;/em&gt; æœ€è¿‘åœ¨è¯»è®ºæ–‡çš„æ—¶å€™ä¼šå‘ç°ä¸€ä¸ªé—®é¢˜ï¼Œæœ‰æ—¶å€™è®ºæ–‡æ ¸å¿ƒæ€æƒ³éå¸¸ç®€å•ï¼Œæ ¸å¿ƒä»£ç å¯èƒ½ä¹Ÿå°±åå‡ è¡Œã€‚ä½†æ˜¯æ‰“å¼€ä½œè€…releaseçš„æºç æ—¶ï¼Œå´å‘ç°æå‡ºçš„æ¨¡å—åµŒå…¥åˆ°åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰ä»»åŠ¡æ¡†æ¶ä¸­ï¼Œå¯¼è‡´ä»£ç æ¯”è¾ƒå†—ä½™ï¼Œå¯¹äºç‰¹å®šä»»åŠ¡æ¡†æ¶ä¸ç†Ÿæ‚‰çš„æˆ‘ï¼Œ&lt;strong&gt;å¾ˆéš¾æ‰¾åˆ°æ ¸å¿ƒä»£ç &lt;/strong&gt;ï¼Œå¯¼è‡´åœ¨è®ºæ–‡å’Œç½‘ç»œæ€æƒ³çš„ç†è§£ä¸Šä¼šæœ‰ä¸€å®šå›°éš¾ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For è¿›é˜¶è€…ï¼ˆLike Youï¼‰ï¼š&lt;/strong&gt;&lt;/em&gt; å¦‚æœæŠŠConvã€FCã€RNNè¿™äº›åŸºæœ¬å•å…ƒçœ‹åšå°çš„Legoç§¯æœ¨ï¼ŒæŠŠTransformerã€ResNetè¿™äº›ç»“æ„çœ‹æˆå·²ç»æ­å¥½çš„LegoåŸå ¡ã€‚é‚£ä¹ˆæœ¬é¡¹ç›®æä¾›çš„æ¨¡å—å°±æ˜¯ä¸€ä¸ªä¸ªå…·æœ‰å®Œæ•´è¯­ä¹‰ä¿¡æ¯çš„Legoç»„ä»¶ã€‚&lt;strong&gt;è®©ç§‘ç ”å·¥ä½œè€…ä»¬é¿å…åå¤é€ è½®å­&lt;/strong&gt;ï¼Œåªéœ€æ€è€ƒå¦‚ä½•åˆ©ç”¨è¿™äº›â€œLegoç»„ä»¶â€ï¼Œæ­å»ºå‡ºæ›´å¤šç»šçƒ‚å¤šå½©çš„ä½œå“ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For å¤§ç¥ï¼ˆMay Be Like Youï¼‰ï¼š&lt;/strong&gt;&lt;/em&gt; èƒ½åŠ›æœ‰é™ï¼Œ&lt;strong&gt;ä¸å–œè½»å–·&lt;/strong&gt;ï¼ï¼ï¼&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;For Allï¼š&lt;/strong&gt;&lt;/em&gt; æœ¬é¡¹ç›®å°±æ˜¯è¦å®ç°ä¸€ä¸ªæ—¢èƒ½&lt;strong&gt;è®©æ·±åº¦å­¦ä¹ å°ç™½ä¹Ÿèƒ½ææ‡‚&lt;/strong&gt;ï¼Œåˆèƒ½&lt;strong&gt;æœåŠ¡ç§‘ç ”å’Œå·¥ä¸šç¤¾åŒº&lt;/strong&gt;çš„ä»£ç åº“ã€‚ä½œä¸º&lt;a href=&#34;https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading&#34;&gt;ã€è®ºæ–‡è§£æé¡¹ç›®ã€‘&lt;/a&gt;çš„è¡¥å……ï¼Œæœ¬é¡¹ç›®çš„å®—æ—¨æ˜¯ä»ä»£ç è§’åº¦ï¼Œå®ç°ğŸš€&lt;strong&gt;è®©ä¸–ç•Œä¸Šæ²¡æœ‰éš¾è¯»çš„è®ºæ–‡&lt;/strong&gt;ğŸš€ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ï¼ˆåŒæ—¶ä¹Ÿéå¸¸æ¬¢è¿å„ä½ç§‘ç ”å·¥ä½œè€…å°†è‡ªå·±çš„å·¥ä½œçš„æ ¸å¿ƒä»£ç æ•´ç†åˆ°æœ¬é¡¹ç›®ä¸­ï¼Œæ¨åŠ¨ç§‘ç ”ç¤¾åŒºçš„å‘å±•ï¼Œä¼šåœ¨readmeä¸­æ³¨æ˜ä»£ç çš„ä½œè€…~ï¼‰&lt;/p&gt; &#xA;&lt;h2&gt;å…¬ä¼—å· &amp;amp; å¾®ä¿¡äº¤æµç¾¤&lt;/h2&gt; &#xA;&lt;p&gt;æ¬¢è¿å¤§å®¶å…³æ³¨å…¬ä¼—å·ï¼š&lt;strong&gt;FightingCV&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;å…¬ä¼—å·&lt;strong&gt;æ¯å¤©&lt;/strong&gt;éƒ½ä¼šè¿›è¡Œ&lt;strong&gt;è®ºæ–‡ã€ç®—æ³•å’Œä»£ç çš„å¹²è´§åˆ†äº«&lt;/strong&gt;å“¦~&lt;/p&gt; &#xA;&lt;!-- å·²å»ºç«‹**æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç®—æ³•/è®¡ç®—æœºè§†è§‰/å¤šæ¨¡æ€äº¤æµç¾¤**å¾®ä¿¡äº¤æµç¾¤ï¼&#xA;&#xA;ï¼ˆåŠ ä¸è¿›å»å¯ä»¥åŠ å¾®ä¿¡ï¼š**775629340**ï¼Œè®°å¾—å¤‡æ³¨ã€**å…¬å¸/å­¦æ ¡+æ–¹å‘+ID**ã€‘ï¼‰ --&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ¯å¤©åœ¨ç¾¤é‡Œåˆ†äº«ä¸€äº›è¿‘æœŸçš„è®ºæ–‡å’Œè§£æ&lt;/strong&gt;ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·&lt;strong&gt;å­¦ä¹ äº¤æµ&lt;/strong&gt;å“ˆ~~~&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/FightingCVimg/wechat.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;å¼ºçƒˆæ¨èå¤§å®¶å…³æ³¨&lt;a href=&#34;https://www.zhihu.com/people/jason-14-58-38/posts&#34;&gt;&lt;strong&gt;çŸ¥ä¹&lt;/strong&gt;&lt;/a&gt;è´¦å·å’Œ&lt;a href=&#34;https://mp.weixin.qq.com/s/sgNw6XFBPcD20Ef3ddfE1w&#34;&gt;&lt;strong&gt;FightingCVå…¬ä¼—å·&lt;/strong&gt;&lt;/a&gt;ï¼Œå¯ä»¥å¿«é€Ÿäº†è§£åˆ°æœ€æ–°ä¼˜è´¨çš„å¹²è´§èµ„æºã€‚&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#attention-series&#34;&gt;Attention Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-external-attention-usage&#34;&gt;1. External Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-self-attention-usage&#34;&gt;2. Self Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-simplified-self-attention-usage&#34;&gt;3. Simplified Self Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-squeeze-and-excitation-attention-usage&#34;&gt;4. Squeeze-and-Excitation Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#5-sk-attention-usage&#34;&gt;5. SK Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#6-cbam-attention-usage&#34;&gt;6. CBAM Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#7-bam-attention-usage&#34;&gt;7. BAM Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#8-eca-attention-usage&#34;&gt;8. ECA Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#9-danet-attention-usage&#34;&gt;9. DANet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#10-Pyramid-Split-Attention-Usage&#34;&gt;10. Pyramid Split Attention (PSA) Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#11-Efficient-Multi-Head-Self-Attention-Usage&#34;&gt;11. Efficient Multi-Head Self-Attention(EMSA) Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#12-Shuffle-Attention-Usage&#34;&gt;12. Shuffle Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#13-MUSE-Attention-Usage&#34;&gt;13. MUSE Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#14-SGE-Attention-Usage&#34;&gt;14. SGE Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#15-A2-Attention-Usage&#34;&gt;15. A2 Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#16-AFT-Attention-Usage&#34;&gt;16. AFT Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#17-Outlook-Attention-Usage&#34;&gt;17. Outlook Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#18-ViP-Attention-Usage&#34;&gt;18. ViP Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#19-CoAtNet-Attention-Usage&#34;&gt;19. CoAtNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#20-HaloNet-Attention-Usage&#34;&gt;20. HaloNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#21-Polarized-Self-Attention-Usage&#34;&gt;21. Polarized Self-Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#22-CoTAttention-Usage&#34;&gt;22. CoTAttention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#23-Residual-Attention-Usage&#34;&gt;23. Residual Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#24-S2-Attention-Usage&#34;&gt;24. S2 Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#25-GFNet-Attention-Usage&#34;&gt;25. GFNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#26-TripletAttention-Usage&#34;&gt;26. Triplet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#27-Coordinate-Attention-Usage&#34;&gt;27. Coordinate Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#28-MobileViT-Attention-Usage&#34;&gt;28. MobileViT Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#29-ParNet-Attention-Usage&#34;&gt;29. ParNet Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#30-UFO-Attention-Usage&#34;&gt;30. UFO Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#31-MobileViTv2-Attention-Usage&#34;&gt;31. MobileViTv2 Attention Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#Backbone-series&#34;&gt;Backbone Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-ResNet-Usage&#34;&gt;1. ResNet Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-ResNeXt-Usage&#34;&gt;2. ResNeXt Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-MobileViT-Usage&#34;&gt;3. MobileViT Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-ConvMixer-Usage&#34;&gt;4. ConvMixer Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#mlp-series&#34;&gt;MLP Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-RepMLP-Usage&#34;&gt;1. RepMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-MLP-Mixer-Usage&#34;&gt;2. MLP-Mixer Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-ResMLP-Usage&#34;&gt;3. ResMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-gMLP-Usage&#34;&gt;4. gMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#5-sMLP-Usage&#34;&gt;5. sMLP Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#Re-Parameter-series&#34;&gt;Re-Parameter(ReP) Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-RepVGG-Usage&#34;&gt;1. RepVGG Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-ACNet-Usage&#34;&gt;2. ACNet Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-Diverse-Branch-Block-Usage&#34;&gt;3. Diverse Branch Block(DDB) Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#Convolution-series&#34;&gt;Convolution Series&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#1-Depthwise-Separable-Convolution-Usage&#34;&gt;1. Depthwise Separable Convolution Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#2-MBConv-Usage&#34;&gt;2. MBConv Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#3-Involution-Usage&#34;&gt;3. Involution Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#4-DynamicConv-Usage&#34;&gt;4. DynamicConv Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/#5-CondConv-Usage&#34;&gt;5. CondConv Usage&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Attention Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2105.02358&#34;&gt;&#34;Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;&#34;Attention Is All You Need---NIPS2017&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1709.01507&#34;&gt;&#34;Squeeze-and-Excitation Networks---CVPR2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1903.06586.pdf&#34;&gt;&#34;Selective Kernel Networks---CVPR2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf&#34;&gt;&#34;CBAM: Convolutional Block Attention Module---ECCV2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1807.06514.pdf&#34;&gt;&#34;BAM: Bottleneck Attention Module---BMCV2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1910.03151.pdf&#34;&gt;&#34;ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1809.02983.pdf&#34;&gt;&#34;Dual Attention Network for Scene Segmentation---CVPR2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.14447.pdf&#34;&gt;&#34;EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2105.13677&#34;&gt;&#34;ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2102.00240.pdf&#34;&gt;&#34;SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1911.09483&#34;&gt;&#34;MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1905.09646.pdf&#34;&gt;&#34;Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1810.11579.pdf&#34;&gt;&#34;A2-Nets: Double Attention Networks---NIPS2018&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.14103v1.pdf&#34;&gt;&#34;An Attention Free Transformer---ICLR2021 (Apple New Work)&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2106.13112&#34;&gt;VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24&#34;&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/385561050&#34;&gt;ã€è®ºæ–‡è§£æã€‘&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2106.12368&#34;&gt;Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23&lt;/a&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/5gonUQgBho_m2O54jyXF_Q&#34;&gt;ã€è®ºæ–‡è§£æã€‘&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2106.04803&#34;&gt;CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/385578588&#34;&gt;ã€è®ºæ–‡è§£æã€‘&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2103.12731.pdf&#34;&gt;Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/388598744&#34;&gt;ã€è®ºæ–‡è§£æã€‘&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2107.00782&#34;&gt;Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/389770482&#34;&gt;ã€è®ºæ–‡è§£æã€‘&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2107.12292&#34;&gt;Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/394795481&#34;&gt;ã€è®ºæ–‡è§£æã€‘&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2108.02456&#34;&gt;Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2108.01072&#34;&gt;SÂ²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/397003638&#34;&gt;ã€è®ºæ–‡è§£æã€‘&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2107.00645&#34;&gt;Global Filter Networks for Image Classification---arXiv 2021.07.01&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2010.03045&#34;&gt;Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2110.07641&#34;&gt;Non-deep Networks---ArXiv 2021.10.20&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2109.14382&#34;&gt;UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2206.02680&#34;&gt;Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1. External Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.02358&#34;&gt;&#34;Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/External_Attention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.ExternalAttention import ExternalAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,49,512)&#xA;ea = ExternalAttention(d_model=512,S=8)&#xA;output=ea(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Self Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;&#34;Attention Is All You Need&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SelfAttention import ScaledDotProductAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,49,512)&#xA;sa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;output=sa(input,input,input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Simplified Self Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;&#34;&gt;None&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SSA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,49,512)&#xA;ssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)&#xA;output=ssa(input,input,input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. Squeeze-and-Excitation Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.01507&#34;&gt;&#34;Squeeze-and-Excitation Networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SEAttention import SEAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;se = SEAttention(channel=512,reduction=8)&#xA;output=se(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. SK Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;5.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1903.06586.pdf&#34;&gt;&#34;Selective Kernel Networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SK.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SKAttention import SKAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;se = SKAttention(channel=512,reduction=8)&#xA;output=se(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;6. CBAM Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;6.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf&#34;&gt;&#34;CBAM: Convolutional Block Attention Module&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;6.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CBAM1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CBAM2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;6.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.CBAM import CBAMBlock&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;kernel_size=input.shape[2]&#xA;cbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)&#xA;output=cbam(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;7. BAM Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;7.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1807.06514.pdf&#34;&gt;&#34;BAM: Bottleneck Attention Module&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/BAM.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;7.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.BAM import BAMBlock&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;bam = BAMBlock(channel=512,reduction=16,dia_val=2)&#xA;output=bam(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;8. ECA Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;8.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.03151.pdf&#34;&gt;&#34;ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;8.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ECA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;8.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.ECAAttention import ECAAttention&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;eca = ECAAttention(kernel_size=3)&#xA;output=eca(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;9. DANet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;9.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1809.02983.pdf&#34;&gt;&#34;Dual Attention Network for Scene Segmentation&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;9.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/danet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;9.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.DANet import DAModule&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;danet=DAModule(d_model=512,kernel_size=3,H=7,W=7)&#xA;print(danet(input).shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;10. Pyramid Split Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;10.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.14447.pdf&#34;&gt;&#34;EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;10.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/psa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;10.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.PSA import PSA&#xA;import torch&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;psa = PSA(channel=512,reduction=8)&#xA;output=psa(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;11. Efficient Multi-Head Self-Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;11.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.13677&#34;&gt;&#34;ResT: An Efficient Transformer for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;11.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/EMSA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;11.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.EMSA import EMSA&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,64,512)&#xA;emsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)&#xA;output=emsa(input,input,input)&#xA;print(output.shape)&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;12. Shuffle Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;12.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.00240.pdf&#34;&gt;&#34;SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;12.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ShuffleAttention.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;12.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.ShuffleAttention import ShuffleAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;se = ShuffleAttention(channel=512,G=8)&#xA;output=se(input)&#xA;print(output.shape)&#xA;&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;13. MUSE Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;13.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.09483&#34;&gt;&#34;MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MUSE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;13.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.MUSEAttention import MUSEAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;&#xA;input=torch.randn(50,49,512)&#xA;sa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;output=sa(input,input,input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;14. SGE Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;14.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.09646.pdf&#34;&gt;Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;14.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/SGE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;14.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.SGE import SpatialGroupEnhance&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;sge = SpatialGroupEnhance(groups=8)&#xA;output=sge(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;15. A2 Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;15.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1810.11579.pdf&#34;&gt;A2-Nets: Double Attention Networks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;15.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/A2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;15.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.A2Atttention import DoubleAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;a2 = DoubleAttention(512,128,128,True)&#xA;output=a2(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;16. AFT Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;16.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.14103v1.pdf&#34;&gt;An Attention Free Transformer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;16.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/AFT.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;16.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.AFT import AFT_FULL&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,49,512)&#xA;aft_full = AFT_FULL(d_model=512, n=49)&#xA;output=aft_full(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;17. Outlook Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;17.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13112&#34;&gt;VOLO: Vision Outlooker for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;17.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/OutlookAttention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;17.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.OutlookAttention import OutlookAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,28,28,512)&#xA;outlook = OutlookAttention(dim=512)&#xA;output=outlook(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;18. ViP Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;18.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.12368&#34;&gt;Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;18.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ViP.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;18.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.ViP import WeightedPermuteMLP&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(64,8,8,512)&#xA;seg_dim=8&#xA;vip=WeightedPermuteMLP(512,seg_dim)&#xA;out=vip(input)&#xA;print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;19. CoAtNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;19.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.04803&#34;&gt;CoAtNet: Marrying Convolution and Attention for All Data Sizes&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;19.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;None&lt;/p&gt; &#xA;&lt;h4&gt;19.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.CoAtNet import CoAtNet&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,3,224,224)&#xA;mbconv=CoAtNet(in_ch=3,image_size=224)&#xA;out=mbconv(input)&#xA;print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;20. HaloNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;20.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.12731.pdf&#34;&gt;Scaling Local Self-Attention for Parameter Efficient Visual Backbones&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;20.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/HaloNet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;20.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.HaloAttention import HaloAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,512,8,8)&#xA;halo = HaloAttention(dim=512,&#xA;    block_size=2,&#xA;    halo_size=1,)&#xA;output=halo(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;21. Polarized Self-Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;21.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.00782&#34;&gt;Polarized Self-Attention: Towards High-quality Pixel-wise Regression&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;21.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/PoSA.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;21.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,512,7,7)&#xA;psa = SequentialPolarizedSelfAttention(channel=512)&#xA;output=psa(input)&#xA;print(output.shape)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;22. CoTAttention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;22.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.12292&#34;&gt;Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;22.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CoT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;22.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.CoTAttention import CoTAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;cot = CoTAttention(dim=512,kernel_size=3)&#xA;output=cot(input)&#xA;print(output.shape)&#xA;&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;23. Residual Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;23.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.02456&#34;&gt;Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;23.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ResAtt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;23.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.attention.ResidualAttention import ResidualAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;resatt = ResidualAttention(channel=512,num_class=1000,la=0.2)&#xA;output=resatt(input)&#xA;print(output.shape)&#xA;&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;24. S2 Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;24.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.01072&#34;&gt;SÂ²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;24.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/S2Attention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;24.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.S2Attention import S2Attention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(50,512,7,7)&#xA;s2att = S2Attention(channels=512)&#xA;output=s2att(input)&#xA;print(output.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;25. GFNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;25.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.00645&#34;&gt;Global Filter Networks for Image Classification---arXiv 2021.07.01&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;25.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/GFNet.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;25.3. Usage Code - Implemented by &lt;a href=&#34;https://scholar.google.com/citations?user=lyPWvuEAAAAJ&amp;amp;hl=en&#34;&gt;Wenliang Zhao (Author)&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.gfnet import GFNet&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;x = torch.randn(1, 3, 224, 224)&#xA;gfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)&#xA;out = gfnet(x)&#xA;print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;26. TripletAttention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;26.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.03045&#34;&gt;Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;26.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/triplet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;26.3. Usage Code - Implemented by &lt;a href=&#34;https://github.com/digantamisra98&#34;&gt;digantamisra98&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.TripletAttention import TripletAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;input=torch.randn(50,512,7,7)&#xA;triplet = TripletAttention()&#xA;output=triplet(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;27. Coordinate Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;27.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;Coordinate Attention for Efficient Mobile Network Design---CVPR 2021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;27.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CoordAttention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;27.3. Usage Code - Implemented by &lt;a href=&#34;https://github.com/Andrew-Qibin&#34;&gt;Andrew-Qibin&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.CoordAttention import CoordAtt&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;inp=torch.rand([2, 96, 56, 56])&#xA;inp_dim, oup_dim = 96, 96&#xA;reduction=32&#xA;&#xA;coord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)&#xA;output=coord_attention(inp)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;28. MobileViT Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;28.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;28.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MobileViTAttention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;28.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.MobileViTAttention import MobileViTAttention&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    m=MobileViTAttention()&#xA;    input=torch.randn(1,3,49,49)&#xA;    output=m(input)&#xA;    print(output.shape)  #output:(1,3,49,49)&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;29. ParNet Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;29.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.07641&#34;&gt;Non-deep Networks---ArXiv 2021.10.20&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;29.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ParNet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;29.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.ParNetAttention import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,512,7,7)&#xA;    pna = ParNetAttention(channel=512)&#xA;    output=pna(input)&#xA;    print(output.shape) #50,512,7,7&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;30. UFO Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;30.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.07641&#34;&gt;UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/UFO.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;30.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.UFOAttention import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,49,512)&#xA;    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;    output=ufo(input,input,input)&#xA;    print(output.shape) #[50, 49, 512]&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;31. MobileViTv2 Attention Usage&lt;/h3&gt; &#xA;&lt;h4&gt;31.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02680&#34;&gt;Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;31.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MobileViTv2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;31.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.attention.UFOAttention import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,49,512)&#xA;    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)&#xA;    output=ufo(input,input,input)&#xA;    print(output.shape) #[50, 49, 512]&#xA;    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Backbone Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;&#34;Deep Residual Learning for Image Recognition---CVPR2016 Best Paper&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1611.05431v2&#34;&gt;&#34;Aggregated Residual Transformations for Deep Neural Networks---CVPR2017&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://openreview.net/forum?id=TVHS5Y4dNvM&#34;&gt;Patches Are All You Need?---ICLR2022 (Under Review)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. ResNet Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;&gt;&#34;Deep Residual Learning for Image Recognition---CVPR2016 Best Paper&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resnet.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resnet2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.resnet import ResNet50,ResNet101,ResNet152&#xA;import torch&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,3,224,224)&#xA;    resnet50=ResNet50(1000)&#xA;    # resnet101=ResNet101(1000)&#xA;    # resnet152=ResNet152(1000)&#xA;    out=resnet50(input)&#xA;    print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. ResNeXt Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.05431v2&#34;&gt;&#34;Aggregated Residual Transformations for Deep Neural Networks---CVPR2017&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resnext.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152&#xA;import torch&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,3,224,224)&#xA;    resnext50=ResNeXt50(1000)&#xA;    # resnext101=ResNeXt101(1000)&#xA;    # resnext152=ResNeXt152(1000)&#xA;    out=resnext50(input)&#xA;    print(out.shape)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. MobileViT Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.02907&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/mobileViT.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.MobileViT import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(1,3,224,224)&#xA;&#xA;    ### mobilevit_xxs&#xA;    mvit_xxs=mobilevit_xxs()&#xA;    out=mvit_xxs(input)&#xA;    print(out.shape)&#xA;&#xA;    ### mobilevit_xs&#xA;    mvit_xs=mobilevit_xs()&#xA;    out=mvit_xs(input)&#xA;    print(out.shape)&#xA;&#xA;&#xA;    ### mobilevit_s&#xA;    mvit_s=mobilevit_s()&#xA;    out=mvit_s(input)&#xA;    print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. ConvMixer Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openreview.net/forum?id=TVHS5Y4dNvM&#34;&gt;Patches Are All You Need?---ICLR2022 (Under Review)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ConvMixer.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.backbone.ConvMixer import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    x=torch.randn(1,3,224,224)&#xA;    convmixer=ConvMixer(dim=512,depth=12)&#xA;    out=convmixer(x)&#xA;    print(out.shape)  #[1, 1000]&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;MLP Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.01883v1.pdf&#34;&gt;&#34;RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.01601.pdf&#34;&gt;&#34;MLP-Mixer: An all-MLP Architecture for Vision---arXiv 2021.05.17&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/pdf/2105.03404.pdf&#34;&gt;&#34;ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2105.08050&#34;&gt;&#34;Pay Attention to MLPs---arXiv 2021.05.17&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2109.05422&#34;&gt;&#34;Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. RepMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.01883v1.pdf&#34;&gt;&#34;RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/repmlp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.repmlp import RepMLP&#xA;import torch&#xA;from torch import nn&#xA;&#xA;N=4 #batch size&#xA;C=512 #input dim&#xA;O=1024 #output dim&#xA;H=14 #image height&#xA;W=14 #image width&#xA;h=7 #patch height&#xA;w=7 #patch width&#xA;fc1_fc2_reduction=1 #reduction ratio&#xA;fc3_groups=8 # groups&#xA;repconv_kernels=[1,3,5,7] #kernel list&#xA;repmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)&#xA;x=torch.randn(N,C,H,W)&#xA;repmlp.eval()&#xA;for module in repmlp.modules():&#xA;    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):&#xA;        nn.init.uniform_(module.running_mean, 0, 0.1)&#xA;        nn.init.uniform_(module.running_var, 0, 0.1)&#xA;        nn.init.uniform_(module.weight, 0, 0.1)&#xA;        nn.init.uniform_(module.bias, 0, 0.1)&#xA;&#xA;#training result&#xA;out=repmlp(x)&#xA;#inference result&#xA;repmlp.switch_to_deploy()&#xA;deployout = repmlp(x)&#xA;&#xA;print(((deployout-out)**2).sum())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. MLP-Mixer Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.01601.pdf&#34;&gt;&#34;MLP-Mixer: An all-MLP Architecture for Vision&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/mlpmixer.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.mlp_mixer import MlpMixer&#xA;import torch&#xA;mlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)&#xA;input=torch.randn(50,3,40,40)&#xA;output=mlp_mixer(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. ResMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.03404.pdf&#34;&gt;&#34;ResMLP: Feedforward networks for image classification with data-efficient training&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/resmlp.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.resmlp import ResMLP&#xA;import torch&#xA;&#xA;input=torch.randn(50,3,14,14)&#xA;resmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)&#xA;out=resmlp(input)&#xA;print(out.shape) #the last dimention is class_num&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. gMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.08050&#34;&gt;&#34;Pay Attention to MLPs&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/gMLP.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.g_mlp import gMLP&#xA;import torch&#xA;&#xA;num_tokens=10000&#xA;bs=50&#xA;len_sen=49&#xA;num_layers=6&#xA;input=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen&#xA;gmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)&#xA;output=gmlp(input)&#xA;print(output.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. sMLP Usage&lt;/h3&gt; &#xA;&lt;h4&gt;5.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.05422&#34;&gt;&#34;Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/sMLP.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.mlp.sMLP_block import sMLPBlock&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(50,3,224,224)&#xA;    smlp=sMLPBlock(h=224,w=224)&#xA;    out=smlp(input)&#xA;    print(out.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Re-Parameter Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2101.03697&#34;&gt;&#34;RepVGG: Making VGG-style ConvNets Great Again---CVPR2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1908.03930&#34;&gt;&#34;ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.13425&#34;&gt;&#34;Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1. RepVGG Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2101.03697&#34;&gt;&#34;RepVGG: Making VGG-style ConvNets Great Again&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/repvgg.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from model.rep.repvgg import RepBlock&#xA;import torch&#xA;&#xA;&#xA;input=torch.randn(50,512,49,49)&#xA;repblock=RepBlock(512,512)&#xA;repblock.eval()&#xA;out=repblock(input)&#xA;repblock._switch_to_deploy()&#xA;out2=repblock(input)&#xA;print(&#39;difference between vgg and repvgg&#39;)&#xA;print(((out2-out)**2).sum())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. ACNet Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1908.03930&#34;&gt;&#34;ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/acnet.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.acnet import ACNet&#xA;import torch&#xA;from torch import nn&#xA;&#xA;input=torch.randn(50,512,49,49)&#xA;acnet=ACNet(512,512)&#xA;acnet.eval()&#xA;out=acnet(input)&#xA;acnet._switch_to_deploy()&#xA;out2=acnet(input)&#xA;print(&#39;difference:&#39;)&#xA;print(((out2-out)**2).sum())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. Diverse Branch Block Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.13425&#34;&gt;&#34;Diverse Branch Block: Building a Convolution as an Inception-like Unit&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/ddb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;h5&gt;2.3.1 Transform I&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transI_conv_bn&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;#conv+bn&#xA;conv1=nn.Conv2d(64,64,3,padding=1)&#xA;bn1=nn.BatchNorm2d(64)&#xA;bn1.eval()&#xA;out1=bn1(conv1(input))&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.2 Transform II&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transII_conv_branch&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1=nn.Conv2d(64,64,3,padding=1)&#xA;conv2=nn.Conv2d(64,64,3,padding=1)&#xA;out1=conv1(input)+conv2(input)&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.3 Transform III&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transIII_conv_sequential&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1=nn.Conv2d(64,64,1,padding=0,bias=False)&#xA;conv2=nn.Conv2d(64,64,3,padding=1,bias=False)&#xA;out1=conv2(conv1(input))&#xA;&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1,bias=False)&#xA;conv_fuse.weight.data=transIII_conv_sequential(conv1,conv2)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.4 Transform IV&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transIV_conv_concat&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1=nn.Conv2d(64,32,3,padding=1)&#xA;conv2=nn.Conv2d(64,32,3,padding=1)&#xA;out1=torch.cat([conv1(input),conv2(input)],dim=1)&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.5 Transform V&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transV_avg&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;avg=nn.AvgPool2d(kernel_size=3,stride=1)&#xA;out1=avg(input)&#xA;&#xA;conv=transV_avg(64,3)&#xA;out2=conv(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;2.3.6 Transform VI&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.rep.ddb import transVI_conv_scale&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,64,7,7)&#xA;&#xA;#conv+conv&#xA;conv1x1=nn.Conv2d(64,64,1)&#xA;conv1x3=nn.Conv2d(64,64,(1,3),padding=(0,1))&#xA;conv3x1=nn.Conv2d(64,64,(3,1),padding=(1,0))&#xA;out1=conv1x1(input)+conv1x3(input)+conv3x1(input)&#xA;&#xA;#conv_fuse&#xA;conv_fuse=nn.Conv2d(64,64,3,padding=1)&#xA;conv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1)&#xA;out2=conv_fuse(input)&#xA;&#xA;print(&#34;difference:&#34;,((out2-out1)**2).sum().item())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Convolution Series&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;&#34;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;http://proceedings.mlr.press/v97/tan19a.html&#34;&gt;&#34;Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2103.06255&#34;&gt;&#34;Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1912.03458&#34;&gt;&#34;Dynamic Convolution: Attention over Convolution Kernels---CVPR2020 Oral&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pytorch implementation of &lt;a href=&#34;https://arxiv.org/abs/1904.04971&#34;&gt;&#34;CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019&#34;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;1. Depthwise Separable Convolution Usage&lt;/h3&gt; &#xA;&lt;h4&gt;1.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;&#34;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/DepthwiseSeparableConv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,3,224,224)&#xA;dsconv=DepthwiseSeparableConvolution(3,64)&#xA;out=dsconv(input)&#xA;print(out.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;2. MBConv Usage&lt;/h3&gt; &#xA;&lt;h4&gt;2.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v97/tan19a.html&#34;&gt;&#34;Efficientnet: Rethinking model scaling for convolutional neural networks&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/MBConv.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.MBConv import MBConvBlock&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,3,224,224)&#xA;mbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)&#xA;out=mbconv(input)&#xA;print(out.shape)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;3. Involution Usage&lt;/h3&gt; &#xA;&lt;h4&gt;3.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.06255&#34;&gt;&#34;Involution: Inverting the Inherence of Convolution for Visual Recognition&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/Involution.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.Involution import Involution&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;input=torch.randn(1,4,64,64)&#xA;involution=Involution(kernel_size=3,in_channel=4,stride=2)&#xA;out=involution(input)&#xA;print(out.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;4. DynamicConv Usage&lt;/h3&gt; &#xA;&lt;h4&gt;4.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.03458&#34;&gt;&#34;Dynamic Convolution: Attention over Convolution Kernels&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/DynamicConv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;4.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.DynamicConv import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(2,32,64,64)&#xA;    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)&#xA;    out=m(input)&#xA;    print(out.shape) # 2,32,64,64&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;5. CondConv Usage&lt;/h3&gt; &#xA;&lt;h4&gt;5.1. Paper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1904.04971&#34;&gt;&#34;CondConv: Conditionally Parameterized Convolutions for Efficient Inference&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.2. Overview&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xmu-xiaoma666/External-Attention-pytorch/master/model/img/CondConv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5.3. Usage Code&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from model.conv.CondConv import *&#xA;import torch&#xA;from torch import nn&#xA;from torch.nn import functional as F&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    input=torch.randn(2,32,64,64)&#xA;    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)&#xA;    out=m(input)&#xA;    print(out.shape)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>bigb0sss/RedTeam-OffensiveSecurity</title>
    <updated>2022-06-25T01:32:05Z</updated>
    <id>tag:github.com,2022-06-25:/bigb0sss/RedTeam-OffensiveSecurity</id>
    <link href="https://github.com/bigb0sss/RedTeam-OffensiveSecurity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tools &amp; Interesting Things for RedTeam Ops&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; height=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/bigb0sss/RedTeam-OffensiveSecurity/master/images/redteam_logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;OSINT&lt;/h2&gt; &#xA;&lt;h3&gt;Passive Discovery&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amass - &lt;a href=&#34;https://github.com/OWASP/Amass&#34;&gt;https://github.com/OWASP/Amass&lt;/a&gt; (Attack Surface Mapping)&lt;/li&gt; &#xA; &lt;li&gt;Metabigor - &lt;a href=&#34;https://github.com/j3ssie/metabigor&#34;&gt;https://github.com/j3ssie/metabigor&lt;/a&gt; (Non-API OSINT)&lt;/li&gt; &#xA; &lt;li&gt;AsINT_Collection - &lt;a href=&#34;https://start.me/p/b5Aow7/asint_collection&#34;&gt;https://start.me/p/b5Aow7/asint_collection&lt;/a&gt; (Massive OSINT Collection)&lt;/li&gt; &#xA; &lt;li&gt;Email --&amp;gt; Phone# - &lt;a href=&#34;https://github.com/iansangaji/email2phonenumber&#34;&gt;https://github.com/iansangaji/email2phonenumber&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MFASweep - &lt;a href=&#34;https://github.com/dafthack/MFASweep&#34;&gt;https://github.com/dafthack/MFASweep&lt;/a&gt; (MFA Check for Microsoft endpoints)&lt;/li&gt; &#xA; &lt;li&gt;Fast-Google-Dorks-Scan - &lt;a href=&#34;https://github.com/IvanGlinkin/Fast-Google-Dorks-Scan?mc_cid=70cff8af7c&amp;amp;mc_eid=eff0f218d6&#34;&gt;https://github.com/IvanGlinkin/Fast-Google-Dorks-Scan?mc_cid=70cff8af7c&amp;amp;mc_eid=eff0f218d6&lt;/a&gt; (Google Dork)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Target User Population Collection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linkedin UserEnum - &lt;a href=&#34;https://github.com/bigb0sss/LinkedinMama&#34;&gt;https://github.com/bigb0sss/LinkedinMama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;US Staff UserEnum - &lt;a href=&#34;https://github.com/bigb0sss/USStaffMama&#34;&gt;https://github.com/bigb0sss/USStaffMama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NameSpi - &lt;a href=&#34;https://github.com/waffl3ss/NameSpi&#34;&gt;https://github.com/waffl3ss/NameSpi&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Public Site Lookup (Github, Gitlab, etc.)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gitrob - &lt;a href=&#34;https://github.com/michenriksen/gitrob/&#34;&gt;https://github.com/michenriksen/gitrob/&lt;/a&gt; (Github Search)&lt;/li&gt; &#xA; &lt;li&gt;truffleHog - &lt;a href=&#34;https://github.com/dxa4481/truffleHog&#34;&gt;https://github.com/dxa4481/truffleHog&lt;/a&gt; (Github Regex Search)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cloud Recon&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloud_Security_Wiki - &lt;a href=&#34;https://cloudsecwiki.com/azure_cloud.html&#34;&gt;https://cloudsecwiki.com/azure_cloud.html&lt;/a&gt; (Awesome cloud resources)&lt;/li&gt; &#xA; &lt;li&gt;cloud_enum - &lt;a href=&#34;https://github.com/initstring/cloud_enum&#34;&gt;https://github.com/initstring/cloud_enum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MicroBurst - &lt;a href=&#34;https://github.com/NetSPI/MicroBurst&#34;&gt;https://github.com/NetSPI/MicroBurst&lt;/a&gt; (AZURE)&lt;/li&gt; &#xA; &lt;li&gt;pacu - &lt;a href=&#34;https://github.com/RhinoSecurityLabs/pacu&#34;&gt;https://github.com/RhinoSecurityLabs/pacu&lt;/a&gt; (AWS)&lt;/li&gt; &#xA; &lt;li&gt;FestIn - &lt;a href=&#34;https://github.com/cr0hn/festin&#34;&gt;https://github.com/cr0hn/festin&lt;/a&gt; (AWS)&lt;/li&gt; &#xA; &lt;li&gt;s3viewer - &lt;a href=&#34;https://github.com/SharonBrizinov/s3viewer&#34;&gt;https://github.com/SharonBrizinov/s3viewer&lt;/a&gt; (AWS)&lt;/li&gt; &#xA; &lt;li&gt;Cloud_Pentest_Cheatsheet - &lt;a href=&#34;https://github.com/dafthack/CloudPentestCheatsheets&#34;&gt;https://github.com/dafthack/CloudPentestCheatsheets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;endgame - &lt;a href=&#34;https://github.com/salesforce/endgame&#34;&gt;https://github.com/salesforce/endgame&lt;/a&gt; (AWS)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Microsoft / Windows&lt;/h3&gt; &#xA;&lt;h4&gt;Active Discovery&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ZGrab - &lt;a href=&#34;https://github.com/zmap/zgrab&#34;&gt;https://github.com/zmap/zgrab&lt;/a&gt; (Banner grabber)&lt;/li&gt; &#xA; &lt;li&gt;Hardenize - &lt;a href=&#34;https://www.hardenize.com/&#34;&gt;https://www.hardenize.com/&lt;/a&gt; (Domain Lookup)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;ADFS&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ADFSpoof - &lt;a href=&#34;https://github.com/fireeye/ADFSpoof&#34;&gt;https://github.com/fireeye/ADFSpoof&lt;/a&gt; (Forge ADFS security tokens)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Web App&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wordpress-Exploit-Framework - &lt;a href=&#34;https://github.com/rastating/wordpress-exploit-framework&#34;&gt;https://github.com/rastating/wordpress-exploit-framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Awesome-Web-Security - &lt;a href=&#34;https://github.com/qazbnm456/awesome-web-security&#34;&gt;https://github.com/qazbnm456/awesome-web-security&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Java Deserialization - &lt;a href=&#34;https://github.com/frohoff/ysoserial&#34;&gt;https://github.com/frohoff/ysoserial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PHP Deserialization - &lt;a href=&#34;https://github.com/ambionics/phpggc&#34;&gt;https://github.com/ambionics/phpggc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kubernetes - &lt;a href=&#34;https://github.com/loodse/kubectl-hacking&#34;&gt;https://github.com/loodse/kubectl-hacking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SSRF - &lt;a href=&#34;https://github.com/jdonsec/AllThingsSSRF&#34;&gt;https://github.com/jdonsec/AllThingsSSRF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Skf-labs - &lt;a href=&#34;https://owasp-skf.gitbook.io/asvs-write-ups/&#34;&gt;https://owasp-skf.gitbook.io/asvs-write-ups/&lt;/a&gt; (Great Write-ups) &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Phishing&lt;/h2&gt; &#xA;&lt;h3&gt;Phishing Techniques - &lt;a href=&#34;https://blog.sublimesecurity.com/&#34;&gt;https://blog.sublimesecurity.com/&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Microsfot 365 Device Code Phishing&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;devicePhish - &lt;a href=&#34;https://github.com/bigb0sss/Microsoft365_devicePhish&#34;&gt;https://github.com/bigb0sss/Microsoft365_devicePhish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TokenTactics - &lt;a href=&#34;https://github.com/rvrsh3ll/TokenTactics&#34;&gt;https://github.com/rvrsh3ll/TokenTactics&lt;/a&gt; &lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Password Spray&lt;/h2&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MSOLSpray - &lt;a href=&#34;https://github.com/dafthack/MSOLSpray&#34;&gt;https://github.com/dafthack/MSOLSpray&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;o365enum.py - &lt;a href=&#34;https://github.com/gremwell/o365enum&#34;&gt;https://github.com/gremwell/o365enum&lt;/a&gt; (Microsoft ActiveSync)&lt;/li&gt; &#xA; &lt;li&gt;goPassGen - &lt;a href=&#34;https://github.com/bigb0sss/goPassGen&#34;&gt;https://github.com/bigb0sss/goPassGen&lt;/a&gt; (PasswordSpray List Generator)&lt;/li&gt; &#xA; &lt;li&gt;go365 - &lt;a href=&#34;https://github.com/optiv/Go365&#34;&gt;https://github.com/optiv/Go365&lt;/a&gt; (Microsoft SOAP API endpoint on login.microsoftonline.com)&lt;/li&gt; &#xA; &lt;li&gt;Okta - &lt;a href=&#34;https://github.com/Rhynorater/Okta-Password-Sprayer&#34;&gt;https://github.com/Rhynorater/Okta-Password-Sprayer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;o365Spray - &lt;a href=&#34;https://github.com/0xZDH/o365spray&#34;&gt;https://github.com/0xZDH/o365spray&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Spray365 - &lt;a href=&#34;https://github.com/MarkoH17/Spray365&#34;&gt;https://github.com/MarkoH17/Spray365&lt;/a&gt; (Microsoft365 / Azure AD)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;IP Rotators&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Burp IPRotate - &lt;a href=&#34;https://github.com/PortSwigger/ip-rotate&#34;&gt;https://github.com/PortSwigger/ip-rotate&lt;/a&gt; (Utilizes AWS IP Gateway)&lt;/li&gt; &#xA; &lt;li&gt;ProxyCannon-NG - &lt;a href=&#34;https://github.com/proxycannon/proxycannon-ng&#34;&gt;https://github.com/proxycannon/proxycannon-ng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cloud-proxy - &lt;a href=&#34;https://github.com/tomsteele/cloud-proxy&#34;&gt;https://github.com/tomsteele/cloud-proxy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Proxy-NG - &lt;a href=&#34;https://github.com/jamesbcook/proxy-ng&#34;&gt;https://github.com/jamesbcook/proxy-ng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mubeng - &lt;a href=&#34;https://github.com/kitabisa/mubeng#proxy-ip-rotator&#34;&gt;https://github.com/kitabisa/mubeng#proxy-ip-rotator&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Default Password Check&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CIRT - &lt;a href=&#34;https://cirt.net/passwords&#34;&gt;https://cirt.net/passwords&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DefaultCreds-cheat-sheet - &lt;a href=&#34;https://github.com/ihebski/DefaultCreds-cheat-sheet&#34;&gt;https://github.com/ihebski/DefaultCreds-cheat-sheet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Infrastructure&lt;/h2&gt; &#xA;&lt;h3&gt;Cobal Strike&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Beacon Command Cheatsheet - &lt;a href=&#34;https://github.com/bigb0sss/RedTeam/tree/master/CobaltStrike&#34;&gt;CS Commands&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cobalt Strike Training Review &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://medium.com/@bigb0ss/red-team-review-of-red-team-operations-with-cobalt-strike-2019-training-course-part-1-962c510565aa&#34;&gt;Part 1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;SharpeningCobaltStrike - &lt;a href=&#34;https://github.com/cube0x0/SharpeningCobaltStrike&#34;&gt;https://github.com/cube0x0/SharpeningCobaltStrike&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alternative ExecuteAssembly - &lt;a href=&#34;https://github.com/med0x2e/ExecuteAssembly&#34;&gt;https://github.com/med0x2e/ExecuteAssembly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inline ExecuteAssembly - &lt;a href=&#34;https://github.com/anthemtotheego/InlineExecute-Assembly&#34;&gt;https://github.com/anthemtotheego/InlineExecute-Assembly&lt;/a&gt; (Executing .NET Assembly in the same process unline CS&#39;s Execute-Assembly)&lt;/li&gt; &#xA; &lt;li&gt;BOF (Beacon Object Files) - &lt;a href=&#34;https://github.com/trustedsec/CS-Situational-Awareness-BOF&#34;&gt;https://github.com/trustedsec/CS-Situational-Awareness-BOF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Malleable C2&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Malleable C2 (Guideline) - &lt;a href=&#34;https://github.com/bigb0sss/RedTeam/raw/master/CobaltStrike/malleable_C2_profile/CS4.0_guideline.profile&#34;&gt;CS4.0_guideline.profile&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Malleable C2 Randomizer - &lt;a href=&#34;https://fortynorthsecurity.com/blog/introducing-c2concealer/&#34;&gt;https://fortynorthsecurity.com/blog/introducing-c2concealer/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SourcePoint - &lt;a href=&#34;https://github.com/Tylous/SourcePoint&#34;&gt;https://github.com/Tylous/SourcePoint&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;C2 (Opensource)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OffensiveNotion - &lt;a href=&#34;https://github.com/mttaggart/OffensiveNotion&#34;&gt;https://github.com/mttaggart/OffensiveNotion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Redirectors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Domain Fronting - &lt;a href=&#34;https://www.bamsoftware.com/papers/fronting/&#34;&gt;https://www.bamsoftware.com/papers/fronting/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Proxy Infrastructure Setup&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cloud-proxy - &lt;a href=&#34;https://github.com/tomsteele/cloud-proxy&#34;&gt;https://github.com/tomsteele/cloud-proxy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Proxy-ng - &lt;a href=&#34;https://github.com/jamesbcook/proxy-ng&#34;&gt;https://github.com/jamesbcook/proxy-ng&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ProxyCannon - &lt;a href=&#34;https://github.com/proxycannon/proxycannon-ng&#34;&gt;https://github.com/proxycannon/proxycannon-ng&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Living Off Trusted Sites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LOTS - &lt;a href=&#34;https://lots-project.com/&#34;&gt;https://lots-project.com/&lt;/a&gt; (Trusted sites for C2/Phishing/Downloading)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Post-Exploitation&lt;/h2&gt; &#xA;&lt;h3&gt;Windows Active Directory Recon/Survey&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seatbelt - &lt;a href=&#34;https://github.com/GhostPack/Seatbelt&#34;&gt;https://github.com/GhostPack/Seatbelt&lt;/a&gt; (Ghostpack)&lt;/li&gt; &#xA; &lt;li&gt;DNS Enum - &lt;a href=&#34;https://github.com/dirkjanm/adidnsdump&#34;&gt;https://github.com/dirkjanm/adidnsdump&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows Active Directory Attacks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Attacking &amp;amp; Securing Active Directory - &lt;a href=&#34;https://rmusser.net/docs/Active_Directory.html&#34;&gt;https://rmusser.net/docs/Active_Directory.html&lt;/a&gt; (Awesome references)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Internal Phishing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pickl3 - &lt;a href=&#34;https://github.com/hlldz/pickl3&#34;&gt;https://github.com/hlldz/pickl3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CredPhisher - &lt;a href=&#34;https://github.com/matterpreter/OffensiveCSharp/tree/master/CredPhisher&#34;&gt;https://github.com/matterpreter/OffensiveCSharp/tree/master/CredPhisher&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Credential Theft&lt;/h3&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mimikatz Command References - &lt;a href=&#34;https://adsecurity.org/?page_id=1821&#34;&gt;https://adsecurity.org/?page_id=1821&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Internet Browsers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SharpChromium - &lt;a href=&#34;https://github.com/djhohnstein/SharpChromium&#34;&gt;https://github.com/djhohnstein/SharpChromium&lt;/a&gt; (Chrome)&lt;/li&gt; &#xA; &lt;li&gt;EvilSeleium - &lt;a href=&#34;https://github.com/mrd0x/EvilSelenium&#34;&gt;https://github.com/mrd0x/EvilSelenium&lt;/a&gt; (Chrome)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;LSASS&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SharpDump - &lt;a href=&#34;https://github.com/GhostPack/SharpDump&#34;&gt;https://github.com/GhostPack/SharpDump&lt;/a&gt; (Highly IOC&#39;d)&lt;/li&gt; &#xA; &lt;li&gt;SharpMiniDump - &lt;a href=&#34;https://github.com/b4rtik/SharpMiniDump&#34;&gt;https://github.com/b4rtik/SharpMiniDump&lt;/a&gt; (Uses dynamic API calls, direct syscall and Native API unhooking to evade the AV / EDR detection - Win10 - WinServer2016)&lt;/li&gt; &#xA; &lt;li&gt;Dumper2020 - &lt;a href=&#34;https://github.com/gitjdm/dumper2020&#34;&gt;https://github.com/gitjdm/dumper2020&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nanodump - &lt;a href=&#34;https://github.com/helpsystems/nanodump&#34;&gt;https://github.com/helpsystems/nanodump&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Lateral Movement&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SpectorOps - &lt;a href=&#34;https://posts.specterops.io/offensive-lateral-movement-1744ae62b14f&#34;&gt;https://posts.specterops.io/offensive-lateral-movement-1744ae62b14f&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pypykatz - &lt;a href=&#34;https://github.com/skelsec/pypykatz&#34;&gt;https://github.com/skelsec/pypykatz&lt;/a&gt; (Python implementation of Mimikatz)&lt;/li&gt; &#xA; &lt;li&gt;Internal-Monologue - &lt;a href=&#34;https://github.com/eladshamir/Internal-Monologue&#34;&gt;https://github.com/eladshamir/Internal-Monologue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MSSQL - &lt;a href=&#34;https://research.nccgroup.com/2021/01/21/mssql-lateral-movement/&#34;&gt;https://research.nccgroup.com/2021/01/21/mssql-lateral-movement/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LiquidSnake - &lt;a href=&#34;https://github.com/RiccardoAncarani/LiquidSnake&#34;&gt;https://github.com/RiccardoAncarani/LiquidSnake&lt;/a&gt; (Fileless LM using WMI Event Subscriptions and GadgetToJScript)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Offensive C#&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OffensiveCSharp - &lt;a href=&#34;https://github.com/matterpreter/OffensiveCSharp&#34;&gt;https://github.com/matterpreter/OffensiveCSharp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C# Collection - &lt;a href=&#34;https://github.com/midnightslacker/Sharp/raw/master/README.md&#34;&gt;https://github.com/midnightslacker/Sharp/blob/master/README.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LiveOffTheLand&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LOLBAS - &lt;a href=&#34;https://lolbas-project.github.io/&#34;&gt;https://lolbas-project.github.io/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;AV/AMSI Evasion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;xencrypt - &lt;a href=&#34;https://github.com/the-xentropy/xencrypt&#34;&gt;https://github.com/the-xentropy/xencrypt&lt;/a&gt; (PowerShell)&lt;/li&gt; &#xA; &lt;li&gt;FalconStrike - &lt;a href=&#34;https://github.com/slaeryan/FALCONSTRIKE&#34;&gt;https://github.com/slaeryan/FALCONSTRIKE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AV_Bypass - &lt;a href=&#34;https://github.com/Techryptic/AV_Bypass&#34;&gt;https://github.com/Techryptic/AV_Bypass&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DotNetToJScript - &lt;a href=&#34;https://github.com/tyranid/DotNetToJScript&#34;&gt;https://github.com/tyranid/DotNetToJScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GadgetToJScript - &lt;a href=&#34;https://github.com/med0x2e/GadgetToJScript&#34;&gt;https://github.com/med0x2e/GadgetToJScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GadgetToJScript - &lt;a href=&#34;https://github.com/rasta-mouse/GadgetToJScript&#34;&gt;https://github.com/rasta-mouse/GadgetToJScript&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Shellcodeloader - &lt;a href=&#34;https://github.com/knownsec/shellcodeloader&#34;&gt;https://github.com/knownsec/shellcodeloader&lt;/a&gt; (ShellcodeLoader of windows can bypass AV)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;EDR Evasion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SharpBlock - &lt;a href=&#34;https://github.com/CCob/SharpBlock&#34;&gt;https://github.com/CCob/SharpBlock&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ScareCrow - &lt;a href=&#34;https://github.com/optiv/ScareCrow&#34;&gt;https://github.com/optiv/ScareCrow&lt;/a&gt; (EDR Bypass Payload Creation Framework)&lt;/li&gt; &#xA; &lt;li&gt;Cobalt Strike Tradecraft &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://hausec.com/2021/07/26/cobalt-strike-and-tradecraft/amp/?__twitter_impression=true&#34;&gt;https://hausec.com/2021/07/26/cobalt-strike-and-tradecraft/amp/?__twitter_impression=true&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.cobaltstrike.com/help-opsec&#34;&gt;https://www.cobaltstrike.com/help-opsec&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;PowerShell&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;p3nt4 - &lt;a href=&#34;https://github.com/p3nt4&#34;&gt;https://github.com/p3nt4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Log/Trace Deletion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;moonwalk - &lt;a href=&#34;https://github.com/mufeedvh/moonwalk&#34;&gt;https://github.com/mufeedvh/moonwalk&lt;/a&gt; (Linux logs/filesystem timestamps deletion)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Exploit Dev&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ondrik8/exploit&#34;&gt;https://github.com/Ondrik8/exploit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Undocumented Func (Win NT/2000/XP/Win7) - &lt;a href=&#34;http://undocumented.ntinternals.net/&#34;&gt;http://undocumented.ntinternals.net/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows Syscall - &lt;a href=&#34;https://j00ru.vexillium.org/syscalls/nt/64/&#34;&gt;https://j00ru.vexillium.org/syscalls/nt/64/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows Undocumented Func - &lt;a href=&#34;http://undocumented.ntinternals.net/&#34;&gt;http://undocumented.ntinternals.net/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows Kernel Exploit Training - &lt;a href=&#34;https://codemachine.com/&#34;&gt;https://codemachine.com/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Anti-Debug - &lt;a href=&#34;https://anti-debug.checkpoint.com/&#34;&gt;https://anti-debug.checkpoint.com/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Nix&lt;/h3&gt; &#xA;&lt;h2&gt;RedTeam Researchers/Githubs/Gitbooks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Vincent Yiu - &lt;a href=&#34;https://vincentyiu.com&#34;&gt;https://vincentyiu.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Outflank - &lt;a href=&#34;https://github.com/outflanknl&#34;&gt;https://github.com/outflanknl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bank Security - &lt;a href=&#34;https://github.com/BankSecurity/Red_Team&#34;&gt;https://github.com/BankSecurity/Red_Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Infosecn1nja - &lt;a href=&#34;https://github.com/infosecn1nja&#34;&gt;https://github.com/infosecn1nja&lt;/a&gt; (Redteam-Toolkit = AWESOME)&lt;/li&gt; &#xA; &lt;li&gt;Yeyintminthuhtut - &lt;a href=&#34;https://github.com/yeyintminthuhtut&#34;&gt;https://github.com/yeyintminthuhtut&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;RedCanary (Atomic RedTeam) - &lt;a href=&#34;https://github.com/redcanaryco/atomic-red-team&#34;&gt;https://github.com/redcanaryco/atomic-red-team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;kmkz - &lt;a href=&#34;https://github.com/kmkz/Pentesting&#34;&gt;https://github.com/kmkz/Pentesting&lt;/a&gt; (Good cheat-sheets)&lt;/li&gt; &#xA; &lt;li&gt;Rastamouse - &lt;a href=&#34;https://offensivedefence.co.uk/authors/rastamouse/&#34;&gt;https://offensivedefence.co.uk/authors/rastamouse/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(Gitbook) dmcxblue - &lt;a href=&#34;https://dmcxblue.gitbook.io/red-team-notes-2-0/&#34;&gt;https://dmcxblue.gitbook.io/red-team-notes-2-0/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lab Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows Server VMs - &lt;a href=&#34;https://www.microsoft.com/en-us/evalcenter&#34;&gt;https://www.microsoft.com/en-us/evalcenter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows 10 - &lt;a href=&#34;https://www.microsoft.com/en-us/software-download/windows10ISO&#34;&gt;https://www.microsoft.com/en-us/software-download/windows10ISO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Archive of WinVMs - &lt;a href=&#34;https://archive.org/search.php?query=subject%3A%22IEVM%22&#34;&gt;https://archive.org/search.php?query=subject%3A%22IEVM%22&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Public MSDN - &lt;a href=&#34;https://the-eye.eu/public/MSDN/&#34;&gt;Link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adversary Tactics: PowerShell - &lt;a href=&#34;https://github.com/specterops/at-ps&#34;&gt;https://github.com/specterops/at-ps&lt;/a&gt; (Specterops)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Cloud&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AWS Threat Simulation and Detection - &lt;a href=&#34;https://github.com/sbasu7241/AWS-Threat-Simulation-and-Detection&#34;&gt;https://github.com/sbasu7241/AWS-Threat-Simulation-and-Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stratus Red Team - &lt;a href=&#34;https://github.com/DataDog/stratus-red-team&#34;&gt;https://github.com/DataDog/stratus-red-team&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sexy Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MITRE ATT&amp;amp;CK - &lt;a href=&#34;https://attack.mitre.org/&#34;&gt;https://attack.mitre.org/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MalwareNews - &lt;a href=&#34;https://malware.news/&#34;&gt;https://malware.news/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CWE - &lt;a href=&#34;http://cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html&#34;&gt;http://cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CTID - &lt;a href=&#34;https://github.com/center-for-threat-informed-defense&#34;&gt;https://github.com/center-for-threat-informed-defense&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SpritesMods - &lt;a href=&#34;http://spritesmods.com/?art=main&#34;&gt;http://spritesmods.com/?art=main&lt;/a&gt; (Product Security)&lt;/li&gt; &#xA; &lt;li&gt;Joeware - &lt;a href=&#34;http://www.joeware.net/&#34;&gt;http://www.joeware.net/&lt;/a&gt; (Windows AD Guru - Many AD Recon bins and amazing blogs)&lt;/li&gt; &#xA; &lt;li&gt;Tenable - &lt;a href=&#34;https://github.com/tenable/poc&#34;&gt;https://github.com/tenable/poc&lt;/a&gt; (Exploit POCs)&lt;/li&gt; &#xA; &lt;li&gt;MalwareUnicorn - &lt;a href=&#34;https://malwareunicorn.org/&#34;&gt;https://malwareunicorn.org/&lt;/a&gt; (Malware/Reversing)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Security Testing Practice Lab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hackthebox - &lt;a href=&#34;https://www.hackthebox.eu/&#34;&gt;https://www.hackthebox.eu/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cyberseclab - &lt;a href=&#34;https://www.cyberseclabs.co.uk/&#34;&gt;https://www.cyberseclabs.co.uk/&lt;/a&gt; (AD Focus)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BlueTeam&lt;/h2&gt; &#xA;&lt;h3&gt;Lab Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Detection Lab - &lt;a href=&#34;https://github.com/clong/DetectionLab&#34;&gt;https://github.com/clong/DetectionLab&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Threat Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KQL - &lt;a href=&#34;https://github.com/DebugPrivilege/KQL&#34;&gt;https://github.com/DebugPrivilege/KQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sigma - &lt;a href=&#34;https://github.com/Neo23x0/sigma&#34;&gt;https://github.com/Neo23x0/sigma&lt;/a&gt; (Generic Signature Format for SIEM)&lt;/li&gt; &#xA; &lt;li&gt;Splunk Security Essential Docs - &lt;a href=&#34;https://docs.splunksecurityessentials.com/content-detail/&#34;&gt;https://docs.splunksecurityessentials.com/content-detail/&lt;/a&gt; (Various IOCs)&lt;/li&gt; &#xA; &lt;li&gt;Cobalt Strike Defense - &lt;a href=&#34;https://github.com/MichaelKoczwara/Awesome-CobaltStrike-Defence&#34;&gt;https://github.com/MichaelKoczwara/Awesome-CobaltStrike-Defence&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dorothy - &lt;a href=&#34;https://github.com/elastic/dorothy&#34;&gt;https://github.com/elastic/dorothy&lt;/a&gt; (Okta SSO Monitoring and Detection)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Windows Security (What will BlueTeam look for?)&lt;/h3&gt; &#xA;&lt;h4&gt;LDAP (Lightweight Directory Access Protocol)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://techcommunity.microsoft.com/t5/microsoft-defender-for-endpoint/hunting-for-reconnaissance-activities-using-ldap-search-filters/ba-p/824726&#34;&gt;Hunting for reconnaissance activities using LDAP search filter (Microsoft)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;All the credits belong to the original authors and publishers.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;@bigb0ss&lt;/li&gt; &#xA; &lt;li&gt;@T145&lt;/li&gt; &#xA; &lt;li&gt;@threat-punter&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>vcheckzen/KeepAliveE5</title>
    <updated>2022-06-25T01:32:05Z</updated>
    <id>tag:github.com,2022-06-25:/vcheckzen/KeepAliveE5</id>
    <link href="https://github.com/vcheckzen/KeepAliveE5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Set the following repo secrets, then trigger &lt;code&gt;Register APP&lt;/code&gt; workflow manually. Read &lt;a href=&#34;https://sl.al/ABzD&#34;&gt;https://sl.al/ABzD&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PAT&lt;/td&gt; &#xA;   &lt;td&gt;Github personal access token with &lt;code&gt;workflow&lt;/code&gt; permission&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;USER&lt;/td&gt; &#xA;   &lt;td&gt;E5 admin emails line seperated&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PASSWD&lt;/td&gt; &#xA;   &lt;td&gt;E5 admin passwords line seperated&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>