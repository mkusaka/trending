<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-20T01:33:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/minbpe</title>
    <updated>2024-02-20T01:33:52Z</updated>
    <id>tag:github.com,2024-02-20:/karpathy/minbpe</id>
    <link href="https://github.com/karpathy/minbpe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimal, clean, code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;minbpe&lt;/h1&gt; &#xA;&lt;p&gt;Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is &#34;byte-level&#34; because it runs on UTF-8 encoded strings.&lt;/p&gt; &#xA;&lt;p&gt;This algorithm was popularized for LLMs by the &lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;GPT-2 paper&lt;/a&gt; and the associated GPT-2 &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;code release&lt;/a&gt; from OpenAI. &lt;a href=&#34;https://arxiv.org/abs/1508.07909&#34;&gt;Sennrich et al. 2015&lt;/a&gt; is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.&lt;/p&gt; &#xA;&lt;p&gt;There are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/base.py&#34;&gt;minbpe/base.py&lt;/a&gt;: Implements the &lt;code&gt;Tokenizer&lt;/code&gt; class, which is the base class. It contains the &lt;code&gt;train&lt;/code&gt;, &lt;code&gt;encode&lt;/code&gt;, and &lt;code&gt;decode&lt;/code&gt; stubs, save/load functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/basic.py&#34;&gt;minbpe/basic.py&lt;/a&gt;: Implements the &lt;code&gt;BasicTokenizer&lt;/code&gt;, the simplest implementation of the BPE algorithm that runs directly on text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/regex.py&#34;&gt;minbpe/regex.py&lt;/a&gt;: Implements the &lt;code&gt;RegexTokenizer&lt;/code&gt; that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4. This class also handles special tokens, if any.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/gpt4.py&#34;&gt;minbpe/gpt4.py&lt;/a&gt;: Implements the &lt;code&gt;GPT4Tokenizer&lt;/code&gt;. This class is a light wrapper around the &lt;code&gt;RegexTokenizer&lt;/code&gt; (2, above) that exactly reproduces the tokenization of GPT-4 in the &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, the script &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/train.py&#34;&gt;train.py&lt;/a&gt; trains the two major tokenizers on the input text &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt&#34;&gt;tests/taylorswift.txt&lt;/a&gt; (this is the Wikipedia entry for her kek) and saves the vocab to disk for visualization. This script runs in about 25 seconds on my (M1) MacBook.&lt;/p&gt; &#xA;&lt;p&gt;All of the files above are very short and thoroughly commented, and also contain a usage example on the bottom of the file.&lt;/p&gt; &#xA;&lt;h2&gt;quick start&lt;/h2&gt; &#xA;&lt;p&gt;As the simplest example, we can reproduce the &lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34;&gt;Wikipedia article on BPE&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import BasicTokenizer&#xA;tokenizer = BasicTokenizer()&#xA;text = &#34;aaabdaaabac&#34;&#xA;tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges&#xA;print(tokenizer.encode(text))&#xA;# [258, 100, 258, 97, 99]&#xA;print(tokenizer.decode([258, 100, 258, 97, 99]))&#xA;# aaabdaaabac&#xA;tokenizer.save(&#34;toy&#34;)&#xA;# writes two files: toy.model (for loading) and toy.vocab (for viewing)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;According to Wikipedia, running bpe on the input string: &#34;aaabdaaabac&#34; for 3 merges results in the string: &#34;XdXac&#34; where X=ZY, Y=ab, and Z=aa. The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their &lt;a href=&#34;https://www.asciitable.com&#34;&gt;ASCII&lt;/a&gt; values). Then when (a,a) is merged to Z, Z will become 256. Likewise Y will become 257 and X 258. So we start with the 256 bytes, and do 3 merges to get to the result above, with the expected output of [258, 100, 258, 97, 99].&lt;/p&gt; &#xA;&lt;h2&gt;inference: GPT-4 comparison&lt;/h2&gt; &#xA;&lt;p&gt;We can verify that the &lt;code&gt;RegexTokenizer&lt;/code&gt; has feature parity with the GPT-4 tokenizer from &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &#34;hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ðŸ˜‰&#34;&#xA;&#xA;# tiktoken&#xA;import tiktoken&#xA;enc = tiktoken.get_encoding(&#34;cl100k_base&#34;)&#xA;print(enc.encode(text))&#xA;# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]&#xA;&#xA;# ours&#xA;from minbpe import GPT4Tokenizer&#xA;tokenizer = GPT4Tokenizer()&#xA;print(tokenizer.encode(text))&#xA;# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(you&#39;ll have to &lt;code&gt;pip install tiktoken&lt;/code&gt; to run). Under the hood, the &lt;code&gt;GPT4Tokenizer&lt;/code&gt; is just a light wrapper around &lt;code&gt;RegexTokenizer&lt;/code&gt;, passing in the merges and the special tokens of GPT-4. We can also ensure the special tokens are handled correctly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &#34;&amp;lt;|endoftext|&amp;gt;hello world&#34;&#xA;&#xA;# tiktoken&#xA;import tiktoken&#xA;enc = tiktoken.get_encoding(&#34;cl100k_base&#34;)&#xA;print(enc.encode(text, allowed_special=&#34;all&#34;))&#xA;# [100257, 15339, 1917]&#xA;&#xA;# ours&#xA;from minbpe import GPT4Tokenizer&#xA;tokenizer = GPT4Tokenizer()&#xA;print(tokenizer.encode(text, allowed_special=&#34;all&#34;))&#xA;# [100257, 15339, 1917]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that just like tiktoken, we have to explicitly declare our intent to use and parse special tokens in the call to encode. Otherwise this can become a major footgun, unintentionally tokenizing attacker-controlled data (e.g. user prompts) with special tokens. The &lt;code&gt;allowed_special&lt;/code&gt; parameter can be set to &#34;all&#34;, &#34;none&#34;, or a list of special tokens to allow.&lt;/p&gt; &#xA;&lt;h2&gt;training&lt;/h2&gt; &#xA;&lt;p&gt;Unlike tiktoken, this code allows you to train your own tokenizer. In principle and to my knowledge, if you train the &lt;code&gt;RegexTokenizer&lt;/code&gt; on a large dataset with a vocabulary size of 100K, you would reproduce the GPT-4 tokenizer.&lt;/p&gt; &#xA;&lt;p&gt;There are two paths you can follow. First, you can decide that you don&#39;t want the complexity of splitting and preprocessing text with regex patterns, and you also don&#39;t care for special tokens. In that case, reach for the &lt;code&gt;BasicTokenizer&lt;/code&gt;. You can train it, and then encode and decode for example as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import BasicTokenizer&#xA;tokenizer = BasicTokenizer()&#xA;tokenizer.train(very_long_training_string, vocab_size=4096)&#xA;tokenizer.encode(&#34;hello world&#34;) # string -&amp;gt; tokens&#xA;tokenizer.decode([1000, 2000, 3000]) # tokens -&amp;gt; string&#xA;tokenizer.save(&#34;mymodel&#34;) # writes mymodel.model and mymodel.vocab&#xA;tokenizer.load(&#34;mymodel.model&#34;) # loads the model back, the vocab is just for vis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you instead want to follow along with OpenAI did for their text tokenizer, it&#39;s a good idea to adopt their approach of using regex pattern to split the text by categories. The GPT-4 pattern is a default with the &lt;code&gt;RegexTokenizer&lt;/code&gt;, so you&#39;d simple do something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import RegexTokenizer&#xA;tokenizer = RegexTokenizer()&#xA;tokenizer.train(very_long_training_string, vocab_size=32768)&#xA;tokenizer.encode(&#34;hello world&#34;) # string -&amp;gt; tokens&#xA;tokenizer.decode([1000, 2000, 3000]) # tokens -&amp;gt; string&#xA;tokenizer.save(&#34;tok32k&#34;) # writes tok32k.model and tok32k.vocab&#xA;tokenizer.load(&#34;tok32k.model&#34;) # loads the model back from disk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where, of course, you&#39;d want to change around the vocabulary size depending on the size of your dataset.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Special tokens&lt;/strong&gt;. Finally, you might wish to add special tokens to your tokenizer. Register these using the &lt;code&gt;register_special_tokens&lt;/code&gt; function. For example if you train with vocab_size of 32768, then the first 256 tokens are raw byte tokens, the next 32768-256 are merge tokens, and after those you can add the special tokens. The last &#34;real&#34; merge token will have id of 32767 (vocab_size - 1), so your first special token should come right after that, with an id of exactly 32768. So:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import RegexTokenizer&#xA;tokenizer = RegexTokenizer()&#xA;tokenizer.train(very_long_training_string, vocab_size=32768)&#xA;tokenizer.register_special_tokens({&#34;&amp;lt;|endoftext|&amp;gt;&#34;: 32768})&#xA;tokenizer.encode(&#34;&amp;lt;|endoftext|&amp;gt;hello world&#34;, allowed_special=&#34;all&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can of course add more tokens after that as well, as you like. Finally, I&#39;d like to stress that I tried hard to keep the code itself clean, readable and hackable. You should not have feel scared to read the code and understand how it works. The tests are also a nice place to look for more usage examples. That reminds me:&lt;/p&gt; &#xA;&lt;h2&gt;tests&lt;/h2&gt; &#xA;&lt;p&gt;We use the pytest library for tests. All of them are located in the &lt;code&gt;tests/&lt;/code&gt; directory. First &lt;code&gt;pip install pytest&lt;/code&gt; if you haven&#39;t already, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pytest -v .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to run the tests. (-v is verbose, slightly prettier).&lt;/p&gt; &#xA;&lt;h2&gt;exercise&lt;/h2&gt; &#xA;&lt;p&gt;For those trying to study BPE, here is the advised progression exercise for how you can build your own minbpe step by step. See &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/exercise.md&#34;&gt;exercise.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;todos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;write a more optimized Python version that could run over large files and big vocabs&lt;/li&gt; &#xA; &lt;li&gt;write an even more optimized C or Rust version (think through)&lt;/li&gt; &#xA; &lt;li&gt;rename GPT4Tokenizer to GPTTokenizer and support GPT-2/GPT-3/GPT-3.5 as well?&lt;/li&gt; &#xA; &lt;li&gt;write a LlamaTokenizer similar to GPT4Tokenizer (i.e. attempt sentencepiece equivalent)&lt;/li&gt; &#xA; &lt;li&gt;video coming soon ;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/magika</title>
    <updated>2024-02-20T01:33:52Z</updated>
    <id>tag:github.com,2024-02-20:/google/magika</id>
    <link href="https://github.com/google/magika" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Detect file content types with deep learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Magika&lt;/h1&gt; &#xA;&lt;p&gt;Magika is a novel AI powered file type detection tool that rely on the recent advance of deep learning to provide accurate detection. Under the hood, Magika employs a custom, highly optimized Keras model that only weighs about 1MB, and enables precise file identification within milliseconds, even when running on a single CPU.&lt;/p&gt; &#xA;&lt;p&gt;In an evaluation with over 1M files and over 100 content types (covering both binary and textual file formats), Magika achieves 99%+ precision and recall. Magika is used at scale to help improve Google usersâ€™ safety by routing Gmail, Drive, and Safe Browsing files to the proper security and content policy scanners.&lt;/p&gt; &#xA;&lt;p&gt;You can try Magika without anything by using our &lt;a href=&#34;https://google.github.io/magika/&#34;&gt;web demo&lt;/a&gt;, which runs locally in your browser!&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of what Magika command line output look like:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/google/magika/main/assets/magika-screenshot.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;For more context you can read our initial &lt;a href=&#34;https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html&#34;&gt;announcement post on Google&#39;s OSS blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Available as a Python command line, a Python API, and an experimental TFJS version (which powers our &lt;a href=&#34;https://google.github.io/magika/&#34;&gt;web demo&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Trained on a dataset of over 25M files across more than 100 content types.&lt;/li&gt; &#xA; &lt;li&gt;On our evaluation, Magika achieves 99%+ average precision and recall, outperforming existing approaches.&lt;/li&gt; &#xA; &lt;li&gt;More than 100 content types (see &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/docs/supported-content-types-list.md&#34;&gt;full list&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;After the model is loaded (this is a one-off overhead), the inference time is about 5ms per file.&lt;/li&gt; &#xA; &lt;li&gt;Batching: You can pass to the command line and API multiple files at the same time, and Magika will use batching to speed up the inference time. You can invoke Magika with even thousands of files at the same time. You can also use &lt;code&gt;-r&lt;/code&gt; for recursively scanning a directory.&lt;/li&gt; &#xA; &lt;li&gt;Near-constant inference time independently from the file size; Magika only uses a limited subset of the file&#39;s bytes.&lt;/li&gt; &#xA; &lt;li&gt;Magika uses a per-content-type threshold system that determines whether to &#34;trust&#34; the prediction for the model, or whether to return a generic label, such as &#34;Generic text document&#34; or &#34;Unknown binary data&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Support three different prediction modes, which tweak the tolerance to errors: &lt;code&gt;high-confidence&lt;/code&gt;, &lt;code&gt;medium-confidence&lt;/code&gt;, and &lt;code&gt;best-guess&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s open source! (And more is yet to come.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, see the documentation for the &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/docs/python.md&#34;&gt;python package&lt;/a&gt; and for the &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/js/README.md&#34;&gt;js package&lt;/a&gt; (dev &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/docs/js.md&#34;&gt;docs&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#python-command-line&#34;&gt;Python command line&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#python-api&#34;&gt;Python API&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#experimental-tfjs-model--npm-package&#34;&gt;Experimental TFJS model &amp;amp; npm package&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#development-setup&#34;&gt;Development Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#known-limitations--contributing&#34;&gt;Known Limitations &amp;amp; Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#frequently-asked-questions&#34;&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#additional-resources&#34;&gt;Additional Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/#disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Magika is available as &lt;code&gt;magika&lt;/code&gt; on PyPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pip install magika&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;h4&gt;Python command line&lt;/h4&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ magika -r tests_data/&#xA;tests_data/README.md: Markdown document (text)&#xA;tests_data/basic/code.asm: Assembly (code)&#xA;tests_data/basic/code.c: C source (code)&#xA;tests_data/basic/code.css: CSS source (code)&#xA;tests_data/basic/code.js: JavaScript source (code)&#xA;tests_data/basic/code.py: Python source (code)&#xA;tests_data/basic/code.rs: Rust source (code)&#xA;...&#xA;tests_data/mitra/7-zip.7z: 7-zip archive data (archive)&#xA;tests_data/mitra/bmp.bmp: BMP image data (image)&#xA;tests_data/mitra/bzip2.bz2: bzip2 compressed data (archive)&#xA;tests_data/mitra/cab.cab: Microsoft Cabinet archive data (archive)&#xA;tests_data/mitra/elf.elf: ELF executable (executable)&#xA;tests_data/mitra/flac.flac: FLAC audio bitstream data (audio)&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ magika code.py --json&#xA;[&#xA;    {&#xA;        &#34;path&#34;: &#34;code.py&#34;,&#xA;        &#34;dl&#34;: {&#xA;            &#34;ct_label&#34;: &#34;python&#34;,&#xA;            &#34;score&#34;: 0.9940916895866394,&#xA;            &#34;group&#34;: &#34;code&#34;,&#xA;            &#34;mime_type&#34;: &#34;text/x-python&#34;,&#xA;            &#34;magic&#34;: &#34;Python script&#34;,&#xA;            &#34;description&#34;: &#34;Python source&#34;&#xA;        },&#xA;        &#34;output&#34;: {&#xA;            &#34;ct_label&#34;: &#34;python&#34;,&#xA;            &#34;score&#34;: 0.9940916895866394,&#xA;            &#34;group&#34;: &#34;code&#34;,&#xA;            &#34;mime_type&#34;: &#34;text/x-python&#34;,&#xA;            &#34;magic&#34;: &#34;Python script&#34;,&#xA;            &#34;description&#34;: &#34;Python source&#34;&#xA;        }&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cat doc.ini | magika -&#xA;-: INI configuration file (text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-help&#34;&gt;$ magika -h&#xA;Usage: magika [OPTIONS] [FILE]...&#xA;&#xA;  Magika - Determine type of FILEs with deep-learning.&#xA;&#xA;Options:&#xA;  -r, --recursive                 When passing this option, magika scans every&#xA;                                  file within directories, instead of&#xA;                                  outputting &#34;directory&#34;&#xA;  --json                          Output in JSON format.&#xA;  --jsonl                         Output in JSONL format.&#xA;  -i, --mime-type                 Output the MIME type instead of a verbose&#xA;                                  content type description.&#xA;  -l, --label                     Output a simple label instead of a verbose&#xA;                                  content type description. Use --list-output-&#xA;                                  content-types for the list of supported&#xA;                                  output.&#xA;  -c, --compatibility-mode        Compatibility mode: output is as close as&#xA;                                  possible to `file` and colors are disabled.&#xA;  -s, --output-score              Output the prediction score in addition to&#xA;                                  the content type.&#xA;  -m, --prediction-mode [best-guess|medium-confidence|high-confidence]&#xA;  --batch-size INTEGER            How many files to process in one batch.&#xA;  --no-dereference                This option causes symlinks not to be&#xA;                                  followed. By default, symlinks are&#xA;                                  dereferenced.&#xA;  --colors / --no-colors          Enable/disable use of colors.&#xA;  -v, --verbose                   Enable more verbose output.&#xA;  -vv, --debug                    Enable debug logging.&#xA;  --generate-report               Generate report useful when reporting&#xA;                                  feedback.&#xA;  --version                       Print the version and exit.&#xA;  --list-output-content-types     Show a list of supported content types.&#xA;  --model-dir DIRECTORY           Use a custom model.&#xA;  -h, --help                      Show this message and exit.&#xA;&#xA;  Magika version: &#34;0.5.0&#34;&#xA;&#xA;  Default model: &#34;standard_v1&#34;&#xA;&#xA;  Send any feedback to magika-dev@google.com or via GitHub issues.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/docs/python.md&#34;&gt;python documentation&lt;/a&gt; for detailed documentation.&lt;/p&gt; &#xA;&lt;h4&gt;Python API&lt;/h4&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from magika import Magika&#xA;&amp;gt;&amp;gt;&amp;gt; m = Magika()&#xA;&amp;gt;&amp;gt;&amp;gt; res = m.identify_bytes(b&#34;# Example\nThis is an example of markdown!&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; print(res.output.ct_label)&#xA;markdown&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/docs/python.md&#34;&gt;python documentation&lt;/a&gt; for detailed documentation.&lt;/p&gt; &#xA;&lt;h4&gt;Experimental TFJS model &amp;amp; npm package&lt;/h4&gt; &#xA;&lt;p&gt;We also provide Magika as an experimental package for people interested in using in a web app. Note that Magika JS implementation performance is significantly slower and you should expect to spend 100ms+ per file.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/docs/js.md&#34;&gt;js documentation&lt;/a&gt; for the details.&lt;/p&gt; &#xA;&lt;h2&gt;Development Setup&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://python-poetry.org/&#34;&gt;poetry&lt;/a&gt; for development and packaging:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/google/magika&#xA;$ cd magika/python&#xA;$ poetry shell &amp;amp;&amp;amp; poetry install&#xA;$ magika -r ../tests_data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the tests:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd magika/python&#xA;$ poetry shell&#xA;$ pytest tests/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Known Limitations &amp;amp; Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Magika significantly improves over the state of the art, but there&#39;s always room for improvement! More work can be done to increase detection accuracy, support for additional content types, bindings for more languages, etc.&lt;/p&gt; &#xA;&lt;p&gt;This initial release is not targeting polyglot detection, and we&#39;re looking forward to seeing adversarial examples from the community. We would also love to hear from the community about encountered problems, misdetections, features requests, need for support for additional content types, etc.&lt;/p&gt; &#xA;&lt;p&gt;Check our open GitHub issues to see what is on our roadmap and please report misdetections or feature requests by either opening GitHub issues (preferred) or by emailing us at &lt;a href=&#34;mailto:magika-dev@google.com&#34;&gt;magika-dev@google.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When reporting misdetections, you may want to use &lt;code&gt;$ magika --generate-report &amp;lt;path&amp;gt;&lt;/code&gt; to generate a report with debug information, which you can include in your github issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE: Do NOT send reports about files that may contain PII, the report contains (a small) part of the file content!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;p&gt;We have collected a number of FAQs &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/docs/faq.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Additional Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html&#34;&gt;Google&#39;s OSS blog post&lt;/a&gt; about Magika announcement.&lt;/li&gt; &#xA; &lt;li&gt;Web demo: &lt;a href=&#34;https://google.github.io/magika/&#34;&gt;web demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this software for your research, please cite it as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{magika,&#xA;author = {Fratantonio, Yanick and Bursztein, Elie and Invernizzi, Luca and Zhang, Marina and Metitieri, Giancarlo and Kurt, Thomas and Galilee, Francois and Petit-Bianco, Alexandre and Farah, Loua and Albertini, Ange},&#xA;title = {{Magika content-type scanner}},&#xA;url = {https://github.com/google/magika}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache 2.0; see &lt;a href=&#34;https://raw.githubusercontent.com/google/magika/main/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project is not an official Google project. It is not supported by Google and Google specifically disclaims all warranties as to its quality, merchantability, or fitness for a particular purpose.&lt;/p&gt;</summary>
  </entry>
</feed>