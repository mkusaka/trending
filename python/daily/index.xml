<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-02T01:30:27Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ymcui/Chinese-LLaMA-Alpaca-3</title>
    <updated>2024-05-02T01:30:27Z</updated>
    <id>tag:github.com,2024-05-02:/ymcui/Chinese-LLaMA-Alpaca-3</id>
    <link href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;中文羊驼大模型三期项目 (Chinese Llama-3 LLMs) developed from Meta Llama 3&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/README.md&#34;&gt;&lt;strong&gt;🇨🇳中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/README_EN.md&#34;&gt;&lt;strong&gt;🌐English&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki&#34;&gt;&lt;strong&gt;📖文档/Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/issues&#34;&gt;&lt;strong&gt;❓提问/Issues&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/discussions&#34;&gt;&lt;strong&gt;💬讨论/Discussions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;http://llm-arena.ymcui.com/&#34;&gt;&lt;strong&gt;⚔️竞技场/Arena&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/pics/banner.png&#34; width=&#34;800&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca-3.svg?color=blue&amp;amp;style=flat-square&#34;&gt; &lt;img alt=&#34;GitHub release (latest by date)&#34; src=&#34;https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca-3&#34;&gt; &lt;img alt=&#34;GitHub top language&#34; src=&#34;https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca-3&#34;&gt; &lt;a href=&#34;https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca-3/dashboard?utm_source=gh&amp;amp;utm_medium=referral&amp;amp;utm_content=&amp;amp;utm_campaign=Badge_grade&#34;&gt;&lt;img src=&#34;https://app.codacy.com/project/badge/Grade/142d688425494644b5b156068f55370d&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;本项目基于Meta最新发布的新一代开源大模型&lt;a href=&#34;https://github.com/facebookresearch/llama3&#34;&gt;Llama-3&lt;/a&gt;开发，是Chinese-LLaMA-Alpaca开源大模型相关系列项目（&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;一期&lt;/a&gt;、&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;二期&lt;/a&gt;）的第三期。本项目开源了&lt;strong&gt;中文Llama-3基座模型和中文Llama-3-Instruct指令精调大模型&lt;/strong&gt;。这些模型在原版Llama-3的基础上使用了大规模中文数据进行增量预训练，并且使用精选指令数据进行精调，进一步提升了中文基础语义和指令理解能力，相比二代相关模型获得了显著性能提升。&lt;/p&gt; &#xA;&lt;h4&gt;主要内容&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🚀 开源Llama-3-Chinese基座模型和Llama-3-Chinese-Instruct指令模型&lt;/li&gt; &#xA; &lt;li&gt;🚀 开源了预训练脚本、指令精调脚本，用户可根据需要进一步训练或微调模型&lt;/li&gt; &#xA; &lt;li&gt;🚀 开源了alpaca_zh_51k, stem_zh_instruction, ruozhiba_gpt4_turbo指令精调数据&lt;/li&gt; &#xA; &lt;li&gt;🚀 提供了利用个人电脑CPU/GPU快速在本地进行大模型量化和部署的教程&lt;/li&gt; &#xA; &lt;li&gt;🚀 支持&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;🤗transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;text-generation-webui&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;, &lt;a href=&#34;https://ollama.com&#34;&gt;Ollama&lt;/a&gt;等Llama-3生态&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;中文Mixtral大模型&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;中文LLaMA-2&amp;amp;Alpaca-2大模型&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;中文LLaMA&amp;amp;Alpaca大模型&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca&#34;&gt;多模态中文LLaMA&amp;amp;Alpaca大模型&lt;/a&gt; | &lt;a href=&#34;https://github.com/iflytek/VLE&#34;&gt;多模态VLE&lt;/a&gt; | &lt;a href=&#34;https://github.com/iflytek/MiniRBT&#34;&gt;中文MiniRBT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/LERT&#34;&gt;中文LERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/PERT&#34;&gt;中英文PERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/MacBERT&#34;&gt;中文MacBERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-ELECTRA&#34;&gt;中文ELECTRA&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-XLNet&#34;&gt;中文XLNet&lt;/a&gt; | &lt;a href=&#34;https://github.com/ymcui/Chinese-BERT-wwm&#34;&gt;中文BERT&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/TextBrewer&#34;&gt;知识蒸馏工具TextBrewer&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/TextPruner&#34;&gt;模型裁剪工具TextPruner&lt;/a&gt; | &lt;a href=&#34;https://github.com/airaria/GRAIN&#34;&gt;蒸馏裁剪一体化GRAIN&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;新闻&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024/04/30] 发布Llama-3-Chinese-8B基座模型和Llama-3-Chinese-8B-Instruct指令模型。详情查看：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/releases/tag/v1.0&#34;&gt;📚v1.0版本发布日志&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2024/04/19] 🚀 正式启动Chinese-LLaMA-Alpaca-3项目&lt;/p&gt; &#xA;&lt;h2&gt;内容导引&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;章节&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/#%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B&#34;&gt;💁🏻‍♂️模型简介&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;简要介绍本项目相关模型的技术特点&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD&#34;&gt;⏬模型下载&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;中文Llama-3大模型下载地址&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/#%E6%8E%A8%E7%90%86%E4%B8%8E%E9%83%A8%E7%BD%B2&#34;&gt;💻推理与部署&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;介绍了如何对模型进行量化并使用个人电脑部署并体验大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/#%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C&#34;&gt;💯模型效果&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;介绍了模型在部分任务上的效果&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/#%E8%AE%AD%E7%BB%83%E4%B8%8E%E7%B2%BE%E8%B0%83&#34;&gt;📝训练与精调&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;介绍了如何训练和精调中文Llama-3大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98&#34;&gt;❓常见问题&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;一些常见问题的回复&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;模型简介&lt;/h2&gt; &#xA;&lt;p&gt;本项目推出了基于Meta Llama-3的中文开源大模型Llama-3-Chinese以及Llama-3-Chinese-Instruct。主要特点如下：&lt;/p&gt; &#xA;&lt;h4&gt;📖 使用原版Llama-3词表&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama-3相比其前两代显著扩充了词表大小，由32K扩充至128K，并且改为BPE词表&lt;/li&gt; &#xA; &lt;li&gt;初步实验发现Llama-3词表的编码效率与我们扩充词表的&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;中文LLaMA-2&lt;/a&gt;相当，效率约为中文LLaMA-2词表的95%（基于维基百科数据上的编码效率测试）&lt;/li&gt; &#xA; &lt;li&gt;结合我们在&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;中文Mixtral&lt;/a&gt;上的相关经验及实验结论[^1]，我们&lt;strong&gt;并未对词表进行额外扩充&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[^1]: &lt;a href=&#34;https://arxiv.org/abs/2403.01851&#34;&gt;Cui and Yao, 2024. Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;🚄 长上下文长度由二代4K扩展至8K&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama-3将原生上下文窗口长度从4K提升至8K，能够进一步处理更长的上下文信息&lt;/li&gt; &#xA; &lt;li&gt;用户也可通过PI、NTK、YaRN等方法对模型进行长上下文的扩展，以支持更长文本的处理&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;⚡ 使用分组查询注意力机制&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama-3采用了Llama-2中大参数量版本应用的分组查询注意力（GQA）机制，能够进一步提升模型的效率&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;🗒 全新的指令模板&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama-3-Instruct采用了全新的指令模板，与Llama-2-chat不兼容，使用时应遵循官方指令模板（见&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/#%E6%8C%87%E4%BB%A4%E6%A8%A1%E6%9D%BF&#34;&gt;指令模板&lt;/a&gt;）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型下载&lt;/h2&gt; &#xA;&lt;h3&gt;模型选择指引&lt;/h3&gt; &#xA;&lt;p&gt;以下是本项目的模型对比以及建议使用场景。&lt;strong&gt;如需聊天交互，请选择Instruct版。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;对比项&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Llama-3-Chinese&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Llama-3-Chinese-Instruct&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型类型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令/Chat模型（类ChatGPT）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;模型大小&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;训练类型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Causal-LM (CLM)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令精调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;训练方式&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA + 全量emb/lm-head&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA + 全量emb/lm-head&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;初始化模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;原版Meta-Llama-3-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;中文Llama-3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;训练语料&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;无标注通用语料（约120GB）&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;有标注指令数据（约500万条）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;词表大小&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;原版词表（128,256）&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;原版词表（128,256）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;支持上下文长度&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8K&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;输入模板&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;不需要&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;需要套用Llama-3-Instruct模板&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;适用场景&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;文本续写：给定上文，让模型生成下文&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令理解：问答、写作、聊天、交互等&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;下载地址&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型名称&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;完整版&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LoRA版&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GGUF版&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Llama-3-Chinese-8B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hfl/llama-3-chinese-8b&#34;&gt;[🤗Hugging Face]&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://modelscope.cn/models/ChineseAlpacaGroup/llama-3-chinese-8b&#34;&gt;[🤖ModelScope]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hfl/llama-3-chinese-8b-lora&#34;&gt;[🤗Hugging Face]&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://modelscope.cn/models/ChineseAlpacaGroup/llama-3-chinese-8b-lora&#34;&gt;[🤖ModelScope]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hfl/llama-3-chinese-8b-gguf&#34;&gt;[🤗Hugging Face]&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://modelscope.cn/models/ChineseAlpacaGroup/llama-3-chinese-8b-gguf&#34;&gt;[🤖ModelScope]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Llama-3-Chinese-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令模型&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hfl/llama-3-chinese-8b-instruct&#34;&gt;[🤗Hugging Face]&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://modelscope.cn/models/ChineseAlpacaGroup/llama-3-chinese-8b-instruct&#34;&gt;[🤖ModelScope]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hfl/llama-3-chinese-8b-instruct-lora&#34;&gt;[🤗Hugging Face]&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://modelscope.cn/models/ChineseAlpacaGroup/llama-3-chinese-8b-instruct-lora&#34;&gt;[🤖ModelScope]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/hfl/llama-3-chinese-8b-instruct-gguf&#34;&gt;[🤗Hugging Face]&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://modelscope.cn/models/ChineseAlpacaGroup/llama-3-chinese-8b-instruct-gguf&#34;&gt;[🤖ModelScope]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;模型类型说明：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;完整模型&lt;/strong&gt;：可直接用于训练和推理，无需其他合并步骤&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA模型&lt;/strong&gt;：需要与原版&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;Meta-Llama-3-8B&lt;/a&gt;合并才能转为完整版模型，合并方法：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/model_conversion_zh&#34;&gt;&lt;strong&gt;💻 模型合并步骤&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GGUF模型&lt;/strong&gt;：&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;推出的量化格式，适配ollama等常见推理工具，推荐只需要做推理部署的用户下载；模型名后缀为&lt;code&gt;-im&lt;/code&gt;表示使用了importance matrix进行量化，通常具有更低的PPL，建议使用（用法与常规版相同）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] 若无法访问HF，可考虑一些镜像站点（如&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/hf-mirror.com&#34;&gt;hf-mirror.com&lt;/a&gt;），具体方法请自行查找解决。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;推理与部署&lt;/h2&gt; &#xA;&lt;p&gt;本项目中的相关模型主要支持以下量化、推理和部署方式，具体内容请参考对应教程。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;工具&lt;/th&gt; &#xA;   &lt;th&gt;特点&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;量化&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GUI&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;API&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;vLLM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;教程&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;丰富的GGUF量化选项和高效本地推理&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/llamacpp_zh&#34;&gt;[link]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;🤗transformers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;原生transformers推理接口&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/inference_with_transformers_zh&#34;&gt;[link]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;仿OpenAI API调用&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;仿OpenAI API接口的服务器Demo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/openai_api_zh&#34;&gt;[link]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;text-generation-webui&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;前端Web UI界面的部署方式&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/text-generation-webui_zh&#34;&gt;[link]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://lmstudio.ai&#34;&gt;LM Studio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;多平台聊天软件（带界面）&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/lmstudio_zh&#34;&gt;[link]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;本地运行大模型推理&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✅&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❌&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/ollama_zh&#34;&gt;[link]&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;模型效果&lt;/h2&gt; &#xA;&lt;p&gt;为了评测相关模型的效果，本项目分别进行了生成效果评测和客观效果评测（NLU类），从不同角度对大模型进行评估。推荐用户在自己关注的任务上进行测试，选择适配相关任务的模型。&lt;/p&gt; &#xA;&lt;h3&gt;生成效果评测&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本项目仿照&lt;a href=&#34;https://chat.lmsys.org/?arena&#34;&gt;Fastchat Chatbot Arena&lt;/a&gt;推出了模型在线对战平台，可浏览和评测模型回复质量。对战平台提供了胜率、Elo评分等评测指标，并且可以查看两两模型的对战胜率等结果。&lt;strong&gt;⚔️ 模型竞技场：&lt;a href=&#34;http://llm-arena.ymcui.com/&#34;&gt;http://llm-arena.ymcui.com&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;本项目已入驻机器之心SOTA!模型平台，后期将实现在线体验：&lt;a href=&#34;https://sota.jiqizhixin.com/project/chinese-llama-alpaca-3&#34;&gt;https://sota.jiqizhixin.com/project/chinese-llama-alpaca-3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;客观效果评测&lt;/h3&gt; &#xA;&lt;h4&gt;C-Eval&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cevalbenchmark.com&#34;&gt;C-Eval&lt;/a&gt;是一个全面的中文基础模型评估套件，其中验证集和测试集分别包含1.3K和12.3K个选择题，涵盖52个学科。C-Eval推理代码请参考本项目：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/ceval_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数量&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Valid (0-shot)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Valid (5-shot)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (0-shot)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (5-shot)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;Llama-3-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-Alpaca-2-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-LLaMA-2-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;CMMLU&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/haonan-li/CMMLU&#34;&gt;CMMLU&lt;/a&gt;是另一个综合性中文评测数据集，专门用于评估语言模型在中文语境下的知识和推理能力，涵盖了从基础学科到高级专业水平的67个主题，共计11.5K个选择题。CMMLU推理代码请参考本项目：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/cmmlu_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数量&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (0-shot)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (5-shot)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;Llama-3-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-Alpaca-2-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-LLaMA-2-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;MMLU&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hendrycks/test&#34;&gt;MMLU&lt;/a&gt;是一个用于评测自然语言理解能力的英文评测数据集，是当今用于评测大模型能力的主要数据集之一，其中验证集和测试集分别包含1.5K和14.1K个选择题，涵盖57个学科。MMLU推理代码请参考本项目：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/mmlu_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数量&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Valid (0-shot)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Valid (5-shot)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (0-shot)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test (5-shot)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;Llama-3-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-Alpaca-2-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;指令&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-LLaMA-2-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;基座&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;LongBench&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/THUDM/LongBench&#34;&gt;LongBench&lt;/a&gt;是一个大模型长文本理解能力的评测基准，由6大类、20个不同的任务组成，多数任务的平均长度在5K-15K之间，共包含约4.75K条测试数据。以下是本项目模型在该中文任务（含代码任务）上的评测效果。LongBench推理代码请参考本项目：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/longbench_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数量&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;单文档QA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;多文档QA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;摘要&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FS学习&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;代码&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;合成&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;平均&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Llama-3-Chinese-8B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;Llama-3-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-Mixtral&#34;&gt;Chinese-Mixtral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-Alpaca-2-13B-16K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-LLaMA-2-13B-16K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-Alpaca-2-7B-64K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-LLaMA-2-7B-64K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;量化效果评测&lt;/h3&gt; &#xA;&lt;p&gt;在llama.cpp下，测试了Llama-3-Chinese-8B（基座模型）的量化性能，如下表所示。实测速度相比二代Llama-2-7B略慢。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;F16&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q8_0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q6_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q5_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q5_0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q4_0&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q3_K&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Q2_K&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Size (GB)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.97&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.95&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.34&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.21&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.58&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.34&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3.74&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2.96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;BPW&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;16.00&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;8.50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.56&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.70&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.57&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.89&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.64&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;4.00&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3.16&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PPL&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.130&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.135&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.148&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.181&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.222&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.312&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.549&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.755&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;11.859&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PP Speed&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;5.99&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.10&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.17&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7.34&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.65&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.38&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.00&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.85&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;6.43&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;TG Speed&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;44.03&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;26.08&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21.61&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;22.33&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;20.93&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.93&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;17.09&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;22.50&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;19.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;模型大小：单位GB&lt;/li&gt; &#xA;  &lt;li&gt;BPW（Bits-Per-Weight）：单位参数比特，例如Q8_0实际平均精度为8.50&lt;/li&gt; &#xA;  &lt;li&gt;PPL（困惑度）：以8K上下文测量（原生支持长度），数值越低越好&lt;/li&gt; &#xA;  &lt;li&gt;PP/TG速度：提供了Apple M3 Max（Metal）的指令处理（PP）和文本生成（TG）速度，单位ms/token，数值越低越快&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;训练与精调&lt;/h2&gt; &#xA;&lt;h3&gt;手动训练与精调&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用无标注数据进行预训练：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/pt_scripts_zh&#34;&gt;📖预训练脚本Wiki&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;使用有标注数据进行指令精调：&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/sft_scripts_zh&#34;&gt;📖指令精调脚本Wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;指令模板&lt;/h3&gt; &#xA;&lt;p&gt;本项目Llama-3-Chinese-Instruct沿用原版Llama-3-Instruct的指令模板。以下是一组对话示例：&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&amp;lt;|begin_of_text|&amp;gt;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;You are a helpful assistant. 你是一个乐于助人的助手。&lt;strong&gt;&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;你好**&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;**&lt;/p&gt; &#xA; &lt;p&gt;你好！有什么可以帮助你的吗？&lt;strong&gt;&amp;lt;|eot_id|&amp;gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;指令数据&lt;/h3&gt; &#xA;&lt;p&gt;以下是本项目开源的部分指令数据。详情请查看：&lt;a href=&#34;https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/data&#34;&gt;📚 指令数据&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;数据名称&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;说明&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;数量&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/hfl/alpaca_zh_51k&#34;&gt;alpaca_zh_51k&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;使用gpt-3.5翻译的Alpaca数据&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51K&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/hfl/stem_zh_instruction&#34;&gt;stem_zh_instruction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;使用gpt-3.5爬取的STEM数据，包含物理、化学、医学、生物学、地球科学&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256K&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo&#34;&gt;ruozhiba_gpt4_turbo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;使用&lt;code&gt;gpt-4-turbo-2024-04-09&lt;/code&gt;获取的ruozhiba问答数据&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2449&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;p&gt;请在提交Issue前务必先查看FAQ中是否已存在解决方案。具体问题和解答请参考本项目 &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/wiki/faq_zh&#34;&gt;📖GitHub Wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;问题1：为什么没有像一期、二期项目一样做词表扩充？&#xA;问题2：会有70B版本发布吗？&#xA;问题3：为什么指令模型不叫Alpaca了？&#xA;问题4：本仓库模型能否商用？&#xA;问题5：为什么不对模型做全量预训练而是用LoRA？&#xA;问题6：为什么Llama-3-Chinese对话效果不好？&#xA;问题7：为什么指令模型会回复说自己是ChatGPT？&#xA;问题8：为什么没有在Meta-Llama-3-Instruct上训练？&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于由Meta发布的Llama-3模型进行开发，使用过程中请严格遵守Llama-3的&lt;a href=&#34;https://github.com/meta-llama/llama3/raw/main/LICENSE&#34;&gt;开源许可协议&lt;/a&gt;。如果涉及使用第三方代码，请务必遵从相关的开源许可协议。模型生成的内容可能会因为计算方法、随机因素以及量化精度损失等影响其准确性，因此，本项目不对模型输出的准确性提供任何保证，也不会对任何因使用相关资源和输出结果产生的损失承担责任。如果将本项目的相关模型用于商业用途，开发者应遵守当地的法律法规，确保模型输出内容的合规性，本项目不对任何由此衍生的产品或服务承担责任。&lt;/p&gt; &#xA;&lt;h2&gt;问题反馈&lt;/h2&gt; &#xA;&lt;p&gt;如有疑问，请在GitHub Issue中提交。礼貌地提出问题，构建和谐的讨论社区。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;在提交问题之前，请先查看FAQ能否解决问题，同时建议查阅以往的issue是否能解决你的问题。&lt;/li&gt; &#xA; &lt;li&gt;提交问题请使用本项目设置的Issue模板，以帮助快速定位具体问题。&lt;/li&gt; &#xA; &lt;li&gt;重复以及与本项目无关的issue会被&lt;a href=&#34;https://github.com/marketplace/stale&#34;&gt;stable-bot&lt;/a&gt;处理，敬请谅解。&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>dusty-nv/jetson-containers</title>
    <updated>2024-05-02T01:30:27Z</updated>
    <id>tag:github.com,2024-05-02:/dusty-nv/jetson-containers</id>
    <link href="https://github.com/dusty-nv/jetson-containers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Machine Learning Containers for NVIDIA Jetson and JetPack-L4T&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://www.jetson-ai-lab.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/header_blueprint_rainbow.jpg&#34; alt=&#34;a header for a software project about building containers for AI and machine learning&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Machine Learning Containers for Jetson and JetPack&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-pytorch_jp51.yml?label=l4t-pytorch&#34; alt=&#34;l4t-pytorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-tensorflow&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-tensorflow-tf2_jp51.yml?label=l4t-tensorflow&#34; alt=&#34;l4t-tensorflow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-ml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-ml_jp51.yml?label=l4t-ml&#34; alt=&#34;l4t-ml&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-diffusion_jp51.yml?label=l4t-diffusion&#34; alt=&#34;l4t-diffusion&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-text-generation&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/dusty-nv/jetson-containers/l4t-text-generation_jp60.yml?label=l4t-text-generation&#34; alt=&#34;l4t-text-generation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Modular container build system that provides various &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages&#34;&gt;&lt;strong&gt;AI/ML packages&lt;/strong&gt;&lt;/a&gt; for &lt;a href=&#34;https://developer.nvidia.com/embedded-computing&#34;&gt;NVIDIA Jetson&lt;/a&gt; &lt;span&gt;🚀&lt;/span&gt;&lt;span&gt;🤖&lt;/span&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;ML&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/pytorch&#34;&gt;&lt;code&gt;pytorch&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/tensorflow&#34;&gt;&lt;code&gt;tensorflow&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/onnxruntime&#34;&gt;&lt;code&gt;onnxruntime&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/deepstream&#34;&gt;&lt;code&gt;deepstream&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/jupyterlab&#34;&gt;&lt;code&gt;jupyterlab&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/diffusion/stable-diffusion-webui&#34;&gt;&lt;code&gt;stable-diffusion&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;LLM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/nano_llm&#34;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/transformers&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/text-generation-webui&#34;&gt;&lt;code&gt;text-generation-webui&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/ollama&#34;&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/llama_cpp&#34;&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/exllama&#34;&gt;&lt;code&gt;exllama&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/llava&#34;&gt;&lt;code&gt;llava&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/awq&#34;&gt;&lt;code&gt;awq&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/auto_gptq&#34;&gt;&lt;code&gt;AutoGPTQ&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/mlc&#34;&gt;&lt;code&gt;MLC&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/llm/optimum&#34;&gt;&lt;code&gt;optimum&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/nemo&#34;&gt;&lt;code&gt;nemo&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;L4T&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&#34;&gt;&lt;code&gt;l4t-pytorch&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-tensorflow&#34;&gt;&lt;code&gt;l4t-tensorflow&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-ml&#34;&gt;&lt;code&gt;l4t-ml&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-diffusion&#34;&gt;&lt;code&gt;l4t-diffusion&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-text-generation&#34;&gt;&lt;code&gt;l4t-text-generation&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;VIT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/nanoowl&#34;&gt;&lt;code&gt;NanoOWL&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/nanosam&#34;&gt;&lt;code&gt;NanoSAM&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/sam&#34;&gt;&lt;code&gt;Segment Anything (SAM)&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/tam&#34;&gt;&lt;code&gt;Track Anything (TAM)&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CUDA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/cuda/cupy&#34;&gt;&lt;code&gt;cupy&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/cuda/cuda-python&#34;&gt;&lt;code&gt;cuda-python&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/cuda/pycuda&#34;&gt;&lt;code&gt;pycuda&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/numba&#34;&gt;&lt;code&gt;numba&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rapids/cudf&#34;&gt;&lt;code&gt;cudf&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rapids/cuml&#34;&gt;&lt;code&gt;cuml&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Robotics&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ros&#34;&gt;&lt;code&gt;ros&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/ros&#34;&gt;&lt;code&gt;ros2&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/opencv&#34;&gt;&lt;code&gt;opencv:cuda&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/realsense&#34;&gt;&lt;code&gt;realsense&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/zed&#34;&gt;&lt;code&gt;zed&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;RAG&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rag/llama-index&#34;&gt;&lt;code&gt;llama-index&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rag/langchain&#34;&gt;&lt;code&gt;langchain&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vectordb/nanodb&#34;&gt;&lt;code&gt;NanoDB&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vectordb/faiss&#34;&gt;&lt;code&gt;FAISS&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/rapids/raft&#34;&gt;&lt;code&gt;RAFT&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Audio&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/audio/whisper&#34;&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/audio/whisperx&#34;&gt;&lt;code&gt;whisperX&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/audio/piper-tts&#34;&gt;&lt;code&gt;piper&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/audio/riva-client&#34;&gt;&lt;code&gt;riva&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/audio/xtts&#34;&gt;&lt;code&gt;XTTS&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/audio/audiocraft&#34;&gt;&lt;code&gt;audiocraft&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Smart Home&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/homeassistant-core&#34;&gt;&lt;code&gt;homeassistant-core&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/homeassistant-base&#34;&gt;&lt;code&gt;homeassistant-base&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/wyoming/wyoming-whisper&#34;&gt;&lt;code&gt;wyoming-whisper&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/wyoming/openwakeword&#34;&gt;&lt;code&gt;wyoming-openwakeword&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/wyoming/piper&#34;&gt;&lt;code&gt;wyoming-piper&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/smart-home/wyoming/assist-microphone&#34;&gt;&lt;code&gt;wyoming-assist-microphone&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages&#34;&gt;&lt;strong&gt;&lt;code&gt;packages&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; directory for the full list, including pre-built container images for JetPack/L4T.&lt;/p&gt; &#xA;&lt;p&gt;Using the included tools, you can easily combine packages together for building your own containers. Want to run ROS2 with PyTorch and Transformers? No problem - just do the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/setup.md&#34;&gt;system setup&lt;/a&gt;, and build it on your Jetson:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ jetson-containers build --name=my_container pytorch transformers ros:humble-desktop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are shortcuts for running containers too - this will pull or build a &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&#34;&gt;&lt;code&gt;l4t-pytorch&lt;/code&gt;&lt;/a&gt; image that&#39;s compatible:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ jetson-containers run $(autotag l4t-pytorch)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/run.md&#34;&gt;&lt;code&gt;jetson-containers run&lt;/code&gt;&lt;/a&gt; launches &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/run/&#34;&gt;&lt;code&gt;docker run&lt;/code&gt;&lt;/a&gt; with some added defaults (like &lt;code&gt;--runtime nvidia&lt;/code&gt;, mounted &lt;code&gt;/data&lt;/code&gt; cache and devices)&lt;/sup&gt;&lt;br&gt; &lt;sup&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/run.md#autotag&#34;&gt;&lt;code&gt;autotag&lt;/code&gt;&lt;/a&gt; finds a container image that&#39;s compatible with your version of JetPack/L4T - either locally, pulled from a registry, or by building it.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you look at any package&#39;s readme (like &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/l4t/l4t-pytorch&#34;&gt;&lt;code&gt;l4t-pytorch&lt;/code&gt;&lt;/a&gt;), it will have detailed instructions for running it.&lt;/p&gt; &#xA;&lt;h4&gt;Changing CUDA Versions&lt;/h4&gt; &#xA;&lt;p&gt;You can rebuild the container stack for different versions of CUDA by setting the &lt;code&gt;CUDA_VERSION&lt;/code&gt; variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VERSION=12.4 jetson-containers build transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will then go off and either pull or build all the dependencies needed, including PyTorch and other packages that would be time-consuming to compile. There is a &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/build.md#pip-server&#34;&gt;Pip server&lt;/a&gt; that caches the wheels to accelerate builds. You can also request specific versions of cuDNN, TensorRT, Python, and PyTorch with similar environment variables like &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/build.md#changing-versions&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jetson-ai-lab.com&#34;&gt;&lt;img align=&#34;right&#34; width=&#34;200&#34; height=&#34;200&#34; src=&#34;https://nvidia-ai-iot.github.io/jetson-generative-ai-playground/images/JON_Gen-AI-panels.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages&#34;&gt;Package List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/packages.md&#34;&gt;Package Definitions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/setup.md&#34;&gt;System Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/build.md&#34;&gt;Building Containers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/run.md&#34;&gt;Running Containers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out the tutorials at the &lt;a href=&#34;https://www.jetson-ai-lab.com&#34;&gt;&lt;strong&gt;Jetson Generative AI Lab&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/docs/setup.md&#34;&gt;System Setup&lt;/a&gt; page for tips about setting up your Docker daemon and memory/storage tuning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install the container tools&#xA;git clone https://github.com/dusty-nv/jetson-containers&#xA;bash jetson-containers/install.sh&#xA;&#xA;# automatically pull &amp;amp; run any container&#xA;jetson-containers run $(autotag l4t-pytorch)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can manually run a &lt;a href=&#34;https://hub.docker.com/r/dustynv&#34;&gt;container image&lt;/a&gt; of your choice without using the helper scripts above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo docker run --runtime nvidia -it --rm --network=host dustynv/l4t-pytorch:r36.2.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Looking for the old jetson-containers? See the &lt;a href=&#34;https://github.com/dusty-nv/jetson-containers/tree/legacy&#34;&gt;&lt;code&gt;legacy&lt;/code&gt;&lt;/a&gt; branch.&lt;/p&gt; &#xA;&lt;h2&gt;Gallery&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UOjqF3YCGkY&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/llamaspeak_llava_clip.gif&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=9ObzbbBTbcc&#34;&gt;Multimodal Voice Chat with LLaVA-1.5 13B on NVIDIA Jetson AGX Orin&lt;/a&gt; (container: &lt;a href=&#34;https://dusty-nv.github.io/NanoLLM/&#34;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hswNSZTvEFE&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/llamaspeak_70b_yt.jpg&#34; width=&#34;800px&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=wzLHAgDxMjQ&#34;&gt;Interactive Voice Chat with Llama-2-70B on NVIDIA Jetson AGX Orin&lt;/a&gt; (container: &lt;a href=&#34;https://dusty-nv.github.io/NanoLLM/&#34;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OJT-Ax0CkhU&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/nanodb_tennis.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=wzLHAgDxMjQ&#34;&gt;Realtime Multimodal VectorDB on NVIDIA Jetson&lt;/a&gt; (container: &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vectordb/nanodb&#34;&gt;&lt;code&gt;nanodb&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jetson-ai-lab.com/tutorial_nanoowl.html&#34;&gt;&lt;img src=&#34;https://github.com/NVIDIA-AI-IOT/nanoowl/raw/main/assets/jetson_person_2x.gif&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.jetson-ai-lab.com/tutorial_nanoowl.html&#34;&gt;NanoOWL - Open Vocabulary Object Detection ViT&lt;/a&gt; (container: &lt;a href=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/packages/vit/nanoowl&#34;&gt;&lt;code&gt;nanoowl&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=w48i8FmVvLA&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava.gif&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://youtu.be/X-OXxPiUTuU&#34;&gt;Live Llava on Jetson AGX Orin&lt;/a&gt; (container: &lt;a href=&#34;https://dusty-nv.github.io/NanoLLM/&#34;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=wZq7ynbgRoE&#34;&gt;&lt;img width=&#34;640px&#34; src=&#34;https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava_bear.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://youtu.be/X-OXxPiUTuU&#34;&gt;Live Llava 2.0 - VILA + Multimodal NanoDB on Jetson Orin&lt;/a&gt; (container: &lt;a href=&#34;https://dusty-nv.github.io/NanoLLM/&#34;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jetson-ai-lab.com/tutorial_slm.html&#34;&gt;&lt;img src=&#34;https://www.jetson-ai-lab.com/images/slm_console.gif&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.jetson-ai-lab.com/tutorial_slm.html&#34;&gt;Small Language Models (SLM) on Jetson Orin Nano&lt;/a&gt; (container: &lt;a href=&#34;https://dusty-nv.github.io/NanoLLM/&#34;&gt;&lt;code&gt;NanoLLM&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>magic-research/PLLaVA</title>
    <updated>2024-05-02T01:30:27Z</updated>
    <id>tag:github.com,2024-05-02:/magic-research/PLLaVA</id>
    <link href="https://github.com/magic-research/PLLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository for the paper PLLaVA&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;&lt;a href=&#34;https://pllava.github.io/&#34;&gt;PLLaVA: Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=_Gu69coAAAAJ&#34;&gt;Lin Xu&lt;/a&gt;, &lt;a href=&#34;https://ermu2001.github.io/me.io/&#34;&gt;Yilin Zhao&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=DdCAbWwAAAAJ&#34;&gt;Daquan Zhou&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=xXMj6_EAAAAJ&#34;&gt;Zhijie Lin&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=_wsommYAAAAJ&#34;&gt;See-Kiong Ng&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&amp;amp;hl=en&#34;&gt;Jiashi Feng&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- [![Paper](https://img.shields.io/badge/cs.CV-2311.17005-b31b1b?logo=arxiv&amp;logoColor=red)](https://arxiv.org/abs/2311.17005) --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Project Page: &lt;a href=&#34;https://pllava.github.io/&#34;&gt;PLLaVA&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.16994&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2404.16994-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=nAEje8tu18U&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YouTube-Video-red&#34; alt=&#34;YouTube Video&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/ermu2001/pllava-34b&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-sm-dark.svg?sanitize=true&#34; alt=&#34;Model on HF&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/zeroshot-video-question-answer-on-activitynet?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/zeroshot-video-question-answer-on-activitynet&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zeroshot-video-question-answer-on-msrvtt-qa?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/zeroshot-video-question-answer-on-msrvtt-qa&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zeroshot-video-question-answer-on-msvd-qa?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/zeroshot-video-question-answer-on-msvd-qa&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-question-answering-on-mvbench?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/video-question-answering-on-mvbench&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zeroshot-video-question-answer-on-tgif-qa?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/zeroshot-video-question-answer-on-tgif-qa&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-based-generative-performance-4?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/video-based-generative-performance-4&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-based-generative-performance-3?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/video-based-generative-performance-3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-based-generative-performance?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/video-based-generative-performance&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-based-generative-performance-2?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/video-based-generative-performance-2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-based-generative-performance-1?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/video-based-generative-performance-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-based-generative-performance-5?p=pllava-parameter-free-llava-extension-from-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pllava-parameter-free-llava-extension-from-1/video-based-generative-performance-5&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://pllava.github.io&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/logo.png&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/magic-research/PLLaVA/assets/55656210/a6619702-12d3-489d-bfcc-0ef7105544b2&#34; width=&#34;100%&#34;&gt; &#xA; &lt;/video&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to PLLAVA!&lt;/p&gt; &#xA;&lt;p&gt;The primary purpose of this repository is to support research and the development of prototype models. It is designed to facilitate ease of experimentation and enable a clear overview of results. Please note that this repo is currently undergoing development and reconstruction.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s important to mention that we have not optimized the response speed of the application or the frontend logic. Our goal is to maintain simplicity, clarity, and ease of development, making it accessible for both researchers and students. If you have suggestions or want to enhance the application&#39;s performance, please feel free to contact us or contribute to the project.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve briefly introduce our work in section &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/#%EF%B8%8F-pllava&#34;&gt;PLLAVA&lt;/a&gt;. For more details, feel free to read our paper. Check out section &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/#hammer-usage&#34;&gt;Usage&lt;/a&gt; to start using this repo. If you felt our works interesting, please star us, your support is all we want. If you find our work helpful, feel free to &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/#page_facing_up-citation&#34;&gt;cite&lt;/a&gt; us directly.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔥&lt;/span&gt; Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024/4/24&lt;/strong&gt;: Release: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We are releasing our code/models/datasets.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🏖️ PLLAVA&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/embed/nAEje8tu18U?si=GXxjgP93j77FzDbw&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/teaser.jpg&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Abstract&lt;/h3&gt; &#xA;&lt;p&gt;Vision-language pre-training (VLP) has significantly elevated performance across a range of vision-language applications. Yet, the pre-training process for video-related tasks demands an exceptionally high degree of computational and data resources. This paper investigates a straightforward, highly efficient, and resource-light approach to adapting an existing image-language pre-training model for video data. Our preliminary experiments reveal that directly fine-tuning pre-trained image-language models with multiple frames on video datasets leads to performance saturation or even a drop in caption-related tasks. Besides, it is also vulnerable to prompts and tends to provide short descriptions. We conducted a deep analysis and observed that the performance saturation and the vulnerability might be related to the dominant patches that exist in some single video patches. We then propose a simple pooling strategy to smooth the feature distribution along the temporal dimension and thus reduce the dominant impacts from some extreme tokens. The new model is termed Pooling LLaVA, or PLLaVA in short. With the proposed pooling strategy, we achieve new state-of-the-art performance on all evaluated datasets. Notably, on the recent popular Video ChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average of five evaluated dimensions, which is the new state-of-the-art score on the leaderboard and is 0.31 higher than the previous SOTA results from GPT4V (IG-VLM). On the latest multi-choice benchmark MVBench, PLLaVA achieves 58.1% accuracy on average across 20 sub-tasks, which is the new state-of-the-art result and is 14.5% higher than GPT4V (IG-VLM).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/module.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h3&gt;SEARCHING FOR OPTIMAL POOLING STRATEGY&lt;/h3&gt; &#xA;&lt;p&gt;There are two dimensions for the pooling strategy: the spatial dimension and the temporal dimension. We empirically found that reducing the spatial dimension with a larger temporal dimension could lead to better model performance, compared to reducing the temporal dimension directly.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/zeroshot.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h3&gt;STATE-OF-THE-ART PERFORMANCE&lt;/h3&gt; &#xA;&lt;p&gt;We compare the performance of PLLAVA with recent popular methods over both question-answer and captioning datasets. The results are shown below.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/performance.png&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔨&lt;/span&gt; Usage&lt;/h2&gt; &#xA;&lt;p&gt;This section provides guidance on how to run, train, and evaluate our models.&lt;/p&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;p&gt;First, you will need to set up the environment and download some pre-trained weights.&lt;/p&gt; &#xA;&lt;p&gt;This repo is built up using &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; for model construction along with &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;accelerate&lt;/a&gt; for distributed training. Follow the instructions to install the needed environment.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Above all, the following environment set up is for python 3.10. If you choose to use conda for environment setup, we recommend creating the virtual environment with:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n pllava python=3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Firstly, install &lt;a href=&#34;https://pytorch.org/&#34;&gt;pytorch&lt;/a&gt; from the official website. The code runs on torch 2.2.1, cu118 or cu122. Select the version that suits your drive version.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch                       2.2.1+cu118&#xA;torchaudio                  2.2.1+cu118&#xA;torchvision                 0.17.1+cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your driver version is higher than cu121, you could probably try installing with the following scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise, you would need to install a torch for your server first, then install the other packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.torch.txt # decide your own requirements, (this is for cu11), or install torch directly following the official website.&#xA;pip install -r requirements.no_torch.txt # install the following&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare the model. We prefer to have huggingface models explicitly downloaded to a MODELS directory. However, if you are familiar with huggingface-hub usage, feel free to organize the model yourself.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python python_scripts/hf.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here are some detailed information of the obtained models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;   &lt;th&gt;Initialized From&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pllava-7b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ermu2001/pllava-7b&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-sm-dark.svg?sanitize=true&#34; alt=&#34;Model on HF&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf/llava-v1.6-vicuna-7b-hf&#34;&gt;llava-hf/llava-v1.6-vicuna-7b-hf · Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pllava-13b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ermu2001/pllava-13b&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-sm-dark.svg?sanitize=true&#34; alt=&#34;Model on HF&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf/llava-v1.6-vicuna-13b-hf&#34;&gt;llava-hf/llava-v1.6-vicuna-13b-hf · Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pllava-34b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ermu2001/pllava-34b&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/model-on-hf-sm-dark.svg?sanitize=true&#34; alt=&#34;Model on HF&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf/llava-v1.6-34b-hf&#34;&gt;llava-hf/llava-v1.6-34b-hf · Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The model directory should look like this, where you would only need the corresponding model&#39;s weights and directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ tree MODELS&#xA;MODELS&#xA;|-- pllava-13b&#xA;|   |-- added_tokens.json&#xA;|   |-- config.json&#xA;|   |-- generation_config.json&#xA;|   |-- model-00001-of-00006.safetensors&#xA;|   |-- model-00002-of-00006.safetensors&#xA;|   |-- model-00003-of-00006.safetensors&#xA;|   |-- model-00004-of-00006.safetensors&#xA;|   |-- model-00005-of-00006.safetensors&#xA;|   |-- model-00006-of-00006.safetensors&#xA;|   |-- model.safetensors.index.json&#xA;|   |-- preprocessor_config.json&#xA;|   |-- processor_config.json&#xA;|   |-- special_tokens_map.json&#xA;|   |-- tokenizer.json&#xA;|   |-- tokenizer.model&#xA;|   `-- tokenizer_config.json&#xA;|-- pllava-34b&#xA;|   |-- added_tokens.json&#xA;|   |-- config.json&#xA;|   |-- generation_config.json&#xA;|   |-- model-00001-of-00015.safetensors&#xA;|   |-- model-00002-of-00015.safetensors&#xA;|   |-- model-00003-of-00015.safetensors&#xA;|   |-- model-00004-of-00015.safetensors&#xA;|   |-- model-00005-of-00015.safetensors&#xA;|   |-- model-00006-of-00015.safetensors&#xA;|   |-- model-00007-of-00015.safetensors&#xA;|   |-- model-00008-of-00015.safetensors&#xA;|   |-- model-00009-of-00015.safetensors&#xA;|   |-- model-00010-of-00015.safetensors&#xA;|   |-- model-00011-of-00015.safetensors&#xA;|   |-- model-00012-of-00015.safetensors&#xA;|   |-- model-00013-of-00015.safetensors&#xA;|   |-- model-00014-of-00015.safetensors&#xA;|   |-- model-00015-of-00015.safetensors&#xA;|   |-- model.safetensors-deprecated&#xA;|   |-- model.safetensors.index.json&#xA;|   |-- preprocessor_config.json&#xA;|   |-- processor_config.json&#xA;|   |-- special_tokens_map.json&#xA;|   |-- tokenizer.json&#xA;|   |-- tokenizer.model&#xA;|   `-- tokenizer_config.json&#xA;|-- pllava-7b&#xA;    |-- added_tokens.json&#xA;    |-- config.json&#xA;    |-- generation_config.json&#xA;    |-- model-00001-of-00003.safetensors&#xA;    |-- model-00002-of-00003.safetensors&#xA;    |-- model-00003-of-00003.safetensors&#xA;    |-- model.safetensors.index.json&#xA;    |-- preprocessor_config.json&#xA;    |-- processor_config.json&#xA;    |-- special_tokens_map.json&#xA;    |-- tokenizer.json&#xA;    |-- tokenizer.model&#xA;    `-- tokenizer_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With the above steps, you should be able to proceed on with the following usages.&lt;/p&gt; &#xA;&lt;h3&gt;Run Application&lt;/h3&gt; &#xA;&lt;p&gt;To run our models, make sure you have downloaded a model pretrained weights from the huggingface spaces. Then, run the following scripts with the corresponding path input. Since we are only training with lora and the projector, the model to be run are determined with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;model_dir&lt;/strong&gt;: model directory, one with config.json as compatible with transformers. This refers to the base model&#39;s directory, for example &#34;llava-hf/llava-v1.6-vicuna-7b-hf&#34;/&#34;ermu2001/pllava-7b&#34;/&#34;MODELS/pllava-7b&#34;. (default to: MODELS/plave-7b)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;weights_dir&lt;/strong&gt;: your weights directory. could be the same as model_dir, but if you have a weights directory for the lora weights, you should set this weights_dir to that directory to load the lora weights. This directory should be local. Also, it would need to contain a config.json file within. (default to: ${model_dir}).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;model_dir=&#34;model directory&#34;&#xA;weights_dir=&#34;weights directory&#34;&#xA;bash scripts/demo.sh ${model_dir} ${weights_dir}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now check out the application demo and try play with PLLAVA!&lt;/p&gt; &#xA;&lt;h3&gt;Train&lt;/h3&gt; &#xA;&lt;p&gt;Follow the following steps to reproduce our results or train your own variant:&lt;/p&gt; &#xA;&lt;h4&gt;1. Data Preparation&lt;/h4&gt; &#xA;&lt;p&gt;To train our model from a starting Image-aligned Vision LLM, you would need to download the data first. Our data set up is mainly based on the original Videochat2&#39;s training data. Check out &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/DATA.md&#34;&gt;Instruction Data&lt;/a&gt; to prepare the instruction training data. Ideally, setting up a root data directory and alter the code &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/tasks/train/instruction_data.py#L6&#34;&gt;here&lt;/a&gt; would accomodate the data for training most smoothly.&lt;/p&gt; &#xA;&lt;h4&gt;2. Start Training&lt;/h4&gt; &#xA;&lt;p&gt;Now you&#39;re only a few step away from starting the training. Follow the instructions:&lt;/p&gt; &#xA;&lt;h5&gt;Setup Accelerator&lt;/h5&gt; &#xA;&lt;p&gt;Customize a accelerate training config. For example, a simple config using multiple gpus with no distribution strategy (only torch DDP) would look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;compute_environment: LOCAL_MACHINE&#xA;debug: false&#xA;distributed_type: MULTI_GPU&#xA;downcast_bf16: &#39;no&#39;&#xA;gpu_ids: all&#xA;machine_rank: 0&#xA;main_training_function: main&#xA;mixed_precision: bf16&#xA;num_machines: 1&#xA;num_processes: 8&#xA;rdzv_backend: static&#xA;same_network: true&#xA;tpu_env: []&#xA;tpu_use_cluster: false&#xA;tpu_use_sudo: false&#xA;use_cpu: false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check out out the &lt;a href=&#34;https://huggingface.co/docs/accelerate/index&#34;&gt;Accelerate&lt;/a&gt; documents for more details.&lt;/p&gt; &#xA;&lt;h5&gt;Overwatch the training configuration&lt;/h5&gt; &#xA;&lt;p&gt;Next, you should go over a basic training configuration of the training process in &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/tasks/train/config_pllava_nframe.py&#34;&gt;here&lt;/a&gt;. Then passing this file as the first arg to the &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/tasks/train/train_pllava_nframe_accel.py&#34;&gt;training script&lt;/a&gt; would utilize every arguments in the file. You can customize some of the hyper parameters for your own training process by passing them in the format of &#34;key&#34; &#34;value&#34; pair in the following arguments. A example training scripts could be find &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/scripts/train_pllava.sh&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We recommand customize a &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/tasks/train/config_pllava_nframe.py&#34;&gt;configuration&lt;/a&gt; to set up a customized training!&lt;/p&gt; &#xA;&lt;p&gt;With the above steps, you would be able to start the training process. The output would be well organized in the output directory, each a qualified model directory to pass in to demo as weights_dir, since we are only saveing the lora weights and projector weights to avoide redundancy.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;This section mainly introduce how to reproduce the evaluation or evaluate your own model.&lt;/p&gt; &#xA;&lt;h4&gt;Set up Evaluation Data&lt;/h4&gt; &#xA;&lt;p&gt;Make sure you set up the &#34;DATAS&#34; directory as in &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/DATA.md&#34;&gt;DATA.md&lt;/a&gt;, then you would be able to run the inference with fortune! The evaluation data directory of DATAS would look like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;DATAS/:&#xA;DATAS/VideoQA:&#xA;DATAS/VideoQA/TGIF_QA:&#xA;                     test_a.json&#xA;                     test_q.json&#xA;DATAS/VideoQA/TGIF_QA/videos:&#xA;                            tumblr_m4387mGrlc1r6m5e8o1_250.gif&#xA;                            ...&#xA;DATAS/VideoQA/TGIF_QA/videos_mp4:&#xA;                                tumblr_m4387mGrlc1r6m5e8o1_250.mp4&#xA;                                ...&#xA;DATAS/VideoQA/TGIF_QA/video_gif:&#xA;                               tumblr_m4387mGrlc1r6m5e8o1_250.gif&#xA;                               ...&#xA;DATAS/VideoQA/MSVD_Zero_Shot_QA:&#xA;                               test_a.json&#xA;                               test_q.json&#xA;DATAS/VideoQA/MSVD_Zero_Shot_QA/videos:&#xA;                                      -4wsuPCjDBc_5_15.avi&#xA;DATAS/VideoQA/MSVD_Zero_Shot_QA/msvd_qa:&#xA;DATAS/VideoQA/ActivityNet:&#xA;                         test_a.json&#xA;                         test_q.json&#xA;DATAS/VideoQA/ActivityNet/all_test:&#xA;                                  v_--tFD65KaK4.mp4&#xA;                                  ...&#xA;DATAS/VideoQA/MSRVTT_Zero_Shot_QA:&#xA;                                 test_a.json&#xA;                                 test_q.json&#xA;DATAS/VideoQA/MSRVTT_Zero_Shot_QA/videos:&#xA;DATAS/VideoQA/MSRVTT_Zero_Shot_QA/videos/all:&#xA;                                            video0.mp4&#xA;                                            ...&#xA;&#xA;DATAS/MVBench:&#xA;             ...&#xA;&#xA;DATAS/Recaption/Inter4K:&#xA;                       annotations.json&#xA;DATAS/Recaption/Inter4K/60fps:&#xA;DATAS/Recaption/Inter4K/60fps/UHD:&#xA;                                 1.mp4&#xA;                                 ...&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Start Evaluate&lt;/h4&gt; &#xA;&lt;p&gt;Once you have construted the evaluation data, you can start the evaluation as in &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/scripts/eval.sh&#34;&gt;here&lt;/a&gt;. This script is for evaluating 7B/13B models. As pllava-34b model uses a slightly different prompting, it is evaluated with this &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/scripts/eval_yiprompt.sh&#34;&gt;script&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/eval.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Same as running the demo, you would need to determine the model_dir and weights_dir to evaluate the model. Feel free to comment out some commands and produce partial evaluation.&lt;/p&gt; &#xA;&lt;h4&gt;Overwatch the Results&lt;/h4&gt; &#xA;&lt;p&gt;The evaluation results would be shown to you with our results gallery demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/gallery.sh &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Feel free to use the compare version to compare differnt models&#39; results or use the single gallery version to check out one model&#39;s results. They are basically the same. Check out this &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/scripts/gallery.sh&#34;&gt;script&lt;/a&gt; for more details&lt;/p&gt; &#xA;&lt;h4&gt;For Captioning and Recaptioning&lt;/h4&gt; &#xA;&lt;p&gt;Follow instructions at &lt;a href=&#34;https://raw.githubusercontent.com/magic-research/PLLaVA/main/DATA.md#extending-reacptioning&#34;&gt;DATA.md&lt;/a&gt; and you can extend the recaptioning data with a few steps.&lt;/p&gt; &#xA;&lt;p&gt;Feel free to point out high quality dataset of videos, we would proceed on doing captioning on those datasets.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;span&gt;📄&lt;/span&gt; Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{xu2024pllava,&#xA;      title={PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning}, &#xA;      author={Lin Xu and Yilin Zhao and Daquan Zhou and Zhijie Lin and See Kiong Ng and Jiashi Feng},&#xA;      year={2024},&#xA;      eprint={2404.16994},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;&lt;span&gt;💫&lt;/span&gt; Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;This code base is mainly built upon &lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2&#34;&gt;Videochat2&lt;/a&gt;. SALUTE.&lt;/p&gt; &#xA;&lt;p&gt;We would also like to recognize and commend the following open source projects, thank you for your great contribution to the open source community:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: Fantastic Open Source Image LLM Model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT/tree/main&#34;&gt;VideoChatGPT&lt;/a&gt;: Great Evaluation Benchmarking Framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA/tree/main/videollava&#34;&gt;VideoLlava&lt;/a&gt;：Video LLM repo with helpful resources.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>