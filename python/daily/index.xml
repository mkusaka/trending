<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-28T01:33:42Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bunkerity/bunkerweb</title>
    <updated>2024-06-28T01:33:42Z</updated>
    <id>tag:github.com,2024-06-28:/bunkerity/bunkerweb</id>
    <link href="https://github.com/bunkerity/bunkerweb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üõ°Ô∏è Make your web services secure by default !&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;BunkerWeb logo&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/misc/logo.png&#34; height=&#34;100&#34; width=&#34;350&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/bunkerity/bunkerweb?label=stable&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/bunkerity/bunkerweb?include_prereleases&amp;amp;label=latest&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/bunkerity/bunkerweb&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/bunkerity/bunkerweb&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/bunkerity/bunkerweb&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/dev.yml?branch=dev&amp;amp;label=CI%2FCD%20dev&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/staging.yml?branch=staging&amp;amp;label=CI%2FCD%20staging&#34;&gt; &lt;a href=&#34;https://www.bestpractices.dev/projects/8001&#34;&gt; &lt;img src=&#34;https://www.bestpractices.dev/projects/8001/badge&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üåê &lt;a href=&#34;https://www.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Website&lt;/a&gt; | ü§ù &lt;a href=&#34;https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Panel&lt;/a&gt; | üìì &lt;a href=&#34;https://docs.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Documentation&lt;/a&gt; | üë®‚Äçüíª &lt;a href=&#34;https://demo.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Demo&lt;/a&gt; | üõ°Ô∏è &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/examples&#34;&gt;Examples&lt;/a&gt; | üí¨ &lt;a href=&#34;https://discord.com/invite/fTf46FmtyD&#34;&gt;Chat&lt;/a&gt; | üìù &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/discussions&#34;&gt;Forum&lt;/a&gt; &lt;br&gt; ‚öôÔ∏è &lt;a href=&#34;https://config.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Configurator&lt;/a&gt; | üó∫Ô∏è &lt;a href=&#34;https://threatmap.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Threatmap&lt;/a&gt; | üîé &lt;a href=&#34;https://forms.gle/e3VgymAteYPnwM1j9&#34;&gt;Feedbacks&lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üõ°Ô∏è Make security by default great again !&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;BunkerWeb&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Overview banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/intro-overview.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;BunkerWeb is a next-generation and open-source Web Application Firewall (WAF).&lt;/p&gt; &#xA;&lt;p&gt;Being a full-featured web server (based on &lt;a href=&#34;https://nginx.org/&#34;&gt;NGINX&lt;/a&gt; under the hood), it will protect your web services to make them &#34;secure by default&#34;. BunkerWeb integrates seamlessly into your existing environments (&lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#linux&#34;&gt;Linux&lt;/a&gt;, &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker&#34;&gt;Docker&lt;/a&gt;, &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#swarm&#34;&gt;Swarm&lt;/a&gt;, &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#kubernetes&#34;&gt;Kubernetes&lt;/a&gt;, ‚Ä¶) and is fully configurable (don&#39;t panic, there is an &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/web-ui/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;awesome web UI&lt;/a&gt; if you don&#39;t like the CLI) to meet your own use-cases . In other words, cybersecurity is no more a hassle.&lt;/p&gt; &#xA;&lt;p&gt;BunkerWeb contains primary &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/security-tuning/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;security features&lt;/a&gt; as part of the core but can be easily extended with additional ones thanks to a &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/plugins/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;plugin system&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why BunkerWeb ?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy integration into existing environments&lt;/strong&gt; : Seamlessly integrate BunkerWeb into various environments such as Linux, Docker, Swarm, Kubernetes and more. Enjoy a smooth transition and hassle-free implementation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Highly customizable&lt;/strong&gt; : Tailor BunkerWeb to your specific requirements with ease. Enable, disable, and configure features effortlessly, allowing you to customize the security settings according to your unique use case.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Secure by default&lt;/strong&gt; : BunkerWeb provides out-of-the-box, hassle-free minimal security for your web services. Experience peace of mind and enhanced protection right from the start.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Awesome web UI&lt;/strong&gt; : Take control of BunkerWeb more efficiently with the exceptional web user interface (UI). Navigate settings and configurations effortlessly through a user-friendly graphical interface, eliminating the need for the command-line interface (CLI).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plugin system&lt;/strong&gt; : Extend the capabilities of BunkerWeb to meet your own use cases. Seamlessly integrate additional security measures and customize the functionality of BunkerWeb according to your specific requirements.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Free as in &#34;freedom&#34;&lt;/strong&gt; : BunkerWeb is licensed under the free &lt;a href=&#34;https://www.gnu.org/licenses/agpl-3.0.en.html&#34;&gt;AGPLv3 license&lt;/a&gt;, embracing the principles of freedom and openness. Enjoy the freedom to use, modify, and distribute the software, backed by a supportive community.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Professional services&lt;/strong&gt; : Get technical support, tailored consulting and custom development directly from the maintainers of BunkerWeb. Visit the &lt;a href=&#34;https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Bunker Panel&lt;/a&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Security features&lt;/h2&gt; &#xA;&lt;p&gt;A non-exhaustive list of security features :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;HTTPS&lt;/strong&gt; support with transparent &lt;strong&gt;Let&#39;s Encrypt&lt;/strong&gt; automation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;State-of-the-art web security&lt;/strong&gt; : HTTP security headers, prevent leaks, TLS hardening, ...&lt;/li&gt; &#xA; &lt;li&gt;Integrated &lt;strong&gt;ModSecurity WAF&lt;/strong&gt; with the &lt;strong&gt;OWASP Core Rule Set&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic ban&lt;/strong&gt; of strange behaviors based on HTTP status code&lt;/li&gt; &#xA; &lt;li&gt;Apply &lt;strong&gt;connections and requests limit&lt;/strong&gt; for clients&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Block bots&lt;/strong&gt; by asking them to solve a &lt;strong&gt;challenge&lt;/strong&gt; (e.g. : cookie, javascript, captcha, hCaptcha or reCAPTCHA)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Block known bad IPs&lt;/strong&gt; with external blacklists and DNSBL&lt;/li&gt; &#xA; &lt;li&gt;And much more ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about the core security features in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/security-tuning/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;security tuning&lt;/a&gt; section of the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=ZhYV-QELzA4&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;BunkerWeb demo&#34; src=&#34;https://img.youtube.com/vi/ZhYV-QELzA4/0.jpg&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;A demo website protected with BunkerWeb is available at &lt;a href=&#34;https://demo.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;demo.bunkerweb.io&lt;/a&gt;. Feel free to visit it and perform some security tests.&lt;/p&gt; &#xA;&lt;h2&gt;BunkerWeb Cloud&lt;/h2&gt; &#xA;&lt;p&gt;Don&#39;t want to self-host and manage your own BunkerWeb instance(s) ? You might be interested into BunkerWeb Cloud, our fully managed SaaS offer for BunkerWeb.&lt;/p&gt; &#xA;&lt;p&gt;Try our &lt;a href=&#34;https://panel.bunkerweb.io/order/bunkerweb-cloud/14?utm_source=github&amp;amp;utm_campaign=self&#34;&gt;BunkerWeb Cloud beta offer for free&lt;/a&gt; and get access to :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fully managed BunkerWeb instance hosted in our cloud&lt;/li&gt; &#xA; &lt;li&gt;All BunkerWeb features including PRO ones&lt;/li&gt; &#xA; &lt;li&gt;Monitoring platform including dashboards and alerts&lt;/li&gt; &#xA; &lt;li&gt;Technical support to assist you in the configuration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will find more information about BunkerWeb Cloud in the &lt;a href=&#34;https://panel.bunkerweb.io/knowledgebase/55/BunkerWeb-Cloud?utm_source=github&amp;amp;utm_campaign=self&#34;&gt;FAQ page&lt;/a&gt; of the BunkerWeb panel.&lt;/p&gt; &#xA;&lt;h2&gt;PRO version&lt;/h2&gt; &#xA;&lt;p&gt;When using BunkerWeb you have the choice of the version you want to use : open-source or PRO.&lt;/p&gt; &#xA;&lt;p&gt;Whether it&#39;s enhanced security, an enriched user experience, or technical supervision, the BunkerWeb PRO version will allow you to fully benefit from BunkerWeb and respond to your professional needs.&lt;/p&gt; &#xA;&lt;p&gt;Be it in the documentation or the user interface, the PRO features are annotated with a crown &lt;img src=&#34;https://docs.bunkerweb.io/1.5.8/assets/img/pro-icon.svg?sanitize=true&#34; alt=&#34;crow pro icon&#34; height=&#34;24px&#34; width=&#34;24px&#34;&gt; to distinguish them from those integrated into the open-source version.&lt;/p&gt; &#xA;&lt;p&gt;You can upgrade from the open-source version to the PRO one easily and at any time you want. The process is pretty straightforward :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Claim your &lt;a href=&#34;https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc&#34;&gt;free trial on the BunkerWeb panel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Once connected to the client area, copy your PRO license key&lt;/li&gt; &#xA; &lt;li&gt;Paste your private key into BunkerWeb using the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/web-ui/#upgrade-to-pro&#34;&gt;web UI&lt;/a&gt; or &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/settings/#pro&#34;&gt;specific setting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Do not hesitate to visit the &lt;a href=&#34;https://panel.bunkerweb.io/knowledgebase?utm_campaign=self&amp;amp;utm_source=doc&#34;&gt;BunkerWeb panel&lt;/a&gt; or &lt;a href=&#34;https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;amp;utm_source=doc&#34;&gt;contact us&lt;/a&gt; if you have any question regarding the PRO version.&lt;/p&gt; &#xA;&lt;h2&gt;Professional services&lt;/h2&gt; &#xA;&lt;p&gt;Get the most of BunkerWeb by getting professional services directly from the maintainers of the project. From technical support to tailored consulting and development, we are here to assist you in the security of your web services.&lt;/p&gt; &#xA;&lt;p&gt;You will find more information by visiting the &lt;a href=&#34;https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=doc&#34;&gt;BunkerWeb Panel&lt;/a&gt;, our dedicated platform for professional services.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t hesitate to &lt;a href=&#34;https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;amp;utm_source=doc&#34;&gt;contact us&lt;/a&gt; if you have any question, we will be more than happy to respond to your needs.&lt;/p&gt; &#xA;&lt;h2&gt;Ecosystem, community and resources&lt;/h2&gt; &#xA;&lt;p&gt;Official websites, tools and resources about BunkerWeb :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; : get more information, news and articles about BunkerWeb&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;&lt;strong&gt;Panel&lt;/strong&gt;&lt;/a&gt; : dedicated platform to order and manage professional services (e.g. technical support) around BunkerWeb&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; : technical documentation of the BunkerWeb solution&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://demo.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/a&gt; : demonstration website of BunkerWeb, don&#39;t hesitate to attempt attacks to test the robustness of the solution&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://config.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;&lt;strong&gt;Configurator&lt;/strong&gt;&lt;/a&gt; : user-friendly tool to help you configure BunkerWeb&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://threatmap.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;&lt;strong&gt;Threatmap&lt;/strong&gt;&lt;/a&gt; : live cyber attack blocked by BunkerWeb instances all around the world&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Community and social networks :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.com/invite/fTf46FmtyD&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/company/bunkerity/&#34;&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/bunkerity&#34;&gt;&lt;strong&gt;Twitter&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/BunkerWeb/&#34;&gt;&lt;strong&gt;Reddit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Concepts&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Concepts banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/concepts.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;You will find more information about the key concepts of BunkerWeb in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/concepts/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;The first concept is the integration of BunkerWeb into the target environment. We prefer to use the word &#34;integration&#34; instead of &#34;installation&#34; because one of the goals of BunkerWeb is to integrate seamlessly into existing environments.&lt;/p&gt; &#xA;&lt;p&gt;The following integrations are officially supported :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#linux&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker-autoconf&#34;&gt;Docker autoconf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#swarm&#34;&gt;Swarm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#microsoft-azure&#34;&gt;Microsoft Azure&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Settings&lt;/h2&gt; &#xA;&lt;p&gt;Once BunkerWeb is integrated into your environment, you will need to configure it to serve and protect your web applications.&lt;/p&gt; &#xA;&lt;p&gt;The configuration of BunkerWeb is done by using what we call the &#34;settings&#34; or &#34;variables&#34;. Each setting is identified by a name such as &lt;code&gt;AUTO_LETS_ENCRYPT&lt;/code&gt; or &lt;code&gt;USE_ANTIBOT&lt;/code&gt;. You can assign values to the settings to configure BunkerWeb.&lt;/p&gt; &#xA;&lt;p&gt;Here is a dummy example of a BunkerWeb configuration :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-conf&#34;&gt;SERVER_NAME=www.example.com&#xA;AUTO_LETS_ENCRYPT=yes&#xA;USE_ANTIBOT=captcha&#xA;REFERRER_POLICY=no-referrer&#xA;USE_MODSECURITY=no&#xA;USE_GZIP=yes&#xA;USE_BROTLI=no&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will find an easy to use settings generator at &lt;a href=&#34;https://config.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;config.bunkerweb.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Multisite mode&lt;/h2&gt; &#xA;&lt;p&gt;The multisite mode is a crucial concept to understand when using BunkerWeb. Because the goal is to protect web applications, we intrinsically inherit the concept of &#34;virtual host&#34; or &#34;vhost&#34; (more info &lt;a href=&#34;https://en.wikipedia.org/wiki/Virtual_hosting&#34;&gt;here&lt;/a&gt;) which makes it possible to serve multiple web applications from a single (or a cluster of) instance.&lt;/p&gt; &#xA;&lt;p&gt;By default, the multisite mode of BunkerWeb is disabled which means that only one web application will be served and all the settings will be applied to it. The typical use case is when you have a single application to protect : you don&#39;t have to worry about the multisite and the default behavior should be the right one for you.&lt;/p&gt; &#xA;&lt;p&gt;When multisite mode is enabled, BunkerWeb will serve and protect multiple web applications. Each web application is identified by a unique server name and have its own set of settings. The typical use case is when you have multiple applications to protect and you want to use a single (or a cluster depending of the integration) instance of BunkerWeb.&lt;/p&gt; &#xA;&lt;h2&gt;Custom configurations&lt;/h2&gt; &#xA;&lt;p&gt;Because meeting all the use cases only using the settings is not an option (even with &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/plugins/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;external plugins&lt;/a&gt;), you can use custom configurations to solve your specific challenges.&lt;/p&gt; &#xA;&lt;p&gt;Under the hood, BunkerWeb uses the notorious NGINX web server, that&#39;s why you can leverage its configuration system for your specific needs. Custom NGINX configurations can be included in different &lt;a href=&#34;https://docs.nginx.com/nginx/admin-guide/basic-functionality/managing-configuration-files/#contexts&#34;&gt;contexts&lt;/a&gt; like HTTP or server (all servers and/or specific server block).&lt;/p&gt; &#xA;&lt;p&gt;Another core component of BunkerWeb is the ModSecurity Web Application Firewall : you can also use custom configurations to fix some false positives or add custom rules for example.&lt;/p&gt; &#xA;&lt;h2&gt;Database&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Database model&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/bunkerweb_db.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;State of the current configuration of BunkerWeb is stored in a backend database which contains the following data :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Settings defined for all the services&lt;/li&gt; &#xA; &lt;li&gt;Custom configurations&lt;/li&gt; &#xA; &lt;li&gt;BunkerWeb instances&lt;/li&gt; &#xA; &lt;li&gt;Metadata about jobs execution&lt;/li&gt; &#xA; &lt;li&gt;Cached files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following backend database are supported : SQLite, MariaDB, MySQL and PostgreSQL&lt;/p&gt; &#xA;&lt;h2&gt;Scheduler&lt;/h2&gt; &#xA;&lt;p&gt;To make things automagically work together, a dedicated service called the scheduler is in charge of :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Storing the settings and custom configurations inside the database&lt;/li&gt; &#xA; &lt;li&gt;Executing various tasks (called jobs)&lt;/li&gt; &#xA; &lt;li&gt;Generating a configuration which is understood by BunkerWeb&lt;/li&gt; &#xA; &lt;li&gt;Being the intermediary for other services (like web UI or autoconf)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In other words, the scheduler is the brain of BunkerWeb.&lt;/p&gt; &#xA;&lt;h1&gt;Setup&lt;/h1&gt; &#xA;&lt;h2&gt;BunkerWeb Cloud&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Docker banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/bunkerweb-cloud.webp&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;BunkerWeb Cloud is the easiest way to get started with BunkerWeb. It offers you a fully managed BunkerWeb service with no hassle. Think of a like a BunkerWeb-as-a-Service !&lt;/p&gt; &#xA;&lt;p&gt;You will find more information about BunkerWeb Cloud beta &lt;a href=&#34;https://www.bunkerweb.io/cloud?utm_campaign=self&amp;amp;utm_source=docs&#34;&gt;here&lt;/a&gt; and you can apply for free &lt;a href=&#34;https://panel.bunkerweb.io/order/bunkerweb-cloud/14?utm_campaign=self&amp;amp;utm_source=docs&#34;&gt;in the BunkerWeb panel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Docker banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/integration-docker.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We provide ready to use prebuilt images for x64, x86, armv7 and arm64 platforms on &lt;a href=&#34;https://hub.docker.com/u/bunkerity&#34;&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Docker integration key concepts are :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Environment variables&lt;/strong&gt; to configure BunkerWeb&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scheduler&lt;/strong&gt; container to store configuration and execute jobs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Networks&lt;/strong&gt; to expose ports for clients and connect to upstream web services&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker&#34;&gt;Docker integration section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Docker autoconf&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Docker autoconf banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/integration-autoconf.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The downside of using environment variables is that the container needs to be recreated each time there is an update which is not very convenient. To counter that issue, you can use another image called &lt;strong&gt;autoconf&lt;/strong&gt; which will listen for Docker events and automatically reconfigure BunkerWeb in real-time without recreating the container.&lt;/p&gt; &#xA;&lt;p&gt;Instead of defining environment variables for the BunkerWeb container, you simply add &lt;strong&gt;labels&lt;/strong&gt; to your web applications containers and the &lt;strong&gt;autoconf&lt;/strong&gt; will &#34;automagically&#34; take care of the rest.&lt;/p&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker-autoconf&#34;&gt;Docker autoconf section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Swarm&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Swarm banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/integration-swarm.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To automatically configure BunkerWeb instances, a special service, called &lt;strong&gt;autoconf&lt;/strong&gt; will listen for Docker Swarm events like service creation or deletion and automatically configure the &lt;strong&gt;BunkerWeb instances&lt;/strong&gt; in real-time without downtime.&lt;/p&gt; &#xA;&lt;p&gt;Like the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#docker-autoconf&#34;&gt;Docker autoconf integration&lt;/a&gt;, configuration for web services is defined using labels starting with the special &lt;strong&gt;bunkerweb.&lt;/strong&gt; prefix.&lt;/p&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#swarm&#34;&gt;Swarm section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Kubernetes banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/integration-kubernetes.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The autoconf acts as an &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress controller&lt;/a&gt; and will configure the BunkerWeb instances according to the &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;Ingress resources&lt;/a&gt;. It also monitors other Kubernetes objects like &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/configmap/&#34;&gt;ConfigMap&lt;/a&gt; for custom configurations.&lt;/p&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#kubernetes&#34;&gt;Kubernetes section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Linux&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Linux banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/integration-linux.svg?sanitize=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;List of supported Linux distros :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Debian 12 &#34;Bookworm&#34;&lt;/li&gt; &#xA; &lt;li&gt;Ubuntu 22.04 &#34;Noble&#34;&lt;/li&gt; &#xA; &lt;li&gt;Ubuntu 24.04 &#34;Jammy&#34;&lt;/li&gt; &#xA; &lt;li&gt;Fedora 40&lt;/li&gt; &#xA; &lt;li&gt;RHEL 8.9&lt;/li&gt; &#xA; &lt;li&gt;RHEL 9.4&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Repositories of Linux packages for BunkerWeb are available on &lt;a href=&#34;https://packagecloud.io/bunkerity/bunkerweb&#34;&gt;PackageCloud&lt;/a&gt;, they provide a bash script to automatically add and trust the repository (but you can also follow the &lt;a href=&#34;https://packagecloud.io/bunkerity/bunkerweb/install&#34;&gt;manual installation&lt;/a&gt; instructions if you prefer).&lt;/p&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#linux&#34;&gt;Linux section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Microsoft Azure&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Azure banner&#34; src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/integration-azure.webp&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;BunkerWeb is referenced in the &lt;a href=&#34;https://azuremarketplace.microsoft.com/fr-fr/marketplace/apps/bunkerity.bunkerweb?tab=Overview&#34;&gt;Azure Marketplace&lt;/a&gt; and a ARM template is available in the &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/misc/integrations/azure-arm-template.json&#34;&gt;misc folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/integrations/?utm_campaign=self&amp;amp;utm_source=github#microsoft-azure&#34;&gt;Microsoft Azure section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h1&gt;Quickstart guide&lt;/h1&gt; &#xA;&lt;p&gt;Once you have setup BunkerWeb with the integration of your choice, you can follow the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/quickstart-guide/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;quickstart guide&lt;/a&gt; that will cover the following common use cases :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Protecting a single HTTP application&lt;/li&gt; &#xA; &lt;li&gt;Protecting multiple HTTP application&lt;/li&gt; &#xA; &lt;li&gt;Retrieving the real IP of clients when operating behind a load balancer&lt;/li&gt; &#xA; &lt;li&gt;Adding custom configurations&lt;/li&gt; &#xA; &lt;li&gt;Protecting generic TCP/UDP applications&lt;/li&gt; &#xA; &lt;li&gt;In combination with PHP&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Security tuning&lt;/h1&gt; &#xA;&lt;p&gt;BunkerWeb offers many security features that you can configure with &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/settings/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;settings&lt;/a&gt;. Even if the default values of settings ensure a minimal &#34;security by default&#34;, we strongly recommend you to tune them. By doing so you will be able to ensure a security level of your choice but also manage false positives.&lt;/p&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/security-tuning/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;security tuning section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h1&gt;Settings&lt;/h1&gt; &#xA;&lt;p&gt;To help you tuning BunkerWeb we have made an easy to use settings generator tool available at &lt;a href=&#34;https://config.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;config.bunkerweb.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As a general rule when multisite mode is enabled, if you want to apply settings with multisite context to a specific server you will need to add the primary (first) server name as a prefix like &lt;code&gt;www.example.com_USE_ANTIBOT=captcha&lt;/code&gt; or &lt;code&gt;myapp.example.com_USE_GZIP=yes&lt;/code&gt; for example.&lt;/p&gt; &#xA;&lt;p&gt;When settings are considered as &#34;multiple&#34;, it means that you can have multiple groups of settings for the same feature by adding numbers as suffix like &lt;code&gt;REVERSE_PROXY_URL_1=/subdir&lt;/code&gt;, &lt;code&gt;REVERSE_PROXY_HOST_1=http://myhost1&lt;/code&gt;, &lt;code&gt;REVERSE_PROXY_URL_2=/anotherdir&lt;/code&gt;, &lt;code&gt;REVERSE_PROXY_HOST_2=http://myhost2&lt;/code&gt;, ... for example.&lt;/p&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/settings/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;settings section&lt;/a&gt; of the documentation to get the full list.&lt;/p&gt; &#xA;&lt;h1&gt;Web UI&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=Ao20SfvQyr4&#34;&gt; &lt;img src=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/docs/assets/img/user_interface_demo.webp&#34; height=&#34;300&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;The &#34;Web UI&#34; is a web application that helps you manage your BunkerWeb instance using a user-friendly interface instead of the command-line one.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start, stop, restart and reload your BunkerWeb instance&lt;/li&gt; &#xA; &lt;li&gt;Add, edit and delete settings for your web applications&lt;/li&gt; &#xA; &lt;li&gt;Add, edit and delete custom configurations for NGINX and ModSecurity&lt;/li&gt; &#xA; &lt;li&gt;Install and uninstall external plugins&lt;/li&gt; &#xA; &lt;li&gt;Explore the cached files&lt;/li&gt; &#xA; &lt;li&gt;Monitor jobs execution&lt;/li&gt; &#xA; &lt;li&gt;View the logs and search pattern&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/web-ui/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;Web UI section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h1&gt;Plugins&lt;/h1&gt; &#xA;&lt;p&gt;BunkerWeb comes with a plugin system to make it possible to easily add new features. Once a plugin is installed, you can manage it using additional settings defined by the plugin.&lt;/p&gt; &#xA;&lt;p&gt;Here is the list of &#34;official&#34; plugins that we maintain (see the &lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;bunkerweb-plugins&lt;/a&gt; repository for more information) :&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;ClamAV&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Automatically scans uploaded files with the ClamAV antivirus engine and denies the request when a file is detected as malicious.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/tree/main/clamav&#34;&gt;bunkerweb-plugins/clamav&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Coraza&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Inspect requests using a the Coraza WAF (alternative of ModSecurity).&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/tree/main/coraza&#34;&gt;bunkerweb-plugins/coraza&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;CrowdSec&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;CrowdSec bouncer for BunkerWeb.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/tree/main/crowdsec&#34;&gt;bunkerweb-plugins/crowdsec&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Send security notifications to a Discord channel using a Webhook.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/tree/main/discord&#34;&gt;bunkerweb-plugins/discord&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Slack&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Send security notifications to a Slack channel using a Webhook.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/tree/main/slack&#34;&gt;bunkerweb-plugins/slack&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;VirusTotal&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Automatically scans uploaded files with the VirusTotal API and denies the request when a file is detected as malicious.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/tree/main/virustotal&#34;&gt;bunkerweb-plugins/virustotal&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;WebHook&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Send security notifications to a custom HTTP endpoint using a Webhook.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/bunkerity/bunkerweb-plugins/tree/main/webhook&#34;&gt;bunkerweb-plugins/slack&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You will find more information in the &lt;a href=&#34;https://docs.bunkerweb.io/1.5.8/plugins/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;plugins section&lt;/a&gt; of the documentation.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;h2&gt;Professional&lt;/h2&gt; &#xA;&lt;p&gt;Get technical support directly from the BunkerWeb maintainers. You will find more information by visiting the &lt;a href=&#34;https://panel.bunkerweb.io/?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;BunkerWeb Panel&lt;/a&gt;, our dedicated platform for professional services.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t hesitate to &lt;a href=&#34;https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;amp;utm_source=github&#34;&gt;contact us&lt;/a&gt; if you have any question, we will be more than happy to respond to your needs.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;To get free community support you can use the following media :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The #help channel of BunkerWeb in the &lt;a href=&#34;https://discord.com/invite/fTf46FmtyD&#34;&gt;Discord server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The help category of &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/discussions&#34;&gt;GitHub discussions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://www.reddit.com/r/BunkerWeb&#34;&gt;/r/BunkerWeb&lt;/a&gt; subreddit&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://serverfault.com/&#34;&gt;Server Fault&lt;/a&gt; and &lt;a href=&#34;https://superuser.com/&#34;&gt;Super User&lt;/a&gt; forums&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please don&#39;t use &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/issues&#34;&gt;GitHub issues&lt;/a&gt; to ask for help, use it only for bug reports and feature requests.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the terms of the &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/LICENSE.md&#34;&gt;GNU Affero General Public License (AGPL) version 3&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contribute&lt;/h1&gt; &#xA;&lt;p&gt;If you would like to contribute to the plugins you can read the &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h1&gt;Security policy&lt;/h1&gt; &#xA;&lt;p&gt;We take security bugs as serious issues and encourage responsible disclosure, see our &lt;a href=&#34;https://github.com/bunkerity/bunkerweb/raw/v1.5.8/SECURITY.md&#34;&gt;security policy&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Stargazers over time&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/bunkerity/bunkerweb&#34;&gt;&lt;img src=&#34;https://starchart.cc/bunkerity/bunkerweb.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>intel-analytics/ipex-llm</title>
    <updated>2024-06-28T01:33:42Z</updated>
    <id>tag:github.com,2024-06-28:/intel-analytics/ipex-llm</id>
    <link href="https://github.com/intel-analytics/ipex-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, Baichuan, Mixtral, Gemma, Phi, etc.) on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, DeepSpeed, vLLM, FastChat, Axolotl, etc.&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;em&gt;&lt;strong&gt;&lt;code&gt;bigdl-llm&lt;/code&gt; has now become &lt;code&gt;ipex-llm&lt;/code&gt; (see the migration guide &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/bigdl_llm_migration.md&#34;&gt;here&lt;/a&gt;); you may find the original &lt;code&gt;BigDL&lt;/code&gt; project &lt;a href=&#34;https://github.com/intel-analytics/BigDL-2.x&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;üí´ Intel¬Æ LLM library for PyTorch*&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;IPEX-LLM&lt;/code&gt;&lt;/strong&gt; is a PyTorch library for running &lt;strong&gt;LLM&lt;/strong&gt; on Intel CPU and GPU &lt;em&gt;(e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max)&lt;/em&gt; with very low latency[^1].&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;em&gt;It is built on top of the excellent work of &lt;strong&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;bitsandbytes&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;vLLM&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;qlora&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;AutoGPTQ&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;AutoAWQ&lt;/code&gt;&lt;/strong&gt;, etc.&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;em&gt;It provides seamless integration with &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/ollama_quickstart.md&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/webui_quickstart.md&#34;&gt;Text-Generation-WebUI&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels&#34;&gt;HuggingFace transformers&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LangChain&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LlamaIndex&#34;&gt;LlamaIndex&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/Deepspeed-AutoTP&#34;&gt;DeepSpeed-AutoTP&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/vLLM_quickstart.md&#34;&gt;vLLM&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/fastchat_quickstart.md&#34;&gt;FastChat&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/axolotl_quickstart.md&#34;&gt;Axolotl&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning&#34;&gt;HuggingFace PEFT&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/DPO&#34;&gt;HuggingFace TRL&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Applications/autogen&#34;&gt;AutoGen&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/ModelScope-Models&#34;&gt;ModeScope&lt;/a&gt;, etc.&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;em&gt;&lt;strong&gt;50+ models&lt;/strong&gt; have been optimized/verified on &lt;code&gt;ipex-llm&lt;/code&gt; (including LLaMA2, Mistral, Mixtral, Gemma, LLaVA, Whisper, ChatGLM, Baichuan, Qwen, RWKV, and more); see the complete list &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/#verified-models&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Latest Update üî•&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/06] We added experimental &lt;strong&gt;NPU&lt;/strong&gt; support for Intel Core Ultra processors; see the examples &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/NPU/HF-Transformers-AutoModels&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] We added extensive support of &lt;strong&gt;pipeline parallel&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/Pipeline-Parallel-Inference&#34;&gt;inference&lt;/a&gt;, which makes it easy to run large-sized LLM using 2 or more Intel GPUs (such as Arc).&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] We added support for running &lt;strong&gt;RAGFlow&lt;/strong&gt; with &lt;code&gt;ipex-llm&lt;/code&gt; on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/ragflow_quickstart.md&#34;&gt;GPU&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] You can now easily run &lt;code&gt;ipex-llm&lt;/code&gt; inference, serving and finetuning using the &lt;strong&gt;Docker&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/#docker&#34;&gt;images&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] You can now install &lt;code&gt;ipex-llm&lt;/code&gt; on Windows using just &#34;&lt;em&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/install_windows_gpu.md#install-ipex-llm&#34;&gt;one command&lt;/a&gt;&lt;/em&gt;&#34;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;strong&gt;Axolotl&lt;/strong&gt; for LLM finetuning on Intel GPU; see the quickstart &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/axolotl_quickstart.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;More updates&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[2024/04] You can now run &lt;strong&gt;Open WebUI&lt;/strong&gt; on Intel GPU using &lt;code&gt;ipex-llm&lt;/code&gt;; see the quickstart &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/04] You can now run &lt;strong&gt;Llama 3&lt;/strong&gt; on Intel GPU using &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;ollama&lt;/code&gt; with &lt;code&gt;ipex-llm&lt;/code&gt;; see the quickstart &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/llama3_llamacpp_ollama_quickstart.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/04] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;strong&gt;Llama 3&lt;/strong&gt; on both Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/llama3&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3&#34;&gt;CPU&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/04] &lt;code&gt;ipex-llm&lt;/code&gt; now provides C++ interface, which can be used as an accelerated backend for running &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/ollama_quickstart.md&#34;&gt;ollama&lt;/a&gt; on Intel GPU.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/03] &lt;code&gt;bigdl-llm&lt;/code&gt; has now become &lt;code&gt;ipex-llm&lt;/code&gt; (see the migration guide &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/bigdl_llm_migration.md&#34;&gt;here&lt;/a&gt;); you may find the original &lt;code&gt;BigDL&lt;/code&gt; project &lt;a href=&#34;https://github.com/intel-analytics/bigdl-2.x&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/02] &lt;code&gt;ipex-llm&lt;/code&gt; now supports directly loading model from &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/ModelScope-Models&#34;&gt;ModelScope&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/ModelScope-Models&#34;&gt;È≠îÊê≠&lt;/a&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;[2024/02] &lt;code&gt;ipex-llm&lt;/code&gt; added initial &lt;strong&gt;INT2&lt;/strong&gt; support (based on llama.cpp &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF-IQ2&#34;&gt;IQ2&lt;/a&gt; mechanism), which makes it possible to run large-sized LLM (e.g., Mixtral-8x7B) on Intel GPU with 16GB VRAM.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/02] Users can now use &lt;code&gt;ipex-llm&lt;/code&gt; through &lt;a href=&#34;https://github.com/intel-analytics/text-generation-webui&#34;&gt;Text-Generation-WebUI&lt;/a&gt; GUI.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/02] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;em&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Inference/Self_Speculative_Decoding.md&#34;&gt;Self-Speculative Decoding&lt;/a&gt;&lt;/em&gt;, which in practice brings &lt;strong&gt;~30% speedup&lt;/strong&gt; for FP16 and BF16 inference latency on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/Speculative-Decoding&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Speculative-Decoding&#34;&gt;CPU&lt;/a&gt; respectively.&lt;/li&gt; &#xA;  &lt;li&gt;[2024/02] &lt;code&gt;ipex-llm&lt;/code&gt; now supports a comprehensive list of LLM &lt;strong&gt;finetuning&lt;/strong&gt; on Intel GPU (including &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/LoRA&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QLoRA&#34;&gt;QLoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/DPO&#34;&gt;DPO&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QA-LoRA&#34;&gt;QA-LoRA&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/ReLora&#34;&gt;ReLoRA&lt;/a&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;[2024/01] Using &lt;code&gt;ipex-llm&lt;/code&gt; &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QLoRA&#34;&gt;QLoRA&lt;/a&gt;, we managed to finetune LLaMA2-7B in &lt;strong&gt;21 minutes&lt;/strong&gt; and LLaMA2-70B in &lt;strong&gt;3.14 hours&lt;/strong&gt; on 8 Intel Max 1550 GPU for &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QLoRA/alpaca-qlora&#34;&gt;Standford-Alpaca&lt;/a&gt; (see the blog &lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/finetuning-llms-on-intel-gpus-using-bigdl-llm.html&#34;&gt;here&lt;/a&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;[2023/12] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/ReLora&#34;&gt;ReLoRA&lt;/a&gt; (see &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.05695&#34;&gt;&#34;ReLoRA: High-Rank Training Through Low-Rank Updates&#34;&lt;/a&gt;&lt;/em&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;[2023/12] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mixtral&#34;&gt;Mixtral-8x7B&lt;/a&gt; on both Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mixtral&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mixtral&#34;&gt;CPU&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/12] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QA-LoRA&#34;&gt;QA-LoRA&lt;/a&gt; (see &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.14717&#34;&gt;&#34;QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models&#34;&lt;/a&gt;&lt;/em&gt;).&lt;/li&gt; &#xA;  &lt;li&gt;[2023/12] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types&#34;&gt;FP8 and FP4 inference&lt;/a&gt; on Intel &lt;em&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/11] Initial support for directly loading &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF&#34;&gt;GGUF&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/AWQ&#34;&gt;AWQ&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GPTQ&#34;&gt;GPTQ&lt;/a&gt; models into &lt;code&gt;ipex-llm&lt;/code&gt; is available.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/11] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/vLLM-Serving&#34;&gt;vLLM continuous batching&lt;/a&gt; on both Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/vLLM-Serving&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/vLLM-Serving&#34;&gt;CPU&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/10] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QLoRA&#34;&gt;QLoRA finetuning&lt;/a&gt; on both Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QLoRA&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/QLoRA-FineTuning&#34;&gt;CPU&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/10] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/src/ipex_llm/llm/serving&#34;&gt;FastChat serving&lt;/a&gt; on on both Intel CPU and GPU.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/09] &lt;code&gt;ipex-llm&lt;/code&gt; now supports &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU&#34;&gt;Intel GPU&lt;/a&gt; (including iGPU, Arc, Flex and MAX).&lt;/li&gt; &#xA;  &lt;li&gt;[2023/09] &lt;code&gt;ipex-llm&lt;/code&gt; &lt;a href=&#34;https://github.com/intel-analytics/ipex-llm-tutorial&#34;&gt;tutorial&lt;/a&gt; is released.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;code&gt;ipex-llm&lt;/code&gt; Performance&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;strong&gt;Token Generation Speed&lt;/strong&gt; on &lt;em&gt;Intel Core Ultra&lt;/em&gt; and &lt;em&gt;Intel Arc GPU&lt;/em&gt; below[^1] (and refer to &lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-meta-llama3-with-intel-ai-solutions.html&#34;&gt;[2]&lt;/a&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-microsoft-phi-3-models-intel-ai-soln.html&#34;&gt;[3]&lt;/a&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/intel-ai-solutions-accelerate-alibaba-qwen2-llms.html&#34;&gt;[4]&lt;/a&gt; for more details).&lt;/p&gt; &#xA;&lt;table width=&#34;100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://llm-assets.readthedocs.io/en/latest/_images/MTL_perf.jpg&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://llm-assets.readthedocs.io/en/latest/_images/MTL_perf.jpg&#34; width=&#34;100%;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://llm-assets.readthedocs.io/en/latest/_images/Arc_perf.jpg&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://llm-assets.readthedocs.io/en/latest/_images/Arc_perf.jpg&#34; width=&#34;100%;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;You may follow the &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/benchmark_quickstart.md&#34;&gt;Benchmarking Guide&lt;/a&gt; to run &lt;code&gt;ipex-llm&lt;/code&gt; performance benchmark yourself.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;ipex-llm&lt;/code&gt; Demo&lt;/h2&gt; &#xA;&lt;p&gt;See demos of running local LLMs &lt;em&gt;on Intel Iris iGPU, Intel Core Ultra iGPU, single-card Arc GPU, or multi-card Arc GPUs&lt;/em&gt; using &lt;code&gt;ipex-llm&lt;/code&gt; below.&lt;/p&gt; &#xA;&lt;table width=&#34;100%&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; colspan=&#34;1&#34;&gt;&lt;strong&gt;Intel Iris iGPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; colspan=&#34;1&#34;&gt;&lt;strong&gt;Intel Core Ultra iGPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; colspan=&#34;1&#34;&gt;&lt;strong&gt;Intel Arc dGPU&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; colspan=&#34;1&#34;&gt;&lt;strong&gt;2-Card Intel Arc dGPUs&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://llm-assets.readthedocs.io/en/latest/_images/iris_phi3-3.8B_q4_0_llamacpp_long.gif&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://llm-assets.readthedocs.io/en/latest/_images/iris_phi3-3.8B_q4_0_llamacpp_long.gif&#34; width=&#34;100%;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://llm-assets.readthedocs.io/en/latest/_images/mtl_mistral-7B_q4_k_m_ollama.gif&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://llm-assets.readthedocs.io/en/latest/_images/mtl_mistral-7B_q4_k_m_ollama.gif&#34; width=&#34;100%;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://llm-assets.readthedocs.io/en/latest/_images/arc_llama3-8B_fp8_textwebui.gif&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://llm-assets.readthedocs.io/en/latest/_images/arc_llama3-8B_fp8_textwebui.gif&#34; width=&#34;100%;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://llm-assets.readthedocs.io/en/latest/_images/2arc_qwen1.5-32B_fp6_fastchat.gif&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://llm-assets.readthedocs.io/en/latest/_images/2arc_qwen1.5-32B_fp6_fastchat.gif&#34; width=&#34;100%;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md&#34;&gt;llama.cpp (Phi-3-mini Q4_0)&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/ollama_quickstart.md&#34;&gt;Ollama (Mistral-7B Q4_K) &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/webui_quickstart.md&#34;&gt;TextGeneration-WebUI (Llama3-8B FP8) &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/fastchat_quickstart.md&#34;&gt;FastChat (QWen1.5-32B FP6)&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;!--&#xA;See the demo of running [*Text-Generation-WebUI*](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/webui_quickstart.html), [*local RAG using LangChain-Chatchat*](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/chatchat_quickstart.html), [*llama.cpp*](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html) and [*Ollama*](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html) *(on either Intel Core Ultra laptop or Arc GPU)* with `ipex-llm`  below.&#xA;&#xA;&lt;table width=&#34;100%&#34;&gt;&#xA;  &lt;tr&gt;&#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;strong&gt;Intel Core Ultra Laptop&lt;/strong&gt;&lt;/td&gt;&#xA;    &lt;td align=&#34;center&#34; colspan=&#34;2&#34;&gt;&lt;strong&gt;Intel Arc GPU&lt;/strong&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;    &lt;td&gt;&#xA;      &lt;video src=&#34;https://private-user-images.githubusercontent.com/1931082/319632616-895d56cd-e74b-4da1-b4d1-2157df341424.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTIyNDE4MjUsIm5iZiI6MTcxMjI0MTUyNSwicGF0aCI6Ii8xOTMxMDgyLzMxOTYzMjYxNi04OTVkNTZjZC1lNzRiLTRkYTEtYjRkMS0yMTU3ZGYzNDE0MjQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDQwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA0MDRUMTQzODQ1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Y2JmYzkxYWFhMGYyN2MxYTkxOTI3MGQ2NTFkZDY4ZjFjYjg3NmZhY2VkMzVhZTU2OGEyYjhjNzI5YTFhOGNhNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.Ga8mmCAO62DFCNzU1fdoyC_4MzqhDHzjZedzmi_2L-I&#34; width=100% controls /&gt;&#xA;    &lt;/td&gt;&#xA;    &lt;td&gt;&#xA;      &lt;video src=&#34;https://private-user-images.githubusercontent.com/1931082/319625142-68da379e-59c6-4308-88e8-c17e40baba7b.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTIyNDA2MzQsIm5iZiI6MTcxMjI0MDMzNCwicGF0aCI6Ii8xOTMxMDgyLzMxOTYyNTE0Mi02OGRhMzc5ZS01OWM2LTQzMDgtODhlOC1jMTdlNDBiYWJhN2IubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDQwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA0MDRUMTQxODU0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NzYwOWI4MmQxZjFhMjJlNGNhZTA3MGUyZDE4OTA0N2Q2YjQ4NTcwN2M2MTY1ODAwZmE3OTIzOWI0Y2U3YzYwNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.g0bYAj3J8IJci7pLzoJI6QDalyzXzMYtQkDY7aqZMc4&#34; width=100% controls /&gt;&#xA;    &lt;/td&gt;&#xA;    &lt;td&gt;&#xA;      &lt;video src=&#34;https://private-user-images.githubusercontent.com/1931082/319625685-ff13b099-bcda-48f1-b11b-05421e7d386d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTIyNDA4MTcsIm5iZiI6MTcxMjI0MDUxNywicGF0aCI6Ii8xOTMxMDgyLzMxOTYyNTY4NS1mZjEzYjA5OS1iY2RhLTQ4ZjEtYjExYi0wNTQyMWU3ZDM4NmQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDQwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA0MDRUMTQyMTU3WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWQ3MmEwZGRkNGVlY2RkNjAzMTliODM1NDEzODU3NWQ0ZGE4MjYyOGEyZjdkMjBiZjI0MjllYTU4ODQ4YzM0NCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.OFxex8Yj6WyqJKMi6B1Q19KkmbYqYCg1rD49wUwxdXQ&#34; width=100% controls /&gt;&#xA;    &lt;/td&gt;&#xA;    &lt;td&gt;&#xA;      &lt;video src=&#34;https://private-user-images.githubusercontent.com/1931082/325939544-2fc0ad5e-9ac7-4f95-b7b9-7885a8738443.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQxMjYwODAsIm5iZiI6MTcxNDEyNTc4MCwicGF0aCI6Ii8xOTMxMDgyLzMyNTkzOTU0NC0yZmMwYWQ1ZS05YWM3LTRmOTUtYjdiOS03ODg1YTg3Mzg0NDMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDQyNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA0MjZUMTAwMzAwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YjZlZDE4YjFjZWJkMzQ4NmY3ZjNlMmRiYWUzMDYxMTI3YzcxYjRiYjgwNmE2NDliMjMwOTI0NWJhMDQ1NDY1YyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.WfA2qwr8EP9W7a3oOYcKqaqsEKDlAkF254zbmn9dVv0&#34; width=100% controls /&gt;&#xA;    &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;    &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt;&#xA;      &lt;a href=&#34;https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/webui_quickstart.html&#34;&gt;Text-Generation-WebUI&lt;/a&gt;&#xA;    &lt;/td&gt;&#xA;    &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt;&#xA;      &lt;a href=&#34;https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/chatchat_quickstart.html&#34;&gt;Local RAG using LangChain-Chatchat&lt;/a&gt;&#xA;    &lt;/td&gt;&#xA;    &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt;&#xA;      &lt;a href=&#34;https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html&#34;&gt;llama.cpp&lt;/a&gt;&#xA;    &lt;/td&gt;&#xA;    &lt;td align=&#34;center&#34; width=&#34;25%&#34;&gt;&#xA;      &lt;a href=&#34;https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html&#34;&gt;Ollama&lt;/a&gt;&#xA;    &lt;/td&gt;  &lt;/tr&gt;&#xA;&lt;/table&gt;&#xA;--&gt; &#xA;&lt;h2&gt;Model Accuracy&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;strong&gt;Perplexity&lt;/strong&gt; result below (tested on Wikitext dataset using the script &lt;a href=&#34;https://github.com/intel-analytics/ipex-llm/tree/main/python/llm/dev/benchmark/perplexity&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Perplexity&lt;/th&gt; &#xA;   &lt;th&gt;sym_int4&lt;/th&gt; &#xA;   &lt;th&gt;q4_k&lt;/th&gt; &#xA;   &lt;th&gt;fp6&lt;/th&gt; &#xA;   &lt;th&gt;fp8_e5m2&lt;/th&gt; &#xA;   &lt;th&gt;fp8_e4m3&lt;/th&gt; &#xA;   &lt;th&gt;fp16&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;6.3638&lt;/td&gt; &#xA;   &lt;td&gt;6.2179&lt;/td&gt; &#xA;   &lt;td&gt;6.0924&lt;/td&gt; &#xA;   &lt;td&gt;6.1796&lt;/td&gt; &#xA;   &lt;td&gt;6.0980&lt;/td&gt; &#xA;   &lt;td&gt;6.0963&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral-7B-Instruct-v0.1&lt;/td&gt; &#xA;   &lt;td&gt;6.0025&lt;/td&gt; &#xA;   &lt;td&gt;5.9581&lt;/td&gt; &#xA;   &lt;td&gt;5.8930&lt;/td&gt; &#xA;   &lt;td&gt;5.8884&lt;/td&gt; &#xA;   &lt;td&gt;5.8820&lt;/td&gt; &#xA;   &lt;td&gt;5.8734&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen1.5-7B-chat&lt;/td&gt; &#xA;   &lt;td&gt;8.8652&lt;/td&gt; &#xA;   &lt;td&gt;8.8163&lt;/td&gt; &#xA;   &lt;td&gt;8.5573&lt;/td&gt; &#xA;   &lt;td&gt;8.8463&lt;/td&gt; &#xA;   &lt;td&gt;8.5304&lt;/td&gt; &#xA;   &lt;td&gt;8.6065&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;[^1]: Performance varies by use, configuration and other factors. &lt;code&gt;ipex-llm&lt;/code&gt; may not optimize to the same degree for non-Intel products. Learn more at &lt;a href=&#34;http://www.Intel.com/PerformanceIndex&#34;&gt;www.Intel.com/PerformanceIndex&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;code&gt;ipex-llm&lt;/code&gt; Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/DockerGuides/docker_cpp_xpu_quickstart.md&#34;&gt;GPU Inference in C++&lt;/a&gt;: running &lt;code&gt;llama.cpp&lt;/code&gt;, &lt;code&gt;ollama&lt;/code&gt;, &lt;code&gt;OpenWebUI&lt;/code&gt;, etc., with &lt;code&gt;ipex-llm&lt;/code&gt; on Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/DockerGuides/docker_pytorch_inference_gpu.md&#34;&gt;GPU Inference in Python&lt;/a&gt; : running HuggingFace &lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;LangChain&lt;/code&gt;, &lt;code&gt;LlamaIndex&lt;/code&gt;, &lt;code&gt;ModelScope&lt;/code&gt;, etc. with &lt;code&gt;ipex-llm&lt;/code&gt; on Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/DockerGuides/vllm_docker_quickstart.md&#34;&gt;vLLM on GPU&lt;/a&gt;: running &lt;code&gt;vLLM&lt;/code&gt; serving with &lt;code&gt;ipex-llm&lt;/code&gt; on Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/DockerGuides/vllm_cpu_docker_quickstart.md&#34;&gt;vLLM on CPU&lt;/a&gt;: running &lt;code&gt;vLLM&lt;/code&gt; serving with &lt;code&gt;ipex-llm&lt;/code&gt; on Intel CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/DockerGuides/fastchat_docker_quickstart.md&#34;&gt;FastChat on GPU&lt;/a&gt;: running &lt;code&gt;FastChat&lt;/code&gt; serving with &lt;code&gt;ipex-llm&lt;/code&gt; on Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/DockerGuides/docker_run_pytorch_inference_in_vscode.md&#34;&gt;VSCode on GPU&lt;/a&gt;: running and developing &lt;code&gt;ipex-llm&lt;/code&gt; applications in Python using VSCode on Intel GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Use&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/llama_cpp_quickstart.md&#34;&gt;llama.cpp&lt;/a&gt;: running &lt;strong&gt;llama.cpp&lt;/strong&gt; (&lt;em&gt;using C++ interface of &lt;code&gt;ipex-llm&lt;/code&gt; as an accelerated backend for &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/em&gt;) on Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/ollama_quickstart.md&#34;&gt;Ollama&lt;/a&gt;: running &lt;strong&gt;ollama&lt;/strong&gt; (&lt;em&gt;using C++ interface of &lt;code&gt;ipex-llm&lt;/code&gt; as an accelerated backend for &lt;code&gt;ollama&lt;/code&gt;&lt;/em&gt;) on Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/llama3_llamacpp_ollama_quickstart.md&#34;&gt;Llama 3 with &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;ollama&lt;/code&gt;&lt;/a&gt;: running &lt;strong&gt;Llama 3&lt;/strong&gt; on Intel GPU using &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;ollama&lt;/code&gt; with &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/vLLM_quickstart.md&#34;&gt;vLLM&lt;/a&gt;: running &lt;code&gt;ipex-llm&lt;/code&gt; in &lt;strong&gt;vLLM&lt;/strong&gt; on both Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/vLLM-Serving&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/vLLM-Serving&#34;&gt;CPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/fastchat_quickstart.md&#34;&gt;FastChat&lt;/a&gt;: running &lt;code&gt;ipex-llm&lt;/code&gt; in &lt;strong&gt;FastChat&lt;/strong&gt; serving on on both Intel GPU and CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/deepspeed_autotp_fastapi_quickstart.md&#34;&gt;Serving on multiple Intel GPUs&lt;/a&gt;: running &lt;code&gt;ipex-llm&lt;/code&gt; &lt;strong&gt;serving on multiple Intel GPUs&lt;/strong&gt; by leveraging DeepSpeed AutoTP and FastAPI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/webui_quickstart.md&#34;&gt;Text-Generation-WebUI&lt;/a&gt;: running &lt;code&gt;ipex-llm&lt;/code&gt; in &lt;code&gt;oobabooga&lt;/code&gt; &lt;strong&gt;WebUI&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/axolotl_quickstart.md&#34;&gt;Axolotl&lt;/a&gt;: running &lt;code&gt;ipex-llm&lt;/code&gt; in &lt;strong&gt;Axolotl&lt;/strong&gt; for LLM finetuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/benchmark_quickstart.md&#34;&gt;Benchmarking&lt;/a&gt;: running (latency and throughput) &lt;strong&gt;benchmarks&lt;/strong&gt; for &lt;code&gt;ipex-llm&lt;/code&gt; on Intel CPU and GPU&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Applications&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/chatchat_quickstart.md&#34;&gt;Local RAG&lt;/a&gt;: running &lt;code&gt;LangChain-Chatchat&lt;/code&gt; (&lt;em&gt;Knowledge Base QA using &lt;strong&gt;RAG&lt;/strong&gt; pipeline&lt;/em&gt;) with &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/continue_quickstart.md&#34;&gt;Coding copilot&lt;/a&gt;: running &lt;code&gt;Continue&lt;/code&gt; (coding copilot in VSCode) with &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/open_webui_with_ollama_quickstart.md&#34;&gt;Open WebUI&lt;/a&gt;: running &lt;code&gt;Open WebUI&lt;/code&gt; with &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/privateGPT_quickstart.md&#34;&gt;PrivateGPT&lt;/a&gt;: running &lt;code&gt;PrivateGPT&lt;/code&gt; to interact with documents with &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/dify_quickstart.md&#34;&gt;Dify platform&lt;/a&gt;: running &lt;code&gt;ipex-llm&lt;/code&gt; in &lt;code&gt;Dify&lt;/code&gt;(&lt;em&gt;production-ready LLM app development platform&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/ragflow_quickstart.md&#34;&gt;RAGFlow&lt;/a&gt;: running &lt;code&gt;RAGFlow&lt;/code&gt; (&lt;em&gt;an open-source RAG engine&lt;/em&gt;) with &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/install_windows_gpu.md&#34;&gt;Windows GPU&lt;/a&gt;: installing &lt;code&gt;ipex-llm&lt;/code&gt; on Windows with Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/install_linux_gpu.md&#34;&gt;Linux GPU&lt;/a&gt;: installing &lt;code&gt;ipex-llm&lt;/code&gt; on Linux with Intel GPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;For more details, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Overview/install.md&#34;&gt;full installation guide&lt;/a&gt;&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Code Examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Low bit inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model&#34;&gt;INT4 inference&lt;/a&gt;: &lt;strong&gt;INT4&lt;/strong&gt; LLM inference on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model&#34;&gt;CPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types&#34;&gt;FP8/FP4 inference&lt;/a&gt;: &lt;strong&gt;FP8&lt;/strong&gt; and &lt;strong&gt;FP4&lt;/strong&gt; LLM inference on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types&#34;&gt;INT8 inference&lt;/a&gt;: &lt;strong&gt;INT8&lt;/strong&gt; LLM inference on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types&#34;&gt;GPU&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/More-Data-Types&#34;&gt;CPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF-IQ2&#34;&gt;INT2 inference&lt;/a&gt;: &lt;strong&gt;INT2&lt;/strong&gt; LLM inference (based on llama.cpp IQ2 mechanism) on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF-IQ2&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;FP16/BF16 inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;FP16&lt;/strong&gt; LLM inference on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/Speculative-Decoding&#34;&gt;GPU&lt;/a&gt;, with possible &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Inference/Self_Speculative_Decoding.md&#34;&gt;self-speculative decoding&lt;/a&gt; optimization&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;BF16&lt;/strong&gt; LLM inference on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Speculative-Decoding&#34;&gt;CPU&lt;/a&gt;, with possible &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Inference/Self_Speculative_Decoding.md&#34;&gt;self-speculative decoding&lt;/a&gt; optimization&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Distributed inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Pipeline Parallel&lt;/strong&gt; inference on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/Pipeline-Parallel-Inference&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;DeepSpeed AutoTP&lt;/strong&gt; inference on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/Deepspeed-AutoTP&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Save and load &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Save-Load&#34;&gt;Low-bit models&lt;/a&gt;: saving and loading &lt;code&gt;ipex-llm&lt;/code&gt; low-bit models (INT4/FP4/FP6/INT8/FP8/FP16/etc.)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF&#34;&gt;GGUF&lt;/a&gt;: directly loading GGUF models into &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/AWQ&#34;&gt;AWQ&lt;/a&gt;: directly loading AWQ models into &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GPTQ&#34;&gt;GPTQ&lt;/a&gt;: directly loading GPTQ models into &lt;code&gt;ipex-llm&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Finetuning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LLM finetuning on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning&#34;&gt;GPU&lt;/a&gt;, including &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/LoRA&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QLoRA&#34;&gt;QLoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/DPO&#34;&gt;DPO&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/QA-LoRA&#34;&gt;QA-LoRA&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/ReLora&#34;&gt;ReLoRA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;QLoRA finetuning on Intel &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/QLoRA-FineTuning&#34;&gt;CPU&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Integration with community libraries &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels&#34;&gt;HuggingFace transformers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/PyTorch-Models&#34;&gt;Standard PyTorch model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LangChain&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LlamaIndex&#34;&gt;LlamaIndex&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/Deepspeed-AutoTP&#34;&gt;DeepSpeed-AutoTP&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Quickstart/axolotl_quickstart.md&#34;&gt;Axolotl&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/HF-PEFT&#34;&gt;HuggingFace PEFT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/LLM-Finetuning/DPO&#34;&gt;HuggingFace TRL&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Applications/autogen&#34;&gt;AutoGen&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/ModelScope-Models&#34;&gt;ModeScope&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intel-analytics/ipex-llm-tutorial&#34;&gt;Tutorials&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;API Doc&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/PythonAPI/transformers.md&#34;&gt;HuggingFace Transformers-style API (Auto Classes)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/intel-analytics/ipex-llm/raw/main/docs/mddocs/PythonAPI/optimize.md&#34;&gt;API for arbitrary PyTorch Model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/docs/mddocs/Overview/FAQ/faq.md&#34;&gt;FAQ &amp;amp; Trouble Shooting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Verified Models&lt;/h2&gt; &#xA;&lt;p&gt;Over 50 models have been optimized/verified on &lt;code&gt;ipex-llm&lt;/code&gt;, including &lt;em&gt;LLaMA/LLaMA2, Mistral, Mixtral, Gemma, LLaVA, Whisper, ChatGLM2/ChatGLM3, Baichuan/Baichuan2, Qwen/Qwen-1.5, InternLM&lt;/em&gt; and more; see the list below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;CPU Example&lt;/th&gt; &#xA;   &lt;th&gt;GPU Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA &lt;em&gt;(such as Vicuna, Guanaco, Koala, Baize, WizardLM, etc.)&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Native-Models&#34;&gt;link1&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/vicuna&#34;&gt;link2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/vicuna&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA 2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Native-Models&#34;&gt;link1&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama2&#34;&gt;link2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/llama2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA 3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/llama3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/chatglm2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/chatglm3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/glm4&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/glm4&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GLM-4V&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/glm-4v&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/glm-4v&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mistral&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mistral&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mixtral&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mixtral&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/falcon&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/falcon&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dolly-v1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/dolly-v1&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dolly-v2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/dolly-v2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Replit Code&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/replit&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/replit&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RedPajama&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Native-Models&#34;&gt;link1&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/redpajama&#34;&gt;link2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phoenix&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Native-Models&#34;&gt;link1&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phoenix&#34;&gt;link2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/Native-Models&#34;&gt;link1&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/starcoder&#34;&gt;link2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/starcoder&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/baichuan&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/baichuan2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/internlm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/qwen&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen1.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen1.5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/qwen1.5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/qwen2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen-vl&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/qwen-vl&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aquila&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/aquila&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/aquila&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aquila2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/aquila2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/aquila2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOSS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/moss&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Whisper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/whisper&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/whisper&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi-1_5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-1_5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phi-1_5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flan-t5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/flan-t5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/flan-t5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/PyTorch-Models/Model/llava&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/PyTorch-Models/Model/llava&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeLlama&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/codellama&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/codellama&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Skywork&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/skywork&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM-XComposer&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm-xcomposer&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardCoder-Python&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/wizardcoder-python&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeShell&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/codeshell&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fuyu&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/fuyu&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distil-Whisper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/distil-whisper&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/distil-whisper&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Yi&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/yi&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/yi&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BlueLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/bluelm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/bluelm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mamba&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/PyTorch-Models/Model/mamba&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/PyTorch-Models/Model/mamba&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SOLAR&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/solar&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/solar&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phixtral&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phixtral&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phixtral&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/internlm2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RWKV4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/rwkv4&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RWKV5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/rwkv5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bark&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/PyTorch-Models/Model/bark&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/PyTorch-Models/Model/bark&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SpeechT5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/PyTorch-Models/Model/speech-t5&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-MoE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/deepseek-moe&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya-Coding-34B-v1.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/ziya&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phi-2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phi-3&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi-3-vision&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-3-vision&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phi-3-vision&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Yuan2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/yuan2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/yuan2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/gemma&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/gemma&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeciLM-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/deciLM-7b&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/deciLM-7b&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deepseek&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/deepseek&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/deepseek&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StableLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/stablelm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/stablelm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeGemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/codegemma&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/codegemma&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Command-R/cohere&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/cohere&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/cohere&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeGeeX2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/codegeex2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/codegeex2&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniCPM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/minicpm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/intel-analytics/ipex-llm/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/minicpm&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Get Support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please report a bug or raise a feature request by opening a &lt;a href=&#34;https://github.com/intel-analytics/ipex-llm/issues&#34;&gt;Github Issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Please report a vulnerability by opening a draft &lt;a href=&#34;https://github.com/intel-analytics/ipex-llm/security/advisories&#34;&gt;GitHub Security Advisory&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>