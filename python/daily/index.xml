<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-14T01:31:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pentestfunctions/BlueDucky</title>
    <updated>2024-05-14T01:31:47Z</updated>
    <id>tag:github.com,2024-05-14:/pentestfunctions/BlueDucky</id>
    <link href="https://github.com/pentestfunctions/BlueDucky" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üö® CVE-2023-45866 - BlueDucky Implementation (Using DuckyScript) üîì Unauthenticated Peering Leading to Code Execution (Using HID Keyboard)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BlueDucky (Android) ü¶Ü&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to all the people at HackNexus. Make sure you come join us on VC ! &lt;a href=&#34;https://discord.gg/HackNexus&#34;&gt;https://discord.gg/HackNexus&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saad0x1&#34;&gt;saad0x1&#39;s GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spicydll&#34;&gt;spicydll&#39;s GitHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pentestfunctions/BlueDucky/main/images/duckmenu.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;üö® CVE-2023-45866 - BlueDucky Implementation (Using DuckyScript)&lt;/p&gt; &#xA;&lt;p&gt;üîì Unauthenticated Peering Leading to Code Execution (Using HID Keyboard)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/marcnewlin/hi_my_name_is_keyboard&#34;&gt;This is an implementation of the CVE discovered by marcnewlin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pentestfunctions/BlueDucky/main/images/BlueDucky.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Introduction üì¢&lt;/h2&gt; &#xA;&lt;p&gt;BlueDucky is a powerful tool for exploiting a vulnerability in Bluetooth devices. By running this script, you can:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üì° Load saved Bluetooth devices that are no longer visible but have Bluetooth still enabled.&lt;/li&gt; &#xA; &lt;li&gt;üìÇ Automatically save any devices you scan.&lt;/li&gt; &#xA; &lt;li&gt;üíå Send messages via ducky script format to interact with devices.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;I&#39;ve successfully run this on a Raspberry Pi 4 using the default Bluetooth module. It works against various phones, with an interesting exception for a New Zealand brand, Vodafone.&lt;/p&gt; &#xA;&lt;h2&gt;Installation and Usage üõ†Ô∏è&lt;/h2&gt; &#xA;&lt;h3&gt;Setup Instructions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# update apt&#xA;sudo apt-get update&#xA;sudo apt-get -y upgrade&#xA;&#xA;# install dependencies from apt&#xA;sudo apt install -y bluez-tools bluez-hcidump libbluetooth-dev \&#xA;                    git gcc python3-pip python3-setuptools \&#xA;                    python3-pydbus&#xA;&#xA;# install pybluez from source&#xA;git clone https://github.com/pybluez/pybluez.git&#xA;cd pybluez&#xA;sudo python3 setup.py install&#xA;&#xA;# build bdaddr from the bluez source&#xA;cd ~/&#xA;git clone --depth=1 https://github.com/bluez/bluez.git&#xA;gcc -o bdaddr ~/bluez/tools/bdaddr.c ~/bluez/src/oui.c -I ~/bluez -lbluetooth&#xA;sudo cp bdaddr /usr/local/bin/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running BlueDucky&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pentestfunctions/BlueDucky.git&#xA;cd BlueDucky&#xA;sudo hciconfig hci0 up&#xA;python3 BlueDucky.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Operational Steps üïπÔ∏è&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;On running, it prompts for the target MAC address.&lt;/li&gt; &#xA; &lt;li&gt;Pressing nothing triggers an automatic scan for devices.&lt;/li&gt; &#xA; &lt;li&gt;Devices previously found are stored in known_devices.txt.&lt;/li&gt; &#xA; &lt;li&gt;If known_devices.txt exists, it checks this file before scanning.&lt;/li&gt; &#xA; &lt;li&gt;Executes using payload.txt file.&lt;/li&gt; &#xA; &lt;li&gt;Successful execution will result in automatic connection and script running.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Duckyscript üíª&lt;/h2&gt; &#xA;&lt;p&gt;üöß Work in Progress:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Suggest me ideas&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;üìù Example payload.txt:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REM Title of the payload&#xA;STRING ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!@#$%^&amp;amp;*()_-=+\|[{]};:&#39;&#34;,&amp;lt;.&amp;gt;/?&#xA;GUI D&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;REM Opens a private browser to hackertyper.net&#xA;DELAY 200&#xA;ESCAPE&#xA;GUI d&#xA;ALT ESCAPE&#xA;GUI b&#xA;DELAY 700&#xA;REM PRIVATE_BROWSER is equal to CTRL + SHIFT + N&#xA;PRIVATE_BROWSER&#xA;DELAY 700&#xA;CTRL l&#xA;DELAY 300&#xA;STRING hackertyper.net&#xA;DELAY 300&#xA;ENTER&#xA;DELAY 300&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Enjoy experimenting with BlueDucky! üåü&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>Alpha-VLLM/Lumina-T2X</title>
    <updated>2024-05-14T01:31:47Z</updated>
    <id>tag:github.com,2024-05-14:/Alpha-VLLM/Lumina-T2X</id>
    <link href="https://github.com/Alpha-VLLM/Lumina-T2X" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lumina-T2X is a unified framework for Text to Any Modality Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;$\textbf{Lumina-T2X}$: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors-anon/Alpha-VLLM/Lumina-T2X?style=flat&amp;amp;label=Contributors&#34; alt=&#34;GitHub repo contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/commits/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/Alpha-VLLM/Lumina-T2X?label=Commit&#34; alt=&#34;GitHub Commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr-closed-raw/Alpha-VLLM/Lumina-T2X.svg?label=Merged+PRs&amp;amp;color=green&#34; alt=&#34;Pr&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Alpha-VLLM/Lumina-T2X?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars&#34; alt=&#34;GitHub repo stars&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/watchers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/Alpha-VLLM/Lumina-T2X?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers&#34; alt=&#34;GitHub repo watchers&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/archive/refs/heads/main.zip&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/repo-size/Alpha-VLLM/Lumina-T2X?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size&#34; alt=&#34;GitHub repo size&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;img src=&#34;https://img.shields.io/badge/-MIT-MIT?logoColor=%231082c3&amp;amp;label=Code%20License&amp;amp;link=https%3A%2F%2Fgithub.com%2FAlpha-VLLM%2FLumina-T2X%2Fblob%2Fmain%2FLICENSE&#34; alt=&#34;Static Badge&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://106.14.2.150:10022/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-6B88E3?logo=youtubegaming&amp;amp;logoColor=DAE4EE&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://www.youtube.com/watch?v=KFtHmS5eUCM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Video%20Introduction%20of%20Lumina--T2X-test?logo=youtube&amp;amp;color=red&amp;amp;link=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DKFtHmS5eUCM&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://imagebind-llm.opengvlab.com/qrcode/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-WeChat@Group-000000?logo=wechat&amp;amp;logoColor=07C160&#34; alt=&#34;Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- [![GitHub issues](https://img.shields.io/github/issues/Alpha-VLLM/Lumina-T2X?color=critical&amp;label=Issues)]() --&gt; &#xA; &lt;!-- [![GitHub closed issues](https://img.shields.io/github/issues-closed/Alpha-VLLM/Lumina-T2X?color=success&amp;label=Issues)]() &lt;br&gt; --&gt; &#xA; &lt;!-- [![GitHub repo forks](https://img.shields.io/github/forks/Alpha-VLLM/Lumina-T2X?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Forks)](https://github.com/Alpha-VLLM/Lumina-T2X/network)  --&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2405.05945&#34;&gt;üìÑ Lumina-T2X arXiv&lt;/a&gt;] [&lt;a href=&#34;https://www.youtube.com/watch?v=KFtHmS5eUCM&#34;&gt;üìΩÔ∏è Video Introduction of Lumina-T2X&lt;/a&gt;] [üëã join our &lt;a href=&#34;http://imagebind-llm.opengvlab.com/qrcode/&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt;]&lt;/p&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://huggingface.co/Alpha-VLLM/Lumina-T2I&#34;&gt;ü§ñÔ∏è Lumina-T2I 5B Checkpoints&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/Alpha-VLLM/Lumina-Next-T2I&#34;&gt;ü§ñÔ∏è Lumina-Next-T2I 2B Checkpoints (recommend)&lt;/a&gt;]&lt;/p&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;http://106.14.2.150:10020/&#34;&gt;üïπÔ∏è GUI Demo for Lumina-T2I 5B model (node1)&lt;/a&gt;]&lt;/p&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;http://106.14.2.150:10021/&#34;&gt;üîÆ GUI Demo for Lumina-Next-T2I 2B model (node2)&lt;/a&gt;] [&lt;a href=&#34;http://106.14.2.150:10022/&#34;&gt;üîÆ GUI Demo for Lumina-Next-T2I 2B model (node3)&lt;/a&gt;]&lt;/p&gt; &#xA; &lt;!-- [[üì∫ Website](https://lumina-t2-x-web.vercel.app/)] --&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/9f52eabb-07dc-4881-8257-6d8a5f2a0a5a&#34; alt=&#34;intro_large&#34;&gt;&lt;/p&gt; &#xA;&lt;!-- [[‰∏≠ÊñáÁâàÊú¨]](./README_cn.md) --&gt; &#xA;&lt;h2&gt;üì∞ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024-05-13] üî•üî•üî• Lumina-Next now supports simple &lt;strong&gt;text-to-music&lt;/strong&gt; generation! See some &lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-music-generation&#34;&gt;examples&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024-05-13] üî•üî•üî• We give &lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#multilingual-generation&#34;&gt;examples&lt;/a&gt; demonstrating Lumina-T2X&#39;s capability to support &lt;strong&gt;multilingual prompts&lt;/strong&gt;, and even support prompts containing &lt;strong&gt;emojis&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-05-12]&lt;/strong&gt; ü§©ü§©ü§© &lt;strong&gt;We excitedly released our &lt;code&gt;Lumina-Next-T2I&lt;/code&gt; model (&lt;a href=&#34;https://huggingface.co/Alpha-VLLM/Lumina-Next-T2I&#34;&gt;checkpoint&lt;/a&gt;) which uses a 2B Next-DiT model as the backbone and Gemma-2B as the text encoder. Try it out at &lt;a href=&#34;http://106.14.2.150:10021/&#34;&gt;demo1&lt;/a&gt; &amp;amp; &lt;a href=&#34;http://106.14.2.150:10022/&#34;&gt;demo2&lt;/a&gt;.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-05-10]&lt;/strong&gt; üî•üî•üî• &lt;strong&gt;We released the technical report on &lt;a href=&#34;https://arxiv.org/abs/2405.05945&#34;&gt;arXiv&lt;/a&gt;.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-05-09]&lt;/strong&gt; üöÄüöÄüöÄ &lt;strong&gt;We released &lt;code&gt;Lumina-T2A&lt;/code&gt; (Text-to-Audio) Demos. &lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-audio-generation&#34;&gt;Examples&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2024-04-29] üî• We released the 5B model &lt;a href=&#34;https://huggingface.co/Alpha-VLLM/Lumina-T2I&#34;&gt;checkpoint&lt;/a&gt; and &lt;a href=&#34;http://106.14.2.150:10020/&#34;&gt;demo&lt;/a&gt; built upon it for text-to-image generation.&lt;/li&gt; &#xA; &lt;li&gt;[2024-04-25] üî• Support 720P video generation with arbitrary aspect ratio. &lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-video-generation&#34;&gt;Examples&lt;/a&gt; üöÄüöÄüöÄ&lt;/li&gt; &#xA; &lt;li&gt;[2024-04-19] Demo examples released.&lt;/li&gt; &#xA; &lt;li&gt;[2024-04-05] Code released for &lt;code&gt;Lumina-T2I&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024-04-01] We release the initial version of &lt;code&gt;Lumina-T2I&lt;/code&gt; for text-to-image generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning] &lt;strong&gt;Since we are updating the code frequently, please pull the latest code:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git pull origin main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;In order to quickly get you guys using our model, we built different versions of the GUI demo site.&lt;/p&gt; &#xA;&lt;h4&gt;Lumina-T2I 5B model demo:&lt;/h4&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;http://106.14.2.150:10020/&#34;&gt;node1&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h4&gt;Lumina-Next-T2I 2B model demo:&lt;/h4&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;http://106.14.2.150:10021/&#34;&gt;node1&lt;/a&gt;] [&lt;a href=&#34;http://106.14.2.150:10022/&#34;&gt;node2&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;For more details about training and inference, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/lumina_t2i/README.md#Installation&#34;&gt;Lumina-T2I&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/lumina_nextr_t2i/README.md#Installation&#34;&gt;Lumina-Next-T2I&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning] &lt;strong&gt;Lumina-T2X employs FSDP for training large diffusion models. FSDP shards parameters, optimizer states, and gradients across GPUs. Thus, at least 8 GPUs are required for full fine-tuning of the Lumina-T2X 5B model. Parameter-efficient Finetuning of Lumina-T2X shall be released soon.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Installation on your environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/Alpha-VLLM/Lumina-T2X&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìë Open-source Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Lumina-T2I (Demos‚úÖ, Training‚úÖ, Inference‚úÖ, Checkpoints‚úÖ)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Lumina-T2V (Demos‚úÖ)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Lumina-T2A (Demos‚úÖ)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Web Demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Cli Demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìú Index of Content&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#lumina-t2x&#34;&gt;Lumina-T2X&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#-news&#34;&gt;üì∞ News&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#-quick-start&#34;&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#-open-source-plan&#34;&gt;üìë Open-source Plan&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#-index-of-content&#34;&gt;üìú Index of Content&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#%EF%B8%8F-demo-examples&#34;&gt;üìΩÔ∏è Demo Examples&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-image-generation&#34;&gt;Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-video-generation&#34;&gt;Text-to-Video Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-3d-generation&#34;&gt;Text-to-3D Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-audio-generation&#34;&gt;Text-to-Audio Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#text-to-music-generation&#34;&gt;Text-to-music Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#multilingual-generation&#34;&gt;Multilingual Examples&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/#%EF%B8%8F-diverse-configurations&#34;&gt;‚öôÔ∏è Diverse Configurations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We introduce the $\textbf{Lumina-T2X}$ family, a series of text-conditioned Diffusion Transformers (DiT) capable of transforming textual descriptions into vivid images, dynamic videos, detailed multi-view 3D images, and synthesized speech. At the core of Lumina-T2X lies the &lt;strong&gt;Flow-based Large Diffusion Transformer (Flag-DiT)&lt;/strong&gt;‚Äîa robust engine that supports up to &lt;strong&gt;7 billion parameters&lt;/strong&gt; and extends sequence lengths to &lt;strong&gt;128,000&lt;/strong&gt; tokens. Drawing inspiration from Sora, Lumina-T2X integrates images, videos, multi-views of 3D objects, and speech spectrograms within a spatial-temporal latent token space, and can generate outputs at &lt;strong&gt;any resolution, aspect ratio, and duration&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üåü &lt;strong&gt;Features&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flow-based Large Diffusion Transformer (Flag-DiT)&lt;/strong&gt;: Lumina-T2X adopts the &lt;strong&gt;flow matching&lt;/strong&gt; formulation and is equipped with many advanced techniques, such as RoPE, RMSNorm, and KQ-norm, &lt;strong&gt;demonstrating faster training convergence, stable training dynamics, and a simplified pipeline&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Any Modalities, Resolution, and Duration within One Framework&lt;/strong&gt;: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;$\textbf{Lumina-T2X}$ can &lt;strong&gt;encode any modality, including mages, videos, multi-views of 3D objects, and spectrograms into a unified 1-D token sequence at any resolution, aspect ratio, and temporal duration.&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;By introducing the &lt;code&gt;[nextline]&lt;/code&gt; and &lt;code&gt;[nextframe]&lt;/code&gt; tokens, our model can &lt;strong&gt;support resolution extrapolation&lt;/strong&gt;, i.e., generating images/videos with out-of-domain resolutions &lt;strong&gt;not encountered during training&lt;/strong&gt;, such as images from 768x768 to 1792x1792 pixels.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low Training Resources&lt;/strong&gt;: Our empirical observations indicate that employing larger models, high-resolution images, and longer-duration video clips can &lt;strong&gt;significantly accelerate the convergence&lt;/strong&gt; &lt;strong&gt;speed&lt;/strong&gt; of diffusion transformers. Moreover, by employing meticulously curated text-image and text-video pairs featuring high aesthetic quality frames and detailed captions, our $\textbf{Lumina-T2X}$ model is learned to generate high-resolution images and coherent videos with minimal computational demands. Remarkably, the default Lumina-T2I configuration, equipped with a 5B Flag-DiT and a 7B LLaMA as the text encoder, &lt;strong&gt;requires only 35% of the computational resources compared to Pixelart-&lt;/strong&gt;$\alpha$.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/60d2f248-67b1-43ef-a530-c75530cf26c5&#34; alt=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìΩÔ∏è Demo Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Text-to-Image Generation&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/27bd36a8-8411-47dd-a3a7-3607c1d5d644&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Video Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;720P Videos:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; The majestic beauty of a waterfall cascading down a cliff into a serene lake.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/17187de8-7a07-49a8-92f9-fdb8e2f5e64c&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/17187de8-7a07-49a8-92f9-fdb8e2f5e64c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/0a20bb39-f6f7-430f-aaa0-7193a71b256a&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/0a20bb39-f6f7-430f-aaa0-7193a71b256a&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/7bf9ce7e-f454-4430-babe-b14264e0f194&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/7bf9ce7e-f454-4430-babe-b14264e0f194&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;360P Videos:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/d7fec32c-3655-4fd1-aa14-c0cb3ace3845&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/d7fec32c-3655-4fd1-aa14-c0cb3ace3845&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-3D Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/cd061b8d-c47b-4c0c-b775-2cbaf8014be9&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/cd061b8d-c47b-4c0c-b775-2cbaf8014be9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Audio Generation&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] &lt;strong&gt;Attention: Mouse over the playbar and click the audio button on the playbar to unmute it.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- &gt; üåüüåüüåü **We recommend visiting the Lumina website to try it out! [üåü visit](https://lumina-t2-x-web.vercel.app/docs/demos/demo-of-audio)** --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Semiautomatic gunfire occurs with slight echo&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Audio:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/25f2a6a8-0386-41e8-ab10-d1303554b944&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/25f2a6a8-0386-41e8-ab10-d1303554b944&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Groundtruth:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/6722a68a-1a5a-4a44-ba9c-405372dc27ef&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/6722a68a-1a5a-4a44-ba9c-405372dc27ef&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A telephone bell rings&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Audio:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/7467dd6d-b163-4436-ac5b-36662d1f9ddf&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/7467dd6d-b163-4436-ac5b-36662d1f9ddf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Groundtruth:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/703ea405-6eb4-4161-b5ff-51a93f81d013&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/703ea405-6eb4-4161-b5ff-51a93f81d013&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; An engine running followed by the engine revving and tires screeching&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Audio:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/5d9dd431-b8b4-41a0-9e78-bb0a234a30b9&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/5d9dd431-b8b4-41a0-9e78-bb0a234a30b9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Groundtruth:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/9ca4af9e-cee3-4596-b826-d6c25761c3c1&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/9ca4af9e-cee3-4596-b826-d6c25761c3c1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Birds chirping with insects buzzing and outdoor ambiance&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Audio:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/b776aacb-783b-4f47-bf74-89671a17d38d&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/b776aacb-783b-4f47-bf74-89671a17d38d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Groundtruth:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/a11333e4-695e-4a8c-8ea1-ee5b83e34682&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/a11333e4-695e-4a8c-8ea1-ee5b83e34682&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-music Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; An electrifying ska tune with prominent saxophone riffs, energetic e-guitar and acoustic drums, lively percussion, soulful keys, groovy e-bass, and a fast tempo that exudes uplifting energy.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Music:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/fef8f6b9-1e77-457e-bf4b-fb0cccefa0ec&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/fef8f6b9-1e77-457e-bf4b-fb0cccefa0ec&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A high-energy synth rock/pop song with fast-paced acoustic drums, a triumphant brass/string section, and a thrilling synth lead sound that creates an adventurous atmosphere.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Music:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/1f796046-64ab-44ed-a4d8-0ebc0cfc484f&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/1f796046-64ab-44ed-a4d8-0ebc0cfc484f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; An uptempo electronic pop song that incorporates digital drums, digital bass and synthpad sounds.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Music:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/4768415e-436a-4d0e-af53-bf7882cb94cd&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/4768415e-436a-4d0e-af53-bf7882cb94cd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A medium-tempo digital keyboard song with a jazzy backing track featuring digital drums, piano, e-bass, trumpet, and acoustic guitar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Music:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/8994a573-e776-488b-a86c-4398a4362398&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/8994a573-e776-488b-a86c-4398a4362398&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; This low-quality folk song features groovy wooden percussion, bass, piano, and flute melodies, as well as sustained strings and shimmering shakers that create a passionate, happy, and joyful atmosphere.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Music:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/e0b5d197-589c-47d6-954b-b9c1d54feebb&#34;&gt;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/e0b5d197-589c-47d6-954b-b9c1d54feebb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multilingual Generation&lt;/h3&gt; &#xA;&lt;p&gt;We present three multilingual capabilities of Lumina-Next-2B.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Images conditioned on Chinese poems:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/08460829-1ed4-4489-ac01-1904ef196db2&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Images with multilignual prompts:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/7c62bb94-42e4-4525-a298-9e25475b511d&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/07fc8138-e67c-4c9f-bc01-e749a6507ada&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Images with emojis:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/86041420/980b4999-9d1c-4fbd-a695-88b6b675f34b&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;!--&#xA;**Prompt:** Water trickling rapidly and draining&#xA;&#xA;**Generated Audio:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/88fcf0e1-b71a-4e94-b9a6-138db6a670f0&#xA;&#xA;**Groundtruth:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/6fb9963f-46a5-4020-b160-f9a004528d7e&#xA;&#xA;**Prompt:** Thunderstorm sounds while raining&#xA;&#xA;**Generated Audio:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/fad8baf3-d80b-4915-ba31-aab13db5ce06&#xA;&#xA;**Groundtruth:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/c01a7e6e-3421-4a28-93c5-831523ec061d&#xA;&#xA;**Prompt:** Birds chirping repeatedly&#xA;&#xA;**Generated Audio:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/0fa673a3-f9de-487b-8812-1f96a335e913&#xA;&#xA;**Groundtruth:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/718289f9-a93e-4ea9-b7db-a14c2b209b28&#xA;&#xA;**Prompt:** Several large bells ring&#xA;&#xA;**Generated Audio:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/362fde84-e4ae-4152-aeb5-4355155c8719&#xA;&#xA;**Groundtruth:**&#xA;&#xA;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/da93e13d-6462-48d2-b6dc-af6ff0c4d07d&#xA;&#xA;--&gt; &#xA;&lt;!-- For more audio demos visit [lumina website - audio demos](https://lumina-t2-x-web.vercel.app/docs/demos/demo-of-audio) --&gt; &#xA;&lt;!-- ### More examples --&gt; &#xA;&lt;!-- For more demos visit [this website](https://lumina-t2-x-web.vercel.app/docs/demos) --&gt; &#xA;&lt;!-- ### High-res. Image Editing&#xA;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/55981976-c989-4f07-982a-1e567c7078ef&#34; width=&#34;90%&#34;/&gt; &#xA; &lt;br&gt;&#xA; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/a1ac7190-c49c-4d8b-965c-9ccf83a4f6a7&#34; width=&#34;90%&#34;/&gt; &#xA;&lt;/p&gt;&#xA;&#xA;### Compositional Generation&#xA;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/8c8eb921-134c-4f55-918a-0ad07f9a47f4&#34; width=&#34;90%&#34;/&gt; &#xA; &lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;### Resolution Extrapolation&#xA;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/e37e2db7-3ead-451e-ba18-b375eb773578&#34; width=&#34;90%&#34;/&gt; &#xA; &lt;br&gt;&#xA; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/9da47c34-5e09-48d3-9c48-78663fd01cc8&#34; width=&#34;100%&#34;/&gt; &#xA;&lt;/p&gt;&#xA;&#xA;### Consistent-Style Generation&#xA;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/6403417a-42c6-4048-9419-375d211e14bb&#34; width=&#34;90%&#34;/&gt; &#xA; &lt;br&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Diverse Configurations&lt;/h2&gt; &#xA;&lt;p&gt;We support diverse configurations, including text encoders, DiTs of different parameter sizes, inference methods, and VAE encoders. Additionally, we offer features such as 1D-RoPE, image enhancement, and more.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Alpha-VLLM/Lumina-T2X/assets/54879512/221de325-d9fb-4b7e-a97c-4b24cd2df0fc&#34; width=&#34;100%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{gao2024luminat2x,&#xA;      title={Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers}, &#xA;      author={Peng Gao and Le Zhuo and Ziyi Lin and Chris Liu and Junsong Chen and Ruoyi Du and Enze Xie and Xu Luo and Longtian Qiu and Yuhang Zhang and Chen Lin and Rongjie Huang and Shijie Geng and Renrui Zhang and Junlin Xi and Wenqi Shao and Zhengkai Jiang and Tianshuo Yang and Weicai Ye and He Tong and Jingwen He and Yu Qiao and Hongsheng Li},&#xA;      journal={arXiv preprint arXiv:2405.05945},&#xA;      year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- &#xA;## Star History&#xA;&#xA; [![Star History Chart](https://api.star-history.com/svg?repos=Alpha-VLLM/Lumina-T2X&amp;type=Date)](https://star-history.com/#Alpha-VLLM/Lumina-T2X&amp;Date) --&gt;</summary>
  </entry>
  <entry>
    <title>LLaVA-VL/LLaVA-NeXT</title>
    <updated>2024-05-14T01:31:47Z</updated>
    <id>tag:github.com,2024-05-14:/LLaVA-VL/LLaVA-NeXT</id>
    <link href="https://github.com/LLaVA-VL/LLaVA-NeXT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LLaVA-NeXT: Open Large Multimodal Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://llava-vl.github.io/blog/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-blog-green&#34; alt=&#34;llava_next-blog&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava-next.lmms-lab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-image_demo-red&#34; alt=&#34;llava_next-demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llavanext-video.lmms-lab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_demo-red&#34; alt=&#34;llava_next-video_demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-6623288e2d61edba3ddbf5ff&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-image_checkpoints-blue&#34; alt=&#34;llava_next-image_checkpoints&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/llava_next-video_checkpoints-blue&#34; alt=&#34;llava_next-video_checkpoints&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Stronger) models are released, with support of stronger LMM inlcuding LLama-3 (8B) and Qwen-1.5 (72B/110B) Check out [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/&#34;&gt;blog&lt;/a&gt;] and [&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-6623288e2d61edba3ddbf5ff&#34;&gt;checkpoints&lt;/a&gt;] to see improved performance!&lt;/li&gt; &#xA; &lt;li&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [&lt;a href=&#34;https://llava-vl.github.io/blog/2024-04-30-llava-next-video/&#34;&gt;Blog&lt;/a&gt;] and [&lt;a href=&#34;https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944&#34;&gt;checkpoints&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;[2024/01/30] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the &lt;a href=&#34;https://llava-vl.github.io/blog/2024-01-30-llava-next/&#34;&gt;blog post&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. Training/eval data and scripts coming soon.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;More&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024/03/10] üî• Releasing &lt;strong&gt;LMMs-Eval&lt;/strong&gt;, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [&lt;a href=&#34;https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/&#34;&gt;Blog&lt;/a&gt;] [&lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;Codebase&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/10] &lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;LLaVA-Plus&lt;/a&gt; is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [&lt;a href=&#34;https://llava-vl.github.io/llava-plus/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavaplus.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Plus-Codebase&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.05437&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/02] &lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;LLaVA-Interactive&lt;/a&gt; is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [&lt;a href=&#34;https://llava-vl.github.io/llava-interactive/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://llavainteractive.ngrok.io/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Interactive-Demo&#34;&gt;Code&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2311.00571&#34;&gt;Paper&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/26] üî• LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (&lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15&#34;&gt;ckpts&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;script&lt;/a&gt;). We also provide a &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md&#34;&gt;doc&lt;/a&gt; on how to finetune LLaVA-1.5 on your own dataset with LoRA.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [&lt;a href=&#34;https://huggingface.co/spaces/etri-vilab/Ko-LLaVA&#34;&gt;ü§ó Demo&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/05] üî• LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the &lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;technical report&lt;/a&gt;, and explore the &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;! Models are available in &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;Model Zoo&lt;/a&gt;. The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project &lt;a href=&#34;https://llava-rlhf.github.io/&#34;&gt;[LLavA-RLHF]&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/22] &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;LLaVA&lt;/a&gt; is accepted by NeurIPS 2023 as &lt;strong&gt;oral presentation&lt;/strong&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;LLaVA-Med&lt;/a&gt; is accepted by NeurIPS 2023 Datasets and Benchmarks Track as &lt;strong&gt;spotlight presentation&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/11/06] Support &lt;strong&gt;Intel&lt;/strong&gt; dGPU and CPU platforms. &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel&#34;&gt;More details here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/12] LLaVA is now supported in &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/pull/3436&#34;&gt;llama.cpp&lt;/a&gt; with 4-bit / 5-bit quantization support!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/11] The training data and scripts of LLaVA-1.5 are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#train&#34;&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/10/10] &lt;a href=&#34;https://blog.roboflow.com/first-impressions-with-llava-1-5/&#34;&gt;Roboflow Deep Dive&lt;/a&gt;: First Impressions with LLaVA-1.5.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2023/09/20] We summarize our empirical study of training 33B and 65B LLaVA models in a &lt;a href=&#34;https://arxiv.org/abs/2309.09958&#34;&gt;note&lt;/a&gt;. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper &lt;a href=&#34;https://arxiv.org/abs/2309.10020&#34;&gt;``Multimodal Foundation Models: From Specialists to General-Purpose Assistants&#39;&#39;.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true&#34; width=&#34;50%/&#34;&gt; &lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;[2023/07/19] üî• We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md&#34;&gt;LLaVA Bench&lt;/a&gt; for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md&#34;&gt;LLaVA-from-LLaMA-2&lt;/a&gt;, and our &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;model zoo&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/26] &lt;a href=&#34;https://vlp-tutorial.github.io/&#34;&gt;CVPR 2023 Tutorial&lt;/a&gt; on &lt;strong&gt;Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4&lt;/strong&gt;! Please check out [&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf&#34;&gt;Slides&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2306.14895&#34;&gt;Notes&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/mkI7EPD1vp8&#34;&gt;YouTube&lt;/a&gt;] [&lt;a href=&#34;https://www.bilibili.com/video/BV1Ng4y1T7v3/&#34;&gt;Bilibli&lt;/a&gt;].&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/inference/docs/LoRA.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/06/01] We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;page&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/06] We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/inference/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/05/02] üî• We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/inference/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;[2023/04/17] üî• We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt; --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage and License Notices&lt;/strong&gt;: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;OpenAI Terms of Use&lt;/a&gt; for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. &lt;a href=&#34;https://ai.meta.com/llama/license/&#34;&gt;Llama-1/2 community license&lt;/a&gt; for LLaMA-2 and Vicuna-v1.5, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/blob/main/LICENSE&#34;&gt;Tongyi Qianwen RESEARCH LICENSE AGREEMENT&lt;/a&gt; and &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;Llama-3 Research License&lt;/a&gt;). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.&lt;/p&gt; &#xA;&lt;h2&gt;Models &amp;amp; Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;1. &lt;strong&gt;Clone this repository and navigate to the LLaVA folder:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LLaVA-VL/LLaVA-NeXT&#xA;cd LLaVA-NeXT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. &lt;strong&gt;Install the inference package:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # Enable PEP 660 support.&#xA;pip install -e &#34;.[train]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Project Navigation&lt;/h3&gt; &#xA;&lt;p&gt;Please checkout the following page for more inference &amp;amp; evaluation details.&lt;/p&gt; &#xA;&lt;h4&gt;- &lt;strong&gt;LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/inference/docs/LLaVA-NeXT.md&#34;&gt;LLaVA-NeXT-Image&lt;/a&gt;: for image demo inference and evaluation of stronger LMMs using &lt;a href=&#34;https://github.com/EvolvingLMMs-Lab/lmms-eval&#34;&gt;lmms-eval&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;- LLaVA-NeXT: A Strong Zero-shot Video Understanding Model&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/inference/docs/LLaVA-NeXT-Video.md&#34;&gt;LLaVA-NeXT-Video&lt;/a&gt;: for video inference and evaluation scripts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find it useful for your research and applications, please cite related papers/blogs using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{li2024llavanext-strong,&#xA;    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},&#xA;    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},&#xA;    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},&#xA;    month={May},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{zhang2024llavanextvideo,&#xA;  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},&#xA;  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},&#xA;  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},&#xA;  month={April},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{liu2024llavanext,&#xA;    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},&#xA;    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},&#xA;    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},&#xA;    month={January},&#xA;    year={2024}&#xA;}&#xA;&#xA;@misc{liu2023improvedllava,&#xA;      title={Improved Baselines with Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},&#xA;      publisher={arXiv:2310.03744},&#xA;      year={2023},&#xA;}&#xA;&#xA;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={NeurIPS},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA; &lt;li&gt;The LLaVA-NeXT project is currently maintained by the team along with our contributors (listed alphabetically by the first names): &lt;a href=&#34;https://brianboli.com/&#34;&gt;Bo Li&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dongguoset/&#34;&gt;Dong Guo&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=ybRe9GcAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate&#34;&gt;Feng Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;amp;hl=en&#34;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg&#34;&gt;Kaichen Zhang&lt;/a&gt;, &lt;a href=&#34;https://zrrskywalker.github.io/&#34;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&#34;https://zhangyuanhan-ai.github.io/&#34;&gt;Yuanhan Zhang&lt;/a&gt;, led by &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li&lt;/a&gt; and with the guidance and help from &lt;a href=&#34;https://hliu.cc/&#34;&gt;Haotian Liu&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;Ôªølmms-eval&lt;/code&gt; framework and its core contributors, including Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, and Kairui Hu, for their support on the evaluation side.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Otter: In-Context Multi-Modal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, please check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>