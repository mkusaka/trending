<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-01T01:42:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ZrrSkywalker/LLaMA-Adapter</title>
    <updated>2023-04-01T01:42:35Z</updated>
    <id>tag:github.com,2023-04-01:/ZrrSkywalker/LLaMA-Adapter</id>
    <link href="https://github.com/ZrrSkywalker/LLaMA-Adapter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fine-tuning LLaMA to follow instructions within 1 Hour and 1.2M Parameters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaMA-Adapter: Efficient Fine-tuning of LLaMA 🚀&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation of &lt;a href=&#34;https://arxiv.org/abs/2303.16199&#34;&gt;&#39;LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention&#39;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repo proposes &lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;, a lightweight adaption method for fine-tuning instruction-following &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; models 🔥, using 52K data provided by &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Efficiency Comparison:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Storage Space&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Training Time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3 Hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA-Adapter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.2M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.7M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1 Hour&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;By inserting adapters into LLaMA&#39;s transformer, our method only introduces &lt;strong&gt;1.2M&lt;/strong&gt; learnable parameters, and turns a LLaMA into an instruction-following model within &lt;strong&gt;1 hour&lt;/strong&gt;. For stablizing training at early stages, we propose a novel &lt;strong&gt;Zero-init Attention&lt;/strong&gt; with zero gating mechanism to adaptively incorporate the instructional signals. After fine-tuning, LLaMA-Adapter can generate high-quality instruction-following sentences, comparable to the fully fine-tuned &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-Lora&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ZrrSkywalker/LLaMA-Adapter/main/pipeline.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Our approach can be simply extended to &lt;strong&gt;Multi-modal Input Instructions&lt;/strong&gt;. The reasoning framework of image-conditioned LLaMA-Adapter for &lt;a href=&#34;https://scienceqa.github.io/&#34;&gt;ScienceQA&lt;/a&gt; is as follows, which is also shared by other modalities, such as audio and video.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ZrrSkywalker/LLaMA-Adapter/main/multimodal.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;TODO&lt;/strong&gt;: training code, multi-modal LLaMA-Adapter, adapters for larger-scale LLaMA models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.16199.pdf&#34;&gt;Paper&lt;/a&gt; is available on arXiv 📌.&lt;/li&gt; &#xA; &lt;li&gt;The generation code of LLaMA-Adapter based on 7B LLaMA has been released.&lt;/li&gt; &#xA; &lt;li&gt;🔥 We are &lt;strong&gt;hiring&lt;/strong&gt; interns, postdocs and full-time researchers in &lt;strong&gt;General Vision Group, Shanghai AI Lab&lt;/strong&gt;, aiming at multi-modality and vision foundation models. If you are interested, please contact &lt;a href=&#34;mailto:gaopeng@pjlab.org.cn&#34;&gt;gaopeng@pjlab.org.cn&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Here is a from-scratch script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n llama_adapter -y&#xA;conda activate llama_adapter&#xA;&#xA;# install pytorch&#xA;conda install pytorch cudatoolkit -c pytorch -y&#xA;&#xA;# install dependency and llama-adapter&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Please request access to the pre-trained LLaMA from &lt;a href=&#34;https://forms.gle/jk851eBVbX1m5TAv5&#34;&gt;this form&lt;/a&gt; (official) or download the LLaMA-7B from &lt;a href=&#34;https://huggingface.co/nyanko7/LLaMA-7B/tree/main&#34;&gt;Hugging Face&lt;/a&gt; (unofficial). Then, obtain the weights of our LLaMA-Adapter from &lt;a href=&#34;https://github.com/ZrrSkywalker/LLaMA-Adapter/releases/download/v.1.0.0/llama_adapter_len10_layer30_release.pth&#34;&gt;here&lt;/a&gt;. We denote the path to the downloaded weights of LLaMA and adapters as &lt;code&gt;TARGET_FOLDER&lt;/code&gt; and &lt;code&gt;ADAPTER_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example to generate instruction-following sentences with 7B LLaMA model and our LLaMA-Adapter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node 1 example.py \&#xA;         --ckpt_dir $TARGET_FOLDER/model_size\&#xA;         --tokenizer_path $TARGET_FOLDER/tokenizer.model \&#xA;         --adapter_path $ADAPTER_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;We will release the training code soon. We use 52K instruction-following &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json&#34;&gt;data&lt;/a&gt; from &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; for training, and adopt the hyper-parameters as following:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;hyper-parameter&lt;/th&gt; &#xA;   &lt;th&gt;value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;batch Size&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;learning rate&lt;/td&gt; &#xA;   &lt;td&gt;9e-3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;warmup epochs&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;epochs&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;weight decay&lt;/td&gt; &#xA;   &lt;td&gt;0.02&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Comparison with Other Methods&lt;/h2&gt; &#xA;&lt;h3&gt;Instruction 1:&lt;/h3&gt; &#xA;&lt;p&gt;Tell me about alpacas.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are typically found in herds of 10-20 animals and are known for their gentle nature and their ability to thrive in harsh climates. Alpacas are typically white or grey in color and have long, silky coats. They are also known for their ability to produce high-quality wool, which is used in the production of clothing and other products.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alpacas are members of the camelid family and are native to the Andes Mountains of South America. They are known for their soft, luxurious fleece, which is used to make clothing, blankets, and other items. Alpacas are herbivores and graze on grasses and other plants. They are social animals and live in herds of up to 20 individuals.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alpacas are small, fluffy animals related to camels and llamas. They are native to Peru and Bolivia, and were first domesticated around 5,000 years ago. They are kept mainly for their fine, soft fleece, which is used to make knitwear and other garments. Alpacas are herd animals and live in small family groups, led by an older male. They are highly social animals and can form strong bonds with their owners.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: Alpacas are a domesticated species of South American camelid, similar to llamas. They are native to the Andes Mountains in South America and are kept as livestock for their soft, luxurious wool. Alpacas are social animals and live in herds of up to 20 individuals. They are typically kept in small herds of two to five animals, and are relatively easy to care for. Alpacas are herbivores and feed on grass, hay, and other vegetation. They are also known for their gentle and friendly nature, making them popular as pets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 2:&lt;/h3&gt; &#xA;&lt;p&gt;Tell me about the president of Mexico in 2019.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: The president of Mexico in 2019 was Andrés Manuel López Obrador, who took office on December 1st, 2018. He is a member of the left-wing party Morena and is known for his populist policies. He has focused on tackling corruption, reducing poverty, and improving the country&#39;s infrastructure. He has also been a vocal critic of the United States&#39; immigration policies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The president of Mexico in 2019 was Andrés Manuel López Obrador, who took office on December 1, 2018. He is a member of the National Regeneration Movement (MORENA) political party and is the first left-wing president of Mexico since 1946. He is known for his anti-corruption and anti-neoliberal policies, as well as his commitment to improving the living conditions of the Mexican people.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, Andres Manuel Lopez Obrador was sworn in as President of Mexico. He is the country’s 58th president and the first from the left-wing Morena party. He was elected in 2018 on a promise to combat corruption and reduce poverty and has faced criticism for his handling of the economy, crime and the country’s relationship with the United States. He is also known for his ambitious social welfare programs and has been praised for raising the minimum wage and providing aid to low-income families. He is seen as a champion of democracy and human rights in Mexico.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The president of Mexico in 2019 is Andrés Manuel López Obrador, commonly known as AMLO. He is a Mexican politician who has been the President of Mexico since December 1, 2018. He is a member of the National Regeneration Movement (MORENA) party and previously served as the Head of Government of Mexico City from 2000 to 2005. He is a left-wing populist and has promised to reduce corruption and inequality in Mexico. He has also promised to increase public spending on social programs and infrastructure projects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 3:&lt;/h3&gt; &#xA;&lt;p&gt;Tell me about the king of France in 2019.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: The king of France in 2019 is King Louis XIV, who has been in power since 2015. He is the 35th monarch of the House of Bourbon and is the longest-serving monarch in French history. He is known for his lavish lifestyle and his ambitious building projects, including the Palace of Versailles. He is also credited with establishing the Code Noir, which granted slaves in the French colonies more rights than they had previously enjoyed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: The king of France in 2019 is Emmanuel Macron, who was elected in 2017. He is the youngest president in the history of France, and has been credited with bringing about a number of reforms, including a new tax system and a crackdown on corruption. He has also been praised for his efforts to strengthen the country&#39;s relationship with the European Union.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: In 2019, the King of France was Emmanuel Macron. He was sworn in as President of France on May 14, 2017, and has been leading the country ever since. He is a member of the centrist La République En Marche! Party. He is known for his reformist agenda and has faced a number of protests from both the right and left of the political spectrum. He is the youngest French president in history.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: The King of France is a largely symbolic role, as France is a republic. The current President of France is Emmanuel Macron, who was elected in 2017.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 4:&lt;/h3&gt; &#xA;&lt;p&gt;List all Canadian provinces in alphabetical order.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan, Yukon.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Nova Scotia, Ontario, Prince Edward Island, Quebec, Saskatchewan, Northwest Territories, Nunavut, Yukon.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Alberta&lt;/li&gt; &#xA;  &lt;li&gt;British Columbia&lt;/li&gt; &#xA;  &lt;li&gt;Manitoba&lt;/li&gt; &#xA;  &lt;li&gt;New Brunswick&lt;/li&gt; &#xA;  &lt;li&gt;Newfoundland and Labrador&lt;/li&gt; &#xA;  &lt;li&gt;Northwest Territories&lt;/li&gt; &#xA;  &lt;li&gt;Nova Scotia&lt;/li&gt; &#xA;  &lt;li&gt;Nunavut&lt;/li&gt; &#xA;  &lt;li&gt;Ontario&lt;/li&gt; &#xA;  &lt;li&gt;Prince Edward Island&lt;/li&gt; &#xA;  &lt;li&gt;Quebec&lt;/li&gt; &#xA;  &lt;li&gt;Saskatchewan&lt;/li&gt; &#xA;  &lt;li&gt;Yukon&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 5:&lt;/h3&gt; &#xA;&lt;p&gt;Write a Python program that prints the first 10 Fibonacci numbers.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fibonacci():&#xA;    a = 0&#xA;    b = 1&#xA;    for i in range(10):&#xA;        a, b = b, a + b&#xA;        print(a, b)&#xA;    print(&#34;Fibonacci numbers: &#34;, a, b)&#xA;&#xA;fibonacci()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def fibonacci(n):&#xA;    if n == 0:&#xA;        return 0&#xA;    elif n == 1:&#xA;        return 1&#xA;    else:&#xA;        return fibonacci(n-1) + fibonacci(n-2)&#xA;&#xA;print(fibonacci(10))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numbers = [0, 1]&#xA;&#xA;for i in range(2, 11):&#xA;    numbers.append(numbers[i-2] + numbers[i-1])&#xA;&#xA;print(numbers)&#xA;# Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fibonacci numbers are the numbers in the following integer sequence, called the Fibonacci sequence, and characterized by the fact that every number after the first two is the sum of the two preceding ones.&#xA;&#xA;# Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...&#xA;&#xA;# Program:&#xA;&#xA;# Initialize the first two numbers of the Fibonacci sequence&#xA;a = 0&#xA;b = 1&#xA;&#xA;# Print the first 10 numbers of the Fibonacci sequence&#xA;print(a)&#xA;print(b)&#xA;&#xA;for i in range(2,10):&#xA;    c = a + b&#xA;    a = b&#xA;    b = c&#xA;    print(c)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 6:&lt;/h3&gt; &#xA;&lt;p&gt;Write a program that prints the numbers from 1 to 100. But for multiples of three print &#39;Fizz&#39; instead of the number and for the multiples of five print &#39;Buzz&#39;. For numbers which are multiples of both three and five print &#39;FizzBuzz&#39;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def print_numbers(n):&#xA;    for i in range(1, 101):&#xA;        if i % 3 == 0 and i % 5 == 0:&#xA;            print(&#34;FizzBuzz&#34;)&#xA;        elif i % 3 == 0:&#xA;            print(&#34;Fizz&#34;)&#xA;        elif i % 5 == 0:&#xA;            print(&#34;Buzz&#34;)&#xA;        else:&#xA;            print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#34;FizzBuzz&#34;)&#xA;    elif i % 3 == 0:&#xA;        print(&#34;Fizz&#34;)&#xA;    elif i % 5 == 0:&#xA;        print(&#34;Buzz&#34;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1, 101):&#xA;    if i % 3 == 0 and i % 5 == 0:&#xA;        print(&#39;FizzBuzz&#39;)&#xA;    elif i % 3 == 0:&#xA;        print(&#39;Fizz&#39;)&#xA;    elif i % 5 == 0:&#xA;        print(&#39;Buzz&#39;)&#xA;    else:&#xA;        print(i)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;for (let i = 1; i &amp;lt;= 100; i++) {&#xA;  if (i % 3 === 0 &amp;amp;&amp;amp; i % 5 === 0) {&#xA;    console.log(&#34;FizzBuzz&#34;);&#xA;  } else if (i % 3 === 0) {&#xA;    console.log(&#34;Fizz&#34;);&#xA;  } else if (i % 5 === 0) {&#xA;    console.log(&#34;Buzz&#34;);&#xA;  } else {&#xA;    console.log(i);&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 7:&lt;/h3&gt; &#xA;&lt;p&gt;Tell me five words that rhyme with &#39;shock&#39;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: Flock, rock, lock, cook, and stock.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: Flock, clock, dock, stock, and sock.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: Five words that rhyme with shock are: rock, pop, shock, cook, and snock.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Block&lt;/li&gt; &#xA;  &lt;li&gt;Lock&lt;/li&gt; &#xA;  &lt;li&gt;Dock&lt;/li&gt; &#xA;  &lt;li&gt;Clock&lt;/li&gt; &#xA;  &lt;li&gt;Rock&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 8:&lt;/h3&gt; &#xA;&lt;p&gt;Translate the sentence &#39;I have no mouth but I must scream&#39; into Spanish.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: No tengo boca pero tengo que gritar.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;OTHER METHODS&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alpaca-LoRA&lt;/strong&gt;: No tengo boca pero tengo que gritar.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stanford Alpaca&lt;/strong&gt;: No tengo boca, pero debo gritar.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;text-davinci-003&lt;/strong&gt;: No tengo boca pero debo gritar.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;More interesting examples&lt;/h2&gt; &#xA;&lt;h3&gt;Instruction 1:&lt;/h3&gt; &#xA;&lt;p&gt;Write an introduction for LLaMA-Adapter, a lightweight and simple adapter for large language model LLaMA.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: LLaMA-Adapter is a lightweight and simple adapter for large language models (LLaMA). It is designed to be used as a drop-in replacement for the standard LLaMA adapter, and is designed to be faster, more efficient, and more flexible. It is capable of handling a variety of different input and output formats, and can be easily configured to adapt to different models and datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 2:&lt;/h3&gt; &#xA;&lt;p&gt;Write a Python object detection inference program with Faster R-CNN algorithm.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import os&#xA;import glob&#xA;import cv2&#xA;import tensorflow as tf&#xA;&#xA;# load dataset&#xA;dataset_dir = os.path.join(os.path.dirname(__file__), &#39;dataset&#39;)&#xA;dataset_path = glob.glob(dataset_dir + &#39;/*.jpg&#39;)&#xA;&#xA;# define model&#xA;model = tf.keras.models.load_model(&#39;model.h5&#39;)&#xA;&#xA;# input images&#xA;images = []&#xA;for image in dataset_path:&#xA;    images.append(cv2.imread(image))&#xA;&#xA;# create a numpy array&#xA;images_np = np.array(images)&#xA;&#xA;# create a batch of images&#xA;batch_size = 1&#xA;batches = np.reshape(images_np, (batch_size, 1, 3, 32, 32))&#xA;&#xA;# create a dataset&#xA;dataset = tf.data.Dataset.from_tensor_slices(batches)&#xA;&#xA;# define model&#xA;model.fit(dataset, steps=100)&#xA;&#xA;# detect objects&#xA;detected_objects = model.predict(images_np)&#xA;&#xA;# print detected objects&#xA;for obj in detected_objects:&#xA;    print(obj)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 3:&lt;/h3&gt; &#xA;&lt;p&gt;Compose the lyrics of a song in the style of Jay Chou.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;I&#39;m a lonely soul who&#39;s searching for a home,&#xA;Looking for someone to make me feel at home.&#xA;&#xA;I&#39;m a lonely soul who&#39;s searching for a place,&#xA;Looking for someone to make me feel at peace.&#xA;&#xA;I&#39;m a lonely soul who&#39;s searching for a love,&#xA;Looking for someone to make me feel alive.&#xA;&#xA;I&#39;m a lonely soul who&#39;s searching for a friend,&#xA;Looking for someone to make me feel complete.&#xA;&#xA;I&#39;m a lonely soul who&#39;s searching for a dream,&#xA;Looking for someone to make me feel free.&#xA;&#xA;I&#39;m a lonely soul who&#39;s searching for a life,&#xA;Looking for someone to make me feel alive.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 4:&lt;/h3&gt; &#xA;&lt;p&gt;Please help me arrange a one week travel route in Bangkok.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Day 1: Explore the Grand Palace and Wat Pho, visit Chinatown and Khao San Road.&#xA;&#xA;Day 2: Take a boat ride along the Chao Phraya River, visit the National Museum and the Jim Thompson House.&#xA;&#xA;Day 3: Explore the Bangkok National Museum and the National Gallery, visit the Temple of the Reclining Buddha and the Dusit Zoo.&#xA;&#xA;Day 4: Visit the Wat Arun, the Royal Barges Museum, and the Siam Paragon Shopping Mall.&#xA;&#xA;Day 5: Explore the Chatuchak Weekend Market, the MBK Center, and the Suan Lum Ratcha Tep.&#xA;&#xA;Day 6: Visit the Floating Market, the Royal Palace, and the Khao San Road.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Instruction 5:&lt;/h3&gt; &#xA;&lt;p&gt;Explain why this poem is good.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Life, thin and light-off time and time again. Frivolous tireless. I heard the echo, from the valleys and the heart. Open to the lonely soul of sickle harvesting. Repeat outrightly, but also repeat the well-being of eventually swaying in the desert oasis. I believe I am. Born as the bright summer flowers.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLaMA-Adapter&lt;/strong&gt;: This poem is good because it uses simple language to convey a powerful message. It explores the themes of loneliness and isolation, and captures the feelings of alienation and despair that many people experience. The poem also uses imagery to create a vivid picture of the emotions being described.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=_go6DPsAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&#34;https://csuhan.com&#34;&gt;Jiaming Han&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=YlL3xN4AAAAJ&amp;amp;hl=zh-CN&#34;&gt;Renrui Zhang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our LLaMA-Adapter code and paper useful, please kindly cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{llamaadapter2023,&#xA;  title = {LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention},&#xA;  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao Yu},&#xA;  journal={arXiv preprint arXiv:2303.16199},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, and &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-Lora&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vocodedev/vocode-python</title>
    <updated>2023-04-01T01:42:35Z</updated>
    <id>tag:github.com,2023-04-01:/vocodedev/vocode-python</id>
    <link href="https://github.com/vocodedev/vocode-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🤖 Build voice-based LLM agents. Modular + open source.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6234599/228337850-e32bb01d-3701-47ef-a433-3221c9e0e56e.png&#34; alt=&#34;Hero&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://twitter.com/vocodehq&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/vocodehq.svg?style=social&amp;amp;label=Follow%20%40vocodehq&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/vocodedev/vocode-python&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/vocodedev/vocode-python?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/NaU4mMgcnC&#34;&gt;Community&lt;/a&gt; | &lt;a href=&#34;https://docs.vocode.dev&#34;&gt;Docs&lt;/a&gt; | &lt;a href=&#34;https://app.vocode.dev&#34;&gt;Dashboard&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;&lt;span&gt;&lt;img style=&#34;vertical-align:middle; display:inline;&#34; src=&#34;https://user-images.githubusercontent.com/6234599/228339858-95a0873a-2d40-4542-963a-6358d19086f5.svg?sanitize=true&#34; width=&#34;5%&#34; height=&#34;5%&#34;&gt;&amp;nbsp; vocode&lt;/span&gt;&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Build voice-based LLM apps in minutes&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Vocode is an open source library that makes it easy to build voice-based LLM apps. Using Vocode, you can build real-time streaming conversations with LLMs and deploy them to phone calls, Zoom meetings, and more. You can also build personal assistants or apps like voice-based chess. Vocode provides easy abstractions and integrations so that everything you need is in a single library.&lt;/p&gt; &#xA;&lt;h1&gt;⭐️ Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🗣 &lt;a href=&#34;https://docs.vocode.dev/python-quickstart&#34;&gt;Spin up a conversation with your system audio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;➡️ 📞 &lt;a href=&#34;https://docs.vocode.dev/telephony#inbound-calls&#34;&gt;Set up a phone number that responds with a LLM-based agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;📞 ➡️ &lt;a href=&#34;https://docs.vocode.dev/telephony#outbound-calls&#34;&gt;Send out phone calls from your phone number managed by an LLM-based agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🧑‍💻 &lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/vocode/streaming/telephony/hosted/zoom_dial_in.py&#34;&gt;Dial into a Zoom call&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Out of the box integrations with: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Transcription services, including: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://deepgram.com/&#34;&gt;Deepgram&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.assemblyai.com/&#34;&gt;AssemblyAI&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://cloud.google.com/speech-to-text&#34;&gt;Google Cloud&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/introducing-chatgpt-and-whisper-apis&#34;&gt;Whisper&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;LLMs, including: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/chatgpt&#34;&gt;ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/models/gpt-4&#34;&gt;GPT-4&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.anthropic.com/&#34;&gt;Anthropic&lt;/a&gt; - coming soon!&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Synthesis services, including: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/products/cognitive-services/text-to-speech/&#34;&gt;Microsoft Azure&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://cloud.google.com/text-to-speech&#34;&gt;Google Cloud&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://elevenlabs.io/&#34;&gt;Eleven Labs&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out our React SDK &lt;a href=&#34;https://github.com/vocodedev/vocode-react-sdk&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;🫂 Contribution&lt;/h1&gt; &#xA;&lt;p&gt;We&#39;d love for you all to build on top of our abstractions to enable new and better LLM voice applications!&lt;/p&gt; &#xA;&lt;p&gt;You can extend our &lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/vocode/streaming/agent/base_agent.py&#34;&gt;&lt;code&gt;BaseAgent&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/vocode/streaming/transcriber/base_transcriber.py&#34;&gt;&lt;code&gt;BaseTranscriber&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/vocode/streaming/synthesizer/base_synthesizer.py&#34;&gt;&lt;code&gt;BaseSynthesizer&lt;/code&gt;&lt;/a&gt; abstractions to integrate with new LLM APIs, speech recognition and speech synthesis providers. More detail &lt;a href=&#34;https://docs.vocode.dev/create-your-own-agent#self-hosted&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also work with our &lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/vocode/streaming/input_device/base_input_device.py&#34;&gt;&lt;code&gt;BaseInputDevice&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/vocode/streaming/output_device/base_output_device.py&#34;&gt;&lt;code&gt;BaseOutputDevice&lt;/code&gt;&lt;/a&gt; abstractions to set up voice applications on new surfaces/platforms. More guides for this coming soon!&lt;/p&gt; &#xA;&lt;p&gt;Because our &lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/vocode/streaming/streaming_conversation.py&#34;&gt;&lt;code&gt;StreamingConversation&lt;/code&gt;&lt;/a&gt; runs locally, it&#39;s relatively quick to develop on! Feel free to fork and create a PR and we will get it merged as soon as possible. And we&#39;d love to talk to you on &lt;a href=&#34;https://discord.gg/NaU4mMgcnC&#34;&gt;Discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;🚀 Quickstart (Self-hosted)&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#39;vocode[io]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;import signal&#xA;&#xA;import vocode&#xA;from vocode.streaming.streaming_conversation import StreamingConversation&#xA;from vocode.helpers import create_microphone_input_and_speaker_output&#xA;from vocode.streaming.models.transcriber import (&#xA;    DeepgramTranscriberConfig,&#xA;    PunctuationEndpointingConfig,&#xA;)&#xA;from vocode.streaming.agent.chat_gpt_agent import ChatGPTAgent&#xA;from vocode.streaming.models.agent import ChatGPTAgentConfig&#xA;from vocode.streaming.models.message import BaseMessage&#xA;from vocode.streaming.models.synthesizer import AzureSynthesizerConfig&#xA;from vocode.streaming.synthesizer.azure_synthesizer import AzureSynthesizer&#xA;from vocode.streaming.transcriber.deepgram_transcriber import DeepgramTranscriber&#xA;&#xA;# these can also be set as environment variables&#xA;vocode.setenv(&#xA;    OPENAI_API_KEY=&#34;&amp;lt;your OpenAI key&amp;gt;&#34;,&#xA;    DEEPGRAM_API_KEY=&#34;&amp;lt;your Deepgram key&amp;gt;&#34;,&#xA;    AZURE_SPEECH_KEY=&#34;&amp;lt;your Azure key&amp;gt;&#34;,&#xA;    AZURE_SPEECH_REGION=&#34;&amp;lt;your Azure region&amp;gt;&#34;,&#xA;)&#xA;&#xA;&#xA;async def main():&#xA;    microphone_input, speaker_output = create_microphone_input_and_speaker_output(&#xA;        streaming=True, use_default_devices=False&#xA;    )&#xA;&#xA;    conversation = StreamingConversation(&#xA;        output_device=speaker_output,&#xA;        transcriber=DeepgramTranscriber(&#xA;            DeepgramTranscriberConfig.from_input_device(&#xA;                microphone_input, endpointing_config=PunctuationEndpointingConfig()&#xA;            )&#xA;        ),&#xA;        agent=ChatGPTAgent(&#xA;            ChatGPTAgentConfig(&#xA;                initial_message=BaseMessage(text=&#34;Hello!&#34;),&#xA;                prompt_preamble=&#34;Have a pleasant conversation about life&#34;,&#xA;            ),&#xA;        ),&#xA;        synthesizer=AzureSynthesizer(&#xA;            AzureSynthesizerConfig.from_output_device(speaker_output)&#xA;        ),&#xA;    )&#xA;    await conversation.start()&#xA;    print(&#34;Conversation started, press Ctrl+C to end&#34;)&#xA;    signal.signal(signal.SIGINT, lambda _0, _1: conversation.terminate())&#xA;    while conversation.is_active():&#xA;        chunk = microphone_input.get_audio()&#xA;        if chunk:&#xA;            conversation.receive_audio(chunk)&#xA;        await asyncio.sleep(0)&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;☁️ Quickstart (Hosted)&lt;/h1&gt; &#xA;&lt;p&gt;First, get a &lt;em&gt;free&lt;/em&gt; API key from our &lt;a href=&#34;https://app.vocode.dev&#34;&gt;dashboard&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#39;vocode[io]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;import signal&#xA;&#xA;import vocode&#xA;from vocode.streaming.hosted_streaming_conversation import HostedStreamingConversation&#xA;from vocode.streaming.streaming_conversation import StreamingConversation&#xA;from vocode.helpers import create_microphone_input_and_speaker_output&#xA;from vocode.streaming.models.transcriber import (&#xA;    DeepgramTranscriberConfig,&#xA;    PunctuationEndpointingConfig,&#xA;)&#xA;from vocode.streaming.models.agent import ChatGPTAgentConfig&#xA;from vocode.streaming.models.message import BaseMessage&#xA;from vocode.streaming.models.synthesizer import AzureSynthesizerConfig&#xA;&#xA;vocode.api_key = &#34;&amp;lt;your API key&amp;gt;&#34;&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    microphone_input, speaker_output = create_microphone_input_and_speaker_output(&#xA;        streaming=True, use_default_devices=False&#xA;    )&#xA;&#xA;    conversation = HostedStreamingConversation(&#xA;        input_device=microphone_input,&#xA;        output_device=speaker_output,&#xA;        transcriber_config=DeepgramTranscriberConfig.from_input_device(&#xA;            microphone_input,&#xA;            endpointing_config=PunctuationEndpointingConfig(),&#xA;        ),&#xA;        agent_config=ChatGPTAgentConfig(&#xA;            initial_message=BaseMessage(text=&#34;Hello!&#34;),&#xA;            prompt_preamble=&#34;Have a pleasant conversation about life&#34;,&#xA;        ),&#xA;        synthesizer_config=AzureSynthesizerConfig.from_output_device(speaker_output),&#xA;    )&#xA;    signal.signal(signal.SIGINT, lambda _0, _1: conversation.deactivate())&#xA;    asyncio.run(conversation.start())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;📞 Phone call quickstarts&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.vocode.dev/telephony#inbound-calls&#34;&gt;Inbound calls - Hosted&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.vocode.dev/telephony#outbound-calls&#34;&gt;Outbound calls - Hosted&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vocodedev/vocode-python/raw/main/examples/telephony_app.py&#34;&gt;Telephony Server - Self-hosted&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;🌱 Documentation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.vocode.dev/&#34;&gt;docs.vocode.dev&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>plasma-umass/ChatDBG</title>
    <updated>2023-04-01T01:42:35Z</updated>
    <id>tag:github.com,2023-04-01:/plasma-umass/ChatDBG</id>
    <link href="https://github.com/plasma-umass/ChatDBG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ChatDBG - AI-assisted debugging. Uses AI to answer &#39;why&#39;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatDBG&lt;/h1&gt; &#xA;&lt;p&gt;by &lt;a href=&#34;https://emeryberger.com&#34;&gt;Emery Berger&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ChatDBG is an experimental debugger for Python &lt;em&gt;and&lt;/em&gt; native C/C++ code that integrates large language models into a standard debugger (&lt;code&gt;pdb&lt;/code&gt;, &lt;code&gt;lldb&lt;/code&gt;, and &lt;code&gt;gdb&lt;/code&gt;) to help debug your code. With ChatDBG, you can ask your debugger &#34;why&#34; your program failed, and it will provide a suggested fix.&lt;/p&gt; &#xA;&lt;p&gt;As far as we are aware, ChatDBG is the &lt;em&gt;first&lt;/em&gt; debugger to automatically perform root cause analysis and to provide suggested fixes. This is an alpha release; we greatly welcome feedback and suggestions!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/chatdbg/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/chatdbg.svg?sanitize=true&#34; alt=&#34;PyPI Latest Release&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://pepy.tech/project/chatdbg&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/chatdbg&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/chatdbg&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/chatdbg/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/chatdbg.svg?style=flat-square&#34; alt=&#34;Python versions&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install ChatDBG using &lt;code&gt;pip&lt;/code&gt; (you need to do this whether you are debugging Python, C, or C++ code):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m pip install chatdbg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using ChatDBG to debug Python programs, you are done. If you want to use ChatDBG to debug native code with &lt;code&gt;gdb&lt;/code&gt; or &lt;code&gt;lldb&lt;/code&gt;, follow the installation instructions below.&lt;/p&gt; &#xA;&lt;h3&gt;Installing as an &lt;tt&gt;lldb&lt;/tt&gt; extension&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;&lt;tt&gt;lldb&lt;/tt&gt; installation instructions&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p&gt;Install ChatDBG into the &lt;code&gt;lldb&lt;/code&gt; debugger by running the following command:&lt;/p&gt; &#xA; &lt;h4&gt;Linux&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code&gt;python3 -m pip install ChatDBG&#xA;python3 -c &#39;import chatdbg; print(f&#34;command script import {chatdbg.__path__[0]}/chatdbg_lldb.py&#34;)&#39; &amp;gt;&amp;gt; ~/.lldbinit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Mac&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code&gt;xcrun python3 -m pip install ChatDBG&#xA;xcrun python3 -c &#39;import chatdbg; print(f&#34;command script import {chatdbg.__path__[0]}/chatdbg_lldb.py&#34;)&#39; &amp;gt;&amp;gt; ~/.lldbinit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This will install ChatDBG as an LLVM extension.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Installing as a &lt;tt&gt;gdb&lt;/tt&gt; extension&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;&lt;tt&gt;gdb&lt;/tt&gt; installation instructions&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;p&gt;Install ChatDBG into the &lt;code&gt;gdb&lt;/code&gt; debugger by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python3 -m pip install ChatDBG&#xA;python3 -c &#39;import chatdbg; print(f&#34;source {chatdbg.__path__[0]}/chatdbg_gdb.py&#34;)&#39; &amp;gt;&amp;gt; ~/.gdbinit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This will install ChatDBG as a GDB extension.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Debugging Python&lt;/h3&gt; &#xA;&lt;p&gt;To use ChatDBG to debug Python programs, simply run your Python script with the &lt;code&gt;-m&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m chatdbg -c continue yourscript.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or just&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;chatdbg -c continue yourscript.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ChatDBG is an extension of the standard Python debugger &lt;code&gt;pdb&lt;/code&gt;. Like &lt;code&gt;pdb&lt;/code&gt;, when your script encounters an uncaught exception, ChatDBG will enter post mortem debugging mode.&lt;/p&gt; &#xA;&lt;p&gt;Unlike other debuggers, you can then use the &lt;code&gt;why&lt;/code&gt; command to ask ChatDBG why your program failed and get a suggested fix.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;ChatDBG example with Python&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &#34;yourscript.py&#34;, line 9, in &amp;lt;module&amp;gt;&#xA;    print(tryme(100))&#xA;  File &#34;yourscript.py&#34;, line 4, in tryme&#xA;    if x / i &amp;gt; 2:&#xA;ZeroDivisionError: division by zero&#xA;Uncaught exception. Entering post mortem debugging&#xA;Running &#39;cont&#39; or &#39;step&#39; will restart the program&#xA;&amp;gt; yourscript.py(4)tryme()&#xA;-&amp;gt; if x / i &amp;gt; 2:&#xA;(ChatDBG Pdb) why&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;ChatDBG will then provide a helpful explanation of why your program failed and a suggested fix:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;The root cause of the error is that the code is attempting to&#xA;divide by zero in the line &#34;if x / i &amp;gt; 2&#34;. As i ranges from 0 to 99,&#xA;it will eventually reach the value of 0, causing a ZeroDivisionError.&#xA;&#xA;A possible fix for this would be to add a check for i being equal to&#xA;zero before performing the division. This could be done by adding an&#xA;additional conditional statement, such as &#34;if i == 0: continue&#34;, to&#xA;skip over the iteration when i is zero. The updated code would look&#xA;like this:&#xA;&#xA;def tryme(x):&#xA;    count = 0&#xA;    for i in range(100):&#xA;        if i == 0:&#xA;            continue&#xA;        if x / i &amp;gt; 2:&#xA;            count += 1&#xA;    return count&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    print(tryme(100))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Debugging native code (&lt;tt&gt;lldb&lt;/tt&gt; / &lt;tt&gt;gdb&lt;/tt&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;To use ChatDBG with &lt;code&gt;lldb&lt;/code&gt; or &lt;code&gt;gdb&lt;/code&gt;, just run native code (compiled with &lt;code&gt;-g&lt;/code&gt; for debugging symbols) with your choice of debugger; when it crashes, ask &lt;code&gt;why&lt;/code&gt;. This also works for post mortem debugging (when you load a core with the &lt;code&gt;-c&lt;/code&gt; option).&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;Example of using &lt;tt&gt;why&lt;/tt&gt; in &lt;tt&gt;lldb&lt;/tt&gt;&lt;/b&gt; &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;(ChatDBG lldb) run&#xA;Process 85494 launched: &#39;/Users/emery/git/ChatDBG/test/a.out&#39; (arm64)&#xA;TEST 1&#xA;TEST -422761288&#xA;TEST 0&#xA;TEST 0&#xA;TEST 0&#xA;TEST 0&#xA;TEST 0&#xA;TEST 0&#xA;Process 85494 stopped&#xA;* thread #1, queue = &#39;com.apple.main-thread&#39;, stop reason = EXC_BAD_ACCESS (code=1, address=0x100056200)&#xA;    frame #0: 0x0000000100002f64 a.out`foo(n=8, b=1) at test.cpp:7:22&#xA;   4     int x[] = { 1, 2, 3, 4, 5 };&#xA;   5     &#xA;   6     void foo(int n, float b) {&#xA;-&amp;gt; 7       cout &amp;lt;&amp;lt; &#34;TEST &#34; &amp;lt;&amp;lt; x[n * 10000] &amp;lt;&amp;lt; endl;&#xA;   8     }&#xA;   9     &#xA;   10    int main()&#xA;Target 0: (a.out) stopped.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Now you can ask &lt;code&gt;why&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;(ChatDBG lldb) why&#xA;The root cause of this error is accessing an index of the array `x`&#xA;that is out of bounds. In `foo()`, the index is calculated as `n *&#xA;10000`, which can be much larger than the size of the array `x` (which&#xA;is only 5 elements). In the given trace, the program is trying to&#xA;access the memory address `0x100056200`, which is outside of the range&#xA;of allocated memory for the array `x`.&#xA;&#xA;To fix this error, we need to ensure that the index is within the&#xA;bounds of the array. One way to do this is to check the value of `n`&#xA;before calculating the index, and ensure that it is less than the size&#xA;of the array divided by the size of the element. For example, we can&#xA;modify `foo()` as follows:&#xA;&#xA;    ```&#xA;    void foo(int n, float b) {&#xA;      if (n &amp;lt; 0 || n &amp;gt;= sizeof(x)/sizeof(x[0])) {&#xA;        cout &amp;lt;&amp;lt; &#34;ERROR: Invalid index&#34; &amp;lt;&amp;lt; endl;&#xA;        return;&#xA;      }&#xA;      cout &amp;lt;&amp;lt; &#34;TEST &#34; &amp;lt;&amp;lt; x[n] &amp;lt;&amp;lt; endl;&#xA;    }&#xA;    ```&#xA;&#xA;This code checks if `n` is within the valid range, and prints an error&#xA;message if it is not. If `n` is within the range, the function prints&#xA;the value of the element at index `n` of `x`. With this modification,&#xA;the program will avoid accessing memory outside the bounds of the&#xA;array, and will print the expected output for valid indices.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
</feed>