<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-15T09:02:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>linyiLYi/bilibot</title>
    <updated>2024-05-15T09:02:32Z</updated>
    <id>tag:github.com,2024-05-15:/linyiLYi/bilibot</id>
    <link href="https://github.com/linyiLYi/bilibot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A local chatbot fine-tuned by bilibili user comments.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;哔哩哔哩聊天机器人&lt;/h1&gt; &#xA;&lt;p&gt;由&lt;a href=&#34;https://bilibili.com&#34;&gt;哔哩哔哩&lt;/a&gt;用户评论微调训练而成的本地聊天机器人。支持文字聊天，也可以通过 questions.txt 生成针对给定问题的语音对话。&lt;/p&gt; &#xA;&lt;p&gt;本项目文字生成使用的基础模型为 &lt;a href=&#34;https://huggingface.co/Qwen/Qwen1.5-32B-Chat&#34;&gt;Qwen1.5-32B-Chat&lt;/a&gt;，借助苹果 &lt;a href=&#34;https://github.com/ml-explore/mlx-examples/raw/main/llms/mlx_lm/LORA.md&#34;&gt;mlx-lm LORA 示例项目&lt;/a&gt; 对基础模型进行微调训练。语音生成部分基于开源项目 &lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt;，问题语音来自 B 站用户&lt;a href=&#34;https://space.bilibili.com/518098961&#34;&gt;白菜工厂1145号员工&lt;/a&gt;训练的派蒙语音模型。&lt;/p&gt; &#xA;&lt;h3&gt;文件结构&lt;/h3&gt; &#xA;&lt;p&gt;项目主要脚本存放在 &lt;code&gt;main/&lt;/code&gt; 文件夹下，模型存放于 &lt;code&gt;models/&lt;/code&gt; 文件夹。提示词模板、问题列表存放在 &lt;code&gt;text/&lt;/code&gt; 文件夹下。&lt;code&gt;tools/compress_model.py&lt;/code&gt; 可以对完整模型进行量化压缩，大大加快模型内容生成速度。&lt;/p&gt; &#xA;&lt;h2&gt;运行指南&lt;/h2&gt; &#xA;&lt;p&gt;本项目基于 Python 编程语言，程序运行使用的 Python 版本为 3.10，建议使用 &lt;a href=&#34;https://www.anaconda.com&#34;&gt;Anaconda&lt;/a&gt; 配置 Python 环境。以下配置过程已在 macOS 系统测试通过。&lt;/p&gt; &#xA;&lt;h3&gt;配置环境&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n bilibot python=3.10&#xA;conda activate bilibot&#xA;cd bilibot&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;模型微调训练与推理测试&lt;/h3&gt; &#xA;&lt;p&gt;使用控制台指令，借助 &lt;a href=&#34;https://github.com/ml-explore/mlx-examples/raw/main/llms/mlx_lm/LORA.md&#34;&gt;mlx-lm&lt;/a&gt; 对 Qwen1.5-32B-Chat 进行微调：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m mlx_lm.lora --model models/Qwen1.5-32B-Chat --data data/ --train --iters 1000 --batch-size 16 --lora-layers 12&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;将微调后的 &lt;code&gt;adapters&lt;/code&gt; 文件与基础模型合并：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m mlx_lm.fuse --model models/Qwen1.5-32B-Chat --save-path models/Qwen1.5-32B-Chat-FT --adapter-path models/Qwen1.5-32B-Chat-Adapters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;对合并后的模型进行量化加速： python tools/compress_model.py&lt;/p&gt; &#xA;&lt;p&gt;对微调训练后的模型进行对话测试： python chat.py&lt;/p&gt; &#xA;&lt;h3&gt;语音生成&lt;/h3&gt; &#xA;&lt;p&gt;本项目借助开源项目 &lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt; 进行语音生成。&lt;/p&gt; &#xA;&lt;p&gt;首先参考 &lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt; 的官方指南配置环境并运行语音生成程序。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n GPTSOVITS python=3.9&#xA;conda activate GPTSOVITS&#xA;cd GPT-SoVITS&#xA;pip install -r requirements.txt&#xA;python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行 api 程序，分别使用端口 9880 与 9881 提供派蒙与林亦的语音生成服务，以下请使用 GPT-SoVITS 代码库完成：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python api.py -s SoVITS_weights/paimeng2_e110_s159940.pth -g GPT_weights/paimeng2-e10.ckpt -dr samples/Paimon/疑问—哇，这个，还有这个…只是和史莱姆打了一场，就有这么多结论吗？.wav -dt &#34;哇，这个，还有这个…只是和史莱姆打了一场，就有这么多结论吗？&#34; -dl &#34;zh&#34; -a 127.0.0.1 -p 9880&#xA;python api.py -s SoVITS_weights/linyi_e25_s1150.pth -g GPT_weights/linyi-e50.ckpt -dr &#34;samples/linyi/【愤怒】你这问题太弱智了，我都不知道该从哪开始骂你。.WAV&#34; -dt &#34;你这问题太弱智了，我都不知道该从哪开始骂你。&#34; -dl &#34;zh&#34; -a 127.0.0.1 -p 9881&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行问答生成程序：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python start_qa_dialogue.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;参考&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;机器学习框架 MLX，来自苹果机器学习研究组：&lt;a href=&#34;https://github.com/ml-explore/mlx&#34;&gt;https://github.com/ml-explore/mlx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;阿里通义千问 Qwen1.5：&lt;a href=&#34;https://qwenlm.github.io/zh/blog/qwen1.5/&#34;&gt;https://qwenlm.github.io/zh/blog/qwen1.5/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;开源文本转语音项目 GPT-SoVITS，作者&lt;a href=&#34;https://space.bilibili.com/5760446&#34;&gt;花儿不哭&lt;/a&gt;：&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;https://github.com/RVC-Boss/GPT-SoVITS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;派蒙语音模型，作者&lt;a href=&#34;https://space.bilibili.com/518098961&#34;&gt;白菜工厂1145号员工&lt;/a&gt;：&lt;a href=&#34;https://www.bilibili.com/video/BV1Yu4m1N79m&#34;&gt;【GPT-SoVITS】30小时超大数据集测试，堆时长真的有用吗？&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>entropy-research/Devon</title>
    <updated>2024-05-15T09:02:32Z</updated>
    <id>tag:github.com,2024-05-15:/entropy-research/Devon</id>
    <link href="https://github.com/entropy-research/Devon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Devon: An open-source pair programmer&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt;Devon: An open-source pair programmer&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/entropy-research/Devon/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/entropy-research/devon?style=for-the-badge&amp;amp;color=lime&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/entropy-research/Devon/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/entropy-research/devon?style=for-the-badge&amp;amp;color=orange&#34; alt=&#34;Forks&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/entropy-research/Devon/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/entropy-research/devon?style=for-the-badge&amp;amp;color=yellow&#34; alt=&#34;Stargazers&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/entropy-research/Devon/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/entropy-research/devon?style=for-the-badge&amp;amp;color=red&#34; alt=&#34;Issues&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://github.com/entropy-research/Devon/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/entropy-research/devon?style=for-the-badge&amp;amp;color=blue&#34; alt=&#34;Apache 2.0 License&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://discord.gg/p5YpZ5vjd9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34; alt=&#34;Join our Discord community&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/entropy-research/Devon/assets/61808204/d42a8b9a-0211-4624-9804-d24df1d4dbf6&#34;&gt;https://github.com/entropy-research/Devon/assets/61808204/d42a8b9a-0211-4624-9804-d24df1d4dbf6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;node.js&lt;/code&gt; and &lt;code&gt;npm&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pipx&lt;/code&gt;, if you don&#39;t have this go &lt;a href=&#34;https://pipx.pypa.io/stable/installation/&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://console.anthropic.com/settings/keys&#34;&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/a&gt; API Key&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation commands&lt;/h2&gt; &#xA;&lt;p&gt;To use, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sSL https://raw.githubusercontent.com/entropy-research/Devon/main/install.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Or to install using &lt;code&gt;pipx&lt;/code&gt; + &lt;code&gt;npm&lt;/code&gt;:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pipx install devon_agent&#xA;npm install -g devon-tui &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This installs the Python backend, and the cli command to run the tool&lt;/p&gt; &#xA;&lt;h3&gt;Thats it! Happy building :)&lt;/h3&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;Navigate to your project folder and open the terminal.&lt;/p&gt; &#xA;&lt;p&gt;Set your Anthropic API key as an environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export ANTHROPIC_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then to &lt;em&gt;run&lt;/em&gt;, the command is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;devon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It&#39;s as easy as that.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Don&#39;t worry, the agent will be able to only access files and folders in the directory you started it from. You can also correct it while it&#39;s performing actions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-file editing&lt;/li&gt; &#xA; &lt;li&gt;Codebase exploration&lt;/li&gt; &#xA; &lt;li&gt;Config writing&lt;/li&gt; &#xA; &lt;li&gt;Test writing&lt;/li&gt; &#xA; &lt;li&gt;Bug fixing&lt;/li&gt; &#xA; &lt;li&gt;Architecture exploration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Limitations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Minimal functionality for non-Python languages&lt;/li&gt; &#xA; &lt;li&gt;Sometimes have to specify the file where you want the change to happen&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Progress&lt;/h1&gt; &#xA;&lt;h3&gt;This project is still super early and &lt;ins&gt;we would love your help&lt;/ins&gt; to make it great!&lt;/h3&gt; &#xA;&lt;h3&gt;Current goals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-model support&lt;/li&gt; &#xA; &lt;li&gt;Launch plugin system for tool and agent builders&lt;/li&gt; &#xA; &lt;li&gt;Create self-hostable Electron app&lt;/li&gt; &#xA; &lt;li&gt;Set SOTA on &lt;a href=&#34;https://www.swebench.com/lite.html&#34;&gt;SWE-bench Lite&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;View our current thoughts on next steps &lt;a href=&#34;https://docs.google.com/document/d/e/2PACX-1vTjLCQcWE_n-uUHFhtBkxTCIJ4FFe5ftY_E4_q69SjXhuEZv_CYpLaQDh3HqrJlAxsgikUx0sTzf9le/pub&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Star history&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://star-history.com/#entropy-research/Devon&amp;amp;Date&#34;&gt; &lt;img src=&#34;https://api.star-history.com/svg?repos=entropy-research/Devon&amp;amp;type=Date&#34; width=&#34;500&#34; alt=&#34;Star History Chart&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Past milestones&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;May 10, 2024&lt;/strong&gt; - Complete interactive agent v0.1.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;May 10, 2024&lt;/strong&gt; - Add steerability features&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;May 8, 2024&lt;/strong&gt; - Beat AutoCodeRover on SWE-Bench Lite&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Mid April, 2024&lt;/strong&gt; - Add repo level code search tooling&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;April 2, 2024&lt;/strong&gt; - Begin development of v0.1.0 interactive agent&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;March 17, 2024&lt;/strong&gt; - Launch non-interactive agent v0.0.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Current development priorities&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Improve context gathering and code indexing abilities ex: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Adding memory modules&lt;/li&gt; &#xA;   &lt;li&gt;Improved code indexing&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Add alternative models and agents to: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;a) Reduce end user cost and&lt;/li&gt; &#xA;   &lt;li&gt;b) Reduce end user latency&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Introduce Electron app and new UI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;How can I contribute?&lt;/h1&gt; &#xA;&lt;p&gt;Devon and the entropy-research org are community-driven, and we welcome contributions from everyone! From tackling issues to building features to creating datasets, there are many ways to get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Core functionality:&lt;/strong&gt; Help us develop the core agents, user experience, tool integrations, plugins, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research:&lt;/strong&gt; Help us research agent performance (including benchmarks!), build data pipelines, and finetune models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feedback and Testing:&lt;/strong&gt; Use Devon, report bugs, suggest features, or provide feedback on usability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For details, please check &lt;a href=&#34;https://raw.githubusercontent.com/entropy-research/Devon/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to contribute to the project, please fill out our &lt;a href=&#34;https://forms.gle/VU7RN7mwNvqEYe3B9&#34;&gt;Contribution Form&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Feedback&lt;/h1&gt; &#xA;&lt;p&gt;We would love feedback! Feel free to drop us a note on our &lt;a href=&#34;https://discord.gg/p5YpZ5vjd9&#34;&gt;Discord&lt;/a&gt; in the #feedback channel, or &lt;a href=&#34;https://github.com/entropy-research/Devon/issues&#34;&gt;create issues&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;We collect basic event type (i.e. &#34;tool call&#34;) and failure telemetry to solve bugs and improve the user experience, but if you want to reach out, we would love to hear from you!&lt;/p&gt; &#xA;&lt;p&gt;To disable telemetry, set the environment variable &lt;code&gt;DEVON_TELEMETRY_DISABLED&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export DEVON_TELEMETRY_DISABLED=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;p&gt;Join our Discord server and say hi! &lt;a href=&#34;https://discord.gg/p5YpZ5vjd9&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Distributed under the Apache 2.0 License. See &lt;a href=&#34;https://raw.githubusercontent.com/entropy-research/Devon/main/LICENSE&#34;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alibaba-damo-academy/FunClip</title>
    <updated>2024-05-15T09:02:32Z</updated>
    <id>tag:github.com,2024-05-15:/alibaba-damo-academy/FunClip</id>
    <link href="https://github.com/alibaba-damo-academy/FunClip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-source, accurate and easy-to-use video clipping tool, LLM based AI clipping intergrated || 开源、精准、方便的视频切片工具，集成了大语言模型AI智能剪辑功能&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;p align=&#34;center&#34;&gt; FunClip🎥&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&lt;strong&gt; ⚡ 开源、精准、方便的视频切片工具 &lt;/strong&gt;&lt;/p&gt; &#xA;&lt;strong&gt;&lt;p align=&#34;center&#34;&gt; 🧠 通过FunClip探索基于大语言模型的视频剪辑 &lt;/p&gt;&lt;/strong&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/interface.jpg&#34; width=&#34;444/&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;p align=&#34;center&#34;&gt;「简体中文 | &lt;a href=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/README_en.md&#34;&gt;English&lt;/a&gt;」&lt;/p&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt;&lt;a href=&#34;#近期更新&#34;&gt; 近期更新 &lt;/a&gt; ｜&lt;a href=&#34;#施工中&#34;&gt; 施工中 &lt;/a&gt; ｜&lt;a href=&#34;#安装环境&#34;&gt; 安装环境 &lt;/a&gt; ｜&lt;a href=&#34;#使用方法&#34;&gt; 使用方法 &lt;/a&gt; ｜&lt;a href=&#34;#社区交流&#34;&gt; 社区交流 &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;FunClip&lt;/strong&gt;是一款完全开源、本地部署的自动化视频剪辑工具，通过调用阿里巴巴通义实验室开源的&lt;a href=&#34;https://github.com/alibaba-damo-academy/FunASR&#34;&gt;FunASR&lt;/a&gt; Paraformer系列模型进行视频的语音识别，随后用户可以自由选择识别结果中的文本片段或说话人，点击裁剪按钮即可获取对应片段的视频（&lt;a href=&#34;https://modelscope.cn/studios/iic/funasr_app_clipvideo/summary&#34;&gt;快速体验&lt;/a&gt;）。&lt;/p&gt; &#xA;&lt;p&gt;在上述基本功能的基础上，FunClip有以下特色：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥FunClip集成了多种大语言模型调用方式并提供了prompt配置接口，尝试通过大语言模型进行视频裁剪~&lt;/li&gt; &#xA; &lt;li&gt;FunClip集成了阿里巴巴开源的工业级模型&lt;a href=&#34;https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary&#34;&gt;Paraformer-Large&lt;/a&gt;，是当前识别效果最优的开源中文ASR模型之一，Modelscope下载量1300w+次，并且能够一体化的准确预测时间戳。&lt;/li&gt; &#xA; &lt;li&gt;FunClip集成了&lt;a href=&#34;https://modelscope.cn/models/iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary&#34;&gt;SeACo-Paraformer&lt;/a&gt;的热词定制化功能，在ASR过程中可以指定一些实体词、人名等作为热词，提升识别效果。&lt;/li&gt; &#xA; &lt;li&gt;FunClip集成了&lt;a href=&#34;https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary&#34;&gt;CAM++&lt;/a&gt;说话人识别模型，用户可以将自动识别出的说话人ID作为裁剪目标，将某一说话人的段落裁剪出来。&lt;/li&gt; &#xA; &lt;li&gt;通过Gradio交互实现上述功能，安装简单使用方便，并且可以在服务端搭建服务通过浏览器使用。&lt;/li&gt; &#xA; &lt;li&gt;FunClip支持多段自由剪辑，并且会自动返回全视频SRT字幕、目标段落SRT字幕，使用简单方便。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;欢迎体验使用，欢迎提出关于字幕生成或语音识别的需求与宝贵建议~&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;近期更新&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;近期更新🚀&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥2024/05/13 FunClip v2.0.0加入大语言模型智能裁剪功能，集成qwen系列，gpt系列等模型，提供默认prompt，您也可以探索并分享prompt的设置技巧，使用方法如下： &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;在进行识别之后，选择大模型名称，配置你自己的apikey；&lt;/li&gt; &#xA;   &lt;li&gt;点击&#39;LLM智能段落选择&#39;按钮，FunClip将自动组合两个prompt与视频的srt字幕；&lt;/li&gt; &#xA;   &lt;li&gt;点击&#39;LLM智能裁剪&#39;按钮，基于前一步的大语言模型输出结果，FunClip将提取其中的时间戳进行裁剪；&lt;/li&gt; &#xA;   &lt;li&gt;您可以尝试改变prompt来借助大语言模型的能力来获取您想要的结果；&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024/05/09 FunClip更新至v1.1.0，包含如下更新与修复： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持配置输出文件目录，保存ASR中间结果与视频裁剪中间文件；&lt;/li&gt; &#xA;   &lt;li&gt;UI升级（见下方演示图例），视频与音频裁剪功能在同一页，按钮位置调整；&lt;/li&gt; &#xA;   &lt;li&gt;修复了由于FunASR接口升级引入的bug，该bug曾导致一些严重的剪辑错误；&lt;/li&gt; &#xA;   &lt;li&gt;支持为每一个段落配置不同的起止时间偏移；&lt;/li&gt; &#xA;   &lt;li&gt;代码优化等；&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024/03/06 命令行调用方式更新与问题修复，相关功能可以正常使用。&lt;/li&gt; &#xA; &lt;li&gt;2024/02/28 FunClip升级到FunASR1.0模型调用方式，通过FunASR开源的SeACo-Paraformer模型在视频剪辑中进一步支持热词定制化功能。&lt;/li&gt; &#xA; &lt;li&gt;2024/02/28 原FunASR-APP/ClipVideo更名为FunClip。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;施工中&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;施工中🌵&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;FunClip将会集成Whisper模型，以提供英文视频剪辑能力。&lt;/li&gt; &#xA; &lt;li&gt;FunClip即将集成大语言模型的能力，提供智能视频剪辑相关功能，敬请期待。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;安装环境&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;安装🔨&lt;/h2&gt; &#xA;&lt;h3&gt;Python环境安装&lt;/h3&gt; &#xA;&lt;p&gt;FunClip的运行仅依赖于一个Python环境，若您是一个小白开发者，可以先了解下如何使用Python，pip等~&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 克隆funclip仓库&#xA;git clone https://github.com/alibaba-damo-academy/FunClip.git&#xA;cd FunClip&#xA;# 安装相关Python依赖&#xA;pip install -r ./requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;安装imagemagick（可选）&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;如果你希望使用自动生成字幕的视频裁剪功能，需要安装imagemagick&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;apt-get -y update &amp;amp;&amp;amp; apt-get -y install ffmpeg imagemagick&#xA;sed -i &#39;s/none/read,write/g&#39; /etc/ImageMagick-6/policy.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install imagemagick&#xA;sed -i &#39;s/none/read,write/g&#39; /usr/local/Cellar/imagemagick/7.1.1-8_1/etc/ImageMagick-7/policy.xml &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;首先下载并安装imagemagick &lt;a href=&#34;https://imagemagick.org/script/download.php#windows&#34;&gt;https://imagemagick.org/script/download.php#windows&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;然后确定您的Python安装位置，在其中的&lt;code&gt;site-packages\moviepy\config_defaults.py&lt;/code&gt;文件中修改&lt;code&gt;IMAGEMAGICK_BINARY&lt;/code&gt;为imagemagick的exe路径&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;下载你需要的字体文件，这里我们提供一个默认的黑体字体文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/STHeitiMedium.ttc -O font/STHeitiMedium.ttc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;使用方法&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;使用FunClip&lt;/h2&gt; &#xA;&lt;h3&gt;A.在本地启动Gradio服务&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python funclip/launch.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;随后在浏览器中访问&lt;code&gt;localhost:7860&lt;/code&gt;即可看到如下图所示的界面，按如下步骤即可进行视频剪辑&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;上传你的视频（或使用下方的视频用例）&lt;/li&gt; &#xA; &lt;li&gt;（可选）设置热词，设置文件输出路径（保存识别结果、视频等）&lt;/li&gt; &#xA; &lt;li&gt;点击识别按钮获取识别结果，或点击识别+区分说话人在语音识别基础上识别说话人ID&lt;/li&gt; &#xA; &lt;li&gt;将识别结果中的选段复制到对应位置，或者将说话人ID输入到对应为止&lt;/li&gt; &#xA; &lt;li&gt;（可选）配置剪辑参数，偏移量与字幕设置等&lt;/li&gt; &#xA; &lt;li&gt;点击“裁剪”或“裁剪+字幕”按钮&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/guide.jpg&#34;&gt; &#xA;&lt;p&gt;使用大语言模型裁剪请参考如下教程&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/LLM_guide.png&#34; width=&#34;360/&#34;&gt; &#xA;&lt;h3&gt;B.通过命令行调用使用FunClip的相关功能&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 步骤一：识别&#xA;python funclip/videoclipper.py --stage 1 \&#xA;                       --file examples/2022云栖大会_片段.mp4 \&#xA;                       --output_dir ./output&#xA;# ./output中生成了识别结果与srt字幕等&#xA;# 步骤二：裁剪&#xA;python funclip/videoclipper.py --stage 2 \&#xA;                       --file examples/2022云栖大会_片段.mp4 \&#xA;                       --output_dir ./output \&#xA;                       --dest_text &#39;我们把它跟乡村振兴去结合起来，利用我们的设计的能力&#39; \&#xA;                       --start_ost 0 \&#xA;                       --end_ost 100 \&#xA;                       --output_file &#39;./output/res.mp4&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;C.通过Modelscope创空间体验FunClip&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://modelscope.cn/studios/iic/funasr_app_clipvideo/summary&#34;&gt;funclip创空间&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;社区交流&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;社区交流🍟&lt;/h2&gt; &#xA;&lt;p&gt;FunClip开源项目由FunASR社区维护，欢迎加入社区，交流与讨论，以及合作开发等。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;钉钉群&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;微信群&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;div align=&#34;left&#34;&gt;&#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/dingding.png&#34; width=&#34;250&#34;&gt;&#xA;    &lt;/div&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/wechat.png&#34; width=&#34;215&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Star一下支持我们🌟&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#alibaba-damo-academy/FunClip&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=alibaba-damo-academy/FunClip&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;通过FunASR了解语音识别相关技术&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/alibaba-damo-academy/FunASR&#34;&gt;FunASR&lt;/a&gt;是阿里巴巴通义实验室开源的端到端语音识别工具包，目前已经成为主流ASR工具包之一。其主要包括Python pipeline，SDK部署与海量开源工业ASR模型等。&lt;/p&gt; &#xA;&lt;p&gt;📚FunASR论文: &lt;a href=&#34;https://arxiv.org/abs/2305.11013&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2305.11013-orange&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;📚SeACo-Paraformer论文：&lt;a href=&#34;https://arxiv.org/abs/2308.03266&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2308.03266-orange&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;⭐支持FunASR: &lt;a href=&#34;https://github.com/alibaba-damo-academy/FunASR.stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/alibaba-damo-academy/FunASR.svg?style=social&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>