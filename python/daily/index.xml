<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-11T01:36:05Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nottelabs/notte</title>
    <updated>2025-08-11T01:36:05Z</updated>
    <id>tag:github.com,2025-08-11:/nottelabs/notte</id>
    <link href="https://github.com/nottelabs/notte" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üî• Reliable Browser AI agents (YC S25)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rapidly build reliable web automation agents&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; The web agent framework built for &lt;strong&gt;speed&lt;/strong&gt;, &lt;strong&gt;cost-efficiency&lt;/strong&gt;, &lt;strong&gt;scale&lt;/strong&gt;, and &lt;strong&gt;reliability&lt;/strong&gt; &lt;br /&gt; ‚Üí Read more at: &lt;a href=&#34;https://github.com/nottelabs/open-operator-evals&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;open-operator-evals&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://x.com/nottecore?ref=github&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;X&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://www.linkedin.com/company/nottelabsinc/?ref=github&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;LinkedIn&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://notte.cc?ref=github&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Landing&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://console.notte.cc/?ref=github&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Console&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/docs/logo/bgd.png&#34; alt=&#34;Notte Logo&#34; width=&#34;100%&#34; /&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nottelabs/notte/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/nottelabs/notte?style=social&#34; alt=&#34;GitHub stars&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://spdx.org/licenses/SSPL-1.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-SSPL%201.0-blue.svg?sanitize=true&#34; alt=&#34;License: SSPL-1.0&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.11+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.11+&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/notte/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/notte?color=blue&#34; alt=&#34;PyPI version&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/projects/notte&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/notte?color=blue&#34; alt=&#34;PyPI Downloads&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h1&gt;What is Notte?&lt;/h1&gt; &#xA;&lt;p&gt;Notte provides all the essential tools for building and deploying AI agents that interact seamlessly with the web. Our full-stack framework combines AI agents with traditional scripting for maximum efficiency - letting you script deterministic parts and use AI only when needed, cutting costs by 50%+ while improving reliability. We allow you to develop, deploy, and scale your own agents and web automations, all with a single API. Read more in our documentation &lt;a href=&#34;https://docs.notte.cc&#34;&gt;here&lt;/a&gt; üî•&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Opensource Core:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/#using-python-sdk-recommended&#34;&gt;Run web agents&lt;/a&gt;&lt;/strong&gt; ‚Üí Give AI agents natural language tasks to complete on websites&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/#structured-output&#34;&gt;Structured Output&lt;/a&gt;&lt;/strong&gt; ‚Üí Get data in your exact format with Pydantic models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/#scraping&#34;&gt;Site Interactions&lt;/a&gt;&lt;/strong&gt; ‚Üí Observe website states, scrape data and execute actions using Playwright compatible primitives and natural language commands&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;API service (Recommended)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/#session-features&#34;&gt;Stealth Browser Sessions&lt;/a&gt;&lt;/strong&gt; ‚Üí Browser instances with built-in CAPTCHA solving, proxies, and anti-detection&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/#workflows&#34;&gt;Hybrid Workflows&lt;/a&gt;&lt;/strong&gt; ‚Üí Combine scripting and AI agents to reduce costs and improve reliability&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/#agent-vault&#34;&gt;Secrets Vaults&lt;/a&gt;&lt;/strong&gt; ‚Üí Enterprise-grade credential management to store emails, passwords, MFA tokens, SSO, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/#agent-persona&#34;&gt;Digital Personas&lt;/a&gt;&lt;/strong&gt; ‚Üí Create digital identities with unique emails, phones, and automated 2FA for account creation workflows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install notte&#xA;patchright install --with-deps chromium&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run in local mode&lt;/h3&gt; &#xA;&lt;p&gt;Use the following script to spinup an agent using opensource features (you&#39;ll need your own LLM API keys):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import notte&#xA;from dotenv import load_dotenv&#xA;load_dotenv()&#xA;&#xA;with notte.Session(headless=False) as session:&#xA;    agent = notte.Agent(session=session, reasoning_model=&#39;gemini/gemini-2.5-flash&#39;, max_steps=30)&#xA;    response = agent.run(task=&#34;doom scroll cat memes on google images&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Python SDK (Recommended)&lt;/h3&gt; &#xA;&lt;p&gt;We also provide an effortless API that hosts the browser sessions for you - and provide plenty of premium features. To run the agent you&#39;ll need to first sign up on the &lt;a href=&#34;https://console.notte.cc&#34;&gt;Notte Console&lt;/a&gt; and create a free Notte API key üîë&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;&#xA;cli = NotteClient(api_key=&#34;your-api-key&#34;)&#xA;&#xA;with cli.Session(headless=False) as session:&#xA;    agent = cli.Agent(session=session, reasoning_model=&#39;gemini/gemini-2.5-flash&#39;, max_steps=30)&#xA;    response = agent.run(task=&#34;doom scroll cat memes on google images&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our setup allows you to experiment locally, then drop-in replace the import and prefix &lt;code&gt;notte&lt;/code&gt; objects with &lt;code&gt;cli&lt;/code&gt; to switch to SDK and get hosted browser sessions plus access to premium features!&lt;/p&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Rank&lt;/th&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;Agent Self-Report&lt;/th&gt; &#xA;   &lt;th&gt;LLM Evaluation&lt;/th&gt; &#xA;   &lt;th&gt;Time per Task&lt;/th&gt; &#xA;   &lt;th&gt;Task Reliability&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üèÜ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nottelabs/notte&#34;&gt;Notte&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;86.2%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;79.0%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;47s&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;96.6%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2Ô∏è‚É£&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use&#34;&gt;Browser-Use&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;77.3%&lt;/td&gt; &#xA;   &lt;td&gt;60.2%&lt;/td&gt; &#xA;   &lt;td&gt;113s&lt;/td&gt; &#xA;   &lt;td&gt;83.3%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3Ô∏è‚É£&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/convergence-ai/proxy-lite&#34;&gt;Convergence&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;38.4%&lt;/td&gt; &#xA;   &lt;td&gt;31.4%&lt;/td&gt; &#xA;   &lt;td&gt;83s&lt;/td&gt; &#xA;   &lt;td&gt;50%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Read the full story here: &lt;a href=&#34;https://github.com/nottelabs/open-operator-evals&#34;&gt;https://github.com/nottelabs/open-operator-evals&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Agent features&lt;/h1&gt; &#xA;&lt;h2&gt;Structured output&lt;/h2&gt; &#xA;&lt;p&gt;Structured output is a feature of the agent&#39;s run function that allows you to specify a Pydantic model as the &lt;code&gt;response_format&lt;/code&gt; parameter. The agent will return data in the specified structure.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;from pydantic import BaseModel&#xA;from typing import List&#xA;&#xA;class HackerNewsPost(BaseModel):&#xA;    title: str&#xA;    url: str&#xA;    points: int&#xA;    author: str&#xA;    comments_count: int&#xA;&#xA;class TopPosts(BaseModel):&#xA;    posts: List[HackerNewsPost]&#xA;&#xA;cli = NotteClient()&#xA;with cli.Session(headless=False, browser_type=&#34;firefox&#34;) as session:&#xA;    agent = cli.Agent(session=session, reasoning_model=&#39;gemini/gemini-2.5-flash&#39;, max_steps=15)&#xA;    response = agent.run(&#xA;        task=&#34;Go to Hacker News (news.ycombinator.com) and extract the top 5 posts with their titles, URLs, points, authors, and comment counts.&#34;,&#xA;        response_format=TopPosts,&#xA;    )&#xA;print(response.answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Agent vault&lt;/h2&gt; &#xA;&lt;p&gt;Vaults are tools you can attach to your Agent instance to securely store and manage credentials. The agent automatically uses these credentials when needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;&#xA;cli = NotteClient()&#xA;&#xA;with cli.Vault() as vault, cli.Session(headless=False) as session:&#xA;    vault.add_credentials(&#xA;        url=&#34;https://x.com&#34;,&#xA;        username=&#34;your-email&#34;,&#xA;        password=&#34;your-password&#34;,&#xA;    )&#xA;    agent = cli.Agent(session=session, vault=vault, max_steps=10)&#xA;    response = agent.run(&#xA;      task=&#34;go to twitter; login and go to my messages&#34;,&#xA;    )&#xA;print(response.answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Agent persona&lt;/h2&gt; &#xA;&lt;p&gt;Personas are tools you can attach to your Agent instance to provide digital identities with unique email addresses, phone numbers, and automated 2FA handling.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;&#xA;cli = NotteClient()&#xA;&#xA;with cli.Persona(create_phone_number=False) as persona:&#xA;    with cli.Session(browser_type=&#34;firefox&#34;, headless=False) as session:&#xA;        agent = cli.Agent(session=session, persona=persona, max_steps=15)&#xA;        response = agent.run(&#xA;            task=&#34;Open the Google form and RSVP yes with your name&#34;,&#xA;            url=&#34;https://forms.google.com/your-form-url&#34;,&#xA;        )&#xA;print(response.answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Session features&lt;/h1&gt; &#xA;&lt;h2&gt;Stealth&lt;/h2&gt; &#xA;&lt;p&gt;Stealth features include automatic CAPTCHA solving and proxy configuration to enhance automation reliability and anonymity.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;from notte_sdk.types import NotteProxy, ExternalProxy&#xA;&#xA;cli = NotteClient()&#xA;&#xA;# Built-in proxies with CAPTCHA solving&#xA;with cli.Session(&#xA;    solve_captchas=True,&#xA;    proxies=True,  # US-based proxy&#xA;    browser_type=&#34;firefox&#34;,&#xA;    headless=False&#xA;) as session:&#xA;    agent = cli.Agent(session=session, max_steps=5)&#xA;    response = agent.run(&#xA;        task=&#34;Try to solve the CAPTCHA using internal tools&#34;,&#xA;        url=&#34;https://www.google.com/recaptcha/api2/demo&#34;&#xA;    )&#xA;&#xA;# Custom proxy configuration&#xA;proxy_settings = ExternalProxy(&#xA;    server=&#34;http://your-proxy-server:port&#34;,&#xA;    username=&#34;your-username&#34;,&#xA;    password=&#34;your-password&#34;,&#xA;)&#xA;&#xA;with cli.Session(proxies=[proxy_settings]) as session:&#xA;    agent = cli.Agent(session=session, max_steps=5)&#xA;    response = agent.run(task=&#34;Navigate to a website&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;File download / upload&lt;/h2&gt; &#xA;&lt;p&gt;File Storage allows you to upload files to a session and download files that agents retrieve during their work. Files are session-scoped and persist beyond the session lifecycle.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;&#xA;cli = NotteClient()&#xA;storage = cli.FileStorage()&#xA;&#xA;# Upload files before agent execution&#xA;storage.upload(&#34;/path/to/document.pdf&#34;)&#xA;&#xA;# Create session with storage attached&#xA;with cli.Session(storage=storage) as session:&#xA;    agent = cli.Agent(session=session, max_steps=5)&#xA;    response = agent.run(&#xA;        task=&#34;Upload the PDF document to the website and download the cat picture&#34;,&#xA;        url=&#34;https://example.com/upload&#34;&#xA;    )&#xA;&#xA;# Download files that the agent downloaded&#xA;downloaded_files = storage.list(type=&#34;downloads&#34;)&#xA;for file_name in downloaded_files:&#xA;    storage.download(file_name=file_name, local_dir=&#34;./results&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cookies / Auth Sessions&lt;/h2&gt; &#xA;&lt;p&gt;Cookies provide a flexible way to authenticate your sessions. While we recommend using the secure vault for credential management, cookies offer an alternative approach for certain use cases.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;import json&#xA;&#xA;cli = NotteClient()&#xA;&#xA;# Upload cookies for authentication&#xA;cookies = [&#xA;    {&#xA;        &#34;name&#34;: &#34;sb-db-auth-token&#34;,&#xA;        &#34;value&#34;: &#34;base64-cookie-value&#34;,&#xA;        &#34;domain&#34;: &#34;github.com&#34;,&#xA;        &#34;path&#34;: &#34;/&#34;,&#xA;        &#34;expires&#34;: 9778363203.913704,&#xA;        &#34;httpOnly&#34;: False,&#xA;        &#34;secure&#34;: False,&#xA;        &#34;sameSite&#34;: &#34;Lax&#34;&#xA;    }&#xA;]&#xA;&#xA;with cli.Session() as session:&#xA;    session.set_cookies(cookies=cookies)  # or cookie_file=&#34;path/to/cookies.json&#34;&#xA;    &#xA;    agent = cli.Agent(session=session, max_steps=5)&#xA;    response = agent.run(&#xA;        task=&#34;go to nottelabs/notte get repo info&#34;,&#xA;    )&#xA;    &#xA;    # Get cookies from the session&#xA;    cookies_resp = session.get_cookies()&#xA;    with open(&#34;cookies.json&#34;, &#34;w&#34;) as f:&#xA;        json.dump(cookies_resp, f)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;CDP Browser compatibility&lt;/h2&gt; &#xA;&lt;p&gt;You can plug in any browser session provider you want and use our agent on top. Use external headless browser providers via CDP to benefit from Notte&#39;s agentic capabilities with any CDP-compatible browser.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;&#xA;cli = NotteClient()&#xA;cdp_url = &#34;wss://your-external-cdp-url&#34;&#xA;&#xA;with cli.Session(cdp_url=cdp_url) as session:&#xA;    agent = cli.Agent(session=session)&#xA;    response = agent.run(task=&#34;extract pricing plans from https://www.notte.cc/&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Workflows&lt;/h1&gt; &#xA;&lt;p&gt;Notte&#39;s close compatibility with Playwright allows you to mix web automation primitives with agents for specific parts that require reasoning and adaptability. This hybrid approach cuts LLM costs and is much faster by using scripting for deterministic parts and agents only when needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;import time&#xA;&#xA;cli = NotteClient()&#xA;&#xA;with cli.Session(headless=False, perception_type=&#34;fast&#34;) as page:&#xA;    # Script execution for deterministic navigation&#xA;    page.execute(type=&#34;goto&#34;, value=&#34;https://www.quince.com/women/organic-stretch-cotton-chino-short&#34;)&#xA;    page.observe()&#xA;&#xA;    # Agent for reasoning-based selection&#xA;    agent = cli.Agent(session=page)&#xA;    agent.run(task=&#34;just select the ivory color in size 6 option&#34;)&#xA;&#xA;    # Script execution for deterministic actions&#xA;    page.execute(type=&#34;click&#34;, selector=&#34;internal:role=button[name=\&#34;ADD TO CART\&#34;i]&#34;)&#xA;    page.observe()&#xA;    page.execute(type=&#34;click&#34;, selector=&#34;internal:role=button[name=\&#34;CHECKOUT\&#34;i]&#34;)&#xA;    page.observe()&#xA;    time.sleep(5)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Scraping&lt;/h1&gt; &#xA;&lt;p&gt;For fast data extraction, we provide a dedicated scraping endpoint that automatically creates and manages sessions. You can pass custom instructions for structured outputs and enable stealth mode.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from notte_sdk import NotteClient&#xA;from pydantic import BaseModel&#xA;&#xA;cli = NotteClient()&#xA;&#xA;# Simple scraping&#xA;response = cli.scrape(&#xA;    url=&#34;https://notte.cc&#34;,&#xA;    scrape_links=True,&#xA;    only_main_content=True&#xA;)&#xA;&#xA;# Structured scraping with custom instructions&#xA;class Article(BaseModel):&#xA;    title: str&#xA;    content: str&#xA;    date: str&#xA;&#xA;response = cli.scrape(&#xA;    url=&#34;https://example.com/blog&#34;,&#xA;    response_format=Article,&#xA;    instructions=&#34;Extract only the title, date and content of the articles&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or directly with cURL&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST &#39;https://api.notte.cc/scrape&#39; \&#xA;  -H &#39;Authorization: Bearer &amp;lt;NOTTE-API-KEY&amp;gt;&#39; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;    &#34;url&#34;: &#34;https://notte.cc&#34;,&#xA;    &#34;only_main_content&#34;: false,&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Search:&lt;/strong&gt; We&#39;ve built a cool demo of an LLM leveraging the scraping endpoint in an MCP server to make real-time search in an LLM chatbot - works like a charm! Available here: &lt;a href=&#34;https://search.notte.cc/&#34;&gt;https://search.notte.cc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the Server Side Public License v1. See the &lt;a href=&#34;https://raw.githubusercontent.com/nottelabs/notte/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use notte in your research or project, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{notte2025,&#xA;  author = {Pinto, Andrea and Giordano, Lucas and {nottelabs-team}},&#xA;  title = {Notte: Software suite for internet-native agentic systems},&#xA;  url = {https://github.com/nottelabs/notte},&#xA;  year = {2025},&#xA;  publisher = {GitHub},&#xA;  license = {SSPL-1.0}&#xA;  version = {1.4.4},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Copyright ¬© 2025 Notte Labs, Inc.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>omkarcloud/botasaurus</title>
    <updated>2025-08-11T01:36:05Z</updated>
    <id>tag:github.com,2025-08-11:/omkarcloud/botasaurus</id>
    <link href="https://github.com/omkarcloud/botasaurus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The All in One Framework to Build Undefeatable Scrapers&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/mascot.png&#34; alt=&#34;botasaurus&#34; /&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top: 0;&#34;&gt; &#xA; &lt;h1&gt;ü§ñ Botasaurus ü§ñ&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; The All in One Framework to Build Undefeatable Scrapers &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;The web has evolved. Finally, web scraping has too.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://views.whatilearened.today/views/github/omkarcloud/botasaurus.svg?sanitize=true&#34; width=&#34;80px&#34; height=&#34;28px&#34; alt=&#34;View&#34; /&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://gitpod.io/#https://github.com/omkarcloud/botasaurus-starter&#34;&gt; &lt;img alt=&#34;Run in Gitpod&#34; src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; /&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üêøÔ∏è Botasaurus In a Nutshell&lt;/h2&gt; &#xA;&lt;p&gt;How wonderful that of all the web scraping tools out there, you chose to learn about Botasaurus. Congratulations!&lt;/p&gt; &#xA;&lt;p&gt;And now that you are here, you are in for an exciting, unusual, and rewarding journey that will make your web scraping life a lot easier.&lt;/p&gt; &#xA;&lt;p&gt;Now, let me tell you about Botasaurus in bullet points. (Because as per marketing gurus, YOU as a member of the Developer Tribe have a VERY short attention span.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;So, what is Botasaurus?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Botasaurus is an all-in-one web scraping framework that enables you to build awesome scrapers in less time, with less code, and with more fun.&lt;/p&gt; &#xA;&lt;p&gt;We have put all our web scraping experience and best practices into Botasaurus to save you hundreds of hours of development time!&lt;/p&gt; &#xA;&lt;p&gt;Now, for the magical powers awaiting you after learning Botasaurus:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In terms of humaneness, what Superman is to Man, Botasaurus is to Selenium and Playwright. Easily pass every (Yes, E-V-E-R-Y) bot test, and build undetected scrapers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In the video below, watch as we &lt;strong&gt;bypass some of the best bot detection systems&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://nopecha.com/demo/cloudflare&#34;&gt;Cloudflare Web Application Firewall (WAF)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://www.browserscan.net/bot-detection&#34;&gt;BrowserScan Bot Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://fingerprint.com/products/bot-detection/&#34;&gt;Fingerprint Bot Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://antoinevastel.com/bots/datadome&#34;&gt;Datadome Bot Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;a href=&#34;https://turnstile.zeroclover.io/&#34;&gt;Cloudflare Turnstile CAPTCHA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/b4f6171f-f2a2-4255-9feb-2973ee9a25ae&#34;&gt;&lt;/video&gt; &lt;/p&gt; &#xA;&lt;p&gt;üîó Want to try it yourself? See the code behind these tests &lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/bot_detection_tests.py&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Perform realistic, human-like mouse movements and say sayonara to detection &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/human-mode-demo.gif&#34; alt=&#34;human-mode-demo&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Convert your scraper into a desktop app for Mac, Windows, and Linux in 1 day, so not only developers but everyone can use your web scraper.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/desktop-app-photo.png&#34; alt=&#34;desktop-app-photo&#34; /&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Turn your scraper into a beautiful website, making it easy for your customers to use it from anywhere, anytime.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/google-maps-scraper/master/screenshots/demo.gif&#34; alt=&#34;pro-gmaps-demo&#34; /&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Save up to 97%, yes 97%, on browser proxy costs by using &lt;a href=&#34;https://github.com/omkarcloud/botasaurus#how-to-significantly-reduce-proxy-costs-when-scraping-at-scale&#34;&gt;browser-based fetch requests.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Easily save hours of development time with easy parallelization, profiles, extensions, and proxy configuration. Botasaurus makes asynchronous, parallel scraping child&#39;s play.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use caching, sitemap, data cleaning, and other utilities to save hours of time spent writing and debugging code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Easily scale your scraper to multiple machines with Kubernetes, and get your data faster than ever.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And those are just the highlights. I mean!&lt;/p&gt; &#xA;&lt;p&gt;There is so much more to Botasaurus that you will be amazed at how much time you will save with it.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started with Botasaurus&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s dive right in with a straightforward example to understand Botasaurus.&lt;/p&gt; &#xA;&lt;p&gt;In this example, we will go through the steps to scrape the heading text from &lt;a href=&#34;https://www.omkar.cloud/&#34;&gt;https://www.omkar.cloud/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-bot-running.gif&#34; alt=&#34;Botasaurus in action&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Install Botasaurus&lt;/h3&gt; &#xA;&lt;p&gt;First things first, you need to install Botasaurus. Run the following command in your terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m pip install --upgrade botasaurus&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Set Up Your Botasaurus Project&lt;/h3&gt; &#xA;&lt;p&gt;Next, let&#39;s set up the project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a directory for your Botasaurus project and navigate into it:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir my-botasaurus-project&#xA;cd my-botasaurus-project&#xA;code .  # This will open the project in VSCode if you have it installed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 3: Write the Scraping Code&lt;/h3&gt; &#xA;&lt;p&gt;Now, create a Python script named &lt;code&gt;main.py&lt;/code&gt; in your project directory and paste the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser&#xA;def scrape_heading_task(driver: Driver, data):&#xA;    # Visit the Omkar Cloud website&#xA;    driver.get(&#34;https://www.omkar.cloud/&#34;)&#xA;    &#xA;    # Retrieve the heading element&#39;s text&#xA;    heading = driver.get_text(&#34;h1&#34;)&#xA;&#xA;    # Save the data as a JSON file in output/scrape_heading_task.json&#xA;    return {&#xA;        &#34;heading&#34;: heading&#xA;    }&#xA;     &#xA;# Initiate the web scraping task&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s understand this code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We define a custom scraping task, &lt;code&gt;scrape_heading_task&lt;/code&gt;, decorated with &lt;code&gt;@browser&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser&#xA;def scrape_heading_task(driver: Driver, data):&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Botasaurus automatically provides a Humane Driver to our function:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def scrape_heading_task(driver: Driver, data):&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inside the function, we: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Visit Omkar Cloud&lt;/li&gt; &#xA;   &lt;li&gt;Extract the heading text&lt;/li&gt; &#xA;   &lt;li&gt;Return the data to be automatically saved as &lt;code&gt;scrape_heading_task.json&lt;/code&gt; by Botasaurus:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    driver.get(&#34;https://www.omkar.cloud/&#34;)&#xA;    heading = driver.get_text(&#34;h1&#34;)&#xA;    return {&#34;heading&#34;: heading}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finally, we initiate the scraping task:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initiate the web scraping task&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 4: Run the Scraping Task&lt;/h3&gt; &#xA;&lt;p&gt;Time to run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After executing the script, it will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Launch Google Chrome&lt;/li&gt; &#xA; &lt;li&gt;Visit &lt;a href=&#34;https://www.omkar.cloud/&#34;&gt;omkar.cloud&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Extract the heading text&lt;/li&gt; &#xA; &lt;li&gt;Save it automatically as &lt;code&gt;output/scrape_heading_task.json&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-bot-running.gif&#34; alt=&#34;Botasaurus in action&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now, let&#39;s explore another way to scrape the heading using the &lt;code&gt;request&lt;/code&gt; module. Replace the previous code in &lt;code&gt;main.py&lt;/code&gt; with the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.request import request, Request&#xA;from botasaurus.soupify import soupify&#xA;&#xA;@request&#xA;def scrape_heading_task(request: Request, data):&#xA;    # Visit the Omkar Cloud website&#xA;    response = request.get(&#34;https://www.omkar.cloud/&#34;)&#xA;&#xA;    # Create a BeautifulSoup object    &#xA;    soup = soupify(response)&#xA;    &#xA;    # Retrieve the heading element&#39;s text&#xA;    heading = soup.find(&#39;h1&#39;).get_text()&#xA;&#xA;    # Save the data as a JSON file in output/scrape_heading_task.json&#xA;    return {&#xA;        &#34;heading&#34;: heading&#xA;    }     &#xA;# Initiate the web scraping task&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We scrape the HTML using &lt;code&gt;request&lt;/code&gt;, which is specifically designed for making browser-like humane requests.&lt;/li&gt; &#xA; &lt;li&gt;Next, we parse the HTML into a &lt;code&gt;BeautifulSoup&lt;/code&gt; object using &lt;code&gt;soupify()&lt;/code&gt; and extract the heading.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step 5: Run the Scraping Task (which makes Humane HTTP Requests)&lt;/h3&gt; &#xA;&lt;p&gt;Finally, run it again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This time, you will observe the exact same result as before, but instead of opening a whole browser, we are making browser-like humane HTTP requests.&lt;/p&gt; &#xA;&lt;h2&gt;üí° Understanding Botasaurus&lt;/h2&gt; &#xA;&lt;h3&gt;What is Botasaurus Driver, and why should I use it over Selenium and Playwright?&lt;/h3&gt; &#xA;&lt;p&gt;Botasaurus Driver is a web automation driver like Selenium, and the single most important reason to use it is because it is truly humane. You will not, and I repeat NOT, have any issues accessing any website.&lt;/p&gt; &#xA;&lt;p&gt;Plus, it is super fast to launch and use, and the API is designed by and for web scrapers, and you will love it.&lt;/p&gt; &#xA;&lt;h3&gt;How do I access Cloudflare-protected pages using Botasaurus?&lt;/h3&gt; &#xA;&lt;p&gt;Cloudflare is the most popular protection system on the web. So, let&#39;s see how Botasaurus can help you solve various Cloudflare challenges.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Connection Challenge&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the single most popular challenge and requires making a browser-like connection with appropriate headers. It&#39;s commonly used for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Product Pages&lt;/li&gt; &#xA; &lt;li&gt;Blog Pages&lt;/li&gt; &#xA; &lt;li&gt;Search Result Pages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Example Page: https://www.g2.com/products/github/reviews --&gt; &#xA;&lt;h4&gt;What Works?&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visiting the website via Google Referrer (which makes it seem as if the user has arrived from a Google search).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser&#xA;def scrape_heading_task(driver: Driver, data):&#xA;    # Visit the website via Google Referrer&#xA;    driver.google_get(&#34;https://www.cloudflare.com/en-in/&#34;)&#xA;    driver.prompt()&#xA;    heading = driver.get_text(&#39;h1&#39;)&#xA;    return heading&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use the request module. The Request Object is smart and, by default, visits any link with a Google Referrer. Although it works, you will need to use retries.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.request import request, Request&#xA;&#xA;@request(max_retry=10)&#xA;def scrape_heading_task(request: Request, data):&#xA;    response = request.get(&#34;https://www.cloudflare.com/en-in/&#34;)&#xA;    print(response.status_code)&#xA;    response.raise_for_status()&#xA;    return response.text&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;JS with Captcha Challenge&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This challenge requires performing JS computations that differentiate a Chrome controlled by Selenium/Puppeteer/Playwright from a real Chrome. It also involves solving a Captcha. It&#39;s used to for pages which are rarely but sometimes visited by people, like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;5th Review page&lt;/li&gt; &#xA; &lt;li&gt;Auth pages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example Page: &lt;a href=&#34;https://nopecha.com/demo/cloudflare&#34;&gt;https://nopecha.com/demo/cloudflare&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;What Does Not Work?&lt;/h4&gt; &#xA;&lt;p&gt;Using &lt;code&gt;@request&lt;/code&gt; does not work because although it can make browser-like HTTP requests, it cannot run JavaScript to solve the challenge.&lt;/p&gt; &#xA;&lt;h4&gt;What Works?&lt;/h4&gt; &#xA;&lt;p&gt;Pass the &lt;code&gt;bypass_cloudflare=True&lt;/code&gt; argument to the &lt;code&gt;google_get&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser&#xA;def scrape_heading_task(driver: Driver, data):&#xA;    driver.google_get(&#34;https://nopecha.com/demo/cloudflare&#34;, bypass_cloudflare=True)&#xA;    driver.prompt()&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/cloudflare-js-captcha-demo.gif&#34; alt=&#34;Cloudflare JS with Captcha Challenge Demo&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What are the benefits of a UI scraper?&lt;/h3&gt; &#xA;&lt;p&gt;Here are some benefits of creating a scraper with a user interface:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simplify your scraper usage for customers, eliminating the need to teach them how to modify and run your code.&lt;/li&gt; &#xA; &lt;li&gt;Protect your code by hosting the scraper on the web and offering a monthly subscription, rather than providing full access to your code. This approach: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Safeguards your Python code from being copied and reused, increasing your customer&#39;s lifetime value.&lt;/li&gt; &#xA;   &lt;li&gt;Generate monthly recurring revenue via subscription from your customers, surpassing a one-time payment.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Enable sorting, filtering, and downloading of data in various formats (JSON, Excel, CSV, etc.).&lt;/li&gt; &#xA; &lt;li&gt;Provide access via a REST API for seamless integration.&lt;/li&gt; &#xA; &lt;li&gt;Create a polished frontend, backend, and API integration with minimal code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to run a UI-based scraper?&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s run the Botasaurus Starter Template (the recommended template for greenfield Botasaurus projects), which scrapes the heading of the provided link by following these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the Starter Template:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/omkarcloud/botasaurus-starter my-botasaurus-project&#xA;cd my-botasaurus-project&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies (will take a few minutes):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python -m pip install -r requirements.txt&#xA;python run.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the scraper:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Your browser will automatically open up at &lt;a href=&#34;http://localhost:3000/&#34;&gt;http://localhost:3000/&lt;/a&gt;. Then, enter the link you want to scrape (e.g., &lt;a href=&#34;https://www.omkar.cloud/&#34;&gt;https://www.omkar.cloud/&lt;/a&gt;) and click on the Run Button.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo.gif&#34; alt=&#34;starter-scraper-demo&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;After some seconds, the data will be scraped. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-result.png&#34; alt=&#34;starter-scraper-demo-result&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;http://localhost:3000/output&#34;&gt;http://localhost:3000/output&lt;/a&gt; to see all the tasks you have started.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-tasks.png&#34; alt=&#34;starter-scraper-demo-tasks&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;http://localhost:3000/about&#34;&gt;http://localhost:3000/about&lt;/a&gt; to see the rendered README.md file of the project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-readme.png&#34; alt=&#34;starter-scraper-demo-readme&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Finally, visit &lt;a href=&#34;http://localhost:3000/api-integration&#34;&gt;http://localhost:3000/api-integration&lt;/a&gt; to see how to access the scraper via API.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-scraper-demo-api.png&#34; alt=&#34;starter-scraper-demo-api&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;The API documentation is generated dynamically based on your scraper&#39;s inputs, sorts, filters, etc., and is unique to your scraper.&lt;/p&gt; &#xA;&lt;p&gt;So, whenever you need to run the scraper via API, visit this tab and copy the code specific to your scraper.&lt;/p&gt; &#xA;&lt;h3&gt;How to create a UI scraper using Botasaurus?&lt;/h3&gt; &#xA;&lt;p&gt;Creating a UI scraper with Botasaurus is a simple 3-step process:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create your scraper function&lt;/li&gt; &#xA; &lt;li&gt;Add the scraper to the server using 1 line of code&lt;/li&gt; &#xA; &lt;li&gt;Define the input controls for the scraper&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To understand these steps, let&#39;s go through the code of the Botasaurus Starter Template that you just ran.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1: Create the Scraper Function&lt;/h4&gt; &#xA;&lt;p&gt;In &lt;code&gt;src/scrape_heading_task.py&lt;/code&gt;, we define a scraping function that basically does the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Receives a &lt;code&gt;data&lt;/code&gt; object and extracts the &#34;link&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Retrieves the HTML content of the webpage using the &#34;link&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Converts the HTML into a BeautifulSoup object.&lt;/li&gt; &#xA; &lt;li&gt;Locates the heading element, extracts its text content, and returns it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.request import request, Request&#xA;from botasaurus.soupify import soupify&#xA;&#xA;@request&#xA;def scrape_heading_task(request: Request, data):&#xA;    # Visit the Link&#xA;    response = request.get(data[&#34;link&#34;])&#xA;&#xA;    # Create a BeautifulSoup object    &#xA;    soup = soupify(response)&#xA;    &#xA;    # Retrieve the heading element&#39;s text&#xA;    heading = soup.find(&#39;h1&#39;).get_text()&#xA;&#xA;    # Save the data as a JSON file in output/scrape_heading_task.json&#xA;    return {&#xA;        &#34;heading&#34;: heading&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 2: Add the Scraper to the Server&lt;/h4&gt; &#xA;&lt;p&gt;In &lt;code&gt;backend/scrapers.py&lt;/code&gt;, we:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Import our scraping function&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;Server.add_scraper()&lt;/code&gt; to register the scraper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus_server.server import Server&#xA;from src.scrape_heading_task import scrape_heading_task&#xA;&#xA;# Add the scraper to the server&#xA;Server.add_scraper(scrape_heading_task)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 3: Define the Input Controls&lt;/h4&gt; &#xA;&lt;p&gt;In &lt;code&gt;backend/inputs/scrape_heading_task.js&lt;/code&gt;, we:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Define a &lt;code&gt;getInput&lt;/code&gt; function that takes the controls parameter&lt;/li&gt; &#xA; &lt;li&gt;Add a link input control to it&lt;/li&gt; &#xA; &lt;li&gt;Use JSDoc comments to enable IntelliSense Code Completion in VSCode as you won&#39;t be able to remember all the controls in botasaurus.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;/**&#xA; * @typedef {import(&#39;../../frontend/node_modules/botasaurus-controls/dist/index&#39;).Controls} Controls&#xA; */&#xA;&#xA;/**&#xA; * @param {Controls} controls&#xA; */&#xA;function getInput(controls) {&#xA;    controls&#xA;        // Render a Link Input, which is required, defaults to &#34;https://stackoverflow.blog/open-source&#34;. &#xA;        .link(&#39;link&#39;, { isRequired: true, defaultValue: &#34;https://stackoverflow.blog/open-source&#34; })&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Above was a simple example; below is a real-world example with multi-text, number, switch, select, section, and other controls.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;/**&#xA; * @typedef {import(&#39;../../frontend/node_modules/botasaurus-controls/dist/index&#39;).Controls} Controls&#xA; */&#xA;&#xA;&#xA;/**&#xA; * @param {Controls} controls&#xA; */&#xA;function getInput(controls) {&#xA;    controls&#xA;        .listOfTexts(&#39;queries&#39;, {&#xA;            defaultValue: [&#34;Web Developers in Bangalore&#34;],&#xA;            placeholder: &#34;Web Developers in Bangalore&#34;,&#xA;            label: &#39;Search Queries&#39;,&#xA;            isRequired: true&#xA;        })&#xA;        .section(&#34;Email and Social Links Extraction&#34;, (section) =&amp;gt; {&#xA;            section.text(&#39;api_key&#39;, {&#xA;                placeholder: &#34;2e5d346ap4db8mce4fj7fc112s9h26s61e1192b6a526af51n9&#34;,&#xA;                label: &#39;Email and Social Links Extraction API Key&#39;,&#xA;                helpText: &#39;Enter your API key to extract email addresses and social media links.&#39;,&#xA;            })&#xA;        })&#xA;        .section(&#34;Reviews Extraction&#34;, (section) =&amp;gt; {&#xA;            section&#xA;                .switch(&#39;enable_reviews_extraction&#39;, {&#xA;                    label: &#34;Enable Reviews Extraction&#34;&#xA;                })&#xA;                .numberGreaterThanOrEqualToZero(&#39;max_reviews&#39;, {&#xA;                    label: &#39;Max Reviews per Place (Leave empty to extract all reviews)&#39;,&#xA;                    placeholder: 20,&#xA;                    isShown: (data) =&amp;gt; data[&#39;enable_reviews_extraction&#39;], defaultValue: 20,&#xA;                })&#xA;                .choose(&#39;reviews_sort&#39;, {&#xA;                    label: &#34;Sort Reviews By&#34;,&#xA;                    isRequired: true, isShown: (data) =&amp;gt; data[&#39;enable_reviews_extraction&#39;], defaultValue: &#39;newest&#39;, options: [{ value: &#39;newest&#39;, label: &#39;Newest&#39; }, { value: &#39;most_relevant&#39;, label: &#39;Most Relevant&#39; }, { value: &#39;highest_rating&#39;, label: &#39;Highest Rating&#39; }, { value: &#39;lowest_rating&#39;, label: &#39;Lowest Rating&#39; }]&#xA;                })&#xA;        })&#xA;        .section(&#34;Language and Max Results&#34;, (section) =&amp;gt; {&#xA;            section&#xA;                .addLangSelect()&#xA;                .numberGreaterThanOrEqualToOne(&#39;max_results&#39;, {&#xA;                    placeholder: 100,&#xA;                    label: &#39;Max Results per Search Query (Leave empty to extract all places)&#39;&#xA;                })&#xA;        })&#xA;        .section(&#34;Geo Location&#34;, (section) =&amp;gt; {&#xA;            section&#xA;                .text(&#39;coordinates&#39;, {&#xA;                    placeholder: &#39;12.900490, 77.571466&#39;&#xA;                })&#xA;                .numberGreaterThanOrEqualToOne(&#39;zoom_level&#39;, {&#xA;                    label: &#39;Zoom Level (1-21)&#39;,&#xA;                    defaultValue: 14,&#xA;                    placeholder: 14&#xA;                })&#xA;        })&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I encourage you to paste the above code into &lt;code&gt;backend/inputs/scrape_heading_task.js&lt;/code&gt; and reload the page, and you will see a complex set of input controls like the image shown.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/complex-input.png&#34; alt=&#34;complex-input&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now, to use the Botasaurus UI for adding new scrapers, remember these points:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;backend/inputs/{your_scraping_function_name}.js&lt;/code&gt; file for each scraping function.&lt;/li&gt; &#xA; &lt;li&gt;Define the &lt;code&gt;getInput&lt;/code&gt; function in the file with the necessary controls.&lt;/li&gt; &#xA; &lt;li&gt;Use JSDoc comments to enable IntelliSense code completion in VSCode, as you won&#39;t be able to remember all the controls in Botasaurus.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Use this template as a starting point for new scraping function&#39;s input controls js file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;/**&#xA; * @typedef {import(&#39;../../frontend/node_modules/botasaurus-controls/dist/index&#39;).Controls} Controls&#xA; */&#xA;&#xA;/**&#xA; * @param {Controls} controls&#xA; */&#xA;function getInput(controls) {&#xA;    // Define your controls here.&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! With these simple steps, you can create a fully functional UI scraper using Botasaurus.&lt;/p&gt; &#xA;&lt;p&gt;Later, you will learn how to add sorts and filters to make your UI scraper even more powerful and user-friendly.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/sorts-filters.png&#34; alt=&#34;sorts-filters&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What is a Desktop Extractor?&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;strong&gt;Desktop Extractor&lt;/strong&gt; is a standalone application that runs on your computer and extracts specific data from websites, PDFs, Excel files, and other documents. Unlike web-based tools, desktop extractors run locally, giving &lt;strong&gt;faster performance&lt;/strong&gt; and &lt;strong&gt;zero cloud costs&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/desktop-app-photo.png&#34; alt=&#34;Desktop Extractor showing an application interface with extraction options&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What advantages do Desktop Scrapers have over web-based scrapers?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Desktop Scrapers&lt;/strong&gt; offer key advantages over web-based scraper solutions like Outscraper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero Infrastructure Costs&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Runs on the user&#39;s machine, eliminating expensive cloud computing fees.&lt;/li&gt; &#xA;   &lt;li&gt;Lower cloud costs allow you to offer lower pricing, attracting more customers and increasing revenue.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Faster Execution&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Instant execution, no delays for cloud resource allocation.&lt;/li&gt; &#xA;   &lt;li&gt;Uses the user&#39;s system, which is much faster than shared cloud servers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Increased Customer Engagement&lt;/strong&gt;:&lt;br /&gt; The app sits right on the user&#39;s desktop, encouraging frequent use compared to web tools they must actively visit via browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cross-Platform Deployment in 1 Day&lt;/strong&gt;:&lt;br /&gt; With &lt;strong&gt;Botasaurus&lt;/strong&gt;, you can launch a desktop scraper for &lt;strong&gt;Windows, macOS, and Linux&lt;/strong&gt; within a day. No need to build a website, manage servers, or handle scaling issues. Bota Desktop includes built-in features such as:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Task management&lt;/li&gt; &#xA;   &lt;li&gt;Data Table&lt;/li&gt; &#xA;   &lt;li&gt;Data export (Excel, CSV, etc.)&lt;/li&gt; &#xA;   &lt;li&gt;Sorting &amp;amp; Filtering&lt;/li&gt; &#xA;   &lt;li&gt;Caching and many more&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With zero usage costs, faster performance, and easier development, Desktop Scrapers outperform web-based alternatives.&lt;/p&gt; &#xA;&lt;h3&gt;How to Build a Desktop Extractor&lt;/h3&gt; &#xA;&lt;p&gt;Creating Desktop Extractors is easier than you think! All you need is a basic understanding of JavaScript. Once you&#39;re ready, read the &lt;a href=&#34;https://www.omkar.cloud/botasaurus/docs/botasaurus-desktop/quick-start&#34;&gt;Desktop Extraction Tutorial&lt;/a&gt;, where we&#39;ll guide you through building two practical extractors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Yahoo Finance Stock Scraper&lt;/strong&gt; ‚Äì Extracts real-time stock prices from Yahoo Finance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/stock-scraper-preview.gif&#34; alt=&#34;Stock Scraper Demo showing the application extracting stock prices from Yahoo Finance&#34; /&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Amazon Invoice PDF Extractor&lt;/strong&gt; ‚Äì Automates the extraction of key invoice data like Document Number, Document Date, and Place of Supply from Amazon PDFs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/pdf-extract-preview.gif&#34; alt=&#34;PDF Extraction Demo showing the application extracting data from Amazon PDF invoices&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;As a web scraper, you might naturally want to focus on web scraping. Still, I want you to create the &lt;strong&gt;Amazon Invoice PDF Extractor&lt;/strong&gt; project. Why? Because many developers overlook the immense potential of extracting data from PDFs, Excel files, and other documents.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Document Data Extraction is a large untapped market.&lt;/strong&gt; For example, even in most developed countries, accountants often spend hundreds of hours manually entering invoice data for tax filings. A desktop extractor can transform this tedious, error-prone process into a task that takes just minutes, delivering 100% accurate results.&lt;/p&gt; &#xA;&lt;p&gt;Please read the step-by-step tutorial &lt;a href=&#34;https://www.omkar.cloud/botasaurus/docs/botasaurus-desktop/quick-start&#34;&gt;here&lt;/a&gt;. By the end of this short guide, you&#39;ll be able to create powerful desktop extractors in very little time.&lt;/p&gt; &#xA;&lt;h3&gt;What is Botasaurus, and what are its main features?&lt;/h3&gt; &#xA;&lt;p&gt;Botasaurus is an all-in-one web scraping framework designed to achieve three main goals:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Provide essential web scraping utilities to streamline the scraping process.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To accomplish these goals, Botasaurus gives you 3 decorators:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;@browser&lt;/code&gt;: For scraping web pages using a humane browser.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@request&lt;/code&gt;: For scraping web pages using lightweight and humane HTTP requests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@task&lt;/code&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For scraping web pages using third-party libraries like &lt;code&gt;playwright&lt;/code&gt; or &lt;code&gt;selenium&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;or, For running non-web scraping tasks, such as data processing (e.g., converting video to audio). Botasaurus is not limited to web scraping tasks; any Python function can be made accessible with a stunning UI and user-friendly API.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In practice, while developing with Botasaurus, you will spend most of your time in the following areas:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Configuring your scrapers via decorators with settings like: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Which proxy to use&lt;/li&gt; &#xA;   &lt;li&gt;How many scrapers to run in parallel, etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Writing your core web scraping logic using BeautifulSoup (bs4) or the Botasaurus Driver.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, you will utilize the following Botasaurus utilities for debugging and development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bt&lt;/code&gt;: Mainly for writing JSON, EXCEL, and HTML temporary files, and for data cleaning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Sitemap&lt;/code&gt;: For accessing the website&#39;s links and sitemap.&lt;/li&gt; &#xA; &lt;li&gt;Minor utilities like: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;LocalStorage&lt;/code&gt;: For storing scraper state.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;soupify&lt;/code&gt;: For creating BeautifulSoup objects from Driver, Requests response, Driver Element, or HTML string.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;IPUtils&lt;/code&gt;: For obtaining information (IP, country, etc.) about the current IP address.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Cache&lt;/code&gt;: For managing the cache.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By simply configuring these three decorators (&lt;code&gt;@browser&lt;/code&gt;, &lt;code&gt;@request&lt;/code&gt;, and &lt;code&gt;@task&lt;/code&gt;) with arguments, you can easily create &lt;code&gt;real-time scrapers&lt;/code&gt; and &lt;code&gt;large-scale datasets&lt;/code&gt;, thus saving you countless hours that would otherwise be spent writing and debugging code from scratch.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Offering a Python-based UI scraper that allows non-technical users to run scrapers online by simply visiting a website link. (As described in the previous FAQ)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make it easy to create desktop applications for Mac, Windows, and Linux, using JavaScript. More details can be found in the &lt;a href=&#34;https://www.omkar.cloud/botasaurus/docs/botasaurus-desktop/introduction&#34;&gt;Botasaurus Desktop Documentation here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;How to use decorators in Botasaurus?&lt;/h3&gt; &#xA;&lt;p&gt;Decorators are the heart of Botasaurus. To use a decorator function, you can call it with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A single item&lt;/li&gt; &#xA; &lt;li&gt;A list of items&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If a scraping function is given a list of items, it will sequentially call the scraping function for each data item.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you pass a list of three links to the &lt;code&gt;scrape_heading_task&lt;/code&gt; function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser&#xA;def scrape_heading_task(driver: Driver, link):&#xA;    driver.get(link)&#xA;    heading = driver.get_text(&#34;h1&#34;)&#xA;    return heading&#xA;&#xA;scrape_heading_task([&#34;https://www.omkar.cloud/&#34;, &#34;https://www.omkar.cloud/blog/&#34;, &#34;https://stackoverflow.com/&#34;]) # &amp;lt;-- list of items&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, Botasaurus will launch a new browser instance for each item, and the final results will be stored in &lt;code&gt;output/scrape_heading_task.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/list-demo.gif&#34; alt=&#34;list-demo&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How does Botasaurus help me in debugging?&lt;/h3&gt; &#xA;&lt;p&gt;Botasaurus helps you in debugging by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Easily viewing the result of the scraping function, as it is saved in &lt;code&gt;output/{your_scraping_function_name}.json&lt;/code&gt;. Say goodbye to print statements.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/scraped-data.png&#34; alt=&#34;scraped data&#34; /&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bringing your attention to errors in browser mode with a beep sound and pausing the browser, allowing you to debug the error on the spot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/error-prompt.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Even if an exception is raised in headless mode, it will still open the website in your default browser, making it easier to debug code in a headless browser. (Isn&#39;t it cool?)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/headless-error.png&#34; alt=&#34;headless-error&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to configure the Browser Decorator?&lt;/h3&gt; &#xA;&lt;p&gt;The Browser Decorator allows you to easily configure various aspects of the browser, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Blocking images and CSS&lt;/li&gt; &#xA; &lt;li&gt;Setting up proxies&lt;/li&gt; &#xA; &lt;li&gt;Specifying profiles&lt;/li&gt; &#xA; &lt;li&gt;Enabling headless mode&lt;/li&gt; &#xA; &lt;li&gt;Using Chrome extensions&lt;/li&gt; &#xA; &lt;li&gt;Captcha Solving&lt;/li&gt; &#xA; &lt;li&gt;Selecting language&lt;/li&gt; &#xA; &lt;li&gt;Passing Arguments to Chrome&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Blocking Images and CSS&lt;/h4&gt; &#xA;&lt;p&gt;Blocking images is one of the most important configurations when scraping at scale. Blocking images can significantly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Speed up your web scraping tasks&lt;/li&gt; &#xA; &lt;li&gt;Reduce bandwidth usage&lt;/li&gt; &#xA; &lt;li&gt;And save money on proxies. (Best of All!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, a page that originally takes 4 seconds and 12 MB to load might only take one second and 100 KB after blocking images and CSS.&lt;/p&gt; &#xA;&lt;p&gt;To block images, use the &lt;code&gt;block_images&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    block_images=True,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To block both images and CSS, use &lt;code&gt;block_images_and_css&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    block_images_and_css=True,&#xA;)    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Proxies&lt;/h4&gt; &#xA;&lt;p&gt;To use proxies, simply specify the &lt;code&gt;proxy&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    proxy=&#34;http://username:password@proxy-provider-domain:port&#34;&#xA;)    &#xA;def visit_what_is_my_ip(driver: Driver, data):&#xA;    driver.get(&#34;https://whatismyipaddress.com/&#34;)&#xA;    driver.prompt()&#xA;&#xA;visit_what_is_my_ip()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also pass a list of proxies, and the proxy will be automatically rotated:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    proxy=[&#xA;        &#34;http://username:password@proxy-provider-domain:port&#34;, &#xA;        &#34;http://username2:password2@proxy-provider-domain:port&#34;&#xA;    ]&#xA;)&#xA;def visit_what_is_my_ip(driver: Driver, data):&#xA;    driver.get(&#34;https://whatismyipaddress.com/&#34;)&#xA;    driver.prompt()&#xA;&#xA;visit_what_is_my_ip() &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Profile&lt;/h4&gt; &#xA;&lt;p&gt;Easily specify the Chrome profile using the &lt;code&gt;profile&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    profile=&#34;pikachu&#34;&#xA;)    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, each Chrome profile can become very large (e.g., 100 MB) and can eat up all your computer storage.&lt;/p&gt; &#xA;&lt;p&gt;To solve this problem, use the &lt;code&gt;tiny_profile&lt;/code&gt; option, which is a lightweight alternative to Chrome profiles.&lt;/p&gt; &#xA;&lt;p&gt;When creating hundreds of Chrome profiles, it is highly recommended to use the &lt;code&gt;tiny_profile&lt;/code&gt; option because:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating 1000 Chrome profiles will take at least 100 GB, whereas 1000 tiny profiles will take up only 1 MB of storage, making tiny profiles easy to store and back up.&lt;/li&gt; &#xA; &lt;li&gt;Tiny profiles are cross-platform, meaning you can create profiles on a Linux server, copy the &lt;code&gt;./profiles&lt;/code&gt; folder to a Windows PC, and easily run them.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Under the hood, tiny profiles persist cookies from visited websites, making them extremely lightweight (around 1 KB) while providing the same session persistence.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s how to use the tiny profile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    tiny_profile=True, &#xA;    profile=&#34;pikachu&#34;,&#xA;)    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Headless Mode&lt;/h4&gt; &#xA;&lt;p&gt;Enable headless mode with &lt;code&gt;headless=True&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    headless=True&#xA;)    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that if you use headless mode, you will surely be identified by services like Cloudflare and Datadome. Therefore, use headless mode only when scraping websites that don&#39;t use such services.&lt;/p&gt; &#xA;&lt;h4&gt;Chrome Extensions&lt;/h4&gt; &#xA;&lt;p&gt;Botasaurus allows the use of ANY Chrome Extension with just 1 line of code. The example below shows how to use the Mouse Coordinates Chrome Extension to show current mouse X and Y coordinates on web pages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;from chrome_extension_python import Extension&#xA;&#xA;@browser(&#xA;    extensions=[&#xA;        Extension(&#xA;            &#34;https://chromewebstore.google.com/detail/mouse-coordinates/mfohnjojhopfcahiddmeljeholnciakl&#34;&#xA;        )&#xA;    ],&#xA;)&#xA;def scrape_while_blocking_ads(driver: Driver, data):&#xA;    driver.get(&#34;https://example.com/&#34;)&#xA;    driver.prompt()&#xA;&#xA;scrape_while_blocking_ads()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In some cases, an extension may require additional configuration, such as API keys or credentials. For such scenarios, you can create a custom extension. Learn more about creating and configuring custom extensions &lt;a href=&#34;https://github.com/omkarcloud/chrome-extension-python&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Captcha Solving&lt;/h4&gt; &#xA;&lt;p&gt;Encountering captchas is common in web scraping. You can use the &lt;a href=&#34;https://github.com/omkarcloud/capsolver-extension-python?tab=readme-ov-file#installation&#34;&gt;capsolver_extension_python&lt;/a&gt; package to automatically solve CAPTCHAs with Capsolver.&lt;/p&gt; &#xA;&lt;p&gt;To use it, first install the package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install capsolver_extension_python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, integrate it into your code as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;from capsolver_extension_python import Capsolver&#xA;&#xA;# Replace &#34;CAP-MY_KEY&#34; with your actual CapSolver API key&#xA;@browser(extensions=[Capsolver(api_key=&#34;CAP-MY_KEY&#34;)])  &#xA;def solve_captcha(driver: Driver, data):&#xA;    driver.get(&#34;https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php&#34;)&#xA;    driver.prompt()&#xA;&#xA;solve_captcha()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Language&lt;/h4&gt; &#xA;&lt;p&gt;Specify the language using the &lt;code&gt;lang&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.lang import Lang&#xA;&#xA;@browser(&#xA;    lang=Lang.Hindi,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;User Agent and Window Size&lt;/h4&gt; &#xA;&lt;p&gt;To make the browser really humane, Botasaurus does not change browser fingerprints by default, because using fingerprints makes the browser easily identifiable by running CSS tests to find mismatches between the provided user agent and the actual user agent.&lt;/p&gt; &#xA;&lt;p&gt;However, if you need fingerprinting, use the &lt;code&gt;user_agent&lt;/code&gt; and &lt;code&gt;window_size&lt;/code&gt; options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;from botasaurus.user_agent import UserAgent&#xA;from botasaurus.window_size import WindowSize&#xA;&#xA;@browser(&#xA;    user_agent=UserAgent.RANDOM,&#xA;    window_size=WindowSize.RANDOM,&#xA;)&#xA;def visit_whatsmyua(driver: Driver, data):&#xA;    driver.get(&#34;https://www.whatsmyua.info/&#34;)&#xA;    driver.prompt()&#xA;&#xA;visit_whatsmyua()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When working with profiles, you want the fingerprints to remain consistent. You don&#39;t want the user&#39;s user agent to be Chrome 106 on the first visit and then become Chrome 102 on the second visit.&lt;/p&gt; &#xA;&lt;p&gt;So, when using profiles, use the &lt;code&gt;HASHED&lt;/code&gt; option to generate a consistent user agent and window size based on the profile&#39;s hash:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;from botasaurus.user_agent import UserAgent&#xA;from botasaurus.window_size import WindowSize&#xA;&#xA;@browser(&#xA;    profile=&#34;pikachu&#34;,&#xA;    user_agent=UserAgent.HASHED,&#xA;    window_size=WindowSize.HASHED,&#xA;)&#xA;def visit_whatsmyua(driver: Driver, data):&#xA;    driver.get(&#34;https://www.whatsmyua.info/&#34;)&#xA;    driver.prompt()&#xA;    &#xA;visit_whatsmyua()&#xA;&#xA;# Everytime Same UserAgent and WindowSize&#xA;visit_whatsmyua()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Passing Arguments to Chrome&lt;/h4&gt; &#xA;&lt;p&gt;To pass arguments to Chrome, use the &lt;code&gt;add_arguments&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    add_arguments=[&#39;--headless=new&#39;],&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To dynamically generate arguments based on the &lt;code&gt;data&lt;/code&gt; parameter, pass a function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_arguments(data):&#xA;    return [&#39;--headless=new&#39;]&#xA;&#xA;@browser(&#xA;    add_arguments=get_arguments,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Wait for Complete Page Load&lt;/h4&gt; &#xA;&lt;p&gt;By default, Botasaurus waits for all page resources (DOM, JavaScript, CSS, images, etc.) to load before calling your scraping function with the driver.&lt;/p&gt; &#xA;&lt;p&gt;However, sometimes the DOM is ready, but JavaScript, images, etc., take forever to load.&lt;/p&gt; &#xA;&lt;p&gt;In such cases, you can set &lt;code&gt;wait_for_complete_page_load&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; to interact with the DOM as soon as the HTML is parsed and the DOM is ready:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    wait_for_complete_page_load=False,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Reuse Driver&lt;/h4&gt; &#xA;&lt;p&gt;Consider the following example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser&#xA;def scrape_data(driver: Driver, link):&#xA;    driver.get(link)&#xA;&#xA;scrape_data([&#34;https://www.omkar.cloud/&#34;, &#34;https://www.omkar.cloud/blog/&#34;, &#34;https://stackoverflow.com/&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you run this code, the browser will be recreated on each page visit, which is inefficient.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/list-demo.gif&#34; alt=&#34;list-demo-omkar&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;To solve this problem, use the &lt;code&gt;reuse_driver&lt;/code&gt; option which is great for cases like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scraping a large number of links and reusing the same browser instance for all page visits.&lt;/li&gt; &#xA; &lt;li&gt;Running your scraper in a cloud server to scrape data on demand, without recreating Chrome on each request.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s how to use &lt;code&gt;reuse_driver&lt;/code&gt; which will reuse the same Chrome instance for visiting each link.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser(&#xA;    reuse_driver=True&#xA;)&#xA;def scrape_data(driver: Driver, link):&#xA;    driver.get(link)&#xA;&#xA;scrape_data([&#34;https://www.omkar.cloud/&#34;, &#34;https://www.omkar.cloud/blog/&#34;, &#34;https://stackoverflow.com/&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/list-demo-reuse-driver.gif&#34; alt=&#34;list-demo-reuse-driver.gif&#34; /&gt;&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;p&gt;Also, by default, whenever the program ends or is canceled, Botasaurus smartly closes any open Chrome instances, leaving no instances running in the background.&lt;/p&gt; &#xA;&lt;p&gt;In rare cases, you may want to explicitly close the Chrome instance. For such scenarios, you can use the &lt;code&gt;.close()&lt;/code&gt; method on the scraping function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scrape_data.close()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will close any Chrome instances that remain open after the scraping function ends.&lt;/p&gt; &#xA;&lt;h3&gt;How to Significantly Reduce Proxy Costs When Scraping at Scale?&lt;/h3&gt; &#xA;&lt;p&gt;Recently, we had a project requiring access to around 100,000 pages from a well-protected website, necessitating the use of Residential Proxies.&lt;/p&gt; &#xA;&lt;p&gt;Even after blocking images, we still required 250GB of proxy bandwidth, costing approximately $1050 (at $4.2 per GB with IP Royal).&lt;/p&gt; &#xA;&lt;p&gt;This was beyond our budget :(&lt;/p&gt; &#xA;&lt;p&gt;To solve this, we implemented a smart strategy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We first visited the website normally.&lt;/li&gt; &#xA; &lt;li&gt;We then made requests for subsequent pages using the browser&#39;s &lt;code&gt;fetch&lt;/code&gt; API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Since we were only requesting the HTML, which was well compressed by the browser, we reduced our proxy bandwidth needs to just 5GB, costing only $30.&lt;/p&gt; &#xA;&lt;p&gt;This resulted in savings of around $1000!&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example of how you can do something similar in Botasaurus:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;from botasaurus.soupify import soupify&#xA;&#xA;@browser(&#xA;    reuse_driver=True,  # Reuse the browser&#xA;    max_retry=5,        # Retry up to 5 times on failure&#xA;)&#xA;def scrape_data(driver: Driver, link):&#xA;    # If the browser is newly opened, first visit the link&#xA;    if driver.config.is_new:&#xA;        driver.google_get(link)&#xA;    &#xA;    # Make requests using the browser fetch API&#xA;    response = driver.requests.get(link)&#xA;    response.raise_for_status()  # Ensure the request was successful&#xA;    html = response.text&#xA;&#xA;    # Parse the HTML to extract the desired data&#xA;    soup = soupify(html)&#xA;    stock_name = soup.select_one(&#39;[data-testid=&#34;quote-hdr&#34;] h1&#39;).get_text()&#xA;    stock_price = soup.select_one(&#39;[data-testid=&#34;qsp-price&#34;]&#39;).get_text()&#xA;    &#xA;    return {&#xA;        &#34;stock_name&#34;: stock_name,&#xA;        &#34;stock_price&#34;: stock_price,&#xA;    }&#xA;&#xA;# List of URLs to scrape&#xA;links = [&#xA;    &#34;https://finance.yahoo.com/quote/AAPL/&#34;,&#xA;    &#34;https://finance.yahoo.com/quote/GOOG/&#34;,&#xA;    &#34;https://finance.yahoo.com/quote/MSFT/&#34;,&#xA;]&#xA;&#xA;# Execute the scraping function for the list of links&#xA;scrape_data(links)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dealing with 429 (Too Many Requests) Errors&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you encounter a 429 error, add a delay before making another request. Most websites using Nginx, setting a rate limit of 1 request per second. To respect this limit, a delay of 1.13 seconds is recommended.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver.sleep(1.13)  # Delay to respect the rate limit&#xA;response = driver.requests.get(link)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Handling 400 Errors Due to Large Cookies&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you encounter a 400 error with a &#34;cookie too large&#34; message, delete the cookies and retry the request.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = driver.requests.get(link)&#xA;&#xA;if response.status_code == 400:&#xA;    driver.delete_cookies()  # Delete cookies to resolve the error&#xA;    driver.short_random_sleep()  # Short delay before retrying&#xA;    response = driver.requests.get(link)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can also use &lt;code&gt;driver.requests.get_mank(links)&lt;/code&gt; to make multiple requests in parallel, which is faster than making them sequentially.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;How to Configure the Browser&#39;s Chrome Profile, Language, and Proxy Dynamically Based on Data Parameters?&lt;/h3&gt; &#xA;&lt;p&gt;The decorators in Botasaurus are really flexible, allowing you to pass a function that can derive the browser configuration based on the data item parameter. This is particularly useful when working with multiple Chrome profiles.&lt;/p&gt; &#xA;&lt;p&gt;You can dynamically configure the browser&#39;s Chrome profile and proxy using decorators in two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Using functions to extract configuration values from data:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Define functions to extract the desired configuration values from the &lt;code&gt;data&lt;/code&gt; parameter.&lt;/li&gt; &#xA;   &lt;li&gt;Pass these functions as arguments to the &lt;code&gt;@browser&lt;/code&gt; decorator.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;def get_profile(data):&#xA;    return data[&#34;profile&#34;]&#xA;&#xA;def get_proxy(data):&#xA;    return data[&#34;proxy&#34;]&#xA;&#xA;@browser(profile=get_profile, proxy=get_proxy)&#xA;def scrape_heading_task(driver: Driver, data):&#xA;    profile, proxy = driver.config.profile, driver.config.proxy&#xA;    print(profile, proxy)&#xA;    return profile, proxy&#xA;&#xA;data = [&#xA;    {&#34;profile&#34;: &#34;pikachu&#34;, &#34;proxy&#34;: &#34;http://142.250.77.228:8000&#34;},&#xA;    {&#34;profile&#34;: &#34;greyninja&#34;, &#34;proxy&#34;: &#34;http://142.250.77.229:8000&#34;},&#xA;]&#xA;&#xA;scrape_heading_task(data)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Directly passing configuration values when calling the decorated function:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pass the profile and proxy values directly as arguments to the decorated function when calling it.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser&#xA;def scrape_heading_task(driver: Driver, data):&#xA;    profile, proxy = driver.config.profile, driver.config.proxy&#xA;    print(profile, proxy)&#xA;    return profile, proxy&#xA;&#xA;scrape_heading_task(&#xA;    profile=&#39;pikachu&#39;,  # Directly pass the profile&#xA;    proxy=&#34;http://142.250.77.228:8000&#34;,  # Directly pass the proxy&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;PS: Most Botasaurus decorators allow passing functions to derive configurations from data parameters. Check the decorator&#39;s argument type hint to see if it supports this functionality.&lt;/p&gt; &#xA;&lt;h3&gt;What is the best way to manage profile-specific data like name, age across multiple profiles?&lt;/h3&gt; &#xA;&lt;p&gt;To store data related to the active profile, use &lt;code&gt;driver.profile&lt;/code&gt;. Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;def get_profile(data):&#xA;    return data[&#34;profile&#34;]&#xA;&#xA;@browser(profile=get_profile)&#xA;def run_profile_task(driver: Driver, data):&#xA;    # Set profile data&#xA;    driver.profile = {&#xA;        &#39;name&#39;: &#39;Amit Sharma&#39;,&#xA;        &#39;age&#39;: 30&#xA;    }&#xA;&#xA;    # Update the name in the profile&#xA;    driver.profile[&#39;name&#39;] = &#39;Amit Verma&#39;&#xA;&#xA;    # Delete the age from the profile&#xA;    del driver.profile[&#39;age&#39;]&#xA;&#xA;    # Print the updated profile&#xA;    print(driver.profile)  # Output: {&#39;name&#39;: &#39;Amit Verma&#39;}&#xA;&#xA;    # Delete the entire profile&#xA;    driver.profile = None&#xA;&#xA;run_profile_task([{&#34;profile&#34;: &#34;amit&#34;}])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For managing all profiles, use the &lt;code&gt;Profiles&lt;/code&gt; utility. Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.profiles import Profiles&#xA;&#xA;# Set profiles&#xA;Profiles.set_profile(&#39;amit&#39;, {&#39;name&#39;: &#39;Amit Sharma&#39;, &#39;age&#39;: 30})&#xA;Profiles.set_profile(&#39;rahul&#39;, {&#39;name&#39;: &#39;Rahul Verma&#39;, &#39;age&#39;: 30})&#xA;&#xA;# Get a profile&#xA;profile = Profiles.get_profile(&#39;amit&#39;)&#xA;print(profile)  # Output: {&#39;name&#39;: &#39;Amit Sharma&#39;, &#39;age&#39;: 30}&#xA;&#xA;# Get all profiles&#xA;all_profiles = Profiles.get_profiles()&#xA;print(all_profiles)  # Output: [{&#39;name&#39;: &#39;Amit Sharma&#39;, &#39;age&#39;: 30}, {&#39;name&#39;: &#39;Rahul Verma&#39;, &#39;age&#39;: 30}]&#xA;&#xA;# Get all profiles in random order&#xA;random_profiles = Profiles.get_profiles(random=True)&#xA;print(random_profiles)  # Output: [{&#39;name&#39;: &#39;Rahul Verma&#39;, &#39;age&#39;: 30}, {&#39;name&#39;: &#39;Amit Sharma&#39;, &#39;age&#39;: 30}] in random order&#xA;&#xA;# Delete a profile&#xA;Profiles.delete_profile(&#39;amit&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: All profile data is stored in the &lt;code&gt;profiles.json&lt;/code&gt; file in the current working directory. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/profiles.png&#34; alt=&#34;profiles&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What are some common methods in Botasaurus Driver?&lt;/h3&gt; &#xA;&lt;p&gt;Botasaurus Driver provides several handy methods for web automation tasks, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Visiting URLs:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver.get(&#34;https://www.example.com&#34;)&#xA;driver.google_get(&#34;https://www.example.com&#34;)  # Use Google as the referer [Recommended]&#xA;driver.get_via(&#34;https://www.example.com&#34;, referer=&#34;https://duckduckgo.com/&#34;)  # Use custom referer&#xA;driver.get_via_this_page(&#34;https://www.example.com&#34;)  # Use current page as referer&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finding elements:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import Wait&#xA;search_results = driver.select(&#34;.search-results&#34;, wait=Wait.SHORT)  # Wait for up to 4 seconds for the element to be present, return None if not found&#xA;all_links = driver.select_all(&#34;a&#34;)  # Get all elements matching the selector&#xA;search_results = driver.wait_for_element(&#34;.search-results&#34;, wait=Wait.LONG)  # Wait for up to 8 seconds for the element to be present, raise exception if not found&#xA;hello_mom = driver.get_element_with_exact_text(&#34;Hello Mom&#34;, wait=Wait.VERY_LONG)  # Wait for up to 16 seconds for an element having the exact text &#34;Hello Mom&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Interacting with elements:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver.type(&#34;input[name=&#39;username&#39;]&#34;, &#34;john_doe&#34;)  # Type into an input field&#xA;driver.click(&#34;button.submit&#34;)  # Click an element&#xA;element = driver.select(&#34;button.submit&#34;)&#xA;element.click()  # Click on an element&#xA;element.select_option(&#34;select#fruits&#34;, index=2)  # Select an option&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Retrieving element properties:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;header_text = driver.get_text(&#34;h1&#34;)  # Get text content&#xA;error_message = driver.get_element_containing_text(&#34;Error: Invalid input&#34;)&#xA;image_url = driver.select(&#34;img.logo&#34;).get_attribute(&#34;src&#34;)  # Get attribute value&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Working with parent-child elements:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;parent_element = driver.select(&#34;.parent&#34;)&#xA;child_element = parent_element.select(&#34;.child&#34;)&#xA;child_element.click()  # Click child element&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Executing JavaScript:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = driver.run_js(&#34;script.js&#34;) # Run a JavaScript file located in the current working directory.&#xA;result = driver.run_js(&#34;return document.title&#34;)&#xA;pikachu = driver.run_js(&#34;return args.pokemon&#34;, {&#34;pokemon&#34;: &#39;pikachu&#39;}) # args can be a dictionary, list, string, etc.&#xA;text_content = driver.select(&#34;body&#34;).run_js(&#34;(el) =&amp;gt; el.textContent&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enable human mode to perform, human-like mouse movements and say sayonara to detection:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Navigate to Cloudflare&#39;s Turnstile Captcha demo&#xA;driver.get(&#xA;  &#34;https://nopecha.com/demo/cloudflare&#34;,&#xA;)&#xA;&#xA;# Wait for page to fully load&#xA;driver.long_random_sleep()&#xA;&#xA;# Locate iframe containing the Cloudflare challenge&#xA;iframe = driver.get_element_at_point(160, 290)&#xA;&#xA;# Find checkbox element within the iframe&#xA;checkbox = iframe.get_element_at_point(30, 30)&#xA;&#xA;# Enable human mode for realistic, human-like mouse movements&#xA;driver.enable_human_mode()&#xA;&#xA;# Click the checkbox to solve the challenge&#xA;checkbox.click()&#xA;&#xA;# (Optional) Disable human mode if no longer needed  &#xA;driver.disable_human_mode()&#xA;&#xA;# Pause execution, for inspection&#xA;driver.prompt()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/human-mode-demo.gif&#34; alt=&#34;human-mode-demo&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Drag and Drop:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Open React DnD tutorial  &#xA;driver.get(&#34;https://react-dnd.github.io/react-dnd/examples/tutorial&#34;)  &#xA;&#xA;# Select draggable and droppable elements  &#xA;draggable = driver.select(&#39;[draggable=&#34;true&#34;]&#39;)  &#xA;droppable = driver.select(&#39;[data-testid=&#34;(3,6)&#34;]&#39;)  &#xA;&#xA;# Perform drag-and-drop  &#xA;draggable.drag_and_drop_to(droppable)  &#xA;&#xA;# Pause execution, for inspection&#xA;driver.prompt()  &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/drag-and-drop-demo.gif&#34; alt=&#34;drag-and-drop-demo&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Selecting Shadow Root Elements:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Visit the website&#xA;driver.get(&#34;https://nopecha.com/demo/cloudflare&#34;)&#xA;&#xA;# Wait for page to fully load&#xA;driver.long_random_sleep()&#xA;&#xA;# Locate the element containing shadow root&#xA;shadow_root_element = driver.select(&#39;[name=&#34;cf-turnstile-response&#34;]&#39;).parent&#xA;&#xA;# Access the iframe&#xA;iframe = shadow_root_element.get_shadow_root()&#xA;&#xA;# Access the nested shadow DOM inside the iframe &#xA;content = iframe.get_shadow_root()&#xA;&#xA;# print the text content of the &#34;label&#34; element.&#xA;print(content.select(&#34;label&#34;, wait = 8).text)&#xA;&#xA;# Pause execution, for inspection&#xA;driver.prompt()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/selecting-shadow-root-elements.gif&#34; alt=&#34;Selecting Shadow Root Elements&#34; /&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Monitoring requests:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver, cdp&#xA;&#xA;@browser()&#xA;def scrape_responses_task(driver: Driver, data):&#xA;    # Define a handler function that will be called after a response is received&#xA;    def after_response_handler(&#xA;        request_id: str,&#xA;        response: cdp.network.Response,&#xA;        event: cdp.network.ResponseReceived,&#xA;    ):&#xA;        # Extract URL, status, and headers from the response&#xA;        url = response.url&#xA;        status = response.status&#xA;        headers = response.headers&#xA;        &#xA;        # Print the response details &#xA;        print(&#xA;            &#34;after_response_handler&#34;,&#xA;            {&#xA;                &#34;request_id&#34;: request_id,&#xA;                &#34;url&#34;: url,&#xA;                &#34;status&#34;: status,&#xA;                &#34;headers&#34;: headers,&#xA;            },&#xA;        )&#xA;&#xA;        # Append the request ID to the driver&#39;s responses list&#xA;        driver.responses.append(request_id)&#xA;&#xA;    # Register the after_response_handler to be called after each response is received&#xA;    driver.after_response_received(after_response_handler)&#xA;&#xA;    # Navigate to the specified URL&#xA;    driver.get(&#34;https://example.com/&#34;)&#xA;&#xA;    # Collect all the responses that were appended during the navigation&#xA;    collected_responses = driver.responses.collect()&#xA;    &#xA;    # Save it in output/scrape_responses_task.json&#xA;    return collected_responses&#xA;&#xA;# Execute the scraping task&#xA;scrape_responses_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Working with iframes:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver.get(&#34;https://www.freecodecamp.org/news/using-entity-framework-core-with-mongodb/&#34;)&#xA;iframe = driver.get_iframe_by_link(&#34;www.youtube.com/embed&#34;) &#xA;# OR the following works as well&#xA;# iframe = driver.select_iframe(&#34;.embed-wrapper iframe&#34;) &#xA;freecodecamp_youtube_subscribers_count = iframe.select(&#34;.ytp-title-expanded-subtitle&#34;).text&#xA;print(freecodecamp_youtube_subscribers_count)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Executing CDP Command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver, cdp&#xA;driver.run_cdp_command(cdp.page.navigate(url=&#39;https://stackoverflow.blog/open-source&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Miscellaneous:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;form.type(&#34;input[name=&#39;password&#39;]&#34;, &#34;secret_password&#34;)  # Type into a form field&#xA;container.is_element_present(&#34;.button&#34;)  # Check element presence&#xA;page_html = driver.page_html  # Current page HTML&#xA;driver.select(&#34;.footer&#34;).scroll_into_view()  # Scroll element into view&#xA;driver.close()  # Close the browser&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How Can I Pause the Browser to Inspect Website when Developing the Scraper?&lt;/h3&gt; &#xA;&lt;p&gt;To pause the scraper and wait for user input before proceeding, use &lt;code&gt;driver.prompt()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver.prompt()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How do I configure authenticated proxies with SSL in Botasaurus?&lt;/h3&gt; &#xA;&lt;p&gt;Proxy providers like BrightData, IPRoyal, and others typically provide authenticated proxies in the format &#34;&lt;a href=&#34;http://username:password@proxy-provider-domain:port&#34;&gt;http://username:password@proxy-provider-domain:port&lt;/a&gt;&#34;. For example, &#34;&lt;a href=&#34;http://greyninja:awesomepassword@geo.iproyal.com:12321&#34;&gt;http://greyninja:awesomepassword@geo.iproyal.com:12321&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;However, if you use an authenticated proxy with a library like seleniumwire to visit a website using Cloudflare, or Datadome, you are GUARANTEED to be identified because you are using a non-SSL connection.&lt;/p&gt; &#xA;&lt;p&gt;To verify this, run the following code:&lt;/p&gt; &#xA;&lt;p&gt;First, install the necessary packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install selenium_wire&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, execute this Python script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from seleniumwire import webdriver  # Import from seleniumwire&#xA;&#xA;# Define the proxy&#xA;proxy_options = {&#xA;    &#39;proxy&#39;: {&#xA;        &#39;http&#39;: &#39;http://username:password@proxy-provider-domain:port&#39;, # TODO: Replace with your own proxy&#xA;        &#39;https&#39;: &#39;http://username:password@proxy-provider-domain:port&#39;, # TODO: Replace with your own proxy&#xA;    }&#xA;}&#xA;&#xA;# Install and set up the driver&#xA;driver = webdriver.Chrome(seleniumwire_options=proxy_options)&#xA;&#xA;# Visit the desired URL&#xA;link = &#39;https://fingerprint.com/products/bot-detection/&#39;&#xA;driver.get(&#34;https://www.google.com/&#34;)&#xA;driver.execute_script(f&#39;window.location.href = &#34;{link}&#34;&#39;)&#xA;&#xA;# Prompt for user input&#xA;input(&#34;Press Enter to exit...&#34;)&#xA;&#xA;# Clean up&#xA;driver.quit()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will SURELY be identified:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/seleniumwireblocked.png&#34; alt=&#34;identified&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;However, using proxies with Botasaurus solves this issue. See the difference by running the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser(proxy=&#34;http://username:password@proxy-provider-domain:port&#34;) # TODO: Replace with your own proxy &#xA;def scrape_heading_task(driver: Driver, data):&#xA;    driver.google_get(&#34;https://fingerprint.com/products/bot-detection/&#34;)&#xA;    driver.prompt()&#xA;&#xA;scrape_heading_task()    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Result: &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/botasaurussuccesspage.png&#34; alt=&#34;not identified&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Important Note: To run the code above, you will need &lt;a href=&#34;https://nodejs.org/en&#34;&gt;Node.js&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;h3&gt;Why am I getting a socket connection error when using a proxy to access a website?&lt;/h3&gt; &#xA;&lt;p&gt;Certain proxy providers like BrightData will block access to specific websites. To determine if this is the case, run the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser(proxy=&#34;http://username:password@proxy-provider-domain:port&#34;)  # TODO: Replace with your own proxy&#xA;def visit_what_is_my_ip(driver: Driver, data):&#xA;    driver.get(&#34;https://whatismyipaddress.com/&#34;)&#xA;    driver.prompt()&#xA;&#xA;visit_what_is_my_ip()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you can successfully access &lt;a href=&#34;https://whatismyipaddress.com/&#34;&gt;whatismyipaddress.com&lt;/a&gt; but not the website you&#39;re attempting to scrape, it means the proxy provider is blocking access to that particular website.&lt;/p&gt; &#xA;&lt;p&gt;In such situations, the only solution is to switch to a different proxy provider.&lt;/p&gt; &#xA;&lt;p&gt;Some good proxy providers we personally use are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For Rotating Datacenter Proxies: &lt;strong&gt;BrightData Datacenter Proxies&lt;/strong&gt;, which cost around $0.6 per GB on a pay-as-you-go basis. No KYC is required.&lt;/li&gt; &#xA; &lt;li&gt;For Rotating Residential Proxies: &lt;strong&gt;IPRoyal Royal Residential Proxies&lt;/strong&gt;, which cost around $7 per GB on a pay-as-you-go basis. No KYC is required.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;As always, nothing good in life comes free. Proxies are expensive, and will take up almost all of your scraping costs.&lt;/p&gt; &#xA;&lt;p&gt;So, use proxies only when you need them, and prefer request-based scrapers over browser-based scrapers to save bandwidth.&lt;/p&gt; &#xA;&lt;p&gt;Note: BrightData and IPRoyal have not paid us. We are recommending them based on our personal experience.&lt;/p&gt; &#xA;&lt;h3&gt;Which country should I choose when using proxies for web scraping?&lt;/h3&gt; &#xA;&lt;p&gt;The United States is often the best choice because:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The United States has a highly developed internet infrastructure and is home to numerous data centers, ensuring faster internet speeds.&lt;/li&gt; &#xA; &lt;li&gt;Most global companies host their websites in the US, so using a US proxy will result in faster scraping speeds.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Should I use a proxy for web scraping?&lt;/h3&gt; &#xA;&lt;p&gt;ONLY IF you encounter IP blocks.&lt;/p&gt; &#xA;&lt;p&gt;Sadly, most scrapers unnecessarily use proxies, even when they are not needed. Everything seems like a nail when you have a hammer.&lt;/p&gt; &#xA;&lt;p&gt;We have seen scrapers which can easily access hundreds of thousands of protected pages using the @browser module on home Wi-Fi without any issues.&lt;/p&gt; &#xA;&lt;p&gt;So, as a best practice scrape using the @browser module on your home Wi-Fi first. Only resort to proxies when you encounter IP blocks.&lt;/p&gt; &#xA;&lt;p&gt;This practice will save you a considerable amount of time (as proxies are really slow) and money (as proxies are expensive as well).&lt;/p&gt; &#xA;&lt;h3&gt;How to configure the Request Decorator?&lt;/h3&gt; &#xA;&lt;p&gt;The Request Decorator is used to make humane requests. Under the hood, it uses botasaurus-requests, a library based on hrequests, which incorporates important features like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using browser-like headers in the correct order.&lt;/li&gt; &#xA; &lt;li&gt;Makes a browser-like connection with correct ciphers.&lt;/li&gt; &#xA; &lt;li&gt;Uses &lt;code&gt;google.com&lt;/code&gt; referer by default to make it appear as if the user has arrived from google search.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Also, The Request Decorator allows you to configure proxy as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@request(&#xA;    proxy=&#34;http://username:password@proxy-provider-domain:port&#34;&#xA;)    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;What Options Can I Configure in all 3 Decorators?&lt;/h3&gt; &#xA;&lt;p&gt;All 3 decorators allow you to configure the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Parallel Execution:&lt;/li&gt; &#xA; &lt;li&gt;Caching Results&lt;/li&gt; &#xA; &lt;li&gt;Passing Common Metadata&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous Queues&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous Execution&lt;/li&gt; &#xA; &lt;li&gt;Handling Crashes&lt;/li&gt; &#xA; &lt;li&gt;Configuring Output&lt;/li&gt; &#xA; &lt;li&gt;Exception Handling&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s dive into each of these options and in later sections we will see their real-world applications.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;parallel&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;parallel&lt;/code&gt; option allows you to scrape data in parallel by launching multiple browser/request/task instances simultaneously. This can significantly speed up the scraping process.&lt;/p&gt; &#xA;&lt;p&gt;Run the example below to see parallelization in action:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser(parallel=3, data=[&#34;https://stackoverflow.blog/open-source&#34;, &#34;https://stackoverflow.blog/ai&#34;, &#34;https://stackoverflow.blog/productivity&#34;,])&#xA;def scrape_heading_task(driver: Driver, link):&#xA;    driver.get(link)&#xA;    heading = driver.get_text(&#39;h1&#39;)&#xA;    return heading&#xA;&#xA;scrape_heading_task()    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;cache&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;cache&lt;/code&gt; option enables caching of web scraping results to avoid re-scraping the same data. This can significantly improve performance and reduce redundant requests.&lt;/p&gt; &#xA;&lt;p&gt;Run the example below to see how caching works:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser(cache=True, data=[&#34;https://stackoverflow.blog/open-source&#34;, &#34;https://stackoverflow.blog/ai&#34;, &#34;https://stackoverflow.blog/productivity&#34;,])&#xA;def scrape_heading_task(driver: Driver, link):&#xA;    driver.get(link)&#xA;    heading = driver.get_text(&#39;h1&#39;)&#xA;    return heading&#xA;&#xA;print(scrape_heading_task())&#xA;print(scrape_heading_task())  # Data will be fetched from cache immediately &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Caching is one of the most important features of Botasaurus.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;metadata&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The metadata option allows you to pass common information shared across all data items. This can include things like API keys, browser cookies, or any other data that remains constant throughout the scraping process.&lt;/p&gt; &#xA;&lt;p&gt;It is commonly used with caching to exclude details like API keys and browser cookies from the cache key.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to use the &lt;code&gt;metadata&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.task import task&#xA;&#xA;@task()&#xA;def scrape_heading_task(driver: Driver, data, metadata):&#xA;    print(&#34;metadata:&#34;, metadata)&#xA;    print(&#34;data:&#34;, data)&#xA;&#xA;data = [&#xA;    {&#34;profile&#34;: &#34;pikachu&#34;, &#34;proxy&#34;: &#34;http://142.250.77.228:8000&#34;},&#xA;    {&#34;profile&#34;: &#34;greyninja&#34;, &#34;proxy&#34;: &#34;http://142.250.77.229:8000&#34;},&#xA;]&#xA;scrape_heading_task(&#xA;  data, &#xA;  metadata={&#34;api_key&#34;: &#34;BDEC26...&#34;}&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;async_queue&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;In the world of web scraping, there are only two types of scrapers:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Dataset Scrapers: These extract data from websites and store it as datasets. Companies like Bright Data use them to build datasets for Crunchbase, Indeed, etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Real-time Scrapers: These fetch data from sources in real-time, like SERP APIs that provide Google and DuckDuckGo search results.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When building real-time scrapers, speed is paramount because customers are waiting for requests to complete. The &lt;code&gt;async_queue&lt;/code&gt; feature is incredibly useful in such cases.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;async_queue&lt;/code&gt; allows you to run scraping tasks asynchronously in a queue and gather the results using the &lt;code&gt;.get()&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;p&gt;A great use case for &lt;code&gt;async_queue&lt;/code&gt; is scraping Google Maps. Instead of scrolling through the list of places and then scraping the details of each place sequentially, you can use &lt;code&gt;async_queue&lt;/code&gt; to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Scroll through the list of places.&lt;/li&gt; &#xA; &lt;li&gt;Simultaneously make HTTP requests to scrape the details of each place in the background.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;By executing the scrolling and requesting tasks concurrently, you can significantly speed up the scraper.&lt;/p&gt; &#xA;&lt;p&gt;Run the code below to see browser scrolling and request scraping happening concurrently (really cool, must try!):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver, AsyncQueueResult&#xA;from botasaurus.request import request, Request&#xA;import json&#xA;&#xA;def extract_title(html):&#xA;    return json.loads(&#xA;        html.split(&#34;;window.APP_INITIALIZATION_STATE=&#34;)[1].split(&#34;;window.APP_FLAGS&#34;)[0]&#xA;    )[5][3][2][1]&#xA;&#xA;@request(&#xA;    parallel=5,&#xA;    async_queue=True,&#xA;    max_retry=5,&#xA;)&#xA;def scrape_place_title(request: Request, link, metadata):&#xA;    cookies = metadata[&#34;cookies&#34;]&#xA;    html = request.get(link, cookies=cookies, timeout=12).text&#xA;    title = extract_title(html)&#xA;    print(&#34;Title:&#34;, title)&#xA;    return title&#xA;&#xA;def has_reached_end(driver):&#xA;    return driver.select(&#39;p.fontBodyMedium &amp;gt; span &amp;gt; span&#39;) is not None&#xA;&#xA;def extract_links(driver):&#xA;    return driver.get_all_links(&#39;[role=&#34;feed&#34;] &amp;gt; div &amp;gt; div &amp;gt; a&#39;)&#xA;&#xA;@browser()&#xA;def scrape_google_maps(driver: Driver, link):&#xA;    driver.google_get(link, accept_google_cookies=True)  # accepts google cookies popup&#xA;&#xA;    scrape_place_obj: AsyncQueueResult = scrape_place_title()  # initialize the async queue for scraping places&#xA;    cookies = driver.get_cookies_dict()  # get the cookies from the driver&#xA;&#xA;    while True:&#xA;        links = extract_links(driver)  # get the links to places&#xA;        scrape_place_obj.put(links, metadata={&#34;cookies&#34;: cookies})  # add the links to the async queue for scraping&#xA;&#xA;        print(&#34;scrolling&#34;)&#xA;        driver.scroll_to_bottom(&#39;[role=&#34;feed&#34;]&#39;)  # scroll to the bottom of the feed&#xA;&#xA;        if has_reached_end(driver):  # we have reached the end, let&#39;s break buddy&#xA;            break&#xA;&#xA;    results = scrape_place_obj.get()  # get the scraped results from the async queue&#xA;    return results&#xA;&#xA;scrape_google_maps(&#34;https://www.google.com/maps/search/web+developers+in+bangalore&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;run_async&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Similarly, the &lt;code&gt;run_async&lt;/code&gt; option allows you to execute scraping tasks asynchronously, enabling concurrent execution.&lt;/p&gt; &#xA;&lt;p&gt;Similar to &lt;code&gt;async_queue&lt;/code&gt;, you can use the &lt;code&gt;.get()&lt;/code&gt; method to retrieve the results of an asynchronous task.&lt;/p&gt; &#xA;&lt;p&gt;Code Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;from time import sleep&#xA;&#xA;@browser(run_async=True)&#xA;def scrape_heading(driver: Driver, data):&#xA;    sleep(5)&#xA;    return {}&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    result1 = scrape_heading()  # Launches asynchronously&#xA;    result2 = scrape_heading()  # Launches asynchronously&#xA;&#xA;    result1.get()  # Wait for the first result&#xA;    result2.get()  # Wait for the second result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;close_on_crash&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;close_on_crash&lt;/code&gt; option determines the behavior of the scraper when an exception occurs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If set to &lt;code&gt;False&lt;/code&gt; (default): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The scraper will make a beep sound and pause the browser.&lt;/li&gt; &#xA;   &lt;li&gt;This makes debugging easier by keeping the browser open at the point of the crash.&lt;/li&gt; &#xA;   &lt;li&gt;Use this setting during development and testing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If set to &lt;code&gt;True&lt;/code&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The scraper will close the browser and continue with the rest of the data items.&lt;/li&gt; &#xA;   &lt;li&gt;This is suitable for production environments when you are confident that your scraper is robust.&lt;/li&gt; &#xA;   &lt;li&gt;Use this setting to avoid interruptions and ensure the scraper processes all data items.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.browser import browser, Driver&#xA;&#xA;@browser(&#xA;    close_on_crash=False  # Determines whether the browser is paused (default: False) or closed when an error occurs&#xA;)&#xA;def scrape_heading_task(driver: Driver, data):&#xA;    raise Exception(&#34;An error occurred during scraping.&#34;)&#xA;&#xA;scrape_heading_task()  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;code&gt;output&lt;/code&gt; and &lt;code&gt;output_formats&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;By default, Botasaurus saves the result of scraping in the &lt;code&gt;output/{your_scraping_function_name}.json&lt;/code&gt; file. Let&#39;s learn about various ways to configure the output.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Change Output Filename&lt;/strong&gt;: Use the &lt;code&gt;output&lt;/code&gt; parameter in the decorator to specify a custom filename for the output.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.task import task&#xA;&#xA;@task(output=&#34;my-output&#34;)&#xA;def scrape_heading_task(data): &#xA;    return {&#34;heading&#34;: &#34;Hello, Mom!&#34;}&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Disable Output&lt;/strong&gt;: If you don&#39;t want any output to be saved, set &lt;code&gt;output&lt;/code&gt; to &lt;code&gt;None&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.task import task&#xA;&#xA;@task(output=None)&#xA;def scrape_heading_task(data): &#xA;    return {&#34;heading&#34;: &#34;Hello, Mom!&#34;}&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamically Write Output&lt;/strong&gt;: To dynamically write output based on data and result, pass a function to the &lt;code&gt;output&lt;/code&gt; parameter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.task import task&#xA;from botasaurus import bt&#xA;&#xA;def write_output(data, result):&#xA;    json_filename = bt.write_json(result, &#39;data&#39;)&#xA;    excel_filename = bt.write_excel(result, &#39;data&#39;)&#xA;    bt.zip_files([json_filename, excel_filename]) # Zip the JSON and Excel files for easy delivery to the customer&#xA;&#xA;@task(output=write_output)  &#xA;def scrape_heading_task(data): &#xA;    return {&#34;heading&#34;: &#34;Hello, Mom!&#34;}&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Upload File to S3&lt;/strong&gt;: Use &lt;code&gt;bt.upload_to_s3&lt;/code&gt; to upload file to S3 bucket.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.task import task&#xA;from botasaurus import bt&#xA;&#xA;def write_output(data, result):&#xA;    json_filename = bt.write_json(result, &#39;data&#39;)&#xA;    bt.upload_to_s3(json_filename, &#39;my-magical-bucket&#39;, &#34;AWS_ACCESS_KEY&#34;, &#34;AWS_SECRET_KEY&#34;)&#xA;&#xA;@task(output=write_output)  &#xA;def scrape_heading_task(data): &#xA;    return {&#34;heading&#34;: &#34;Hello, Mom!&#34;}&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;5.&lt;strong&gt;Save Outputs in Multiple Formats&lt;/strong&gt;: Use the &lt;code&gt;output_formats&lt;/code&gt; parameter to save outputs in different formats like JSON and EXCEL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.task import task&#xA;&#xA;@task(output_formats=[bt.Formats.JSON, bt.Formats.EXCEL])  &#xA;def scrape_heading_task(data): &#xA;    return {&#34;heading&#34;: &#34;Hello, Mom!&#34;}&#xA;&#xA;scrape_heading_task()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;PRO TIP: When delivering data to customers, provide the dataset in JSON and Excel formats. Avoid CSV unless the customer asks, because Microsoft Excel has a hard time rendering CSV files with nested JSON.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CSV vs Excel&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/csv-vs-excel.png&#34; alt=&#34;csv-vs-excel&#34; /&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Exception Handling Options&lt;/h4&gt; &#xA;&lt;p&gt;Botasaurus provides various exception handling options to make your scrapers more robust:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;max_retry&lt;/code&gt;: By default, any failed task is not retried. You can specify the maximum number of times to retry scraping when an error occurs using the &lt;code&gt;max_retry&lt;/code&gt; option.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;retry_wait&lt;/code&gt;: Specifies the waiting time between retries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;raise_exception&lt;/code&gt;: By default, Botasaurus does not raise an exception when an error occurs during scraping, because let&#39;s say you are keeping your PC running overnight to scrape 10,000 links. If one link fails, you really don&#39;t want to stop the entire scraping process, and ruin your morning by seeing an unfinished dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;must_raise_exceptions&lt;/code&gt;: Specifies exceptions that must be raised, even if &lt;code&gt;raise_exception&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;create_error_logs&lt;/code&gt;: Determines whether error logs should be created when exceptions occur. In production, when scraping hundreds of thousands of links, it&#39;s recommended to set &lt;code&gt;create_error_logs&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; to avoid using computational resources for creating error logs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(&#xA;    raise_exception=True,  # Raise an exception and halt the scraping process when an error occurs&#xA;    max_retry=5,  # Retry scraping a failed task a maximum of 5 times&#xA;    retry_wait=10,  # Wait for 10 seconds before retrying a failed task&#xA;    must_raise_exceptions=[CustomException],  # Definitely raise CustomException, even if raise_exception is set to False&#xA;    create_error_logs=False  # Disable the creation of error logs to optimize scraper performance&#xA;)&#xA;def scrape_heading_task(driver: Driver, data):&#xA;  # ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;What are some examples of common web scraping utilities provided by Botasaurus that make scraping easier?&lt;/h3&gt; &#xA;&lt;h4&gt;bt Utility&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;bt&lt;/code&gt; utility provides helper functions for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Writing and reading JSON, EXCEL, and CSV files&lt;/li&gt; &#xA; &lt;li&gt;Data cleaning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some key functions are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bt.write_json&lt;/code&gt; and &lt;code&gt;bt.read_json&lt;/code&gt;: Easily write and read JSON files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;&#xA;data = {&#34;name&#34;: &#34;pikachu&#34;, &#34;power&#34;: 101}&#xA;bt.write_json(data, &#34;output&#34;)&#xA;loaded_data = bt.read_json(&#34;output&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bt.write_excel&lt;/code&gt; and &lt;code&gt;bt.read_excel&lt;/code&gt;: Easily write and read EXCEL files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;&#xA;data = {&#34;name&#34;: &#34;pikachu&#34;, &#34;power&#34;: 101}&#xA;bt.write_excel(data, &#34;output&#34;)&#xA;loaded_data = bt.read_excel(&#34;output&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bt.write_csv&lt;/code&gt; and &lt;code&gt;bt.read_csv&lt;/code&gt;: Easily write and read CSV files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;&#xA;data = {&#34;name&#34;: &#34;pikachu&#34;, &#34;power&#34;: 101}&#xA;bt.write_csv(data, &#34;output&#34;)&#xA;loaded_data = bt.read_csv(&#34;output&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bt.write_html&lt;/code&gt; and &lt;code&gt;bt.read_html&lt;/code&gt;: Write HTML content to a file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;&#xA;html_content = &#34;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello, Mom!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&#34;&#xA;bt.write_html(html_content, &#34;output&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bt.write_temp_json&lt;/code&gt;, &lt;code&gt;bt.write_temp_csv&lt;/code&gt;, &lt;code&gt;bt.write_temp_html&lt;/code&gt;: Write temporary JSON, CSV, or HTML files for debugging purposes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;&#xA;data = {&#34;name&#34;: &#34;pikachu&#34;, &#34;power&#34;: 101}&#xA;bt.write_temp_json(data)&#xA;bt.write_temp_csv(data)&#xA;bt.write_temp_html(&#34;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello, Mom!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data cleaning functions like &lt;code&gt;bt.extract_numbers&lt;/code&gt;, &lt;code&gt;bt.extract_links&lt;/code&gt;, &lt;code&gt;bt.remove_html_tags&lt;/code&gt;, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &#34;The price is $19.99 and the website is https://www.example.com&#34;&#xA;numbers = bt.extract_numbers(text)  # [19.99]&#xA;links = bt.extract_links(text)  # [&#34;https://www.example.com&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Local Storage Utility&lt;/h4&gt; &#xA;&lt;p&gt;The Local Storage utility allows you to store and retrieve key-value pairs, which can be useful for maintaining state between scraper runs.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s how to use it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.local_storage import LocalStorage&#xA;&#xA;LocalStorage.set_item(&#34;credits_used&#34;, 100)&#xA;print(LocalStorage.get_item(&#34;credits_used&#34;, 0))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;soupify Utility&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;soupify&lt;/code&gt; utility creates a BeautifulSoup object from a Driver, Requests response, Driver Element, or HTML string.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.soupify import soupify&#xA;from botasaurus.request import request, Request&#xA;from botasaurus.browser import browser, Driver&#xA;&#xA;@request&#xA;def get_heading_from_request(req: Request, data):&#xA;   &#34;&#34;&#34;&#xA;   Get the heading of a web page using the request object.&#xA;   &#34;&#34;&#34;&#xA;   response = req.get(&#34;https://www.example.com&#34;)&#xA;   soup = soupify(response)&#xA;   heading = soup.find(&#34;h1&#34;).text&#xA;   print(f&#34;Page Heading: {heading}&#34;)&#xA;&#xA;@browser&#xA;def get_heading_from_driver(driver: Driver, data):&#xA;   &#34;&#34;&#34;&#xA;   Get the heading of a web page using the driver object.&#xA;   &#34;&#34;&#34;&#xA;   driver.get(&#34;https://www.example.com&#34;)&#xA;&#xA;   # Get the heading from the entire page&#xA;   page_soup = soupify(driver)&#xA;   page_heading = page_soup.find(&#34;h1&#34;).text&#xA;   print(f&#34;Heading from Driver&#39;s Soup: {page_heading}&#34;)&#xA;&#xA;   # Get the heading from the body element&#xA;   body_soup = soupify(driver.select(&#34;body&#34;))&#xA;   body_heading = body_soup.find(&#34;h1&#34;).text&#xA;   print(f&#34;Heading from Element&#39;s Soup: {body_heading}&#34;)&#xA;&#xA;# Call the functions&#xA;get_heading_from_request()&#xA;get_heading_from_driver()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;IP Utils&lt;/h4&gt; &#xA;&lt;p&gt;IP Utils provide functions to get information about the current IP address, such as the IP itself, country, ISP, and more:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.ip_utils import IPUtils&#xA;&#xA;# Get the current IP address&#xA;current_ip = IPUtils.get_ip()&#xA;print(current_ip)&#xA;# Output: 47.31.226.180&#xA;&#xA;# Get detailed information about the current IP address&#xA;ip_info = IPUtils.get_ip_info()&#xA;print(ip_info)&#xA;# Output: {&#xA;#     &#34;ip&#34;: &#34;47.31.226.180&#34;,&#xA;#     &#34;country&#34;: &#34;IN&#34;,&#xA;#     &#34;region&#34;: &#34;Delhi&#34;,&#xA;#     &#34;city&#34;: &#34;Delhi&#34;,&#xA;#     &#34;postal&#34;: &#34;110001&#34;,&#xA;#     &#34;coordinates&#34;: &#34;28.6519,77.2315&#34;,&#xA;#     &#34;latitude&#34;: &#34;28.6519&#34;,&#xA;#     &#34;longitude&#34;: &#34;77.2315&#34;,&#xA;#     &#34;timezone&#34;: &#34;Asia/Kolkata&#34;,&#xA;#     &#34;org&#34;: &#34;AS55836 Reliance Jio Infocomm Limited&#34;&#xA;# }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Cache Utility&lt;/h4&gt; &#xA;&lt;p&gt;The Cache utility in Botasaurus allows you to manage cached data for your scraper. You can put, get, has, remove, and clear cache data.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Basic Usage&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.task import task&#xA;from botasaurus.cache import Cache&#xA;&#xA;# Example scraping function&#xA;@task&#xA;def scrape_data(data):&#xA;    # Your scraping logic here&#xA;    return {&#34;processed&#34;: data}&#xA;&#xA;# Sample data for scraping&#xA;input_data = {&#34;key&#34;: &#34;value&#34;}&#xA;&#xA;# Adding data to the cache&#xA;Cache.put(&#39;scrape_data&#39;, input_data, scrape_data(input_data))&#xA;&#xA;# Checking if data is in the cache&#xA;if Cache.has(&#39;scrape_data&#39;, input_data):&#xA;    # Retrieving data from the cache&#xA;    cached_data = Cache.get(&#39;scrape_data&#39;, input_data)&#xA;    print(f&#34;Cached data: {cached_data}&#34;)&#xA;&#xA;# Removing specific data from the cache&#xA;Cache.remove(&#39;scrape_data&#39;, input_data)&#xA;&#xA;# Clearing the complete cache for the scrape_data function&#xA;Cache.clear(&#39;scrape_data&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Advanced Usage for large-scale scraping projects&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Count Cached Items&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can count the number of items cached for a particular function, which can serve as a scraping progress bar.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.cache import Cache&#xA;&#xA;Cache.print_cached_items_count(&#39;scraping_function&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Filter Cached/Uncached Items&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can filter items that have been cached or not cached for a particular function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.cache import Cache&#xA;&#xA;all_items = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;]&#xA;&#xA;# Get items that are cached&#xA;cached_items = Cache.filter_items_in_cache(&#39;scraping_function&#39;, all_items)&#xA;print(cached_items)&#xA;&#xA;# Get items that are not cached&#xA;uncached_items = Cache.filter_items_not_in_cache(&#39;scraping_function&#39;, all_items)&#xA;print(uncached_items)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Delete Cache&lt;/em&gt; The cache for a function is stored in the &lt;code&gt;cache/{your_scraping_function_name}/&lt;/code&gt; folder. To delete the cache, simply delete that folder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-cache.png&#34; alt=&#34;delete-cache&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Delete Specific Items&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can delete specific items from the cache for a particular function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.cache import Cache&#xA;&#xA;all_items = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;]&#xA;deleted_count = Cache.delete_items(&#39;scraping_function&#39;, all_items)&#xA;print(f&#34;Deleted {deleted_count} items from the cache.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Delete Items by Filter&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;In some cases, you may want to delete specific items from the cache based on a condition. For example, if you encounter honeypots (mock HTML served to dupe web scrapers) while scraping a website, you may want to delete those items from the cache.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def should_delete_item(item, result):&#xA;    if &#39;Honeypot Item&#39; in result:&#xA;        return True  # Delete the item&#xA;    return False  # Don&#39;t delete the item&#xA;&#xA;all_items = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;]&#xA;# List of items to iterate over, it is fine if the list contains items which have not been cached, as they will be simply ignored.&#xA;Cache.delete_items_by_filter(&#39;scraping_function&#39;, should_delete_item, all_items)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Importantly, be cautious and first use &lt;code&gt;delete_items_by_filter&lt;/code&gt; on a small set of items which you want to be deleted. Here&#39;s an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;from botasaurus.cache import Cache&#xA;&#xA;def should_delete_item(item, result):&#xA;    # TODO: Update the logic&#xA;    if &#39;Honeypot Item&#39; in result:&#xA;        return True # Delete the item&#xA;    return False # Don&#39;t delete the item&#xA;&#xA;test_items = [&#39;1&#39;, &#39;2&#39;] # TODO: update with target items&#xA;scraping_function_name = &#39;scraping_function&#39; # TODO:  update with target scraping function name&#xA;Cache.delete_items_by_filter(scraping_function_name, test_items, should_delete_item)&#xA;&#xA;for item in test_items:&#xA;    if Cache.has(scraping_function_name, item):&#xA;        bt.prompt(f&#34;Item {item} was not deleted. Please review the logic of the should_delete_item function.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to Extract Links from a Sitemap?&lt;/h3&gt; &#xA;&lt;p&gt;In web scraping, it is a common use case to scrape product pages, blogs, etc. But before scraping these pages, you need to get the links to these pages.&lt;/p&gt; &#xA;&lt;p&gt;Sadly, many developers unnecessarily increase their work by writing code to visit each page one by one and scrape links, which they could have easily obtained by just looking at the Sitemap.&lt;/p&gt; &#xA;&lt;p&gt;The Botasaurus Sitemap Module makes this process easy as cake by allowing you to get all links or sitemaps using:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The homepage URL (e.g., &lt;code&gt;https://www.omkar.cloud/&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;A direct sitemap link (e.g., &lt;code&gt;https://www.omkar.cloud/sitemap.xml&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;A &lt;code&gt;.gz&lt;/code&gt; compressed sitemap&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, if you&#39;re an Angel Investor seeking innovative tech startups to invest in, G2 is an ideal platform to find such startups. You can run the following code to fetch over 190K+ product links from G2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;from botasaurus.sitemap import Sitemap, Filters, Extractors&#xA;&#xA;links = (&#xA;    Sitemap(&#34;https://www.g2.com/sitemaps/sitemap_index.xml.gz&#34;)&#xA;    .filter(Filters.first_segment_equals(&#34;products&#34;))&#xA;    .extract(Extractors.extract_link_upto_second_segment())&#xA;    .write_links(&#39;g2-products&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/g2-sitemap-links.png&#34; alt=&#34;g2-sitemap-links.png&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or, let&#39;s say you&#39;re in the mood for some reading and looking for good stories. The following code will get you over 1000+ stories from &lt;a href=&#34;https://moralstories26.com/&#34;&gt;moralstories26.com&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;from botasaurus.sitemap import Sitemap, Filters&#xA;&#xA;links = (&#xA;    Sitemap(&#34;https://moralstories26.com/&#34;)&#xA;    .filter(&#xA;        Filters.has_exactly_1_segment(),&#xA;        Filters.first_segment_not_equals(&#xA;            [&#34;about&#34;, &#34;privacy-policy&#34;, &#34;akbar-birbal&#34;, &#34;animal&#34;, &#34;education&#34;, &#34;fables&#34;, &#34;facts&#34;, &#34;family&#34;, &#34;famous-personalities&#34;, &#34;folktales&#34;, &#34;friendship&#34;, &#34;funny&#34;, &#34;heartbreaking&#34;, &#34;inspirational&#34;, &#34;life&#34;, &#34;love&#34;, &#34;management&#34;, &#34;motivational&#34;, &#34;mythology&#34;, &#34;nature&#34;, &#34;quotes&#34;, &#34;spiritual&#34;, &#34;uncategorized&#34;, &#34;zen&#34;]&#xA;        ),&#xA;    )&#xA;    .write_links(&#39;moral-stories&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/moralstories26-sitemap-links.png&#34; alt=&#34;moralstories26-sitemap-links.png&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also, before scraping a site, it&#39;s useful to identify the available sitemaps. This can be easily done with the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;from botasaurus.sitemap import Sitemap&#xA;&#xA;sitemaps = Sitemap(&#34;https://www.omkar.cloud/&#34;).write_sitemaps(&#39;omkar-sitemaps&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/omkar-sitemap-links.png&#34; alt=&#34;omkar-sitemap-links.png&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;To ensure your scrapers run super fast, we cache the Sitemap, but you may want to periodically refresh the cache. To do so, pass the Cache.REFRESH parameter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;from botasaurus.sitemap import Sitemap, Filters, Extractors&#xA;from botasaurus.cache import Cache&#xA;&#xA;links = (&#xA;    Sitemap(&#34;https://moralstories26.com/&#34;, cache=Cache.REFRESH)&#xA;    .write_links(&#39;moral-stories&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How can I filter a list of links, similar to working with Sitemaps?&lt;/h3&gt; &#xA;&lt;p&gt;Filtering links from a webpage is a common requirement in web scraping. For example, you might want to filter out all non-product pages.&lt;/p&gt; &#xA;&lt;p&gt;Botasaurus&#39;s &lt;code&gt;Links&lt;/code&gt; module simplifies link filtering and extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.links import Links, Filters, Extractors&#xA;&#xA;# Sample list of links&#xA;links = [&#xA;    &#34;https://finance.yahoo.com/topic/stock-market-news/&#34;,&#xA;    &#34;https://finance.yahoo.com/topic/morning-brief/&#34;, &#xA;    &#34;https://finance.yahoo.com/quote/AAPL/&#34;, &#xA;    &#34;https://finance.yahoo.com/quote/GOOG/&#34;&#xA;]&#xA;&#xA;# Filter and extract links&#xA;filtered_links = (&#xA;    Links(links)&#xA;    .filter(Filters.first_segment_equals(&#34;quote&#34;))&#xA;    .extract(Extractors.extract_link_upto_second_segment())&#xA;    .write(&#39;stocks&#39;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;What is the best way to use caching in Botasaurus?&lt;/h3&gt; &#xA;&lt;p&gt;Sadly, when using caching, most developers write a scraping function that scrapes the HTML and extracts the data from the HTML in the same function, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus.request import request, Request&#xA;from botasaurus.soupify import soupify&#xA;&#xA;@request&#xA;def scrape_data(request: Request, data):&#xA;    # Visit the Link&#xA;    response = request.get(data)&#xA;    &#xA;    # Create a BeautifulSoup object&#xA;    soup = soupify(response)&#xA;    &#xA;    # Retrieve the heading element&#39;s text&#xA;    heading = soup.find(&#39;h1&#39;).get_text()&#xA;    &#xA;    # Save the data as a JSON file in output/scrape_data.json&#xA;    return {&#34;heading&#34;: heading}&#xA;&#xA;data_items = [&#xA;    &#34;https://stackoverflow.blog/open-source&#34;,&#xA;    &#34;https://stackoverflow.blog/ai&#34;,&#xA;    &#34;https://stackoverflow.blog/productivity&#34;,&#xA;]&#xA;&#xA;scrape_data(data_items)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, let&#39;s say, after 50% of the dataset has been scraped, what if:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Your customer wants to add another data point (which is very likely), or&lt;/li&gt; &#xA; &lt;li&gt;One of your BeautifulSoup selectors happens to be flaky and needs to be updated (which is super likely)?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In such cases, you will have to scrape all the pages again, which is painful as it will take a lot of time and incur high proxy costs.&lt;/p&gt; &#xA;&lt;p&gt;To resolve this issue, you can:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Write a function that only scrapes and caches the HTML.&lt;/li&gt; &#xA; &lt;li&gt;Write a separate function that calls the HTML scraping function, extracts data using BeautifulSoup, and caches the result.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here&#39;s a practical example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bs4 import BeautifulSoup&#xA;from botasaurus.task import task&#xA;from botasaurus.request import request, Request&#xA;from botasaurus.soupify import soupify&#xA;&#xA;@request(cache=True)&#xA;def scrape_html(request: Request, link):&#xA;    # Scrape the HTML and cache it&#xA;    html = request.get(link).text&#xA;    return html&#xA;&#xA;def extract_data(soup: BeautifulSoup):&#xA;    # Extract the heading from the HTML&#xA;    heading = soup.find(&#34;h1&#34;).get_text()&#xA;    return {&#34;heading&#34;: heading}&#xA;&#xA;# Cache the scrape_data task as well&#xA;@task(cache=True)&#xA;def scrape_data(link):&#xA;    # Call the scrape_html function to get the cached HTML&#xA;    html = scrape_html(link)&#xA;    # Extract data from the HTML using the extract_data function&#xA;    return extract_data(soupify(html))&#xA;&#xA;data_items = [&#xA;    &#34;https://stackoverflow.blog/open-source&#34;,&#xA;    &#34;https://stackoverflow.blog/ai&#34;,&#xA;    &#34;https://stackoverflow.blog/productivity&#34;,&#xA;]&#xA;&#xA;scrape_data(data_items)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With this approach:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you need to add data points or fix BeautifulSoup bugs, delete the &lt;code&gt;cache/scrape_data&lt;/code&gt; folder and re-run the scraper. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-cache.png&#34; alt=&#34;delete-cache&#34; /&gt;&lt;/li&gt; &#xA; &lt;li&gt;You only need to re-run the BeautifulSoup extraction, not the entire HTML scraping, saving time and proxy costs. Yahoo!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;PRO TIP&lt;/strong&gt;: This approach also makes your &lt;code&gt;extract_data&lt;/code&gt; code easier and faster to test, like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bs4 import BeautifulSoup&#xA;from botasaurus import bt&#xA;&#xA;def extract_data(soup: BeautifulSoup):&#xA;    heading = soup.find(&#39;h1&#39;).get_text()&#xA;    return {&#34;heading&#34;: heading}&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    # Will use the cached HTML and run the extract_data function again.&#xA;    bt.write_temp_json(scrape_data(&#34;https://stackoverflow.blog/open-source&#34;, cache=False))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;What are the recommended settings for each decorator to build a production-ready scraper in Botasaurus?&lt;/h3&gt; &#xA;&lt;p&gt;For websites with minimal protection, use the &lt;code&gt;Request&lt;/code&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a template for creating production-ready datasets using the &lt;code&gt;Request&lt;/code&gt; module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bs4 import BeautifulSoup&#xA;from botasaurus.task import task&#xA;from botasaurus.request import request, Request,NotFoundException&#xA;from botasaurus.soupify import soupify&#xA;&#xA;@request(&#xA;    # proxy=&#39;http://username:password@datacenter-proxy-domain:proxy-port&#39;, # Uncomment to use Proxy ONLY if you face IP blocking&#xA;    cache=True,&#xA;&#xA;    max_retry=20, # Retry up to 20 times, which is a good default&#xA;&#xA;    output=None,&#xA;&#xA;    close_on_crash=True,&#xA;    raise_exception=True,&#xA;    create_error_logs=False,&#xA;)&#xA;def scrape_html(request: Request, link):&#xA;    # Scrape the HTML and cache it&#xA;    response = request.get(link)&#xA;    if response.status_code == 404:&#xA;        # A Special Exception to skip retrying this link&#xA;        raise NotFoundException(link)&#xA;    response.raise_for_status()&#xA;    return response.text&#xA;&#xA;def extract_data(soup: BeautifulSoup):&#xA;    # Extract the heading from the HTML&#xA;    heading = soup.find(&#34;h1&#34;).get_text()&#xA;    return {&#34;heading&#34;: heading}&#xA;&#xA;# Cache the scrape_data task as well&#xA;@task(&#xA;    cache=True,&#xA;    close_on_crash=True,&#xA;    create_error_logs=False,&#xA;    parallel=40, # Run 40 requests in parallel, which is a good default&#xA;)&#xA;def scrape_data(link):&#xA;    # Call the scrape_html function to get the cached HTML&#xA;    html = scrape_html(link)&#xA;    # Extract data from the HTML using the extract_data function&#xA;    return extract_data(soupify(html))&#xA;&#xA;data_items = [&#xA;    &#34;https://stackoverflow.blog/open-source&#34;,&#xA;    &#34;https://stackoverflow.blog/ai&#34;,&#xA;    &#34;https://stackoverflow.blog/productivity&#34;,&#xA;]&#xA;&#xA;scrape_data(data_items)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For visiting well protected websites, use the &lt;code&gt;Browser&lt;/code&gt; module.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a template for creating production-ready datasets using the &lt;code&gt;Browser&lt;/code&gt; module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bs4 import BeautifulSoup&#xA;from botasaurus.task import task&#xA;from botasaurus.browser import browser, Driver, NotFoundException&#xA;from botasaurus.soupify import soupify&#xA;&#xA;@browser(&#xA;    # proxy=&#39;http://username:password@datacenter-proxy-domain:proxy-port&#39;, # Uncomment to use Proxy ONLY if you face IP blocking&#xA;&#xA;    # block_images_and_css=True, # Uncomment to block images and CSS, which can speed up scraping&#xA;    # wait_for_complete_page_load=False, # Uncomment to proceed once the DOM (Document Object Model) is loaded, without waiting for all resources to finish loading. This is recommended for faster scraping of Server Side Rendered (HTML) pages.&#xA;&#xA;    cache=True,&#xA;    max_retry=5,  # Retry up to 5 times, which is a good default&#xA;&#xA;    reuse_driver= True, # Reuse the same driver for all tasks&#xA;    &#xA;    output=None,&#xA;&#xA;    close_on_crash=True,&#xA;    raise_exception=True,&#xA;    create_error_logs=False,&#xA;)&#xA;def scrape_html(driver: Driver, link):&#xA;    # Scrape the HTML and cache it&#xA;    if driver.config.is_new:&#xA;        driver.google_get(&#xA;            link,&#xA;            bypass_cloudflare=True,  # delete this line if the website you&#39;re accessing is not protected by Cloudflare&#xA;        )&#xA;    response = driver.requests.get(link)&#xA;    &#xA;    if response.status_code == 404:&#xA;        # A Special Exception to skip retrying this link&#xA;        raise NotFoundException(link)&#xA;    response.raise_for_status()&#xA;    &#xA;    html = response.text        &#xA;    return html&#xA;&#xA;def extract_data(soup: BeautifulSoup):&#xA;    # Extract the heading from the HTML&#xA;    stock_name = soup.select_one(&#39;[data-testid=&#34;quote-hdr&#34;] h1&#39;).get_text()&#xA;    stock_price = soup.select_one(&#39;[data-testid=&#34;qsp-price&#34;]&#39;).get_text()&#xA;    &#xA;    return {&#xA;        &#34;stock_name&#34;: stock_name,&#xA;        &#34;stock_price&#34;: stock_price,&#xA;    }&#xA;&#xA;# Cache the scrape_data task as well&#xA;@task(&#xA;    cache=True,&#xA;    close_on_crash=True,&#xA;    create_error_logs=False,&#xA;)&#xA;def scrape_data(link):&#xA;    # Call the scrape_html function to get the cached HTML&#xA;    html = scrape_html(link)&#xA;    # Extract data from the HTML using the extract_data function&#xA;    return extract_data(soupify(html))&#xA;&#xA;data_items = [&#xA;    &#34;https://finance.yahoo.com/quote/AAPL/&#34;,&#xA;    &#34;https://finance.yahoo.com/quote/GOOG/&#34;,&#xA;    &#34;https://finance.yahoo.com/quote/MSFT/&#34;,&#xA;]&#xA;&#xA;scrape_data(data_items)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Why Am I Getting Detected on Some Websites?&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re getting detected, it&#39;s likely due to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Using a non-residential proxy ‚Äî Services like Datadome and Cloudflare often flag datacenter IPs/VPNs.&lt;/li&gt; &#xA; &lt;li&gt;Clicking without Human Mode enabled ‚Äî Unnatural mouse movements can trigger detection.&lt;/li&gt; &#xA; &lt;li&gt;Visiting websites too quickly ‚Äî Rapid, bot-like navigation is easy to detect.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To reduce detection, follow these best practices:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Upgrade all Botasaurus packages to the latest version:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install --upgrade bota botasaurus botasaurus-api botasaurus-requests botasaurus-driver botasaurus-proxy-authentication botasaurus-server botasaurus-humancursor&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enable Human Mode for human-like mouse movements:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver.enable_human_mode()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Avoid rapid &lt;code&gt;driver.get&lt;/code&gt; calls. Instead, try:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Clicking within pages with Human Mode enabled, if possible.&lt;/li&gt; &#xA;   &lt;li&gt;Using &lt;code&gt;driver.google_get&lt;/code&gt; or &lt;code&gt;driver.get_via_this_page&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Using &lt;a href=&#34;https://github.com/omkarcloud/botasaurus?tab=readme-ov-file#how-to-significantly-reduce-proxy-costs-when-scraping-at-scale&#34;&gt;&lt;code&gt;driver.requests.get&lt;/code&gt;&lt;/a&gt; to fetch the page HTML content.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Slow down your scraper with random delays:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;driver.short_random_sleep()&#xA;driver.long_random_sleep()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Avoid using &lt;code&gt;headless&lt;/code&gt; mode, as it can be easily detected by Cloudflare, Datadome, and Imperva.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use a residential proxy, as datacenter IPs are often flagged.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Remove the &lt;code&gt;--no-default-browser-check&lt;/code&gt; argument as it is detectable by systems like Datadome, as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(remove_default_browser_check_argument=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If your IP has been flagged, you can perform this technique to change it:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit &lt;a href=&#34;https://whatismyipaddress.com/&#34;&gt;whatismyipaddress.com&lt;/a&gt; to see your current IP Address.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Connect your PC to a smartphone&#39;s hotspot.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;On your smartphone, turn airplane mode on and off.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Turn the hotspot on again.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now visit &lt;a href=&#34;https://whatismyipaddress.com/&#34;&gt;whatismyipaddress.com&lt;/a&gt;. You&#39;ll see a new IP address.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;How Do I Close All Running Chrome Instances?&lt;/h3&gt; &#xA;&lt;p&gt;While developing a scraper, multiple browser instances may remain open in the background (because of interrupting it with CTRL + C). This situation can cause your computer to hang.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/chrome-running.png&#34; alt=&#34;Many Chrome processes running in Task Manager&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;To prevent your PC from hanging, you can run the following command to close all Chrome instances:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m close_chrome&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to Run Scraper in Docker?&lt;/h3&gt; &#xA;&lt;p&gt;To run a Scraper in Docker, use the Botasaurus Starter Template, which includes the necessary Dockerfile and Docker Compose configurations.&lt;/p&gt; &#xA;&lt;p&gt;Use the following commands to clone the Botasaurus Starter Template, build a Docker image from it, and execute the scraper within a Docker environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/omkarcloud/botasaurus-starter my-botasaurus-project&#xA;cd my-botasaurus-project&#xA;docker-compose build&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to Run Scraper in Gitpod?&lt;/h3&gt; &#xA;&lt;p&gt;Running a scraper in Gitpod offers several benefits:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allows your scraper to use a powerful 8-core machine with 1000 Mbps internet speed&lt;/li&gt; &#xA; &lt;li&gt;Makes it easy to showcase your scraper to customers without them having to install anything, by simply sharing the Gitpod machine link&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In this example, we will run the Botasaurus Starter template in Gitpod:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;First, visit &lt;a href=&#34;https://gitpod.io/#https://github.com/omkarcloud/botasaurus-starter&#34;&gt;this link&lt;/a&gt; and sign up using your GitHub account.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/google-maps-scraper/master/screenshots/open-in-gitpod.png&#34; alt=&#34;Screenshot (148)&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once signed up, open the starter project in Gitpod.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/assets/master/images/gitpod-continue.png&#34; alt=&#34;gp-continue&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To use UI Scraper, run the following command in Terminal:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You will see a popup notification with the heading &#34;A service is available on port 3000&#34;. In the popup notification, click the &lt;strong&gt;&#34;Open Browser&#34;&lt;/strong&gt; button to open the UI Dashboard in your browser&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/open-browser.png&#34; alt=&#34;open-browser.png&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, you can press the &lt;code&gt;Run&lt;/code&gt; button to get the results.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/starter-photo.png&#34; alt=&#34;starter-photo.png&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: Gitpod is not suitable for long-running tasks, as the environment will automatically shut down after a short period of inactivity. Use your local machine or a cloud VM for long-running scrapers.&lt;/p&gt; &#xA;&lt;h2&gt;Should I Scrape Datasets Locally or in the Cloud?&lt;/h2&gt; &#xA;&lt;p&gt;For most scraping tasks, we recommend running the scraper &lt;strong&gt;locally&lt;/strong&gt; on your system because:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It requires far fewer setup steps&lt;/li&gt; &#xA; &lt;li&gt;It saves time and costs&lt;/li&gt; &#xA; &lt;li&gt;Most importantly, it allows you to quickly fix bugs and continue scraping.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;However, consider cloud scraping when:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Running tasks longer than 5 days.&lt;/li&gt; &#xA; &lt;li&gt;Scraping large-scale data (terabytes).&lt;/li&gt; &#xA; &lt;li&gt;Performing recurring monthly scrapes.&lt;/li&gt; &#xA; &lt;li&gt;Having slow Internet or data caps&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cloud scraping is also significantly faster due to superior internet speeds (often 10x+ faster than home Wi-Fi).&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;How to Run a Data Scraper in a Virtual Machine?&lt;/h2&gt; &#xA;&lt;p&gt;To run a scraper in a virtual machine (VM), follow these steps:&lt;/p&gt; &#xA;&lt;h3&gt;1. Prepare Your Scraper&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://github.com/omkarcloud/botasaurus-starter&#34;&gt;Botasaurus Starter Template&lt;/a&gt; to create your dataset scraper.&lt;/li&gt; &#xA; &lt;li&gt;For large datasets, ensure memory efficiency (e.g., by using file formats like &lt;code&gt;ndjson&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Add project dependencies to &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Push your project to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. Set Up a Virtual Machine&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;If you don&#39;t already have one, create a Google Cloud Account. You&#39;ll receive a $300 credit to use over 3 months. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/Select-your-billing-country.png&#34; alt=&#34;Select-your-billing-country&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/click-to-deploy-images/nodejs&#34;&gt;Google Click to Deploy&lt;/a&gt;, create a deployment and configure it as follows or as appropriate based on your workload:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Zone: us-central1-a # Use us-central1 (Iowa) for the lowest-cost VMs&#xA;Series: N1&#xA;Machine Type: n1-standard-2 (2 vCPU, 1 core, 7.5 GB memory)&#xA;Boot Disk Type: Standard persistent disk&#x9;# This is the most cost-effective disk option.&#xA;Boot disk size in GB: 20 GB # Adjust based on storage needs  &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/deploy-node-vm.gif&#34; alt=&#34;Deployment setup&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;VM Instances&lt;/a&gt; and click the SSH button to SSH into the VM. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/ssh-vm.png&#34; alt=&#34;ssh-vm&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, run the following command in the terminal and wait for it to complete:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sL https://raw.githubusercontent.com/omkarcloud/botasaurus/master/vm-scripts/install-bota.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Install your scraper by running the following command. It may take 5 minutes to install the scraper, so grab a coffee or watch &lt;a href=&#34;https://www.youtube.com/watch?v=nwAYpLVyeFU&#34;&gt;this awesome video&lt;/a&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m bota install-scraper --repo-url https://github.com/omkarcloud/botasaurus-starter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/install-scraper-vm.gif&#34; alt=&#34;install-scraper&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: If you are using a different repo, replace &lt;code&gt;https://github.com/omkarcloud/botasaurus-starter&lt;/code&gt; with your repo URL.&lt;/p&gt; &#xA;&lt;p&gt;That&#39;s it! You have successfully installed the scraper in a virtual machine. The scraper will now start running and succeed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/vm-succeed.png&#34; alt=&#34;ls-output&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Notes&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;main.py&lt;/code&gt; file serves as the scraper&#39;s entry point.&lt;/li&gt; &#xA; &lt;li&gt;Update your project&#39;s &lt;code&gt;requirements.txt&lt;/code&gt; to ensure it has all the dependencies required to run the scraper.&lt;/li&gt; &#xA; &lt;li&gt;Ensure your VM has enough memory for your scraper&#39;s needs.&lt;/li&gt; &#xA; &lt;li&gt;If running a headful browser, enable a virtual display by setting &lt;code&gt;enable_xvfb_virtual_display=True&lt;/code&gt;. This creates a virtual display required for running a headful browser in a VM. &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(enable_xvfb_virtual_display=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;The scraper will run until it completes successfully or fails three times. You can also configure retries as follows: For example, to allow a maximum of 5 retries: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m bota install-scraper --repo-url &amp;lt;your-repo&amp;gt; --max-retry=5&#xA;&lt;/code&gt;&lt;/pre&gt; or, to allow unlimited retries: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m bota install-scraper --repo-url &amp;lt;your-repo&amp;gt; --max-retry=unlimited&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;If your scraper fails, you can check the logs of the current boot by running: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;journalctl -u botasaurus-starter.service -b # replace botasaurus-starter with your repo name&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Downloading Data&lt;/em&gt; To download the scraped data, you can either:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download it directly from the VM. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/download-data-vm.png&#34; alt=&#34;Download Data from VM&#34; /&gt;&lt;/li&gt; &#xA; &lt;li&gt;Upload it to Amazon S3: &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from botasaurus import bt&#xA;bt.upload_to_s3(&#39;data.json&#39;, &#39;my-bucket&#39;, &#34;AWS_ACCESS_KEY&#34;, &#34;AWS_SECRET_KEY&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to Stop the Scraper&lt;/h2&gt; &#xA;&lt;p&gt;If you are performing recurring monthly scrapes, it&#39;s best to stop the scraper instead of deleting it. Note that you will only incur storage costs (~$0.4 per month for a 10GB Standard Persistent Disk) but not compute costs.&lt;/p&gt; &#xA;&lt;p&gt;To stop the scraper:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Visit &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;VM Instances&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Select your VM and stop it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/stop-scraper-in-vm.png&#34; alt=&#34;stop-scraper-in-vm&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;To restart later:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start the VM from &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;VM Instances&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/run-scraper-in-vm.png&#34; alt=&#34;run-scraper-in-vm&#34; /&gt; 2. SSH into it. 3. Delete caches and update sitemaps if needed. 4. Restart with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;shutdown -r now&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to Delete the Scraper and Avoid Incurring Further Charges&lt;/h2&gt; &#xA;&lt;p&gt;If you no longer need the scraper, please ensure you have downloaded your data before deleting it.&lt;/p&gt; &#xA;&lt;p&gt;Next, follow these steps to delete the scraper:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://console.cloud.google.com/products/solutions/deployments&#34;&gt;Deployments&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Delete your deployment.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-deployment.png&#34; alt=&#34;Delete deployment&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;That&#39;s it! You have successfully deleted the scraper, and you will not incur any disk or compute charges.&lt;/p&gt; &#xA;&lt;h2&gt;How to Run a UI Scraper in a Virtual Machine&lt;/h2&gt; &#xA;&lt;p&gt;To run your scraper in a virtual machine, we will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a static IP&lt;/li&gt; &#xA; &lt;li&gt;Create a VM with that IP&lt;/li&gt; &#xA; &lt;li&gt;SSH into the VM&lt;/li&gt; &#xA; &lt;li&gt;Install the scraper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now, follow these steps to run your scraper in a virtual machine:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a Google Cloud Account if you don&#39;t already have one. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/Select-your-billing-country.png&#34; alt=&#34;Select-your-billing-country&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit the &lt;a href=&#34;https://console.cloud.google.com/welcome?cloudshell=true&#34;&gt;Google Cloud Console&lt;/a&gt; and click the Cloud Shell button. A terminal will open up. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/click-cloud-shell-btn.png&#34; alt=&#34;click-cloud-shell-btn&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following commands in the terminal:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install bota&#xA;python -m bota create-ip&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will be asked for a VM name. Enter any name you like, such as &#34;pikachu&#34;.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Name: pikachu&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;Then, you will be asked for the region for the scraper. Press Enter to go with the default, which is &#34;us-central1&#34;, as it has the lowest-cost VMs.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Region: Default&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/install-bota.gif&#34; alt=&#34;Install bota&#34; /&gt;&lt;/p&gt; &lt;p&gt;With this a static IP address will be created for your VM, which you can use to access your app later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/click-to-deploy-images/nodejs&#34;&gt;Google Click to Deploy&lt;/a&gt;, create a deployment and configure it as follows or as appropriate based on your workload:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Zone: us-central1-a # Use the zone from the region you selected in the previous step.&#xA;Series: N1&#xA;Machine Type: n1-standard-2 (2 vCPU, 1 core, 7.5 GB memory)&#xA;Boot Disk Type: Standard persistent disk&#x9;# This is the most cost-effective disk option.&#xA;Boot disk size in GB: 20 GB # Adjust based on storage needs  &#xA;Network Interface [External IP]: pikachu-ip # Use the IP name you created in the previous step.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/deploy-node.gif&#34; alt=&#34;deploy-node&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;VM Instances&lt;/a&gt; and click the SSH button to SSH into the VM. &lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/ssh-vm.png&#34; alt=&#34;ssh-vm&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Now, run the following command in the terminal:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sL https://raw.githubusercontent.com/omkarcloud/botasaurus/master/vm-scripts/install-bota.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, install the UI scraper by running the following command, then wait for 5 minutes for it to complete:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m bota install-ui-scraper --repo-url https://github.com/omkarcloud/botasaurus-starter&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/install-scraper.gif&#34; alt=&#34;install-scraper&#34; /&gt; Note: If you are using a different repo, replace &lt;code&gt;https://github.com/omkarcloud/botasaurus-starter&lt;/code&gt; with your repo URL.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;That&#39;s it! You have successfully launched the scraper in a virtual machine. When the previous commands are done, you will see a &lt;strong&gt;link&lt;/strong&gt; to your scraper. Visit it to run your scraper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/vm-success.gif&#34; alt=&#34;vm-success&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Notes&lt;/em&gt; - Update your project&#39;s &lt;code&gt;requirements.txt&lt;/code&gt; to ensure it has all the dependencies required to run the scraper. - Ensure your VM has enough memory for your scraper&#39;s needs. - If running a headful browser, enable a virtual display by setting &lt;code&gt;enable_xvfb_virtual_display=True&lt;/code&gt;. This creates a virtual display required for running a headful browser in a VM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@browser(enable_xvfb_virtual_display=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;- The UI scraper will run indefinitely and will be available at the printed link.&#xA;- If your UI scraper fails, you can check the logs of the current boot by running:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;journalctl -u backend.service -b &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to Delete the UI Scraper and Avoid Incurring Further Charges&lt;/h2&gt; &#xA;&lt;p&gt;If you no longer need the scraper, please ensure you have downloaded your data before deleting it.&lt;/p&gt; &#xA;&lt;p&gt;Next, follow these steps to delete the scraper:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Delete the static IP by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m bota delete-ip&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will be asked for the name of the VM you created in the first step. Enter the name and press Enter.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-ip.png&#34; alt=&#34;Delete IP&#34; /&gt;&lt;/p&gt; &lt;p&gt;Note: If you forgot the name of the IP, you can also delete all the IPs by running &lt;code&gt;python -m bota delete-all-ips&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;https://console.cloud.google.com/products/solutions/deployments&#34;&gt;Deployments&lt;/a&gt; and delete your deployment.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/botasaurus/master/images/delete-deployment.png&#34; alt=&#34;Delete deployment&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;That&#39;s it! You have successfully deleted the scraper, and you will not incur any further charges.&lt;/p&gt; &#xA;&lt;h3&gt;How to Run Scraper in Kubernetes?&lt;/h3&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/run-scraper-in-kubernetes.md&#34;&gt;this link&lt;/a&gt; to learn how to run scraper at scale using Kubernetes.&lt;/p&gt; &#xA;&lt;h3&gt;I have a feature request!&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;d love to hear it! Share them on &lt;a href=&#34;https://github.com/omkarcloud/botasaurus/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.omkar.cloud/l/botasaurus-make-discussions/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/omkarcloud/google-maps-scraper/master/screenshots/ask-on-github.png&#34; alt=&#34;Make&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- &#xA;### Do you have a Discord community?&#xA;&#xA;Yes, we have a Discord community where you can connect with other developers, ask questions, and share your experiences. Join our Discord community [here](https://discord.com/invite/rw9VeyuSM8). --&gt; &#xA;&lt;h3&gt;‚ùì Advanced Questions&lt;/h3&gt; &#xA;&lt;p&gt;Congratulations on completing the Botasaurus Documentation! Now, you have all the knowledge needed to effectively use Botasaurus.&lt;/p&gt; &#xA;&lt;p&gt;You may choose to read the following questions based on your interests:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-run-botasaurus-in-google-colab&#34;&gt;How to Run Botasaurus in Google Colab?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-allow-users-to-filter-the-scraped-data&#34;&gt;How can I allow users to filter the scraped data?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-allow-the-user-to-sort-the-scraped-data&#34;&gt;How can I allow the user to sort the scraped data?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-present-the-scraped-data-in-different-views&#34;&gt;How can I present the scraped data in different views?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#when-building-a-large-dataset-customers-often-request-data-in-different-formats-like-overview-and-review-how-can-i-do-that&#34;&gt;When building a large dataset, customers often request data in different formats like overview and review. How can I do that?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#what-more-can-i-configure-when-adding-a-scraper&#34;&gt;What more can I configure when adding a scraper?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-control-the-maximum-number-of-browsers-and-requests-running-at-any-point-of-time&#34;&gt;How to control the maximum number of browsers and requests running at any point of time?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-do-i-change-the-title-header-title-and-description-of-the-scraper&#34;&gt;How do I change the title, header title, and description of the scraper?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-can-i-use-a-database-like-postgresql-with-ui-scraper&#34;&gt;How can I use a database like PostgreSQL with UI Scraper?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#which-postgresql-provider-should-i-choose-among-supabase-google-cloud-sql-heroku-and-amazon-rds&#34;&gt;Which PostgreSQL provider should I choose among Supabase, Google Cloud SQL, Heroku, and Amazon RDS?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-create-a-postgresql-database-on-supabase&#34;&gt;How to create a PostgreSQL database on Supabase?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#how-to-create-a-postgresql-database-on-google-cloud&#34;&gt;How to create a PostgreSQL database on Google Cloud?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/raw/master/advanced.md#i-am-a-youtuber-should-i-create-youtube-videos-about-botasaurus-if-so-how-can-you-help-me&#34;&gt;I am a Youtuber, Should I create YouTube videos about Botasaurus? If so, how can you help me?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Thank You&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To That, who has given me a sufficiently intelligent mind to create Botasaurus and do a lot of good.&lt;/li&gt; &#xA; &lt;li&gt;I made Botasaurus because I would be really happy if you could use it to successfully complete your project. So, a Gigantic Thank you for using Botasaurus!&lt;/li&gt; &#xA; &lt;li&gt;A heartfelt thank you to &lt;a href=&#34;https://zcbenz.com/&#34;&gt;Cheng Zhao&lt;/a&gt; from GitHub for creating Electron, which powers Botasaurus Desktop.&lt;/li&gt; &#xA; &lt;li&gt;Kudos to the Apify Team for creating the &lt;code&gt;proxy-chain&lt;/code&gt; library. The implementation of SSL-based Proxy Authentication wouldn&#39;t have been possible without their groundbreaking work on &lt;code&gt;proxy-chain&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Shout out to &lt;a href=&#34;https://github.com/ultrafunkamsterdam&#34;&gt;ultrafunkamsterdam&lt;/a&gt; for creating &lt;code&gt;nodriver&lt;/code&gt;, which inspired the creation of Botasaurus Driver.&lt;/li&gt; &#xA; &lt;li&gt;A big thank you to &lt;a href=&#34;https://github.com/daijro&#34;&gt;daijro&lt;/a&gt; for creating &lt;a href=&#34;https://github.com/daijro/hrequests&#34;&gt;hrequest&lt;/a&gt;, which inspired the creation of botasaurus-requests.&lt;/li&gt; &#xA; &lt;li&gt;Deepest gratitude to &lt;a href=&#34;https://github.com/riflosnake&#34;&gt;Flori Batusha&lt;/a&gt; and &lt;a href=&#34;https://github.com/iLeaf30/&#34;&gt;Ambri&lt;/a&gt; for their contributions in creating &lt;strong&gt;Botasaurus Humancursor&lt;/strong&gt;, which brings human-like mouse movements to Botasaurus.&lt;/li&gt; &#xA; &lt;li&gt;A humongous thank you to Cloudflare, DataDome, Imperva, and all bot recognition systems. Had you not been there, we wouldn&#39;t be either üòÖ.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Now, what are you waiting for? ü§î Go and make something mastastic! üöÄ&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Love It? &lt;a href=&#34;https://github.com/omkarcloud/botasaurus&#34;&gt;Star It! ‚≠ê&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Become one of our amazing stargazers by giving us a star ‚≠ê on GitHub!&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s just one click, but it means the world to me.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/omkarcloud/botasaurus/stargazers&#34;&gt; &lt;img src=&#34;https://bytecrank.com/nastyox/reporoster/php/stargazersSVG.php?user=omkarcloud&amp;amp;repo=botasaurus&#34; alt=&#34;Stargazers for @omkarcloud/botasaurus&#34; /&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Disclaimer for Botasaurus Project&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;By using Botasaurus, you agree to comply with all applicable local and international laws related to data scraping, copyright, and privacy. The developers of Botasaurus are not responsible for any misuse of this software. It is the sole responsibility of the user to ensure adherence to all relevant laws regarding data scraping, copyright, and privacy, and to use Botasaurus in an ethical and legal manner.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We take the concerns of the Botasaurus Project very seriously. For any inquiries or issues, please contact Chetan Jain at &lt;a href=&#34;mailto:chetan@omkar.cloud&#34;&gt;chetan@omkar.cloud&lt;/a&gt;. We will take prompt and necessary action in response to your emails.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DevilXD/TwitchDropsMiner</title>
    <updated>2025-08-11T01:36:05Z</updated>
    <id>tag:github.com,2025-08-11:/DevilXD/TwitchDropsMiner</id>
    <link href="https://github.com/DevilXD/TwitchDropsMiner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An app that allows you to AFK mine timed Twitch drops, with automatic drop claiming and channel switching.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Twitch Drops Miner&lt;/h1&gt; &#xA;&lt;p&gt;This application allows you to AFK mine timed Twitch drops, without having to worry about switching channels when the one you were watching goes offline, claiming the drops, or even receiving the stream data itself. This helps you save on bandwidth and hassle.&lt;/p&gt; &#xA;&lt;h3&gt;How It Works:&lt;/h3&gt; &#xA;&lt;p&gt;Every several seconds, the application pretends to watch a particular stream by fetching stream metadata - this is enough to advance the drops. Note that this completely bypasses the need to download any actual stream video and sound. To keep the status (ONLINE or OFFLINE) of the channels up-to-date, there&#39;s a websocket connection established that receives events about streams going up or down, or updates regarding the current amount of viewers.&lt;/p&gt; &#xA;&lt;h3&gt;Features:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stream-less drop mining - save on bandwidth.&lt;/li&gt; &#xA; &lt;li&gt;Game priority and exclusion lists, allowing you to focus on mining what you want, in the order you want, and ignore what you don&#39;t want.&lt;/li&gt; &#xA; &lt;li&gt;Sharded websocket connection, allowing for tracking up to &lt;code&gt;199&lt;/code&gt; channels at the same time.&lt;/li&gt; &#xA; &lt;li&gt;Automatic drop campaigns discovery based on linked accounts (requires you to do &lt;a href=&#34;https://www.twitch.tv/drops/campaigns&#34;&gt;account linking&lt;/a&gt; yourself though).&lt;/li&gt; &#xA; &lt;li&gt;Stream tags and drop campaign validation, to ensure you won&#39;t end up mining a stream that can&#39;t earn you the drop.&lt;/li&gt; &#xA; &lt;li&gt;Automatic channel stream switching, when the one you were currently watching goes offline, as well as when a channel streaming a higher priority game goes online.&lt;/li&gt; &#xA; &lt;li&gt;Login session is saved in a cookies file, so you don&#39;t need to login every time.&lt;/li&gt; &#xA; &lt;li&gt;Mining is automatically started as new campaigns appear, and stopped when the last available drops have been mined.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Usage:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and unzip &lt;a href=&#34;https://github.com/DevilXD/TwitchDropsMiner/releases&#34;&gt;the latest release&lt;/a&gt; - it&#39;s recommended to keep it in the folder it comes in.&lt;/li&gt; &#xA; &lt;li&gt;Run it and login/connect the miner to your Twitch account by using the in-app login form.&lt;/li&gt; &#xA; &lt;li&gt;After a successful login, the app should fetch a list of all available campaigns and games you can mine drops for - you can then select and add games of choice to the Priority List available on the Settings tab, and then press on the &lt;code&gt;Reload&lt;/code&gt; button to start processing. It will fetch a list of all applicable streams it can watch, and start mining right away. You can also manually switch to a different channel as needed.&lt;/li&gt; &#xA; &lt;li&gt;If you wish to keep the miner occupied with mining anything it can, beyond what you&#39;ve selected via the Priority List, you can use the Priority Mode setting to specify the mining order for the rest of the games.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to link your Twitch account to game accounts on the &lt;a href=&#34;https://www.twitch.tv/drops/campaigns&#34;&gt;campaigns page&lt;/a&gt;, to enable more games to be mined.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pictures:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4180725/164298155-c0880ad7-6423-4419-8d73-f3c053730a1b.png&#34; alt=&#34;Main&#34; /&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/4180725/164298315-81cae0d2-24a4-4822-a056-154fd763c284.png&#34; alt=&#34;Inventory&#34; /&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/4180725/164298391-b13ad40d-3881-436c-8d4c-34e2bbe33a78.png&#34; alt=&#34;Settings&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Notes:&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING]&lt;br /&gt; Due to how Twitch handles the drop progression on their side, watching a stream in the browser (or by any other means) on the same account that is actively being used by the miner, will usually cause the miner to misbehave, reporting false progress and getting stuck mining the current drop.&lt;/p&gt; &#xA; &lt;p&gt;Using the same account to watch other streams during mining is thus discouraged, in order to avoid any problems arising from it.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION]&lt;br /&gt; Persistent cookies will be stored in the &lt;code&gt;cookies.jar&lt;/code&gt; file, from which the authorization (login) information will be restored on each subsequent run. Make sure to keep your cookies file safe, as the authorization information it stores can give another person access to your Twitch account, even without them knowing your password!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br /&gt; Successfully logging into your Twitch account in the application may cause Twitch to send you a &#34;New Login&#34; notification email. This is normal - you can verify that it comes from your own IP address. The detected browser during the login will be &#34;Chrome&#34;, as that&#39;s what the miner currently presents itself to the Twitch server.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br /&gt; The time remaining timer always countdowns a single minute and then stops - it is then restarted only after the application redetermines the remaining time. This &#34;redetermination&#34; can happen at any time Twitch decides to report on the drop&#39;s progress, but not later than 20 seconds after the timer reaches zero. The seconds timer is only an approximation and does not represent nor affect actual mining speed. The time variations are due to Twitch sometimes not reporting drop progress at all, or reporting progress for the wrong drop - these cases have all been accounted for in the application though.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br /&gt; The source code requires Python 3.10 or higher to run.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Notes about the Windows build:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To achieve a portable-executable format, the application is packaged with PyInstaller into an &lt;code&gt;EXE&lt;/code&gt;. Some antivirus engines (including Windows Defender) might report the packaged executable as a trojan, because PyInstaller has been used by others to package malicious Python code in the past. These reports can be safely ignored. If you absolutely do not trust the executable, you&#39;ll have to install Python yourself and run everything from source.&lt;/li&gt; &#xA; &lt;li&gt;The executable uses the &lt;code&gt;%TEMP%&lt;/code&gt; directory for temporary runtime storage of files, that don&#39;t need to be exposed to the user (like compiled code and translation files). For persistent storage, the directory the executable resides in is used instead.&lt;/li&gt; &#xA; &lt;li&gt;The autostart feature is implemented as a registry entry to the current user&#39;s (&lt;code&gt;HKCU&lt;/code&gt;) autostart key. It is only altered when toggling the respective option. If you relocate the app to a different directory, the autostart feature will stop working, until you toggle the option off and back on again&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Notes about the Linux build:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Linux app is built and distributed using two distinct portable-executable formats: &lt;a href=&#34;https://appimage.org/&#34;&gt;AppImage&lt;/a&gt; and &lt;a href=&#34;https://pyinstaller.org/&#34;&gt;PyInstaller&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;There are no major differences between the two formats, but if you&#39;re looking for a recommendation, use the AppImage.&lt;/li&gt; &#xA; &lt;li&gt;The Linux app should work out of the box on any modern distribution, as long as it has &lt;code&gt;glibc&amp;gt;=2.35&lt;/code&gt;, plus a working display server.&lt;/li&gt; &#xA; &lt;li&gt;Every feature of the app is expected to work on Linux just as well as it does on Windows. If you find something that&#39;s broken, please &lt;a href=&#34;https://github.com/DevilXD/TwitchDropsMiner/issues/new&#34;&gt;open a new issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The size of the Linux app is significantly larger than the Windows app due to the inclusion of the &lt;code&gt;gtk3&lt;/code&gt; library (and its dependencies), which is required for proper system tray/notifications support.&lt;/li&gt; &#xA; &lt;li&gt;As an alternative to the native Linux app, you can run the Windows app via &lt;a href=&#34;https://www.winehq.org/&#34;&gt;Wine&lt;/a&gt; instead. It works really well!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Usage:&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d be interested in running the latest master from source or building your own executable, see the wiki page explaining how to do so: &lt;a href=&#34;https://github.com/DevilXD/TwitchDropsMiner/wiki/Setting-up-the-environment,-building-and-running&#34;&gt;https://github.com/DevilXD/TwitchDropsMiner/wiki/Setting-up-the-environment,-building-and-running&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Support&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/DevilXD&#34;&gt;&lt;img src=&#34;https://i.imgur.com/cL95gzE.png&#34; alt=&#34;Buy me a coffee&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://www.patreon.com/bePatron?u=26937862&#34;&gt;&lt;img src=&#34;https://i.imgur.com/Mdkb9jq.png&#34; alt=&#34;Support me on Patreon&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Project goals:&lt;/h3&gt; &#xA;&lt;p&gt;Twitch Drops Miner (TDM for short) has been designed with a couple of simple goals in mind. These are, specifically:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Twitch Drops oriented - it&#39;s in the name. That&#39;s what I made it for.&lt;/li&gt; &#xA; &lt;li&gt;Easy to use for an average person. Includes a nice looking GUI and is packaged as a ready-to-go executable, without requiring an existing Python installation to work.&lt;/li&gt; &#xA; &lt;li&gt;Intended as a helper tool that starts together with your PC, runs in the background through out the day, and then closes together with your PC shutting down at the end of the day. If it can run continuously for 24 hours at minimum, and not run into any errors, I&#39;d call that good enough already.&lt;/li&gt; &#xA; &lt;li&gt;Requiring a minimum amount of attention during operation - check it once or twice through out the day to see if everything&#39;s fine with it.&lt;/li&gt; &#xA; &lt;li&gt;Underlying service friendly - the amount of interactions done with the Twitch site is kept to the minimum required for reliable operation, at a level achievable by a diligent site user.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;TDM is not intended for/as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mining channel points - again, it&#39;s about the drops: only.&lt;/li&gt; &#xA; &lt;li&gt;Mining anything else besides Twitch drops - no, I won&#39;t be adding support for a random 3rd party site that also happens to rely on watching Twitch streams.&lt;/li&gt; &#xA; &lt;li&gt;Unattended operation: worst case scenario, it&#39;ll stop working and you&#39;ll hopefully notice that at some point. Hopefully.&lt;/li&gt; &#xA; &lt;li&gt;100% uptime application, due to the underlying nature of it, expect fatal errors to happen every so often.&lt;/li&gt; &#xA; &lt;li&gt;Being hosted on a remote server as a 24/7 miner.&lt;/li&gt; &#xA; &lt;li&gt;Being used with more than one managed account.&lt;/li&gt; &#xA; &lt;li&gt;Mining campaigns the managed account isn&#39;t linked to.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This means that features such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It being possible to run it without a GUI, or with only a console attached.&lt;/li&gt; &#xA; &lt;li&gt;Any form of automatic restart when an error happens.&lt;/li&gt; &#xA; &lt;li&gt;Docker or any other form of remote deployment.&lt;/li&gt; &#xA; &lt;li&gt;Using it with more than one managed account.&lt;/li&gt; &#xA; &lt;li&gt;Making it possible to mine campaigns that the managed account isn&#39;t linked to.&lt;/li&gt; &#xA; &lt;li&gt;Anything that increases the site processing load caused by the application.&lt;/li&gt; &#xA; &lt;li&gt;Any form of additional notifications system (email, webhook, etc.), beyond what&#39;s already implemented.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;..., are most likely not going to be a feature, ever. You&#39;re welcome to search through the existing issues to comment on your point of view on the relevant matters, where applicable. Otherwise, most of the new issues that go against these goals will be closed and the user will be pointed to this paragraph.&lt;/p&gt; &#xA;&lt;p&gt;For more context about these goals, please check out these issues: &lt;a href=&#34;https://github.com/DevilXD/TwitchDropsMiner/issues/161&#34;&gt;#161&lt;/a&gt;, &lt;a href=&#34;https://github.com/DevilXD/TwitchDropsMiner/issues/105&#34;&gt;#105&lt;/a&gt;, &lt;a href=&#34;https://github.com/DevilXD/TwitchDropsMiner/issues/84&#34;&gt;#84&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Credits:&lt;/h3&gt; &#xA;&lt;!--&#xA;Note: The translations credits are sorted alphabetically, based on their English language name.&#xA;When adding a new entry, please ensure to insert it in the correct place in the second section.&#xA;Non-translations related credits should be added to the first section instead.&#xA;&#xA;Note: When adding a new credits line below, please add two trailing spaces at the end&#xA;of the previous line, if they aren&#39;t already there. Doing so ensures proper markdown&#xA;rendering on Github. In short: Each credits line should end with two trailing spaces,&#xA;placed past the period character at the end.&#xA;&#xA;‚Ä¢ Last line can have the two trailing spaces omitted.&#xA;‚Ä¢ Please ensure your editor won&#39;t trim the trailing spaces upon saving the file.&#xA;‚Ä¢ Please ensure to leave a single empty new line at the end of the file.&#xA;--&gt; &#xA;&lt;p&gt;@guihkx - For the CI script, CI maintenance, and everything related to Linux builds.&lt;/p&gt; &#xA;&lt;p&gt;@Bamboozul - For the entirety of the Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©) translation.&lt;br /&gt; @Suz1e - For the entirety of the Chinese (ÁÆÄ‰Ωì‰∏≠Êñá) translation and revisions.&lt;br /&gt; @wwj010 - For the Chinese (ÁÆÄ‰Ωì‰∏≠Êñá) translation corrections and revisions.&lt;br /&gt; @zhangminghao1989 - For the Chinese (ÁÆÄ‰Ωì‰∏≠Êñá) translation corrections and revisions.&lt;br /&gt; @Ricky103403 - For the entirety of the Traditional Chinese (ÁπÅÈ´î‰∏≠Êñá) translation.&lt;br /&gt; @LusTerCsI - For the Traditional Chinese (ÁπÅÈ´î‰∏≠Êñá) translation corrections and revisions.&lt;br /&gt; @nwvh - For the entirety of the Czech (ƒåe≈°tina) translation.&lt;br /&gt; @Kjerne - For the entirety of the Danish (Dansk) translation.&lt;br /&gt; @roobini-gamer - For the entirety of the French (Fran√ßais) translation.&lt;br /&gt; @Calvineries - For the French (Fran√ßais) translation revisions.&lt;br /&gt; @ThisIsCyreX - For the entirety of the German (Deutsch) translation.&lt;br /&gt; @Eriza-Z - For the entirety of the Indonesian translation.&lt;br /&gt; @casungo - For the entirety of the Italian (Italiano) translation.&lt;br /&gt; @ShimadaNanaki - For the entirety of the Japanese (Êó•Êú¨Ë™û) translation.&lt;br /&gt; @Patriot99 - For the Polish (Polski) translation and revisions (co-authored with @DevilXD).&lt;br /&gt; @zarigata - For the entirety of the Portuguese (Portugu√™s) translation.&lt;br /&gt; @Sergo1217 - For the entirety of the Russian (–†—É—Å—Å–∫–∏–π) translation.&lt;br /&gt; @Shofuu - For the entirety of the Spanish (Espa√±ol) translation and revisions.&lt;br /&gt; @alikdb - For the entirety of the Turkish (T√ºrk√ße) translation.&lt;br /&gt; @Nollasko - For the entirety of the Ukrainian (–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞) translation and revisions.&lt;/p&gt;</summary>
  </entry>
</feed>