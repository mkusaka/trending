<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-04T01:34:26Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DachunKai/Ev-DeblurVSR</title>
    <updated>2025-05-04T01:34:26Z</updated>
    <id>tag:github.com,2025-05-04:/DachunKai/Ev-DeblurVSR</id>
    <link href="https://github.com/DachunKai/Ev-DeblurVSR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[AAAI 2025] Event-Enhanced Blurry Video Super-Resolution&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/32438&#34;&gt;Ev-DeblurVSR (AAAI 2025)&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Official Pytorch implementation for the &#34;Event-Enhanced Blurry Video Super-Resolution&#34; paper (AAAI 2025).&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üåê &lt;a href=&#34;https://dachunkai.github.io/ev-deblurvsr.github.io/&#34; target=&#34;_blank&#34;&gt;Project&lt;/a&gt; | üìÉ &lt;a href=&#34;https://arxiv.org/pdf/2504.13042&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: &lt;a href=&#34;https://github.com/DachunKai/&#34;&gt;Dachun Kai&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;mailto:dachunkai@mail.ustc.edu.cn&#34;&gt;:email:Ô∏è&lt;/a&gt;&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=LatWlFAAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Yueyi Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/booker-max&#34;&gt;Jin Wang&lt;/a&gt;, &lt;a href=&#34;https://dblp.org/pid/276/3139.html&#34;&gt;Zeyu Xiao&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Snl0HPEAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=VRG3dw4AAAAJ&amp;amp;hl=zh-CN&#34;&gt;Xiaoyan Sun&lt;/a&gt;, &lt;em&gt;University of Science and Technology of China&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feel free to ask questions. If our work helps, please don&#39;t hesitate to give us a &lt;span&gt;‚≠ê&lt;/span&gt;!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 2025/04/17: Release pretrained models and test sets for quick testing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 2025/01/07: Video demos released&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 2024/12/15: Initialize the repository&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 2024/12/09: &lt;span&gt;üéâ&lt;/span&gt; &lt;span&gt;üéâ&lt;/span&gt; Our paper was accepted in AAAI&#39;2025&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîñ&lt;/span&gt; Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DachunKai/Ev-DeblurVSR/main/#video-demos&#34;&gt;Video Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DachunKai/Ev-DeblurVSR/main/#code&#34;&gt;Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DachunKai/Ev-DeblurVSR/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DachunKai/Ev-DeblurVSR/main/#contact&#34;&gt;Contact&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DachunKai/Ev-DeblurVSR/main/#license-and-acknowledgement&#34;&gt;License and Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;&lt;span&gt;üî•&lt;/span&gt; Video Demos&lt;/h2&gt; &#xA;&lt;p&gt;A $4\times$ blurry video upsampling results on the synthetic dataset &lt;a href=&#34;https://seungjunnah.github.io/Datasets/gopro.html&#34;&gt;GoPro&lt;/a&gt; and real-world dataset &lt;a href=&#34;https://sites.google.com/view/neid2023&#34;&gt;NCER&lt;/a&gt; test sets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/df54a750-25fd-4ac1-9980-20ef7f73c738&#34;&gt;https://github.com/user-attachments/assets/df54a750-25fd-4ac1-9980-20ef7f73c738&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/4d58c85f-1a47-4292-8e4a-4ea0ccfe1b0d&#34;&gt;https://github.com/user-attachments/assets/4d58c85f-1a47-4292-8e4a-4ea0ccfe1b0d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/cb7c3a62-5927-4f5a-8aec-258d7e1d513e&#34;&gt;https://github.com/user-attachments/assets/cb7c3a62-5927-4f5a-8aec-258d7e1d513e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/0c030756-f2a0-4a9d-81a2-99943a0f881f&#34;&gt;https://github.com/user-attachments/assets/0c030756-f2a0-4a9d-81a2-99943a0f881f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Dependencies: &lt;a href=&#34;https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#34;&gt;Miniconda&lt;/a&gt;, &lt;a href=&#34;https://developer.nvidia.com/cuda-11.1.1-download-archive&#34;&gt;CUDA Toolkit 11.1.1&lt;/a&gt;, &lt;a href=&#34;https://download.pytorch.org/whl/cu111/torch-1.10.2%2Bcu111-cp37-cp37m-linux_x86_64.whl&#34;&gt;torch 1.10.2+cu111&lt;/a&gt;, and &lt;a href=&#34;https://download.pytorch.org/whl/cu111/torchvision-0.11.3%2Bcu111-cp37-cp37m-linux_x86_64.whl&#34;&gt;torchvision 0.11.3+cu111&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run in Conda (&lt;strong&gt;Recommended&lt;/strong&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -y -n ev-deblurvsr python=3.7&#xA;conda activate ev-deblurvsr&#xA;pip install torch-1.10.2+cu111-cp37-cp37m-linux_x86_64.whl&#xA;pip install torchvision-0.11.3+cu111-cp37-cp37m-linux_x86_64.whl&#xA;git clone https://github.com/DachunKai/Ev-DeblurVSR&#xA;cd Ev-DeblurVSR &amp;amp;&amp;amp; pip install -r requirements.txt &amp;amp;&amp;amp; python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run in Docker &lt;span&gt;üëè&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Note: &lt;strong&gt;We use the same docker image as our previous work &lt;a href=&#34;https://github.com/DachunKai/EvTexture&#34;&gt;EvTexture&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;[Option 1] Directly pull the published Docker image we have provided from &lt;a href=&#34;https://cr.console.aliyun.com/cn-hangzhou/instances&#34;&gt;Alibaba Cloud&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull registry.cn-hangzhou.aliyuncs.com/dachunkai/evtexture:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;[Option 2] We also provide a &lt;a href=&#34;https://github.com/DachunKai/Ev-DeblurVSR/raw/main/docker/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt; that you can use to build the image yourself.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd EvTexture &amp;amp;&amp;amp; docker build -t evtexture ./docker&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The pulled or self-built Docker image contains a complete conda environment named &lt;code&gt;evtexture&lt;/code&gt;. After running the image, you can mount your data and operate within this environment.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source activate evtexture &amp;amp;&amp;amp; cd EvTexture &amp;amp;&amp;amp; python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Test&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the pretrained models from (&lt;a href=&#34;https://github.com/DachunKai/Ev-DeblurVSR/releases&#34;&gt;Releases&lt;/a&gt; / &lt;a href=&#34;https://pan.baidu.com/s/1Y4ZW9PDV_ff2Z4VxadzrzA?pwd=n8hg&#34;&gt;Baidu Cloud&lt;/a&gt; (n8hg)) and place them to &lt;code&gt;experiments/pretrained_models/EvDeblurVSR/&lt;/code&gt;. The network architecture code is in &lt;a href=&#34;https://github.com/DachunKai/Ev-DeblurVSR/raw/main/basicsr/archs/evdeblurvsr_arch.py&#34;&gt;evdeblurvsr_arch.py&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Synthetic dataset model: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;EvDeblurVSR_GOPRO_BIx4.pth&lt;/em&gt;: trained on &lt;a href=&#34;https://seungjunnah.github.io/Datasets/gopro.html&#34;&gt;GoPro&lt;/a&gt; dataset with Blur-Sharp pairs and BI degradation for $4\times$ SR scale.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;em&gt;EvDeblurVSR_BSD_BIx4.pth&lt;/em&gt;: trained on &lt;a href=&#34;https://github.com/zzh-tech/ESTRNN&#34;&gt;BSD&lt;/a&gt; dataset with Blur-Sharp pairs and BI degradation for $4\times$ SR scale.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Real-world dataset model: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;em&gt;EvDeblurVSR_NCER_BIx4.pth&lt;/em&gt;: trained on &lt;a href=&#34;https://sites.google.com/view/neid2023&#34;&gt;NCER&lt;/a&gt; dataset with Blur-Sharp pairs and BI degradation for $4\times$ SR scale.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the preprocessed test sets (including events) for &lt;a href=&#34;https://seungjunnah.github.io/Datasets/gopro.html&#34;&gt;GoPro&lt;/a&gt;, &lt;a href=&#34;https://github.com/zzh-tech/ESTRNN&#34;&gt;BSD&lt;/a&gt;, and &lt;a href=&#34;https://sites.google.com/view/neid2023&#34;&gt;NCER&lt;/a&gt; from (&lt;a href=&#34;https://pan.baidu.com/s/1Y4ZW9PDV_ff2Z4VxadzrzA?pwd=n8hg&#34;&gt;Baidu Cloud&lt;/a&gt; (n8hg) / &lt;a href=&#34;https://drive.google.com/drive/folders/1Py9uESwTAD0lhRgvhBGXo-uODxC-wGTw?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;), and place them to &lt;code&gt;datasets/&lt;/code&gt;.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;GoPro_h5&lt;/em&gt;: HDF5 files containing preprocessed test datasets for the GoPro test set.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;BSD_h5&lt;/em&gt;: HDF5 files containing preprocessed test datasets for the BSD dataset.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;em&gt;NCER_h5&lt;/em&gt;: HDF5 files containing preprocessed test datasets for the NCER dataset.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Test on GoPro for 4x Blurry VSR: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/dist_test.sh [num_gpus] options/test/EvDeblurVSR/test_EvDeblurVSR_GoPro_x4.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Test on BSD for 4x Blurry VSR: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/dist_test.sh [num_gpus] options/test/EvDeblurVSR/test_EvDeblurVSR_BSD_x4.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Test on NCER for 4x Blurry VSR: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/dist_test.sh [num_gpus] options/test/EvDeblurVSR/test_EvDeblurVSR_NCER_x4.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;This will generate the inference results in &lt;code&gt;results/&lt;/code&gt;. The output results on GoPro, BSD and NCER datasets can be downloaded from (&lt;a href=&#34;https://github.com/DachunKai/Ev-DeblurVSR/releases&#34;&gt;Releases&lt;/a&gt; / &lt;a href=&#34;https://pan.baidu.com/s/1Y4ZW9PDV_ff2Z4VxadzrzA?pwd=n8hg&#34;&gt;Baidu Cloud&lt;/a&gt; (n8hg)).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Test the number of parameters, runtime, and FLOPs:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python test_scripts/test_params_runtime.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Input Data Structure&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Both video and event data are required as input, as shown in the &lt;a href=&#34;https://github.com/DachunKai/Ev-DeblurVSR/raw/main/basicsr/archs/evdeblurvsr_arch.py#L229&#34;&gt;snippet&lt;/a&gt;. We package each video and its event data into an &lt;a href=&#34;https://docs.h5py.org/en/stable/quick.html#quick&#34;&gt;HDF5&lt;/a&gt; file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Example: The structure of &lt;code&gt;GOPR0384_11_00.h5&lt;/code&gt; file from the GoPro dataset is shown below.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-arduino&#34;&gt;GOPR0384_11_00.h5&#xA;‚îú‚îÄ‚îÄ images&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 000000 # frame, ndarray, [H, W, C]&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ vFwd&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 000000 # inter-frame forward event voxel, ndarray, [Bins, H, W]&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ vBwd&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 000000 # inter-frame backward event voxel, ndarray, [Bins, H, W]&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ vExpo&#xA;‚îÇ   ‚îú‚îÄ‚îÄ 000000 # intra-frame exposure event voxel, ndarray, [Bins, H, W]&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üòä&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find the code and pre-trained models useful for your research, please consider citing our paper. &lt;span&gt;üòÉ&lt;/span&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{kai2025event,&#xA;  title={Event-{E}nhanced {B}lurry {V}ideo {S}uper-{R}esolution},&#xA;  author={Kai, Dachun and Zhang, Yueyi and Wang, Jin and Xiao, Zeyu and Xiong, Zhiwei and Sun, Xiaoyan},&#xA;  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},&#xA;  volume={39},&#xA;  number={4},&#xA;  pages={4175--4183},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you meet any problems, please describe them in issues or contact:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dachun Kai: &lt;a href=&#34;mailto:dachunkai@mail.ustc.edu.cn&#34;&gt;dachunkai@mail.ustc.edu.cn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License and Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache 2.0 License&lt;/a&gt;. Our work builds significantly upon our previous project &lt;a href=&#34;https://github.com/DachunKai/EvTexture&#34;&gt;EvTexture&lt;/a&gt;. We would also like to sincerely thank the developers of &lt;a href=&#34;https://github.com/XPixelGroup/BasicSR&#34;&gt;BasicSR&lt;/a&gt;, an open-source toolbox for image and video restoration tasks. Additionally, we appreciate the inspiration and code provided by &lt;a href=&#34;https://github.com/ckkelvinchan/BasicVSR_PlusPlus&#34;&gt;BasicVSR++&lt;/a&gt;, &lt;a href=&#34;https://github.com/princeton-vl/RAFT&#34;&gt;RAFT&lt;/a&gt; and &lt;a href=&#34;https://github.com/TimoStoff/event_utils&#34;&gt;event_utils&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>