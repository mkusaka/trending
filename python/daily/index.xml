<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-29T01:37:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/setfit</title>
    <updated>2022-09-29T01:37:54Z</updated>
    <id>tag:github.com,2022-09-29:/huggingface/setfit</id>
    <link href="https://github.com/huggingface/setfit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Efficient few-shot learning with Sentence Transformers&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/setfit/main/assets/setfit.png&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/setfit&#34; target=&#34;_blank&#34;&gt;Models &amp;amp; Datasets&lt;/a&gt; | üìñ &lt;a href=&#34;https://huggingface.co/blog/setfit&#34; target=&#34;_blank&#34;&gt;Blog&lt;/a&gt; | üìÉ &lt;a href=&#34;https://arxiv.org/abs/2209.11055&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;SetFit - Efficient Few-shot Learning with Sentence Transformers&lt;/h1&gt; &#xA;&lt;p&gt;We introduce SetFit, an efficient and prompt-free framework for few-shot fine-tuning of &lt;a href=&#34;https://sbert.net/&#34;&gt;Sentence Transformers&lt;/a&gt;. SetFit achieves high accuracy with little labeled data - for instance, with only 8 labeled examples per class on the Customer Reviews sentiment dataset, SetFit is competitive with fine-tuning RoBERTa Large on the full training set of 3k examples ü§Ø!&lt;/p&gt; &#xA;&lt;p&gt;Compared to other few-shot learning methods, SetFit has several unique features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üó£ &lt;strong&gt;No prompts or verbalisers:&lt;/strong&gt; Current techniques for few-shot fine-tuning require handcrafted prompts or verbalisers to convert examples into a format that&#39;s suitable for the underlying language model. SetFit dispenses with prompts altogether by generating rich embeddings directly from text examples.&lt;/li&gt; &#xA; &lt;li&gt;üèé &lt;strong&gt;Fast to train:&lt;/strong&gt; SetFit doesn&#39;t require large-scale models like T0 or GPT-3 to achieve high accuracy. As a result, it is typically an order of magnitude (or more) faster to train and run inference with.&lt;/li&gt; &#xA; &lt;li&gt;üåé &lt;strong&gt;Multilingual support&lt;/strong&gt;: SetFit can be used with any &lt;a href=&#34;https://huggingface.co/models?library=sentence-transformers&amp;amp;sort=downloads&#34;&gt;Sentence Transformer&lt;/a&gt; on the Hub, which means you can classify text in multiple languages by simply fine-tuning a multilingual checkpoint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Download and install &lt;code&gt;setfit&lt;/code&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install setfit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training a SetFit model&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;setfit&lt;/code&gt; is integrated with the &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face Hub&lt;/a&gt; and provides two main classes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;SetFitModel&lt;/code&gt;: a wrapper that combines a pretrained body from &lt;code&gt;sentence_transformers&lt;/code&gt; and a classification head from &lt;code&gt;scikit-learn&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SetFitTrainer&lt;/code&gt;: a helper class that wraps the fine-tuning process of SetFit.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here is an end-to-end example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;from sentence_transformers.losses import CosineSimilarityLoss&#xA;&#xA;from setfit import SetFitModel, SetFitTrainer&#xA;&#xA;&#xA;# Load a dataset from the Hugging Face Hub&#xA;dataset = load_dataset(&#34;emotion&#34;)&#xA;&#xA;# Simulate the few-shot regime by sampling 8 examples per class&#xA;num_classes = 6&#xA;train_ds = dataset[&#34;train&#34;].shuffle(seed=42).select(range(8 * num_classes))&#xA;test_ds = dataset[&#34;test&#34;]&#xA;&#xA;# Load SetFit model from Hub&#xA;model = SetFitModel.from_pretrained(&#34;sentence-transformers/paraphrase-mpnet-base-v2&#34;)&#xA;&#xA;# Create trainer&#xA;trainer = SetFitTrainer(&#xA;    model=model,&#xA;    train_dataset=train_ds,&#xA;    eval_dataset=test_ds,&#xA;    loss_class=CosineSimilarityLoss,&#xA;    batch_size=16,&#xA;    num_iterations=20, # The number of text pairs to generate&#xA;)&#xA;&#xA;# Train and evaluate&#xA;trainer.train()&#xA;metrics = trainer.evaluate()&#xA;&#xA;# Push model to the Hub&#xA;trainer.push_to_hub(&#34;my-awesome-setfit-model&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, check out the &lt;code&gt;notebooks/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Reproducing the results from the paper&lt;/h2&gt; &#xA;&lt;p&gt;We provide scripts to reproduce the results for SetFit and various baselines presented in Table 2 of our paper. Checkout the setup and training instructions in the &lt;code&gt;scripts/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Developer installation&lt;/h2&gt; &#xA;&lt;p&gt;To run the code in this project, first create a Python virtual environment using e.g. Conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n setfit python=3.9 &amp;amp;&amp;amp; conda activate setfit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install the base requirements with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install -e &#39;.[dev]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install &lt;code&gt;datasets&lt;/code&gt; and packages like &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;isort&lt;/code&gt; that we use to ensure consistent code formatting. Next, go to one of the dedicated baseline directories and install the extra dependencies, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd scripts/setfit&#xA;python -m pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Formatting your code&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;isort&lt;/code&gt; to ensure consistent code formatting. After following the installation steps, you can check your code locally by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make style &amp;amp;&amp;amp; make quality&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Project structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ LICENSE&#xA;‚îú‚îÄ‚îÄ Makefile        &amp;lt;- Makefile with commands like `make style` or `make tests`&#xA;‚îú‚îÄ‚îÄ README.md       &amp;lt;- The top-level README for developers using this project.&#xA;‚îú‚îÄ‚îÄ notebooks       &amp;lt;- Jupyter notebooks.&#xA;‚îú‚îÄ‚îÄ final_results   &amp;lt;- Model predictions from the paper&#xA;‚îú‚îÄ‚îÄ scripts         &amp;lt;- Scripts for training and inference&#xA;‚îú‚îÄ‚îÄ setup.cfg       &amp;lt;- Configuration file to define package metadata&#xA;‚îú‚îÄ‚îÄ setup.py        &amp;lt;- Make this project pip installable with `pip install -e`&#xA;‚îú‚îÄ‚îÄ src             &amp;lt;- Source code for SetFit&#xA;‚îî‚îÄ‚îÄ tests           &amp;lt;- Unit tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;[Coming soon!]&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmaction2</title>
    <updated>2022-09-29T01:37:54Z</updated>
    <id>tag:github.com,2022-09-29:/open-mmlab/mmaction2</id>
    <link href="https://github.com/open-mmlab/mmaction2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab&#39;s Next Generation Video Understanding Toolbox and Benchmark&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_logo.png&#34; width=&#34;600&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/mmaction2/badge/?version=latest&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;actions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmaction2&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/mmaction2/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/mmaction2&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmaction2.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg?sanitize=true&#34; alt=&#34;Percentage of issues still open&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/&#34;&gt;üìòDocumentation&lt;/a&gt; | &lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/install.html&#34;&gt;üõ†Ô∏èInstallation&lt;/a&gt; | &lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/modelzoo.html&#34;&gt;üëÄModel Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/changelog.html&#34;&gt;üÜïUpdate News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/projects&#34;&gt;üöÄOngoing Projects&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/issues/new/choose&#34;&gt;ü§îReporting Issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/README_zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;MMAction2 is an open-source toolbox for video understanding based on PyTorch. It is a part of the &lt;a href=&#34;http://openmmlab.org/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;The master branch works with &lt;strong&gt;PyTorch 1.5+&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div style=&#34;float:left;margin-right:10px;&#34;&gt; &#xA;  &lt;img src=&#34;https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_overview.gif&#34; width=&#34;380px&#34;&gt;&#xA;  &lt;br&gt; &#xA;  &lt;p style=&#34;font-size:1.5vw;&#34;&gt;Action Recognition Results on Kinetics-400&lt;/p&gt; &#xA; &lt;/div&gt; &#xA; &lt;div style=&#34;float:right;margin-right:0px;&#34;&gt; &#xA;  &lt;img src=&#34;https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif&#34; width=&#34;380px&#34;&gt;&#xA;  &lt;br&gt; &#xA;  &lt;p style=&#34;font-size:1.5vw;&#34;&gt;Skeleton-based Action Recognition Results on NTU-RGB+D-120&lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/30782254/155710881-bb26863e-fcb4-458e-b0c4-33cd79f96901.gif&#34; width=&#34;580px&#34;&gt;&#xA; &lt;br&gt; &#xA; &lt;p style=&#34;font-size:1.5vw;&#34;&gt;Skeleton-based Spatio-Temporal Action Detection and Action Recognition Results on Kinetics-400&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/open-mmlab/mmaction2/raw/master/resources/spatio-temporal-det.gif&#34; width=&#34;800px&#34;&gt;&#xA; &lt;br&gt; &#xA; &lt;p style=&#34;font-size:1.5vw;&#34;&gt;Spatio-Temporal Action Detection Results on AVA-2.1&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Major Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular design&lt;/strong&gt;: We decompose a video understanding framework into different components. One can easily construct a customized video understanding framework by combining different modules.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support four major video understanding tasks&lt;/strong&gt;: MMAction2 implements various algorithms for multiple video understanding tasks, including action recognition, action localization, spatio-temporal action detection, and skeleton-based action detection. We support &lt;strong&gt;27&lt;/strong&gt; different algorithms and &lt;strong&gt;20&lt;/strong&gt; different datasets for the four major tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Well tested and documented&lt;/strong&gt;: We provide detailed documentation and API reference, as well as unit tests.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2022-03-04) We support &lt;strong&gt;Multigrid&lt;/strong&gt; on Kinetics400, achieve 76.07% Top-1 accuracy and accelerate training speed.&lt;/li&gt; &#xA; &lt;li&gt;(2021-11-24) We support &lt;strong&gt;2s-AGCN&lt;/strong&gt; on NTU60 XSub, achieve 86.06% Top-1 accuracy on joint stream and 86.89% Top-1 accuracy on bone stream respectively.&lt;/li&gt; &#xA; &lt;li&gt;(2021-10-29) We provide a demo for skeleton-based and rgb-based spatio-temporal detection and action recognition (demo/demo_video_structuralize.py).&lt;/li&gt; &#xA; &lt;li&gt;(2021-10-26) We train and test &lt;strong&gt;ST-GCN&lt;/strong&gt; on NTU60 with 3D keypoint annotations, achieve 84.61% Top-1 accuracy (higher than 81.5% in the &lt;a href=&#34;https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17135&#34;&gt;paper&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;(2021-10-25) We provide a script(tools/data/skeleton/gen_ntu_rgbd_raw.py) to convert the NTU60 and NTU120 3D raw skeleton data to our format.&lt;/li&gt; &#xA; &lt;li&gt;(2021-10-25) We provide a &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/skeleton/posec3d/custom_dataset_training.md&#34;&gt;guide&lt;/a&gt; on how to train PoseC3D with custom datasets, &lt;a href=&#34;https://github.com/bit-scientist&#34;&gt;bit-scientist&lt;/a&gt; authored this PR!&lt;/li&gt; &#xA; &lt;li&gt;(2021-10-16) We support &lt;strong&gt;PoseC3D&lt;/strong&gt; on UCF101 and HMDB51, achieves 87.0% and 69.3% Top-1 accuracy with 2D skeletons only. Pre-extracted 2D skeletons are also available.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Release&lt;/strong&gt;: v0.24.0 was released in 05/05/2022. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/changelog.md&#34;&gt;changelog.md&lt;/a&gt; for details and release history.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;MMAction2 depends on &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt; (optional), and &lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMPose&lt;/a&gt;(optional). Below are quick steps for installation. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/install.md&#34;&gt;install.md&lt;/a&gt; for more detailed instruction.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n open-mmlab python=3.8 pytorch=1.10 cudatoolkit=11.3 torchvision -c pytorch -y&#xA;conda activate open-mmlab&#xA;pip3 install openmim&#xA;mim install mmcv-full&#xA;mim install mmdet  # optional&#xA;mim install mmpose  # optional&#xA;git clone https://github.com/open-mmlab/mmaction2.git&#xA;cd mmaction2&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/getting_started.md&#34;&gt;getting_started.md&lt;/a&gt; for the basic usage of MMAction2. There are also tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/tutorials/1_config.md&#34;&gt;learn about configs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/tutorials/2_finetune.md&#34;&gt;finetuning models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/tutorials/3_new_dataset.md&#34;&gt;adding new dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/tutorials/4_data_pipeline.md&#34;&gt;designing data pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/tutorials/5_new_modules.md&#34;&gt;adding new modules&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/tutorials/6_export_model.md&#34;&gt;exporting model to onnx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/tutorials/7_customize_runtime.md&#34;&gt;customizing runtime settings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A Colab tutorial is also provided. You may preview the notebook &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/demo/mmaction2_tutorial.ipynb&#34;&gt;here&lt;/a&gt; or directly &lt;a href=&#34;https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb&#34;&gt;run&lt;/a&gt; on Colab.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Methods&lt;/h2&gt; &#xA;&lt;table style=&#34;margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;5&#34; style=&#34;font-weight:bold;&#34;&gt;Action Recognition&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/c3d/README.md&#34;&gt;C3D&lt;/a&gt; (CVPR&#39;2014)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/tsn/README.md&#34;&gt;TSN&lt;/a&gt; (ECCV&#39;2016)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/i3d/README.md&#34;&gt;I3D&lt;/a&gt; (CVPR&#39;2017)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/i3d/README.md&#34;&gt;I3D Non-Local&lt;/a&gt; (CVPR&#39;2018)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/r2plus1d/README.md&#34;&gt;R(2+1)D&lt;/a&gt; (CVPR&#39;2018)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/trn/README.md&#34;&gt;TRN&lt;/a&gt; (ECCV&#39;2018)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/tsm/README.md&#34;&gt;TSM&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/tsm/README.md&#34;&gt;TSM Non-Local&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/slowonly/README.md&#34;&gt;SlowOnly&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/slowfast/README.md&#34;&gt;SlowFast&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/csn/README.md&#34;&gt;CSN&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/tin/README.md&#34;&gt;TIN&lt;/a&gt; (AAAI&#39;2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/tpn/README.md&#34;&gt;TPN&lt;/a&gt; (CVPR&#39;2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/x3d/README.md&#34;&gt;X3D&lt;/a&gt; (CVPR&#39;2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/omnisource/README.md&#34;&gt;OmniSource&lt;/a&gt; (ECCV&#39;2020)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition_audio/resnet/README.md&#34;&gt;MultiModality: Audio&lt;/a&gt; (ArXiv&#39;2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/tanet/README.md&#34;&gt;TANet&lt;/a&gt; (ArXiv&#39;2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/recognition/timesformer/README.md&#34;&gt;TimeSformer&lt;/a&gt; (ICML&#39;2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;5&#34; style=&#34;font-weight:bold;&#34;&gt;Action Localization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/localization/ssn/README.md&#34;&gt;SSN&lt;/a&gt; (ICCV&#39;2017)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/localization/bsn/README.md&#34;&gt;BSN&lt;/a&gt; (ECCV&#39;2018)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/localization/bmn/README.md&#34;&gt;BMN&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;5&#34; style=&#34;font-weight:bold;&#34;&gt;Spatio-Temporal Action Detection&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/detection/acrn/README.md&#34;&gt;ACRN&lt;/a&gt; (ECCV&#39;2018)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/detection/ava/README.md&#34;&gt;SlowOnly+Fast R-CNN&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/detection/ava/README.md&#34;&gt;SlowFast+Fast R-CNN&lt;/a&gt; (ICCV&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/detection/lfb/README.md&#34;&gt;LFB&lt;/a&gt; (CVPR&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;5&#34; style=&#34;font-weight:bold;&#34;&gt;Skeleton-based Action Recognition&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/skeleton/stgcn/README.md&#34;&gt;ST-GCN&lt;/a&gt; (AAAI&#39;2018)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/skeleton/2s-agcn/README.md&#34;&gt;2s-AGCN&lt;/a&gt; (CVPR&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/configs/skeleton/posec3d/README.md&#34;&gt;PoseC3D&lt;/a&gt; (ArXiv&#39;2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Results and models are available in the &lt;em&gt;README.md&lt;/em&gt; of each method&#39;s config directory. A summary can be found on the &lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/recognition_models.html&#34;&gt;&lt;strong&gt;model zoo&lt;/strong&gt;&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;p&gt;We will keep up with the latest progress of the community and support more popular algorithms and frameworks. If you have any feature requests, please feel free to leave a comment in &lt;a href=&#34;https://github.com/open-mmlab/mmaction2/issues/19&#34;&gt;Issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Datasets&lt;/h2&gt; &#xA;&lt;table style=&#34;margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;4&#34; style=&#34;font-weight:bold;&#34;&gt;Action Recognition&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/hmdb51/README.md&#34;&gt;HMDB51&lt;/a&gt; (&lt;a href=&#34;https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/&#34;&gt;Homepage&lt;/a&gt;) (ICCV&#39;2011)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/ucf101/README.md&#34;&gt;UCF101&lt;/a&gt; (&lt;a href=&#34;https://www.crcv.ucf.edu/research/data-sets/ucf101/&#34;&gt;Homepage&lt;/a&gt;) (CRCV-IR-12-01)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/activitynet/README.md&#34;&gt;ActivityNet&lt;/a&gt; (&lt;a href=&#34;http://activity-net.org/&#34;&gt;Homepage&lt;/a&gt;) (CVPR&#39;2015)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/kinetics/README.md&#34;&gt;Kinetics-[400/600/700]&lt;/a&gt; (&lt;a href=&#34;https://deepmind.com/research/open-source/kinetics/&#34;&gt;Homepage&lt;/a&gt;) (CVPR&#39;2017)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/sthv1/README.md&#34;&gt;SthV1&lt;/a&gt; (&lt;a href=&#34;https://20bn.com/datasets/something-something/v1/&#34;&gt;Homepage&lt;/a&gt;) (ICCV&#39;2017)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/sthv2/README.md&#34;&gt;SthV2&lt;/a&gt; (&lt;a href=&#34;https://20bn.com/datasets/something-something/&#34;&gt;Homepage&lt;/a&gt;) (ICCV&#39;2017)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/diving48/README.md&#34;&gt;Diving48&lt;/a&gt; (&lt;a href=&#34;http://www.svcl.ucsd.edu/projects/resound/dataset.html&#34;&gt;Homepage&lt;/a&gt;) (ECCV&#39;2018)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/jester/README.md&#34;&gt;Jester&lt;/a&gt; (&lt;a href=&#34;https://20bn.com/datasets/jester/v1&#34;&gt;Homepage&lt;/a&gt;) (ICCV&#39;2019)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/mit/README.md&#34;&gt;Moments in Time&lt;/a&gt; (&lt;a href=&#34;http://moments.csail.mit.edu/&#34;&gt;Homepage&lt;/a&gt;) (TPAMI&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/mmit/README.md&#34;&gt;Multi-Moments in Time&lt;/a&gt; (&lt;a href=&#34;http://moments.csail.mit.edu/challenge_iccv_2019.html&#34;&gt;Homepage&lt;/a&gt;) (ArXiv&#39;2019)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/hvu/README.md&#34;&gt;HVU&lt;/a&gt; (&lt;a href=&#34;https://github.com/holistic-video-understanding/HVU-Dataset&#34;&gt;Homepage&lt;/a&gt;) (ECCV&#39;2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/omnisource/README.md&#34;&gt;OmniSource&lt;/a&gt; (&lt;a href=&#34;https://kennymckormick.github.io/omnisource/&#34;&gt;Homepage&lt;/a&gt;) (ECCV&#39;2020)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/gym/README.md&#34;&gt;FineGYM&lt;/a&gt; (&lt;a href=&#34;https://sdolivia.github.io/FineGym/&#34;&gt;Homepage&lt;/a&gt;) (CVPR&#39;2020)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;4&#34; style=&#34;font-weight:bold;&#34;&gt;Action Localization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/thumos14/README.md&#34;&gt;THUMOS14&lt;/a&gt; (&lt;a href=&#34;https://www.crcv.ucf.edu/THUMOS14/download.html&#34;&gt;Homepage&lt;/a&gt;) (THUMOS Challenge 2014)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/activitynet/README.md&#34;&gt;ActivityNet&lt;/a&gt; (&lt;a href=&#34;http://activity-net.org/&#34;&gt;Homepage&lt;/a&gt;) (CVPR&#39;2015)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;4&#34; style=&#34;font-weight:bold;&#34;&gt;Spatio-Temporal Action Detection&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/ucf101_24/README.md&#34;&gt;UCF101-24*&lt;/a&gt; (&lt;a href=&#34;http://www.thumos.info/download.html&#34;&gt;Homepage&lt;/a&gt;) (CRCV-IR-12-01)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/jhmdb/README.md&#34;&gt;JHMDB*&lt;/a&gt; (&lt;a href=&#34;http://jhmdb.is.tue.mpg.de/&#34;&gt;Homepage&lt;/a&gt;) (ICCV&#39;2015)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/ava/README.md&#34;&gt;AVA&lt;/a&gt; (&lt;a href=&#34;https://research.google.com/ava/index.html&#34;&gt;Homepage&lt;/a&gt;) (CVPR&#39;2018)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;4&#34; style=&#34;font-weight:bold;&#34;&gt;Skeleton-based Action Recognition&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/skeleton/README.md&#34;&gt;PoseC3D-FineGYM&lt;/a&gt; (&lt;a href=&#34;https://kennymckormick.github.io/posec3d/&#34;&gt;Homepage&lt;/a&gt;) (ArXiv&#39;2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/skeleton/README.md&#34;&gt;PoseC3D-NTURGB+D&lt;/a&gt; (&lt;a href=&#34;https://kennymckormick.github.io/posec3d/&#34;&gt;Homepage&lt;/a&gt;) (ArXiv&#39;2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/skeleton/README.md&#34;&gt;PoseC3D-UCF101&lt;/a&gt; (&lt;a href=&#34;https://kennymckormick.github.io/posec3d/&#34;&gt;Homepage&lt;/a&gt;) (ArXiv&#39;2021)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2/raw/master/tools/data/skeleton/README.md&#34;&gt;PoseC3D-HMDB51&lt;/a&gt; (&lt;a href=&#34;https://kennymckormick.github.io/posec3d/&#34;&gt;Homepage&lt;/a&gt;) (ArXiv&#39;2021)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Datasets marked with * are not fully supported yet, but related dataset preparation steps are provided. A summary can be found on the &lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/supported_datasets.html&#34;&gt;&lt;strong&gt;Supported Datasets&lt;/strong&gt;&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;To demonstrate the efficacy and efficiency of our framework, we compare MMAction2 with some other popular frameworks and official releases in terms of speed. Details can be found in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/benchmark.md&#34;&gt;benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/data_preparation.md&#34;&gt;data_preparation.md&lt;/a&gt; for a general knowledge of data preparation. The supported datasets are listed in &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/supported_datasets.md&#34;&gt;supported_datasets.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/faq.md&#34;&gt;FAQ&lt;/a&gt; for frequently asked questions.&lt;/p&gt; &#xA;&lt;h2&gt;Projects built on MMAction2&lt;/h2&gt; &#xA;&lt;p&gt;Currently, there are many research works and projects built on MMAction2 by users from community, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Video Swin Transformer. &lt;a href=&#34;https://arxiv.org/abs/2106.13230&#34;&gt;[paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/SwinTransformer/Video-Swin-Transformer&#34;&gt;[github]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Evidential Deep Learning for Open Set Action Recognition, ICCV 2021 &lt;strong&gt;Oral&lt;/strong&gt;. &lt;a href=&#34;https://arxiv.org/abs/2107.10161&#34;&gt;[paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/Cogito2012/DEAR&#34;&gt;[github]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective, ICCV 2021 &lt;strong&gt;Oral&lt;/strong&gt;. &lt;a href=&#34;https://arxiv.org/abs/2103.17263&#34;&gt;[paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/xvjiarui/VFS&#34;&gt;[github]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;etc., check &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/projects.md&#34;&gt;projects.md&lt;/a&gt; to see all related projects.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMAction2. Please refer to &lt;a href=&#34;https://github.com/open-mmlab/mmcv/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; in MMCV for more details about the contributing guideline.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMAction2 is an open-source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features and users who give valuable feedback. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their new models.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{2020mmaction2,&#xA;    title={OpenMMLab&#39;s Next Generation Video Understanding Toolbox and Benchmark},&#xA;    author={MMAction2 Contributors},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmaction2}},&#xA;    year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmaction2/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Projects in OpenMMLab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmclassification&#34;&gt;MMClassification&lt;/a&gt;: OpenMMLab image classification toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt;: OpenMMLab image and video editing toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;: OpenMMLab image and video generative models toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Netflix/vmaf</title>
    <updated>2022-09-29T01:37:54Z</updated>
    <id>tag:github.com,2022-09-29:/Netflix/vmaf</id>
    <link href="https://github.com/Netflix/vmaf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Perceptual video quality assessment based on multi-method fusion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VMAF - Video Multi-Method Assessment Fusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://travis-ci.com/Netflix/vmaf&#34;&gt;&lt;img src=&#34;https://travis-ci.com/Netflix/vmaf.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Netflix/vmaf/actions?query=workflow%3Alibvmaf&#34;&gt;&lt;img src=&#34;https://github.com/Netflix/vmaf/workflows/libvmaf/badge.svg?sanitize=true&#34; alt=&#34;libvmaf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Netflix/vmaf/actions?query=workflow%3AWindows&#34;&gt;&lt;img src=&#34;https://github.com/Netflix/vmaf/workflows/Windows/badge.svg?sanitize=true&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Netflix/vmaf/actions?query=workflow%3Affmpeg&#34;&gt;&lt;img src=&#34;https://github.com/Netflix/vmaf/workflows/ffmpeg/badge.svg?sanitize=true&#34; alt=&#34;ffmpeg&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Netflix/vmaf/actions?query=workflow%3ADocker&#34;&gt;&lt;img src=&#34;https://github.com/Netflix/vmaf/workflows/Docker/badge.svg?sanitize=true&#34; alt=&#34;Docker&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;VMAF is an &lt;a href=&#34;https://theemmys.tv/&#34;&gt;Emmy-winning&lt;/a&gt; perceptual video quality assessment algorithm developed by Netflix. This software package includes a stand-alone C library &lt;code&gt;libvmaf&lt;/code&gt; and its wrapping Python library. The Python library also provides a set of tools that allows a user to train and test a custom VMAF model.&lt;/p&gt; &#xA;&lt;p&gt;Read &lt;a href=&#34;https://medium.com/netflix-techblog/toward-a-practical-perceptual-video-quality-metric-653f208b9652&#34;&gt;this&lt;/a&gt; tech blog post for an overview, &lt;a href=&#34;https://medium.com/netflix-techblog/vmaf-the-journey-continues-44b51ee9ed12&#34;&gt;this&lt;/a&gt; post for the tips of best practices, and &lt;a href=&#34;https://netflixtechblog.com/toward-a-better-quality-metric-for-the-video-community-7ed94e752a30&#34;&gt;this&lt;/a&gt; post for our latest efforts on speed optimization, new API design and the introduction of a codec evaluation-friendly &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/models.md#disabling-enhancement-gain-neg-mode&#34;&gt;NEG mode&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also included in &lt;code&gt;libvmaf&lt;/code&gt; are implementations of several other metrics: PSNR, PSNR-HVS, SSIM, MS-SSIM and CIEDE2000.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/images/vmaf_logo.jpg&#34; alt=&#34;vmaf logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2021-12-15) We have added to CAMBI the &lt;code&gt;full_ref&lt;/code&gt; input parameter to allow running CAMBI as a full-reference metric, taking into account the banding that was already present on the source. Check out the &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/cambi.md&#34;&gt;usage&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;(2021-12-1) We have added to CAMBI the &lt;code&gt;max_log_contrast&lt;/code&gt; input parameter to allow to capture banding with higher contrasts than the default. We have also sped up CAMBI (e.g., around 4.5x for 4k). Check out the &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/cambi.md&#34;&gt;usage&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;(2021-10-7) We are open-sourcing CAMBI (Contrast Aware Multiscale Banding Index) - Netflix&#39;s detector for banding (aka contouring) artifacts. Check out the &lt;a href=&#34;https://netflixtechblog.medium.com/cambi-a-banding-artifact-detector-96777ae12fe2&#34;&gt;tech blog&lt;/a&gt; for an overview and the &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/papers/CAMBI_PCS2021.pdf&#34;&gt;technical paper&lt;/a&gt; published in PCS 2021 (note that the paper describes an initial version of CAMBI that no longer matches the code exactly, but it is still a good introduction). Also check out the &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/cambi.md&#34;&gt;usage&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;(2020-12-7) Check out our &lt;a href=&#34;https://netflixtechblog.com/toward-a-better-quality-metric-for-the-video-community-7ed94e752a30&#34;&gt;latest tech blog&lt;/a&gt; on speed optimization, new API design and the introduction of a codec evaluation-friendly NEG mode.&lt;/li&gt; &#xA; &lt;li&gt;(2020-12-3) We are releasing &lt;code&gt;libvmaf v2.0.0&lt;/code&gt;. It has a new fixed-point and x86 SIMD-optimized (AVX2, AVX-512) implementation that achieves 2x speed up compared to the previous floating-point version. It also has a &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/libvmaf/README.md&#34;&gt;new API&lt;/a&gt; that is more flexible and extensible.&lt;/li&gt; &#xA; &lt;li&gt;(2020-7-13) We have created a &lt;a href=&#34;https://docs.google.com/document/d/1dJczEhXO0MZjBSNyKmd3ARiCTdFVMNPBykH4_HMPoyY/edit?usp=sharing&#34;&gt;memo&lt;/a&gt; to share our thoughts on VMAF&#39;s property in the presence of image enhancement operations, its impact on codec evaluation, and our solutions. Accordingly, we have added a new mode called &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/models.md#disabling-enhancement-gain-neg-mode&#34;&gt;No Enhancement Gain (NEG)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(2020-2-27) We have changed VMAF&#39;s license from Apache 2.0 to &lt;a href=&#34;https://opensource.org/licenses/BSDplusPatent&#34;&gt;BSD+Patent&lt;/a&gt;, a more permissive license compared to Apache that also includes an express patent grant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;There is an &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/index.md&#34;&gt;overview of the documentation&lt;/a&gt; with links to specific pages, covering FAQs, available models and features, software usage guides, and a list of resources.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The software package offers a number of ways to interact with the VMAF implementation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The command-line tool &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/libvmaf/tools/README.md&#34;&gt;&lt;code&gt;vmaf&lt;/code&gt;&lt;/a&gt; provides a complete algorithm implementation, such that one can easily deploy VMAF in a production environment. Additionally, the &lt;code&gt;vmaf&lt;/code&gt; tool provides a number of auxillary features such as PSNR, SSIM and MS-SSIM.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/libvmaf/README.md&#34;&gt;C library &lt;code&gt;libvmaf&lt;/code&gt;&lt;/a&gt; provides an interface to incorporate VMAF into your code, and tools to integrate other feature extractors into the library.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/python.md&#34;&gt;Python library&lt;/a&gt; offers a full array of wrapper classes and scripts for software testing, VMAF model training and validation, dataset processing, data visualization, etc.&lt;/li&gt; &#xA; &lt;li&gt;VMAF is now included as a filter in FFmpeg, and can be configured using: &lt;code&gt;./configure --enable-libvmaf&lt;/code&gt;. Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/ffmpeg.md&#34;&gt;Using VMAF with FFmpeg&lt;/a&gt; page.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/Dockerfile&#34;&gt;VMAF Dockerfile&lt;/a&gt; generates a docker image from the &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/python.md&#34;&gt;Python library&lt;/a&gt;. Refer to &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/docker.md&#34;&gt;this&lt;/a&gt; document for detailed usage.&lt;/li&gt; &#xA; &lt;li&gt;To build VMAF on Windows, follow &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/windows.md&#34;&gt;these&lt;/a&gt; instructions.&lt;/li&gt; &#xA; &lt;li&gt;AOM CTC: &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/(http://aomedia.org/)&#34;&gt;AOM&lt;/a&gt; has specified vmaf to be the standard implementation metrics tool according to the AOM common test conditions (CTC). Refer to &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/resource/doc/aom_ctc.md&#34;&gt;this page&lt;/a&gt; for usage compliant with AOM CTC.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution Guide&lt;/h2&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Netflix/vmaf/master/CONTRIBUTING.md&#34;&gt;contribution&lt;/a&gt; page. Also refer to this &lt;a href=&#34;https://docs.google.com/presentation/d/1Gr4-MvOXu9HUiH4nnqLGWupJYMeh6nl2MNz6Qy9153c/edit#slide=id.gc20398b4b7_0_132&#34;&gt;slide deck&lt;/a&gt; for an overview contribution guide.&lt;/p&gt;</summary>
  </entry>
</feed>