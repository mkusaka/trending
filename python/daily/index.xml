<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-29T01:41:26Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>GreyDGL/PentestGPT</title>
    <updated>2023-04-29T01:41:26Z</updated>
    <id>tag:github.com,2023-04-29:/GreyDGL/PentestGPT</id>
    <link href="https://github.com/GreyDGL/PentestGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A GPT-empowered penetration testing tool&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PentestGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;We&#39;re testing PentestGPT on HackTheBox&lt;/strong&gt;. You may follow &lt;a href=&#34;https://www.hackthebox.com/home/users/profile/1489431&#34;&gt;this link&lt;/a&gt;. More details will be released soon. &lt;strong&gt;We include a video of using PentestGPT for OSCP-like machine: &lt;a href=&#34;https://youtu.be/lAjLIj1JT3c&#34;&gt;HTB-Jarvis&lt;/a&gt;&lt;/strong&gt;. This is the first part only, and I&#39;ll complete the rest when I have time.&lt;/p&gt; &#xA;&lt;h2&gt;Common Questions&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: What is PentestGPT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: PentestGPT is a penetration testing tool empowered by ChatGPT. It is designed to automate the penetration testing process. It is built on top of ChatGPT and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Do I need to be a ChatGPT plus member to use PentestGPT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: Yes. PentestGPT relies on GPT-4 model for high-quality reasoning. Since there is no public GPT-4 API yet, a wrapper is included to use ChatGPT session to support PentestGPT.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why GPT-4? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: After empirical evaluation, we found that GPT-4 performs better than GPT-3.5 in terms of penetration testing reasoning. In fact, GPT-3.5 leads to failed test in simple tasks.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Why not just use GPT-4 directly? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: We found that GPT-4 suffers from losses of context as test goes deeper. It is essential to maintain a &#34;test status awareness&#34; in this process. You may check the PentestGPT design &lt;a href=&#34;https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/PentestGPT_design.md&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: What about AutoGPT? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: AutoGPT is not designed for pentest. It may perform malicious operations. Due to this consideration, we design PentestGPT in an interactive mode. Of course, our end goal is an automated pentest solution.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Q&lt;/strong&gt;: Future plan? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;: We&#39;re working on a paper to explore the tech details behind automated pentest. Meanwhile, please feel free to raise issues/discussions. I&#39;ll do my best to address all of them.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;PentestGPT&lt;/strong&gt; is a penetration testing tool empowered by &lt;strong&gt;ChatGPT&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It is designed to automate the penetration testing process. It is built on top of ChatGPT and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;PentestGPT&lt;/strong&gt; is able to solve easy to medium HackTheBox machines, and other CTF challenges. You can check &lt;a href=&#34;https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/resources/README.md&#34;&gt;this&lt;/a&gt; example in &lt;code&gt;resources&lt;/code&gt; where we use it to solve HackTheBox challenge &lt;strong&gt;TEMPLATED&lt;/strong&gt; (web challenge).&lt;/li&gt; &#xA; &lt;li&gt;A sample testing process of &lt;strong&gt;PentestGPT&lt;/strong&gt; on a target VulnHub machine (Hackable II) is available at &lt;a href=&#34;https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/resources/PentestGPT_Hackable2.pdf&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A sample usage video is below: (or available here: &lt;a href=&#34;https://youtu.be/h0k6kWWaCEU&#34;&gt;Demo&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/78410652/232327920-7318a0c4-bee0-4cb4-becb-6658b80180ff.mov&#34;&gt;https://user-images.githubusercontent.com/78410652/232327920-7318a0c4-bee0-4cb4-becb-6658b80180ff.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The project is still in its early stage. Feel free to raise any issues when using the tool.&lt;/li&gt; &#xA; &lt;li&gt;Please help to contribute by submitting the vulnerabilities you identified or challenges you solved with &lt;strong&gt;PentestGPT&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;This project is for research purpose. Please contact me if you&#39;re interested in collaboration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;code&gt;requirements.txt&lt;/code&gt; with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;(Deprecated: Will update support for non-plus member later.) &lt;del&gt;Install &lt;code&gt;chatgpt-wrapper&lt;/code&gt; if you&#39;re non-plus members: &lt;code&gt;pip install git+https://github.com/mmabrouk/chatgpt-wrapper&lt;/code&gt;. More details at: &lt;a href=&#34;https://github.com/mmabrouk/chatgpt-wrapper&#34;&gt;https://github.com/mmabrouk/chatgpt-wrapper&lt;/a&gt;. Note that the support for non-plus members are not optimized.&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;Configure the cookies in &lt;code&gt;config&lt;/code&gt;. You may follow a sample by &lt;code&gt;cp config/chatgpt_config_sample.py. config/chatgpt_config.py&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Login to ChatGPT session page.&lt;/li&gt; &#xA;   &lt;li&gt;In &lt;code&gt;Inspect - Network&lt;/code&gt;, find the connections to the ChatGPT session page.&lt;/li&gt; &#xA;   &lt;li&gt;Find the cookie in the &lt;strong&gt;request header&lt;/strong&gt; in the request to &lt;code&gt;https://chat.openai.com/api/auth/session&lt;/code&gt; and paste it into the &lt;code&gt;cookie&lt;/code&gt; field of &lt;code&gt;config/chatgpt_config.py&lt;/code&gt;. (You may use Inspect-&amp;gt;Network, find session and copy the &lt;code&gt;cookie&lt;/code&gt; field in &lt;code&gt;request_headers&lt;/code&gt; to &lt;code&gt;https://chat.openai.com/api/auth/session&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Note that the other fields are temporarily deprecated due to the update of ChatGPT page.&lt;/li&gt; &#xA;   &lt;li&gt;Fill in &lt;code&gt;userAgent&lt;/code&gt; with your user agent.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;To verify that the connection is configured properly, you may run &lt;code&gt;python3 test_connection.py&lt;/code&gt;. You should see some sample conversation with ChatGPT.&lt;/li&gt; &#xA; &lt;li&gt;(Notice) The above verification process is not stable. If you encounter errors after several trials, please try to refresh the page, repeat the above steps, and try again. You may also try with the cookie to &lt;code&gt;https://chat.openai.com/backend-api/conversations&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;To start, run &lt;code&gt;python3 main.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The tool works similar to &lt;em&gt;msfconsole&lt;/em&gt;. Follow the guidance to perform penetration testing.&lt;/li&gt; &#xA; &lt;li&gt;In general, PentestGPT intakes commands similar to chatGPT. There are several basic commands. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The commands are: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;help&lt;/code&gt;: show the help message.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;next&lt;/code&gt;: key in the test execution result and get the next step.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;more&lt;/code&gt;: let &lt;strong&gt;PentestGPT&lt;/strong&gt; to explain more details of the current step. Also, a new sub-task solver will be created to guide the tester.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;todo&lt;/code&gt;: show the todo list.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;discuss&lt;/code&gt;: discuss with the &lt;strong&gt;PentestGPT&lt;/strong&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;google&lt;/code&gt;: search on Google. This function is still under development.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;quit&lt;/code&gt;: exit the tool and save the output as log file (see the &lt;strong&gt;reporting&lt;/strong&gt; section below).&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;You can use &amp;lt;SHIFT + right arrow&amp;gt; to end your input (and &#xA;    &lt;enter&gt;&#xA;      is for next line).&#xA;    &lt;/enter&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You may always use &lt;code&gt;TAB&lt;/code&gt; to autocomplete the commands.&lt;/li&gt; &#xA;   &lt;li&gt;When you&#39;re given a drop-down selection list, you can use cursor or arrow key to navigate the list. Press &lt;code&gt;ENTER&lt;/code&gt; to select the item. Similarly, use &amp;lt;SHIFT + right arrow&amp;gt; to confirm selection.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;In the sub-task handler initiated by &lt;code&gt;more&lt;/code&gt;, users can execute more commands to investigate into a specific problem: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The commands are: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;help&lt;/code&gt;: show the help message.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;brainstorm&lt;/code&gt;: let PentestGPT brainstorm on the local task for all the possible solutions.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;discuss&lt;/code&gt;: discuss with PentestGPT about this local task.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;google&lt;/code&gt;: search on Google. This function is still under development.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;continue&lt;/code&gt;: exit the sub task and continue the main testing session.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Report&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;After finishing the penetration testing, a report will be automatically generated in &lt;code&gt;logs&lt;/code&gt; folder (if you quit with &lt;code&gt;quit&lt;/code&gt; command).&lt;/li&gt; &#xA; &lt;li&gt;The report can be printed in a human-readable format by running &lt;code&gt;python3 utils/report_generator.py &amp;lt;log file&amp;gt;&lt;/code&gt;. A sample report &lt;code&gt;sample_pentestGPT_log.txt&lt;/code&gt; is also uploaded.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Design Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The current design is mainly for web penetration testing&lt;/p&gt; &#xA;&lt;h3&gt;General Design&lt;/h3&gt; &#xA;&lt;p&gt;PentestGPT provides a unified terminal input handler, and backed by three main components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A test generation module which generates the exact penetration testing commands or operations for the users to execute.&lt;/li&gt; &#xA; &lt;li&gt;A test reasoning module which conducts the reasoning of the test, guiding the penetration testers on what to do next.&lt;/li&gt; &#xA; &lt;li&gt;A parsing module which parses the output of the penetration tools and the contents on the webUI.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Function Design&lt;/h3&gt; &#xA;&lt;p&gt;The handler is the main entry point of the penetration testing tool. It allows pentesters to perform the following operations:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(initialize itself with some pre-designed prompts.)&lt;/li&gt; &#xA; &lt;li&gt;Start a new penetration testing session by providing the target information.&lt;/li&gt; &#xA; &lt;li&gt;Ask for todo-list, and acquire the next step to perform.&lt;/li&gt; &#xA; &lt;li&gt;After completing the operation, pass the information to PentestGPT. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Pass a tool output.&lt;/li&gt; &#xA;   &lt;li&gt;Pass a webpage content.&lt;/li&gt; &#xA;   &lt;li&gt;Pass a human description.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>nlpxucan/WizardLM</title>
    <updated>2023-04-29T01:41:26Z</updated>
    <id>tag:github.com,2023-04-29:/nlpxucan/WizardLM</id>
    <link href="https://github.com/nlpxucan/WizardLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;WizardLM: An Instruction-following LLM Using Evol-Instruct&lt;/h2&gt; &#xA;&lt;p&gt;Empowering Large Pre-Trained Language Models to Follow Complex Instructions&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/WizardLM.png&#34; alt=&#34;WizardLM&#34; style=&#34;width: 20%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;At present, our core contributors are fully engaged in preparing the WizardLM-7B model trained with full evolved instructions (&lt;strong&gt;approximately 300k&lt;/strong&gt;). We apologize for any possible delay in responding to your questions. If you find that the demo is temporarily unavailable, please be patient and &lt;strong&gt;wait a while&lt;/strong&gt;. Our contributors regularly check the demo&#39;s status and handle any issues.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî• We released &lt;strong&gt;7B&lt;/strong&gt; version of &lt;strong&gt;WizardLM&lt;/strong&gt; trained with &lt;strong&gt;70k&lt;/strong&gt; evolved instructions. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.12244&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://6f8173a3550ed441ab.gradio.live&#34;&gt;demo1&lt;/a&gt; , &lt;a href=&#34;https://261f01fdd31bfe1ca0.gradio.live/&#34;&gt;demo2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì£ We are looking for highly motivated students to join us as interns to create more intelligent AI together. Please contact &lt;a href=&#34;mailto:caxu@microsoft.com&#34;&gt;caxu@microsoft.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Although &lt;strong&gt;on our complexity-balanced test set, WizardLM-7B outperforms ChatGPT in the high-complexity instructions&lt;/strong&gt;, it still lags behind ChatGPT on the entire test set, and we also consider WizardLM to still be in a &lt;strong&gt;baby state&lt;/strong&gt;. This repository will &lt;strong&gt;continue to improve WizardLM&lt;/strong&gt;, train on larger scales, add more training data, and innovate more advanced large-model training methods.&lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;Note for demo usage:&lt;/b&gt; Demo 1-4 are all WizardLM-7B. Please use them as evenly as possible to prevent one of them from being too heavy and responding slowly. We only recommend using &lt;strong&gt;English&lt;/strong&gt; to experience our model. Support for other languages will be introduced in the future. The demo currently only supports &lt;strong&gt;single-turn&lt;/strong&gt; conversation.&lt;/p&gt; &#xA;&lt;h2&gt;Call for Feedbacks&lt;/h2&gt; &#xA;&lt;p&gt;We welcome everyone to use your professional and difficult instructions to evaluate WizardLM, and show us examples of poor performance and your suggestions in the &lt;a href=&#34;https://github.com/nlpxucan/WizardLM/issues&#34;&gt;issue discussion&lt;/a&gt; area. We are focusing on improving the Evol-Instruct now and hope to relieve existing weaknesses and issues in the the next version of WizardLM. After that, we will open the code and pipeline of up-to-date Evol-Instruct algorithm and work with you together to improve it.&lt;/p&gt; &#xA;&lt;h2&gt;Unofficial Video Introductions&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to the enthusiastic friends, their video introductions are more lively and interesting.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SaJ8wyKMBds&#34;&gt;GET WizardLM NOW! 7B LLM KING That Can Beat ChatGPT! I&#39;m IMPRESSED!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=I6sER-qivYk&#34;&gt;WizardLM: Enhancing Large Language Models to Follow Complex Instructions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Case Show&lt;/h2&gt; &#xA;&lt;p&gt;We just sample some cases to demonstrate the performance of WizardLM and ChatGPT on data of varying difficulty, and the details pls refer &lt;a href=&#34;https://github.com/nlpxucan/WizardLM/raw/main/src/case_show.md&#34;&gt;Case Show&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Overview of Evol-Instruct&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nlpxucan/evol-instruct&#34;&gt;Evol-Instruct&lt;/a&gt; is a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/git_overall.png&#34; alt=&#34;WizardLM&#34; style=&#34;width: 68%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/git_running.png&#34; alt=&#34;WizardLM&#34; style=&#34;width: 68%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#online-demo&#34;&gt;Online Demo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#train_data&#34;&gt;Training Data&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#wizardlm-weights&#34;&gt;WizardLM Weights&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#finetune&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/#disclaimer&#34;&gt;Disclaimer&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;We will provide our latest models for you to try for as long as possible. If you find a link is not working, please try another one. At the same time, please try as many &lt;strong&gt;real-world&lt;/strong&gt; and &lt;strong&gt;challenging&lt;/strong&gt; problems that you encounter in your work and life as possible. We will continue to evolve our models with your feedbacks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://6f8173a3550ed441ab.gradio.live&#34;&gt;Demo Link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://261f01fdd31bfe1ca0.gradio.live/&#34;&gt;Demo Backup 1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training Data&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/datasets/victor123/evol_instruct_70k&#34;&gt;&lt;code&gt;alpaca_evol_instruct_70k.json&lt;/code&gt;&lt;/a&gt; contains 70K instruction-following data generated from Evol-Instruct. We used it for fine-tuning the WizardLM model. This JSON file is a list of dictionaries, each dictionary contains the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 70K instructions is unique.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;WizardLM Weights&lt;/h2&gt; &#xA;&lt;p&gt;We release [WizardLM] weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the WizardLM weights. Instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the original LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Please download our delta model at the following &lt;a href=&#34;https://huggingface.co/victor123/WizardLM&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get WizardLM weights by applying our delta:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python src/weight_diff_wizard.py recover --path_raw &amp;lt;path_to_step_1_dir&amp;gt; --path_diff &amp;lt;path_to_step_2_dir&amp;gt; --path_tuned &amp;lt;path_to_store_recovered_weights&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;We fine-tune WizardLM using code from &lt;a href=&#34;https://github.com/AetherCortex/Llama-X&#34;&gt;Llama-X&lt;/a&gt;. We fine-tune LLaMA-7B with the following hyperparameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;LLaMA-7B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Batch size&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning rate&lt;/td&gt; &#xA;   &lt;td&gt;2e-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Epochs&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Max length&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Warmup step&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LR scheduler&lt;/td&gt; &#xA;   &lt;td&gt;cosine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To reproduce our fine-tuning of WizardLM, please follow the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;According to the instructions of &lt;a href=&#34;https://github.com/AetherCortex/Llama-X&#34;&gt;Llama-X&lt;/a&gt;, install the environment, download the training code, and deploy.&lt;/li&gt; &#xA; &lt;li&gt;Replace the train.py with the train_freeform.py in our repo(src/train_freeform.py)&lt;/li&gt; &#xA; &lt;li&gt;Execute the following training command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepspeed train_freeform.py \&#xA;    --model_name_or_path /path/to/llama-7B/hf \&#xA;    --data_path /path/to/alpaca_evol_instruct_70k.json \&#xA;    --output_dir /path/to/wizardlm-7B/hf/ft \&#xA;    --num_train_epochs 3 \&#xA;    --model_max_length 2048 \&#xA;    --per_device_train_batch_size 8 \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 800 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --warmup_steps 2 \&#xA;    --logging_steps 2 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --report_to &#34;tensorboard&#34; \&#xA;    --gradient_checkpointing True \&#xA;    --deepspeed configs/deepspeed_config.json \&#xA;    --fp16 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We provide the decoding script for WizardLM, which reads a input file and generates corresponding responses for each sample, and finally consolidates them into an output file.&lt;/p&gt; &#xA;&lt;p&gt;You can specify &lt;code&gt;base_model&lt;/code&gt;, &lt;code&gt;input_data_path&lt;/code&gt; and &lt;code&gt;output_data_path&lt;/code&gt; in src\inference_wizardlm.py to set the decoding model, path of input file and path of output file. The decoding command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python src\inference_wizardlm.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To evaluate Wizard, we conduct human evaluation on the inputs from our human instruct evaluation set &lt;a href=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/data/WizardLM_testset.jsonl&#34;&gt;&lt;code&gt;WizardLM_testset.jsonl&lt;/code&gt;&lt;/a&gt; . This evaluation set was collected by the authors and covers a diverse list of user-oriented instructions including difficult Coding Generation &amp;amp; Debugging, Math, Reasoning, Complex Formats, Academic Writing, Extensive Disciplines, and so on. We performed a blind pairwise comparison between Wizard and baselines. Specifically, we recruit 10 well-educated annotators to rank the models from 1 to 5 on relevance, knowledgeable, reasoning, calculation and accuracy.&lt;/p&gt; &#xA;&lt;p&gt;WizardLM achieved significantly better results than Alpaca and Vicuna-7b.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;60%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/win.png&#34; alt=&#34;WizardLM&#34; style=&#34;width: 60%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;In the high-difficulty section of our test set (difficulty level &amp;gt;= 8), WizardLM even outperforms ChatGPT, with a win rate 7.9% larger than Chatgpt (42.9% vs. 35.0%). This indicates that our method can significantly improve the ability of large language models to handle complex instructions.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;60%&#34;&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/windiff.png&#34; alt=&#34;WizardLM&#34; style=&#34;width: 60%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{xu2023wizardlm,&#xA;      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, &#xA;      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},&#xA;      year={2023},&#xA;      eprint={2304.12244},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The resources, including code, data, and model weights, associated with this project are restricted for academic research purposes only and cannot be used for commercial purposes. The content produced by any version of WizardLM is influenced by uncontrollable variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by this project. This project does not accept any legal liability for the content of the model output, nor does it assume responsibility for any losses incurred due to the use of associated resources and output results.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>UX-Decoder/Segment-Everything-Everywhere-All-At-Once</title>
    <updated>2023-04-29T01:41:26Z</updated>
    <id>tag:github.com,2023-04-29:/UX-Decoder/Segment-Everything-Everywhere-All-At-Once</id>
    <link href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of the paper &#34;Segment Everything Everywhere All at Once&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üëÄ&lt;em&gt;SEEM:&lt;/em&gt; Segment Everything Everywhere All at Once&lt;/h1&gt; &#xA;&lt;p&gt;We introduce &lt;strong&gt;SEEM&lt;/strong&gt; that can &lt;strong&gt;S&lt;/strong&gt;egment &lt;strong&gt;E&lt;/strong&gt;verything &lt;strong&gt;E&lt;/strong&gt;verywhere with &lt;strong&gt;M&lt;/strong&gt;ulti-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts!&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üçá&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/pdf/2304.06718.pdf&#34;&gt;Read our arXiv Paper&lt;/a&gt;] &amp;nbsp; &lt;span&gt;üçé&lt;/span&gt; [&lt;a href=&#34;https://huggingface.co/spaces/xdecoder/SEEM&#34;&gt;Try Hugging Face Demo&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üëâ&lt;/span&gt; &lt;em&gt;[New]&lt;/em&gt; &lt;strong&gt;One-Line Getting Started with Linux:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone git@github.com:UX-Decoder/Segment-Everything-Everywhere-All-At-Once.git &amp;amp;&amp;amp; cd Segment-Everything-Everywhere-All-At-Once/demo_code &amp;amp;&amp;amp; sh run_demo.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;strong&gt;Related projects:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/FocalNet&#34;&gt;FocalNet&lt;/a&gt; : Focal Modulation Networks; &lt;strong&gt;We used FocalNet as the vision backbone&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/UniCL&#34;&gt;UniCL&lt;/a&gt; : Unified Contrastive Learning; &lt;strong&gt;We used this technique for image-text contrastive learning&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/X-Decoder&#34;&gt;X-Decoder&lt;/a&gt; : Generic decoder that can do multiple tasks with one model onlyÔºõ&lt;strong&gt;We built SEEM based on X-Decoder&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; &lt;strong&gt;Other projects you may find interesting:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/OpenSeeD&#34;&gt;OpenSeed&lt;/a&gt; : Strong open-set segmentation methods.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounding SAM&lt;/a&gt; : Combining Grounding DINO and Segment Anything; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt;: A strong open-set detection model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/X-Decoder/tree/xgpt&#34;&gt;X-GPT&lt;/a&gt; : Conversational Visual Agent supported by X-Decoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt; : Large Language and Vision Assistant.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.28]&lt;/strong&gt; We have updated the &lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/raw/main/SEEM_arXiv.pdf&#34;&gt;Paper&lt;/a&gt; that shows &lt;em&gt;better interactive segmentation results than SAM&lt;/em&gt;, which trained on x50 more data than us!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.26]&lt;/strong&gt; We have released the &lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/tree/main/demo_code&#34;&gt;Demo Code&lt;/a&gt; and &lt;a href=&#34;https://projects4jw.blob.core.windows.net/x-decoder/release/seem_focalt_v1.pt&#34;&gt;SEEM-Tiny Checkpoint&lt;/a&gt;! Please try the One-Line Started!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023.04.20]&lt;/strong&gt; SEEM Referring Video Segmentation is out! Please try the &lt;a href=&#34;https://huggingface.co/spaces/xdecoder/SEEM&#34;&gt;Video Demo&lt;/a&gt; and take a look at the &lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once#tulip-nerf-examples&#34;&gt;NERF examples&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/233255289-35c0c1e2-35f7-48e4-a7e9-68da50c839d3.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/233526415-a0a44963-19a3-4e56-965a-afaa598e6127.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üí°&lt;/span&gt; Highlights&lt;/h2&gt; &#xA;&lt;p&gt;Inspired by the appealing universal interface in LLMs, we are advocating a universal, interactive multi-modal interface for any type of segmentation with &lt;strong&gt;ONE SINGLE MODEL&lt;/strong&gt;. We emphasize &lt;strong&gt;4&lt;/strong&gt; important features of &lt;strong&gt;SEEM&lt;/strong&gt; below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versatility&lt;/strong&gt;: work with various types of prompts, for example, clicks, boxes, polygons, scribbles, texts, and referring image;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compositionaliy&lt;/strong&gt;: deal with any compositions of prompts;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interactivity&lt;/strong&gt;: interact with user in multi-rounds, thanks to the memory prompt of &lt;strong&gt;SEEM&lt;/strong&gt; to store the session history;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Semantic awareness&lt;/strong&gt;: give a semantic label to any predicted mask;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/teaser_new.png?raw=true&#34; alt=&#34;SEEM design&#34;&gt; A brief introduction of all the generic and interactive segmentation tasks we can do.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ü¶Ñ&lt;/span&gt; How to use the demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try our default examples first;&lt;/li&gt; &#xA; &lt;li&gt;Upload an image;&lt;/li&gt; &#xA; &lt;li&gt;Select at least one type of prompt of your choice (If you want to use referred region of another image please check &#34;Example&#34; and upload another image in referring image panel);&lt;/li&gt; &#xA; &lt;li&gt;Remember to provide the actual prompt for each prompt type you select, otherwise you will meet an error (e.g., remember to draw on the referring image);&lt;/li&gt; &#xA; &lt;li&gt;Our model by default support the &lt;strong&gt;vocabulary&lt;/strong&gt; of COCO 80 categories, others will be classified to &#39;others&#39; or misclassified. If you want to segment using open-vocabulary labels, include the text label in &#39;text&#39; button after drawing scribbles.&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Submit&#34; and wait for a few seconds.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåã&lt;/span&gt; An interesting example&lt;/h2&gt; &#xA;&lt;p&gt;An example of Transformers. The referred image is the truck form of Optimus Prime. Our model can always segment Optimus Prime in target images no matter which form it is in. Thanks Hongyang Li for this fun example.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/transformers_gh.png&#34; width=&#34;700&#34; alt=&#34;assets/transformers_gh.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;span&gt;üå∑&lt;/span&gt; NERF Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspired by the example in SA3D, we tried SEEM on NERF Examples and works well :)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/234230320-2189056d-1c89-4f0c-88da-851d12e8323c.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/11957155/234231284-0adc4bae-ef90-41d3-9883-41f6407a883b.gif&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèï&lt;/span&gt; Click, scribble to mask&lt;/h2&gt; &#xA;&lt;p&gt;With a simple click or stoke from the user, we can generate the masks and the corresponding category labels for it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/click.png?raw=true&#34; alt=&#34;SEEM design&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèî&lt;/span&gt; Text to mask&lt;/h2&gt; &#xA;&lt;p&gt;SEEM can generate the mask with text input from the user, providing multi-modality interaction with human.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/text.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;!-- &#xA;&lt;div  align=&#34;center&#34;&gt;    &#xA;&lt;img src=&#34;assets/text.png&#34; width = &#34;700&#34; alt=&#34;assets/text.png&#34; align=center /&gt;&#xA;&lt;/div&gt; --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üïå&lt;/span&gt; Referring image to mask&lt;/h2&gt; &#xA;&lt;p&gt;With a simple click or stroke on the referring image, the model is able to segment the objects with similar semantics on the target images. &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/ref_seg.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;SEEM understands the spatial relationship very well. Look at the three zebras! The segmented zebras have similar positions with the referred zebras. For example, when the leftmost zebra is referred on the upper row, the leftmost zebra on the bottom row is segmented. &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/spatial_relation.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåº&lt;/span&gt; Referring image to video mask&lt;/h2&gt; &#xA;&lt;p&gt;No training on video data needed, SEEM works perfectly for you to segment videos with whatever queries you specify! &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/referring_video_visualize.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåª&lt;/span&gt; Audio to mask&lt;/h2&gt; &#xA;&lt;p&gt;We use Whisper to turn audio into text prompt to segment the object. Try it in our demo!&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/audio.png&#34; width=&#34;900&#34; alt=&#34;assets/audio.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ## üî• Combination of different prompts to mask --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üå≥&lt;/span&gt; Examples of different styles&lt;/h2&gt; &#xA;&lt;p&gt;An example of segmenting a meme.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/emoj.png&#34; width=&#34;500&#34; alt=&#34;assets/emoj.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;An example of segmenting trees in cartoon style.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/trees_text.png&#34; width=&#34;700&#34; alt=&#34;assets/trees_text.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;An example of segmenting a Minecraft image.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/minecraft.png&#34; width=&#34;700&#34; alt=&#34;assets/minecraft.png&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![example](assets/minecraft.png?raw=true) --&gt; An example of using referring image on a popular teddy bear. &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/fox_v2.png?raw=true&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/model.png?raw=true&#34; alt=&#34;SEEM design&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with SAM&lt;/h2&gt; &#xA;&lt;p&gt;In the following figure, we compare the levels of interaction and semantics of three segmentation tasks (edge detection, open-set, and interactive segmentation). Open-set Segmentation usually requires a high level of semantics and does not require interaction. Compared with &lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;SAM&lt;/a&gt;, SEEM covers a wider range of interaction and semantics levels. For example, SAM only supports limited interaction types like points and boxes, while misses high-semantic tasks since it does not output semantic labels itself. The reasons are: First, SEEM has a unified prompt encoder that encodes all visual and language prompts into a joint representation space. In consequence, SEEM can support more general usages. It has potential to extend to custom prompts. Second, SEEM works very well on text to mask (grounding segmentation) and outputs semantic-aware predictions.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/compare.jpg&#34; width=&#34;500&#34; alt=&#34;assets/compare.jpg&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- This figure shows a comparison of our model with concurrent work SAM on the level of interactions and semantics. The x-axis and y-axis denote the level of interaction and semantics, respectively. Three segmentation tasks are shown, including Open-set Segmentation, Edge detection, and Interactive Segmentation. These tasks have different levels of interactions and semantics. For example, Open-set Segmentation usually requires a high level of semantics and does not require interaction. Compared with SAM, our model covers a wider range of interaction and semantics levels. For example, SAM only supports limited interaction types like points and boxes, while misses high-semantic tasks since it does not output semantic labels itself. Note that although we do not report edge detection results, our model can support it by simply converting masks to edges. --&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìë&lt;/span&gt; Catelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SEEM Demo&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference and Installation Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; (Soon) Evaluation Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; (TBD When) Training Code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíò&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We appreciate hugging face for the GPU support on demo!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ## Citation (update when paper is available on arxiv)&#xA;If you find this project helpful for your research, please consider citing the following BibTeX entry.&#xA;```BibTex&#xA;&#xA;``` --&gt;</summary>
  </entry>
</feed>