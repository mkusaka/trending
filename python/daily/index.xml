<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-17T01:39:15Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>av/harbor</title>
    <updated>2025-02-17T01:39:15Z</updated>
    <id>tag:github.com,2025-02-17:/av/harbor</id>
    <link href="https://github.com/av/harbor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Effortlessly run LLM backends, APIs, frontends, and services with one command.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/av/harbor/raw/main/docs/harbor-2.png&#34; alt=&#34;Harbor project logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/av/harbor/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/tag/av/harbor&#34; alt=&#34;GitHub Tag&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/@avcodes/harbor&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/%40avcodes%2Fharbor?labelColor=red&amp;amp;color=white&#34; alt=&#34;NPM Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/llm-harbor/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/llm-harbor?labelColor=blue&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/repo-size/av/harbor&#34; alt=&#34;GitHub repo size&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/directory-file-count/av/harbor?type=file&amp;amp;extension=yml&amp;amp;label=compose%20files&amp;amp;color=orange&#34; alt=&#34;GitHub repo file or directory count&#34;&gt; &lt;a href=&#34;https://visitorbadge.io/status?path=av%2Fharbor&#34;&gt;&lt;img src=&#34;https://api.visitorbadge.io/api/visitors?path=av%2Fharbor&amp;amp;countColor=%23263759&amp;amp;style=flat&#34; alt=&#34;Visitors&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/languages/count/av/harbor&#34; alt=&#34;GitHub language count&#34;&gt; &lt;a href=&#34;https://discord.gg/8nDRphrhSF&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Harbor-blue?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Effortlessly run LLM backends, APIs, frontends, and services with one command.&lt;/p&gt; &#xA;&lt;p&gt;Harbor is a containerized LLM toolkit that allows you to run LLMs and additional services. It consists of a CLI and a companion App that allows you to manage and run AI services with ease.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/av/harbor/wiki/harbor-app-3.png&#34; alt=&#34;Screenshot of Harbor CLI and App together&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Services&lt;/h2&gt; &#xA;&lt;h5&gt;UIs&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.1-Frontend:-Open-WebUI&#34;&gt;Open WebUI&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.2-Frontend:-ComfyUI&#34;&gt;ComfyUI&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.3-Frontend:-LibreChat&#34;&gt;LibreChat&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.4-Frontend:-ChatUI&#34;&gt;HuggingFace ChatUI&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.5-Frontend:-Lobe-Chat&#34;&gt;Lobe Chat&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.6-Frontend:-hollama&#34;&gt;Hollama&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.7-Frontend:-parllama&#34;&gt;parllama&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.8-Frontend:-BionicGPT&#34;&gt;BionicGPT&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.9-Frontend:-AnythingLLM&#34;&gt;AnythingLLM&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.1.10-Frontend:-Chat-Nio&#34;&gt;Chat Nio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Backends&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.1-Backend:-Ollama&#34;&gt;Ollama&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.2-Backend:-llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.3-Backend:-vLLM&#34;&gt;vLLM&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.4-Backend:-TabbyAPI&#34;&gt;TabbyAPI&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.5-Backend:-Aphrodite-Engine&#34;&gt;Aphrodite Engine&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.6-Backend:-mistral.rs&#34;&gt;mistral.rs&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.7-Backend:-openedai-speech&#34;&gt;openedai-speech&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.14-Backend:-Speaches&#34;&gt;Speaches&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.8-Backend:-Parler&#34;&gt;Parler&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.9-Backend:-text-generation-inference&#34;&gt;text-generation-inference&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.10-Backend:-lmdeploy&#34;&gt;LMDeploy&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.11-Backend:-AirLLM&#34;&gt;AirLLM&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.12-Backend:-SGLang&#34;&gt;SGLang&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.13-Backend:-KTransformers&#34;&gt;KTransformers&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.15-Backend:-Nexa-SDK&#34;&gt;Nexa SDK&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.2.16-Backend:-KoboldCpp&#34;&gt;KoboldCpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Satellites&lt;/h5&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/5.1.-Harbor-Bench&#34;&gt;Harbor Bench&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/5.2.-Harbor-Boost&#34;&gt;Harbor Boost&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.1-Satellite:-SearXNG&#34;&gt;SearXNG&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.2-Satellite:-Perplexica&#34;&gt;Perplexica&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.3-Satellite:-Dify&#34;&gt;Dify&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.4-Satellite:-Plandex&#34;&gt;Plandex&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.5-Satellite:-LiteLLM&#34;&gt;LiteLLM&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.6-Satellite:-langfuse&#34;&gt;LangFuse&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.7-Satellite:-Open-Interpreter&#34;&gt;Open Interpreter&lt;/a&gt; ⦁ ︎&lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.8-Satellite:-cloudflared&#34;&gt;cloudflared&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.9-Satellite:-cmdh&#34;&gt;cmdh&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.10-Satellite:-fabric&#34;&gt;fabric&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.11-Satellite:-txtai-RAG&#34;&gt;txtai RAG&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.12-Satellite:-TextGrad&#34;&gt;TextGrad&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.13-Satellite:-aider&#34;&gt;Aider&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.14-Satellite:-aichat&#34;&gt;aichat&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.16-Satellite:-omnichain&#34;&gt;omnichain&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.17-Satellite:-lm-evaluation-harness&#34;&gt;lm-evaluation-harness&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.18-Satellite:-JupyterLab&#34;&gt;JupyterLab&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.19-Satellite:-ol1&#34;&gt;ol1&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.20-Satellite:-OpenHands&#34;&gt;OpenHands&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.21-Satellite:-LitLytics&#34;&gt;LitLytics&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.22-Satellite:-Repopack&#34;&gt;Repopack&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.23-Satellite:-n8n&#34;&gt;n8n&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.24-Satellite:-Bolt.new&#34;&gt;Bolt.new&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.25-Satellite:-Open-WebUI-Pipelines&#34;&gt;Open WebUI Pipelines&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.26-Satellite:-Qdrant&#34;&gt;Qdrant&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.27-Satellite:-K6&#34;&gt;K6&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.28-Satellite:-Promptfoo&#34;&gt;Promptfoo&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.29-Satellite:-Webtop&#34;&gt;Webtop&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.30-Satellite:-OmniParser&#34;&gt;OmniParser&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.31-Satellite:-Flowise&#34;&gt;Flowise&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.32-Satellite:-LangFlow&#34;&gt;Langflow&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.33-Satellite:-OptiLLM&#34;&gt;OptiLLM&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.34-Satellite-Morphic&#34;&gt;Morphic&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.35-Satellite-SQL-Chat&#34;&gt;SQL Chat&lt;/a&gt; ⦁︎ &lt;a href=&#34;https://github.com/av/harbor/wiki/2.3.36-Satellite-gptme&#34;&gt;gptme&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/av/harbor/wiki/2.-Services&#34;&gt;services documentation&lt;/a&gt; for a brief overview of each.&lt;/p&gt; &#xA;&lt;h2&gt;Blitz Tour&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wiki/av/harbor/harbor-arch-diag.png&#34; alt=&#34;Diagram outlining Harbor&#39;s service structure&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run Harbor with default services:&#xA;# Open WebUI and Ollama&#xA;harbor up&#xA;&#xA;# Run Harbor with additional services&#xA;# Running SearXNG automatically enables Web RAG in Open WebUI&#xA;harbor up searxng&#xA;&#xA;# Speaches includes OpenAI-compatible SST and TTS&#xA;# and connected to Open WebUI out of the box&#xA;harbor up speaches&#xA;&#xA;# Run additional/alternative LLM Inference backends&#xA;# Open Webui is automatically connected to them.&#xA;harbor up llamacpp tgi litellm vllm tabbyapi aphrodite sglang ktransformers&#xA;&#xA;# Run different Frontends&#xA;harbor up librechat chatui bionicgpt hollama&#xA;&#xA;# Get a free quality boost with&#xA;# built-in optimizing proxy&#xA;harbor up boost&#xA;&#xA;# Use FLUX in Open WebUI in one command&#xA;harbor up comfyui&#xA;&#xA;# Use custom models for supported backends&#xA;harbor llamacpp model https://huggingface.co/user/repo/model.gguf&#xA;&#xA;# Access service CLIs without installing them&#xA;# Caches are shared between services where possible&#xA;harbor hf scan-cache&#xA;harbor hf download google/gemma-2-2b-it&#xA;harbor ollama list&#xA;&#xA;# Shortcut to HF Hub to find the models&#xA;harbor hf find gguf gemma-2&#xA;# Use HFDownloader and official HF CLI to download models&#xA;harbor hf dl -m google/gemma-2-2b-it -c 10 -s ./hf&#xA;harbor hf download google/gemma-2-2b-it&#xA;&#xA;# Where possible, cache is shared between the services&#xA;harbor tgi model google/gemma-2-2b-it&#xA;harbor vllm model google/gemma-2-2b-it&#xA;harbor aphrodite model google/gemma-2-2b-it&#xA;harbor tabbyapi model google/gemma-2-2b-it-exl2&#xA;harbor mistralrs model google/gemma-2-2b-it&#xA;harbor opint model google/gemma-2-2b-it&#xA;harbor sglang model google/gemma-2-2b-it&#xA;&#xA;# Convenience tools for docker setup&#xA;harbor logs llamacpp&#xA;harbor exec llamacpp ./scripts/llama-bench --help&#xA;harbor shell vllm&#xA;&#xA;# Tell your shell exactly what you think about it&#xA;harbor opint&#xA;harbor aider&#xA;harbor aichat&#xA;harbor cmdh&#xA;&#xA;# Use fabric to LLM-ify your linux pipes&#xA;cat ./file.md | harbor fabric --pattern extract_extraordinary_claims | grep &#34;LK99&#34;&#xA;&#xA;# Open services from the CLI&#xA;harbor open webui&#xA;harbor open llamacpp&#xA;# Print yourself a QR to quickly open the&#xA;# service on your phone&#xA;harbor qr&#xA;# Feeling adventurous? Expose your Harbor&#xA;# to the internet&#xA;harbor tunnel&#xA;&#xA;# Config management&#xA;harbor config list&#xA;harbor config set webui.host.port 8080&#xA;&#xA;# Create and manage config profiles&#xA;harbor profile save l370b&#xA;harbor profile use default&#xA;&#xA;# Lookup recently used harbor commands&#xA;harbor history&#xA;&#xA;# Eject from Harbor into a standalone Docker Compose setup&#xA;# Will export related services and variables into a standalone file.&#xA;harbor eject searxng llamacpp &amp;gt; docker-compose.harbor.yml&#xA;&#xA;# Run a built-in LLM benchmark with&#xA;# your own tasks&#xA;harbor bench run&#xA;&#xA;# Gimmick/Fun Area&#xA;&#xA;# Argument scrambling, below commands are all the same as above&#xA;# Harbor doesn&#39;t care if it&#39;s &#34;vllm model&#34; or &#34;model vllm&#34;, it&#39;ll&#xA;# figure it out.&#xA;harbor model vllm&#xA;harbor vllm model&#xA;&#xA;harbor config get webui.name&#xA;harbor get config webui_name&#xA;&#xA;harbor tabbyapi shell&#xA;harbor shell tabbyapi&#xA;&#xA;# 50% gimmick, 50% useful&#xA;# Ask harbor about itself&#xA;harbor how to ping ollama container from the webui?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Harbor App Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/a5cd2ef1-3208-400a-8866-7abd85808503&#34;&gt;https://github.com/user-attachments/assets/a5cd2ef1-3208-400a-8866-7abd85808503&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the demo, Harbor App is used to launch a default stack with &lt;a href=&#34;https://raw.githubusercontent.com/av/harbor/main/2.2.1-Backend:-Ollama&#34;&gt;Ollama&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/av/harbor/main/2.1.1-Frontend:-Open-WebUI&#34;&gt;Open WebUI&lt;/a&gt; services. Later, &lt;a href=&#34;https://raw.githubusercontent.com/av/harbor/main/2.3.1-Satellite:-SearXNG&#34;&gt;SearXNG&lt;/a&gt; is also started, and WebUI can connect to it for the Web RAG right out of the box. After that, &lt;a href=&#34;https://raw.githubusercontent.com/av/harbor/main/5.2.-Harbor-Boost&#34;&gt;Harbor Boost&lt;/a&gt; is also started and connected to the WebUI automatically to induce more creative outputs. As a final step, Harbor config is adjusted in the App for the &lt;a href=&#34;https://raw.githubusercontent.com/av/harbor/main/5.2.-Harbor-Boost#klmbr---boost-llm-creativity&#34;&gt;&lt;code&gt;klmbr&lt;/code&gt;&lt;/a&gt; module in the &lt;a href=&#34;https://raw.githubusercontent.com/av/harbor/main/5.2.-Harbor-Boost&#34;&gt;Harbor Boost&lt;/a&gt;, which makes the output unparsable for the LLM (yet still undetstandable for humans).&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/1.0.-Installing-Harbor&#34;&gt;Installing Harbor&lt;/a&gt;&lt;br&gt; Guides to install Harbor CLI and App&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/1.-Harbor-User-Guide&#34;&gt;Harbor User Guide&lt;/a&gt;&lt;br&gt; High-level overview of working with Harbor&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/1.1-Harbor-App&#34;&gt;Harbor App&lt;/a&gt;&lt;br&gt; Overview and manual for the Harbor companion application&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/2.-Services&#34;&gt;Harbor Services&lt;/a&gt;&lt;br&gt; Catalog of services available in Harbor&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/3.-Harbor-CLI-Reference&#34;&gt;Harbor CLI Reference&lt;/a&gt;&lt;br&gt; Read more about Harbor CLI commands and options. Read about supported services and the ways to configure them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/4.-Compatibility&#34;&gt;Compatibility&lt;/a&gt;&lt;br&gt; Known compatibility issues between the services and models as well as possible workarounds.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/5.1.-Harbor-Bench&#34;&gt;Harbor Bench&lt;/a&gt;&lt;br&gt; Documentation for the built-in LLM benchmarking service.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/5.2.-Harbor-Boost&#34;&gt;Harbor Boost&lt;/a&gt;&lt;br&gt; Documentation for the built-in LLM optimiser proxy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/6.-Harbor-Compose-Setup&#34;&gt;Harbor Compose Setup&lt;/a&gt;&lt;br&gt; Read about the way Harbor uses Docker Compose to manage services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/av/harbor/wiki/7.-Adding-A-New-Service&#34;&gt;Adding A New Service&lt;/a&gt;&lt;br&gt; Documentation on bringing more services into the Harbor toolkit.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Convenience factor&lt;/li&gt; &#xA; &lt;li&gt;Workflow/setup centralisation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re comfortable with Docker and Linux administration - you likely don&#39;t need Harbor per se to manage your local LLM environment. However, you&#39;re also likely to eventually arrive to a similar solution. I know this for a fact, since I was rocking pretty much similar setup, just without all the whistles and bells.&lt;/p&gt; &#xA;&lt;p&gt;Harbor is not designed as a deployment solution, but rather as a helper for the local LLM development environment. It&#39;s a good starting point for experimenting with LLMs and related services.&lt;/p&gt; &#xA;&lt;p&gt;You can later eject from Harbor and use the services in your own setup, or continue using Harbor as a base for your own configuration.&lt;/p&gt; &#xA;&lt;h2&gt;Overview and Features&lt;/h2&gt; &#xA;&lt;p&gt;This project consists of a fairly large shell CLI, fairly small &lt;code&gt;.env&lt;/code&gt; file and enormous (for one repo) amount of &lt;code&gt;docker-compose&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;h4&gt;Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manage local LLM stack with a concise CLI&lt;/li&gt; &#xA; &lt;li&gt;Convenience utilities for common tasks (model management, configuration, service debug, URLs, tunnels, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Access service CLIs (&lt;code&gt;hf&lt;/code&gt;, &lt;code&gt;ollama&lt;/code&gt;, etc.) via Docker without install&lt;/li&gt; &#xA; &lt;li&gt;Services are pre-configured to work together (contributions welcome)&lt;/li&gt; &#xA; &lt;li&gt;Host cache is shared and reused - Hugging Face, ollama, etc.&lt;/li&gt; &#xA; &lt;li&gt;Co-located service configs&lt;/li&gt; &#xA; &lt;li&gt;Built-in LLM benchmarking service&lt;/li&gt; &#xA; &lt;li&gt;Manage configuration profiles for different use cases&lt;/li&gt; &#xA; &lt;li&gt;Eject to run without harbor with &lt;code&gt;harbor eject&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>