<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-14T01:35:17Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>MiuLab/Taiwan-LLaMa</title>
    <updated>2023-08-14T01:35:17Z</updated>
    <id>tag:github.com,2023-08-14:/MiuLab/Taiwan-LLaMa</id>
    <link href="https://github.com/MiuLab/Taiwan-LLaMa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Traditional Mandarin LLMs for Taiwan&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Language Models for Taiwanese Culture&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; âœï¸ &lt;a href=&#34;https://huggingface.co/spaces/yentinglin/Taiwan-LLaMa2&#34; target=&#34;_blank&#34;&gt;Online Demo&lt;/a&gt; â€¢ ğŸ¤— &lt;a href=&#34;https://huggingface.co/yentinglin&#34; target=&#34;_blank&#34;&gt;HF Repo&lt;/a&gt; â€¢ ğŸ¦ &lt;a href=&#34;https://twitter.com/yentinglin56&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt; â€¢ ğŸ“ƒ &lt;a href=&#34;https://arxiv.org/pdf/2305.13711.pdf&#34; target=&#34;_blank&#34;&gt;[Paper Coming Soon]&lt;/a&gt; â€¢ ğŸ‘¨ï¸ &lt;a href=&#34;https://yentingl.com/&#34; target=&#34;_blank&#34;&gt;Yen-Ting Lin&lt;/a&gt; &lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://www.csie.ntu.edu.tw/~miulab/taiwan-llama/logo-v2.png&#34; width=&#34;100&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Taiwan-LLaMa is a full parameter fine-tuned model based on LLaMa 2 for Traditional Mandarin applications.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taiwan-LLaMa v1.0&lt;/strong&gt; pretrained on over 5 billion tokens and instruction-tuned on over 490k conversations both in traditional mandarin.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;A live demonstration of the model can be accessed at &lt;a href=&#34;https://huggingface.co/spaces/yentinglin/Taiwan-LLaMa2&#34;&gt;Hugging Face Spaces&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Traditional Mandarin Support&lt;/strong&gt;: The model is fine-tuned to understand and generate text in Traditional Mandarin, making it suitable for Taiwanese culture and related applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Instruction-Tuned&lt;/strong&gt;: Further fine-tuned on conversational data to offer context-aware and instruction-following responses.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance on Vicuna Benchmark&lt;/strong&gt;: Taiwan-LLaMa&#39;s relative performance on Vicuna Benchmark is measured against models like GPT-4 and ChatGPT. It&#39;s particularly optimized for Taiwanese culture.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible Customization&lt;/strong&gt;: Advanced options for controlling the model&#39;s behavior like system prompt, temperature, top-p, and top-k are available in the demo.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Work in progress&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;Improved pretraining&lt;/strong&gt;: A refined pretraining process (e.g. more data from Taiwan, training strategies) is under development, aiming to enhance model performance for better Taiwanese culture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;strong&gt;Extend max length&lt;/strong&gt;: Utilizing the Rope mechanism as described in &lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;the paper&lt;/a&gt;, the model&#39;s length will be extended from 4k to 8k.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Taiwanese Culture Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Question&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;ChatGPT&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Taiwan LLaMa v1.0&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;strong&gt;Taiwan LLaMa v0.0 (w/o pretraining)&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Llama2_Traditional_Chinese_13b_Chat&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä»¥æƒ¡åœ°æ™¯è§€è‘—åçš„æ˜¯é«˜é›„å“ªä¸€å€‹æ™¯é»?&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä»¥æƒ¡åœ°åœ°æ™¯è‘—åçš„æ˜¯é«˜é›„å¸‚çš„æ——æ´¥ã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é«˜é›„ä»¥æ“æœ‰æƒ¡åœ°æ™¯è§€è‘—åçš„æ™¯é»æ˜¯ç”°å¯®æœˆä¸–ç•Œã€‚âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ä»¥æƒ¡åœ°æ™¯è§€è‘—åçš„æ˜¯é«˜é›„å¸‚çš„æ——æ´¥å³¶ã€‚æ——æ´¥å³¶æ˜¯ä¸€å€‹å°å³¶ï¼Œ...&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é«˜é›„å¸‚å…§æœ‰å¤šå€‹ä»¥æƒ¡åœ°æ™¯è§€è‘—åçš„æ™¯é»ï¼Œå…¶ä¸­ä»¥ä¸‹é¢å¹¾å€‹æœ€ç‚ºè‘—åï¼š 1. é«˜é›„æ·é‹ç´…ç·šæ²¿é€”æ™¯é»ï¼šé«˜é›„å¸‚æ”¿åºœç‚ºäº†æ¨å»£é«˜é›„&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å”é³³æ˜¯èª°ï¼Ÿ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å”é³³ï¼ˆAudrey Tangï¼‰æ˜¯å°ç£çš„æ”¿æ²»å®¶ã€ç¤¾æœƒæ´»å‹•å®¶å’Œè»Ÿé«”å·¥ç¨‹å¸«ã€‚å¥¹æ–¼2016å¹´æ“”ä»»å°ç£è¡Œæ”¿é™¢æ•¸ä½æ”¿å‹™å§”å“¡ï¼Œæ˜¯å°ç£æ”¿åºœæ­·å²ä¸Šé¦–ä½æ²’æœ‰æ”¿é»¨èƒŒæ™¯çš„æ”¿å‹™å®˜ã€‚å”é³³åœ¨è»Ÿé«”å·¥ç¨‹é ˜åŸŸæœ‰è±å¯Œçš„ç¶“é©—ã€‚ âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å”é³³æ˜¯æ”¿æ²»è©•è«–å®¶ã€è³‡è¨Šç¤¾æœƒå­¸ç ”ç©¶è€…ï¼Œä¹Ÿæ˜¯å°ç£ç¬¬ä¸€ä½æ“”ä»»æ”¿åºœæ•¸ä½æ”¿å§”çš„äººã€‚ âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å”é³³ï¼ˆTang Fengï¼‰æ˜¯ä¸€ä½è‘—åçš„ä¸­åœ‹æ¼”å“¡å’Œæ­Œæ‰‹ã€‚ä»–å‡ºç”Ÿæ–¼1974å¹´12æœˆ17æ—¥ï¼Œå‡ºç”Ÿåœ°é»åœ¨ä¸­åœ‹åŒ—äº¬ã€‚å”é³³åœ¨1995å¹´é–‹å§‹ä»–çš„æ¼”è—ç”Ÿæ¶¯ï¼Œä¸¦åœ¨åŒå¹´ç™¼è¡Œäº†ä»–çš„é¦–å¼µå°ˆè¼¯ã€Šæˆ‘ä¸å­¤å–®ã€‹ã€‚æ­¤å¾Œï¼Œä»–ç™¼è¡Œäº†è¨±å¤šå—æ­¡è¿çš„æ­Œæ›²ï¼Œä¸¦åœ¨å¤šéƒ¨é›»è¦–åŠ‡å’Œé›»å½±ä¸­æ“”ä»»ä¸»è§’ã€‚å”é³³å› å…¶ç²¾æ¹›çš„æ¼”æŠ€å’Œè¿·äººçš„æ­Œè²è€Œè¢«è­½ç‚ºâ€œäºæ´²é­…åŠ›ç”·ç‹â€ã€‚ä»–çš„éŸ³æ¨‚ç²å¾—äº†å¤šå€‹çé …ï¼ŒåŒ…æ‹¬â€œè¯è¡¨æœ€ä½³æ–°ç”·æ¼”å”±çâ€ã€â€œè¯è¡¨æœ€ä½³æ­Œæ›²çâ€å’Œâ€œè¯è¡¨æœ€ä½³èˆè¹ˆçâ€ç­‰ã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;å”é³³æ˜¯å”æœæ™‚æœŸçš„ä¸€ä½è‘—åæ–‡å­¸å®¶ï¼Œä»–çš„ä»£è¡¨ä½œå“åŒ…æ‹¬ã€Šé™°é¨­ã€‹ã€ã€Šç‹‚å‡¡å­ã€‹ç­‰ã€‚ä»–çš„æ–‡å­¸é¢¨æ ¼å…·æœ‰å¼·çƒˆçš„å€‹æ€§å’Œå‰µæ„ï¼Œè¢«è­½ç‚ºã€Œå”ä»£æ–‡å­¸ä¹‹çˆ¶ã€ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NTU åœ¨å“ª?&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NTUä½æ–¼æ–°åŠ å¡ï¼Œå…·é«”åœ°å€æ˜¯50 Nanyang Avenue, Singapore 639798ã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NTU æ˜¯åœ‹ç«‹è‡ºç£å¤§å­¸çš„ç¸®å¯«ã€‚å®ƒæ˜¯è‡ºç£çš„ä¸€æ‰€é ‚å°–ç ”ç©¶å‹å¤§å­¸ï¼Œä½æ–¼è‡ºåŒ—å¸‚ã€‚ âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NTU æ˜¯å°ç£çš„ä¸€æ‰€å¤§å­¸ï¼Œä½æ–¼å°åŒ—å¸‚ã€‚âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;NTU çš„ç¸½éƒ¨ä½æ–¼å°ç£æ–°åŒ—å¸‚æ¿æ©‹å€ï¼Œä½”åœ°é¢ç©ç´„ç‚º1,000å…¬é ƒã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ç”šéº¼æ˜¯22K?&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#34;22K&#34; å¯èƒ½æŒ‡çš„æ˜¯é‡‘çš„ç´”åº¦ï¼Œå°¤å…¶æ˜¯åœ¨ç å¯¶å’Œé‡‘é£¾æ–¹é¢ã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&#34;22K&#34;æ˜¯æŒ‡å°ç£çš„å¤§å­¸ç•¢æ¥­ç”Ÿçš„èµ·è–ªï¼Œå¤§ç´„æ˜¯æ–°å°å¹£22,000å…ƒã€‚é€™å€‹è©è¢«ç”¨ä¾†å½¢å®¹å¤§å­¸ç•¢æ¥­ç”Ÿçš„èµ·è–ªåä½ï¼Œä¸¦ä¸”åœ¨å°±æ¥­å¸‚å ´ä¸Šå°æ–¼å¤§å­¸ç•¢æ¥­ç”Ÿçš„éœ€æ±‚ä¸è¶³ã€‚âœ…&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22Kæ˜¯æŒ‡æº«åº¦å°ºåº¦ä¸Šçš„22,000Kï¼ˆåƒåº¦ï¼‰ã€‚1Kä»£è¡¨ä¸€åº¦ï¼Œå³æº«åº¦å·®è·ï¼Œæ˜¯æ”æ°æº«æ¨™ï¼ˆæ”æ°åº¦ï¼‰çš„ä¸€ç™¾è¬åˆ†ä¹‹ä¸€ã€‚å› æ­¤ï¼Œ22Kç­‰æ–¼-253.15Â°Cï¼ˆæ”æ°æº«æ¨™ï¼‰æˆ–-423.67Â°Fï¼ˆè¯æ°æº«æ¨™ï¼‰ã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;22Kæ˜¯ä¸€ç¨®ç°¡å¯«ï¼Œå³22,000ã€‚å®ƒæ˜¯ä¸€å€‹æ•¸å­—ï¼Œè¡¨ç¤º22,000å€‹å–®ä½ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è‚‰åœ“è¦ä¸è¦åŠ é¦™èœï¼Ÿ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é€™æ˜¯å€‹å€‹äººå£å‘³çš„å•é¡Œï¼Œæœ‰äº›äººå–œæ­¡åœ¨è‚‰åœ“ä¸­åŠ å…¥é¦™èœï¼Œå› ç‚ºé¦™èœçš„æ¸…é¦™å¯ä»¥å¢æ·»é¢¨å‘³ï¼›è€Œæœ‰äº›äººå‰‡ä¸å–œæ­¡é¦™èœçš„å‘³é“ï¼Œè¦ºå¾—å®ƒæœƒè“‹éå…¶ä»–é£Ÿæçš„å‘³é“ã€‚æ‰€ä»¥è¦ä¸è¦åŠ é¦™èœï¼Œå¯ä»¥æ ¹æ“šå€‹äººå–œå¥½ä¾†æ±ºå®šã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;é€™æ˜¯ä¸€å€‹å€‹äººå–œå¥½çš„å•é¡Œï¼Œæœ‰äº›äººå–œæ­¡åœ¨è‚‰åœ“è£¡åŠ é¦™èœï¼Œæœ‰äº›äººä¸å–œæ­¡ã€‚&lt;strong&gt;åœ¨å°ç£&lt;/strong&gt;ï¼Œè‚‰åœ“åŠ é¦™èœæ˜¯ä¸€ç¨®å¸¸è¦‹çš„åšæ³•ï¼Œé€™æ˜¯ä¸€ç¨®åœ¨è‚‰åœ“ä¸ŠåŠ é¦™èœçš„åšæ³•ã€‚ç„¶è€Œï¼Œåœ¨å…¶ä»–åœ‹é€™æ˜¯ä¸€å€‹æ–‡åŒ–å·®ç•°çš„å•é¡Œã€‚ğŸ˜Š&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;è‚‰åœ“åŠ é¦™èœå¯ä»¥å¢æ·»ä¸€äº›èŠ³é¦™å’Œç¾å‘³çš„å£æ„Ÿï¼Œå¦‚æœä½ å–œæ­¡é¦™èœçš„å‘³é“ï¼Œå¯ä»¥è©¦è©¦åŠ å…¥è‚‰åœ“ä¸­ã€‚é¦™èœé€šå¸¸åœ¨è£½ä½œéç¨‹ä¸­ç‚’é¦™ï¼Œå¯ä»¥å¢æ·»ä¸€äº›èŠ³é¦™å’Œç¾å‘³çš„å‘³é“ã€‚å¦‚æœä½ æƒ³è¦åŠ å…¥é¦™èœï¼Œå¯ä»¥åœ¨è£½ä½œéç¨‹ä¸­ç‚’é¦™ä¸€äº›é¦™èœï¼Œç„¶å¾Œå°‡å…¶æ··å…¥è‚‰åœ“ä¸­ã€‚è‚‰åœ“åŠ é¦™èœå¯ä»¥å¢æ·»ä¸€äº›èŠ³é¦™å’Œç¾å‘³çš„å£æ„Ÿï¼Œå¦‚æœä½ å–œæ­¡é¦™èœçš„å‘³é“ï¼Œå¯ä»¥è©¦è©¦åŠ å…¥è‚‰åœ“ä¸­ã€‚&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;åŠ é¦™èœçš„è‚‰åœ“å‘³é“æ›´å¥½ï¼Œä½†æ˜¯ä¸åŠ é¦™èœçš„è‚‰åœ“ä¹Ÿå¾ˆå¥½åƒã€‚å¦‚æœä½ å–œæ­¡é¦™èœçš„å‘³é“ï¼Œé‚£éº¼åŠ ä¸Šé¦™èœå°±å¥½äº†ã€‚å¦‚æœä½ ä¸å–œæ­¡é¦™èœï¼Œé‚£éº¼ä¸åŠ ä¹Ÿä¸æœƒå½±éŸ¿è‚‰&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;We provide a number of model checkpoints that we trained. Please find them on Hugging Face &lt;a href=&#34;https://huggingface.co/models?search=taiwan-llama&#34;&gt;here&lt;/a&gt;. Here are some quick links to the checkpoints that are finetuned from LLaMa 2:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;13B&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Taiwan-LLaMa v1.0&lt;/strong&gt; (&lt;em&gt;better for Taiwanese Culture&lt;/em&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;ğŸ¤— &lt;a href=&#34;https://huggingface.co/yentinglin/Taiwan-LLaMa-v1.0&#34; target=&#34;_blank&#34;&gt;yentinglin/Taiwan-LLaMa-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Taiwan-LLaMa v0.9 (partial instruction set)&lt;/td&gt; &#xA;   &lt;td&gt;ğŸ¤— &lt;a href=&#34;https://huggingface.co/yentinglin/Taiwan-LLaMa-v0.9&#34; target=&#34;_blank&#34;&gt;yentinglin/Taiwan-LLaMa-v0.9&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Taiwan-LLaMa v0.0 (no Traditional Mandarin pretraining)&lt;/td&gt; &#xA;   &lt;td&gt;ğŸ¤— &lt;a href=&#34;https://huggingface.co/yentinglin/Taiwan-LLaMa-v0.0&#34; target=&#34;_blank&#34;&gt;yentinglin/Taiwan-LLaMa-v0.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;Here are some quick links to the datasets that we used to train the models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Link&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Instruction-tuning&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ğŸ¤— &lt;a href=&#34;https://huggingface.co/datasets/yentinglin/traditional_mandarin_instructions&#34; target=&#34;_blank&#34;&gt;yentinglin/traditional_mandarin_instructions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Traditional Mandarin Pretraining&lt;/td&gt; &#xA;   &lt;td&gt;ğŸ¤— &lt;a href=&#34;https://huggingface.co/datasets/yentinglin/zh_TW_c4&#34; target=&#34;_blank&#34;&gt;yentinglin/zh_TW_c4&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;Taiwan-LLaMa is based on LLaMa 2, leveraging transformer architecture, &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34; target=&#34;_blank&#34;&gt;flash attention 2&lt;/a&gt;, and bfloat16.&lt;/p&gt; &#xA;&lt;p&gt;It includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretraining Phase: Pretrained on a vast corpus of over 5 billion tokens, extracted from common crawl in Traditional Mandarin.&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuning Phase: Further instruction-tuned on over 490k multi-turn conversational data to enable more instruction-following and context-aware responses.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generic Capabilities on Vicuna Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;The data is translated into traditional mandarin for evaluating the general capability.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MiuLab/Taiwan-LLaMa/main/images/zhtw_vicuna_bench_chatgptbaseline.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;p&gt;The scores are calculated with ChatGPT as the baseline, represented as 100%. The other values show the relative performance of different models compared to ChatGPT.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language Model&lt;/th&gt; &#xA;   &lt;th&gt;Relative Score (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td&gt;102.59%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT&lt;/td&gt; &#xA;   &lt;td&gt;100.00%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Taiwan-LLaMa v1.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;76.76%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Claude-Instant-1.2&lt;/td&gt; &#xA;   &lt;td&gt;74.04%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2_Traditional_Chinese_13b_Chat&lt;/td&gt; &#xA;   &lt;td&gt;56.21%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;How to deploy the model on my own machine?&lt;/h2&gt; &#xA;&lt;p&gt;We recommend hosting models with &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;ğŸ¤— Text Generation Inference&lt;/a&gt;. Please see their &lt;a href=&#34;https://github.com/huggingface/text-generation-inference/raw/main/LICENSE&#34;&gt;license&lt;/a&gt; for details on usage and limitations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash run_text_generation_inference.sh &#34;yentinglin/Taiwan-LLaMa&#34; NUM_GPUS DIR_TO_SAVE_MODEL PORT MAX_INPUT_LEN MODEL_MAX_LEN&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Prompt format follows vicuna-v1.1 template:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&#39;s questions. USER: {user} ASSISTANT:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setup development environment&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n taiwan-llama python=3.10 -y &#xA;conda activate taiwan-llama&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you use our code, data, or models in your research, please cite this repository. You can use the following BibTeX entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{lin-chen-2023-llm,&#xA;    title = &#34;{LLM}-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models&#34;,&#xA;    author = &#34;Lin, Yen-Ting  and Chen, Yun-Nung&#34;,&#xA;    booktitle = &#34;Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023)&#34;,&#xA;    month = jul,&#xA;    year = &#34;2023&#34;,&#xA;    address = &#34;Toronto, Canada&#34;,&#xA;    publisher = &#34;Association for Computational Linguistics&#34;,&#xA;    url = &#34;https://aclanthology.org/2023.nlp4convai-1.5&#34;,&#xA;    pages = &#34;47--58&#34;&#xA;}&#xA;&#xA;@misc{taiwanllama,&#xA;    author={Lin, Yen-Ting and Chen, Yun-Nung},&#xA;    title={Language Models for Taiwanese Culture},&#xA;    year={2023},&#xA;    url={https://github.com/adamlin120/Taiwan-LLaMa},&#xA;    note={Code and models available at https://github.com/adamlin120/Taiwan-LLaMa},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Collaborate With Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in contributing to the development of Traditional Mandarin language models, exploring new applications, or leveraging Taiwan-LLaMa for your specific needs, please don&#39;t hesitate to contact us. We welcome collaborations from academia, industry, and individual contributors.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this project is licensed under the Apache 2.0 License - see the &lt;a href=&#34;https://raw.githubusercontent.com/MiuLab/Taiwan-LLaMa/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;p&gt;The models included in this project are licensed under the LLAMA 2 Community License. See the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/LICENSE&#34;&gt;LLAMA2 License&lt;/a&gt; for full details.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI Data Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;The data included in this project were generated using OpenAI&#39;s models and are subject to OpenAI&#39;s Terms of Use. Please review &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;OpenAI&#39;s Terms of Use&lt;/a&gt; for details on usage and limitations.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Meta LLaMA team&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna team&lt;/a&gt; for their open-source efforts in democratizing large language models.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Jack-Cherish/dsi</title>
    <updated>2023-08-14T01:35:17Z</updated>
    <id>tag:github.com,2023-08-14:/Jack-Cherish/dsi</id>
    <link href="https://github.com/Jack-Cherish/dsi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Do Something Interestingç¼©å†™ï¼Œåšä¸€äº›æœ‰è¶£çš„äº‹&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DSI&lt;/h1&gt; &#xA;&lt;p&gt;Do Something Interesting çš„ç¼©å†™ï¼Œåšä¸€äº›æœ‰è¶£çš„äº‹ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/facechain</title>
    <updated>2023-08-14T01:35:17Z</updated>
    <id>tag:github.com,2023-08-14:/modelscope/facechain</id>
    <link href="https://github.com/modelscope/facechain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h1&gt;FaceChain&lt;/h1&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨ç†Ÿæ‚‰ä¸­æ–‡ï¼Œå¯ä»¥é˜…è¯»&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/README_ZH.md&#34;&gt;ä¸­æ–‡ç‰ˆæœ¬çš„README&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin. With a minimum of 1 portrait-photo, you can create a Digital-Twin of your own and start generating personal photos in different settings (work photos as starter!). You may train your Digital-Twin model and generate photos via FaceChain&#39;s Python scripts, or via the familiar Gradio interface. You can also experience FaceChain directly with our &lt;a href=&#34;https://modelscope.cn/studios/CVstudio/cv_human_portrait/summary&#34;&gt;ModelScope Studio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;FaceChain is powered by &lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/example1.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/example2.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/example3.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Compatibility Verification&lt;/h2&gt; &#xA;&lt;p&gt;The following are the environment dependencies that have been verified:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python: py3.8, py3.10&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.0.0, torch2.0.1&lt;/li&gt; &#xA; &lt;li&gt;tensorflow: 2.7.0, tensorflow-cpu&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.7&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;OS: Ubuntu 20.04, CentOS 7.9&lt;/li&gt; &#xA; &lt;li&gt;GPU: Nvidia-A10 24G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resource Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU: About 19G&lt;/li&gt; &#xA; &lt;li&gt;Disk: About 50GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;The following installation methods are supported:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;ModelScope notebookã€recommendedã€‘ The ModelScope notebook has a free tier that allows you to run the FaceChain application, refer to &lt;a href=&#34;https://modelscope.cn/my/mynotebook/preset&#34;&gt;ModelScope Notebook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In addition to ModelScope notebook and ECS, I would suggest that we add that user may also start DSW instance with the option of ModelScope (GPU) image, to create a ready-to-use environment.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1&#xA;æˆ‘çš„notebook -&amp;gt; PAI-DSW -&amp;gt; GPUç¯å¢ƒ&#xA;&#xA;# Step2&#xA;Open the Terminalï¼Œclone FaceChain from github:&#xA;GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git&#xA;&#xA;# Step3&#xA;Entry the Notebook cell:&#xA;import os&#xA;os.chdir(&#39;/mnt/workspace/facechain&#39;)&#xA;print(os.getcwd())&#xA;&#xA;!pip3 install gradio&#xA;!python3 app.py&#xA;&#xA;&#xA;# Step4&#xA;click &#34;public URL&#34; or &#34;local URL&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you are familiar with using docker, we recommend to use this way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1&#xA;Prepare the environment with GPU on local or cloud, we recommend to use Alibaba Cloud ECS, refer to: https://www.aliyun.com/product/ecs&#xA;&#xA;# Step2&#xA;Download the docker image (for installing docker engine, refer to https://docs.docker.com/engine/install/ï¼‰&#xA;docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0&#xA;&#xA;# Step3&#xA;docker images&#xA;docker run -it --name facechain -p 7860:7860 --gpus all your_xxx_image_id /bin/bash&#xA;(Noteï¼š you may need to install the nvidia-container-runtime, refer to https://github.com/NVIDIA/nvidia-container-runtime)&#xA;&#xA;# Step4&#xA;Install the gradio in the docker container:&#xA;pip3 install gradio&#xA;&#xA;# Step5&#xA;GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git&#xA;cd facechain&#xA;python3 app.py&#xA;&#xA;# Step6&#xA;Run the app server: click &#34;public URL&#34; --&amp;gt; in the form of: https://xxx.gradio.live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Script Execution&lt;/h1&gt; &#xA;&lt;p&gt;FaceChain supports direct training and inference in the python environment. Run the following command in the cloned folder to start training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTHONPATH=. sh train_lora.sh &#34;ly261666/cv_portrait_model&#34; &#34;v2.0&#34; &#34;film/film&#34; &#34;./imgs&#34; &#34;./processed&#34; &#34;./output&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Parameter meaning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;ly261666/cv_portrait_model: The stable diffusion base model of the ModelScope model hub, which will be used for training, no need to be changed.&#xA;v2.0: The version number of this base model, no need to be changed&#xA;film/film: This base model may contains multiple subdirectories of different styles, currently we use film/film, no need to be changed&#xA;./imgs: This parameter needs to be replaced with the actual value. It means a local file directory that contains the original photos used for training and generation&#xA;./processed: The folder of the processed images after preprocessing, this parameter needs to be passed the same value in inference, no need to be changed&#xA;./output: The folder where the model weights stored after training, no need to be changed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait for 5-20 minutes to complete the training. Users can also adjust other training hyperparameters. The hyperparameters supported by training can be viewed in the file of &lt;code&gt;train_lora.sh&lt;/code&gt;, or the complete hyperparameter list in &lt;code&gt;facechain/train_text_to_image_lora.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When inferring, please edit the code in run_inference.py:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fill in the folder of the images after preprocessing above, it should be the same as during training&#xA;processed_dir = &#39;./processed&#39;&#xA;# The number of images to generate in inference&#xA;num_generate = 5&#xA;# The stable diffusion base model used in training, no need to be changed&#xA;base_model = &#39;ly261666/cv_portrait_model&#39;&#xA;# The version number of this base model, no need to be changed&#xA;revision = &#39;v2.0&#39;&#xA;# This base model may contains multiple subdirectories of different styles, currently we use film/film, no need to be changed&#xA;base_model_sub_dir = &#39;film/film&#39;&#xA;# The folder where the model weights stored after training, it must be the same as during training&#xA;train_output_dir = &#39;./output&#39;&#xA;# Specify a folder to save the generated images, this parameter can be modified as needed&#xA;output_dir = &#39;./generated&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Algorithm Introduction&lt;/h1&gt; &#xA;&lt;h2&gt;Principle&lt;/h2&gt; &#xA;&lt;p&gt;The ability of the personal portrait model comes from the text generation image function of the Stable Diffusion model. It inputs a piece of text or a series of prompt words and outputs corresponding images. We consider the main factors that affect the generation effect of personal portraits: portrait style information and user character information. For this, we use the style LoRA model trained offline and the face LoRA model trained online to learn the above information. LoRA is a fine-tuning model with fewer trainable parameters. In Stable Diffusion, the information of the input image can be injected into the LoRA model by the way of text generation image training with a small amount of input image. Therefore, the ability of the personal portrait model is divided into training and inference stages. The training stage generates image and text label data for fine-tuning the Stable Diffusion model, and obtains the face LoRA model. The inference stage generates personal portrait images based on the face LoRA model and style LoRA model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/framework_eng.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Input: User-uploaded images that contain clear face areas&lt;/p&gt; &#xA;&lt;p&gt;Output: Face LoRA model&lt;/p&gt; &#xA;&lt;p&gt;Description: First, we process the user-uploaded images using an image rotation model based on orientation judgment and a face refinement rotation method based on face detection and keypoint models, and obtain images containing forward faces. Next, we use a human body parsing model and a human portrait beautification model to obtain high-quality face training images. Afterwards, we use a face attribute model and a text annotation model, combined with tag post-processing methods, to generate fine-grained labels for training images. Finally, we use the above images and label data to fine-tune the Stable Diffusion model to obtain the face LoRA model.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Input: User-uploaded images in the training phase, preset input prompt words for generating personal portraits&lt;/p&gt; &#xA;&lt;p&gt;Output: Personal portrait image&lt;/p&gt; &#xA;&lt;p&gt;Description: First, we fuse the weights of the face LoRA model and style LoRA model into the Stable Diffusion model. Next, we use the text generation image function of the Stable Diffusion model to preliminarily generate personal portrait images based on the preset input prompt words. Then we further improve the face details of the above portrait image using the face fusion model. The template face used for fusion is selected from the training images through the face quality evaluation model. Finally, we use the face recognition model to calculate the similarity between the generated portrait image and the template face, and use this to sort the portrait images, and output the personal portrait image that ranks first as the final output result.&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;The models used in FaceChain:&lt;/p&gt; &#xA;&lt;p&gt;[1] Face detection model DamoFDï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&#34;&gt;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] Image rotating model, offered in the ModelScope studio&lt;/p&gt; &#xA;&lt;p&gt;[3] Human parsing model M2FPï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&#34;&gt;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[4] Skin retouching model ABPNï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet_skin-retouching&#34;&gt;https://modelscope.cn/models/damo/cv_unet_skin-retouching&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[5] Face attribute recognition model FairFaceï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&#34;&gt;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[6] DeepDanbooru modelï¼š&lt;a href=&#34;https://github.com/KichangKim/DeepDanbooru&#34;&gt;https://github.com/KichangKim/DeepDanbooru&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[7] Face quality assessment FQAï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_manual_face-quality-assessment_fqa&#34;&gt;https://modelscope.cn/models/damo/cv_manual_face-quality-assessment_fqa&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[8] Face fusion modelï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet-image-face-fusion_damo&#34;&gt;https://modelscope.cn/models/damo/cv_unet-image-face-fusion_damo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[9] Face recognition model RTSï¼š&lt;a href=&#34;https://modelscope.cn/models/damo/cv_ir_face-recognition-ood_rts&#34;&gt;https://modelscope.cn/models/damo/cv_ir_face-recognition-ood_rts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;More Information&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope/&#34;&gt;ModelScope library&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;â€‹ ModelScope Library provides the foundation for building the model-ecosystem of ModelScope, including the interface and implementation to integrate various models into ModelScope.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88&#34;&gt;Contribute models to ModelScope&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>