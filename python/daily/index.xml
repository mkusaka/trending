<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-21T01:41:22Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pytorch-labs/segment-anything-fast</title>
    <updated>2023-11-21T01:41:22Z</updated>
    <id>tag:github.com,2023-11-21:/pytorch-labs/segment-anything-fast</id>
    <link href="https://github.com/pytorch-labs/segment-anything-fast" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A batched offline inference oriented version of segment-anything&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Segment anything ... Fast&lt;/h1&gt; &#xA;&lt;p&gt;This work is based on a fork of &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;https://github.com/facebookresearch/segment-anything&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The corresponding blog post is &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai/&#34;&gt;https://pytorch.org/blog/accelerating-generative-ai/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Step 1&lt;/p&gt; &#xA;&lt;p&gt;Get latest PyTorch nightly&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Installation instructions vary by platform. Please see the website &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Step 2&lt;/p&gt; &#xA;&lt;p&gt;Install the package&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/pytorch-labs/segment-anything-fast.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The package acts like a drop-in replacement for segment-anything.&lt;/p&gt; &#xA;&lt;p&gt;So, for example, if you&#39;re currently doing &lt;code&gt;from segment_anything import sam_model_registry&lt;/code&gt; you should be able to do &lt;code&gt;from segment_anything_fast import sam_model_registry&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;However, you&#39;re likely here because you want to try a fast, inference version. So we also created a &lt;code&gt;sam_model_fast_registry&lt;/code&gt; that automatically applies&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sets &lt;code&gt;eval&lt;/code&gt; mode&lt;/li&gt; &#xA; &lt;li&gt;Uses &lt;code&gt;bfloat16&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enables torch.compile with max-autotune&lt;/li&gt; &#xA; &lt;li&gt;Uses a custom Triton kernel that implements SDPA for relative positional encodings for long sequence lengths&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The custom Triton kernel in particular was written for A100. If you&#39;re not using an A100, we will try to rerun autotuning on your device and locally save the best configs. You might still run into performance issues, so you can disable the kernel by setting the environment variable &lt;code&gt;SEGMENT_ANYTHING_FAST_USE_FLASH_4=0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please also note that the first time you&#39;re running this model you&#39;ll likely need to wait a bit for it to compile.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to see the details on how to reproduce all results, please see the README in the experiments folder above.&lt;/p&gt; &#xA;&lt;p&gt;Please don&#39;t be shy to open a Github issue if you&#39;re missing functionality or find an issue. Thank you.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;The results show a waterfall of techniques.&lt;/p&gt; &#xA;&lt;p&gt;Left to right these techniques are combined.&lt;/p&gt; &#xA;&lt;p&gt;That means the very last bar is the combination of&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;bfloat16&lt;/li&gt; &#xA; &lt;li&gt;torch.compile with max-autotune&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/docs/main/generated/torch.nn.functional.scaled_dot_product_attention.html&#34;&gt;torch.scaled_dot_product_attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A custom Triton kernel that implements SDPA for relative positional encodings for long sequence lengths&lt;/li&gt; &#xA; &lt;li&gt;NestedTensors&lt;/li&gt; &#xA; &lt;li&gt;Dynamic int8 symmetric quantization&lt;/li&gt; &#xA; &lt;li&gt;2:4 sparse format&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pytorch-labs/segment-anything-fast/main/experiments/bar_chart.svg?sanitize=true&#34; alt=&#34;High level results&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;segment-anything-fast&lt;/code&gt; is released under the &lt;a href=&#34;https://github.com/pytorch-labs/segment-anything-fast/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/spinningup</title>
    <updated>2023-11-21T01:41:22Z</updated>
    <id>tag:github.com,2023-11-21:/openai/spinningup</id>
    <link href="https://github.com/openai/spinningup" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An educational resource to help anyone learn deep reinforcement learning.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Maintenance (expect bug fixes and minor updates)&lt;/p&gt; &#xA;&lt;h1&gt;Welcome to Spinning Up in Deep RL!&lt;/h1&gt; &#xA;&lt;p&gt;This is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).&lt;/p&gt; &#xA;&lt;p&gt;For the unfamiliar: &lt;a href=&#34;https://en.wikipedia.org/wiki/Reinforcement_learning&#34;&gt;reinforcement learning&lt;/a&gt; (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with &lt;a href=&#34;http://ufldl.stanford.edu/tutorial/&#34;&gt;deep learning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This module contains a variety of helpful resources, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a short &lt;a href=&#34;https://spinningup.openai.com/en/latest/spinningup/rl_intro.html&#34;&gt;introduction&lt;/a&gt; to RL terminology, kinds of algorithms, and basic theory,&lt;/li&gt; &#xA; &lt;li&gt;an &lt;a href=&#34;https://spinningup.openai.com/en/latest/spinningup/spinningup.html&#34;&gt;essay&lt;/a&gt; about how to grow into an RL research role,&lt;/li&gt; &#xA; &lt;li&gt;a &lt;a href=&#34;https://spinningup.openai.com/en/latest/spinningup/keypapers.html&#34;&gt;curated list&lt;/a&gt; of important papers organized by topic,&lt;/li&gt; &#xA; &lt;li&gt;a well-documented &lt;a href=&#34;https://github.com/openai/spinningup&#34;&gt;code repo&lt;/a&gt; of short, standalone implementations of key algorithms,&lt;/li&gt; &#xA; &lt;li&gt;and a few &lt;a href=&#34;https://spinningup.openai.com/en/latest/spinningup/exercises.html&#34;&gt;exercises&lt;/a&gt; to serve as warm-ups.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Get started at &lt;a href=&#34;https://spinningup.openai.com&#34;&gt;spinningup.openai.com&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Citing Spinning Up&lt;/h2&gt; &#xA;&lt;p&gt;If you reference or use Spinning Up in your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{SpinningUp2018,&#xA;    author = {Achiam, Joshua},&#xA;    title = {{Spinning Up in Deep Reinforcement Learning}},&#xA;    year = {2018}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>llmware-ai/llmware</title>
    <updated>2023-11-21T01:41:22Z</updated>
    <id>tag:github.com,2023-11-21:/llmware-ai/llmware</id>
    <link href="https://github.com/llmware-ai/llmware" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Providing enterprise-grade LLM-based development framework, tools, and fine-tuned models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llmware&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9_%7C_3.10-blue?color=blue&#34; alt=&#34;Static Badge&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/llmware?color=blue&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;llmware&lt;/code&gt; is a unified, open, extensible framework for LLM-based application patterns including Retrieval Augmented Generation (RAG). This project provides a comprehensive set of tools that anyone can use – from beginner to the most sophisticated AI developer – to rapidly build industrial-grade enterprise LLM-based applications. &lt;em&gt;Key differentiators include: source citation for Q &amp;amp; A scenarios, fact checking, and other guardrails for model hallucination&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;llmware&lt;/code&gt;, our goal is to contribute to and help catalyze an open community around the new combination of open, extensible technologies being assembled to accomplish fact-based generative workflows.&lt;/p&gt; &#xA;&lt;h2&gt;🎯 Key features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;llmware&lt;/code&gt; is an integrated framework comprised of four major components:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;&lt;u&gt;Retrieval&lt;/u&gt;: Assemble fact-sets &lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;A comprehensive set of querying methods: semantic, text, and hybrid retrieval with integrated metadata.&lt;/li&gt; &#xA;  &lt;li&gt;Ranking and filtering strategies to enable semantic search and rapid retrieval of information.&lt;/li&gt; &#xA;  &lt;li&gt;Web scrapers, Wikipedia integration, and Yahoo Finance API integration as additional tools to assemble fact-sets for generation.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;&lt;u&gt;Prompt&lt;/u&gt;: Tools for sophisticated generative scenarios &lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Connect Models:&lt;/strong&gt; Open interface designed to support AI21, Ai Bloks READ-GPT, Anthropic, Cohere, HuggingFace Generative models, llmware BLING and DRAGON models, OpenAI.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Prepare Sources:&lt;/strong&gt; Tools for packaging and tracking a wide range of materials into model context window sizes. Sources include files, websites, audio, AWS Transcribe transcripts, Wikipedia and Yahoo Finance.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Prompt Catalog:&lt;/strong&gt; Dynamically configurable prompts to experiment with multiple models without any change in the code.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Post Processing:&lt;/strong&gt; a full set of metadata and tools for evidence verification, classification of a response, and fact-checking.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Human in the Loop:&lt;/strong&gt; Ability to enable user ratings, feedback, and corrections of AI responses.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Auditability:&lt;/strong&gt; A flexible state mechanism to capture, track, analyze and audit the LLM prompt lifecycle&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;&lt;u&gt;Vector Embeddings&lt;/u&gt;: swappable embedding models and vector databases&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Custom trained sentence transformer embedding models and support for embedding models from Cohere, Google, HuggingFace Embedding models, and OpenAI.&lt;/li&gt; &#xA;  &lt;li&gt;Mix-and-match among multiple options to find the right solution for any particular application.&lt;/li&gt; &#xA;  &lt;li&gt;Out-of-the-box support for 3 vector databases - Milvus, FAISS, and Pinecone.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;&lt;u&gt;Parsing and Text Chunking&lt;/u&gt;: Prepare your data for RAG&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Parsers for: PDF, PowerPoint, Word, Excel, HTML, Text, WAV, AWS Transcribe transcripts.&lt;/li&gt; &#xA;  &lt;li&gt;A complete set of text-chunking tools to separate information and associated metadata to a consistent block format.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;📚 Explore &lt;a href=&#34;https://github.com/llmware-ai/llmware/raw/main/examples/README.md&#34;&gt;additional llmware capabilities&lt;/a&gt; and 🎬 Check out these videos on how to quickly get started with RAG:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0naqpH93eEU&#34;&gt;Fast Start to RAG with LLMWare Open Source Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tAGz6yR14lw&#34;&gt;Use Retrieval Augmented Generation (RAG) without a Database&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8aV5p3tErP0&#34;&gt;Use small LLMs for RAG for Contract Analysis (feat. LLMWare)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JjgqOZ2v5oU&#34;&gt;RAG using CPU-based (No-GPU required) Hugging Face Models with LLMWare on your laptop&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🌱 Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install llmware:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install llmware&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m pip install llmware&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/llmware-ai/llmware/main/#%EF%B8%8F-working-with-the-llmware-github-repository&#34;&gt;Working with llmware&lt;/a&gt; for other options to get up and running.&lt;/p&gt; &#xA;&lt;h3&gt;2. MongoDB and Milvus&lt;/h3&gt; &#xA;&lt;p&gt;MongoDB and Milvus are optional and used to provide production-grade database and vector embedding capabilities. The fastest way to get started is to use the provided Docker Compose file which takes care of running them both:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -o docker-compose.yaml https://raw.githubusercontent.com/llmware-ai/llmware/main/docker-compose.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and then run the containers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Not ready to install MongoDB or Milvus? Check out what you can do without them in our &lt;a href=&#34;https://github.com/llmware-ai/llmware/raw/main/examples/README.md#using-llmware-without-mongodb-or-an-embedding-database&#34;&gt;examples section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/llmware-ai/llmware/main/#%EF%B8%8F-alternate-options-for-running-mongodb-and-milvus&#34;&gt;Running MongoDB and Milvus&lt;/a&gt; for other options to get up and running with these optional dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;3. 🔥 Start coding - Quick Start For RAG 🔥&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# This example demonstrates Retrieval Augmented Retrieval (RAG):&#xA;import os&#xA;from llmware.library import Library&#xA;from llmware.retrieval import Query&#xA;from llmware.prompts import Prompt&#xA;from llmware.setup import Setup&#xA;&#xA;# Update this value with your own API Key, either by setting the env var or editing it directly here:&#xA;openai_api_key = os.environ[&#34;OPENAI_API_KEY&#34;]&#xA;&#xA;# A self-contained end-to-end example of RAG&#xA;def end_to_end_rag():&#xA;    &#xA;    # Create a library called &#34;Agreements&#34;, and load it with llmware sample files&#xA;    print (f&#34;\n &amp;gt; Creating library &#39;Agreements&#39;...&#34;)&#xA;    library = Library().create_new_library(&#34;Agreements&#34;)&#xA;    sample_files_path = Setup().load_sample_files()&#xA;    library.add_files(os.path.join(sample_files_path,&#34;Agreements&#34;))&#xA;&#xA;    # Create vector embeddings for the library using the &#34;industry-bert-contracts model and store them in Milvus&#xA;    print (f&#34;\n &amp;gt; Generating vector embeddings using embedding model: &#39;industry-bert-contracts&#39;...&#34;)&#xA;    library.install_new_embedding(embedding_model_name=&#34;industry-bert-contracts&#34;, vector_db=&#34;milvus&#34;)&#xA;&#xA;    # Perform a semantic search against our library.  This will gather evidence to be used in the LLM prompt&#xA;    print (f&#34;\n &amp;gt; Performing a semantic query...&#34;)&#xA;    os.environ[&#34;TOKENIZERS_PARALLELISM&#34;] = &#34;false&#34; # Avoid a HuggingFace tokenizer warning&#xA;    query_results = Query(library).semantic_query(&#34;Termination&#34;, result_count=20)&#xA;&#xA;    # Create a new prompter using the GPT-4 and add the query_results captured above&#xA;    prompt_text = &#34;Summarize the termination provisions&#34;&#xA;    print (f&#34;\n &amp;gt; Prompting LLM with &#39;{prompt_text}&#39;&#34;)&#xA;    prompter = Prompt().load_model(&#34;gpt-4&#34;, api_key=openai_api_key)&#xA;    sources = prompter.add_source_query_results(query_results)&#xA;&#xA;    # Prompt the LLM with the sources and a query string&#xA;    responses = prompter.prompt_with_source(prompt_text, prompt_name=&#34;summarize_with_bullets&#34;)&#xA;    for response in responses:&#xA;        print (&#34;\n &amp;gt; LLM response\n&#34; + response[&#34;llm_response&#34;])&#xA;    &#xA;    # Finally, generate a CSV report that can be shared&#xA;    print (f&#34;\n &amp;gt; Generating CSV report...&#34;)&#xA;    report_data = prompter.send_to_human_for_review()&#xA;    print (&#34;File: &#34; + report_data[&#34;report_fp&#34;] + &#34;\n&#34;)&#xA;&#xA;end_to_end_rag()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Response from end-to-end RAG example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; python examples/rag_with_openai.py&#xA;&#xA; &amp;gt; Creating library &#39;Agreements&#39;...&#xA;&#xA; &amp;gt; Generating vector embeddings using embedding model: &#39;industry-bert-contracts&#39;...&#xA;&#xA; &amp;gt; Performing a semantic query...&#xA;&#xA; &amp;gt; Prompting LLM with &#39;Summarize the termination provisions&#39;&#xA;&#xA; &amp;gt; LLM response&#xA;- Employment period ends on the first occurrence of either the 6th anniversary of the effective date or a company sale.&#xA;- Early termination possible as outlined in sections 3.1 through 3.4.&#xA;- Employer can terminate executive&#39;s employment under section 3.1 anytime without cause, with at least 30 days&#39; prior written notice.&#xA;- If notice is given, the executive is allowed to seek other employment during the notice period.&#xA;&#xA; &amp;gt; Generating CSV report...&#xA;File: /Users/llmware/llmware_data/prompt_history/interaction_report_Fri Sep 29 12:07:42 2023.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;📚 See 20+ &lt;a href=&#34;https://github.com/llmware-ai/llmware/raw/main/examples/README.md&#34;&gt;llmware examples&lt;/a&gt; for more RAG examples and other code samples and ideas.&lt;/h4&gt; &#xA;&lt;h3&gt;4. Accessing LLMs and setting-up API keys &amp;amp; secrets&lt;/h3&gt; &#xA;&lt;p&gt;To get started with a proprietary model, you need to provide your own API Keys. If you don&#39;t yet have one, more information can be found at: &lt;a href=&#34;https://docs.ai21.com/docs/quickstart&#34;&gt;AI21&lt;/a&gt;, &lt;a href=&#34;https://www.aibloks.com/contact-us&#34;&gt;Ai Bloks&lt;/a&gt;, &lt;a href=&#34;https://docs.anthropic.com/claude/reference/getting-started-with-the-api&#34;&gt;Anthropic&lt;/a&gt;, &lt;a href=&#34;https://cohere.com/&#34;&gt;Cohere&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart&#34;&gt;Google&lt;/a&gt;, &lt;a href=&#34;https://help.openai.com/en/collections/3675940-getting-started-with-openai-api&#34;&gt;OpenAI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;API keys and secrets for models, aws, and pinecone can be set-up for use in environment variables or managed however you prefer.&lt;/p&gt; &#xA;&lt;p&gt;You can also access the &lt;code&gt;llmware&lt;/code&gt; public model repository which includes out-of-the-box custom trained sentence transformer embedding models fine-tuned for the following industries: Insurance, Contracts, Asset Management, SEC. These domain specific models along with llmware&#39;s generative BLING model series (&#34;Best Little Instruction-following No-GPU-required&#34;) and DRAGON model series (&#34;Delivering RAG on ...&#34;) are available at &lt;a href=&#34;https://huggingface.co/llmware&#34;&gt;llmware on Huggingface&lt;/a&gt;. Explore using the model repository and the &lt;code&gt;llmware&lt;/code&gt; Huggingface integration in &lt;a href=&#34;https://github.com/llmware-ai/llmware/raw/main/examples/README.md&#34;&gt;llmware examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🔹 Alternate options for running MongoDB and Milvus&lt;/h2&gt; &#xA;&lt;p&gt;There are several options for getting MongoDB running&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;🐳 A. Run mongo container with docker &lt;/b&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -d -p 27017:27017  -v mongodb-volume:/data/db --name=mongodb mongo:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;🐳 B. Run container with docker compose &lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Create a &lt;em&gt;docker-compose.yaml&lt;/em&gt; file with the content:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: &#34;3&#34;&#xA;&#xA;services:&#xA;  mongodb:&#xA;    container_name: mongodb&#xA;    image: &#39;mongo:latest&#39;&#xA;    volumes:&#xA;      - mongodb-volume:/data/db&#xA;    ports:&#xA;      - &#39;27017:27017&#39;&#xA;&#xA;volumes:&#xA;    llmware-mongodb:&#xA;      driver: local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;and then run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;📖 C. Install MongoDB natively &lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;See the &lt;a href=&#34;https://www.mongodb.com/docs/manual/installation/&#34;&gt;Official MongoDB Installation Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;🔗 D. Connect to an existing MongoDB deployment &lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;You can connect to an existing MongoDB deployment by setting the connection string to the environment variable, &lt;code&gt;COLLECTION_DB_URI&lt;/code&gt;. See the example script, &lt;a href=&#34;https://github.com/llmware-ai/llmware/raw/main/examples/using_mongo_atlas.py&#34;&gt;Using Mongo Atlas&lt;/a&gt;, for detailed information on how to use Mongo Atlas as the NoSQL and/or Vector Database for &lt;code&gt;llmware&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Additional information on finding and formatting connection strings can be found in the &lt;a href=&#34;https://www.mongodb.com/docs/manual/reference/connection-string/&#34;&gt;MongoDB Connection Strings Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;✍️ Working with the llmware Github repository&lt;/h2&gt; &#xA;&lt;p&gt;The llmware repo can be pulled locally to get access to all the examples, or to work directly with the latest version of the llmware code.&lt;/p&gt; &#xA;&lt;h3&gt;Pull the repo locally&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:llmware-ai/llmware.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or download/extract a &lt;a href=&#34;https://github.com/llmware-ai/llmware/archive/refs/heads/main.zip&#34;&gt;zip of the llmware repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run llmware natively&lt;/h3&gt; &#xA;&lt;p&gt;Update the local copy of the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git pull&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the shared llmware native libraries and dependencies by running the load_native_libraries.sh script. This pulls the right wheel for your platform and extracts the llmware native libraries and dependencies into the proper place in the local repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/dev/load_native_libraries.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At the top level of the llmware repository run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;✨ Getting help or sharing your ideas with the community&lt;/h2&gt; &#xA;&lt;p&gt;Questions and discussions are welcome in our &lt;a href=&#34;https://github.com/llmware-ai/llmware/discussions&#34;&gt;github discussions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Interested in contributing to llmware? We welcome involvement from the community to extend and enhance the framework!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;💡 What&#39;s your favorite model or is there one you&#39;d like to check out in your experiments?&lt;/li&gt; &#xA; &lt;li&gt;💡 Have you had success with a different embedding databases?&lt;/li&gt; &#xA; &lt;li&gt;💡 Is there a prompt that shines in a RAG workflow?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Information on ways to participate can be found in our &lt;a href=&#34;https://github.com/llmware-ai/llmware/raw/main/CONTRIBUTING.md#contributing-to-llmware&#34;&gt;Contributors Guide&lt;/a&gt;. As with all aspects of this project, contributing is governed by our &lt;a href=&#34;https://github.com/llmware-ai/llmware/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📣 Release notes and Change Log&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Operating Systems:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS&lt;/li&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;(Windows is a roadmap item)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Vector Databases:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Milvus&lt;/li&gt; &#xA; &lt;li&gt;FAISS&lt;/li&gt; &#xA; &lt;li&gt;Pinecone&lt;/li&gt; &#xA; &lt;li&gt;MongoDB Atlas Vector Search&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prereqs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All Platforms: &lt;a href=&#34;https://www.python.org/about/gettingstarted/&#34;&gt;Python v3.9 - 3.10&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;To enable the OCR parsing capabilities, install &lt;a href=&#34;https://tesseract-ocr.github.io/tessdoc/Installation.html&#34;&gt;Tesseract v5.3.3&lt;/a&gt; and &lt;a href=&#34;https://poppler.freedesktop.org/&#34;&gt;Poppler v23.10.0&lt;/a&gt; native packages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Known issues:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A segmentation fault can occur when parsing if the native package for mongo-c-driver is 1.25 or above. To address this issue, install llmware v0.1.6 and above or downgrade mongo-c-driver to v1.24.4.&lt;/li&gt; &#xA; &lt;li&gt;The llmware parsers optimize for speed by using large stack frames. If you receive a &#34;Segmentation Fault&#34; during a parsing operation, increase the system&#39;s &#39;stack size&#39; resource limit: &lt;code&gt;ulimit -s 160000&lt;/code&gt;. If running in a linux container on Mac, we&#39;ve found this has to be set signficantly higher and must be set by the host with a command like the following: &lt;code&gt;docker run --ulimit stack=32768000:32768000 ...&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;For llmware versions &amp;lt;= v0.1.6, the pip package attempts to install the native dependencies. If it is run without root permission or a package manager other than Apt is used, you will need to manually install the following native packages: &lt;code&gt;apt install -y libxml2 libpng-dev libmongoc-dev libzip4 tesseract-ocr poppler-utils&lt;/code&gt; *Note: libmongoc-dev &amp;lt;= v1.24.4 is required.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;🚧 Change Log&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;17 Nov 2023: llmwre v0.1.8&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Enhanced generation performance by allowing each model to specific the trailing space parameter.&lt;/li&gt; &#xA;    &lt;li&gt;Improved handling for eos_token_id for llama2 and mistral.&lt;/li&gt; &#xA;    &lt;li&gt;Improved support for Hugging Face dynamic loading&lt;/li&gt; &#xA;    &lt;li&gt;New examples with the new llmware DRAGON models.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;14 Nov 2023: llmware v0.1.7&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Moved to Python Wheel package format for PyPi distribution to provide seamless installation of native dependencies on all supported platforms.&lt;/li&gt; &#xA;    &lt;li&gt;ModelCatalog enhancements: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;OpenAI update to include newly announced ‘turbo’ 4 and 3.5 models.&lt;/li&gt; &#xA;      &lt;li&gt;Cohere embedding v3 update to include new Cohere embedding models.&lt;/li&gt; &#xA;      &lt;li&gt;BLING models as out-of-the-box registered options in the catalog. They can be instantiated like any other model, even without the “hf=True” flag.&lt;/li&gt; &#xA;      &lt;li&gt;Ability to register new model names, within existing model classes, with the register method in ModelCatalog.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Prompt enhancements: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;“evidence_metadata” added to prompt_main output dictionaries allowing prompt_main responses to be plug into the evidence and fact-checking steps without modification.&lt;/li&gt; &#xA;      &lt;li&gt;API key can now be passed directly in a prompt.load_model(model_name, api_key = “[my-api-key]”)&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;LLMWareInference Server - Initial delivery: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;New Class for LLMWareModel which is a wrapper on a custom HF-style API-based model.&lt;/li&gt; &#xA;      &lt;li&gt;LLMWareInferenceServer is a new class that can be instantiated on a remote (GPU) server to create a testing API-server that can be integrated into any Prompt workflow.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;03 Nov 2023: llmware v0.1.6&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Updated packaging to require mongo-c-driver 1.24.4 to temporarily workaround segmentation fault with mongo-c-driver 1.25.&lt;/li&gt; &#xA;    &lt;li&gt;Updates in python code needed in anticipation of future Windows support.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;27 Oct 2023: llmware v0.1.5&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Four new example scripts focused on RAG workflows with small, fine-tuned instruct models that run on a laptop (&lt;code&gt;llmware&lt;/code&gt; &lt;a href=&#34;https://huggingface.co/llmware&#34;&gt;BLING&lt;/a&gt; models).&lt;/li&gt; &#xA;    &lt;li&gt;Expanded options for setting temperature inside a prompt class.&lt;/li&gt; &#xA;    &lt;li&gt;Improvement in post processing of Hugging Face model generation.&lt;/li&gt; &#xA;    &lt;li&gt;Streamlined loading of Hugging Face generative models into prompts.&lt;/li&gt; &#xA;    &lt;li&gt;Initial delivery of a central status class: read/write of embedding status with a consistent interface for callers.&lt;/li&gt; &#xA;    &lt;li&gt;Enhanced in-memory dictionary search support for multi-key queries.&lt;/li&gt; &#xA;    &lt;li&gt;Removed trailing space in human-bot wrapping to improve generation quality in some fine-tuned models.&lt;/li&gt; &#xA;    &lt;li&gt;Minor defect fixes, updated test scripts, and version update for Werkzeug to address &lt;a href=&#34;https://github.com/llmware-ai/llmware/security/dependabot/2&#34;&gt;dependency security alert&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;20 Oct 2023: llmware v0.1.4&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;GPU support for Hugging Face models.&lt;/li&gt; &#xA;    &lt;li&gt;Defect fixes and additional test scripts.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;13 Oct 2023: llmware v0.1.3&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;MongoDB Atlas Vector Search support.&lt;/li&gt; &#xA;    &lt;li&gt;Support for authentication using a MongoDB connection string.&lt;/li&gt; &#xA;    &lt;li&gt;Document summarization methods.&lt;/li&gt; &#xA;    &lt;li&gt;Improvements in capturing the model context window automatically and passing changes in the expected output length.&lt;/li&gt; &#xA;    &lt;li&gt;Dataset card and description with lookup by name.&lt;/li&gt; &#xA;    &lt;li&gt;Processing time added to model inference usage dictionary.&lt;/li&gt; &#xA;    &lt;li&gt;Additional test scripts, examples, and defect fixes.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;06 Oct 2023: llmware v0.1.1&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Added test scripts to the github repository for regression testing.&lt;/li&gt; &#xA;    &lt;li&gt;Minor defect fixes and version update of Pillow to address &lt;a href=&#34;https://github.com/llmware-ai/llmware/security/dependabot/1&#34;&gt;dependency security alert&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;02 Oct 2023: llmware v0.1.0&lt;/strong&gt; 🔥 Initial release of llmware to open source!! 🔥&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
</feed>