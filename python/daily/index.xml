<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-24T01:40:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>s0md3v/sd-webui-roop</title>
    <updated>2023-08-24T01:40:56Z</updated>
    <id>tag:github.com,2023-08-24:/s0md3v/sd-webui-roop</id>
    <link href="https://github.com/s0md3v/sd-webui-roop" rel="alternate"></link>
    <summary type="html">&lt;p&gt;roop extension for StableDiffusion web-ui&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;roop for StableDiffusion&lt;/h1&gt; &#xA;&lt;p&gt;This is an extension for StableDiffusion&#39;s &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/&#34;&gt;AUTOMATIC1111 web-ui&lt;/a&gt; that allows face-replacement in images. It is based on &lt;a href=&#34;https://github.com/s0md3v/roop&#34;&gt;roop&lt;/a&gt; but will be developed seperately.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/s0md3v/sd-webui-roop/main/example/example.png&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc.&lt;/p&gt; &#xA;&lt;p&gt;The developers of this software are aware of its possible unethical applicaitons and are committed to take preventative measures against them. It has a built-in check which prevents the program from working on inappropriate media. We will continue to develop this project in the positive direction while adhering to law and ethics. This project may be shut down or include watermarks on the output if requested by law.&lt;/p&gt; &#xA;&lt;p&gt;Users of this software are expected to use this software responsibly while abiding the local law. If face of a real person is being used, users are suggested to get consent from the concerned person and clearly mention that it is a deepfake when posting content online. Developers of this software will not be responsible for actions of end-users.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First of all, if you can&#39;t install it for some reason, don&#39;t open an issue here. Google your errors.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;On Windows, download and install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio&lt;/a&gt;. During the install, make sure to include the Python and C++ packages.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run this command: &lt;code&gt;pip install insightface==0.7.3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;In web-ui, go to the &#34;Extensions&#34; tab and use this URL &lt;code&gt;https://github.com/s0md3v/sd-webui-roop&lt;/code&gt; in the &#34;install from URL&#34; tab.&lt;/li&gt; &#xA; &lt;li&gt;Close webui and run it again&lt;/li&gt; &#xA; &lt;li&gt;If you encounter &lt;code&gt;&#39;NoneType&#39; object has no attribute &#39;get&#39;&lt;/code&gt; error, download the &lt;a href=&#34;https://huggingface.co/henryruhs/roop/resolve/main/inswapper_128.onnx&#34;&gt;inswapper_128.onnx&lt;/a&gt; model and put it inside &lt;code&gt;&amp;lt;webui_dir&amp;gt;/models/roop/&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For rest of the errors, use google. Good luck.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Under &#34;roop&#34; drop-down menu, import an image containing a face.&lt;/li&gt; &#xA; &lt;li&gt;Turn on the &#34;Enable&#34; checkbox&lt;/li&gt; &#xA; &lt;li&gt;That&#39;s it, now the generated result will have the face you selected&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;h4&gt;Getting good quality results&lt;/h4&gt; &#xA;&lt;p&gt;First of all, make sure the &#34;Restore Face&#34; option is enabled. You can also try the &#34;Upscaler&#34; option or for more finer control, use an upscaler from the &#34;Extras&#34; tab.&lt;/p&gt; &#xA;&lt;p&gt;For even better quality, use img2img with denoise set to &lt;code&gt;0.1&lt;/code&gt; and gradually increase it until you get a balance of quality and resembelance.&lt;/p&gt; &#xA;&lt;h4&gt;Replacing specific faces&lt;/h4&gt; &#xA;&lt;p&gt;If there are multiple faces in an image, select the face numbers you wish to swap using the &#34;Comma separated face number(s)&#34; option.&lt;/p&gt; &#xA;&lt;h4&gt;The face didn&#39;t get swapped?&lt;/h4&gt; &#xA;&lt;p&gt;Did you click &#34;Enable&#34;?&lt;/p&gt; &#xA;&lt;p&gt;If you did and your console doesn&#39;t show any errors, it means roop detected that your image is either NSFW or wasn&#39;t able to detect a face at all.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DLLXW/baby-llama2-chinese</title>
    <updated>2023-08-24T01:40:56Z</updated>
    <id>tag:github.com,2023-08-24:/DLLXW/baby-llama2-chinese</id>
    <link href="https://github.com/DLLXW/baby-llama2-chinese" rel="alternate"></link>
    <summary type="html">&lt;p&gt;用于从头预训练+SFT一个小参数量的中文LLaMa2的仓库；24G单卡即可运行得到一个具备简单中文问答能力的chat-llama2.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;baby-llama2-chinese&lt;/h2&gt; &#xA;&lt;p&gt;用于从头预训练+SFT一个小参数量的中文LLaMa2的仓库；24G单卡即可运行得到一个流畅中文问答的chat-llama2.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;20230818更新，因为第一版（50M参数）的版本，当时很多评测样例其实出现在了SFT数据中，所以让我误以为模型具备很流畅的问答能力，但是后面发现效果并没有那么好。后面使用了更多的数据和更大的模型，效果逐步提升。所以大家如果有充足的算力和时间，可以逐步尝试加大模型，将参数量扩到百M以上，其实消费级显卡也是完全可以接受的。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;训练数据&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wiki中文百科（25w词条）&lt;a href=&#34;https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered&#34;&gt;wikipedia-cn-20230720-filtered&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;BaiduBaiKe（563w词条） &lt;a href=&#34;https://pan.baidu.com/s/1jIpCHnWLTNYabftavo3DVw?pwd=bwvb&#34;&gt;百度网盘&lt;/a&gt; 提取码: bwvb&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/shibing624/medical/tree/main&#34;&gt;Medical Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;除此之外，为了让模型具备在某一个专有领域的能力，这里选用了“医疗问答”作为切入点，尝试收集了很多的医疗数据和上面的通用语料一起喂给模型。&lt;/p&gt; &#xA;&lt;h2&gt;中文分词器&lt;/h2&gt; &#xA;&lt;p&gt;因为在llama官方所提供的词表中，中文的部分只有700个，这也是llama中文能力聊胜于无的原因。为了训练自己的中文LLaMa，这里将引入新的中文分词器。为了方便，这里选择采用ChatGLM2的分词器，词表大小64793，这是一个很妙的数字，因为它刚好在uint16的表示范围（0～65535的无符号整数），每一个token只需要两个字节即可表示，当我们的语料较大时候，相比常用的int32可以节省一半的存储空间。&lt;/p&gt; &#xA;&lt;h2&gt;预训练语料预处理&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#脚本里面每一个函数对应一个语料库的预处理，搭建新加语料可以自行扩展。&#xA;python data_process.py&#xA;#运行结束后，会在./data目录下产生.bin文件&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;数据预处理采取GPT的通用做法，对语料进行提前分词，对一个样本做完分词后在末尾加上一个结束符号，与下一个样本区分开。然后将所有的训练语料拼接成一个数组（np.uint16）以.bin二进制格式存储到磁盘上。如果语料过大，避免内存溢出，可以选择mmap格式。&lt;/p&gt; &#xA;&lt;h2&gt;SFT样本构建&lt;/h2&gt; &#xA;&lt;p&gt;中文SFT语料最近陆陆续续开源了很多（&lt;a href=&#34;https://huggingface.co/datasets/BelleGroup/train_1M_CN&#34;&gt;bell&lt;/a&gt;、&lt;a href=&#34;https://github.com/OpenLMLab/MOSS/tree/main/SFT_data&#34;&gt;MOSS&lt;/a&gt;、&lt;a href=&#34;https://huggingface.co/datasets/shibing624/alpaca-zh&#34;&gt;alpaca-zh&lt;/a&gt;等），但是坦白讲，质量都不高，大家可自行下载并需要进行清洗，清洗SFT数据是个耗时耗力的工作，但根据作者微调经验，一份高质量的SFT数据是相当重要的‼️（如果不清洗SFT数据，可能无法获得满意的SFT效果，建议大家在这块多花些时间） 中文SFT语料网上最近很多，大家自行下载。因为SFT语料一般较小，我们没必要提前分词，而是在构建Dataloader的时候进行分词构建batch送给模型。所以自行参考dataset_sft.py即可！&lt;/p&gt; &#xA;&lt;p&gt;基本逻辑如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;prompt和answer之间一定要有一个开始符隔开，然后answer后需要一个结束符。&lt;/li&gt; &#xA; &lt;li&gt;计算loss的时候，对prompt部分的loss进行mask，只计算answer部分的loss即可。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;预训练+SFT&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#预训练&#xA;python pretrain.py&#xA;#SFT&#xA;python sft.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;根据自己算力的情况合理的调节以下参数，控制模型的计算量和参数量，这是第一版使用的参数&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;max_seq_len = 512&lt;/li&gt; &#xA; &lt;li&gt;dim = 512&lt;/li&gt; &#xA; &lt;li&gt;n_layers = 8&lt;/li&gt; &#xA; &lt;li&gt;n_heads = 8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;推理脚本可以参考eval.py，这里使用100条比赛数据做了bleu的验证，大家感兴趣可以自行修改，后续作者也会不断完善代码。&lt;/p&gt; &#xA;&lt;h2&gt;训练效果评测&lt;/h2&gt; &#xA;&lt;p&gt;作者目前用了20亿中文token，单卡3090训练了一个参数量大概50M的极小的baby-llama2。经过SFT后可以具备一定的中文问答效果，特别是在医疗问答上，因为加了大量相关预训练语料，效果不错。但是缺乏全面严谨的开放问答评测指标，后续有时间会补上，也欢迎大家提pr，平时工作繁忙，只能周末更新，后续有时间了会持续更新语料，迭代模型。&lt;/p&gt; &#xA;&lt;h2&gt;号召&lt;/h2&gt; &#xA;&lt;p&gt;平时工作繁忙，只能周末玩耍，欢迎大家一起共建这个小项目，这对于希望入门LLM的同学来说，是一次不可多得的练手机会，除了大规模预训练需要的（数据并行、模型并行、流水线并行）那一套，其余的LLM基本技术栈基本都有涵盖！&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;参考llama2.c&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>rese1f/StableVideo</title>
    <updated>2023-08-24T01:40:56Z</updated>
    <id>tag:github.com,2023-08-24:/rese1f/StableVideo</id>
    <link href="https://github.com/rese1f/StableVideo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICCV 2023] StableVideo: Text-driven Consistency-aware Diffusion Video Editing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StableVideo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.09592&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/cs.CV-arXiv%3A2308.09592-B31B1B.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;StableVideo: Text-driven Consistency-aware Diffusion Video Editing&lt;br&gt; &lt;a href=&#34;https://rese1f.github.io/&#34;&gt;Wenhao Chai&lt;/a&gt;, Xun Guo, Gaoang Wang, Yan Lu&lt;br&gt; ICCV 2023&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rese1f/StableVideo/assets/58205475/558555f1-711c-46f0-85bc-9c229ff1f511&#34;&gt;https://github.com/rese1f/StableVideo/assets/58205475/558555f1-711c-46f0-85bc-9c229ff1f511&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rese1f/StableVideo/assets/58205475/c152d0fa-16d3-4528-b9c2-ad2ec53944b9&#34;&gt;https://github.com/rese1f/StableVideo/assets/58205475/c152d0fa-16d3-4528-b9c2-ad2ec53944b9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/rese1f/StableVideo/assets/58205475/0edbefdd-9b5f-4868-842c-9bf3156a54d3&#34;&gt;https://github.com/rese1f/StableVideo/assets/58205475/0edbefdd-9b5f-4868-842c-9bf3156a54d3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VRAM requirement&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;VRAM (MiB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;float32&lt;/td&gt; &#xA;   &lt;td&gt;29145&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;amp&lt;/td&gt; &#xA;   &lt;td&gt;23005&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;amp + cpu&lt;/td&gt; &#xA;   &lt;td&gt;17639&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;amp + cpu + xformers&lt;/td&gt; &#xA;   &lt;td&gt;14185&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cpu: use cpu cache as ControlNet, args: &lt;code&gt;save_memory&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;under default setting (&lt;em&gt;e.g.&lt;/em&gt; resolution, &lt;em&gt;etc.&lt;/em&gt;) in &lt;code&gt;app.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/rese1f/StableVideo.git&#xA;conda create -n stablevideo python=3.11&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;optional but recommanded&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install xformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download Pretrained Model&lt;/h2&gt; &#xA;&lt;p&gt;All models and detectors can be downloaded from ControlNet Hugging Face page at &lt;a href=&#34;https://huggingface.co/lllyasviel/ControlNet&#34;&gt;Download Link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download example videos&lt;/h2&gt; &#xA;&lt;p&gt;Download the example atlas for car-turn, boat, libby, blackswa, bear, bicycle_tali, giraffe, kite-surf, lucia and motorbike at &lt;a href=&#34;https://www.dropbox.com/s/oiyhbiqdws2p6r1/nla_share.zip?dl=0&#34;&gt;Download Link&lt;/a&gt; shared by &lt;a href=&#34;https://github.com/omerbt/Text2LIVE&#34;&gt;Text2LIVE&lt;/a&gt; authors.&lt;/p&gt; &#xA;&lt;p&gt;You can also train on your own video following &lt;a href=&#34;https://github.com/ykasten/layered-neural-atlases&#34;&gt;NLA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;And it will create a folder data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;StableVideo&#xA;├── ...&#xA;├── ckpt&#xA;│   ├── cldm_v15.yaml&#xA;|   ├── dpt_hybrid-midas-501f0c75.pt&#xA;│   ├── control_sd15_canny.pth&#xA;│   └── control_sd15_depth.pth&#xA;├── data&#xA;│   └── car-turn&#xA;│       ├── checkpoint # NLA models are stored here&#xA;│       ├── car-turn # contains video frames&#xA;│       ├── ...&#xA;│   ├── blackswan&#xA;│   ├── ...&#xA;└── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run and Play!&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command to start. We provide some &lt;a href=&#34;https://raw.githubusercontent.com/rese1f/StableVideo/master/prompt_template.md&#34;&gt;prompt template&lt;/a&gt; to help you achieve better result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the result &lt;code&gt;.mp4&lt;/code&gt; video and keyframe will be stored in the directory &lt;code&gt;./log&lt;/code&gt; after clicking &lt;code&gt;render&lt;/code&gt; button.&lt;/p&gt; &#xA;&lt;p&gt;You can also edit the mask region for the foreground atlas as follows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/rese1f/StableVideo/assets/58205475/13e11c07-39ae-4d2d-8b66-f900d168ceff&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This implementation is built partly on &lt;a href=&#34;https://github.com/omerbt/Text2LIVE&#34;&gt;Text2LIVE&lt;/a&gt; and &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- ## Citation --&gt;</summary>
  </entry>
</feed>