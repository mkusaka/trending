<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-16T01:31:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>princeton-nlp/SWE-bench</title>
    <updated>2024-03-16T01:31:32Z</updated>
    <id>tag:github.com,2024-03-16:/princeton-nlp/SWE-bench</id>
    <link href="https://github.com/princeton-nlp/SWE-bench" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICLR 2024] SWE-Bench: Can Language Models Resolve Real-world Github Issues?&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/princeton-nlp/Llamao&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/swellama_banner.png&#34; width=&#34;50%&#34; alt=&#34;Kawi the SWE-Llama&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;| &lt;a href=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/docs/README_JP.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench&#34;&gt;English&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Code and data for our ICLR 2024 paper &lt;a href=&#34;http://swe-bench.github.io/paper.pdf&#34;&gt;SWE-bench: Can Language Models Resolve Real-World GitHub Issues?&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://www.python.org/&#34;&gt; &lt;img alt=&#34;Build&#34; src=&#34;https://img.shields.io/badge/Python-3.8+-1f425f.svg?color=purple&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://copyright.princeton.edu/policy&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/License-MIT-blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Please refer our &lt;a href=&#34;http://swe-bench.github.io&#34;&gt;website&lt;/a&gt; for the public leaderboard and the &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/CHANGELOG.md&#34;&gt;change log&lt;/a&gt; for information on the latest updates to the SWE-bench benchmark.&lt;/p&gt; &#xA;&lt;h2&gt;üëã Overview&lt;/h2&gt; &#xA;&lt;p&gt;SWE-bench is a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a &lt;em&gt;codebase&lt;/em&gt; and an &lt;em&gt;issue&lt;/em&gt;, a language model is tasked with generating a &lt;em&gt;patch&lt;/em&gt; that resolves the described problem.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/teaser.png&#34;&gt; &#xA;&lt;h2&gt;üöÄ Set Up&lt;/h2&gt; &#xA;&lt;p&gt;To build SWE-bench from source, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository locally&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd&lt;/code&gt; into the repository.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;conda env create -f environment.yml&lt;/code&gt; to created a conda environment named &lt;code&gt;swe-bench&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Activate the environment with &lt;code&gt;conda activate swe-bench&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üíΩ Usage&lt;/h2&gt; &#xA;&lt;p&gt;You can download the SWE-bench dataset directly (&lt;a href=&#34;https://drive.google.com/uc?export=download&amp;amp;id=1SbOxHiR0eXlq2azPSSOIDZz-Hva0ETpX&#34;&gt;dev&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/uc?export=download&amp;amp;id=164g55i3_B78F6EphCZGtgSrd2GneFyRM&#34;&gt;test&lt;/a&gt; sets) or from &lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To use SWE-Bench, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train your own models on our pre-processed datasets&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/inference/&#34;&gt;inference&lt;/a&gt; on existing models (either models you have on-disk like LLaMA, or models you have access to through an API like GPT-4). The inference step is where you get a repo and an issue and have the model try to generate a fix for it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/harness/&#34;&gt;Evaluate&lt;/a&gt; models against SWE-bench. This is where you take a SWE-Bench task and a model-proposed solution and evaluate its correctness.&lt;/li&gt; &#xA; &lt;li&gt;Run SWE-bench&#39;s &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/raw/master/collect/&#34;&gt;data collection procedure&lt;/a&gt; on your own repositories, to make new SWE-Bench tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚¨áÔ∏è Downloads&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Datasets&lt;/th&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench&#34;&gt;ü§ó SWE-bench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-13b&#34;&gt;ü¶ô SWE-Llama 13b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_oracle&#34;&gt;ü§ó &#34;Oracle&#34; Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-13b-peft&#34;&gt;ü¶ô SWE-Llama 13b (PEFT)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_13K&#34;&gt;ü§ó BM25 Retrieval 13K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-7b&#34;&gt;ü¶ô SWE-Llama 7b&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_27K&#34;&gt;ü§ó BM25 Retrieval 27K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/princeton-nlp/SWE-Llama-7b-peft&#34;&gt;ü¶ô SWE-Llama 7b (PEFT)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_40K&#34;&gt;ü§ó BM25 Retrieval 40K&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/princeton-nlp/SWE-bench_bm25_50k_llama&#34;&gt;ü§ó BM25 Retrieval 50K (Llama tokens)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üçé Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve also written the following blog posts on how to use different parts of SWE-bench. If you&#39;d like to see a post about a particular topic, please let us know via an issue.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Nov 1. 2023] Collecting Evaluation Tasks for SWE-Bench (&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/tutorials/collection.md&#34;&gt;üîó&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[Nov 6. 2023] Evaluating on SWE-bench (&lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/tutorials/evaluation.md&#34;&gt;üîó&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí´ Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We would love to hear from the broader NLP, Machine Learning, and Software Engineering research communities, and we welcome any contributions, pull requests, or issues! To do so, please either file a new pull request or issue and fill in the corresponding templates accordingly. We&#39;ll be sure to follow up shortly!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;http://www.carlosejimenez.com/&#34;&gt;Carlos E. Jimenez&lt;/a&gt; and &lt;a href=&#34;https://john-b-yang.github.io/&#34;&gt;John Yang&lt;/a&gt; (Email: {carlosej, jy1682}@princeton.edu).&lt;/p&gt; &#xA;&lt;h2&gt;‚úçÔ∏è Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please use the following citations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{&#xA;    jimenez2024swebench,&#xA;    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},&#xA;    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},&#xA;    booktitle={The Twelfth International Conference on Learning Representations},&#xA;    year={2024},&#xA;    url={https://openreview.net/forum?id=VTF8yNQM66}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü™™ License&lt;/h2&gt; &#xA;&lt;p&gt;MIT. Check &lt;code&gt;LICENSE.md&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>luijait/DarkGPT</title>
    <updated>2024-03-16T01:31:32Z</updated>
    <id>tag:github.com,2024-03-16:/luijait/DarkGPT</id>
    <link href="https://github.com/luijait/DarkGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DarkGPT is an OSINT assistant based on GPT-4-200K (recommended use) designed to perform queries on leaked databases, thus providing an artificial intelligence assistant that can be useful in your traditional OSINT processes.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/bYW6pai.jpg&#34; alt=&#34;Descripci√≥n de la imagen&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation Guide for DarkGPT Project&lt;/h1&gt; &#xA;&lt;p&gt;DarkGPT is an artificial intelligence assistant based on GPT-4-200K designed to perform queries on leaked databases. This guide will help you set up and run the project on your local environment.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Before starting, make sure you have Python installed on your system. This project has been tested with Python 3.8 and higher versions.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;First, you need to clone the GitHub repository to your local machine. You can do this by executing the following command in your terminal:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;git clone &lt;a href=&#34;https://github.com/luijait/DarkGPT.git&#34;&gt;https://github.com/luijait/DarkGPT.git&lt;/a&gt; cd DarkGPT&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure Environment Variables&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You will need to set up some environment variables for the script to work correctly. Copy the &lt;code&gt;.env.example&lt;/code&gt; file to a new file named &lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;DEHASHED_API_KEY=&#34;your_dehashed_api_key_here&#34;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Dependencies&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This project requires certain Python packages to run. Install them by running the following command:&lt;/p&gt; &lt;p&gt;pip install -r requirements.txt&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then Run the project: python3 main.py&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>ELLA-Diffusion/ELLA</title>
    <updated>2024-03-16T01:31:32Z</updated>
    <id>tag:github.com,2024-03-16:/ELLA-Diffusion/ELLA</id>
    <link href="https://github.com/ELLA-Diffusion/ELLA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;span class=&#34;author-block&#34;&gt; &lt;a href=&#34;https://openreview.net/profile?id=~Xiwei_Hu1&#34;&gt;Xiwei Hu*&lt;/a&gt;, &lt;/span&gt; &#xA; &lt;span class=&#34;author-block&#34;&gt; &lt;a href=&#34;https://wrong.wang/&#34;&gt;Rui Wang*&lt;/a&gt;, &lt;/span&gt; &#xA; &lt;span class=&#34;author-block&#34;&gt; &lt;a href=&#34;https://openreview.net/profile?id=~Yixiao_Fang1&#34;&gt;Yixiao Fang*&lt;/a&gt;, &lt;/span&gt; &#xA; &lt;span class=&#34;author-block&#34;&gt; &lt;a href=&#34;https://openreview.net/profile?id=~BIN_FU2&#34;&gt;Bin Fu*&lt;/a&gt;, &lt;/span&gt; &#xA; &lt;span class=&#34;author-block&#34;&gt; &lt;a href=&#34;https://openreview.net/profile?id=~Pei_Cheng1&#34;&gt;Pei Cheng&lt;/a&gt;, &lt;/span&gt; &#xA; &lt;span class=&#34;author-block&#34;&gt; &lt;a href=&#34;https://www.skicyyu.org/&#34;&gt;Gang Yu‚ú¶&lt;/a&gt; &lt;/span&gt; &#xA; &lt;p&gt; * Equal contributions, ‚ú¶ Corresponding Author &lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/assets/ELLA-Diffusion.jpg&#34; width=&#34;30%&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://ella-diffusion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.05135&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2403.05135-b31b1b.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Official code of &#34;ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment&#34;.&lt;/p&gt; &#xA;&lt;p&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/assets/teaser_3img.png&#34; width=&#34;100%&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/assets/teaser1_raccoon.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üåü Changelog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.3.11]&lt;/strong&gt; üî• Release DPG-Bench! Welcome to try!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.3.7]&lt;/strong&gt; Initial update&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìä DPG-Bench&lt;/h2&gt; &#xA;&lt;p&gt;The guideline of DPG-Bench:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate your images according to our &lt;a href=&#34;https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/dpg_bench/prompts/&#34;&gt;prompts&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It is recommended to generate 4 images per prompt and grid them to 2x2 format. &lt;strong&gt;Please Make sure your generated image&#39;s filename is the same with the prompt&#39;s filename.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command to conduct evaluation.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash dpg_bench/dist_eval.sh $YOUR_IMAGE_PATH $RESOLUTION&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Thanks to the excellent work of &lt;a href=&#34;https://github.com/j-min/DSG&#34;&gt;DSG&lt;/a&gt; sincerely, we follow their instructions to generate questions and answers of DPG-Bench.&lt;/p&gt; &#xA;&lt;h2&gt;üìù TODO&lt;/h2&gt; &#xA;&lt;p&gt;ü•π We are sorry that due to our company&#39;s review process, the release of the checkpoint and inference code will be slightly delayed. We appreciate your patience and PLEASE STAY TUNED.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; release checkpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; release inference code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release DPG-Bench&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí° Others&lt;/h2&gt; &#xA;&lt;p&gt;We have also found &lt;a href=&#34;https://arxiv.org/abs/2403.07860&#34;&gt;LaVi-Bridge&lt;/a&gt;, another independent but similar work completed almost concurrently, which offers additional insights not covered by ELLA. The difference between ELLA and LaVi-Bridge can be found in &lt;a href=&#34;https://github.com/ELLA-Diffusion/ELLA/issues/13&#34;&gt;issue 13&lt;/a&gt;. We are delighted to welcome other researchers and community users to promote the development of this field.&lt;/p&gt; &#xA;&lt;h2&gt;üòâ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;strong&gt;ELLA&lt;/strong&gt; useful for your research and applications, please cite us using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{hu2024ella,&#xA;      title={ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment}, &#xA;      author={Xiwei Hu and Rui Wang and Yixiao Fang and Bin Fu and Pei Cheng and Gang Yu},&#xA;      year={2024},&#xA;      eprint={2403.05135},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>