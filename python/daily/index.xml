<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-09T01:45:21Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nadermx/backgroundremover</title>
    <updated>2023-05-09T01:45:21Z</updated>
    <id>tag:github.com,2023-05-09:/nadermx/backgroundremover</id>
    <link href="https://github.com/nadermx/backgroundremover" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Background Remover lets you Remove Background from images and video using AI with a simple command line interface that is free and open source.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BackgroundRemover&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nadermx/backgroundremover/main/examplefiles/backgroundremoverexample.png&#34; alt=&#34;Background Remover&#34;&gt; &lt;img alt=&#34;background remover video&#34; src=&#34;https://raw.githubusercontent.com/nadermx/backgroundremover/main/examplefiles/backgroundremoverprocessed.gif&#34; height=&#34;200&#34;&gt;&lt;br&gt; BackgroundRemover is a command line tool to remove background from &lt;a href=&#34;https://github.com/nadermx/backgroundremover#image&#34;&gt;image&lt;/a&gt; and &lt;a href=&#34;https://github.com/nadermx/backgroundremover#video&#34;&gt;video&lt;/a&gt;, made by &lt;a href=&#34;https://john.nader.mx&#34;&gt;nadermx&lt;/a&gt; to power &lt;a href=&#34;https://backgroundremoverai.com&#34;&gt;https://BackgroundRemoverAI.com&lt;/a&gt;. If you wonder why it was made read this &lt;a href=&#34;https://johnathannader.com/my-first-open-source-project/&#34;&gt;short blog post&lt;/a&gt;.&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;python3.6-dev #or what ever version of python you use&lt;/li&gt; &#xA; &lt;li&gt;torch and torchvision stable version (&lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ffmpeg 4.4+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;How to install torch and fmpeg&lt;/h4&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt; and scroll down to &lt;code&gt;INSTALL PYTORCH&lt;/code&gt; section and follow the instructions.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PyTorch Build: Stable (1.7.1)&#xA;Your OS: Windows&#xA;Package: Pip&#xA;Language: Python&#xA;CUDA: None&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install ffmpeg and python-dev&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install ffmpeg python3.6-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;To Install backgroundremover, install it from pypi&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip&#xA;pip install backgroundremover&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please note that when you first run the program, it will check to see if you have the u2net models, if you do not, it will pull them from this repo&lt;/p&gt; &#xA;&lt;p&gt;It is also possible to run this without installing it via pip, just clone the git to local start a virtual env and install requirements and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m backgroundremover.cmd.cli -i &#34;video.mp4&#34; -mk -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and for windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python.exe -m backgroundremover.cmd.cli -i &#34;video.mp4&#34; -mk -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage as a cli&lt;/h3&gt; &#xA;&lt;h2&gt;Image&lt;/h2&gt; &#xA;&lt;p&gt;Remove the background from a local file image&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/image.jpeg&#34; -o &#34;output.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Advance usage for image background removal&lt;/h3&gt; &#xA;&lt;p&gt;Sometimes it is possible to achieve better results by turning on alpha matting. Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/image.jpeg&#34; -a -ae 15 -o &#34;output.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;change the model for diferent background removal methods between &lt;code&gt;u2netp&lt;/code&gt;, &lt;code&gt;u2net&lt;/code&gt;, or &lt;code&gt;u2net_human_seg&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/image.jpeg&#34; -m &#34;u2net_human_seg&#34; -o &#34;output.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Video&lt;/h2&gt; &#xA;&lt;h3&gt;remove background from video and make transparent mov&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -tv -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;remove background from local video and overlay it over other video&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -tov &#34;/path/to/videtobeoverlayed.mp4&#34; -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;remove background from local video and overlay it over an image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -toi &#34;/path/to/videtobeoverlayed.mp4&#34; -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;remove background from video and make transparent gif&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -tg -o &#34;output.gif&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Make matte key file (green screen overlay)&lt;/h3&gt; &#xA;&lt;p&gt;Make a matte file for premiere&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -mk -o &#34;output.matte.mp4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Advance usage for video&lt;/h3&gt; &#xA;&lt;p&gt;Change the framerate of the video (default is set to 30)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -fr 30 -tv -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set total number of frames of the video (default is set to -1, ie the remove background from full video)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -fl 150 -tv -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change the gpu batch size of the video (default is set to 1)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -gb 4 -tv -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change the number of workers working on video (default is set to 1)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -wn 4 -tv -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;change the model for diferent background removal methods between &lt;code&gt;u2netp&lt;/code&gt;, &lt;code&gt;u2net&lt;/code&gt;, or &lt;code&gt;u2net_human_seg&lt;/code&gt; and limit the frames to 150&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;backgroundremover -i &#34;/path/to/video.mp4&#34; -m &#34;u2net_human_seg&#34; -fl 150 -tv -o &#34;output.mov&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;convert logic from video to image to utilize more GPU on image removal&lt;/li&gt; &#xA; &lt;li&gt;clean up documentation a bit more&lt;/li&gt; &#xA; &lt;li&gt;add ability to adjust and give feedback images or videos to datasets&lt;/li&gt; &#xA; &lt;li&gt;add ability to realtime background removal for videos, for streaming&lt;/li&gt; &#xA; &lt;li&gt;finish flask server api&lt;/li&gt; &#xA; &lt;li&gt;add ability to use other models than u2net, ie your own&lt;/li&gt; &#xA; &lt;li&gt;other&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pull requests&lt;/h3&gt; &#xA;&lt;p&gt;Accepted&lt;/p&gt; &#xA;&lt;h3&gt;If you like this library&lt;/h3&gt; &#xA;&lt;p&gt;Give a link to our project &lt;a href=&#34;https://backgroundremoverai.com&#34;&gt;BackgroundRemoverAI.com&lt;/a&gt; or this git, telling people that you like it or use it.&lt;/p&gt; &#xA;&lt;h4&gt;bitcoin&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;bitcoin:BC1Q80PSHGQGQR7WN3KAX59XWVMGQ9FTVWLA7DEW7W?label=backgroundremover&amp;amp;message=BackgroundRemover&#34;&gt;bc1q80pshgqgqr7wn3kax59xwvmgq9ftvwla7dew7w&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Reason for project&lt;/h3&gt; &#xA;&lt;p&gt;We made it our own package after merging together parts of others, adding in a few features of our own via posting parts as bounty questions on superuser, etc. As well as asked on hackernews earlier to open source the image part, so decided to add in video, and a bit more.&lt;/p&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2005.09007.pdf&#34;&gt;https://arxiv.org/pdf/2005.09007.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NathanUA/U-2-Net&#34;&gt;https://github.com/NathanUA/U-2-Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pymatting/pymatting&#34;&gt;https://github.com/pymatting/pymatting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/danielgatis/rembg&#34;&gt;https://github.com/danielgatis/rembg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ecsplendid/rembg-greenscreen&#34;&gt;https://github.com/ecsplendid/rembg-greenscreen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://superuser.com/questions/1647590/have-ffmpeg-merge-a-matte-key-file-over-the-normal-video-file-removing-the-backg&#34;&gt;https://superuser.com/questions/1647590/have-ffmpeg-merge-a-matte-key-file-over-the-normal-video-file-removing-the-backg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://superuser.com/questions/1648680/ffmpeg-alphamerge-two-videos-into-a-gif-with-transparent-background/1649339?noredirect=1#comment2522687_1649339&#34;&gt;https://superuser.com/questions/1648680/ffmpeg-alphamerge-two-videos-into-a-gif-with-transparent-background/1649339?noredirect=1#comment2522687_1649339&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://superuser.com/questions/1649817/ffmpeg-overlay-a-video-after-alphamerging-two-others/1649856#1649856&#34;&gt;https://superuser.com/questions/1649817/ffmpeg-overlay-a-video-after-alphamerging-two-others/1649856#1649856&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copyright (c) 2021-present &lt;a href=&#34;https://github.com/nadermx&#34;&gt;Johnathan Nader&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copyright (c) 2020-present &lt;a href=&#34;https://github.com/ClashLuke&#34;&gt;Lucas Nestler&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copyright (c) 2020-present &lt;a href=&#34;https://github.com/ecsplendid&#34;&gt;Dr. Tim Scarfe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copyright (c) 2020-present &lt;a href=&#34;https://github.com/danielgatis&#34;&gt;Daniel Gatis&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Code Licensed under &lt;a href=&#34;https://raw.githubusercontent.com/nadermx/backgroundremover/main/LICENSE.txt&#34;&gt;MIT License&lt;/a&gt; Models Licensed under &lt;a href=&#34;https://raw.githubusercontent.com/nadermx/backgroundremover/main/models/license&#34;&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CiaraStrawberry/TemporalKit</title>
    <updated>2023-05-09T01:45:21Z</updated>
    <id>tag:github.com,2023-05-09:/CiaraStrawberry/TemporalKit</id>
    <link href="https://github.com/CiaraStrawberry/TemporalKit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An all in one solution for adding Temporal Stability to a Stable Diffusion Render via an automatic1111 extension&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TemporalKit&lt;/h1&gt; &#xA;&lt;p&gt;An all in one solution for adding Temporal Stability to a Stable Diffusion Render via an automatic1111 extension&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;You must install FFMPEG to path before running this&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can find a demonstration run through with a single batch here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/CiaraRowles1/status/1645923461343363072&#34;&gt;https://twitter.com/CiaraRowles1/status/1645923461343363072&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And a batch demonstration here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mobile.twitter.com/CiaraRowles1/status/1646458056803250178&#34;&gt;https://mobile.twitter.com/CiaraRowles1/status/1646458056803250178&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ebsynth tutorial:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/CiaraRowles1/status/1648462374125576192&#34;&gt;https://twitter.com/CiaraRowles1/status/1648462374125576192&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NOTE: EBSYNTH DOES NOT REGISTER THE KEYFRAMES IF YOU USE ABOVE 20,&lt;/p&gt; &#xA;&lt;p&gt;Ebsynth split frames tutorial:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=z3YNHiuvxyg&amp;amp;ab_channel=CiaraRowles&#34;&gt;https://www.youtube.com/watch?v=z3YNHiuvxyg&amp;amp;ab_channel=CiaraRowles&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example results you can get:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/13116982/234425054-9a1bbf30-93a8-4f5b-9e80-4376ab3c510a.mp4&#34;&gt;https://user-images.githubusercontent.com/13116982/234425054-9a1bbf30-93a8-4f5b-9e80-4376ab3c510a.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The values in the extension are as follows&lt;/p&gt; &#xA;&lt;p&gt;FPS: This is the fps the video is extracted and produced at.&lt;/p&gt; &#xA;&lt;p&gt;batch_Size: this is the number of frames between each keyframe, so for example if you had an fps of 30, and a batch size of 10, it would make 3 keyframes a second and estimate the rest.&lt;/p&gt; &#xA;&lt;p&gt;per side: this is the square root of the number of frames per plate, so for example a per side value of 2 would make 4 plates, 3, 9 plates, 4 16 plates.&lt;/p&gt; &#xA;&lt;p&gt;Resolution: the size of each plate, it is strongly reccomended you set this to a multiple of your per side variable&lt;/p&gt; &#xA;&lt;p&gt;batch settings: only open this drop down if you want to generate a folder of plates.&lt;/p&gt; &#xA;&lt;p&gt;Max Frames: when generating a folder of plates, this gets how many frames at the above fps you want to get, and then divides them into plates in groups of (per side * per side * batch size)&lt;/p&gt; &#xA;&lt;p&gt;Border Frames: every batch generated plate will contain this many frames from the next plate and blend between them.&lt;/p&gt; &#xA;&lt;p&gt;Batch Folder: If you&#39;re generating a batch of plates, just specify a empty folder and on clicking run, it will populate it with the relevant folders and files, all you need to do is go to img2img batch processing in original sd, enter the newly create input folder as the input, the newly created output folder as the output, generate, move back to the temporal-kit Batch-Warp Tab, put in the whole folder directory and click read and it will set everything up.&lt;/p&gt; &#xA;&lt;p&gt;Output Resolution (in the batch warp tab): the maximum resolution on any side of the output video.&lt;/p&gt; &#xA;&lt;p&gt;FAQ:&lt;/p&gt; &#xA;&lt;p&gt;Q: my video has smearing&lt;/p&gt; &#xA;&lt;p&gt;A: use a higher fps and/or lower batchnumber, the closer together the keyframes the less artifacts.&lt;/p&gt; &#xA;&lt;p&gt;#TODO&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;set up diffusion based upscaling for the plates output&lt;/li&gt; &#xA; &lt;li&gt;get the img2img button working with batch processing.&lt;/li&gt; &#xA; &lt;li&gt;add a check to see if the output folder was added.&lt;/li&gt; &#xA; &lt;li&gt;fix that weird shutdown error it gives after running&lt;/li&gt; &#xA; &lt;li&gt;hook up to the api.&lt;/li&gt; &#xA; &lt;li&gt;flowmaps from game engine export\import support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thanks to RAFT for the optical flow system.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bigcode-project/starcoder</title>
    <updated>2023-05-09T01:45:21Z</updated>
    <id>tag:github.com,2023-05-09:/bigcode-project/starcoder</id>
    <link href="https://github.com/bigcode-project/starcoder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Home of StarCoder: fine-tuning &amp; inference!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ’« StarCoder&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;Model&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-playground&#34;&gt;Playground&lt;/a&gt; | &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode&#34;&gt;VSCode&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/chat/?model=bigcode/starcoder&#34;&gt;Chat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is this about?&lt;/h1&gt; &#xA;&lt;p&gt;ðŸ’« StarCoder is a language model (LM) trained on source code and natural language text. Its training data incorporates more that 80 different programming languages as well as text extracted from GitHub issues and commits and from notebooks. This repository showcases how we get an overview of this LM&#39;s capabilities.&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;Before you can use the model go to &lt;code&gt;hf.co/bigcode/starcoder&lt;/code&gt; and accept the agreement. And make sure you are logged into the Hugging Face hub with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#code-generation&#34;&gt;Code generation with StarCoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#text-generation-inference&#34;&gt;Text-generation-inference code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#step-by-step-installation-with-conda&#34;&gt;Step by step installation with conda&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#datasets&#34;&gt;Datasets&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#stack-exchange-se&#34;&gt;Stack Exchange&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#merging-peft-adapter-layers&#34;&gt;Merging PEFT adapter layers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;StarCoder was trained on GitHub code, thus it can be used to perform code generation. More precisely, the model can complete the implementation of a function or infer the following characters in a line of code. This can be done with the help of the ðŸ¤—&#39;s &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First, we have to install all the libraries listed in &lt;code&gt;requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code generation&lt;/h2&gt; &#xA;&lt;p&gt;The code generation pipeline is as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;checkpoint = &#34;bigcode/starcoder&#34;&#xA;device = &#34;cuda&#34; # for GPU usage or &#34;cpu&#34; for CPU usage&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(checkpoint)&#xA;# to save memory consider using fp16 or bf16 by specifying torch.dtype=torch.float16 for example&#xA;model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)&#xA;&#xA;inputs = tokenizer.encode(&#34;def print_hello_world():&#34;, return_tensors=&#34;pt&#34;).to(device)&#xA;outputs = model.generate(inputs)&#xA;print(tokenizer.decode(outputs[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline&#xA;checkpoint = &#34;bigcode/starcoder&#34;&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(checkpoint)&#xA;tokenizer = AutoTokenizer.from_pretrained(checkpoint)&#xA;&#xA;pipe = pipeline(&#34;text-generation&#34;, model=model, tokenizer=tokenizer, device=0)&#xA;print( pipe(&#34;def hello():&#34;) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text-generation-inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus &#39;&#34;device:0&#34;&#39; -p 8080:80 -v $PWD/data:/data -e HUGGING_FACE_HUB_TOKEN=&amp;lt;YOUR BIGCODE ENABLED TOKEN&amp;gt; -e HF_HUB_ENABLE_HF_TRANSFER=0 -d  ghcr.io/huggingface/text-generation-inference:sha-880a76e --model-id bigcode/starcoder --max-total-tokens 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;Here, we showcase how we can fine-tune this LM on a specific downstream task.&lt;/p&gt; &#xA;&lt;h2&gt;Step by step installation with conda&lt;/h2&gt; &#xA;&lt;p&gt;Create a new conda environment and activate it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n env&#xA;conda activate env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;pytorch&lt;/code&gt; version compatible with your version of cuda &lt;a href=&#34;https://pytorch.org/get-started/previous-versions/&#34;&gt;here&lt;/a&gt;, for example the following command works with cuda 11.6&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;peft&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c huggingface transformers &#xA;pip install git+https://github.com/huggingface/peft.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you can install the latest stable version of transformers by using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;datasets&lt;/code&gt;, &lt;code&gt;accelerate&lt;/code&gt; and &lt;code&gt;huggingface_hub&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c huggingface -c conda-forge datasets&#xA;conda install -c conda-forge accelerate&#xA;conda install -c conda-forge huggingface_hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and &lt;code&gt;wandb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install bitsandbytes&#xA;pip install wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get the full list of arguments with descriptions you can run the following command on any script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/some_script.py --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before you run any of the scripts make sure you are logged in and can push to the hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure you are logged in &lt;code&gt;wandb&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now that everything is done, you can clone the repository and get into the corresponding directory.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;ðŸ’« StarCoder can be fine-tuned to achieve multiple downstream tasks. Our interest here is to fine-tune StarCoder in order to make it follow instructions. &lt;a href=&#34;https://arxiv.org/pdf/2109.01652.pdf&#34;&gt;Instruction fine-tuning&lt;/a&gt; has gained a lot of attention recently as it proposes a simple framework that teaches language models to align their outputs with human needs. That procedure requires the availability of quality instruction datasets, which contain multiple &lt;code&gt;instruction - answer&lt;/code&gt; pairs. Unfortunately such datasets are not ubiquitous but thanks to Hugging Face ðŸ¤—&#39;s &lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;datasets&lt;/a&gt; library we can have access to some good proxies. To fine-tune cheaply and efficiently, we use Hugging Face ðŸ¤—&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; as well as Tim Dettmers&#39; &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Stack Exchange SE&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Stack_Exchange&#34;&gt;Stack Exchange&lt;/a&gt; is a well-known network of Q&amp;amp;A websites on topics in diverse fields. It is a place where a user can ask a question and obtain answers from other users. Those answers are scored and ranked based on their quality. &lt;a href=&#34;https://huggingface.co/datasets/ArmelR/stack-exchange-instruction&#34;&gt;Stack exchange instruction&lt;/a&gt; is a dataset that was obtained by scrapping the site in order to build a collection of Q&amp;amp;A pairs. A language model can then be fine-tuned on that dataset to make it elicit strong and diverse question-answering skills.&lt;/p&gt; &#xA;&lt;p&gt;To execute the fine-tuning script run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/finetune.py \&#xA;  --model_path=&#34;bigcode/starcoder&#34;\&#xA;  --dataset_name=&#34;ArmelR/stack-exchange-instruction&#34;\&#xA;  --subset=&#34;data/finetune&#34;\&#xA;  --split=&#34;train&#34;\&#xA;  --size_valid_set 10000\&#xA;  --streaming\&#xA;  --seq_length 2048\&#xA;  --max_steps 1000\&#xA;  --batch_size 1\&#xA;  --input_column_name=&#34;question&#34;\&#xA;  --output_column_name=&#34;response&#34;\ &#xA;  --gradient_accumulation_steps 16\&#xA;  --learning_rate 1e-4\&#xA;  --lr_scheduler_type=&#34;cosine&#34;\&#xA;  --num_warmup_steps 100\&#xA;  --weight_decay 0.05\&#xA;  --output_dir=&#34;./checkpoints&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The size of the SE dataset is better manageable when using streaming. We also have to precise the split of the dataset that is used. For more details, check the &lt;a href=&#34;https://huggingface.co/datasets/ArmelR/stack-exchange-instruction&#34;&gt;dataset&#39;s page&lt;/a&gt; on ðŸ¤—. Similarly we can modify the command to account for the availability of GPUs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m torch.distributed.launch \&#xA;  --nproc_per_node number_of_gpus finetune/finetune.py \&#xA;  --model_path=&#34;bigcode/starcoder&#34;\&#xA;  --dataset_name=&#34;ArmelR/stack-exchange-instruction&#34;\&#xA;  --subset=&#34;data/finetune&#34;\&#xA;  --split=&#34;train&#34;\&#xA;  --size_valid_set 10000\&#xA;  --streaming \&#xA;  --seq_length 2048\&#xA;  --max_steps 1000\&#xA;  --batch_size 1\&#xA;  --input_column_name=&#34;question&#34;\&#xA;  --output_column_name=&#34;response&#34;\ &#xA;  --gradient_accumulation_steps 16\&#xA;  --learning_rate 1e-4\&#xA;  --lr_scheduler_type=&#34;cosine&#34;\&#xA;  --num_warmup_steps 100\&#xA;  --weight_decay 0.05\&#xA;  --output_dir=&#34;./checkpoints&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Merging PEFT adapter layers&lt;/h2&gt; &#xA;&lt;p&gt;If you train a model with PEFT, you&#39;ll need to merge the adapter layers with the base model if you want to run inference / evaluation. To do so, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/merge_peft_adapters.py --model_name_or_path model_to_merge --peft_model_path model_checkpoint&#xA;&#xA;# Push merged model to the Hub&#xA;python finetune/merge_peft_adapters.py --model_name_or_path model_to_merge --peft_model_path model_checkpoint --push_to_hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/merge_peft_adapters.py --model_name_or_path bigcode/starcoder --peft_model_path checkpoints/checkpoint-1000 --push_to_hub&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>