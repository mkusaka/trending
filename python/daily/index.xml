<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-10T01:32:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Kroery/DiffMOT</title>
    <updated>2024-05-10T01:32:12Z</updated>
    <id>tag:github.com,2024-05-10:/Kroery/DiffMOT</id>
    <link href="https://github.com/Kroery/DiffMOT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;code for CVPR2024 paper: DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffMOT (CVPR2024)&lt;/h1&gt; &#xA;&lt;h2&gt;DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction &lt;a href=&#34;https://diffmot.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34; alt=&#34;arxiv paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.02075&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-red&#34; alt=&#34;arxiv paper&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/teaser_git.png&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Framework&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/diffmot_git.png&#34; alt=&#34;Framework&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/ddmp_git.png&#34; alt=&#34;Framework&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We now upload the trained motion model.&lt;/li&gt; &#xA; &lt;li&gt;2024-02-27: This work is accepted by &lt;strong&gt;CVPR-2024&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tracking performance&lt;/h2&gt; &#xA;&lt;h3&gt;Benchmark Evaluation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;HOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;Assa&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;DetA&lt;/th&gt; &#xA;   &lt;th&gt;Weight&lt;/th&gt; &#xA;   &lt;th&gt;Results&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DanceTrack&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;47.2&lt;/td&gt; &#xA;   &lt;td&gt;92.8&lt;/td&gt; &#xA;   &lt;td&gt;82.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/DanceTrack_epoch800.pt&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.2/DanceTrack_DiffMOT.zip&#34;&gt;DanceTrack_Results&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SportsMOT&lt;/td&gt; &#xA;   &lt;td&gt;76.2&lt;/td&gt; &#xA;   &lt;td&gt;76.1&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;97.1&lt;/td&gt; &#xA;   &lt;td&gt;89.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/SportsMOT_epoch1200.pt&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.2/SportsMOT_DiffMOT.zip&#34;&gt;SportsMOT_Results&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT17&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;79.3&lt;/td&gt; &#xA;   &lt;td&gt;64.6&lt;/td&gt; &#xA;   &lt;td&gt;79.8&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/MOT_epoch800.pt&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.2/MOT17_DiffMOT.zip&#34;&gt;MOT17_Results&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT20&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;60.5&lt;/td&gt; &#xA;   &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;td&gt;63.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/MOT_epoch800.pt&#34;&gt;download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.2/MOT20_DiffMOT.zip&#34;&gt;MOT20_Results&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Results on DanceTrack test set with different detector&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Detector&lt;/th&gt; &#xA;   &lt;th&gt;HOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOX-S&lt;/td&gt; &#xA;   &lt;td&gt;53.3&lt;/td&gt; &#xA;   &lt;td&gt;56.6&lt;/td&gt; &#xA;   &lt;td&gt;88.4&lt;/td&gt; &#xA;   &lt;td&gt;30.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOX-M&lt;/td&gt; &#xA;   &lt;td&gt;57.2&lt;/td&gt; &#xA;   &lt;td&gt;58.6&lt;/td&gt; &#xA;   &lt;td&gt;91.2&lt;/td&gt; &#xA;   &lt;td&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOX-L&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;92.0&lt;/td&gt; &#xA;   &lt;td&gt;24.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YOLOX-X&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;92.8&lt;/td&gt; &#xA;   &lt;td&gt;22.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The tracking speed (including detection and tracking speed) is test on an RTX 3090 GPU. Smaller detectors can achieve higher FPS, which indicates that DiffMOT can flexibly choose different detectors for various real-world application scenarios. With YOLOX-S, the tracking speed of the entire system can reach up to &lt;strong&gt;30.3 FPS&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Video demos&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/DiffMOT_DanceTrack.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/DiffMOT_SportsMOT.gif&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;I. Installation.&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install torch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n diffmot python=3.9&#xA;conda activate diffmot&#xA;pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install other packages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirement.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install external dependencies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd external/YOLOX/&#xA;pip install -r requirements.txt &amp;amp;&amp;amp; python setup.py develop&#xA;cd ../external/deep-person-reid/&#xA;pip install -r requirements.txt &amp;amp;&amp;amp; python setup.py develop&#xA;cd ../external/fast_reid/&#xA;pip install -r docs/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;II. Prepare Data.&lt;/h2&gt; &#xA;&lt;p&gt;The file structure should look like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;DanceTrack&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{DanceTrack ROOT}&#xA;|-- dancetrack&#xA;|   |-- train&#xA;|   |   |-- dancetrack0001&#xA;|   |   |   |-- img1&#xA;|   |   |   |   |-- 00000001.jpg&#xA;|   |   |   |   |-- ...&#xA;|   |   |   |-- gt&#xA;|   |   |   |   |-- gt.txt            &#xA;|   |   |   |-- seqinfo.ini&#xA;|   |   |-- ...&#xA;|   |-- val&#xA;|   |   |-- ...&#xA;|   |-- test&#xA;|   |   |-- ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;SportsMOT&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{SportsMOT ROOT}&#xA;|-- sportsmot&#xA;|   |-- splits_txt&#xA;|   |-- scripts&#xA;|   |-- dataset&#xA;|   |   |-- train&#xA;|   |   |   |-- v_1LwtoLPw2TU_c006&#xA;|   |   |   |   |-- img1&#xA;|   |   |   |   |   |-- 000001.jpg&#xA;|   |   |   |   |   |-- ...&#xA;|   |   |   |   |-- gt&#xA;|   |   |   |   |   |-- gt.txt&#xA;|   |   |   |   |-- seqinfo.ini         &#xA;|   |   |   |-- ...&#xA;|   |   |-- val&#xA;|   |   |   |-- ...&#xA;|   |   |-- test&#xA;|   |   |   |-- ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MOT17/20&lt;/strong&gt; We train the MOT17 and MOT20 together.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{MOT17/20 ROOT}&#xA;|-- mot&#xA;|   |-- train&#xA;|   |   |-- MOT17-02&#xA;|   |   |   |-- img1&#xA;|   |   |   |   |-- 000001.jpg&#xA;|   |   |   |   |-- ...&#xA;|   |   |   |-- gt&#xA;|   |   |   |   |-- gt.txt            &#xA;|   |   |   |-- seqinfo.ini&#xA;|   |   |-- ...&#xA;|   |   |-- MOT20-01&#xA;|   |   |   |-- img1&#xA;|   |   |   |   |-- 000001.jpg&#xA;|   |   |   |   |-- ...&#xA;|   |   |   |-- gt&#xA;|   |   |   |   |-- gt.txt            &#xA;|   |   |   |-- seqinfo.ini&#xA;|   |   |-- ...&#xA;|   |-- test&#xA;|   |   |-- ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python dancetrack_data_process.py&#xA;python sports_data_process.py&#xA;python mot_data_process.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;III. Model ZOO.&lt;/h2&gt; &#xA;&lt;h3&gt;Detection Model&lt;/h3&gt; &#xA;&lt;p&gt;We provide some trained YOLOX weights in &lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/tag/v1.0&#34;&gt;download&lt;/a&gt; for DiffMOT. Some of them are inherited from &lt;a href=&#34;https://github.com/ifzhang/ByteTrack#model-zoo&#34;&gt;ByteTrack&lt;/a&gt;, &lt;a href=&#34;https://github.com/DanceTrack/DanceTrack#evaluation&#34;&gt;DanceTrack&lt;/a&gt;, and &lt;a href=&#34;https://github.com/MCG-NJU/MixSort#model-zoo&#34;&gt;MixSort&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ReID Model&lt;/h3&gt; &#xA;&lt;p&gt;Ours ReID models for &lt;strong&gt;MOT17/MOT20&lt;/strong&gt; is the same as &lt;a href=&#34;https://github.com/NirAharon/BOT-SORT&#34;&gt;BoT-SORT&lt;/a&gt; , you can download from &lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/mot17_sbs_S50.pth&#34;&gt;MOT17-SBS-S50&lt;/a&gt;, &lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/mot20_sbs_S50.pth&#34;&gt;MOT20-SBS-S50&lt;/a&gt;. The ReID model for &lt;strong&gt;DanceTrack&lt;/strong&gt; is the same as &lt;a href=&#34;https://github.com/GerardMaggiolino/Deep-OC-SORT&#34;&gt;Deep-OC-SORT&lt;/a&gt;, you can download from &lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/dance_sbs_S50.pth&#34;&gt;Dance-SBS-S50&lt;/a&gt;. The ReID model for &lt;strong&gt;SportsMOT&lt;/strong&gt; is trained by ourself, you can download from &lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.0/sports_sbs_S50.pth&#34;&gt;Sports-SBS-S50&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/18IsZGeGiyKDshhYIzbpYXoNEcBhPY8lN?usp=sharing&#34;&gt;MOT20-SBS-S50&lt;/a&gt; is trained by &lt;a href=&#34;https://github.com/GerardMaggiolino/Deep-OC-SORT&#34;&gt;Deep-OC-SORT&lt;/a&gt;, because the weight from BOT-SORT is corrupted. Refer to &lt;a href=&#34;https://github.com/GerardMaggiolino/Deep-OC-SORT/issues/6&#34;&gt;Issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ReID models for SportsMOT is trained by ourself.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Motion Model (D$^2$MP)&lt;/h3&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://github.com/Kroery/DiffMOT?tab=readme-ov-file#benchmark-evaluation&#34;&gt;models&lt;/a&gt;. We train on DanceTrack and MOT17/20 for 800 epochs, and train on SportsMOT for 1200 epochs.&lt;/p&gt; &#xA;&lt;h2&gt;IV. Training.&lt;/h2&gt; &#xA;&lt;h3&gt;Train the detection model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can refer to the &lt;a href=&#34;https://github.com/ifzhang/ByteTrack#training&#34;&gt;ByteTrack&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Train the ReID model&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can refer to the &lt;a href=&#34;https://github.com/NirAharon/BoT-SORT#train-the-reid-module&#34;&gt;BoT-SORT&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Train the motion model (D$^2$MP)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change the data_dir in config&lt;/li&gt; &#xA; &lt;li&gt;Train on DanceTrack, SportsMOT, and MOT17/20:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --config ./configs/dancetrack.yaml&#xA;python main.py --config ./configs/sportsmot.yaml&#xA;python main.py --config ./configs/mot.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;V. Tracking.&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare detections&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can obtain the detections by &lt;a href=&#34;https://github.com/Kroery/DiffMOT#detection-model&#34;&gt;detection_model&lt;/a&gt; or use the &lt;a href=&#34;https://github.com/Kroery/DiffMOT/releases/download/v1.1/Detections.zip&#34;&gt;detection_results&lt;/a&gt; we have provided.&lt;/li&gt; &#xA; &lt;li&gt;Change the det_dir in config.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prepare ReID embeddings&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We have provided the &lt;a href=&#34;https://drive.google.com/drive/folders/1mScXsSgq_t7Y4AakPMhwrQ8Lu0vbGj1Z?usp=sharing&#34;&gt;ReID embeddings&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Change the reid_dir in config.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Track on DanceTrack&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change the info_dir, and save_dir in config.&lt;/li&gt; &#xA; &lt;li&gt;High_thres is set to 0.6, low_thres is set to 0.4, w_assoc_emb is set to 2.2, and aw_param is set to 1.7.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --config ./configs/dancetrack_test.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Track on SportsMOT&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change the info_dir, and save_dir in config.&lt;/li&gt; &#xA; &lt;li&gt;High_thres is set to 0.6, low_thres is set to 0.4, w_assoc_emb is set to 2.0, and aw_param is set to 1.2.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --config ./configs/sportsmot_test.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Track on MOT17&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change the info_dir, and save_dir in config.&lt;/li&gt; &#xA; &lt;li&gt;High_thres is set to 0.6, low_thres is set to 0.1, w_assoc_emb is set to 2.2, and aw_param is set to 1.7.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --config ./configs/mot17_test.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Track on MOT20&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Change the info_dir, and save_dir in config.&lt;/li&gt; &#xA; &lt;li&gt;High_thres is set to 0.4, low_thres is set to 0.1, w_assoc_emb is set to 2.2, and aw_param is set to 1.7.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --config ./configs/mot20_test.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have some questions, please concat with &lt;a href=&#34;mailto:kroery@shu.edu.cn&#34;&gt;kroery@shu.edu.cn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;A large part of the code is borrowed from &lt;a href=&#34;https://github.com/GuHuangAI/DDM-Public&#34;&gt;DDM-Public&lt;/a&gt; and &lt;a href=&#34;https://github.com/GerardMaggiolino/Deep-OC-SORT&#34;&gt;Deep-OC-SORT&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{lv2024diffmot,&#xA;  title={DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction},&#xA;  author={Lv, Weiyi and Huang, Yuhang and Zhang, Ning and Lin, Ruei-Sung and Han, Mei and Zeng, Dan},&#xA;  journal={arXiv preprint arXiv:2403.02075},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/lerobot</title>
    <updated>2024-05-10T01:32:12Z</updated>
    <id>tag:github.com,2024-05-10:/huggingface/lerobot</id>
    <link href="https://github.com/huggingface/lerobot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ó LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;img alt=&#34;LeRobot, Hugging Face Robotics Library&#34; src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/huggingface/lerobot&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/lerobot&#34; alt=&#34;Python versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/lerobot&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lerobot&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/tree/main/examples&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Examples-green.svg?sanitize=true&#34; alt=&#34;Examples&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/s3KuuzsPFb&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;State-of-the-art Machine Learning for real-world robotics&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ü§ó LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;ü§ó LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;ü§ó LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; &#xA;&lt;p&gt;ü§ó LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/aloha_act.gif&#34; width=&#34;100%&#34; alt=&#34;ACT policy on ALOHA env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/simxarm_tdmpc.gif&#34; width=&#34;100%&#34; alt=&#34;TDMPC policy on SimXArm env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/pusht_diffusion.gif&#34; width=&#34;100%&#34; alt=&#34;Diffusion policy on PushT env&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACT policy on ALOHA env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TDMPC policy on SimXArm env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Diffusion policy on PushT env&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Acknowledgment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to Tony Zaho, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href=&#34;https://tonyzhaozh.github.io/aloha&#34;&gt;ALOHA&lt;/a&gt; and &lt;a href=&#34;https://mobile-aloha.github.io&#34;&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu&#34;&gt;Diffusion Policy&lt;/a&gt; and &lt;a href=&#34;https://umi-gripper.github.io&#34;&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href=&#34;https://github.com/nicklashansen/tdmpc&#34;&gt;TDMPC&lt;/a&gt; and &lt;a href=&#34;https://www.yunhaifeng.com/FOWM&#34;&gt;FOWM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Vincent Moens and colleagues for open sourcing &lt;a href=&#34;https://github.com/pytorch/rl&#34;&gt;TorchRL&lt;/a&gt;. It allowed for quick experimentations on the design of &lt;code&gt;LeRobot&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download our source code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/huggingface/lerobot.git &amp;amp;&amp;amp; cd lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/index.html&#34;&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -y -n lerobot python=3.10 &amp;amp;&amp;amp; conda activate lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install ü§ó LeRobot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For simulations, ü§ó LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-aloha&#34;&gt;aloha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-xarm&#34;&gt;xarm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-pusht&#34;&gt;pusht&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For instance, to install ü§ó LeRobot with aloha and pusht, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;.[aloha, pusht]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://docs.wandb.ai/quickstart&#34;&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Walkthrough&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îú‚îÄ‚îÄ examples             # contains demonstration examples, start here to learn about LeRobot&#xA;‚îú‚îÄ‚îÄ lerobot&#xA;|   ‚îú‚îÄ‚îÄ configs          # contains hydra yaml files with all options that you can override in the command line&#xA;|   |   ‚îú‚îÄ‚îÄ default.yaml   # selected by default, it loads pusht environment and diffusion policy&#xA;|   |   ‚îú‚îÄ‚îÄ env            # various sim environments and their datasets: aloha.yaml, pusht.yaml, xarm.yaml&#xA;|   |   ‚îî‚îÄ‚îÄ policy         # various policies: act.yaml, diffusion.yaml, tdmpc.yaml&#xA;|   ‚îú‚îÄ‚îÄ common           # contains classes and utilities&#xA;|   |   ‚îú‚îÄ‚îÄ datasets       # various datasets of human demonstrations: aloha, pusht, xarm&#xA;|   |   ‚îú‚îÄ‚îÄ envs           # various sim environments: aloha, pusht, xarm&#xA;|   |   ‚îú‚îÄ‚îÄ policies       # various policies: act, diffusion, tdmpc&#xA;|   |   ‚îî‚îÄ‚îÄ utils          # various utilities&#xA;|   ‚îî‚îÄ‚îÄ scripts          # contains functions to execute via command line&#xA;|       ‚îú‚îÄ‚îÄ eval.py                 # load policy and evaluate it on an environment&#xA;|       ‚îú‚îÄ‚îÄ train.py                # train a policy via imitation learning and/or reinforcement learning&#xA;|       ‚îú‚îÄ‚îÄ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub&#xA;|       ‚îî‚îÄ‚îÄ visualize_dataset.py    # load a dataset and render its demonstrations&#xA;‚îú‚îÄ‚îÄ outputs               # contains results of scripts execution: logs, videos, model checkpoints&#xA;‚îî‚îÄ‚îÄ tests                 # contains pytest utilities for continuous integration&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualize datasets&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&#34;&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically download data from the Hugging Face hub.&lt;/p&gt; &#xA;&lt;p&gt;You can also locally visualize episodes from a dataset by executing our script from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/visualize_dataset.py \&#xA;    --repo-id lerobot/pusht \&#xA;    --episode-index 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&#34;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;python lerobot/scripts/visualize_dataset.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluate a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py&#34;&gt;example 2&lt;/a&gt; that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py \&#xA;    -p lerobot/diffusion_pusht \&#xA;    eval.n_episodes=10 \&#xA;    eval.batch_size=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: After training your own policy, you can re-evaluate the checkpoints with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py \&#xA;    -p PATH/TO/TRAIN/OUTPUT/FOLDER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Train your own policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py&#34;&gt;example 3&lt;/a&gt; that illustrates how to start training a model.&lt;/p&gt; &#xA;&lt;p&gt;In general, you can use our training script to easily train any policy. To use wandb for logging training and evaluation curves, make sure you ran &lt;code&gt;wandb login&lt;/code&gt;. Here is an example of training the ACT policy on trajectories collected by humans on the Aloha simulation environment for the insertion task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py \&#xA;    policy=act \&#xA;    env=aloha \&#xA;    env.task=AlohaInsertion-v0 \&#xA;    dataset_repo_id=lerobot/aloha_sim_insertion_human&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The experiment directory is automatically generated and will show up in yellow in your terminal. It looks like &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default&lt;/code&gt;. You can manually specify an experiment directory by adding this argument to the &lt;code&gt;train.py&lt;/code&gt; python command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    hydra.run.dir=your/new/experiment/dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of logs from wandb: &lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can deactivate wandb by adding these arguments to the &lt;code&gt;train.py&lt;/code&gt; python command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    wandb.disable_artifact=true \&#xA;    wandb.enable=false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. After training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to ü§ó LeRobot, please check out our &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a new dataset&lt;/h3&gt; &#xA;&lt;p&gt;To add a dataset to the hub, you need to login using a write-access token, which can be generated from the &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;Hugging Face settings&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then move your dataset folder in &lt;code&gt;data&lt;/code&gt; directory (e.g. &lt;code&gt;data/aloha_ping_pong&lt;/code&gt;), and push your dataset to the hub with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/push_dataset_to_hub.py \&#xA;--data-dir data \&#xA;--dataset-id aloha_ping_ping \&#xA;--raw-format aloha_hdf5 \&#xA;--community-id lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/push_dataset_to_hub.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;p&gt;If your dataset format is not supported, implement your own in &lt;code&gt;lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py&lt;/code&gt; by copying examples like &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py&#34;&gt;pusht_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py&#34;&gt;umi_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py&#34;&gt;aloha_hdf5&lt;/a&gt;, or &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py&#34;&gt;xarm_pkl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You first need to find the checkpoint located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). It should contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy&#39;s dataclass config).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href=&#34;https://huggingface.co/docs/safetensors/index&#34;&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: A consolidated Hydra training configuration containing the policy, environment, and dataset configs. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. The environment config is useful for anyone who wants to evaluate your policy. The dataset config just serves as a paper trail for reproducibility.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/checkpoint/dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py&#34;&gt;eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; &#xA;&lt;h3&gt;Improve your code with profiling&lt;/h3&gt; &#xA;&lt;p&gt;An example of a code snippet to profile the evaluation of a policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.profiler import profile, record_function, ProfilerActivity&#xA;&#xA;def trace_handler(prof):&#xA;    prof.export_chrome_trace(f&#34;tmp/trace_schedule_{prof.step_num}.json&#34;)&#xA;&#xA;with profile(&#xA;    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],&#xA;    schedule=torch.profiler.schedule(&#xA;        wait=2,&#xA;        warmup=2,&#xA;        active=3,&#xA;    ),&#xA;    on_trace_ready=trace_handler&#xA;) as prof:&#xA;    with record_function(&#34;eval_policy&#34;):&#xA;        for i in range(num_episodes):&#xA;            prof.step()&#xA;            # insert code to profile, potentially whole body of eval_policy function&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{cadene2024lerobot,&#xA;    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},&#xA;    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},&#xA;    howpublished = &#34;\url{https://github.com/huggingface/lerobot}&#34;,&#xA;    year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>hbb1/2d-gaussian-splatting</title>
    <updated>2024-05-10T01:32:12Z</updated>
    <id>tag:github.com,2024-05-10:/hbb1/2d-gaussian-splatting</id>
    <link href="https://github.com/hbb1/2d-gaussian-splatting" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[SIGGRAPH&#39;24] 2D Gaussian Splatting for Geometrically Accurate Radiance Fields&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;2D Gaussian Splatting for Geometrically Accurate Radiance Fields&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://surfsplatting.github.io/&#34;&gt;Project page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2403.17888&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=oaHCtB6yiKU&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://github.com/hbb1/diff-surfel-rasterization&#34;&gt;Surfel Rasterizer (CUDA)&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/drive/1qoclD7HJ3-o0O1R8cvV3PxLhoDCMsH8W?usp=sharing&#34;&gt;Surfel Rasterizer (Python)&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/1SJFgt8qhQomHX55Q4xSvYE2C6-8tFll9&#34;&gt;DTU+COLMAP (3.5GB)&lt;/a&gt; |&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hbb1/2d-gaussian-splatting/main/assets/teaser.jpg&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains the official implementation for the paper &#34;2D Gaussian Splatting for Geometrically Accurate Radiance Fields&#34;. Our work represents a scene with a set of 2D oriented disks (surface elements) and rasterizes the surfels with &lt;a href=&#34;https://colab.research.google.com/drive/1qoclD7HJ3-o0O1R8cvV3PxLhoDCMsH8W?usp=sharing&#34;&gt;perspective correct differentiable raseterization&lt;/a&gt;. Our work also develops regularizations that enhance the reconstruction quality. We also devise meshing approaches for Gaussian splatting.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê New Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024/05/05: Important updates - Now our algorithm supports &lt;strong&gt;unbounded mesh extraction&lt;/strong&gt;! Our key idea is to contract the space into a sphere and then perform &lt;strong&gt;adaptive TSDF truncation&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hbb1/2d-gaussian-splatting/main/assets/unbounded.gif&#34; alt=&#34;visualization&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download&#xA;git clone https://github.com/hbb1/2d-gaussian-splatting.git --recursive&#xA;&#xA;# if you have an environment used for 3dgs, use it&#xA;# if not, create a new environment&#xA;conda env create --file environment.yml&#xA;conda activate surfel_splatting&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train a scene, simply use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py -s &amp;lt;path to COLMAP or NeRF Synthetic dataset&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Commandline arguments for regularizations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--lambda_normal  # hyperparameter for normal consistency&#xA;--lambda_distortion # hyperparameter for depth distortion&#xA;--depth_ratio # 0 for mean depth and 1 for median depth, 0 works for most cases&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tips for adjusting the parameters on your own dataset:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For unbounded/large scenes, we suggest using mean depth, i.e., &lt;code&gt;depth_ratio=0&lt;/code&gt;, for less &#34;disk-aliasing&#34; artefacts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;h3&gt;Bounded Mesh Extraction&lt;/h3&gt; &#xA;&lt;p&gt;To export a mesh within a bounded volume, simply use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python render.py -m &amp;lt;path to pre-trained model&amp;gt; -s &amp;lt;path to COLMAP dataset&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Commandline arguments you should adjust accordingly for meshing for bounded TSDF fusion, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--depth_ratio # 0 for mean depth and 1 for median depth&#xA;--voxel_size # voxel size&#xA;--depth_trunc # depth truncation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Unbounded Mesh Extraction&lt;/h3&gt; &#xA;&lt;p&gt;To export a mesh with an arbitrary size, we devised an unbounded TSDF fusion with space contraction and adaptive truncation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python render.py -m &amp;lt;path to pre-trained model&amp;gt; -s &amp;lt;path to COLMAP dataset&amp;gt; --mesh_res 1024&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick Examples&lt;/h3&gt; &#xA;&lt;p&gt;Assuming you have downloaded MipNeRF360, simply use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py -s &amp;lt;path to m360&amp;gt;/&amp;lt;garden&amp;gt; -m output/m360/garden&#xA;# use our unbounded mesh extraction!!&#xA;python render.py -s &amp;lt;path to m360&amp;gt;/&amp;lt;garden&amp;gt; -m output/m360/garden --unbounded --skip_test --skip_train&#xA;# or use the bounded mesh extraction if you focus on foreground&#xA;python render.py -s &amp;lt;path to m360&amp;gt;/&amp;lt;garden&amp;gt; -m output/m360/garden -depth_trunc 6 --voxel_size 0.008 --skip_test --skip_train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have downloaded the DTU dataset, you can use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py -s &amp;lt;path to dtu&amp;gt;/&amp;lt;scan105&amp;gt; -m output/date/scan105 -r 2 --depth_ratio 1&#xA;python render.py -r 2 --depth_ratio 1 --depth_trunc 3 --voxel_size 0.004 --skip_test --skip_train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom Dataset&lt;/strong&gt;: We use the same COLMAP loader as 3DGS, you can prepare your data following &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting?tab=readme-ov-file#processing-your-own-scenes&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Full evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We provide two scripts to evaluate our method of novel view synthesis and geometric reconstruction. For novel view synthesis on MipNeRF360 (which also works for other colmap datasets), use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/mipnerf_eval.py -m60 &amp;lt;path to the MipNeRF360 dataset&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For geometry reconstruction on DTU dataset, please download the preprocessed &lt;a href=&#34;&#34;&gt;data&lt;/a&gt;. You also need to download the ground truth &lt;a href=&#34;https://roboimagedata.compute.dtu.dk/?page_id=36&#34;&gt;DTU point cloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/dtu_eval.py --dtu &amp;lt;path to the preprocessed DTU dataset&amp;gt;   \&#xA;     --DTU_Official &amp;lt;path to the official DTU dataset&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project is built upon &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3DGS&lt;/a&gt;. The TSDF fusion for extracting mesh is based on &lt;a href=&#34;https://github.com/isl-org/Open3D&#34;&gt;Open3D&lt;/a&gt;. The rendering script for MipNeRF360 is adopted from &lt;a href=&#34;https://github.com/google-research/multinerf/&#34;&gt;Multinerf&lt;/a&gt;, while the evaluation scripts for DTU and Tanks and Temples dataset are taken from &lt;a href=&#34;https://github.com/jzhangbs/DTUeval-python&#34;&gt;DTUeval-python&lt;/a&gt; and &lt;a href=&#34;https://github.com/isl-org/TanksAndTemples/tree/master/python_toolbox/evaluation&#34;&gt;TanksAndTemples&lt;/a&gt;, respectively. We thank all the authors for their great repos.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our code or paper helps, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Huang2DGS2024,&#xA;    title={2D Gaussian Splatting for Geometrically Accurate Radiance Fields},&#xA;    author={Huang, Binbin and Yu, Zehao and Chen, Anpei and Geiger, Andreas and Gao, Shenghua},&#xA;    publisher = {Association for Computing Machinery},&#xA;    booktitle = {SIGGRAPH 2024 Conference Papers},&#xA;    year      = {2024},&#xA;    doi       = {10.1145/3641519.3657428}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>