<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-22T01:40:39Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FDX100/Auto_Tor_IP_changer</title>
    <updated>2024-01-22T01:40:39Z</updated>
    <id>tag:github.com,2024-01-22:/FDX100/Auto_Tor_IP_changer</id>
    <link href="https://github.com/FDX100/Auto_Tor_IP_changer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;change your Ip address automatically This tool based on tor project&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Auto_Tor_IP_changer V 2.1&lt;/h1&gt; &#xA;&lt;p&gt;change your Ip Address automatically This tool based on tor project&lt;/p&gt; &#xA;&lt;p&gt;how to install this tools :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;: requirements:&lt;/p&gt; &lt;p&gt;sudo apt-get install tor pip3 install requests[socks] or just run autoTor it will install everything&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;1: git clone &lt;a href=&#34;https://github.com/FDX100/Auto_Tor_IP_changer.git&#34;&gt;https://github.com/FDX100/Auto_Tor_IP_changer.git&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;2 : cd Auto_Tor_IP_changer&lt;/p&gt; &#xA;&lt;p&gt;3 : python3 install.py&lt;/p&gt; &#xA;&lt;p&gt;4 : interminal type ( aut ) any where you want&lt;/p&gt; &#xA;&lt;p&gt;5 : type time to change IP&lt;/p&gt; &#xA;&lt;p&gt;6: type how many time to change your ip&lt;/p&gt; &#xA;&lt;p&gt;*[0 to infinte IP change]&lt;/p&gt; &#xA;&lt;p&gt;6 : go to your browser / pc change sock proxy to 127.0.0.1:9050&lt;/p&gt; &#xA;&lt;p&gt;7 : BOOOOOOMM&lt;/p&gt; &#xA;&lt;p&gt;============ &lt;a href=&#34;http://facebook.com/ninja.hackerz.kurdish/&#34;&gt;http://facebook.com/ninja.hackerz.kurdish/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sgl-project/sglang</title>
    <updated>2024-01-22T01:40:39Z</updated>
    <id>tag:github.com,2024-01-22:/sgl-project/sglang</id>
    <link href="https://github.com/sgl-project/sglang" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SGLang is a structured generation language designed for large language models (LLMs). It makes your interaction with LLMs faster and more controllable.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SGLang&lt;/h1&gt; &#xA;&lt;p&gt;| &lt;a href=&#34;https://lmsys.org/blog/2024-01-17-sglang/&#34;&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2312.07104&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;p&gt;SGLang is a structured generation language designed for large language models (LLMs). It makes your interaction with LLMs faster and more controllable by co-designing the frontend language and the runtime system.&lt;/p&gt; &#xA;&lt;p&gt;The core features of SGLang include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Flexible Front-End Language&lt;/strong&gt;: This allows for easy programming of LLM applications with multiple chained generation calls, advanced prompting techniques, control flow, multiple modalities, parallelism, and external interaction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A High-Performance Runtime with RadixAttention&lt;/strong&gt;: This feature significantly accelerates the execution of complex LLM programs by automatic KV cache reuse across multiple calls. It also supports other common techniques like continuous batching and tensor parallelism.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/#frontend-structured-generation-language-sglang&#34;&gt;Frontend: Structured Generation Language (SGLang)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/#backend-sglang-runtime-srt&#34;&gt;Backend: SGLang Runtime (SRT)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/#benchmark-and-performance&#34;&gt;Benchmark And Performance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/#citation-and-acknowledgment&#34;&gt;Citation And Acknowledgment&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Method 1: With pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;sglang[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Method 2: From source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:sgl-project/sglang.git&#xA;cd sglang&#xA;&#xA;pip install --upgrade pip&#xA;pip install -e &#34;python[all]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Notes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are using older GPUs (NVIDIA V100, T4), please pick the correct triton compiler version to avoid some known bugs. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For NVIDIA T4, please use &lt;code&gt;pip install &#34;triton&amp;gt;=2.2.0&#34;&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For NVIDIA V100, please install the &lt;a href=&#34;https://triton-lang.org/main/getting-started/installation.html&#34;&gt;nightly&lt;/a&gt; version.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you only need to use the OpenAI backend, you can avoid installing other dependencies by using &lt;code&gt;pip install sglang[openai]&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;The example below shows how to use sglang to answer a mulit-turn question.&lt;/p&gt; &#xA;&lt;h3&gt;Using OpenAI Models&lt;/h3&gt; &#xA;&lt;p&gt;Set the OpenAI API Key&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=sk-******&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, answer a multi-turn question.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sglang import function, system, user, assistant, gen, set_default_backend, OpenAI&#xA;&#xA;@function&#xA;def multi_turn_question(s, question_1, question_2):&#xA;    s += system(&#34;You are a helpful assistant.&#34;)&#xA;    s += user(question_1)&#xA;    s += assistant(gen(&#34;answer_1&#34;, max_tokens=256))&#xA;    s += user(question_2)&#xA;    s += assistant(gen(&#34;answer_2&#34;, max_tokens=256))&#xA;&#xA;set_default_backend(OpenAI(&#34;gpt-3.5-turbo&#34;))&#xA;&#xA;state = multi_turn_question.run(&#xA;    question_1=&#34;What is the capital of the United States?&#34;,&#xA;    question_2=&#34;List two local attractions.&#34;,&#xA;)&#xA;&#xA;for m in state.messages():&#xA;    print(m[&#34;role&#34;], &#34;:&#34;, m[&#34;content&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Local Models&lt;/h3&gt; &#xA;&lt;p&gt;First, launch a server with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, connect to the server and answer a multi-turn question.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint&#xA;&#xA;@function&#xA;def multi_turn_question(s, question_1, question_2):&#xA;    s += system(&#34;You are a helpful assistant.&#34;)&#xA;    s += user(question_1)&#xA;    s += assistant(gen(&#34;answer_1&#34;, max_tokens=256))&#xA;    s += user(question_2)&#xA;    s += assistant(gen(&#34;answer_2&#34;, max_tokens=256))&#xA;&#xA;set_default_backend(RuntimeEndpoint(&#34;http://localhost:30000&#34;))&#xA;&#xA;state = multi_turn_question.run(&#xA;    question_1=&#34;What is the capital of the United States?&#34;,&#xA;    question_2=&#34;List two local attractions.&#34;,&#xA;)&#xA;&#xA;for m in state.messages():&#xA;    print(m[&#34;role&#34;], &#34;:&#34;, m[&#34;content&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;More Examples&lt;/h3&gt; &#xA;&lt;p&gt;Anthropic and VertexAI (Gemini) models are also supported. You can find more examples at &lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/examples/quick_start&#34;&gt;examples/quick_start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Frontend: Structured Generation Language (SGLang)&lt;/h2&gt; &#xA;&lt;p&gt;To begin with, import sglang.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sglang as sgl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;sglang&lt;/code&gt; provides some simple primitives such as &lt;code&gt;gen&lt;/code&gt;, &lt;code&gt;select&lt;/code&gt;, &lt;code&gt;fork&lt;/code&gt;, &lt;code&gt;image&lt;/code&gt;. You can implement your prompt flow in a function decorated by &lt;code&gt;sgl.function&lt;/code&gt;. You can then invoke the function with &lt;code&gt;run&lt;/code&gt; or &lt;code&gt;run_batch&lt;/code&gt;. The system will manage the state, chat template, and parallelism for you.&lt;/p&gt; &#xA;&lt;h3&gt;Control Flow&lt;/h3&gt; &#xA;&lt;p&gt;You can use any Python code within the function body, including control flow, nested function calls, and external libraries.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@sgl.function&#xA;def control_flow(s, question):&#xA;    s += &#34;To answer this question: &#34; + question + &#34;, &#34;&#xA;    s += &#34;I need to use a &#34; + sgl.gen(&#34;tool&#34;, choices=[&#34;calculator&#34;, &#34;web browser&#34;]) + &#34;. &#34;&#xA;&#xA;    if s[&#34;tool&#34;] == &#34;calculator&#34;:&#xA;        s += &#34;The math expression is&#34; + sgl.gen(&#34;expression&#34;)&#xA;    elif s[&#34;tool&#34;] == &#34;web browser&#34;:&#xA;        s += &#34;The website url is&#34; + sgl.gen(&#34;url&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Parallelism&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;fork&lt;/code&gt; to launch parallel prompts. Because &lt;code&gt;sgl.gen&lt;/code&gt; is non-blocking, the for loop below issues two generation calls in parallel.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@sgl.function&#xA;def tip_suggestion(s):&#xA;    s += (&#xA;        &#34;Here are two tips for staying healthy: &#34;&#xA;        &#34;1. Balanced Diet. 2. Regular Exercise.\n\n&#34;&#xA;    )&#xA;&#xA;    forks = s.fork(2)&#xA;    for i, f in enumerate(forks):&#xA;        f += f&#34;Now, expand tip {i+1} into a paragraph:\n&#34;&#xA;        f += sgl.gen(f&#34;detailed_tip&#34;, max_tokens=256, stop=&#34;\n\n&#34;)&#xA;&#xA;    s += &#34;Tip 1:&#34; + forks[0][&#34;detailed_tip&#34;] + &#34;\n&#34;&#xA;    s += &#34;Tip 2:&#34; + forks[1][&#34;detailed_tip&#34;] + &#34;\n&#34;&#xA;    s += &#34;In summary&#34; + sgl.gen(&#34;summary&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Multi Modality&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;sgl.image&lt;/code&gt; to pass an image as input.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@sgl.function&#xA;def image_qa(s, image_file, question):&#xA;    s += sgl.user(sgl.image(image_file) + question)&#xA;    s += sgl.assistant(sgl.gen(&#34;answer&#34;, max_tokens=256)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Constrained Decoding&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;regex&lt;/code&gt; to specify a regular expression as a decoding constraint. This is only supported for local models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@sgl.function&#xA;def regular_expression_gen(s):&#xA;    s += &#34;Q: What is the IP address of the Google DNS servers?\n&#34;&#xA;    s += &#34;A: &#34; + sgl.gen(&#xA;        &#34;answer&#34;,&#xA;        temperature=0,&#xA;        regex=r&#34;((25[0-5]|2[0-4]\d|[01]?\d\d?).){3}(25[0-5]|2[0-4]\d|[01]?\d\d?)&#34;,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Batching&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;run_batch&lt;/code&gt; to run a batch of requests with continuous batching.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@sgl.function&#xA;def text_qa(s, question):&#xA;    s += &#34;Q: &#34; + question + &#34;\n&#34;&#xA;    s += &#34;A:&#34; + sgl.gen(&#34;answer&#34;, stop=&#34;\n&#34;)&#xA;&#xA;states = text_qa.run_batch(&#xA;    [&#xA;        {&#34;question&#34;: &#34;What is the capital of the United Kingdom?&#34;},&#xA;        {&#34;question&#34;: &#34;What is the capital of France?&#34;},&#xA;        {&#34;question&#34;: &#34;What is the capital of Japan?&#34;},&#xA;    ],&#xA;    progress_bar=True&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Streaming&lt;/h3&gt; &#xA;&lt;p&gt;Add &lt;code&gt;stream=True&lt;/code&gt; to enable streaming.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@sgl.function&#xA;def text_qa(s, question):&#xA;    s += &#34;Q: &#34; + question + &#34;\n&#34;&#xA;    s += &#34;A:&#34; + sgl.gen(&#34;answer&#34;, stop=&#34;\n&#34;)&#xA;&#xA;states = text_qa.run(&#xA;    question=&#34;What is the capital of France?&#34;,&#xA;    temperature=0.1,&#xA;    stream=True&#xA;)&#xA;&#xA;for out in state.text_iter():&#xA;    print(out, end=&#34;&#34;, flush=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Backend: SGLang Runtime (SRT)&lt;/h2&gt; &#xA;&lt;p&gt;The SGLang Runtime (SRT) is designed to work best with the SGLang frontend. However, it can also be used as a standalone API server. In this case, the &lt;a href=&#34;https://arxiv.org/abs/2312.07104&#34;&gt;RadixAttention&lt;/a&gt; can still greatly accelerate many use cases with automatic KV cache reuse.&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Launch a server&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Send a request&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl http://localhost:30000/generate \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;    &#34;text&#34;: &#34;Once upon a time,&#34;,&#xA;    &#34;parameters&#34;: {&#xA;      &#34;max_new_tokens&#34;: 16,&#xA;      &#34;temperature&#34;: 0&#xA;    }&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Learn more about the argument format &lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/docs/sampling_params.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;OpenAI Compatible API&lt;/h3&gt; &#xA;&lt;p&gt;In addition, the server supports an experimental OpenAI-compatible API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;client = openai.Client(&#xA;    base_url=&#34;http://127.0.0.1:30000/v1&#34;, api_key=&#34;EMPTY&#34;)&#xA;&#xA;# Text completion&#xA;response = client.completions.create(&#xA;&#x9;model=&#34;default&#34;,&#xA;&#x9;prompt=&#34;The capital of France is&#34;,&#xA;&#x9;temperature=0,&#xA;&#x9;max_tokens=32,&#xA;)&#xA;print(response)&#xA;&#xA;# Chat completion&#xA;response = client.chat.completions.create(&#xA;    model=&#34;default&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful AI assistant&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;List 3 countries and their capitals.&#34;},&#xA;    ],&#xA;    temperature=0,&#xA;    max_tokens=64,&#xA;)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In above example, the server uses the chat template specified in the model tokenizer. You can override the chat template if needed when launching the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --chat-template llama-2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the chat template you are looking for is missing, you are welcome to contribute it. Meanwhile, you can also temporary register your chat template as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;name&#34;: &#34;my_model&#34;,&#xA;  &#34;system&#34;: &#34;&amp;lt;|im_start|&amp;gt;system&#34;,&#xA;  &#34;user&#34;: &#34;&amp;lt;|im_start|&amp;gt;user&#34;,&#xA;  &#34;assistant&#34;: &#34;&amp;lt;|im_start|&amp;gt;assistant&#34;,&#xA;  &#34;sep_style&#34;: &#34;CHATML&#34;,&#xA;  &#34;sep&#34;: &#34;&amp;lt;|im_end|&amp;gt;&#34;,&#xA;  &#34;stop_str&#34;: [&#34;&amp;lt;|im_end|&amp;gt;&#34;, &#34;&amp;lt;|im_start|&amp;gt;&#34;]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --chat-template ./my_model_template.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional Arguments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--tp 2&lt;/code&gt; to enable tensor parallelism.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --tp 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you see out-of-memory errors during serving, please try to reduce the memory usage of the KV cache pool by setting a smaller value of &lt;code&gt;--mem-fraction-static&lt;/code&gt;. The default value is &lt;code&gt;0.9&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --mem-fraction-static 0.7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Supported Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Llama&lt;/li&gt; &#xA; &lt;li&gt;Mistral&lt;/li&gt; &#xA; &lt;li&gt;Mixtral&lt;/li&gt; &#xA; &lt;li&gt;LLaVA &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;AWQ quantization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark And Performance&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Llama-7B on NVIDIA A10G, FP16, Tensor Parallelism=1 &lt;img src=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/assets/llama_7b.jpg&#34; alt=&#34;llama_7b&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mixtral-8x7B on NVIDIA A10G, FP16, Tensor Parallelism=8 &lt;img src=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/assets/mixtral_8x7b.jpg&#34; alt=&#34;mixtral_8x7b&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more &lt;a href=&#34;https://raw.githubusercontent.com/sgl-project/sglang/main/docs/benchmark_results.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Function call APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; S-LoRA (expect by Feb. 5)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support more models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support more hardware backends&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation And Acknowledgment&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zheng2023efficiently,&#xA;      title={Efficiently Programming Large Language Models using SGLang},&#xA;      author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Jeff Huang and Chuyue Sun and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},&#xA;      year={2023},&#xA;      eprint={2312.07104},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We learned from the design and reused some code of the following projects: &lt;a href=&#34;https://github.com/guidance-ai/guidance&#34;&gt;Guidance&lt;/a&gt;, &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;, &lt;a href=&#34;https://github.com/ModelTC/lightllm&#34;&gt;LightLLM&lt;/a&gt;, &lt;a href=&#34;https://github.com/flashinfer-ai/flashinfer&#34;&gt;FlashInfer&lt;/a&gt;, &lt;a href=&#34;https://github.com/outlines-dev/outlines&#34;&gt;Outlines&lt;/a&gt;, &lt;a href=&#34;https://github.com/eth-sri/lmql&#34;&gt;LMQL&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-deepmind/alphageometry</title>
    <updated>2024-01-22T01:40:39Z</updated>
    <id>tag:github.com,2024-01-22:/google-deepmind/alphageometry</id>
    <link href="https://github.com/google-deepmind/alphageometry" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Solving Olympiad Geometry without Human Demonstrations&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code necessary to reproduce DDAR and AlphaGeometry, the two geometry theorem provers introduced in the &lt;a href=&#34;https://www.nature.com/articles/s41586-023-06747-5&#34;&gt;Nature 2024&lt;/a&gt; paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA; &lt;em&gt;&#34;Solving Olympiad Geometry without Human Demonstrations&#34;.&lt;/em&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;center&gt; &#xA; &lt;img alt=&#34;fig1&#34; width=&#34;800px&#34; src=&#34;https://raw.githubusercontent.com/google-deepmind/alphageometry/main/fig1.svg?sanitize=true&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;For the instructions presented below, we use Python 3.10.9, and dependencies with their exact version numbers listed in &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Our code depends on &lt;code&gt;meliad&lt;/code&gt;, which is not a registered package with &lt;code&gt;pip&lt;/code&gt;. See instructions below for how to manually install &lt;code&gt;meliad&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that one can still run the DDAR solver without the &lt;code&gt;meliad&lt;/code&gt; and &lt;code&gt;sentencepiece&lt;/code&gt; dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Run the instructions&lt;/h2&gt; &#xA;&lt;p&gt;All instructions in this &lt;code&gt;README.md&lt;/code&gt; can be run in one go by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Below, we explain these instructions step-by-step.&lt;/p&gt; &#xA;&lt;h2&gt;Install dependencies, download weights and vocabulary.&lt;/h2&gt; &#xA;&lt;p&gt;Installation is done in a virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;virtualenv -p python3 .&#xA;source ./bin/activate&#xA;pip install --require-hashes -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download weights and vocabulary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download.sh&#xA;DATA=ag_ckpt_vocab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, install &lt;code&gt;meliad&lt;/code&gt; separately as it is not registered with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MELIAD_PATH=meliad_lib/meliad&#xA;mkdir -p $MELIAD_PATH&#xA;git clone https://github.com/google-research/meliad $MELIAD_PATH&#xA;export PYTHONPATH=$PYTHONPATH:$MELIAD_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Set up common flags&lt;/h2&gt; &#xA;&lt;p&gt;Before running the python scripts, let us first prepare some commonly used flags. The symbolic engine needs definitions and deduction rules to operate. These definitions and rules are provided in two text files &lt;code&gt;defs.txt&lt;/code&gt; and &lt;code&gt;rules.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;DDAR_ARGS=(&#xA;  --defs_file=$(pwd)/defs.txt \&#xA;  --rules_file=$(pwd)/rules.txt \&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we define the flags relevant to the proof search. To reproduce the simple examples below, we use lightweight values for the proof search parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;BATCH_SIZE=2&#xA;BEAM_SIZE=2&#xA;DEPTH=2&#xA;&#xA;SEARCH_ARGS=(&#xA;  --beam_size=$BEAM_SIZE&#xA;  --search_depth=$DEPTH&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: The results in our paper can be obtained by setting &lt;code&gt;BATCH_SIZE=32&lt;/code&gt;, &lt;code&gt;BEAM_SIZE=512&lt;/code&gt;, &lt;code&gt;DEPTH=16&lt;/code&gt; as described in section Methods. To stay under IMO time limits, 4 V100-GPUs and 250 CPU workers are needed as shown in Extended Data - Figure 1. Note that we also strip away other memory/speed optimizations due to internal dependencies and to promote code clarity.&lt;/p&gt; &#xA;&lt;p&gt;Assume the downloaded checkpoint and vocabulary is placed in &lt;code&gt;DATA&lt;/code&gt;, and the installed &lt;code&gt;meliad&lt;/code&gt; source code is at &lt;code&gt;MELIAD_PATH&lt;/code&gt;. We make use of the &lt;code&gt;gin&lt;/code&gt; library to manage model configurations, following &lt;code&gt;meliad&lt;/code&gt; conventions. We now define the flags relevant to the language model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;LM_ARGS=(&#xA;  --ckpt_path=$DATA \&#xA;  --vocab_path=$DATA/geometry.757.model&#xA;  --gin_search_paths=$MELIAD_PATH/transformer/configs,$(pwd) \&#xA;  --gin_file=base_htrans.gin \&#xA;  --gin_file=size/medium_150M.gin \&#xA;  --gin_file=options/positions_t5.gin \&#xA;  --gin_file=options/lr_cosine_decay.gin \&#xA;  --gin_file=options/seq_1024_nocache.gin \&#xA;  --gin_file=geometry_150M_generate.gin \&#xA;  --gin_param=DecoderOnlyLanguageModelGenerate.output_token_losses=True \&#xA;  --gin_param=TransformerTaskConfig.batch_size=$BATCH_SIZE \&#xA;  --gin_param=TransformerTaskConfig.sequence_length=128 \&#xA;  --gin_param=Trainer.restore_state_variables=False&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;TIP: Note that you can still run the DDAR solver without defining &lt;code&gt;SEARCH_ARGS&lt;/code&gt; and &lt;code&gt;LM_ARGS&lt;/code&gt;. In such case, simply disable the import of the &lt;code&gt;lm_inference&lt;/code&gt; module inside &lt;code&gt;alphageometry.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run DDAR&lt;/h2&gt; &#xA;&lt;p&gt;The script loads a problem by reading a list of problems from a text file and solves the specific problem in the list according to its name. We pass these two pieces of information through the flags &lt;code&gt;--problems_file&lt;/code&gt; and &lt;code&gt;--problem_name&lt;/code&gt;. We use &lt;code&gt;--mode=ddar&lt;/code&gt; to indicate that we want to use the DDAR solver.&lt;/p&gt; &#xA;&lt;p&gt;Below we showed this solver solving IMO 2000 P1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m alphageometry \&#xA;--alsologtostderr \&#xA;--problems_file=$(pwd)/imo_ag_30.txt \&#xA;--problem_name=translated_imo_2000_p1 \&#xA;--mode=ddar \&#xA;&#34;${DDAR_ARGS[@]}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Expect the following output&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;graph.py:468] translated_imo_2000_p1&#xA;graph.py:469] a b = segment a b; g1 = on_tline g1 a a b; g2 = on_tline g2 b b a; m = on_circle m g1 a, on_circle m g2 b; n = on_circle n g1 a, on_circle n g2 b; c = on_pline c m a b, on_circle c g1 a; d = on_pline d m a b, on_circle d g2 b; e = on_line e a c, on_line e b d; p = on_line p a n, on_line p c d; q = on_line q b n, on_line q c d ? cong e p e q&#xA;ddar.py:41] Depth 1/1000 time = 1.7772269248962402&#xA;ddar.py:41] Depth 2/1000 time = 5.63526177406311&#xA;ddar.py:41] Depth 3/1000 time = 6.883412837982178&#xA;ddar.py:41] Depth 4/1000 time = 10.275688409805298&#xA;ddar.py:41] Depth 5/1000 time = 12.048273086547852&#xA;alphageometry.py:190]&#xA;==========================&#xA; * From theorem premises:&#xA;A B G1 G2 M N C D E P Q : Points&#xA;AG_1 ⟂ AB [00]&#xA;BA ⟂ G_2B [01]&#xA;G_2M = G_2B [02]&#xA;G_1M = G_1A [03]&#xA;&#xA;...&#xA;[log omitted]&#xA;...&#xA;&#xA;036. ∠QEB = ∠(QP-EA) [46] &amp;amp; ∠(BE-QP) = ∠AEP [55] ⇒  ∠EQP = ∠QPE [56]&#xA;037. ∠PQE = ∠EPQ [56] ⇒  EP = EQ&#xA;&#xA;==========================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output first includes a list of relevant premises that it uses, and then proof steps that gradually build up the proof. All predicates are numbered to track how they are derived from the premises, and to show that the proof is fully justified.&lt;/p&gt; &#xA;&lt;p&gt;TIP: Additionally passing the flag &lt;code&gt;--out_file=path/to/output/text/file.txt&lt;/code&gt; will write the proof to a text file.&lt;/p&gt; &#xA;&lt;p&gt;Running on all problems in &lt;code&gt;imo_ag_30.txt&lt;/code&gt; will yield solutions to 14 of them, as reported in Table 1 in our paper.&lt;/p&gt; &#xA;&lt;h2&gt;Run AlphaGeometry:&lt;/h2&gt; &#xA;&lt;p&gt;As a simple example, we load &lt;code&gt;--problem_name=orthocenter&lt;/code&gt; from &lt;code&gt;--problem_file=examples.txt&lt;/code&gt;. This time, we pass &lt;code&gt;--mode=alphageometry&lt;/code&gt; to use the AlphaGeometry solver and pass the &lt;code&gt;SEARCH_ARGS&lt;/code&gt; and &lt;code&gt;LM_ARGS&lt;/code&gt; flags.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m alphageometry \&#xA;--alsologtostderr \&#xA;--problems_file=$(pwd)/examples.txt \&#xA;--problem_name=orthocenter \&#xA;--mode=alphageometry \&#xA;&#34;${DDAR_ARGS[@]}&#34; \&#xA;&#34;${SEARCH_ARGS[@]}&#34; \&#xA;&#34;${LM_ARGS[@]}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Expect the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;...&#xA;[log omitted]&#xA;...&#xA;training_loop.py:725] Total parameters: 152072288&#xA;training_loop.py:739] Total state size: 0&#xA;training_loop.py:492] Training loop: creating task for mode beam_search&#xA;&#xA;graph.py:468] orthocenter&#xA;graph.py:469] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b ? perp a d b c&#xA;ddar.py:41] Depth 1/1000 time = 0.009987592697143555 branch = 4&#xA;ddar.py:41] Depth 2/1000 time = 0.00672602653503418 branch = 0&#xA;alphageometry.py:221] DD+AR failed to solve the problem.&#xA;alphageometry.py:457] Depth 0. There are 1 nodes to expand:&#xA;alphageometry.py:460] {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00&#xA;alphageometry.py:465] Decoding from {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00&#xA;...&#xA;[log omitted]&#xA;...&#xA;alphageometry.py:470] LM output (score=-1.102287): &#34;e : C a c e 02 C b d e 03 ;&#34;&#xA;alphageometry.py:471] Translation: &#34;e = on_line e a c, on_line e b d&#34;&#xA;&#xA;alphageometry.py:480] Solving: &#34;a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c&#34;&#xA;graph.py:468]&#xA;graph.py:469] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c&#xA;ddar.py:41] Depth 1/1000 time = 0.021120786666870117&#xA;ddar.py:41] Depth 2/1000 time = 0.033370018005371094&#xA;ddar.py:41] Depth 3/1000 time = 0.04297471046447754&#xA;alphageometry.py:140]&#xA;==========================&#xA; * From theorem premises:&#xA;A B C D : Points&#xA;BD ⟂ AC [00]&#xA;CD ⟂ AB [01]&#xA;&#xA; * Auxiliary Constructions:&#xA;E : Points&#xA;E,B,D are collinear [02]&#xA;E,C,A are collinear [03]&#xA;&#xA; * Proof steps:&#xA;001. E,B,D are collinear [02] &amp;amp; E,C,A are collinear [03] &amp;amp; BD ⟂ AC [00] ⇒  ∠BEA = ∠CED [04]&#xA;002. E,B,D are collinear [02] &amp;amp; E,C,A are collinear [03] &amp;amp; BD ⟂ AC [00] ⇒  ∠BEC = ∠AED [05]&#xA;003. A,E,C are collinear [03] &amp;amp; E,B,D are collinear [02] &amp;amp; AC ⟂ BD [00] ⇒  EC ⟂ EB [06]&#xA;004. EC ⟂ EB [06] &amp;amp; CD ⟂ AB [01] ⇒  ∠(EC-BA) = ∠(EB-CD) [07]&#xA;005. E,C,A are collinear [03] &amp;amp; E,B,D are collinear [02] &amp;amp; ∠(EC-BA) = ∠(EB-CD) [07] ⇒  ∠BAE = ∠CDE [08]&#xA;006. ∠BEA = ∠CED [04] &amp;amp; ∠BAE = ∠CDE [08] (Similar Triangles)⇒  EB:EC = EA:ED [09]&#xA;007. EB:EC = EA:ED [09] &amp;amp; ∠BEC = ∠AED [05] (Similar Triangles)⇒  ∠BCE = ∠ADE [10]&#xA;008. EB:EC = EA:ED [09] &amp;amp; ∠BEC = ∠AED [05] (Similar Triangles)⇒  ∠EBC = ∠EAD [11]&#xA;009. ∠BCE = ∠ADE [10] &amp;amp; E,C,A are collinear [03] &amp;amp; E,B,D are collinear [02] &amp;amp; ∠EBC = ∠EAD [11] ⇒  AD ⟂ BC&#xA;==========================&#xA;&#xA;alphageometry.py:505] Solved.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: Point &lt;code&gt;H&lt;/code&gt; is automatically renamed to &lt;code&gt;D&lt;/code&gt;, as the LM is trained on synthetic problems where the points are named alphabetically, and so it expects the same during test time.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: In this implementation of AlphaGeometry, we removed all optimizations that are dependent on internal infrastructure, e.g., parallelized model inference on multi GPUs, parallelized DDAR on multiple CPUs, parallel execution of LM and DDAR, shared pool of CPU workers across different problems, etc. We also removed some memory/speed optimizations and code abstractions in favor of code clarity.&lt;/p&gt; &#xA;&lt;p&gt;As can be seen in the output, initially DDAR failed to solve the problem. The LM proposes two auxiliary constructions (because &lt;code&gt;BATCH_SIZE=2&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;e = eqdistance e c a b, eqdistance e b a c&lt;/code&gt;, i.e., construct &lt;code&gt;E&lt;/code&gt; as the intersection of circle (center=C, radius=AB) and circle (center=B, radius=AC). This construction has a score of &lt;code&gt;-1.186&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;e = on_line e a c, on_line e b d&lt;/code&gt;, i.e., &lt;code&gt;E&lt;/code&gt; is the intersection of &lt;code&gt;AC&lt;/code&gt; and &lt;code&gt;BD&lt;/code&gt;. This construction has a higher score (&lt;code&gt;-1.102287&lt;/code&gt;) than the previous.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Since the second construction has a higher score, DDAR attempted the second construction first and found the solution right away. The proof search therefore terminates and there is no second iteration.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Before attempting to reproduce the AlphaGeometry numbers in our paper, please make sure to pass all tests in the prepared test suite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash run_tests.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: &lt;a href=&#34;https://github.com/google-deepmind/alphageometry/issues/14&#34;&gt;Issues#14&lt;/a&gt; reports that although the top beam decodes are still the same, the LM is not giving the same score for different users.&lt;/p&gt; &#xA;&lt;p&gt;Then, pass the corresponding values for &lt;code&gt;--problem_file&lt;/code&gt; (column) and &lt;code&gt;--mode&lt;/code&gt; (row), and iterate on all problems to obtain the following results:&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;p&gt;&lt;b&gt;Number of solved problems:&lt;/b&gt;&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;imo_ag_30.txt&lt;/code&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;jgex_ag_231.txt&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;ddar&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;14&lt;/td&gt; &#xA;    &lt;td&gt;198&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;code&gt;alphageometry&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;25&lt;/td&gt; &#xA;    &lt;td&gt;228&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/center&gt; &#xA;&lt;h2&gt;Source code description&lt;/h2&gt; &#xA;&lt;p&gt;Files in this repository include python modules/scripts to run the solvers and resource files necessary for the script to execute. We listed below each of them and their description.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;File name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;geometry.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements nodes (Point, Line, Circle, etc) in the proof state graph.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;numericals.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the numerical engine in the dynamic geometry environment.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;graph_utils.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements utilities for the proof state graph.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;graph.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the proof state graph.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;problem.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the classes that represent the problem premises, conclusion, DAG nodes.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dd.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements DD and its traceback.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ar.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements AR and its traceback.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;trace_back.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the recursive traceback and dependency difference algorithm.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ddar.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the combination DD+AR.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;beam_search.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements beam decoding of a language model in JAX.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;models.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the transformer model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;transformer_layer.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the transformer layer.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;decoder_stack.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the transformer decoder stack.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;lm_inference.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements an interface to a trained LM to perform decoding.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;alphageometry.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main script that loads problems, calls DD+AR or AlphaGeometry solver, and prints solutions.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;pretty.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pretty formating the solutions output by solvers.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;*_test.py&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tests for the corresponding module.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;download.sh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Script to download model checkpoints and LM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;run.sh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Script to execute instructions in README.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;run_tests.sh&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Script to execute the test suite.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Resource files:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resource file name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;defs.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Definitions of different geometric construction actions.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;rules.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deduction rules for DD.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;geometry_150M_generate.gin&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Gin config of the LM implemented in meliad.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;imo_ag_30.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Problems in IMO-AG-30.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;jgex_ag_231.txt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Problems in JGEX-AG-231.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citing this work&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Article{AlphaGeometryTrinh2024,&#xA;  author  = {Trinh, Trieu and Wu, Yuhuai and Le, Quoc and He, He and Luong, Thang},&#xA;  journal = {Nature},&#xA;  title   = {Solving Olympiad Geometry without Human Demonstrations},&#xA;  year    = {2024},&#xA;  doi     = {10.1038/s41586-023-06747-5}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This research is a collaboration between the Google Brain team (now Google Deepmind) and the Computer Science Department of New York University. We thank Rif A. Saurous, Denny Zhou, Christian Szegedy, Delesley Hutchins, Thomas Kipf, Hieu Pham, Petar Veličković, Debidatta Dwibedi, Kyunghyun Cho, Lerrel Pinto, Alfredo Canziani, Thomas Wies, He He’s research group, Evan Chen (the USA’s IMO team coach), Mirek Olsak, Patrik Bak, and all three Nature&#39;s referees for their help and support.&lt;/p&gt; &#xA;&lt;p&gt;The code of AlphaGeometry communicates with and/or references the following separate libraries and packages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/abseil/abseil-py&#34;&gt;Abseil&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/jax/&#34;&gt;JAX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scipy.org&#34;&gt;SciPy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/meliad&#34;&gt;Meliad&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/flax&#34;&gt;Flax&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/gin-config&#34;&gt;Gin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer&#34;&gt;T5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/sentencepiece&#34;&gt;SentencePiece&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We thank all their contributors and maintainers!&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; &#xA;&lt;p&gt;This research code is provided &#34;as-is&#34; to the broader research community. Google does not promise to maintain or otherwise support this code in any way.&lt;/p&gt; &#xA;&lt;h2&gt;Code License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2023 DeepMind Technologies Limited&lt;/p&gt; &#xA;&lt;p&gt;All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license. You may obtain a copy of the Apache 2.0 license at: &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY). You may obtain a copy of the CC-BY license at: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the licenses for the specific language governing permissions and limitations under those licenses.&lt;/p&gt; &#xA;&lt;h2&gt;Model Parameters License&lt;/h2&gt; &#xA;&lt;p&gt;The AlphaGeometry checkpoints and vocabulary are made available under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license. You can find details at: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>