<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-10T01:45:10Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xinyu1205/Recognize_Anything-Tag2Text</title>
    <updated>2023-06-10T01:45:10Z</updated>
    <id>tag:github.com,2023-06-10:/xinyu1205/Recognize_Anything-Tag2Text</id>
    <link href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the Recognize Anything Model and Tag2Text Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;span&gt;üè∑&lt;/span&gt; Recognize Anything: A Strong Image Tagging Model &amp;amp; Tag2Text: Guiding Vision-Language Model via Image Tagging&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch Implementation of the &lt;a href=&#34;https://recognize-anything.github.io/&#34;&gt;Recognize Anything Model (RAM)&lt;/a&gt; and the &lt;a href=&#34;https://tag2text.github.io/&#34;&gt;Tag2Text Model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RAM is a strong image tagging model, which can recognize any common category with high accuracy.&lt;/li&gt; &#xA; &lt;li&gt;Tag2Text is an efficient and controllable vision-language model with tagging guidance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When combined with localization models (&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-SAM&lt;/a&gt;), Tag2Text and RAM form a strong and general pipeline for visual semantic analysis.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/Recognize_Anything-Tag2Text/main/images/ram_grounded_sam.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåû&lt;/span&gt; Helpful Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;üçÑ&lt;/span&gt; [&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;Try our RAM &amp;amp; Tag2Text web Demo! ü§ó&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üçé&lt;/span&gt; [&lt;a href=&#34;https://recognize-anything.github.io/&#34;&gt;Access RAM Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üçá&lt;/span&gt; [&lt;a href=&#34;https://tag2text.github.io/&#34;&gt;Access Tag2Text Homepage&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üåª&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/abs/2306.03514&#34;&gt;Read RAM arXiv Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üåπ&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/abs/2303.05657&#34;&gt;Read Tag2Text arXiv Paper&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üí°&lt;/span&gt; Highlight&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recognition and localization are two foundation computer vision tasks.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Segment Anything Model (SAM)&lt;/strong&gt; excels in &lt;strong&gt;localization capabilities&lt;/strong&gt;, while it falls short when it comes to &lt;strong&gt;recognition tasks&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The Recognize Anything Model (RAM) and Tag2Text&lt;/strong&gt; exhibits &lt;strong&gt;exceptional recognition abilities&lt;/strong&gt;, in terms of &lt;strong&gt;both accuracy and scope&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table class=&#34;tg&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/Recognize_Anything-Tag2Text/main/images/localization_and_recognition.jpg&#34; align=&#34;center&#34; width=&#34;800&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Tag2Text for Vision-Language Tasks. &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Tagging.&lt;/strong&gt; Without manual annotations, Tag2Text achieves &lt;strong&gt;superior&lt;/strong&gt; image tag recognition ability of &lt;a href=&#34;https://raw.githubusercontent.com/xinyu1205/Recognize_Anything-Tag2Text/main/data/tag_list.txt&#34;&gt;&lt;strong&gt;3,429&lt;/strong&gt;&lt;/a&gt; commonly human-used categories.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Efficient.&lt;/strong&gt; Tagging guidance effectively enhances the performance of vision-language models on both &lt;strong&gt;generation-based&lt;/strong&gt; and &lt;strong&gt;alignment-based&lt;/strong&gt; tasks.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Controllable.&lt;/strong&gt; Tag2Text permits users to input &lt;strong&gt;desired tags&lt;/strong&gt;, providing the flexibility in composing corresponding texts based on the input tags.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;table class=&#34;tg&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/Recognize_Anything-Tag2Text/main/images/tag2text_framework.png&#34; align=&#34;center&#34; width=&#34;800&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Advancements of RAM on Tag2Text. &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Accuracy.&lt;/strong&gt; RAM utilizes a data engine to generate additional annotations and clean incorrect ones, resulting higher accuracy compared to Tag2Text.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Scope.&lt;/strong&gt; Tag2Text recognizes 3,400+ fixed tags. RAM upgrades the number to 6,400+, covering more valuable categories. With open-set capability, RAM is feasible to recognize any common category.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA; &lt;table class=&#34;tg&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;td class=&#34;tg-c3ow&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinyu1205/Recognize_Anything-Tag2Text/main/images/tagging_results.jpg&#34; align=&#34;center&#34; width=&#34;800&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ú®&lt;/span&gt; Highlight Projects with other Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Tag2Text/RAM with Grounded-SAM&lt;/a&gt; is trong and general pipeline for visual semantic analysis, which can automatically &lt;strong&gt;recognize&lt;/strong&gt;, detect, and segment for an image!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything&#34;&gt;Ask-Anything&lt;/a&gt; is a multifunctional video question answering tool. Tag2Text provides powerful tagging and captioning capabilities as a fundamental component.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/positive666/Prompt-Can-Anything&#34;&gt;Prompt-can-anything&lt;/a&gt; is a gradio web library that integrates SOTA multimodal large models, including Tag2text as the core model for graphic understanding&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üî•&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/08&lt;/code&gt;&lt;/strong&gt;: We release the &lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;Recognize Anything Model (RAM) Tag2Text web demo ü§ó&lt;/a&gt;, checkpoints and inference code!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/07&lt;/code&gt;&lt;/strong&gt;: We release the &lt;a href=&#34;https://recognize-anything.github.io/&#34;&gt;Recognize Anything Model (RAM)&lt;/a&gt;, a strong image tagging model!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/05&lt;/code&gt;&lt;/strong&gt;: Tag2Text is combined with &lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything&#34;&gt;Prompt-can-anything&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/05/20&lt;/code&gt;&lt;/strong&gt;: Tag2Text is combined with &lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything&#34;&gt;VideoChat&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/20&lt;/code&gt;&lt;/strong&gt;: We marry Tag2Text with with &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-SAM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/10&lt;/code&gt;&lt;/strong&gt;: Code and checkpoint is available Now!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/14&lt;/code&gt;&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text&#34;&gt;Tag2Text web demo ü§ó&lt;/a&gt; is available on Hugging Face Space!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚úç&lt;/span&gt; TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release Tag2Text demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release inference code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release RAM demo and checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release training codes (until July 8st at the latest).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release training datasets (until July 15st at the latest).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üß∞&lt;/span&gt; Checkpoints&lt;/h2&gt; &#xA;&lt;!-- insert a table --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;Illustration&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1&lt;/th&gt; &#xA;   &lt;td&gt;RAM-Swin&lt;/td&gt; &#xA;   &lt;td&gt;Swin-Large&lt;/td&gt; &#xA;   &lt;td&gt;COCO, VG, SBU, CC-3M, CC-12M&lt;/td&gt; &#xA;   &lt;td&gt;Demo version can recognize any common category with high accuracy.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/blob/main/ram_swin_large_14m.pth&#34;&gt;Download link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;2&lt;/th&gt; &#xA;   &lt;td&gt;Tag2Text-Swin&lt;/td&gt; &#xA;   &lt;td&gt;Swin-Base&lt;/td&gt; &#xA;   &lt;td&gt;COCO, VG, SBU, CC-3M, CC-12M&lt;/td&gt; &#xA;   &lt;td&gt;Demo version with comprehensive captions.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/blob/main/tag2text_swin_14m.pth&#34;&gt;Download link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèÉ&lt;/span&gt; Model Inference&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;RAM Inference&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the dependencies, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt;pip install -r requirements.txt&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Download RAM pretrained checkpoints.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get the English and Chinese outputs of the images:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;/pre&gt; python inference_ram.py --image images/1641173_2291260800.jpg \ --pretrained pretrained/ram_swin_large_14m.pth  &#xA;&lt;p&gt;&lt;strong&gt;RAM Zero-Shot Inference is Comming!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Tag2Text Inference&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the dependencies, run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;/pre&gt;pip install -r requirements.txt&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Download Tag2Text pretrained checkpoints.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Get the tagging and captioning results:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;/pre&gt; python inference_tag2text.py --image images/1641173_2291260800.jpg \ --pretrained pretrained/tag2text_swin_14m.pth  Or get the tagging and sepcifed captioning results (optional): &#xA;&lt;pre&gt;&lt;/pre&gt;python inference_tag2text.py --image images/1641173_2291260800.jpg \ --pretrained pretrained/tag2text_swin_14m.pth \ --specified-tags &#34;cloud,sky&#34; &#xA;&lt;h2&gt;&lt;span&gt;‚úí&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work to be useful for your research, please consider citing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhang2023recognize,&#xA;  title={Recognize Anything: A Strong Image Tagging Model}, &#xA;  author={Youcai Zhang and Xinyu Huang and Jinyu Ma and Zhaoyang Li and Zhaochuan Luo and Yanchun Xie and Yuzhuo Qin and Tong Luo and Yaqian Li and Shilong Liu and Yandong Guo and Lei Zhang},&#xA;  year={2023},&#xA;  eprint={2306.03514},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&#xA;@article{huang2023tag2text,&#xA;  title={Tag2Text: Guiding Vision-Language Model via Image Tagging},&#xA;  author={Huang, Xinyu and Zhang, Youcai and Ma, Jinyu and Tian, Weiwei and Feng, Rui and Zhang, Yuejie and Li, Yaqian and Guo, Yandong and Zhang, Lei},&#xA;  journal={arXiv preprint arXiv:2303.05657},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ô•&lt;/span&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This work is done with the help of the amazing code base of &lt;a href=&#34;https://github.com/salesforce/BLIP&#34;&gt;BLIP&lt;/a&gt;, thanks very much!&lt;/p&gt; &#xA;&lt;p&gt;We also want to thank @Cheng Rui @Shilong Liu @Ren Tianhe for their help in &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;marrying Tag2Text with Grounded-SAM&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Vahe1994/SpQR</title>
    <updated>2023-06-10T01:45:10Z</updated>
    <id>tag:github.com,2023-06-10:/Vahe1994/SpQR</id>
    <link href="https://github.com/Vahe1994/SpQR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SPQR model compression&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This repository contains quantization algorithm and the model evaluation code for SpQR method for LLM compression; The efficient inference code will be added soon.&lt;/p&gt; &#xA;&lt;p&gt;It accompanies the research paper &#34;&lt;a href=&#34;https://arxiv.org/abs/2306.03078&#34;&gt;SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression&lt;/a&gt;&#34; .&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h3&gt;Packages&lt;/h3&gt; &#xA;&lt;p&gt;Install packages from &lt;code&gt;requirements.txt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the results reported in the ArXiv paper where obtained using &lt;code&gt;4.28.dev0&lt;/code&gt; version of &lt;code&gt;transformers&lt;/code&gt;, commit id &lt;a href=&#34;https://github.com/huggingface/transformers/archive/464d420775653885760e30d24d3703e14f4e8a14.zip&#34;&gt;&lt;code&gt;464d420775&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Downloading model weights and dataset(s)&lt;/h3&gt; &#xA;&lt;p&gt;This scripts assume that model weights are preloaded and stored locally. See &lt;code&gt;MODEL_PATH&lt;/code&gt; references below.&lt;/p&gt; &#xA;&lt;p&gt;The scripts can use a variety of datasets for training. To use Red Pajamas, download it locally and then pass the location to the scripts. See &lt;code&gt;PAJAMAS_PATH&lt;/code&gt; references below.&lt;/p&gt; &#xA;&lt;h3&gt;Loading / caching datasets and tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;The script will require downloading and caching locally the relevant LLaMA tokenizer and one or few datasets for testing. They will be saved in default locations.&lt;/p&gt; &#xA;&lt;h4&gt;Data&lt;/h4&gt; &#xA;&lt;p&gt;The tokenized and preproccessed subset of &lt;a href=&#34;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample&#34;&gt;RedPajamas&lt;/a&gt; (mixture of datasets used for LLaMA training) is located here: &lt;code&gt;data/red_pajama_n=1024.pth&lt;/code&gt;. Below &lt;code&gt;PAJAMAS_PATH&lt;/code&gt; denotes the path to this subset.&lt;/p&gt; &#xA;&lt;h1&gt;Launching&lt;/h1&gt; &#xA;&lt;h3&gt;GPU requirements&lt;/h3&gt; &#xA;&lt;p&gt;This code was developed and tested using a single A100 GPU with 80GB GPU RAM. It may successfully run on GPUs with 32 - 40GB&lt;/p&gt; &#xA;&lt;h3&gt;Model downloading&lt;/h3&gt; &#xA;&lt;p&gt;The code requires the LLaMA model to be dowloaded in Hugging Face format and saved locally. The scripts below require such model folder path as argument.&lt;/p&gt; &#xA;&lt;h3&gt;Perplexity benchmarks:&lt;/h3&gt; &#xA;&lt;p&gt;This script compresses the model and then tests its performance in terms of perplexity using WikiText2, C4, and Penn Treebank datasets.&lt;/p&gt; &#xA;&lt;p&gt;The command to launch the script should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export MODEL_PATH=&amp;lt;INSERT PATH_TO_MODEL_DIR&amp;gt;&#xA;export PAJAMAS_PATH=&amp;lt;INSERT PATH TO PAJAMAS DIR&amp;gt;&#xA;&#xA;python main.py $MODEL_PATH custom \&#xA;    --load_from_saved=$PAJAMAS_PATH \&#xA;    --wbits 4 \&#xA;    --groupsize 16 \&#xA;    --perchannel \&#xA;    --qq_scale_bits 3 \&#xA;    --qq_zero_bits 3 \&#xA;    --qq_groupsize 16 \&#xA;    --outlier_threshold=0.2 \&#xA;    --permutation_order act_order \&#xA;    --percdamp 1e0 \&#xA;    --nsamples 128 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command above runs near-lossless compression as described in the article. Adjusting the above parameters allows for tighter compression with a slightly greater loss.&lt;/p&gt; &#xA;&lt;p&gt;Note the launch arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;PATH_TO_MODEL_DIR&amp;gt;&lt;/code&gt; - path to model folder, which contains &lt;code&gt;config.json &lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;one of [c4, ptb, wikitext2, custom]&lt;/code&gt; -- name of dataset to use for compression&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--load_from_saved&lt;/code&gt; - path to preprocessed and tokenized dataset (if &lt;code&gt;custom&lt;/code&gt; chosen). Otherwise do not specify.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--wbits 3&lt;/code&gt; -- number of bits for quantized weights representation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--groupsize 16&lt;/code&gt; -- size of first-order groups for compression&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--qq_groupsize 16&lt;/code&gt; -- size of second-order (quantized) groups for compression&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--qq_scale_bits 3 --qq_zero_bits 3&lt;/code&gt; -- bit sizes for quantizing first order weights&#39; scale and zeros. run &lt;code&gt;python main.py --help&lt;/code&gt; for more details on command line arguments, including compression parameters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LM Evaluation Harness benchmark.&lt;/h3&gt; &#xA;&lt;p&gt;To perform zero-shot evaluation, we use &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-eval-harness&lt;/a&gt; framework with slight modifications. The LICENSE and CODEOWNERS files inside lm-evaluation-harness refer to the original authors of lm-eval-harness and not the authors of this paper.&lt;/p&gt; &#xA;&lt;p&gt;For instructions about zero-shot evaluation refer to &lt;code&gt;README.md&lt;/code&gt; inside &lt;code&gt;lm-evaluation-harness&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{dettmers2023spqr,&#xA;      title={SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression}, &#xA;      author={Tim Dettmers and Ruslan Svirschevski and Vage Egiazarian and Denis Kuznedelev and Elias Frantar and Saleh Ashkboos and Alexander Borzunov and Torsten Hoefler and Dan Alistarh},&#xA;      year={2023},&#xA;      eprint={2306.03078},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>OpenTalker/SadTalker</title>
    <updated>2023-06-10T01:45:10Z</updated>
    <id>tag:github.com,2023-06-10:/OpenTalker/SadTalker</id>
    <link href="https://github.com/OpenTalker/SadTalker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2023] SadTalkerÔºöLearning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/4397546/229094115-862c747e-7397-4b54-ba4a-bd368bfe2e0f.png&#34; width=&#34;500px&#34;&gt; &#xA; &lt;!--&lt;h2&gt; üò≠ SadTalkerÔºö &lt;span style=&#34;font-size:12px&#34;&gt;Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation &lt;/span&gt; &lt;/h2&gt; --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-PDF-red&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://sadtalker.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://huggingface.co/spaces/vinthony/SadTalker&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/video/stable/stable_diffusion_1_5_video_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Automatic1111-Colab-green&#34; alt=&#34;sd webui-colab&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://replicate.com/cjwbw/sadtalker&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/sadtalker/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;div&gt; &#xA;  &lt;a target=&#34;_blank&#34;&gt;Wenxuan Zhang &lt;sup&gt;*,1,2&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://vinthony.github.io/&#34; target=&#34;_blank&#34;&gt;Xiaodong Cun &lt;sup&gt;*,2&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://xuanwangvc.github.io/&#34; target=&#34;_blank&#34;&gt;Xuan Wang &lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://yzhang2016.github.io/&#34; target=&#34;_blank&#34;&gt;Yong Zhang &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://xishen0220.github.io/&#34; target=&#34;_blank&#34;&gt;Xi Shen &lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://yuguo-xjtu.github.io/&#34; target=&#34;_blank&#34;&gt;Yu Guo&lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?hl=zh-CN&amp;amp;user=4oXBp9UAAAAJ&#34; target=&#34;_blank&#34;&gt;Ying Shan &lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA;  &lt;a target=&#34;_blank&#34;&gt;Fei Wang &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;‚ÄÉ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;1&lt;/sup&gt; Xi&#39;an Jiaotong University ‚ÄÉ &#xA;  &lt;sup&gt;2&lt;/sup&gt; Tencent AI Lab ‚ÄÉ &#xA;  &lt;sup&gt;3&lt;/sup&gt; Ant Group ‚ÄÉ &#xA; &lt;/div&gt; &#xA; &lt;br&gt; &#xA; &lt;i&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12194&#34; target=&#34;_blank&#34;&gt;CVPR 2023&lt;/a&gt;&lt;/strong&gt;&lt;/i&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/4397546/222490039-b1f6156b-bf00-405b-9fda-0c9a9156f991.gif&#34; alt=&#34;sadtalker&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;b&gt;TL;DR: &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; single portrait image üôé‚Äç‚ôÇÔ∏è &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;+ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; audio üé§ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; = &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; talking head video üéû.&lt;/b&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üî• Highlight&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî• The extension of the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;stable-diffusion-webui&lt;/a&gt; is online. Checkout more details &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/webui_extension.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/4397546/231495639-5d4bb925-ea64-4a36-a519-6389917dac29.mp4&#34;&gt;https://user-images.githubusercontent.com/4397546/231495639-5d4bb925-ea64-4a36-a519-6389917dac29.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî• &lt;code&gt;full image mode&lt;/code&gt; is online! checkout &lt;a href=&#34;https://github.com/Winfredy/SadTalker#full-bodyimage-generation&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;still+enhancer in v0.0.1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;still + enhancer in v0.0.2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/bagbag1815/status/1642754319094108161&#34;&gt;input image @bagbag1815&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://user-images.githubusercontent.com/48216707/229484996-5d7be64f-2553-4c9e-a452-c5cf0b8ebafe.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA;    &lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&#xA;    &lt;video src=&#34;https://user-images.githubusercontent.com/4397546/230717873-355b7bf3-d3de-49f9-a439-9220e623fce7.mp4&#34; type=&#34;video/mp4&#34;&gt; &#xA;    &lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/examples/source_image/full_body_2.png&#34; width=&#34;380&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üî• Several new mode, eg, &lt;code&gt;still mode&lt;/code&gt;, &lt;code&gt;reference mode&lt;/code&gt;, &lt;code&gt;resize mode&lt;/code&gt; are online for better and custom applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üî• Happy to see more community demos at &lt;a href=&#34;https://search.bilibili.com/all?keyword=sadtalker&amp;amp;from_source=webtop_search&amp;amp;spm_id_from=333.1007&amp;amp;search_source=3&#34;&gt;bilibili&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/results?search_query=sadtalker&amp;amp;sp=CAM%253D&#34;&gt;Youtube&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/search?q=%23sadtalker&amp;amp;src=typed_query&#34;&gt;twitter #sadtalker&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìã Changelog (Previous changelog can be founded &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/changlelog.md&#34;&gt;here&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.06.05]&lt;/strong&gt;: release a new 512 beta face model. Fixed some bugs and improve the performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.15]&lt;/strong&gt;: Adding automatic1111 colab by @camenduru, thanks for this awesome colab: &lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/video/stable/stable_diffusion_1_5_video_webui_colab.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Automatic1111-Colab-green&#34; alt=&#34;sd webui-colab&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.12]&lt;/strong&gt;: adding a more detailed sd-webui installation document, fixed reinstallation problem.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.12]&lt;/strong&gt;: Fixed the sd-webui safe issues becasue of the 3rd packages, optimize the output path in &lt;code&gt;sd-webui-extension&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.08]&lt;/strong&gt;: ‚ùóÔ∏è‚ùóÔ∏è‚ùóÔ∏è In v0.0.2, we add a logo watermark to the generated video to prevent abusing since it is very realistic.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2023.04.08]&lt;/strong&gt;: v0.0.2, full image animation, adding baidu driver for download checkpoints. Optimizing the logic about enhancer.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöß TODO&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; Previous TODOs &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generating 2D face from a single Image.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generating 3D face from Audio.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generating 4D free-view talking examples from audio and a single image.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio/Colab Demo.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Full body/image Generation.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; integrade with stable-diffusion-web-ui. (stay tunning!)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Audio-driven Anime Avatar.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; training code of each componments.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;If you have any problem, please view our &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/FAQ.md&#34;&gt;FAQ&lt;/a&gt; before opening an issue.&lt;/h2&gt; &#xA;&lt;h2&gt;‚öôÔ∏è 1. Installation.&lt;/h2&gt; &#xA;&lt;p&gt;Tutorials from communities: &lt;a href=&#34;https://www.bilibili.com/video/BV1Dc411W7V6/&#34;&gt;‰∏≠ÊñáwindowsÊïôÁ®ã&lt;/a&gt; | &lt;a href=&#34;https://br-d.fanbox.cc/posts/5685086?utm_campaign=manage_post_page&amp;amp;utm_medium=share&amp;amp;utm_source=twitter&#34;&gt;Êó•Êú¨Ë™û„Ç≥„Éº„Çπ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Linux:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Installing &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;anaconda&lt;/a&gt;, python and git.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Creating the env and install the requirements.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Winfredy/SadTalker.git&#xA;&#xA;cd SadTalker &#xA;&#xA;conda create -n sadtalker python=3.8&#xA;&#xA;conda activate sadtalker&#xA;&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;&#xA;conda install ffmpeg&#xA;&#xA;pip install -r requirements.txt&#xA;&#xA;### tts is optional for gradio demo. &#xA;### pip install TTS&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows (&lt;a href=&#34;https://www.bilibili.com/video/BV1Dc411W7V6/&#34;&gt;‰∏≠ÊñáwindowsÊïôÁ®ã&lt;/a&gt;):&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.python.org/downloads/windows/&#34;&gt;Python 3.10.6&lt;/a&gt;, checking &#34;Add Python to PATH&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;git&lt;/a&gt; manually (OR &lt;code&gt;scoop install git&lt;/code&gt; via &lt;a href=&#34;https://scoop.sh/&#34;&gt;scoop&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;ffmpeg&lt;/code&gt;, following &lt;a href=&#34;https://www.wikihow.com/Install-FFmpeg-on-Windows&#34;&gt;this instruction&lt;/a&gt; (OR using &lt;code&gt;scoop install ffmpeg&lt;/code&gt; via &lt;a href=&#34;https://scoop.sh/&#34;&gt;scoop&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Download our SadTalker repository, for example by running &lt;code&gt;git clone https://github.com/Winfredy/SadTalker.git&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download the &lt;code&gt;checkpoint&lt;/code&gt; and &lt;code&gt;gfpgan&lt;/code&gt; &lt;a href=&#34;https://github.com/Winfredy/SadTalker#-2-download-trained-models&#34;&gt;below‚Üì&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;start.bat&lt;/code&gt; from Windows Explorer as normal, non-administrator, user, a gradio WebUI demo will be started.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Macbook:&lt;/h3&gt; &#xA;&lt;p&gt;More tips about installnation on Macbook and the Docker file can be founded &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/install.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üì• 2. Download Trained Models.&lt;/h2&gt; &#xA;&lt;p&gt;You can run the following script to put all the models in the right place.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/download_models.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other alternatives:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;we also provide an offline patch (&lt;code&gt;gfpgan/&lt;/code&gt;), thus, no model will be downloaded when generating.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Google Driver&lt;/strong&gt;: download our pre-trained model from &lt;a href=&#34;https://drive.google.com/file/d/1gwWh45pF7aelNP_P78uDJL8Sycep-K7j/view?usp=sharing&#34;&gt; this link (main checkpoints)&lt;/a&gt; and &lt;a href=&#34;https://drive.google.com/file/d/19AIBsmfcHW6BRJmeqSFlG5fL445Xmsyi?usp=sharing&#34;&gt; gfpgan (offline patch)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Github Release Page&lt;/strong&gt;: download all the files from the &lt;a href=&#34;https://github.com/Winfredy/SadTalker/releases&#34;&gt;lastest github release page&lt;/a&gt;, and then, put it in ./checkpoints.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ÁôæÂ∫¶‰∫ëÁõò&lt;/strong&gt;: we provided the downloaded model in &lt;a href=&#34;https://pan.baidu.com/s/1P4fRgk9gaSutZnn8YW034Q?pwd=sadt&#34;&gt;checkpoints, ÊèêÂèñÁ†Å: sadt.&lt;/a&gt; And &lt;a href=&#34;https://pan.baidu.com/s/1kb1BCPaLOWX1JJb9Czbn6w?pwd=sadt&#34;&gt;gfpgan, ÊèêÂèñÁ†Å: sadt.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Model Details&lt;/summary&gt; &#xA; &lt;p&gt;Model explains:&lt;/p&gt; &#xA; &lt;h5&gt;New version&lt;/h5&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00229-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00109-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/SadTalker_V0.0.2_256.safetensors&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;packaged sadtalker checkpoints of old version, 256 face render).&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/SadTalker_V0.0.2_512.safetensors&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;packaged sadtalker checkpoints of old version, 512 face render).&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;gfpgan/weights&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection and enhanced models used in &lt;code&gt;facexlib&lt;/code&gt; and &lt;code&gt;gfpgan&lt;/code&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h5&gt;Old version&lt;/h5&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2exp_00300-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained ExpNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/auido2pose_00140-model.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained PoseVAE in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00229-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/mapping_00109-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained MappingNet in Sadtalker.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/facevid2vid_00189-model.pth.tar&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained face-vid2vid model from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;the reappearance of face-vid2vid&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/epoch_20.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Pre-trained 3DMM extractor in &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/wav2lip.pth&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Highly accurate lip-sync model in &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/shape_predictor_68_face_landmarks.dat&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face landmark model used in &lt;a href=&#34;http://dlib.net/&#34;&gt;dilb&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/BFM&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;3DMM library file.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;checkpoints/hub&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection models used in &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face alignment&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;gfpgan/weights&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Face detection and enhanced models used in &lt;code&gt;facexlib&lt;/code&gt; and &lt;code&gt;gfpgan&lt;/code&gt;.&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;The final folder will be shown as:&lt;/p&gt; &#xA; &lt;img width=&#34;331&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/4397546/232511411-4ca75cbf-a434-48c5-9ae0-9009e8316484.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üîÆ 3. Quick Start (&lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/best_practice.md&#34;&gt;Best Practice&lt;/a&gt;).&lt;/h2&gt; &#xA;&lt;h3&gt;WebUI Demos:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Online&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/spaces/vinthony/SadTalker&#34;&gt;Huggingface&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/main/video/stable/stable_diffusion_1_5_video_webui_colab.ipynb&#34;&gt;SDWebUI-Colab&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb&#34;&gt;Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local Autiomatic1111 stable-diffusion webui extension&lt;/strong&gt;: please refer to &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/webui_extension.md&#34;&gt;Autiomatic1111 stable-diffusion webui docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local gradio demo&lt;/strong&gt;: Similar to our &lt;a href=&#34;https://huggingface.co/spaces/vinthony/SadTalker&#34;&gt;hugging-face demo&lt;/a&gt; can be run by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## you need manually install TTS(https://github.com/coqui-ai/TTS) via `pip install tts` in advanced.&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Local windows gradio demo&lt;/strong&gt;: just double click &lt;code&gt;webui.bat&lt;/code&gt;, the requirements will be installed automatically.&lt;/p&gt; &#xA;&lt;h3&gt;Manually usages:&lt;/h3&gt; &#xA;&lt;h5&gt;Animating a portrait image from default config:&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --enhancer gfpgan &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results will be saved in &lt;code&gt;results/$SOME_TIMESTAMP/*.mp4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Full body/image Generation:&lt;/h5&gt; &#xA;&lt;p&gt;Using &lt;code&gt;--still&lt;/code&gt; to generate a natural full body video. You can add &lt;code&gt;enhancer&lt;/code&gt; to improve the quality of the generated video.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --driven_audio &amp;lt;audio.wav&amp;gt; \&#xA;                    --source_image &amp;lt;video.mp4 or picture.png&amp;gt; \&#xA;                    --result_dir &amp;lt;a file to store results&amp;gt; \&#xA;                    --still \&#xA;                    --preprocess full \&#xA;                    --enhancer gfpgan &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More examples and configuration and tips can be founded in the &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/docs/best_practice.md&#34;&gt; &amp;gt;&amp;gt;&amp;gt; best practice documents &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üõé Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhang2022sadtalker,&#xA;  title={SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation},&#xA;  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},&#xA;  journal={arXiv preprint arXiv:2211.12194},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üíó Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Facerender code borrows heavily from &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;zhanglonghao&#39;s reproduction of face-vid2vid&lt;/a&gt; and &lt;a href=&#34;https://github.com/RenYurui/PIRender&#34;&gt;PIRender&lt;/a&gt;. We thank the authors for sharing their wonderful code. In training process, We also use the model from &lt;a href=&#34;https://github.com/microsoft/Deep3DFaceReconstruction&#34;&gt;Deep3DFaceReconstruction&lt;/a&gt; and &lt;a href=&#34;https://github.com/Rudrabha/Wav2Lip&#34;&gt;Wav2lip&lt;/a&gt;. We thank for their wonderful work.&lt;/p&gt; &#xA;&lt;p&gt;See also these wonderful 3rd libraries we use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Utils&lt;/strong&gt;: &lt;a href=&#34;https://github.com/xinntao/facexlib&#34;&gt;https://github.com/xinntao/facexlib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Enhancement&lt;/strong&gt;: &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;https://github.com/TencentARC/GFPGAN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Image/Video Enhancement&lt;/strong&gt;:&lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;https://github.com/xinntao/Real-ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü•Ç Extensions:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Zz-ww/SadTalker-Video-Lip-Sync&#34;&gt;SadTalker-Video-Lip-Sync&lt;/a&gt; from &lt;a href=&#34;https://github.com/Zz-ww&#34;&gt;@Zz-ww&lt;/a&gt;: SadTalker for Video Lip Editing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü•Ç Related Works&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/StyleHEAT&#34;&gt;StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN (ECCV 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Doubiiu/CodeTalker&#34;&gt;CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vinthony/video-retalking&#34;&gt;VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild (SIGGRAPH Asia 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Carlyx/DPE&#34;&gt;DPE: Disentanglement of Pose and Expression for General Video Portrait Editing (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FeiiYin/SPI/&#34;&gt;3D GAN Inversion with Facial Symmetry Prior (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mael-zys/T2M-GPT&#34;&gt;T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations (CVPR 2023)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì¢ Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This is not an official product of Tencent. This repository can only be used for personal/research/non-commercial purposes.&lt;/p&gt; &#xA;&lt;p&gt;LOGO: color and font suggestion: &lt;a href=&#34;https://raw.githubusercontent.com/OpenTalker/SadTalker/main/ai.com&#34;&gt;ChatGPT&lt;/a&gt;, logo fontÔºö&lt;a href=&#34;https://fonts.google.com/specimen/Montserrat+Alternates?preview.text=SadTalker&amp;amp;preview.text_type=custom&amp;amp;query=mont&#34;&gt;Montserrat Alternates &lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All the copyright of the demo images and audio are from communities users or the geneartion from stable diffusion. Free free to contact us if you feel uncomfortable.&lt;/p&gt;</summary>
  </entry>
</feed>