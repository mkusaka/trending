<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-06T01:42:13Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>replit/ReplitLM</title>
    <updated>2023-05-06T01:42:13Z</updated>
    <id>tag:github.com,2023-05-06:/replit/ReplitLM</id>
    <link href="https://github.com/replit/ReplitLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code and configs for the ReplitLM model family&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ReplitLM&lt;/h1&gt; &#xA;&lt;p&gt;Inference code and configs for the ReplitLM model family.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint [CC BY-SA 4.0]&lt;/th&gt; &#xA;   &lt;th&gt;Vocabulary [CC BY-SA 4.0]&lt;/th&gt; &#xA;   &lt;th&gt;Code [Apache 2.0]&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;replit-code-v1-3b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/replit/replit-code-v1-3b/resolve/main/pytorch_model.bin&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/replit/replit-code-v1-3b/resolve/main/spiece.model&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/replit/ReplitLM/tree/main/replit-code-v1-3b&#34;&gt;Repo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;May 2, 2023: &lt;a href=&#34;https://github.com/replit/ReplitLM/tree/main/replit-code-v1-3b&#34;&gt;&lt;code&gt;replit-code-v1-3b&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>abertsch72/unlimiformer</title>
    <updated>2023-05-06T01:42:13Z</updated>
    <id>tag:github.com,2023-05-06:/abertsch72/unlimiformer</id>
    <link href="https://github.com/abertsch72/unlimiformer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Public repo for the preprint &#34;Unlimiformer: Long-Range Transformers with Unlimited Length Input&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Unlimiformer&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/42593540/236538293-1d5fdfe3-3e34-4979-9611-a9c9f56e3a00.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2305.01625&#34;&gt;Unlimiformer: Long-Range Transformers with Unlimited Length Input&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unlimiformer is a method for augmenting pretrained encoder-decoder models with a type of retrieval-based attention. This allows the use of unlimited length inputs with any pretrained encoder-decoder!&lt;/p&gt; &#xA;&lt;p&gt;Unlimiformer can be used to improve performance of an already-trained model. However, for best results, the model should be trained with Unlimiformer.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Paste these files from &lt;code&gt;src&lt;/code&gt; into your source code folder.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll need to set values for the Unlimiformer-specific arguments outlined in &lt;code&gt;usage.py&lt;/code&gt;-- you can add these arguments wherever you usually process hyperparameters.&lt;/p&gt; &#xA;&lt;p&gt;To use the model, you must set &lt;code&gt;test_unlimiformer=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;run.py&lt;/code&gt; is an example of a full training setup that integrates Unlimiformer, adopted from &lt;a href=&#34;https://github.com/Mivg/SLED&#34;&gt;SLED&lt;/a&gt; -- this is likely more complex than you will need.&lt;/p&gt; &#xA;&lt;h2&gt;Trained models&lt;/h2&gt; &#xA;&lt;p&gt;The following models from the paper are available on HuggingFace. Please note that you must add the Unlimiformer-specific files to your repository, and load these models with &lt;code&gt;knn=True&lt;/code&gt;. &lt;em&gt;If you download these models from Huggingface, they may not use Unlimiformer by default!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Table 3: low-cost training methods&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;HuggingFace link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GovReport&lt;/td&gt; &#xA;   &lt;td&gt;Baseline: BART-base&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/bart-base-govreport&#34;&gt;abertsch/bart-base-govreport&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GovReport&lt;/td&gt; &#xA;   &lt;td&gt;BART-base + Unlimiformer early stopping&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/unlimiformer-bart-govreport-earlyk&#34;&gt;abertsch/unlimiformer-bart-govreport-earlyk&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SummScreen&lt;/td&gt; &#xA;   &lt;td&gt;Baseline: BART-base&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/bart-base-summscreen&#34;&gt;abertsch/bart-base-summscreen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SummScreen&lt;/td&gt; &#xA;   &lt;td&gt;BART-base + Unlimiformer early stopping&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/unlimiformer-bart-summscreen-earlyk&#34;&gt;abertsch/unlimiformer-bart-summscreen-earlyk&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Table 4: Long-range training methods&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;HuggingFace link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GovReport&lt;/td&gt; &#xA;   &lt;td&gt;BART + Unlimiformer (alternating training)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/unlimiformer-bart-govreport-alternating&#34;&gt;abertsch/unlimiformer-bart-govreport-alternating&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SummScreen&lt;/td&gt; &#xA;   &lt;td&gt;BART + Unlimiformer (retrieval training)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/unlimiformer-bart-summscreen-retrieval&#34;&gt;abertsch/unlimiformer-bart-summscreen-retrieval&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Table 5: BookSum&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;HuggingFace link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BookSum&lt;/td&gt; &#xA;   &lt;td&gt;Baseline: BART-base&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/bart-base-booksum&#34;&gt;abertsch/bart-base-booksum&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BookSum&lt;/td&gt; &#xA;   &lt;td&gt;BART-base + Unlimiformer early stopping&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/unlimiformer-bart-booksum-earlyk&#34;&gt;abertsch/unlimiformer-bart-booksum-earlyk&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Booksum&lt;/td&gt; &#xA;   &lt;td&gt;BART-base + Unlimiformer (random-encoding training)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/unlimiformer-bart-booksum-random-encoding&#34;&gt;abertsch/unlimiformer-bart-booksum-random-encoding&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Booksum&lt;/td&gt; &#xA;   &lt;td&gt;BART-base + Unlimiformer (alternating training)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/abertsch/unlimiformer-bart-booksum-alternating&#34;&gt;abertsch/unlimiformer-bart-booksum-alternating&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Recommended settings&lt;/h2&gt; &#xA;&lt;h3&gt;To evaluate with Unlimiformer&lt;/h3&gt; &#xA;&lt;p&gt;At evaluation time, we recommend the default value for each setting.&lt;/p&gt; &#xA;&lt;h3&gt;To train with Unlimiformer&lt;/h3&gt; &#xA;&lt;p&gt;For an inexpensive method, we recommend training as usual and using Unlimiformer during early stopping. To do so, set &lt;code&gt;knn=True&lt;/code&gt; and leave all other values at default.&lt;/p&gt; &#xA;&lt;p&gt;For best performance, there are 3 expensive settings for training. The best one varies by dataset.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set &lt;code&gt;random_unlimiformer_training=True&lt;/code&gt;: this is the &lt;em&gt;random-encoded training&lt;/em&gt; setting from the paper&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;unlimiformer_training=True&lt;/code&gt;: this is the &lt;em&gt;approximate-retrieval training&lt;/em&gt; setting from the paper&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;random_unlimiformer_training=True&lt;/code&gt; AND &lt;code&gt;unlimiformer_training=True&lt;/code&gt;: this is the &lt;em&gt;alternating training&lt;/em&gt; setting from the paper&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See Table 5 in the paper for a more detailed breakdown of relative training costs.&lt;/p&gt; &#xA;&lt;h2&gt;Tips for very large inputs&lt;/h2&gt; &#xA;&lt;h3&gt;For training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;you may need to truncate your inputs at training time, e.g. to 8k or 16k tokens. You can use the full inputs at evaluation time&lt;/li&gt; &#xA; &lt;li&gt;you can also try splitting your inputs into 16k-token-chunks and training on each one as its own example&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;For evaluation (including early stopping)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;if you&#39;re consistently running out of CUDA memory, set &lt;code&gt;use_datastore=True&lt;/code&gt; to use a Faiss datastore to store hidden states.&lt;/li&gt; &#xA; &lt;li&gt;if you&#39;re still having issues, set &lt;code&gt;gpu_datastore=False&lt;/code&gt; or &lt;code&gt;gpu_index=False&lt;/code&gt;, but note that this will degrade performance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use our method or models, please cite &lt;a href=&#34;https://arxiv.org/abs/2305.01625&#34;&gt;our paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{bertsch2023unlimiformer,&#xA;      title={Unlimiformer: Long-Range Transformers with Unlimited Length Input}, &#xA;      author={Amanda Bertsch and Uri Alon and Graham Neubig and Matthew R. Gormley},&#xA;      year={2023},&#xA;      eprint={2305.01625},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have any questions on this work, please open a GitHub issue or email the authors at &lt;code&gt;abertsch@cs.cmu.edu, ualon@cs.cmu.edu&lt;/code&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>richardyc/Chrome-GPT</title>
    <updated>2023-05-06T01:42:13Z</updated>
    <id>tag:github.com,2023-05-06:/richardyc/Chrome-GPT</id>
    <link href="https://github.com/richardyc/Chrome-GPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AutoGPT agent that controls Chrome on your desktop&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🤖 Chrome-GPT: An experimental AutoGPT agent that interacts with Chrome&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/richardyc/chrome-gpt/actions/workflows/lint.yml&#34;&gt;&lt;img src=&#34;https://github.com/richardyc/chrome-gpt/actions/workflows/lint.yml/badge.svg?sanitize=true&#34; alt=&#34;lint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/richardyc/chrome-gpt/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/richardyc/chrome-gpt/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/RealRichomie&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/RealRichomie.svg?style=social&amp;amp;label=Follow%20%40RealRichomie&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;⚠️This is an experimental AutoGPT agent that might take incorrect actions and could lead to serious consequences. Please use it at your own discretion⚠️&lt;/p&gt; &#xA;&lt;p&gt;Chrome-GPT is an AutoGPT experiment that utilizes &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;Langchain&lt;/a&gt; and &lt;a href=&#34;https://github.com/SeleniumHQ/selenium&#34;&gt;Selenium&lt;/a&gt; to enable an AutoGPT agent take control of an entire Chrome session. With the ability to interactively scroll, click, and input text on web pages, the AutoGPT agent can navigate and manipulate web content.&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; 🖥️ Demo &lt;/h2&gt; &#xA;&lt;p&gt;Input Prompt: &lt;code&gt;Find me a bar that can host a 20 person event near Chelsea, Manhattan evening of Apr 30th. Fill out contact us form if they have one with info: Name Richard, email he@hrichard.com.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/14324698/234191011-ec73af54-4a8e-4298-be1d-4252050f08c1.mov&#34;&gt;https://user-images.githubusercontent.com/14324698/234191011-ec73af54-4a8e-4298-be1d-4252050f08c1.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Demo made by &lt;a href=&#34;https://twitter.com/RealRichomie&#34;&gt;Richard He&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; 🔮 Features &lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🌎 Google search&lt;/li&gt; &#xA; &lt;li&gt;🧠 Long-term and short-term memory management&lt;/li&gt; &#xA; &lt;li&gt;🔨 Chrome actions: describe a webpage, scroll to element, click on buttons/links, input forms, switch tabs&lt;/li&gt; &#xA; &lt;li&gt;🤖 Supports multiple agent types: Zero-shot, BabyAGI and Auto-GPT&lt;/li&gt; &#xA; &lt;li&gt;🔥 (IN PROGRESS) Chrome plugin support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; 🧱 Known Limitations &lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are limited web crawling features, with buttons and input fields sometimes failing to appear in prompt.&lt;/li&gt; &#xA; &lt;li&gt;The response time is slow, with each action taking between 1-10 seconds to run.&lt;/li&gt; &#xA; &lt;li&gt;At times, langchain agents are unable to parse GPT outputs (refer to langchain discussion: &lt;a href=&#34;https://github.com/hwchase17/langchain/discussions/4065&#34;&gt;https://github.com/hwchase17/langchain/discussions/4065&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; Requirements &lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chrome&lt;/li&gt; &#xA; &lt;li&gt;Python &amp;gt;3.8&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://python-poetry.org/docs/#installation&#34;&gt;Poetry&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; 🛠️ Setup &lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set up your OpenAI &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;API Keys&lt;/a&gt; and add &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; env variable&lt;/li&gt; &#xA; &lt;li&gt;Install Python requirements via poetry &lt;code&gt;poetry install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open a poetry shell &lt;code&gt;poetry shell&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run chromegpt via &lt;code&gt;python -m chromegpt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; 🧠 Usage &lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT-3.5 Usage (Default): &lt;code&gt;python -m chromegpt -v -t &#34;{your request}&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;GPT-4 Usage (Recommended, needs GPT-4 access): &lt;code&gt;python -m chromegpt -v -a auto-gpt -m gpt-4 -t &#34;{your request}&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;For help: &lt;code&gt;python -m chromegpt --help&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Usage: python -m chromegpt [OPTIONS]&#xA;&#xA;  Run ChromeGPT: An AutoGPT agent that interacts with Chrome&#xA;&#xA;Options:&#xA;  -t, --task TEXT                 The task to execute  [required]&#xA;  -a, --agent [auto-gpt|baby-agi|zero-shot]&#xA;                                  The agent type to use&#xA;  -m, --model TEXT                The model to use&#xA;  --headless                      Run in headless mode&#xA;  -v, --verbose                   Run in verbose mode&#xA;  --human-in-loop                 Run in human-in-loop mode, only available&#xA;                                  when using auto-gpt agent&#xA;  --help                          Show this message and exit.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>