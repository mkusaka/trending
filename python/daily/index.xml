<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-03T01:31:30Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>iperov/DeepFaceLive</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/iperov/DeepFaceLive</id>
    <link href="https://github.com/iperov/DeepFaceLive" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Real-time face swap for PC streaming or video calls&lt;/p&gt;&lt;hr&gt;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/deepfacelive_intro.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_onnx.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_directx.png&#34; alt=&#34;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_python.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Face Swapper&lt;/h2&gt; &lt;p&gt;You can swap your face from a webcam or the face in the video using trained face models.&lt;/p&gt; &lt;p&gt;Here is a list of available ready-to-use public face models.&lt;/p&gt; &lt;p&gt;These persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he&#39;s breathtaking!&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Keanu Reeves &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Keanu_Reeves/Keanu_Reeves.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Keanu_Reeves/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Ava de Addario &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ava_de_Addario/Ava_de_Addario.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ava_de_Addario/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Dilraba Dilmurat &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Dilraba_Dilmurat/Dilraba_Dilmurat.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;examples&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Ewon Spice &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ewon_Spice/Ewon_Spice.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ewon_Spice/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Yohanna Coralson &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Yohanna_Coralson/Yohanna_Coralson.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Yohanna_Coralson/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr align=&#34;center&#34;&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Kim Jarrey &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Kim_Jarrey/Kim_Jarrey.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Kim_Jarrey/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; David Kovalniy &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/David_Kovalniy/David_Kovalniy.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/David_Kovalniy/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Matilda Bobbie &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Matilda_Bobbie/Matilda_Bobbie.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Matilda_Bobbie/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Bryan Greynolds &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Bryan_Greynolds/Bryan_Greynolds.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Bryan_Greynolds/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Nicola Badge &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Nicola_Badge/Nicola_Badge.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Nicola_Badge/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &#xA;    &lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA;     &lt;tbody&gt;&#xA;      &lt;tr align=&#34;center&#34;&gt;&#xA;       &lt;td align=&#34;center&#34;&gt; Silwan Stillwone &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Silwan_Stillwone/Silwan_Stillwone.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Silwan_Stillwone/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Tim Chrys &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Chrys/Tim_Chrys.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Chrys/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Zahar Lupin &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Zahar_Lupin/Zahar_Lupin.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Zahar_Lupin/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;       &lt;td&gt; Tim Norland &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Norland/Tim_Norland.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Norland/examples.md&#34;&gt;examples&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;     &lt;/tbody&gt;&#xA;    &lt;/table&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; If you want a higher quality or better face match, you can train your own face model using &lt;a href=&#34;https://github.com/iperov/DeepFaceLab&#34;&gt;DeepFaceLab&lt;/a&gt; &lt;p&gt;Here is an &lt;a href=&#34;https://www.tiktok.com/@arnoldschwarzneggar/video/6995538782204300545&#34;&gt;example&lt;/a&gt; of Arnold Schwarzneggar trained on a particular face and used in a video call. Read the FAQ for more information.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Face Animator&lt;/h2&gt; &lt;p&gt;There is also a Face Animator module in DeepFaceLive app. You can control a static face picture using video or your own face from the camera. The quality is not the best, and requires fine face matching and tuning parameters for every face pair, but enough for funny videos and memes or real-time streaming at 25 fps using 35 TFLOPS GPU.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/face_animator_example.gif&#34;&gt;&lt;/p&gt; &lt;p&gt;Here is a &lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/FaceAnimator_tutor.webm?raw=true&#34;&gt;mini video&lt;/a&gt; showing the process of setting up the Face Animator for Obama controlling Kim Chen&#39;s face.&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table align=&#34;center&#34; border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;System requirements&lt;/h2&gt; &lt;p&gt;any DirectX12 compatible graphics card&lt;/p&gt; &lt;p&gt;(Recommended RTX 2070+ / Radeon RX 5700 XT+ )&lt;/p&gt; &lt;p&gt;Modern CPU with AVX instructions&lt;/p&gt; &lt;p&gt;4GB RAM, 32GB+ paging file&lt;/p&gt; &lt;p&gt;Windows 10&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Setup tutorial&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/setup_tutorial_windows/index.md&#34;&gt;Windows 10 x64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/build/linux&#34;&gt;Linux build info&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Documentation&lt;/h2&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/user_faq/user_faq.md&#34;&gt;User FAQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/developer_faq/developer_faq.md&#34;&gt;Developer FAQ&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Releases&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://disk.yandex.ru/d/7i5XTKIKVg5UUg&#34;&gt;Windows 10 x64 (yandex.ru)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://mega.nz/folder/m10iELBK#Y0H6BflF9C4k_clYofC7yA&#34;&gt;Windows 10 x64 (mega.nz)&lt;/a&gt;&lt;/p&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt; Contains stand-alone zero-dependency all-in-one ready-to-use portable self-extracting folder! You don&#39;t need to install anything other than video drivers. &lt;br&gt;&lt;br&gt; DirectX12 build : NVIDIA, AMD, Intel videocards. &lt;br&gt;&lt;br&gt; NVIDIA build : NVIDIA cards only, GT730 and higher. Works faster than DX12. FaceMerger can work also on AMD/Intel. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;Communication groups&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://discord.gg/S2h7kPySQp&#34;&gt;Discord&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;Official discord channel. English / Russian.&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://mrdeepfakes.com/forums/&#34;&gt;mrdeepfakes&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;the biggest NSFW English deepfake community&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td align=&#34;right&#34;&gt; &lt;a href=&#34;https://www.dfldata.xyz&#34;&gt;dfldata.xyz&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td align=&#34;left&#34;&gt;中文交流论坛，免费软件教程、模型、人脸数据&lt;/td&gt;&#xA;  &lt;/tr&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;h2&gt;How can I help the project?&lt;/h2&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; I need the computing power to train models. &lt;br&gt; If you have a free computer with 2080TI or better card with 12GB+ VRAM, you can give me remote access to it. I will train 1 model in a month. Contact me(iperov#6528) in Discord channel. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; Register github account and push &#34;Star&#34; button. &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;!--&lt;tr&gt;&lt;td colspan=2 align=&#34;center&#34;&gt;&#xA;&lt;a href=&#34;https://www.paypal.com/paypalme/DeepFaceLab&#34;&gt;Donate via Paypal&lt;/a&gt;&#xA;&lt;/td&gt;&lt;/tr&gt;--&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &lt;a href=&#34;https://money.yandex.ru/to/41001142318065&#34;&gt;Donate via Yandex.Money&lt;/a&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; bitcoin:bc1qewl062v70rszulml3f0mjdjrys8uxdydw3v6rq &lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td colspan=&#34;2&#34; align=&#34;center&#34;&gt; &#xA;    &lt;!--&#xA;    &lt;a href=&#34;https://br-stone.online&#34;&gt;&lt;img src=&#34;doc/logo_barclay_stone.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;&lt;a href=&#34;https://exmo.com&#34;&gt;&lt;img src=&#34;doc/logo_exmo.png&#34;&gt;&lt;/img&gt;&lt;/a&gt;&#xA;&#xA;    presents&#xA;&#xA;    &lt;tr&gt;&lt;td align=&#34;right&#34;&gt;&#xA;&#xA;&#xA;    &lt;a href=&#34;&#34;&gt;Windows (magnet link)&lt;/a&gt;&#xA;    &lt;/td&gt;&lt;td align=&#34;center&#34;&gt;Latest release. Use torrent client to download.&lt;/td&gt;&lt;/tr&gt;&#xA;    &lt;/tr&gt;&#xA;--&gt; &lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>PyTorchLightning/pytorch-lightning</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/PyTorchLightning/pytorch-lightning</id>
    <link href="https://github.com/PyTorchLightning/pytorch-lightning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/docs/source/_static/images/logo.png&#34; width=&#34;400px&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.pytorchlightning.ai/&#34;&gt;Website&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/#key-features&#34;&gt;Key Features&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/#how-to-use&#34;&gt;How To Use&lt;/a&gt; • &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/&#34;&gt;Docs&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/#examples&#34;&gt;Examples&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/#community&#34;&gt;Community&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/#grid-ai&#34;&gt;Grid AI&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/#license&#34;&gt;License&lt;/a&gt; &lt;/p&gt; &#xA; &lt;!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pytorch-lightning/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pytorch-lightning&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/pytorch-lightning.svg?sanitize=true&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/pytorch-lightning&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/v/conda-forge/pytorch-lightning?label=conda&amp;amp;color=success&#34; alt=&#34;Conda&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/pytorchlightning/pytorch_lightning&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/pytorchlightning/pytorch_lightning.svg?sanitize=true&#34; alt=&#34;DockerHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/PyTorchLightning/pytorch-lightning&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/PyTorchLightning/pytorch-lightning/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pytorch-lightning/badge/?version=stable&#34; alt=&#34;ReadTheDocs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.pytorchlightning.ai/community&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-chat-green.svg?logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PytorchLightning/pytorch-lightning/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!--&#xA;[![CodeFactor](https://www.codefactor.io/repository/github/pytorchlightning/pytorch-lightning/badge)](https://www.codefactor.io/repository/github/pytorchlightning/pytorch-lightning)&#xA;--&gt; &#xA;&lt;/div&gt; &#xA;&lt;h6&gt;*Codecov is &amp;gt; 90%+ but build delays may show less&lt;/h6&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;PyTorch Lightning is just organized PyTorch&lt;/h2&gt; &#xA;&lt;p&gt;Lightning disentangles PyTorch code to decouple the science from the engineering. &lt;img src=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/docs/source/_static/images/general/pl_quick_start_full_compressed.gif&#34; alt=&#34;PT to PL&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Lightning Design Philosophy&lt;/h2&gt; &#xA;&lt;p&gt;Lightning structures PyTorch code with these principles:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/philosophies.jpg&#34; max-height=&#34;250px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Lightning forces the following structure to your code which makes it reusable and shareable:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Research code (the LightningModule).&lt;/li&gt; &#xA; &lt;li&gt;Engineering code (you delete, and is handled by the Trainer).&lt;/li&gt; &#xA; &lt;li&gt;Non-essential research code (logging, etc... this goes in Callbacks).&lt;/li&gt; &#xA; &lt;li&gt;Data (use PyTorch DataLoaders or organize them into a LightningDataModule).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you do this, you can train on multiple-GPUs, TPUs, CPUs, IPUs, HPUs and even in 16-bit precision without changing your code!&lt;/p&gt; &#xA;&lt;p&gt;Get started with our &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/starter/new-project.html&#34;&gt;2 step guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Continuous Integration&lt;/h2&gt; &#xA;&lt;p&gt;Lightning is rigorously tested across multiple CPUs, GPUs, TPUs, IPUs, and HPUs and against major Python and PyTorch versions.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Current build statuses&lt;/summary&gt; &#xA; &lt;center&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;System / PyTorch ver.&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;1.8 (LTS, min. req.)&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;1.9&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;1.10&lt;/th&gt; &#xA;     &lt;th align=&#34;center&#34;&gt;1.11 (latest)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.7 [GPUs**]&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_build/latest?definitionId=6&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_apis/build/status/PL.pytorch-lightning%20(GPUs)?branchName=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.7 [TPUs***]&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://circleci.com/gh/PyTorchLightning/pytorch-lightning/tree/master&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/PyTorchLightning/pytorch-lightning/tree/master.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.8 [IPUs]&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_build/latest?definitionId=6&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_apis/build/status/PL.pytorch-lightning%20(IPUs)?branchName=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.8 [HPUs]&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_build/latest?definitionId=6&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/PytorchLightning/pytorch-lightning/_apis/build/status/PL.pytorch-lightning%20(HPUs)?branchName=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.8 (with Conda)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.9 (with Conda)&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Linux py3.{7,9}&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;OSX py3.{7,9}&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;Windows py3.{7,9}&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml&#34;&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/actions/workflows/ci_test-full.yml/badge.svg?branch=master&amp;amp;event=push&#34; alt=&#34;Test&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;** tests run on two NVIDIA P100&lt;/em&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;*** tests run on Google GKE TPUv2/3. TPU py3.7 means we support Colab and Kaggle env.&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/center&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;How To Use&lt;/h2&gt; &#xA;&lt;h3&gt;Step 0: Install&lt;/h3&gt; &#xA;&lt;p&gt;Simple installation from PyPI&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pytorch-lightning&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- following section will be skipped from PyPI description --&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Other installation options&lt;/summary&gt; &#xA; &lt;!-- following section will be skipped from PyPI description --&gt; &#xA; &lt;h4&gt;Install with optional dependencies&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pytorch-lightning[&#39;extra&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Conda&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch-lightning -c conda-forge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Install stable 1.5.x&lt;/h4&gt; &#xA; &lt;p&gt;the actual status of 1.5 [stable] is following:&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20basic%20testing/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;CI basic testing&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20complete%20testing/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;CI complete testing&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/PyTorch%20&amp;amp;%20Conda/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;PyTorch &amp;amp; Conda&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/TPU%20tests/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;TPU tests&#34;&gt; &lt;img src=&#34;https://github.com/PyTorchLightning/pytorch-lightning/workflows/Docs%20check/badge.svg?branch=release%2F1.5.x&amp;amp;event=push&#34; alt=&#34;Docs check&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;Install future release from the source&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@release/1.5.x --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Install bleeding-edge - future 1.6&lt;/h4&gt; &#xA; &lt;p&gt;Install nightly from the source (no guarantees)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install https://github.com/PyTorchLightning/pytorch-lightning/archive/master.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;or from testing PyPI&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -iU https://test.pypi.org/simple/ pytorch-lightning&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- end skipping PyPI description --&gt; &#xA;&lt;h3&gt;Step 1: Add these imports&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import torch&#xA;from torch import nn&#xA;import torch.nn.functional as F&#xA;from torchvision.datasets import MNIST&#xA;from torch.utils.data import DataLoader, random_split&#xA;from torchvision import transforms&#xA;import pytorch_lightning as pl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Define a LightningModule (nn.Module subclass)&lt;/h3&gt; &#xA;&lt;p&gt;A LightningModule defines a full &lt;em&gt;system&lt;/em&gt; (ie: a GAN, autoencoder, BERT or a simple Image Classifier).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LitAutoEncoder(pl.LightningModule):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))&#xA;        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))&#xA;&#xA;    def forward(self, x):&#xA;        # in lightning, forward defines the prediction/inference actions&#xA;        embedding = self.encoder(x)&#xA;        return embedding&#xA;&#xA;    def training_step(self, batch, batch_idx):&#xA;        # training_step defines the train loop. It is independent of forward&#xA;        x, y = batch&#xA;        x = x.view(x.size(0), -1)&#xA;        z = self.encoder(x)&#xA;        x_hat = self.decoder(z)&#xA;        loss = F.mse_loss(x_hat, x)&#xA;        self.log(&#34;train_loss&#34;, loss)&#xA;        return loss&#xA;&#xA;    def configure_optimizers(self):&#xA;        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)&#xA;        return optimizer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Training_step defines the training loop. Forward defines how the LightningModule behaves during inference/prediction.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Train!&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())&#xA;train, val = random_split(dataset, [55000, 5000])&#xA;&#xA;autoencoder = LitAutoEncoder()&#xA;trainer = pl.Trainer()&#xA;trainer.fit(autoencoder, DataLoader(train), DataLoader(val))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advanced features&lt;/h2&gt; &#xA;&lt;p&gt;Lightning has over &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags&#34;&gt;40+ advanced features&lt;/a&gt; designed for professional AI research at scale.&lt;/p&gt; &#xA;&lt;p&gt;Here are some examples:&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg&#34; max-height=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Highlighted feature code snippets&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 8 GPUs&#xA;# no code changes needed&#xA;trainer = Trainer(max_epochs=1, accelerator=&#34;gpu&#34;, devices=8)&#xA;&#xA;# 256 GPUs&#xA;trainer = Trainer(max_epochs=1, accelerator=&#34;gpu&#34;, devices=8, num_nodes=32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Train on TPUs without code changes&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# no code changes needed&#xA;trainer = Trainer(accelerator=&#34;tpu&#34;, devices=8)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;16-bit precision&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# no code changes needed&#xA;trainer = Trainer(precision=16)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Experiment managers&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pytorch_lightning import loggers&#xA;&#xA;# tensorboard&#xA;trainer = Trainer(logger=TensorBoardLogger(&#34;logs/&#34;))&#xA;&#xA;# weights and biases&#xA;trainer = Trainer(logger=loggers.WandbLogger())&#xA;&#xA;# comet&#xA;trainer = Trainer(logger=loggers.CometLogger())&#xA;&#xA;# mlflow&#xA;trainer = Trainer(logger=loggers.MLFlowLogger())&#xA;&#xA;# neptune&#xA;trainer = Trainer(logger=loggers.NeptuneLogger())&#xA;&#xA;# ... and dozens more&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;EarlyStopping&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;es = EarlyStopping(monitor=&#34;val_loss&#34;)&#xA;trainer = Trainer(callbacks=[es])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Checkpointing&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;checkpointing = ModelCheckpoint(monitor=&#34;val_loss&#34;)&#xA;trainer = Trainer(callbacks=[checkpointing])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Export to torchscript (JIT) (production use)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# torchscript&#xA;autoencoder = LitAutoEncoder()&#xA;torch.jit.save(autoencoder.to_torchscript(), &#34;model.pt&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;summary&gt;Export to ONNX (production use)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# onnx&#xA;with tempfile.NamedTemporaryFile(suffix=&#34;.onnx&#34;, delete=False) as tmpfile:&#xA;    autoencoder = LitAutoEncoder()&#xA;    input_sample = torch.randn((1, 64))&#xA;    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)&#xA;    os.path.isfile(tmpfile.name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Pro-level control of training loops (advanced users)&lt;/h3&gt; &#xA;&lt;p&gt;For complex/professional level work, you have optional full control of the training loop and optimizers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LitAutoEncoder(pl.LightningModule):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        self.automatic_optimization = False&#xA;&#xA;    def training_step(self, batch, batch_idx):&#xA;        # access your optimizers with use_pl_optimizer=False. Default is True&#xA;        opt_a, opt_b = self.optimizers(use_pl_optimizer=True)&#xA;&#xA;        loss_a = ...&#xA;        self.manual_backward(loss_a, opt_a)&#xA;        opt_a.step()&#xA;        opt_a.zero_grad()&#xA;&#xA;        loss_b = ...&#xA;        self.manual_backward(loss_b, opt_b, retain_graph=True)&#xA;        self.manual_backward(loss_b, opt_b)&#xA;        opt_b.step()&#xA;        opt_b.zero_grad()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Advantages over unstructured PyTorch&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Models become hardware agnostic&lt;/li&gt; &#xA; &lt;li&gt;Code is clear to read because engineering code is abstracted away&lt;/li&gt; &#xA; &lt;li&gt;Easier to reproduce&lt;/li&gt; &#xA; &lt;li&gt;Make fewer mistakes because lightning handles the tricky engineering&lt;/li&gt; &#xA; &lt;li&gt;Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate&lt;/li&gt; &#xA; &lt;li&gt;Lightning has dozens of integrations with popular machine learning tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/tree/master/tests&#34;&gt;Tested rigorously with every new PR&lt;/a&gt;. We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.&lt;/li&gt; &#xA; &lt;li&gt;Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Lightning Lite&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/docs/source/_static/images/lightning_lite/lite.gif&#34; height=&#34;200px&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;In the Lightning 1.5 release, LightningLite now enables you to leverage all the capabilities of PyTorch Lightning Accelerators without any refactoring to your training loop. Check out the &lt;a href=&#34;https://devblog.pytorchlightning.ai/scale-your-pytorch-code-with-lightninglite-d5692a303f00&#34;&gt;blogpost&lt;/a&gt; and &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/starter/lightning_lite.html&#34;&gt;docs&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h6&gt;Hello world&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/mnist-hello-world.html&#34;&gt;MNIST hello world&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Contrastive Learning&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#byol&#34;&gt;BYOL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#cpc-v2&#34;&gt;CPC v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#moco-v2-api&#34;&gt;Moco v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/self_supervised.html#simclr&#34;&gt;SIMCLR&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;NLP&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/convolutional.html#gpt-2&#34;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/text-transformers.html&#34;&gt;BERT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Reinforcement Learning&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/reinforce_learn.html#dqn-models&#34;&gt;DQN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/reinforce_learn.html#dueling-dqn&#34;&gt;Dueling-DQN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/reinforce_learn.html#reinforce&#34;&gt;Reinforce&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Vision&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/basic-gan.html&#34;&gt;GAN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h6&gt;Classic ML&lt;/h6&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/classic_ml.html#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lightning-bolts.readthedocs.io/en/stable/deprecated/models/classic_ml.html#linear-regression&#34;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;The lightning community is maintained by&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/latest/governance.html&#34;&gt;10+ core contributors&lt;/a&gt; who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.&lt;/li&gt; &#xA; &lt;li&gt;590+ active community contributors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want to help us build Lightning and reduce boilerplate for thousands of researchers? &lt;a href=&#34;https://devblog.pytorchlightning.ai/quick-contribution-guide-86d977171b3a&#34;&gt;Learn how to make your first contribution here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lightning is also part of the &lt;a href=&#34;https://pytorch.org/ecosystem/&#34;&gt;PyTorch ecosystem&lt;/a&gt; which requires projects to have solid testing, documentation and support.&lt;/p&gt; &#xA;&lt;h3&gt;Asking for help&lt;/h3&gt; &#xA;&lt;p&gt;If you have any questions please:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch-lightning.rtfd.io/en/latest&#34;&gt;Read the docs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/discussions&#34;&gt;Search through existing Discussions&lt;/a&gt;, or &lt;a href=&#34;https://github.com/PyTorchLightning/pytorch-lightning/discussions/new&#34;&gt;add a new question&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pytorchlightning.ai/community&#34;&gt;Join our slack&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Funding&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://techcrunch.com/2020/10/08/grid-ai-raises-18-6m-series-a-to-help-ai-researchers-and-engineers-bring-their-models-to-production/&#34;&gt;We&#39;re venture funded&lt;/a&gt; to make sure we can provide around the clock support, hire a full-time staff, attend conferences, and move faster through implementing features you request.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Grid AI&lt;/h2&gt; &#xA;&lt;p&gt;Grid AI is our platform for training models at scale on the cloud!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sign up for our FREE community Tier &lt;a href=&#34;https://www.grid.ai/pricing/&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use grid, take your regular command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python my_model.py --learning_rate 1e-6 --layers 2 --accelerator &#39;gpu&#39; --devices 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And change it to use the grid train command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;grid train --grid_gpus 4 my_model.py --learning_rate &#39;uniform(1e-6, 1e-1, 20)&#39; --layers &#39;[2, 4, 8, 16]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above command will launch (20 * 4) experiments each running on 4 GPUs (320 GPUs!) - by making ZERO changes to your code.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ergrelet/unlicense</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/ergrelet/unlicense</id>
    <link href="https://github.com/ergrelet/unlicense" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dynamic unpacker and import fixer for Themida/WinLicense 2.x and 3.x.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Unlicense &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.8+-blue.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/ergrelet/unlicense/actions/workflows/win64-ci.yml/badge.svg?branch=main&#34; alt=&#34;CI status x64&#34;&gt; &lt;img src=&#34;https://github.com/ergrelet/unlicense/actions/workflows/win32-ci.yml/badge.svg?branch=main&#34; alt=&#34;CI status x86&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A Python 3 tool to dynamically unpack executables protected with Themida/WinLicense 2.x and 3.x.&lt;/p&gt; &#xA;&lt;p&gt;Warning: This tool will execute the target executable. Make sure to use this tool in a VM if you&#39;re unsure about what the target executable does.&lt;/p&gt; &#xA;&lt;p&gt;Note: You need to use a 32-bit Python interpreter to dump 32-bit executables.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Handles Themida/Winlicense 2.x and 3.x&lt;/li&gt; &#xA; &lt;li&gt;Handles 32-bit and 64-bit PEs (EXEs and DLLs)&lt;/li&gt; &#xA; &lt;li&gt;Handles 32-bit and 64-bit .NET assemblies (EXEs only)&lt;/li&gt; &#xA; &lt;li&gt;Recovers the original entry point (OEP) automatically&lt;/li&gt; &#xA; &lt;li&gt;Recovers the (obfuscated) import table automatically&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Doesn&#39;t handle .NET assembly DLLs&lt;/li&gt; &#xA; &lt;li&gt;Doesn&#39;t automatically recover OEPs for executables with virtualized entry points&lt;/li&gt; &#xA; &lt;li&gt;Doesn&#39;t produce runnable dumps in most cases&lt;/li&gt; &#xA; &lt;li&gt;Resolving imports for 32-bit executables packed with Themida 2.x is pretty slow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How To&lt;/h2&gt; &#xA;&lt;h3&gt;Download&lt;/h3&gt; &#xA;&lt;p&gt;You can either download the PyInstaller-generated executables from the &#34;Releases&#34; section or fetch the project with &lt;code&gt;git&lt;/code&gt; and install it with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ergrelet/unlicense.git&#xA;$ pip install unlicense/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want to deal the command-line interface (CLI) you can simply drag-and-drop the target binary on the appropriate (32-bit or 64-bit) &lt;code&gt;unlicense&lt;/code&gt; executable (which is available in the &#34;Releases&#34; section).&lt;/p&gt; &#xA;&lt;p&gt;Otherwise here&#39;s what the CLI looks like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ unlicense --help&#xA;NAME&#xA;    unlicense - Unpack executables protected with Themida/WinLicense 2.x and 3.x&#xA;&#xA;SYNOPSIS&#xA;    unlicense EXE_TO_DUMP &amp;lt;flags&amp;gt;&#xA;&#xA;DESCRIPTION&#xA;    Unpack executables protected with Themida/WinLicense 2.x and 3.x&#xA;&#xA;POSITIONAL ARGUMENTS&#xA;    EXE_TO_DUMP&#xA;        Type: str&#xA;&#xA;FLAGS&#xA;    --verbose=VERBOSE&#xA;        Type: bool&#xA;        Default: False&#xA;    --pause_on_oep=PAUSE_ON_OEP&#xA;        Type: bool&#xA;        Default: False&#xA;    --force_oep=FORCE_OEP&#xA;        Type: Optional[typing.Optional[int]]&#xA;        Default: None&#xA;    --target_version=TARGET_VERSION&#xA;        Type: Optional[typing.Optional[int]]&#xA;        Default: None&#xA;    --timeout=TIMEOUT&#xA;        Type: int&#xA;        Default: 10&#xA;&#xA;NOTES&#xA;    You can also use flags syntax for POSITIONAL ARGUMENTS&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>elebumm/RedditVideoMakerBot</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/elebumm/RedditVideoMakerBot</id>
    <link href="https://github.com/elebumm/RedditVideoMakerBot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create Reddit Videos with just✨ one command ✨&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Reddit Video Maker Bot 🎥&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/6053155/170525726-2db23ae0-97b8-4bd1-8c95-00da60ce099f.mp4&#34;&gt;https://user-images.githubusercontent.com/6053155/170525726-2db23ae0-97b8-4bd1-8c95-00da60ce099f.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All done WITHOUT video editing or asset compiling. Just pure ✨programming magic✨.&lt;/p&gt; &#xA;&lt;p&gt;Created by Lewis Menelaws &amp;amp; &lt;a href=&#34;https://tmrrwinc.ca&#34;&gt;TMRRW&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&#xA; &lt;picture&gt;&lt;/picture&gt;&lt;/p&gt; &#xA;&lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://user-images.githubusercontent.com/6053155/170528535-e274dc0b-7972-4b27-af22-637f8c370133.png&#34;&gt; &#xA;&lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png&#34;&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png&#34; width=&#34;350&#34;&gt; ](https://tmrrwinc.ca) &#xA;&lt;h2&gt;Motivation 🤔&lt;/h2&gt; &#xA;&lt;p&gt;These videos on TikTok, YouTube and Instagram get MILLIONS of views across all platforms and require very little effort. The only original thing being done is the editing and gathering of all materials...&lt;/p&gt; &#xA;&lt;p&gt;... but what if we can automate that process? 🤔&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimers 🚨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is purely for fun purposes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;At the moment&lt;/strong&gt;, this repository won&#39;t attempt to upload this content through this bot. It will give you a file that you will then have to upload manually. This is for the sake of avoiding any sort of community guideline issues.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.6+&lt;/li&gt; &#xA; &lt;li&gt;Playwright (this should install automatically during installation)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation 👩‍💻&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository&lt;/li&gt; &#xA; &lt;li&gt;Rename &lt;code&gt;.env.template&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; and replace all values with the appropriate fields. To get Reddit keys (&lt;strong&gt;required&lt;/strong&gt;), visit &lt;a href=&#34;https://www.reddit.com/prefs/apps&#34;&gt;the Reddit Apps page.&lt;/a&gt; TL;DR set up an app that is a &#34;script&#34;. Copy your keys into the &lt;code&gt;.env&lt;/code&gt; file, along with whether your account uses two-factor authentication.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;playwright install&lt;/code&gt; and &lt;code&gt;playwright install-deps&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python3 main.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA; &lt;li&gt;Enjoy 😎&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing &amp;amp; Ways to improve 📈&lt;/h2&gt; &#xA;&lt;p&gt;In its current state, this bot does exactly what it needs to do. However, lots of improvements can be made.&lt;/p&gt; &#xA;&lt;p&gt;I have tried to simplify the code so anyone can read it and start contributing at any skill level. Don&#39;t be shy :) contribute!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Allowing users to choose a reddit thread instead of being randomized.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Allowing users to choose a background that is picked instead of the Minecraft one.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Allowing users to choose between any subreddit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Allowing users to change voice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Creating better documentation and adding a command line interface.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>EssayKillerBrain/EssayKiller_V2</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/EssayKillerBrain/EssayKiller_V2</id>
    <link href="https://github.com/EssayKillerBrain/EssayKiller_V2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;基于开源GPT2.0的初代创作型人工智能 | 可扩展、可进化&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EssayKiller&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache--2.0-green&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-orange&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Anti--996-red&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/pypi-v0.0.1a4-yellowgreen&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/stars-%3C%201k-blue&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/issues-1%20open-brightgreen&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;通用型议论文创作人工智能框架，仅限交流与科普。&lt;/p&gt; &#xA;&lt;p&gt;Bilibili视频地址：&lt;a href=&#34;https://www.bilibili.com/video/BV1pr4y1w7uM/&#34;&gt;https://www.bilibili.com/video/BV1pr4y1w7uM/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目简介&lt;/h2&gt; &#xA;&lt;p&gt;EssayKiller是基于OCR、NLP领域的最新模型所构建的生成式文本创作AI框架，目前第一版finetune模型针对高考作文（主要是议论文），可以有效生成符合人类认知的文章，多数文章经过测试可以达到正常高中生及格作文水平。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;项目作者&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;主页1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;主页2&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;主页3&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;图灵的猫&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.zhihu.com/people/dong-xi-97-29&#34;&gt;知乎&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://space.bilibili.com/371846699&#34;&gt;B站&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCoEVP6iTw5sfozUGLLWJyDg/featured&#34;&gt;Youtube&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;感谢开源作者&lt;a href=&#34;https://github.com/imcaspar&#34;&gt;@imcaspar&lt;/a&gt; 提供GPT-2中文预训练框架与数据支持。 感谢&lt;a href=&#34;https://www.zhihu.com/people/youngfish42&#34;&gt;@白小鱼博士&lt;/a&gt; 、&lt;a href=&#34;https://www.zhihu.com/people/YJango&#34;&gt;@YJango博士&lt;/a&gt; 、&lt;a href=&#34;https://space.bilibili.com/402576555&#34;&gt;@画渣花小烙&lt;/a&gt;、&lt;a href=&#34;https://space.bilibili.com/328531988/&#34;&gt;@万物拣史&lt;/a&gt; 、&lt;a href=&#34;https://space.bilibili.com/26798384/&#34;&gt;@柴知道&lt;/a&gt;、&lt;a href=&#34;https://space.bilibili.com/17466521/&#34;&gt;@风羽酱-sdk&lt;/a&gt;、&lt;a href=&#34;https://space.bilibili.com/410527811/&#34;&gt;@WhatOnEarth&lt;/a&gt;、&lt;a href=&#34;https://space.bilibili.com/403943112/&#34;&gt;@这知识好冷&lt;/a&gt;、&lt;a href=&#34;https://space.bilibili.com/40433405/&#34;&gt;@科技狐&lt;/a&gt; 的参与和支持 &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;框架说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 基于EAST、CRNN、Bert和GPT-2语言模型的高考作文生成AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 支持bert tokenizer，当前版本基于clue chinese vocab&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 17亿参数多模块异构深度神经网络，超2亿条预训练数据&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 线上点击即用的文本生成效果demo：&lt;a href=&#34;https://colab.research.google.com/github/EssayKillerBrain/EssayKiller_V2/blob/master/colab_online.ipynb&#34;&gt;17亿参数作文杀手&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 端到端生成，从试卷识别到答题卡输出一条龙服务&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Colab线上作文生成功能&lt;/h3&gt; &#xA;&lt;p&gt;国内没有足够显存的免费GPU平台，所以配合Google Drive将训练好的AI核心功能Language Network写作模块迁移到Colab。&lt;/p&gt; &#xA;&lt;p&gt;当前线上仅开放文本生成功能，输入对应句子，AI返回生成文章。同一个句子可以输入多次，每一次输出都不同。也可以选择同时生成多篇文章。具体见：&lt;a href=&#34;https://colab.research.google.com/github/EssayKillerBrain/EssayKiller_V2/blob/master/colab_online.ipynb&#34;&gt;17亿参数作文杀手&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;第一步：安装环境 &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-15-22-13.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;第二部：加载模型 &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-15-27-38.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;第三步：文章生成 &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-15-27-14.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;写作效果 &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-15-23-27.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;本地环境&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 18.04.2&lt;/li&gt; &#xA; &lt;li&gt;Pandas 0.24.2&lt;/li&gt; &#xA; &lt;li&gt;Regex 2019.4.14&lt;/li&gt; &#xA; &lt;li&gt;h5py 2.9.0&lt;/li&gt; &#xA; &lt;li&gt;Numpy 1.16.2&lt;/li&gt; &#xA; &lt;li&gt;Tensorboard 1.15.2&lt;/li&gt; &#xA; &lt;li&gt;Tensorflow-gpu 1.15.2&lt;/li&gt; &#xA; &lt;li&gt;Requests 2.22.0&lt;/li&gt; &#xA; &lt;li&gt;OpenCV 3.4.2&lt;/li&gt; &#xA; &lt;li&gt;CUDA &amp;gt;= 10.0&lt;/li&gt; &#xA; &lt;li&gt;CuDNN &amp;gt;= 7.6.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;开发日志&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2020.06.23 本地Git项目建立&lt;/li&gt; &#xA; &lt;li&gt;2020.07.03 整体模型架构搭建，开始语料收集&lt;/li&gt; &#xA; &lt;li&gt;2020.07.13 基于OCR的视觉网络训练&lt;/li&gt; &#xA; &lt;li&gt;2020.08.01 GPT-2中文预训练模型微调&lt;/li&gt; &#xA; &lt;li&gt;2020.08.14 Bert文本摘要模型&lt;/li&gt; &#xA; &lt;li&gt;2020.08.23 通顺度判分网络测试&lt;/li&gt; &#xA; &lt;li&gt;2020.09.14 排版脚本与输出装置改装&lt;/li&gt; &#xA; &lt;li&gt;2021.02.15 修复网页版模型打分&lt;/li&gt; &#xA; &lt;li&gt;2021.06.10 训练集中增加了《共产党宣言》、《毛泽东选集》、《陈独秀文集》、《鲁迅文集》等著作&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型结构&lt;/h2&gt; &#xA;&lt;p&gt;整个框架分为EAST、CRNN、Bert、GPT-2、DNN 5个模块，每个模块的网络单独训练，参数相互独立。infer过程使用pipeline串联，通过外接装置直接输出到答题卡。&lt;br&gt; &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-15-35-00.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1. 输入&lt;/h3&gt; &#xA;&lt;p&gt;高考语文试卷作文题&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://images.shobserver.com/img/2020/7/7/37b2224ee3de441a8a040cb4f5576c2d.jpg&#34; alt=&#34;浙江卷&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2. 识别网络&lt;/h3&gt; &#xA;&lt;h4&gt;2.1 EAST文本检测&lt;/h4&gt; &#xA;&lt;p&gt;OpenCV 的EAST文本检测器是一个深度学习模型，它能够在 720p 的图像上以13帧/秒的速度实时检测任意方向的文本，并可以获得很好的文本检测精度。&lt;br&gt; &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-15-45-54.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;模型亮点&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;简单的管道实现在当时较高精度的文本检测。&lt;/li&gt; &#xA; &lt;li&gt;图像通过FCN处理产生像素级文本缩放地图和几何图形的多个频道。&lt;/li&gt; &#xA; &lt;li&gt;可旋转的文本框，可以检测文本也可以检测单词。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;EAST文本检测器需要 OpenCV3.4.2 或更高的版本，有需要的读者可以查看 &lt;a href=&#34;https://www.pyimagesearch.com/opencv-tutorials-resources-guides/&#34;&gt;OpenCV 安装教程&lt;/a&gt;。虽然EAST的模型在检测自然场景下的英文文本有着较好的性能，要实现中文场景下的中文文本检测，仍然需要重新训练模型。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;数据集处理&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;中文文本识别的数据集要按照原作者的命名方式修改，即使使用ICDAR3013这类标准数据集，也需要修改对应的图片命名方式。原代码数据集的命名方式：图片1.jpg 图片1.txt。&lt;/p&gt; &#xA;&lt;p&gt;此外，代码是通过获取文件类型然后重新命名以原来的文件类型保存的，所以文本数据和图片数据需要分开处理。&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;训练命令：&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python multigpu_train.py --gpu_list=0 --input_size=512 --batch_size_per_gpu=14 --checkpoint_path=/tmp/east_icdar2015_resnet_v1_50_rbox/ \ --text_scale=512 --training_data_path=/data/ocr/icdar2015/ --geometry=RBOX --learning_rate=0.0001 --num_readers=24 \ --pretrained_model_path=/tmp/resnet_v1_50.ckpt &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;更多细节可以参考：&lt;a href=&#34;https://zhuanlan.zhihu.com/p/64737915&#34;&gt;https://zhuanlan.zhihu.com/p/64737915&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;检测结果&lt;/em&gt; &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-16-25-01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;除了EAST，也可以把识别网络替换为传统的CTPN等模型，github上有已经成熟的项目：&lt;a href=&#34;https://github.com/Walleclipse/ChineseAddress_OCR&#34;&gt;https://github.com/Walleclipse/ChineseAddress_OCR&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;2.2 CRNN文本识别&lt;/h4&gt; &#xA;&lt;p&gt;参考 &lt;a href=&#34;https://github.com/ooooverflow/chinese-ocr&#34;&gt;https://github.com/ooooverflow/chinese-ocr&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;数据准备&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;下载&lt;a href=&#34;https://pan.baidu.com/s/1E_1iFERWr9Ro-dmlSVY8pA&#34;&gt;训练集&lt;/a&gt;：共约364万张图片，按照99: 1划分成训练集和验证集&lt;/p&gt; &#xA;&lt;p&gt;数据利用中文语料库（新闻 + 文言文），通过字体、大小、灰度、模糊、透视、拉伸等变化随机生成。包含汉字、英文字母、数字和标点共5990个字符，每个样本固定10个字符，字符随机截取自语料库中的句子，图片分辨率统一为280x32。&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;修改/train/config.py中train_data_root，validation_data_root以及image_path&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;训练&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd train  &#xA;python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;训练结果&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Epoch 3/100&#xA;25621/25621 [==============================] - 15856s 619ms/step - loss: 0.1035 - acc: 0.9816 - val_loss: 0.1060 - val_acc: 0.9823&#xA;Epoch 4/100&#xA;25621/25621 [==============================] - 15651s 611ms/step - loss: 0.0798 - acc: 0.9879 - val_loss: 0.0848 - val_acc: 0.9878&#xA;Epoch 5/100&#xA;25621/25621 [==============================] - 16510s 644ms/step - loss: 0.0732 - acc: 0.9889 - val_loss: 0.0815 - val_acc: 0.9881&#xA;Epoch 6/100&#xA;25621/25621 [==============================] - 15621s 610ms/step - loss: 0.0691 - acc: 0.9895 - val_loss: 0.0791 - val_acc: 0.9886&#xA;Epoch 7/100&#xA;25621/25621 [==============================] - 15782s 616ms/step - loss: 0.0666 - acc: 0.9899 - val_loss: 0.0787 - val_acc: 0.9887&#xA;Epoch 8/100&#xA;25621/25621 [==============================] - 15560s 607ms/step - loss: 0.0645 - acc: 0.9903 - val_loss: 0.0771 - val_acc: 0.9888&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/ooooverflow/chinese-ocr/raw/master/demo/ocr.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;2. 语言网络&lt;/h3&gt; &#xA;&lt;h4&gt;2.1 BERT文本摘要&lt;/h4&gt; &#xA;&lt;p&gt;BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder。模型的主要创新点在pre-train方法上，用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。&lt;/p&gt; &#xA;&lt;p&gt;模型的构成元素Transformer可以参考Google的 &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention is all you need&lt;/a&gt; ，BERT模型的结构如下图最左： &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-16-44-54.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向RNN和双向RNN的区别，直觉上来讲效果会好一些。 &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;在原论文中，作者展示了新的语言训练模型，称为编码语言模型与下一句预测&lt;/p&gt; &#xA;&lt;p&gt;Original Paper : 3.3.1 Task #1: Masked LM&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Input Sequence : The man went to [MASK] store with [MASK] dog Target Sequence : the his&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;规则: 会有15%的随机输入被改变，这些改变基于以下规则&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;80%的tokens会成为‘掩码’token&lt;/li&gt; &#xA; &lt;li&gt;10%的tokens会称为‘随机’token&lt;/li&gt; &#xA; &lt;li&gt;10%的tokens会保持不变但需要被预测&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;下一句预测&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP] Label : Is Next Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;规则:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;50%的下一句会（随机）成为连续句子&lt;/li&gt; &#xA; &lt;li&gt;50%的下一句会（随机）成为不关联句子&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;训练&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;哈工大的新浪微博短文本摘要&lt;a href=&#34;http://icrc.hitsz.edu.cn/Article/show/139.html&#34;&gt;LCSTS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;教育新闻自动摘要语料&lt;a href=&#34;https://github.com/wonderfulsuccess/chinese_abstractive_corpus&#34;&gt;chinese_abstractive_corpus&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --model bert&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-16-40-19.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;测试时，需要用正则表达式过滤考试专用词，包括“阅读下面的材料，根据要求写作”，“要求：xxx”，“请完成/请结合/请综合xx”。&lt;/p&gt; &#xA;&lt;p&gt;比如&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-17-17-30.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;人们用眼睛看他人、看世界，却无法直接看到完整的自己。所以，在人生的旅程中，我们需要寻找各种“镜子”、不断绘制“自画像”来审视自我，尝试回答“我是怎样的人”“我想过怎样的生活”“我能做些什么”“如何生活得更有意义”等重要的问题。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h4&gt;2.2 GPT-2文本生成&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/prakhar21/TextAugmentation-GPT2/raw/master/gpt2-sizes.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;参考：&lt;a href=&#34;https://github.com/imcaspar/gpt2-ml/&#34;&gt;https://github.com/imcaspar/gpt2-ml/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;预训练语料来自 &lt;a href=&#34;http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews&#34;&gt;THUCNews&lt;/a&gt; 以及 &lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;nlp_chinese_corpus&lt;/a&gt;，清洗后总文本量约 15G。 Finetune语料来自历年满分高考作文、优质散文集以及近现代散文作品，约1000篇。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;预训练&lt;/strong&gt;&lt;br&gt; 参考 &lt;a href=&#34;https://github.com/imcaspar/gpt2-ml/&#34;&gt;GPT2-ML&lt;/a&gt; 预训练模型，使用 &lt;a href=&#34;https://www.nvidia.com/en-us/design-visualization/quadro/rtx-8000/&#34;&gt;Quadro RTX 8000&lt;/a&gt; 训练 28w 步&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/2233.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Finetune&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;1、进入dataset目录&#xA;python pre_data.py --filepath /data/home/share1/gpt2-ml-Finetune/data-mayun_xiugai --outfile /data/home/share1/gpt2-ml-Finetune/data/22.json&#xA;filepath为finetune数据目录&#xA;&#xA;2、生成tfrecord训练数据&#xA;python prepare_data.py -input_fn /data/home/share1/gpt2-ml-Finetune/data&#xA;&#xA;3、finetune&#xA;CUDA_VISIBLE_DEVICES=0  python train/train_wc.py --input_file=/data/EssayKiller/gpt2-ml-Finetune/data/train.tfrecord --output_dir=/data/EssayKiller/gpt2-ml-Finetune/finetune_model --init_checkpoint=/data/EssayKiller/gpt2-ml/models/mega/model.ckpt-220000&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;3.判分网络&lt;/h3&gt; &#xA;&lt;h4&gt;3.1 DNN判分模型&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-18-59-12.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;这部分直接调用百度API。有现成的模型就不重复造轮子了，具体实现方式百度没有开源，这里简单描述一下语言模型的概念： 语言模型是通过计算给定词组成的句子的概率，从而判断所组成的句子是否符合客观语言表达习惯。通常用于机器翻译、拼写纠错、语音识别、问答系统、词性标注、句法分析和信息检索等。&lt;br&gt; &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-18-59-57.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;这里使用通顺度打分作为判断依据。&lt;/p&gt; &#xA;&lt;h4&gt;3.2 高考排版器&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;标题&lt;/em&gt;&lt;br&gt; 复用BERT_SUM生成Top3的NER粒度token作为标题&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;主体&lt;/em&gt;&lt;br&gt; 高考议论文的写作格式要求如下：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;标题居中，一般少于20字&lt;/li&gt; &#xA; &lt;li&gt;每段段首缩进两格&lt;/li&gt; &#xA; &lt;li&gt;每个字符尽量保持在字体框内&lt;/li&gt; &#xA; &lt;li&gt;字数不能过长或过短&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;由于模型输出的文章不保证换行和分段，通过统计高考作文的常见段数、每段句数，编写脚本对输出进行划分。大多数情况下分段排版的结果都比较合理。&lt;br&gt; &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-19-04-24.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;输出&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;答题卡&lt;/strong&gt;&lt;br&gt; &lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-19-07-53.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;外接装置&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;基于aedraw，一款开源的CNC(Computer Numerical Control数控机床)画图机器人，具有绘制图案、写字等功能，它也可以升级为激光雕刻等用途。 详细教程见 &lt;a href=&#34;http://aelab.net/&#34;&gt;http://aelab.net/&lt;/a&gt; ，不仅能自己制作一台写字绘画机器人，而且能够掌握其工作原理拓展更多的应用。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/EssayKillerBrain/EssayKiller_V2/raw/master/References/attachments/Clipboard_2020-09-29-19-12-07.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;原版的输出临摹装置存在速度慢和格式不准的问题，通过改装和修改源代码得以优化&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;因为时间原因目前的手写装置还有些问题，偶尔会有漏写、越格的问题&lt;/li&gt; &#xA; &lt;li&gt;视频中的作文经过后期的人工处理，补上了漏字&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;预训练模型&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数量&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;下载链接&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;备注&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EAST&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;lt; 0.1 Billion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1fF4IYaL7CWghYCDvRrACM57WVx83Yvny/view?usp=sharing&#34;&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;检测模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CRNN&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&amp;lt; 0.1 Billion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://eyun.baidu.com/s/3dEUJJg9&#34;&gt;网盘链接&lt;/a&gt; 提取码：vKeD&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;识别模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.1 Billion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/15DbA07DZNT3gMXu2aLliA3CkuR5XHhlt/view?usp=sharing&#34;&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;摘要模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5 Billion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ujWYTOvRLGJX0raH-f-lPZa3-RN58ZQx/view?usp=sharing&#34;&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;生成模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;整个AI的参数量分布不均匀，主要原因在于，这是一个语言类AI，99%的参数量集中在语言网络中，其中GPT-2（15亿）占88%，BERT（1.1亿）占7%，其他的识别网络和判分网络共占5%。&lt;/p&gt; &#xA;&lt;h3&gt;当前问题&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;输出的格式和高考作文还不能完美契合，之后的参数需要微调一下。为了国庆前完成，我还没来得及优化&lt;/li&gt; &#xA; &lt;li&gt;生成的100篇作文里有很大一部分其实算不上合格的作文，有些只能勉强及格，有些甚至能拿零分（占比不多），显然GPT-2的能力有限。为了视频效果我只选了相对好的几篇做展示&lt;/li&gt; &#xA; &lt;li&gt;英文版的说明还没来得及写，有空的同学可以翻译一下提个pr&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;我能否用EssayKiller来帮自己写作业？&lt;/strong&gt;&lt;br&gt; 不能。所以有下一个问题：&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;为什么缺少一些关键文件？&lt;/strong&gt;&lt;br&gt; 项目在一开始是完全开源的，经过慎重考虑我认为完全开源会被部分别有用心的人用以牟利，甚至用作不法用途。参考咸鱼和淘宝上一些魔改的开源框架应用。部分懂技术又不想动笔的小同志可能会让Essaykiller帮自己写作业，比如读后感、课后作文、思修小论文。我想说，这样不好。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;为什么不直接加密？&lt;/strong&gt;&lt;br&gt; 本来打算用混淆加密，但一些模块本就是开源的，所以我开源了整体的模型文件，只隐藏了关键的，包括pipeline、输入输出在内的文件，另外有些文件里也加了盐。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;有哪些模组可用？&lt;/strong&gt;&lt;br&gt; 目前完全开源，可以独立复用的部分包括：&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 检测网络&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 文本摘要网络&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 文本生成网络&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 判分网络与排版脚本&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;为什么不用GPT-3&lt;/strong&gt;&lt;br&gt; 训练一个中文GPT-3的价格至少为1200万美元，折合人民币将近1亿。要是真有人训练出来一个中文GPT-3还开源模型文件了，我愿称之为最强。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;训练EssayKiller需要多少钱？&lt;/strong&gt;&lt;br&gt; 从头到尾训练完pipeline的话在1K～100K人民币不等，取决于你有无分布式集群可用&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{EssayKillerBrain,&#xA;  author = {Turing&#39;s Cat},&#xA;  title = {Autowritting Ai Framework},&#xA;  year = {2020},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/EssayKillerBrain/EssayKiller}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;参考资料&lt;/h2&gt; &#xA;&lt;p&gt;[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;br&gt; [2] ERNIE: Enhanced Representation through Knowledge Integration&lt;br&gt; [3] Fine-tune BERT for Extractive Summarization&lt;br&gt; [4] EAST: An Efficient and Accurate Scene Text Detector&lt;br&gt; [5] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition&lt;br&gt; [6] Language Models are Unsupervised Multitask Learners&lt;br&gt; [7] &lt;a href=&#34;https://github.com/Morizeyao/GPT2-Chinese&#34;&gt;https://github.com/Morizeyao/GPT2-Chinese&lt;/a&gt;&lt;br&gt; [8] &lt;a href=&#34;https://github.com/argman/EAST&#34;&gt;https://github.com/argman/EAST&lt;/a&gt;&lt;br&gt; [9] &lt;a href=&#34;https://github.com/bgshih/crnn&#34;&gt;https://github.com/bgshih/crnn&lt;/a&gt;&lt;br&gt; [10] &lt;a href=&#34;https://github.com/zhiyou720/chinese_summarizer&#34;&gt;https://github.com/zhiyou720/chinese_summarizer&lt;/a&gt;&lt;br&gt; [11] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/64737915&#34;&gt;https://zhuanlan.zhihu.com/p/64737915&lt;/a&gt;&lt;br&gt; [12] &lt;a href=&#34;https://github.com/ouyanghuiyu/chineseocr_lite&#34;&gt;https://github.com/ouyanghuiyu/chineseocr_lite&lt;/a&gt;&lt;br&gt; [13] &lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://github.com/google-research/bert&lt;/a&gt;&lt;br&gt; [14] &lt;a href=&#34;https://github.com/rowanz/grover&#34;&gt;https://github.com/rowanz/grover&lt;/a&gt;&lt;br&gt; [15] &lt;a href=&#34;https://github.com/wind91725/gpt2-ml-finetune-&#34;&gt;https://github.com/wind91725/gpt2-ml-finetune-&lt;/a&gt;&lt;br&gt; [16] &lt;a href=&#34;https://github.com/guodongxiaren/README&#34;&gt;https://github.com/guodongxiaren/README&lt;/a&gt;&lt;br&gt; [17] &lt;a href=&#34;https://www.jianshu.com/p/55560d3e0e8a&#34;&gt;https://www.jianshu.com/p/55560d3e0e8a&lt;/a&gt;&lt;br&gt; [18] &lt;a href=&#34;https://github.com/YCG09/chinese_ocr&#34;&gt;https://github.com/YCG09/chinese_ocr&lt;/a&gt;&lt;br&gt; [19] &lt;a href=&#34;https://github.com/xiaomaxiao/keras_ocr&#34;&gt;https://github.com/xiaomaxiao/keras_ocr&lt;/a&gt;&lt;br&gt; [20] &lt;a href=&#34;https://github.com/nghuyong/ERNIE-Pytorch&#34;&gt;https://github.com/nghuyong/ERNIE-Pytorch&lt;/a&gt;&lt;br&gt; [21] &lt;a href=&#34;https://zhuanlan.zhihu.com/p/43534801&#34;&gt;https://zhuanlan.zhihu.com/p/43534801&lt;/a&gt;&lt;br&gt; [22] &lt;a href=&#34;https://blog.csdn.net/xuxunjie147/article/details/87178774/&#34;&gt;https://blog.csdn.net/xuxunjie147/article/details/87178774/&lt;/a&gt;&lt;br&gt; [23] &lt;a href=&#34;https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset&#34;&gt;https://github.com/JiangYanting/Pre-modern_Chinese_corpus_dataset&lt;/a&gt;&lt;br&gt; [24] &lt;a href=&#34;https://github.com/brightmart/nlp_chinese_corpus&#34;&gt;https://github.com/brightmart/nlp_chinese_corpus&lt;/a&gt;&lt;br&gt; [25] &lt;a href=&#34;https://github.com/SophonPlus/ChineseNlpCorpus&#34;&gt;https://github.com/SophonPlus/ChineseNlpCorpus&lt;/a&gt;&lt;br&gt; [26] &lt;a href=&#34;https://github.com/THUNLP-AIPoet/Resources&#34;&gt;https://github.com/THUNLP-AIPoet/Resources&lt;/a&gt;&lt;br&gt; [27] &lt;a href=&#34;https://github.com/OYE93/Chinese-NLP-Corpus&#34;&gt;https://github.com/OYE93/Chinese-NLP-Corpus&lt;/a&gt;&lt;br&gt; [28] &lt;a href=&#34;https://github.com/CLUEbenchmark/CLUECorpus2020&#34;&gt;https://github.com/CLUEbenchmark/CLUECorpus2020&lt;/a&gt;&lt;br&gt; [29] &lt;a href=&#34;https://github.com/zhiyou720/chinese_summarizer&#34;&gt;https://github.com/zhiyou720/chinese_summarizer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;该项目中的内容仅供技术研究与科普，不作为任何结论性依据，不提供任何商业化应用授权&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Asabeneh/30-Days-Of-Python</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/Asabeneh/30-Days-Of-Python</id>
    <link href="https://github.com/Asabeneh/30-Days-Of-Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🐍 30 Days Of Python&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;# Day&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/readme.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md&#34;&gt;Variables, Built-in Functions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/03_Day_Operators/03_operators.md&#34;&gt;Operators&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/04_Day_Strings/04_strings.md&#34;&gt;Strings&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/05_Day_Lists/05_lists.md&#34;&gt;Lists&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/06_Day_Tuples/06_tuples.md&#34;&gt;Tuples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/07_Day_Sets/07_sets.md&#34;&gt;Sets&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/08_Day_Dictionaries/08_dictionaries.md&#34;&gt;Dictionaries&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/09_Day_Conditionals/09_conditionals.md&#34;&gt;Conditionals&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/10_Day_Loops/10_loops.md&#34;&gt;Loops&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/11_Day_Functions/11_functions.md&#34;&gt;Functions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/12_Day_Modules/12_modules.md&#34;&gt;Modules&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/13_Day_List_comprehension/13_list_comprehension.md&#34;&gt;List Comprehension&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/14_Day_Higher_order_functions/14_higher_order_functions.md&#34;&gt;Higher Order Functions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/15_Day_Python_type_errors/15_python_type_errors.md&#34;&gt;Python Type Errors&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/16_Day_Python_date_time/16_python_datetime.md&#34;&gt;Python Date time&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/17_Day_Exception_handling/17_exception_handling.md&#34;&gt;Exception Handling&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/18_Day_Regular_expressions/18_regular_expressions.md&#34;&gt;Regular Expressions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/19_Day_File_handling/19_file_handling.md&#34;&gt;File Handling&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/20_Day_Python_package_manager/20_python_package_manager.md&#34;&gt;Python Package Manager&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/21_Day_Classes_and_objects/21_classes_and_objects.md&#34;&gt;Classes and Objects&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;22&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/22_Day_Web_scraping/22_web_scraping.md&#34;&gt;Web Scraping&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/23_Day_Virtual_environment/23_virtual_environment.md&#34;&gt;Virtual Environment&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/24_Day_Statistics/24_statistics.md&#34;&gt;Statistics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/25_Day_Pandas/25_pandas.md&#34;&gt;Pandas&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/26_Day_Python_web/26_python_web.md&#34;&gt;Python web&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/27_Day_Python_with_mongodb/27_python_with_mongodb.md&#34;&gt;Python with MongoDB&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/28_Day_API/28_API.md&#34;&gt;API&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/29_Day_Building_API/29_building_API.md&#34;&gt;Building API&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/30_Day_Conclusions/30_conclusions.md&#34;&gt;Conclusions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;🧡🧡🧡 HAPPY CODING 🧡🧡🧡&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;small&gt;Support the &lt;strong&gt;author&lt;/strong&gt; to create more educational materials&lt;/small&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://www.paypal.me/asabeneh&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/paypal_lg.png&#34; alt=&#34;Paypal Logo&#34; style=&#34;width:10%&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; 30 Days Of Python: Day 1 - Introduction&lt;/h1&gt; &#xA; &lt;a class=&#34;header-badge&#34; target=&#34;_blank&#34; href=&#34;https://www.linkedin.com/in/asabeneh/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social&#34;&gt; &lt;/a&gt; &#xA; &lt;a class=&#34;header-badge&#34; target=&#34;_blank&#34; href=&#34;https://twitter.com/Asabeneh&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/twitter/follow/asabeneh?style=social&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;&lt;sub&gt;Author: &lt;a href=&#34;https://www.linkedin.com/in/asabeneh/&#34; target=&#34;_blank&#34;&gt;Asabeneh Yetayeh&lt;/a&gt;&lt;br&gt; &lt;small&gt; Second Edition: July, 2021&lt;/small&gt; &lt;/sub&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md&#34;&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/30DaysOfPython_banner3@2x.png&#34; alt=&#34;30DaysOfPython&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-30-days-of-python&#34;&gt;🐍 30 Days Of Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-day-1&#34;&gt;📘 Day 1&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#welcome&#34;&gt;Welcome&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#why-python-&#34;&gt;Why Python ?&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#environment-setup&#34;&gt;Environment Setup&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-python&#34;&gt;Installing Python&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-shell&#34;&gt;Python Shell&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#installing-visual-studio-code&#34;&gt;Installing Visual Studio Code&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#how-to-use-visual-studio-code&#34;&gt;How to use visual studio code&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#basic-python&#34;&gt;Basic Python&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-syntax&#34;&gt;Python Syntax&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-indentation&#34;&gt;Python Indentation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#comments&#34;&gt;Comments&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#data-types&#34;&gt;Data types&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#number&#34;&gt;Number&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#string&#34;&gt;String&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#booleans&#34;&gt;Booleans&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#list&#34;&gt;List&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#dictionary&#34;&gt;Dictionary&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#tuple&#34;&gt;Tuple&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#set&#34;&gt;Set&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#checking-data-types&#34;&gt;Checking Data types&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#python-file&#34;&gt;Python File&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#-exercises---day-1&#34;&gt;💻 Exercises - Day 1&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-1&#34;&gt;Exercise: Level 1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-2&#34;&gt;Exercise: Level 2&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/#exercise-level-3&#34;&gt;Exercise: Level 3&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;📘 Day 1&lt;/h1&gt; &#xA;&lt;h2&gt;Welcome&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Congratulations&lt;/strong&gt; for deciding to participate in a &lt;em&gt;30 days of Python&lt;/em&gt; programming challenge . In this challenge you will learn everything you need to be a python programmer and the whole concept of programming. In the end of the challenge you will get a &lt;em&gt;30DaysOfPython&lt;/em&gt; programming challenge certificate.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to actively engage in the challenge, you may join the &lt;a href=&#34;https://t.me/ThirtyDaysOfPython&#34;&gt;30DaysOfPython challenge&lt;/a&gt; telegram group.&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Python is a high-level programming language for general-purpose programming. It is an open source, interpreted, objected-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of Python programming language was derived from a British sketch comedy series, &lt;em&gt;Month Python&#39;s Flying Circus&lt;/em&gt;. The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, many hands on exercises and projects.&lt;/p&gt; &#xA;&lt;p&gt;This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge, people who actively participate on the telegram group have a high probability of completing the challenge. If you are a visual learner or in favor of videos, you may get started with this &lt;a href=&#34;https://www.youtube.com/watch?v=11OYpBrhdyM&#34;&gt;Python for Absolute Beginners video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why Python ?&lt;/h2&gt; &#xA;&lt;p&gt;It is a programming language which is very close to human language and because of that it is easy to learn and use. Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system adminstration, and machine learning libraries. Python is highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Installing Python&lt;/h3&gt; &#xA;&lt;p&gt;To run a python script you need to install python. Let&#39;s &lt;a href=&#34;https://www.python.org/&#34;&gt;download&lt;/a&gt; python. If your are a windows user. Click the button encircled in red.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_windows.png&#34; alt=&#34;installing on Windows&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are a macOS user. Click the button encircled in red.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/installing_on_macOS.png&#34; alt=&#34;installing on Windows&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To check if python is installed write the following command on your device terminal.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/python_versio.png&#34; alt=&#34;Python Version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As you can see from the terminal, I am using &lt;em&gt;Python 3.7.5&lt;/em&gt; version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you mange to see the python version, well done. Python has been installed on your machine. Continue to the next section.&lt;/p&gt; &#xA;&lt;h3&gt;Python Shell&lt;/h3&gt; &#xA;&lt;p&gt;Python is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a &lt;em&gt;Python Shell (Python Interactive Shell)&lt;/em&gt;. It is used to execute a single python command and get the result.&lt;/p&gt; &#xA;&lt;p&gt;Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line. Open your terminal or command prompt(cmd) and write:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png&#34; alt=&#34;Python Scripting Shell&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol &amp;gt;&amp;gt;&amp;gt; and then click Enter. Let us write our very first script on the Python scripting shell.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/adding_on_python_shell.png&#34; alt=&#34;Python script on Python shell&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ? To close the shell, next to this symbol &amp;gt;&amp;gt; write &lt;strong&gt;exit()&lt;/strong&gt; command and press Enter.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/exit_from_shell.png&#34; alt=&#34;Exit from python shell&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now, you know how to open the Python interactive shell and how to exit from it.&lt;/p&gt; &#xA;&lt;p&gt;Python will give you results if you write scripts that Python understands, if not it returns errors. Let&#39;s make a deliberate mistake and see what Python will return.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/invalid_syntax_error.png&#34; alt=&#34;Invalid Syntax Error&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;As you can see from the returned error, Python is so clever that it knows the mistake we made and which was &lt;em&gt;Syntax Error: invalid syntax&lt;/em&gt;. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of (&lt;strong&gt;x&lt;/strong&gt;) we use asterisk (*) for multiplication. The returned error clearly shows what to fix.&lt;/p&gt; &#xA;&lt;p&gt;The process of identifying and removing errors from a program is called &lt;em&gt;debugging&lt;/em&gt;. Let us debug it by putting * in place of &lt;strong&gt;x&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/fixing_syntax_error.png&#34; alt=&#34;Fixing Syntax Error&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are &lt;em&gt;SyntaxError&lt;/em&gt;, &lt;em&gt;IndexError&lt;/em&gt;, &lt;em&gt;NameError&lt;/em&gt;, &lt;em&gt;ModuleNotFoundError&lt;/em&gt;, &lt;em&gt;KeyError&lt;/em&gt;, &lt;em&gt;ImportError&lt;/em&gt;, &lt;em&gt;AttributeError&lt;/em&gt;, &lt;em&gt;TypeError&lt;/em&gt;, &lt;em&gt;ValueError&lt;/em&gt;, &lt;em&gt;ZeroDivisionError&lt;/em&gt; etc. We will see more about different Python &lt;strong&gt;&lt;em&gt;error types&lt;/em&gt;&lt;/strong&gt; in later sections.&lt;/p&gt; &#xA;&lt;p&gt;Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word &lt;strong&gt;python&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_python_shell.png&#34; alt=&#34;Python Scripting Shell&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus, exponential).&lt;/p&gt; &#xA;&lt;p&gt;Let us do some maths first before we write any Python code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2 + 3 = 5&lt;/li&gt; &#xA; &lt;li&gt;3 - 2 = 1&lt;/li&gt; &#xA; &lt;li&gt;3 * 2 = 6&lt;/li&gt; &#xA; &lt;li&gt;3 / 2 = 1.5&lt;/li&gt; &#xA; &lt;li&gt;3 ^ 2 = 3 x 3 = 9&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In python we have the following additional operations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;3 % 2 = 1 =&amp;gt; which means finding the remainder&lt;/li&gt; &#xA; &lt;li&gt;3 // 2 = 1 =&amp;gt; which means removing the remainder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;em&gt;comment&lt;/em&gt; is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol. This is how you write a comment in python&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt; # comment starts with hash&#xA; # this is a python comment, because it starts with a (#) symbol&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/maths_on_python_shell.png&#34; alt=&#34;Maths on python shell&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing &lt;em&gt;exit()&lt;/em&gt; on the shell and open it again and let us practice how to write text on the Python shell.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/writing_string_on_shell.png&#34; alt=&#34;Writing String on python shell&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installing Visual Studio Code&lt;/h3&gt; &#xA;&lt;p&gt;The Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge we will use visual studio code. Visual studio code is a very popular open source text editor. I am a fan of vscode and I would recommend to &lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;download&lt;/a&gt; visual studio code, but if you are in favor of other editors, feel free to follow with what you have.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode.png&#34; alt=&#34;Visual Studio Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you installed visual studio code, let us see how to use it. If you prefer a video, you can follow this Visual Studio Code for Python &lt;a href=&#34;https://www.youtube.com/watch?v=bn7Cx4z-vSo&#34;&gt;Video tutorial&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;How to use visual studio code&lt;/h4&gt; &#xA;&lt;p&gt;Open the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/vscode_ui.png&#34; alt=&#34;Visual studio Code&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/how_to_open_project_on_vscode.png&#34; alt=&#34;Opening Project on Visual studio&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/opening_project.png&#34; alt=&#34;Opening a project&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;After opening it you will see shortcuts for creating files and folders inside of 30DaysOfPython project&#39;s directory. As you can see below, I have created the very first file, helloworld.py. You can do the same.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/helloworld.png&#34; alt=&#34;Creating a python file&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/closing_opened_project.png&#34; alt=&#34;Closing project&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Congratulations, you have finished setting up the development environment. Let us start coding.&lt;/p&gt; &#xA;&lt;h2&gt;Basic Python&lt;/h2&gt; &#xA;&lt;h3&gt;Python Syntax&lt;/h3&gt; &#xA;&lt;p&gt;A Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.&lt;/p&gt; &#xA;&lt;h3&gt;Python Indentation&lt;/h3&gt; &#xA;&lt;p&gt;An indentation is a white space in a text. Indentation in many languages is used to increase code readability, however Python uses indentation to create block of codes. In other programming languages curly brackets are used to create blocks of codes instead of indentation. One of the common bugs when writing python code is wrong indentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/indentation.png&#34; alt=&#34;Indentation Error&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Comments&lt;/h3&gt; &#xA;&lt;p&gt;Comments are very important to make the code more readable and to leave remarks in our code. Python does not run comment parts of our code. Any text starting with hash(#) in Python is a comment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example: Single Line Comment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;    # This is the first comment&#xA;    # This is the second comment&#xA;    # Python is eating the world&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example: Multiline Comment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Triple quote can be used for multiline comment if it is not assigned to a variable&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;&#34;&#34;&#34;This is multiline comment&#xA;multiline comment takes multiple lines.&#xA;python is eating the world&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data types&lt;/h3&gt; &#xA;&lt;p&gt;In Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.&lt;/p&gt; &#xA;&lt;h4&gt;Number&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integer: Integer(negative, zero and positive) numbers Example: ... -3, -2, -1, 0, 1, 2, 3 ...&lt;/li&gt; &#xA; &lt;li&gt;Float: Decimal number Example ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...&lt;/li&gt; &#xA; &lt;li&gt;Complex Example 1 + j, 2 + 4j&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;String&lt;/h4&gt; &#xA;&lt;p&gt;A collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&#39;Asabeneh&#39;&#xA;&#39;Finland&#39;&#xA;&#39;Python&#39;&#xA;&#39;I love teaching&#39;&#xA;&#39;I hope you are enjoying the first day of 30DaysOfPython Challenge&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Booleans&lt;/h4&gt; &#xA;&lt;p&gt;A boolean data type is either a True or False value. T and F should be always uppercase.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    True  #  Is the light on? If it is on, then the value is True&#xA;    False # Is the light on? If it is off, then the value is False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;List&lt;/h4&gt; &#xA;&lt;p&gt;Python list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers&#xA;[&#39;Banana&#39;, &#39;Orange&#39;, &#39;Mango&#39;, &#39;Avocado&#39;] # all the same data types - a list of strings (fruits)&#xA;[&#39;Finland&#39;,&#39;Estonia&#39;, &#39;Sweden&#39;,&#39;Norway&#39;] # all the same data types - a list of strings (countries)&#xA;[&#39;Banana&#39;, 10, False, 9.81] # different data types in the list - string, integer, boolean and float&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Dictionary&lt;/h4&gt; &#xA;&lt;p&gt;A Python dictionary object is an unordered collection of data in a key value pair format.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;{&#xA;&#39;first_name&#39;:&#39;Asabeneh&#39;,&#xA;&#39;last_name&#39;:&#39;Yetayeh&#39;,&#xA;&#39;country&#39;:&#39;Finland&#39;, &#xA;&#39;age&#39;:250, &#xA;&#39;is_married&#39;:True,&#xA;&#39;skills&#39;:[&#39;JS&#39;, &#39;React&#39;, &#39;Node&#39;, &#39;Python&#39;]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Tuple&lt;/h4&gt; &#xA;&lt;p&gt;A tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;(&#39;Asabeneh&#39;, &#39;Pawel&#39;, &#39;Brook&#39;, &#39;Abraham&#39;, &#39;Lidiya&#39;) # Names&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;(&#39;Earth&#39;, &#39;Jupiter&#39;, &#39;Neptune&#39;, &#39;Mars&#39;, &#39;Venus&#39;, &#39;Saturn&#39;, &#39;Uranus&#39;, &#39;Mercury&#39;) # planets&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Set&lt;/h4&gt; &#xA;&lt;p&gt;A set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.&lt;/p&gt; &#xA;&lt;p&gt;In later sections, we will go in detail about each and every Python data type.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;{2, 4, 3, 5}&#xA;{3.14, 9.81, 2.7} # order is not important in set&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Checking Data types&lt;/h3&gt; &#xA;&lt;p&gt;To check the data type of certain data/variable we use the &lt;strong&gt;type&lt;/strong&gt; function. In the following terminal you will see different python data types:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/checking_data_types.png&#34; alt=&#34;Checking Data types&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Python File&lt;/h3&gt; &#xA;&lt;p&gt;First open your project folder, 30DaysOfPython. If you don&#39;t have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let&#39;s do what we did on python interactive shell using visual studio code.&lt;/p&gt; &#xA;&lt;p&gt;The Python interactive shell was printing without using &lt;strong&gt;print&lt;/strong&gt; but on visual studio code to see our result we should use a built in function *print(). The &lt;em&gt;print()&lt;/em&gt; built-in function takes one or more arguments as follows &lt;em&gt;print(&#39;arument1&#39;, &#39;argument2&#39;, &#39;argument3&#39;)&lt;/em&gt;. See the examples below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The file name is helloworld.py&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# Day 1 - 30DaysOfPython Challenge&#xA;&#xA;print(2 + 3)             # addition(+)&#xA;print(3 - 1)             # subtraction(-)&#xA;print(2 * 3)             # multiplication(*)&#xA;print(3 / 2)             # division(/)&#xA;print(3 ** 2)            # exponential(**)&#xA;print(3 % 2)             # modulus(%)&#xA;print(3 // 2)            # Floor division operator(//)&#xA;&#xA;# Checking data types&#xA;print(type(10))          # Int&#xA;print(type(3.14))        # Float&#xA;print(type(1 + 3j))      # Complex number&#xA;print(type(&#39;Asabeneh&#39;))  # String&#xA;print(type([1, 2, 3]))   # List&#xA;print(type({&#39;name&#39;:&#39;Asabeneh&#39;})) # Dictionary&#xA;print(type({9.8, 3.14, 2.7}))    # Set&#xA;print(type((9.8, 3.14, 2.7)))    # Tuple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing &lt;em&gt;python helloworld.py&lt;/em&gt; in the terminal .&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/images/running_python_script.png&#34; alt=&#34;Running python script&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;🌕 You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.&lt;/p&gt; &#xA;&lt;h2&gt;💻 Exercises - Day 1&lt;/h2&gt; &#xA;&lt;h3&gt;Exercise: Level 1&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Check the python version you are using&lt;/li&gt; &#xA; &lt;li&gt;Open the python interactive shell and do the following operations. The operands are 3 and 4. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;addition(+)&lt;/li&gt; &#xA;   &lt;li&gt;subtraction(-)&lt;/li&gt; &#xA;   &lt;li&gt;multiplication(*)&lt;/li&gt; &#xA;   &lt;li&gt;modulus(%)&lt;/li&gt; &#xA;   &lt;li&gt;division(/)&lt;/li&gt; &#xA;   &lt;li&gt;exponential(**)&lt;/li&gt; &#xA;   &lt;li&gt;floor division operator(//)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Write strings on the python interactive shell. The strings are the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Your name&lt;/li&gt; &#xA;   &lt;li&gt;Your family name&lt;/li&gt; &#xA;   &lt;li&gt;Your country&lt;/li&gt; &#xA;   &lt;li&gt;I am enjoying 30 days of python&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Check the data types of the following data: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;10&lt;/li&gt; &#xA;   &lt;li&gt;9.8&lt;/li&gt; &#xA;   &lt;li&gt;3.14&lt;/li&gt; &#xA;   &lt;li&gt;4 - 4j&lt;/li&gt; &#xA;   &lt;li&gt;[&#39;Asabeneh&#39;, &#39;Python&#39;, &#39;Finland&#39;]&lt;/li&gt; &#xA;   &lt;li&gt;Your name&lt;/li&gt; &#xA;   &lt;li&gt;Your family name&lt;/li&gt; &#xA;   &lt;li&gt;Your country&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Exercise: Level 2&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a folder named day_1 inside 30DaysOfPython folder. Inside day_1 folder, create a python file helloworld.py and repeat questions 1, 2, 3 and 4. Remember to use &lt;em&gt;print()&lt;/em&gt; when you are working on a python file. Navigate to the directory where you have saved your file, and run it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Exercise: Level 3&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Write an example for different Python data types such as Number(Integer, Float, Complex), String, Boolean, List, Tuple, Set and Dictionary.&lt;/li&gt; &#xA; &lt;li&gt;Find an &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance.&#34;&gt;Euclidian distance&lt;/a&gt; between (2, 3) and (10, 8)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;🎉 CONGRATULATIONS ! 🎉&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Asabeneh/30-Days-Of-Python/master/02_Day_Variables_builtin_functions/02_variables_builtin_functions.md&#34;&gt;Day 2 &amp;gt;&amp;gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/DeepSpeed</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/microsoft/DeepSpeed</id>
    <link href="https://github.com/microsoft/DeepSpeed" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed/actions&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/deepspeed/workflows/Build/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/deepspeed/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/deepspeed.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepspeed.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/deepspeed/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Microsoft/DeepSpeed/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only&#34; width=&#34;400px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only&#34; width=&#34;400px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!--&#xA;Remove until pypi issue is resolved: https://status.python.org/incidents/2jj696st6yn5&#xA;[![Downloads](https://pepy.tech/badge/deepspeed/month)](https://pepy.tech/project/deepspeed)&#xA;--&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2022/03/21] &lt;a href=&#34;https://cloudblogs.microsoft.com/opensource/2022/03/21/supporting-efficient-large-model-training-on-amd-instinct-gpus-with-deepspeed/&#34;&gt;Supporting efficient large model training on AMD Instinct GPUs with DeepSpeed&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/03/07] &lt;a href=&#34;https://www.deepspeed.ai/tutorials/zero-one-adam/&#34;&gt;Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/01/19] &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/&#34;&gt;DeepSpeed: Advancing MoE inference and training to power next-generation AI scale&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/mixture-of-experts-nlg/&#34;&gt;Mixture of Experts (MoE) for NLG tutorial&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/moe-inference-tutorial&#34;&gt;Mixture of Experts (MoE) Inference tutorial&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[2021/11/15] &lt;a href=&#34;https://www.deepspeed.ai/news/2021/11/15/autotuning.html&#34;&gt;Autotuning: Automatically discover the optimal DeepSpeed configuration that delivers good training speed&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2021/10/11] &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/&#34;&gt;Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Read more on how to &lt;a href=&#34;https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/&#34;&gt;train large models with DeepSpeed&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DeepSpeed is hiring, &lt;a href=&#34;https://careers.microsoft.com/us/en/search-results?keywords=http:%2F%2Fdeepspeed.ai&#34;&gt;come join us!&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;DeepSpeed&lt;/a&gt; is a deep learning optimization library that makes distributed training easy, efficient, and effective.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;&lt;b&gt;10x Larger Models&lt;/b&gt;&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;&lt;b&gt;10x Faster Training&lt;/b&gt;&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;i&gt;&lt;b&gt;Minimal Code Change&lt;/b&gt;&lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;DeepSpeed delivers extreme-scale model training for everyone, from data scientists training on massive supercomputers to those training on low-end clusters or even on a single GPU:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extreme scale: Using current generation of GPU clusters with hundreds of devices, 3D parallelism of DeepSpeed can efficiently train deep learning models with trillions of parameters.&lt;/li&gt; &#xA; &lt;li&gt;Extremely memory efficient: With just a single GPU, ZeRO-Offload of DeepSpeed can train models with over 10B parameters, 10x bigger than the state of arts, democratizing multi-billion-parameter model training such that many deep learning scientists can explore bigger and better models.&lt;/li&gt; &#xA; &lt;li&gt;Extremely long sequence length: Sparse attention of DeepSpeed powers an order-of-magnitude longer input sequence and obtains up to 6x faster execution comparing with dense transformers.&lt;/li&gt; &#xA; &lt;li&gt;Extremely communication efficient: 3D parallelism improves communication efficiency allows users to train multi-billion-parameter models 2–7x faster on clusters with limited network bandwidth. 1-bit Adam, 0/1 Adam and 1-bit LAMB reduce communication volume by up to 26x while achieving similar convergence efficiency to Adam/LAMB, allowing for scaling to different types of GPU clusters and networks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Early adopters of DeepSpeed have already produced a language model (LM) with over 17B parameters called &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft&#34;&gt;Turing-NLG&lt;/a&gt;, establishing a new SOTA in the LM category.&lt;/p&gt; &#xA;&lt;p&gt;DeepSpeed is an important part of Microsoft’s new &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/ai-at-scale/&#34;&gt;AI at Scale&lt;/a&gt; initiative to enable next-generation AI capabilities at scale, where you can find more information &lt;a href=&#34;https://innovation.microsoft.com/en-us/exploring-ai-at-scale&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;For further documentation, tutorials, and technical deep-dives please see &lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;deepspeed.ai&lt;/a&gt;!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Section&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/#why-deepspeed&#34;&gt;Why DeepSpeed?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed overview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/#installation&#34;&gt;Install&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Installation details&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/#features&#34;&gt;Features&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Feature list and overview&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/#further-reading&#34;&gt;Further Reading&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Documentation, tutorials, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Instructions for contributing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/#publications&#34;&gt;Publications&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Publications related to DeepSpeed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/#videos&#34;&gt;Videos&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Videos related to DeepSpeed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Why DeepSpeed?&lt;/h1&gt; &#xA;&lt;p&gt;Training advanced deep learning models is challenging. Beyond model design, model scientists also need to set up the state-of-the-art training techniques such as distributed training, mixed precision, gradient accumulation, and checkpointing. Yet still, scientists may not achieve the desired system performance and convergence rate. Large model sizes are even more challenging: a large model easily runs out of memory with pure data parallelism and it is difficult to use model parallelism. DeepSpeed addresses these challenges to accelerate model development &lt;em&gt;and&lt;/em&gt; training.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;The quickest way to get started with DeepSpeed is via pip, this will install the latest release of DeepSpeed which is not tied to specific PyTorch or CUDA versions. DeepSpeed includes several C++/CUDA extensions that we commonly refer to as our &#39;ops&#39;. By default, all of these extensions/ops will be built just-in-time (JIT) using &lt;a href=&#34;https://pytorch.org/docs/stable/cpp_extension.html&#34;&gt;torch&#39;s JIT C++ extension loader that relies on ninja&lt;/a&gt; to build and dynamically link them at runtime.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; must be installed &lt;em&gt;before&lt;/em&gt; installing DeepSpeed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install deepspeed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installation, you can validate your install and see which extensions/ops your machine is compatible with via the DeepSpeed environment report.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ds_report&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to pre-install any of the DeepSpeed extensions/ops (instead of JIT compiling) or install pre-compiled ops via PyPI please see our &lt;a href=&#34;https://www.deepspeed.ai/tutorials/advanced-install/&#34;&gt;advanced installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;On Windows you can build wheel with following steps, currently only inference mode is supported.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install pytorch, such as pytorch 1.8 + cuda 11.1&lt;/li&gt; &#xA; &lt;li&gt;Install visual cpp build tools, such as VS2019 C++ x64/x86 build tools&lt;/li&gt; &#xA; &lt;li&gt;Launch cmd console with Administrator privilege for creating required symlink folders&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python setup.py bdist_wheel&lt;/code&gt; to build wheel in &lt;code&gt;dist&lt;/code&gt; folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;p&gt;Below we provide a brief feature list, see our detailed &lt;a href=&#34;https://www.deepspeed.ai/features/&#34;&gt;feature overview&lt;/a&gt; for descriptions and usage.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#distributed-training-with-mixed-precision&#34;&gt;Distributed Training with Mixed Precision&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;16-bit mixed precision&lt;/li&gt; &#xA;   &lt;li&gt;Single-GPU/Multi-GPU/Multi-Node&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#model-parallelism&#34;&gt;Model Parallelism&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support for Custom Model Parallelism&lt;/li&gt; &#xA;   &lt;li&gt;Integration with Megatron-LM&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/pipeline/&#34;&gt;Pipeline Parallelism&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;3D Parallelism&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/zero/&#34;&gt;The Zero Redundancy Optimizer (ZeRO)&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optimizer State and Gradient Partitioning&lt;/li&gt; &#xA;   &lt;li&gt;Activation Partitioning&lt;/li&gt; &#xA;   &lt;li&gt;Constant Buffer Optimization&lt;/li&gt; &#xA;   &lt;li&gt;Contiguous Memory Optimization&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/zero-offload/&#34;&gt;ZeRO-Offload&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Leverage both CPU/GPU memory for model training&lt;/li&gt; &#xA;   &lt;li&gt;Support 10B model training on a single GPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/2020/05/18/bert-record.html&#34;&gt;Ultra-fast dense transformer kernels&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/2020/09/08/sparse-attention-news.html&#34;&gt;Sparse attention&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Memory- and compute-efficient sparse kernels&lt;/li&gt; &#xA;   &lt;li&gt;Support 10x longer sequences than dense&lt;/li&gt; &#xA;   &lt;li&gt;Flexible support to different sparse structures&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/2020/09/08/onebit-adam-blog-post.html&#34;&gt;1-bit Adam&lt;/a&gt;, &lt;a href=&#34;https://www.deepspeed.ai/tutorials/zero-one-adam/&#34;&gt;0/1 Adam&lt;/a&gt; and &lt;a href=&#34;https://www.deepspeed.ai/tutorials/onebit-lamb/&#34;&gt;1-bit LAMB&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Custom communication collective&lt;/li&gt; &#xA;   &lt;li&gt;Up to 26x communication volume saving&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#additional-memory-and-bandwidth-optimizations&#34;&gt;Additional Memory and Bandwidth Optimizations&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Smart Gradient Accumulation&lt;/li&gt; &#xA;   &lt;li&gt;Communication/Computation Overlap&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#training-features&#34;&gt;Training Features&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Simplified training API&lt;/li&gt; &#xA;   &lt;li&gt;Gradient Clipping&lt;/li&gt; &#xA;   &lt;li&gt;Automatic loss scaling with mixed precision&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#training-optimizers&#34;&gt;Training Optimizers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fused Adam optimizer and arbitrary &lt;code&gt;torch.optim.Optimizer&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Memory bandwidth optimized FP16 Optimizer&lt;/li&gt; &#xA;   &lt;li&gt;Large Batch Training with LAMB Optimizer&lt;/li&gt; &#xA;   &lt;li&gt;Memory efficient Training with ZeRO Optimizer&lt;/li&gt; &#xA;   &lt;li&gt;CPU-Adam&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#training-agnostic-checkpointing&#34;&gt;Training Agnostic Checkpointing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#advanced-parameter-search&#34;&gt;Advanced Parameter Search&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Learning Rate Range Test&lt;/li&gt; &#xA;   &lt;li&gt;1Cycle Learning Rate Schedule&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#simplified-data-loader&#34;&gt;Simplified Data Loader&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/curriculum-learning/&#34;&gt;Curriculum Learning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A curriculum learning-based data pipeline that presents easier or simpler examples earlier during training&lt;/li&gt; &#xA;   &lt;li&gt;Stable and 3.3x faster GPT-2 pre-training with 8x/4x larger batch size/learning rate while maintaining token-wise convergence speed&lt;/li&gt; &#xA;   &lt;li&gt;Complementary to many other DeepSpeed features&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/#performance-analysis-and-debugging&#34;&gt;Performance Analysis and Debugging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/mixture-of-experts/&#34;&gt;Mixture of Experts (MoE)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Further Reading&lt;/h1&gt; &#xA;&lt;p&gt;All DeepSpeed documentation can be found on our website: &lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;deepspeed.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Article&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/features/&#34;&gt;DeepSpeed Features&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed features&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/getting-started/&#34;&gt;Getting Started&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;First steps with DeepSpeed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/docs/config-json/&#34;&gt;DeepSpeed JSON Configuration&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Configuring DeepSpeed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://deepspeed.readthedocs.io/en/latest/&#34;&gt;API Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generated DeepSpeed API documentation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/cifar-10&#34;&gt;CIFAR-10 Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Getting started with CIFAR-10 and DeepSpeed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/megatron/&#34;&gt;Megatron-LM Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Train GPT2 with DeepSpeed and Megatron-LM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/bert-pretraining/&#34;&gt;BERT Pre-training Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pre-train BERT with DeepSpeed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/lrrt/&#34;&gt;Learning Rate Range Test Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Faster training with large learning rates&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/one-cycle/&#34;&gt;1Cycle Tutorial&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;SOTA learning schedule in DeepSpeed&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;DeepSpeed welcomes your contributions! Please see our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed/master/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; guide for more details on formatting, testing, etc.&lt;/p&gt; &#xA;&lt;h2&gt;Contributor License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h1&gt;Publications&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. &lt;a href=&#34;https://arxiv.org/abs/1910.02054&#34;&gt;arXiv:1910.02054&lt;/a&gt; and &lt;a href=&#34;https://dl.acm.org/doi/10.5555/3433701.3433727&#34;&gt;In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &#39;20)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3394486.3406703&#34;&gt;In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining (KDD &#39;20, Tutorial)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. &lt;a href=&#34;https://arxiv.org/abs/2010.13369&#34;&gt;arXiv:2010.13369&lt;/a&gt; and &lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html&#34;&gt;NeurIPS 2020&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. &lt;a href=&#34;https://arxiv.org/abs/2101.06840&#34;&gt;arXiv:2101.06840&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam&#39;s Convergence Speed. &lt;a href=&#34;https://arxiv.org/abs/2102.02888&#34;&gt;arXiv:2102.02888&lt;/a&gt; and &lt;a href=&#34;http://proceedings.mlr.press/v139/tang21a.html&#34;&gt;ICML 2021&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. &lt;a href=&#34;https://arxiv.org/abs/2104.07857&#34;&gt;arXiv:2104.07857&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB&#39;s Convergence Speed. &lt;a href=&#34;https://arxiv.org/abs/2104.06069&#34;&gt;arXiv:2104.06069&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. &lt;a href=&#34;https://arxiv.org/abs/2108.06084&#34;&gt;arXiv:2108.06084&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. &lt;a href=&#34;https://arxiv.org/abs/2202.06009&#34;&gt;arXiv:2202.06009&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale &lt;a href=&#34;https://arxiv.org/abs/2201.05596&#34;&gt;arXiv:2201.05596&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model &lt;a href=&#34;https://arxiv.org/abs/2201.11990&#34;&gt;arXiv:2201.11990&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Videos&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;DeepSpeed KDD 2020 Tutorial &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CaseqC45DNc&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=29&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=y4_bCiAsIAk&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=28&#34;&gt;ZeRO + large model training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=9V-ZbP92drg&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=27&#34;&gt;17B T-NLG demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=o1K-ZG9F6u0&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=26&#34;&gt;Fastest BERT training + RScan tuning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;DeepSpeed hands on deep dive: &lt;a href=&#34;https://www.youtube.com/watch?v=_NOk-mBwDYg&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=92&#34;&gt;part 1&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=sG6_c4VXLww&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=94&#34;&gt;part 2&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=k9yPkBTayos&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=93&#34;&gt;part 3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nsHu6vEgPew&amp;amp;list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&amp;amp;index=24&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Microsoft Research Webinar &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Registration is free and all videos are available on-demand.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html&#34;&gt;ZeRO &amp;amp; Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/yBVXR8G8Bg8&#34;&gt;DeepSpeed on AzureML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Community Tutorials &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=pDGI668pNg0&#34;&gt;DeepSpeed: All the tricks to scale to gigantic models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tC01FRB0M7w&#34;&gt;Turing-NLG, DeepSpeed and the ZeRO optimizer&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>PaddlePaddle/PaddleNLP</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/PaddlePaddle/PaddleNLP</id>
    <link href="https://github.com/PaddlePaddle/PaddleNLP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use and powerful NLP library with Awesome model zoo, supporting wide-range of NLP tasks from research to industrial applications, including Neural Search, Question Answering, Information Extraction and Sentiment Analysis end-to-end system.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;简体中文&lt;/strong&gt;🀄 | &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/README_en.md&#34;&gt;English🌎&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/imgs/paddlenlp.png&#34; align=&#34;middle&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202-dfd.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/PaddlePaddle/PaddleNLP?color=ffa&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6.2+-aff.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/PaddlePaddle/PaddleNLP?color=9ea&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/commits&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/PaddlePaddle/PaddleNLP?color=3af&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/paddlenlp/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/paddlenlp?color=9cf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PaddlePaddle/PaddleNLP?color=9cc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP?color=ccf&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;#特性&#34;&gt; 特性 &lt;/a&gt; | &lt;a href=&#34;#安装&#34;&gt; 安装 &lt;/a&gt; | &lt;a href=&#34;#快速开始&#34;&gt; 快速开始 &lt;/a&gt; | &lt;a href=&#34;#api文档&#34;&gt; API文档 &lt;/a&gt; | &lt;a href=&#34;#社区交流&#34;&gt; 社区交流 &lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaddleNLP&lt;/strong&gt;是一款&lt;strong&gt;简单易用&lt;/strong&gt;且&lt;strong&gt;功能强大&lt;/strong&gt;的自然语言处理开发库。聚合业界&lt;strong&gt;优质预训练模型&lt;/strong&gt;并提供&lt;strong&gt;开箱即用&lt;/strong&gt;的开发体验，覆盖NLP多场景的模型库搭配&lt;strong&gt;产业实践范例&lt;/strong&gt;可满足开发者&lt;strong&gt;灵活定制&lt;/strong&gt;的需求。&lt;/p&gt; &#xA;&lt;h2&gt;News 📢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 2022.5.18-19 B站&lt;a href=&#34;https://space.bilibili.com/476867757&#34;&gt;飞桨直播课&lt;/a&gt;，解读通用信息抽取技术&lt;strong&gt;UIE&lt;/strong&gt;和&lt;strong&gt;ERNIE 3.0&lt;/strong&gt;轻量级模型能力，欢迎报名来交流。&lt;/p&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168411900-d9f3d777-99ab-4b5c-8cdc-ef747a48b864.jpg&#34; width=&#34;150&#34; height=&#34;150&#34;&gt; &#xA;  &lt;/div&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 2022.5.16 &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/releases/tag/v2.3.0&#34;&gt;&lt;strong&gt;PaddleNLP v2.3&lt;/strong&gt;&lt;/a&gt;全新发布！🎉&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;💎 发布通用信息抽取技术&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;&lt;strong&gt;UIE&lt;/strong&gt;&lt;/a&gt;，单模型支持实体识别、关系和事件抽取、情感分析等多种开放域信息抽取任务，不限领域和抽取目标，支持&lt;strong&gt;一键抽取&lt;/strong&gt;与全流程&lt;strong&gt;小样本&lt;/strong&gt;高效定制开发。&lt;/li&gt; &#xA;   &lt;li&gt;😊 发布文心大模型&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/ernie-3.0&#34;&gt;&lt;strong&gt;ERNIE 3.0&lt;/strong&gt;&lt;/a&gt;轻量级模型，在&lt;a href=&#34;https://www.cluebenchmarks.com/&#34;&gt;CLUE&lt;/a&gt;上实现同规模结构效果最佳，并提供&lt;strong&gt;🗜️无损压缩&lt;/strong&gt;和&lt;strong&gt;⚙️全场景部署&lt;/strong&gt;方案。&lt;/li&gt; &#xA;   &lt;li&gt;🏥 发布中文医疗领域预训练模型&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/ernie-health&#34;&gt;&lt;strong&gt;ERNIE-Health&lt;/strong&gt;&lt;/a&gt;，&lt;a href=&#34;https://github.com/CBLUEbenchmark/CBLUE&#34;&gt;CBLUE&lt;/a&gt;中文医疗信息处理评测冠军模型。&lt;/li&gt; &#xA;   &lt;li&gt;💬 发布大规模百亿开放域对话预训练模型&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/plato-xl&#34;&gt;&lt;strong&gt;PLATO-XL&lt;/strong&gt;&lt;/a&gt; ，配合⚡&lt;strong&gt;FasterGeneration&lt;/strong&gt;⚡快速实现高性能GPU并行推理加速。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;📬 2022.12.12 &lt;strong&gt;PaddleNLP v2.2&lt;/strong&gt;发布！新增开箱即用的NLP能力&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/taskflow.md&#34;&gt;Taskflow&lt;/a&gt;！配套语义检索、智能问答、评论观点抽取&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications&#34;&gt;产业案例&lt;/a&gt;，快速搭建端到端NLP系统！配套视频课程&lt;a href=&#34;https://aistudio.baidu.com/aistudio/course/introduce/24902&#34;&gt;直通车&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;特性&lt;/h2&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#开箱即用的nlp工具集&#34;&gt; 📦 开箱即用的NLP工具集 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#丰富完备的中文模型库&#34;&gt; 🤗 丰富完备的中文模型库 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#产业级端到端系统范例&#34;&gt; 🎛️ 产业级端到端系统范例 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;#高性能分布式训练与推理&#34;&gt; 🚀 高性能分布式训练与推理 &lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h3&gt;开箱即用的NLP工具集&lt;/h3&gt; &#xA;&lt;p&gt;Taskflow提供丰富的&lt;strong&gt;📦开箱即用&lt;/strong&gt;的产业级NLP预置模型，覆盖自然语言理解与生成两大场景，提供&lt;strong&gt;💪产业级的效果&lt;/strong&gt;与&lt;strong&gt;⚡️极致的推理性能&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/11793384/159693816-fda35221-9751-43bb-b05c-7fc77571dd76.gif&#34; alt=&#34;taskflow1&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;更多使用方法可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/taskflow.md&#34;&gt;Taskflow文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;丰富完备的中文模型库&lt;/h3&gt; &#xA;&lt;h4&gt;🀄 业界最全的中文预训练模型&lt;/h4&gt; &#xA;&lt;p&gt;精选 45+ 个网络结构和 500+ 个预训练模型参数，涵盖业界最全的中文预训练模型：既包括文心NLP大模型的ERNIE、PLATO等，也覆盖BERT、GPT、RoBERTa、T5等主流结构。通过&lt;code&gt;AutoModel&lt;/code&gt; API一键⚡&lt;strong&gt;高速下载&lt;/strong&gt;⚡。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from paddlenlp.transformers import *&#xA;&#xA;ernie = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;bert = AutoModel.from_pretrained(&#39;bert-wwm-chinese&#39;)&#xA;albert = AutoModel.from_pretrained(&#39;albert-chinese-tiny&#39;)&#xA;roberta = AutoModel.from_pretrained(&#39;roberta-wwm-ext&#39;)&#xA;electra = AutoModel.from_pretrained(&#39;chinese-electra-small&#39;)&#xA;gpt = AutoModelForPretraining.from_pretrained(&#39;gpt-cpm-large-cn&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;对预训练模型应用范式如语义表示、文本分类、句对匹配、序列标注、问答等，提供统一的API体验。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import paddle&#xA;from paddlenlp.transformers import *&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;text = tokenizer(&#39;自然语言处理&#39;)&#xA;&#xA;# 语义表示&#xA;model = AutoModel.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;sequence_output, pooled_output = model(input_ids=paddle.to_tensor([text[&#39;input_ids&#39;]]))&#xA;# 文本分类 &amp;amp; 句对匹配&#xA;model = AutoModelForSequenceClassification.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# 序列标注&#xA;model = AutoModelForTokenClassification.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;# 问答&#xA;model = AutoModelForQuestionAnswering.from_pretrained(&#39;ernie-3.0-medium-zh&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;💯 全场景覆盖的应用示例&lt;/h4&gt; &#xA;&lt;p&gt;覆盖从学术到产业的NLP应用示例，涵盖NLP基础技术、NLP系统应用以及拓展应用。全面基于飞桨核心框架2.0全新API体系开发，为开发者提供飞桨文本领域的最佳实践。&lt;/p&gt; &#xA;&lt;p&gt;精选预训练模型示例可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo&#34;&gt;Model Zoo&lt;/a&gt;，更多场景示例文档可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples&#34;&gt;examples目录&lt;/a&gt;。更有免费算力支持的&lt;a href=&#34;https://aistudio.baidu.com&#34;&gt;AI Studio&lt;/a&gt;平台的&lt;a href=&#34;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/574995&#34;&gt;Notbook交互式教程&lt;/a&gt;提供实践。&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt; PaddleNLP预训练模型适用任务汇总（&lt;b&gt;点击展开详情&lt;/b&gt;）&lt;/summary&gt;&#xA; &lt;div&gt; &#xA;  &lt;table&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Sequence Classification&lt;/th&gt; &#xA;     &lt;th&gt;Token Classification&lt;/th&gt; &#xA;     &lt;th&gt;Question Answering&lt;/th&gt; &#xA;     &lt;th&gt;Text Generation&lt;/th&gt; &#xA;     &lt;th&gt;Multiple Choice&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ALBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BART&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BigBird&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;BlenderBot&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ChineseBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ConvBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;CTRL&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;DistilBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ELECTRA&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-CTM&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-Doc&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-GEN&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-Gram&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ERNIE-M&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;FNet&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Funnel-Transformer&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;GPT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutLM&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutLMv2&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LayoutXLM&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;LUKE&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;mBART&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MegatronBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MobileBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;MPNet&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;NEZHA&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;PP-MiniLM&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;ProphetNet&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;Reformer&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RemBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RoBERTa&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;RoFormer&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;SKEP&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;SqueezeBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;T5&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;TinyBERT&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;UnifiedTransformer&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td align=&#34;left&#34;&gt;XLNet&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;     &lt;td&gt;❌&lt;/td&gt; &#xA;     &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt;&#xA;&lt;/details&gt; &#xA;&lt;p&gt;可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/index.rst&#34;&gt;Transformer 文档&lt;/a&gt; 查看目前支持的预训练模型结构、参数和详细用法。&lt;/p&gt; &#xA;&lt;h3&gt;产业级端到端系统范例&lt;/h3&gt; &#xA;&lt;p&gt;PaddleNLP针对信息抽取、语义检索、智能问答、情感分析等高频NLP场景，提供了端到端系统范例，打通&lt;em&gt;数据标注&lt;/em&gt;-&lt;em&gt;模型训练&lt;/em&gt;-&lt;em&gt;模型调优&lt;/em&gt;-&lt;em&gt;预测部署&lt;/em&gt;全流程，持续降低NLP技术产业落地门槛。更多详细的系统级产业范例使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications&#34;&gt;Applications&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;🔍 语义检索系统&lt;/h4&gt; &#xA;&lt;p&gt;针对无监督数据、有监督数据等多种数据情况，结合SimCSE、In-batch Negatives、ERNIE-Gram单塔模型等，推出前沿的语义检索方案，包含召回、排序环节，打通训练、调优、高效向量检索引擎建库和查询全流程。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168514909-8817d79a-72c4-4be1-8080-93d1f682bb46.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/neural_search&#34;&gt;语义检索系统&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;❓ 智能问答系统&lt;/h4&gt; &#xA;&lt;p&gt;基于&lt;a href=&#34;https://github.com/PaddlePaddle/RocketQA&#34;&gt;🚀RocketQA&lt;/a&gt;技术的检索式问答系统，支持FAQ问答、说明书问答等多种业务场景。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168514868-1babe981-c675-4f89-9168-dd0a3eede315.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/question_answering&#34;&gt;智能问答系统&lt;/a&gt;与&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/doc_vqa&#34;&gt;文档智能问答&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;💌 评论观点抽取与情感分析&lt;/h4&gt; &#xA;&lt;p&gt;基于情感知识增强预训练模型SKEP，针对产品评论进行评价维度和观点抽取，以及细粒度的情感分析。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407260-b7f92800-861c-4207-98f3-2291e0102bbe.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/sentiment_analysis&#34;&gt;情感分析&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;🎙️ 智能语音指令解析&lt;/h4&gt; &#xA;&lt;p&gt;集成了&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech&#34;&gt;PaddleSpeech&lt;/a&gt;和&lt;a href=&#34;https://ai.baidu.com/&#34;&gt;百度开放平台&lt;/a&gt;的的语音识别和&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie&#34;&gt;UIE&lt;/a&gt;通用信息抽取等技术，打造智能一体化的语音指令解析系统范例，该方案可应用于智能语音填单、智能语音交互、智能语音检索等场景，提高人机交互效率。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/16698950/168589100-a6c6f346-97bb-47b2-ac26-8d50e71fddc5.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多使用说明请参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/applications/speech_cmd_analysis&#34;&gt;智能语音指令解析&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;高性能分布式训练与推理&lt;/h3&gt; &#xA;&lt;h4&gt;⚡ FasterTokenizers：高性能文本处理库&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407921-b4395b1d-44bd-41a0-8c58-923ba2b703ef.png&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;AutoTokenizer.from_pretrained(&#34;ernie-3.0-medium-zh&#34;, use_faster=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;为了实现更极致的模型部署性能，安装FastTokenizers后只需在&lt;code&gt;AutoTokenizer&lt;/code&gt; API上打开 &lt;code&gt;use_faster=True&lt;/code&gt;选项，即可调用C++实现的高性能分词算子，轻松获得超Python百余倍的文本处理加速，更多使用说明可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/faster_tokenizers&#34;&gt;FasterTokenizers文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;⚡️ FasterGeneration：高性能生成加速库&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168407831-914dced0-3a5a-40b8-8a65-ec82bf13e53c.gif&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = GPTLMHeadModel.from_pretrained(&#39;gpt-cpm-large-cn&#39;)&#xA;...&#xA;outputs, _ = model.generate(&#xA;    input_ids=inputs_ids, max_length=10, decode_strategy=&#39;greedy_search&#39;,&#xA;    use_faster=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;简单地在&lt;code&gt;generate()&lt;/code&gt;API上打开&lt;code&gt;use_faster=True&lt;/code&gt;选项，轻松在Transformer、GPT、BART、PLATO、UniLM等生成式预训练模型上获得5倍以上GPU加速，更多使用说明可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/faster_generation&#34;&gt;FasterGeneration文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;🚀 Fleet：飞桨4D混合并行分布式训练技术&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168515134-513f13e0-9902-40ef-98fa-528271dcccda.png&#34; width=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;更多关于千亿级AI模型的分布式训练使用说明可参考&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples/language_model/gpt-3&#34;&gt;GPT-3&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;h3&gt;环境依赖&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python &amp;gt;= 3.6&lt;/li&gt; &#xA; &lt;li&gt;paddlepaddle &amp;gt;= 2.2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;pip安装&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install --upgrade paddlenlp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;更多关于PaddlePaddle和PaddleNLP安装的详细教程请查看&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/get_started/installation.rst&#34;&gt;Installation&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;快速开始&lt;/h2&gt; &#xA;&lt;p&gt;这里以信息抽取-命名实体识别任务，UIE模型为例，来说明如何快速使用PaddleNLP:&lt;/p&gt; &#xA;&lt;h3&gt;一键预测&lt;/h3&gt; &#xA;&lt;p&gt;PaddleNLP提供&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/docs/model_zoo/taskflow.md&#34;&gt;一键预测功能&lt;/a&gt;，无需训练，直接输入数据即可开放域抽取结果：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from pprint import pprint&#xA;&amp;gt;&amp;gt;&amp;gt; from paddlenlp import Taskflow&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; schema = [&#39;时间&#39;, &#39;选手&#39;, &#39;赛事名称&#39;] # Define the schema for entity extraction&#xA;&amp;gt;&amp;gt;&amp;gt; ie = Taskflow(&#39;information_extraction&#39;, schema=schema)&#xA;&amp;gt;&amp;gt;&amp;gt; pprint(ie(&#34;2月8日上午北京冬奥会自由式滑雪女子大跳台决赛中中国选手谷爱凌以188.25分获得金牌！&#34;))&#xA;[{&#39;时间&#39;: [{&#39;end&#39;: 6,&#xA;          &#39;probability&#39;: 0.9857378532924486,&#xA;          &#39;start&#39;: 0,&#xA;          &#39;text&#39;: &#39;2月8日上午&#39;}],&#xA;  &#39;赛事名称&#39;: [{&#39;end&#39;: 23,&#xA;            &#39;probability&#39;: 0.8503089953268272,&#xA;            &#39;start&#39;: 6,&#xA;            &#39;text&#39;: &#39;北京冬奥会自由式滑雪女子大跳台决赛&#39;}],&#xA;  &#39;选手&#39;: [{&#39;end&#39;: 31,&#xA;          &#39;probability&#39;: 0.8981548639781138,&#xA;          &#39;start&#39;: 28,&#xA;          &#39;text&#39;: &#39;谷爱凌&#39;}]}]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;小样本学习&lt;/h3&gt; &#xA;&lt;p&gt;如果对一键预测效果不满意，也可以使用少量数据进行模型精调，进一步提升特定场景的效果，详见&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo/uie/&#34;&gt;UIE小样本定制训练&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;更多PaddleNLP内容可参考：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/model_zoo&#34;&gt;精选模型库&lt;/a&gt;，包含优质预训练模型的端到端全流程使用。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/examples&#34;&gt;多场景示例&lt;/a&gt;，了解如何使用PaddleNLP解决NLP多种技术问题，包含基础技术、系统应用与拓展应用。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aistudio.baidu.com/aistudio/personalcenter/thirdview/574995&#34;&gt;交互式教程&lt;/a&gt;，在🆓免费算力平台AI Studio上快速学习PaddleNLP。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;API文档&lt;/h2&gt; &#xA;&lt;p&gt;PaddleNLP提供全流程的文本领域API，可大幅提升NLP任务建模的效率：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持&lt;a href=&#34;https://www.luge.ai&#34;&gt;千言&lt;/a&gt;等丰富中文数据集加载的&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_list.html&#34;&gt;Dataset API&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;提供🤗Hugging Face Style的API，支持 &lt;strong&gt;500+&lt;/strong&gt; 优质预训练模型加载的&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html&#34;&gt;Transformers API&lt;/a&gt;。&lt;/li&gt; &#xA; &lt;li&gt;提供30+多语言词向量的&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/model_zoo/embeddings.html&#34;&gt;Embedding API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;更多使用方法请参考&lt;a href=&#34;https://paddlenlp.readthedocs.io/zh/latest/&#34;&gt;API文档&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;社区交流&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;微信扫描二维码并填写问卷之后，加入交流群领取福利&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;获取5月18-19日每晚20:30《产业级通用信息抽取技术UIE+ERNIE轻量级模型》直播课链接。&lt;/li&gt; &#xA;   &lt;li&gt;10G重磅NLP学习大礼包！&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;img src=&#34;https://user-images.githubusercontent.com/11793384/168411900-d9f3d777-99ab-4b5c-8cdc-ef747a48b864.jpg&#34; width=&#34;150&#34; height=&#34;150&#34;&gt; &#xA;  &lt;/div&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;如果PaddleNLP对您的研究有帮助，欢迎引用&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{=paddlenlp,&#xA;    title={PaddleNLP: An Easy-to-use and High Performance NLP Library},&#xA;    author={PaddleNLP Contributors},&#xA;    howpublished = {\url{https://github.com/PaddlePaddle/PaddleNLP}},&#xA;    year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;p&gt;我们借鉴了Hugging Face的&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;🤗关于预训练模型使用的优秀设计，在此对Hugging Face作者及其开源社区表示感谢。&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;PaddleNLP遵循&lt;a href=&#34;https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/LICENSE&#34;&gt;Apache-2.0开源协议&lt;/a&gt;。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sqlalchemy/sqlalchemy</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/sqlalchemy/sqlalchemy</id>
    <link href="https://github.com/sqlalchemy/sqlalchemy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Database Toolkit for Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SQLAlchemy&lt;/h1&gt; &#xA;&lt;p&gt;|PyPI| |Python| |Downloads|&lt;/p&gt; &#xA;&lt;p&gt;.. |PyPI| image:: &lt;a href=&#34;https://img.shields.io/pypi/v/sqlalchemy&#34;&gt;https://img.shields.io/pypi/v/sqlalchemy&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/sqlalchemy&#34;&gt;https://pypi.org/project/sqlalchemy&lt;/a&gt; :alt: PyPI&lt;/p&gt; &#xA;&lt;p&gt;.. |Python| image:: &lt;a href=&#34;https://img.shields.io/pypi/pyversions/sqlalchemy&#34;&gt;https://img.shields.io/pypi/pyversions/sqlalchemy&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/sqlalchemy&#34;&gt;https://pypi.org/project/sqlalchemy&lt;/a&gt; :alt: PyPI - Python Version&lt;/p&gt; &#xA;&lt;p&gt;.. |Downloads| image:: &lt;a href=&#34;https://img.shields.io/pypi/dm/sqlalchemy&#34;&gt;https://img.shields.io/pypi/dm/sqlalchemy&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/sqlalchemy&#34;&gt;https://pypi.org/project/sqlalchemy&lt;/a&gt; :alt: PyPI - Downloads&lt;/p&gt; &#xA;&lt;p&gt;The Python SQL Toolkit and Object Relational Mapper&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. SQLAlchemy provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language.&lt;/p&gt; &#xA;&lt;p&gt;Major SQLAlchemy features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An industrial strength ORM, built from the core on the identity map, unit of work, and data mapper patterns. These patterns allow transparent persistence of objects using a declarative configuration system. Domain models can be constructed and manipulated naturally, and changes are synchronized with the current transaction automatically.&lt;/li&gt; &#xA; &lt;li&gt;A relationally-oriented query system, exposing the full range of SQL&#39;s capabilities explicitly, including joins, subqueries, correlation, and most everything else, in terms of the object model. Writing queries with the ORM uses the same techniques of relational composition you use when writing SQL. While you can drop into literal SQL at any time, it&#39;s virtually never needed.&lt;/li&gt; &#xA; &lt;li&gt;A comprehensive and flexible system of eager loading for related collections and objects. Collections are cached within a session, and can be loaded on individual access, all at once using joins, or by query per collection across the full result set.&lt;/li&gt; &#xA; &lt;li&gt;A Core SQL construction system and DBAPI interaction layer. The SQLAlchemy Core is separate from the ORM and is a full database abstraction layer in its own right, and includes an extensible Python-based SQL expression language, schema metadata, connection pooling, type coercion, and custom types.&lt;/li&gt; &#xA; &lt;li&gt;All primary and foreign key constraints are assumed to be composite and natural. Surrogate integer primary keys are of course still the norm, but SQLAlchemy never assumes or hardcodes to this model.&lt;/li&gt; &#xA; &lt;li&gt;Database introspection and generation. Database schemas can be &#34;reflected&#34; in one step into Python structures representing database metadata; those same structures can then generate CREATE statements right back out - all within the Core, independent of the ORM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;SQLAlchemy&#39;s philosophy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SQL databases behave less and less like object collections the more size and performance start to matter; object collections behave less and less like tables and rows the more abstraction starts to matter. SQLAlchemy aims to accommodate both of these principles.&lt;/li&gt; &#xA; &lt;li&gt;An ORM doesn&#39;t need to hide the &#34;R&#34;. A relational database provides rich, set-based functionality that should be fully exposed. SQLAlchemy&#39;s ORM provides an open-ended set of patterns that allow a developer to construct a custom mediation layer between a domain model and a relational schema, turning the so-called &#34;object relational impedance&#34; issue into a distant memory.&lt;/li&gt; &#xA; &lt;li&gt;The developer, in all cases, makes all decisions regarding the design, structure, and naming conventions of both the object model as well as the relational schema. SQLAlchemy only provides the means to automate the execution of these decisions.&lt;/li&gt; &#xA; &lt;li&gt;With SQLAlchemy, there&#39;s no such thing as &#34;the ORM generated a bad query&#34; - you retain full control over the structure of queries, including how joins are organized, how subqueries and correlation is used, what columns are requested. Everything SQLAlchemy does is ultimately the result of a developer-initiated decision.&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t use an ORM if the problem doesn&#39;t need one. SQLAlchemy consists of a Core and separate ORM component. The Core offers a full SQL expression language that allows Pythonic construction of SQL constructs that render directly to SQL strings for a target database, returning result sets that are essentially enhanced DBAPI cursors.&lt;/li&gt; &#xA; &lt;li&gt;Transactions should be the norm. With SQLAlchemy&#39;s ORM, nothing goes to permanent storage until commit() is called. SQLAlchemy encourages applications to create a consistent means of delineating the start and end of a series of operations.&lt;/li&gt; &#xA; &lt;li&gt;Never render a literal value in a SQL statement. Bound parameters are used to the greatest degree possible, allowing query optimizers to cache query plans effectively and making SQL injection attacks a non-issue.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Latest documentation is at:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.sqlalchemy.org/docs/&#34;&gt;https://www.sqlalchemy.org/docs/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation / Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Full documentation for installation is at &lt;code&gt;Installation &amp;lt;https://www.sqlalchemy.org/docs/intro.html#installation&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Help / Development / Bug reporting&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;code&gt;SQLAlchemy Community Guide &amp;lt;https://www.sqlalchemy.org/support.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;Above all, SQLAlchemy places great emphasis on polite, thoughtful, and constructive communication between users and developers. Please see our current Code of Conduct at &lt;code&gt;Code of Conduct &amp;lt;https://www.sqlalchemy.org/codeofconduct.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;SQLAlchemy is distributed under the &lt;code&gt;MIT license &amp;lt;https://www.opensource.org/licenses/mit-license.php&amp;gt;&lt;/code&gt;_.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jupyter/docker-stacks</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/jupyter/docker-stacks</id>
    <link href="https://github.com/jupyter/docker-stacks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ready-to-run Docker images containing Jupyter applications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jupyter Docker Stacks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jupyter/docker-stacks/actions/workflows/docker.yml&#34; title=&#34;Docker images build status&#34;&gt;&lt;img src=&#34;https://github.com/jupyter/docker-stacks/actions/workflows/docker.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub actions badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/&#34; title=&#34;Documentation build status&#34;&gt;&lt;img src=&#34;https://img.shields.io/readthedocs/jupyter-docker-stacks.svg?sanitize=true&#34; alt=&#34;Read the Docs badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://results.pre-commit.ci/latest/github/jupyter/docker-stacks/master&#34; title=&#34;pre-commit.ci build status&#34;&gt;&lt;img src=&#34;https://results.pre-commit.ci/badge/github/jupyter/docker-stacks/master.svg?sanitize=true&#34; alt=&#34;pre-commit.ci status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discourse.jupyter.org/&#34; title=&#34;Jupyter Discourse Forum&#34;&gt;&lt;img src=&#34;https://img.shields.io/discourse/users.svg?color=%23f37626&amp;amp;server=https%3A%2F%2Fdiscourse.jupyter.org&#34; alt=&#34;Discourse badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mybinder.org/v2/gh/jupyter/docker-stacks/master?filepath=README.ipynb&#34; title=&#34;Launch a jupyter/base-notebook container on mybinder.org&#34;&gt;&lt;img src=&#34;https://static.mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Jupyter Docker Stacks are a set of ready-to-run &lt;a href=&#34;https://hub.docker.com/u/jupyter&#34;&gt;Docker images&lt;/a&gt; containing Jupyter applications and interactive computing tools. You can use a stack image to do any of the following (and more):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start a personal Jupyter Server with JupyterLab frontend (default)&lt;/li&gt; &#xA; &lt;li&gt;Run JupyterLab for a team using JupyterHub&lt;/li&gt; &#xA; &lt;li&gt;Start a personal Jupyter Notebook server in a local Docker container&lt;/li&gt; &#xA; &lt;li&gt;Write your own project Dockerfile&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You can try a &lt;a href=&#34;https://mybinder.org/v2/gh/jupyter/docker-stacks/master?urlpath=lab/tree/README.ipynb&#34;&gt;relatively recent build of the jupyter/base-notebook image on mybinder.org&lt;/a&gt; by simply clicking the preceding link. Otherwise, the examples below may help you get started if you &lt;a href=&#34;https://docs.docker.com/install/&#34;&gt;have Docker installed&lt;/a&gt;, know &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html&#34;&gt;which Docker image&lt;/a&gt; you want to use and want to launch a single Jupyter Server in a container.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/&#34;&gt;User Guide on ReadTheDocs&lt;/a&gt; describes additional uses and features in detail.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This command pulls the &lt;code&gt;jupyter/scipy-notebook&lt;/code&gt; image tagged &lt;code&gt;6b49f3337709&lt;/code&gt; from Docker Hub if it is not already present on the local host. It then starts a container running a Jupyter Server and exposes the container&#39;s internal port &lt;code&gt;8888&lt;/code&gt; to port &lt;code&gt;10000&lt;/code&gt; of the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -p 10000:8888 jupyter/scipy-notebook:6b49f3337709&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can modify the port on which the container&#39;s port is exposed by &lt;a href=&#34;https://docs.docker.com/engine/reference/run/#expose-incoming-ports&#34;&gt;changing the value of the &lt;code&gt;-p&lt;/code&gt; option&lt;/a&gt; to &lt;code&gt;-p 8888:8888&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Visiting &lt;code&gt;http://&amp;lt;hostname&amp;gt;:10000/?token=&amp;lt;token&amp;gt;&lt;/code&gt; in a browser loads JupyterLab, where:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;hostname&lt;/code&gt; is the name of the computer running Docker&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;token&lt;/code&gt; is the secret token printed in the console.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The container remains intact for restart after the Jupyter Server exits.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example 2:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This command pulls the &lt;code&gt;jupyter/datascience-notebook&lt;/code&gt; image tagged &lt;code&gt;6b49f3337709&lt;/code&gt; from Docker Hub if it is not already present on the local host. It then starts an &lt;em&gt;ephemeral&lt;/em&gt; container running a Jupyter Server and exposes the server on host port 10000.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -p 10000:8888 -v &#34;${PWD}&#34;:/home/jovyan/work jupyter/datascience-notebook:6b49f3337709&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The use of the &lt;code&gt;-v&lt;/code&gt; flag in the command mounts the current working directory on the host (&lt;code&gt;{PWD}&lt;/code&gt; in the example command) as &lt;code&gt;/home/jovyan/work&lt;/code&gt; in the container. The server logs appear in the terminal.&lt;/p&gt; &#xA;&lt;p&gt;Visiting &lt;code&gt;http://&amp;lt;hostname&amp;gt;:10000/?token=&amp;lt;token&amp;gt;&lt;/code&gt; in a browser loads JupyterLab.&lt;/p&gt; &#xA;&lt;p&gt;Due to the usage of &lt;a href=&#34;https://docs.docker.com/engine/reference/run/#clean-up---rm&#34;&gt;the flag &lt;code&gt;--rm&lt;/code&gt;&lt;/a&gt; Docker automatically cleans up the container and removes the file system when the container exits, but any changes made to the &lt;code&gt;~/work&lt;/code&gt; directory and its files in the container will remain intact on the host. &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/run/#assign-name-and-allocate-pseudo-tty---name--it&#34;&gt;The &lt;code&gt;-it&lt;/code&gt; flag&lt;/a&gt; allocates pseudo-TTY.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/&#34;&gt;Contributor Guide on ReadTheDocs&lt;/a&gt; for information about how to contribute package updates, recipes, features, tests, and community maintained stacks.&lt;/p&gt; &#xA;&lt;h2&gt;Maintainer Help Wanted&lt;/h2&gt; &#xA;&lt;p&gt;We value all positive contributions to the Docker stacks project, from &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/contributing/issues.html&#34;&gt;bug reports&lt;/a&gt; to &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/contributing/packages.html&#34;&gt;pull requests&lt;/a&gt; to help with answering questions. We&#39;d also like to invite members of the community to help with two maintainer activities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Issue triaging&lt;/strong&gt;: Reading and providing a first response to issues, labeling issues appropriately, redirecting cross-project questions to Jupyter Discourse&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pull request reviews&lt;/strong&gt;: Reading proposed documentation and code changes, working with the submitter to improve the contribution, deciding if the contribution should take another form (e.g., a recipe instead of a permanent change to the images)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Anyone in the community can jump in and help with these activities at any time. We will happily grant additional permissions (e.g., ability to merge PRs) to anyone who shows an ongoing interest in working on the project.&lt;/p&gt; &#xA;&lt;h2&gt;Jupyter Notebook Deprecation Notice&lt;/h2&gt; &#xA;&lt;p&gt;Following &lt;a href=&#34;https://github.com/jupyter/notebook#notice&#34;&gt;Jupyter Notebook notice&lt;/a&gt;, JupyterLab is now the default for all the Jupyter Docker stack images. It is still possible to switch back to Jupyter Notebook (or to launch a different startup command). You can achieve this by passing the environment variable &lt;code&gt;DOCKER_STACKS_JUPYTER_CMD=notebook&lt;/code&gt; (or any other valid &lt;code&gt;jupyter&lt;/code&gt; subcommand) at container startup, more information is available in the &lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/using/common.html#alternative-commands&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;According to the Jupyter Notebook project status and its compatibility with JupyterLab, these Docker images may remove the classic Jupyter Notebook interface altogether in favor of another &lt;em&gt;classic-like&lt;/em&gt; UI built atop JupyterLab.&lt;/p&gt; &#xA;&lt;p&gt;This change is tracked in the issue &lt;a href=&#34;https://github.com/jupyter/docker-stacks/issues/1217&#34;&gt;#1217&lt;/a&gt;; please check its content for more information.&lt;/p&gt; &#xA;&lt;h2&gt;Alternatives&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jupyterhub/repo2docker&#34;&gt;jupyter/repo2docker&lt;/a&gt; - Turn git repositories into Jupyter-enabled Docker Images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openshift/source-to-image&#34;&gt;openshift/source-to-image&lt;/a&gt; - A tool for building/building artifacts from source and injecting into docker images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jupyter-on-openshift/jupyter-notebooks&#34;&gt;jupyter-on-openshift/jupyter-notebooks&lt;/a&gt; - OpenShift compatible S2I builder for basic notebook images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter-docker-stacks.readthedocs.io/en/latest/&#34;&gt;Documentation on ReadTheDocs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jupyter/docker-stacks&#34;&gt;Issue Tracker on GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discourse.jupyter.org/&#34;&gt;Jupyter Discourse Forum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jupyter.org&#34;&gt;Jupyter Website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hub.docker.com/u/jupyter&#34;&gt;Images on DockerHub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;CPU Architectures&lt;/h2&gt; &#xA;&lt;p&gt;All published containers support amd64 (x86_64) and aarch64, except for &lt;code&gt;datascience-notebook&lt;/code&gt; and &lt;code&gt;tensorflow-notebook&lt;/code&gt;, which only support amd64 for now.&lt;/p&gt; &#xA;&lt;h3&gt;Caveats for arm64 images&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The manifests we publish in this project&#39;s wiki as well as the image tags for the multi-platform images that also support arm, are all based on the amd64 version even though details about the installed packages versions could differ between architectures. For the status about this, see &lt;a href=&#34;https://github.com/jupyter/docker-stacks/issues/1401&#34;&gt;#1401&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Only the amd64 images are actively tested currently. For the status about this, see &lt;a href=&#34;https://github.com/jupyter/docker-stacks/issues/1402&#34;&gt;#1402&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>matrix-org/synapse</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/matrix-org/synapse</id>
    <link href="https://github.com/matrix-org/synapse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Synapse: Matrix homeserver written in Python 3/Twisted.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;========================================================================= Synapse |support| |development| |documentation| |license| |pypi| |python|&lt;/h1&gt; &#xA;&lt;p&gt;.. contents::&lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Matrix is an ambitious new ecosystem for open federated Instant Messaging and VoIP. The basics you need to know to get up and running are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Everything in Matrix happens in a room. Rooms are distributed and do not exist on any single server. Rooms can be located using convenience aliases like &lt;code&gt;#matrix:matrix.org&lt;/code&gt; or &lt;code&gt;#test:localhost:8448&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Matrix user IDs look like &lt;code&gt;@matthew:matrix.org&lt;/code&gt; (although in the future you will normally refer to yourself and others using a third party identifier (3PID): email address, phone number, etc rather than manipulating Matrix user IDs)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The overall architecture is::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  client &amp;lt;----&amp;gt; homeserver &amp;lt;=====================&amp;gt; homeserver &amp;lt;----&amp;gt; client&#xA;         https://somewhere.org/_matrix      https://elsewhere.net/_matrix&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;#matrix:matrix.org&lt;/code&gt; is the official support room for Matrix, and can be accessed by any client from &lt;a href=&#34;https://matrix.org/docs/projects/try-matrix-now.html&#34;&gt;https://matrix.org/docs/projects/try-matrix-now.html&lt;/a&gt; or via IRC bridge at irc://irc.libera.chat/matrix.&lt;/p&gt; &#xA;&lt;p&gt;Synapse is currently in rapid development, but as of version 0.5 we believe it is sufficiently stable to be run as an internet-facing service for real usage!&lt;/p&gt; &#xA;&lt;h1&gt;About Matrix&lt;/h1&gt; &#xA;&lt;p&gt;Matrix specifies a set of pragmatic RESTful HTTP JSON APIs as an open standard, which handle:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating and managing fully distributed chat rooms with no single points of control or failure&lt;/li&gt; &#xA; &lt;li&gt;Eventually-consistent cryptographically secure synchronisation of room state across a global open network of federated servers and services&lt;/li&gt; &#xA; &lt;li&gt;Sending and receiving extensible messages in a room with (optional) end-to-end encryption&lt;/li&gt; &#xA; &lt;li&gt;Inviting, joining, leaving, kicking, banning room members&lt;/li&gt; &#xA; &lt;li&gt;Managing user accounts (registration, login, logout)&lt;/li&gt; &#xA; &lt;li&gt;Using 3rd Party IDs (3PIDs) such as email addresses, phone numbers, Facebook accounts to authenticate, identify and discover users on Matrix.&lt;/li&gt; &#xA; &lt;li&gt;Placing 1:1 VoIP and Video calls&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These APIs are intended to be implemented on a wide range of servers, services and clients, letting developers build messaging and VoIP functionality on top of the entirely open Matrix ecosystem rather than using closed or proprietary solutions. The hope is for Matrix to act as the building blocks for a new generation of fully open and interoperable messaging and VoIP apps for the internet.&lt;/p&gt; &#xA;&lt;p&gt;Synapse is a Matrix &#34;homeserver&#34; implementation developed by the matrix.org core team, written in Python 3/Twisted.&lt;/p&gt; &#xA;&lt;p&gt;In Matrix, every user runs one or more Matrix clients, which connect through to a Matrix homeserver. The homeserver stores all their personal chat history and user account information - much as a mail client connects through to an IMAP/SMTP server. Just like email, you can either run your own Matrix homeserver and control and own your own communications and history or use one hosted by someone else (e.g. matrix.org) - there is no single point of control or mandatory service provider in Matrix, unlike WhatsApp, Facebook, Hangouts, etc.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;d like to invite you to join #matrix:matrix.org (via &lt;a href=&#34;https://matrix.org/docs/projects/try-matrix-now.html&#34;&gt;https://matrix.org/docs/projects/try-matrix-now.html&lt;/a&gt;), run a homeserver, take a look at the &lt;code&gt;Matrix spec &amp;lt;https://matrix.org/docs/spec&amp;gt;&lt;/code&gt;&lt;em&gt;, and experiment with the &lt;code&gt;APIs &amp;lt;https://matrix.org/docs/api&amp;gt;&lt;/code&gt;&lt;/em&gt; and &lt;code&gt;Client SDKs &amp;lt;https://matrix.org/docs/projects/try-matrix-now.html#client-sdks&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;Thanks for using Matrix!&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;For support installing or managing Synapse, please join |room|_ (from a matrix.org account if necessary) and ask questions there. We do not use GitHub issues for support requests, only for bug reports and feature requests.&lt;/p&gt; &#xA;&lt;p&gt;Synapse&#39;s documentation is &lt;code&gt;nicely rendered on GitHub Pages &amp;lt;https://matrix-org.github.io/synapse&amp;gt;&lt;/code&gt;&lt;em&gt;, with its source available in |docs|&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;.. |room| replace:: &lt;code&gt;#synapse:matrix.org&lt;/code&gt; .. _room: &lt;a href=&#34;https://matrix.to/#/#synapse:matrix.org&#34;&gt;https://matrix.to/#/#synapse:matrix.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |docs| replace:: &lt;code&gt;docs&lt;/code&gt; .. _docs: docs&lt;/p&gt; &#xA;&lt;h1&gt;Synapse Installation&lt;/h1&gt; &#xA;&lt;p&gt;.. _federation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For details on how to install synapse, see &lt;code&gt;Installation Instructions &amp;lt;https://matrix-org.github.io/synapse/latest/setup/installation.html&amp;gt;&lt;/code&gt;_.&lt;/li&gt; &#xA; &lt;li&gt;For specific details on how to configure Synapse for federation see &lt;code&gt;docs/federate.md &amp;lt;docs/federate.md&amp;gt;&lt;/code&gt;_&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Connecting to Synapse from a client&lt;/h1&gt; &#xA;&lt;p&gt;The easiest way to try out your new Synapse installation is by connecting to it from a web client.&lt;/p&gt; &#xA;&lt;p&gt;Unless you are running a test instance of Synapse on your local machine, in general, you will need to enable TLS support before you can successfully connect from a client: see &lt;code&gt;TLS certificates &amp;lt;https://matrix-org.github.io/synapse/latest/setup/installation.html#tls-certificates&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;An easy way to get started is to login or register via Element at &lt;a href=&#34;https://app.element.io/#/login&#34;&gt;https://app.element.io/#/login&lt;/a&gt; or &lt;a href=&#34;https://app.element.io/#/register&#34;&gt;https://app.element.io/#/register&lt;/a&gt; respectively. You will need to change the server you are logging into from &lt;code&gt;matrix.org&lt;/code&gt; and instead specify a Homeserver URL of &lt;code&gt;https://&amp;lt;server_name&amp;gt;:8448&lt;/code&gt; (or just &lt;code&gt;https://&amp;lt;server_name&amp;gt;&lt;/code&gt; if you are using a reverse proxy). If you prefer to use another client, refer to our &lt;code&gt;client breakdown &amp;lt;https://matrix.org/docs/projects/clients-matrix&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;If all goes well you should at least be able to log in, create a room, and start sending messages.&lt;/p&gt; &#xA;&lt;p&gt;.. _&lt;code&gt;client-user-reg&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;h2&gt;Registering a new user from a client&lt;/h2&gt; &#xA;&lt;p&gt;By default, registration of new users via Matrix clients is disabled. To enable it, specify &lt;code&gt;enable_registration: true&lt;/code&gt; in &lt;code&gt;homeserver.yaml&lt;/code&gt;. (It is then recommended to also set up CAPTCHA - see &lt;code&gt;&amp;lt;docs/CAPTCHA_SETUP.md&amp;gt;&lt;/code&gt;_.)&lt;/p&gt; &#xA;&lt;p&gt;Once &lt;code&gt;enable_registration&lt;/code&gt; is set to &lt;code&gt;true&lt;/code&gt;, it is possible to register a user via a Matrix client.&lt;/p&gt; &#xA;&lt;p&gt;Your new user name will be formed partly from the &lt;code&gt;server_name&lt;/code&gt;, and partly from a localpart you specify when you create the account. Your name will take the form of::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@localpart:my.domain.name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(pronounced &#34;at localpart on my dot domain dot name&#34;).&lt;/p&gt; &#xA;&lt;p&gt;As when logging in, you will need to specify a &#34;Custom server&#34;. Specify your desired &lt;code&gt;localpart&lt;/code&gt; in the &#39;User name&#39; box.&lt;/p&gt; &#xA;&lt;h1&gt;Security note&lt;/h1&gt; &#xA;&lt;p&gt;Matrix serves raw, user-supplied data in some APIs -- specifically the &lt;code&gt;content repository endpoints&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;.. _content repository endpoints: &lt;a href=&#34;https://matrix.org/docs/spec/client_server/latest.html#get-matrix-media-r0-download-servername-mediaid&#34;&gt;https://matrix.org/docs/spec/client_server/latest.html#get-matrix-media-r0-download-servername-mediaid&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Whilst we make a reasonable effort to mitigate against XSS attacks (for instance, by using &lt;code&gt;CSP&lt;/code&gt;_), a Matrix homeserver should not be hosted on a domain hosting other web applications. This especially applies to sharing the domain with Matrix web clients and other sensitive applications like webmail. See &lt;a href=&#34;https://developer.github.com/changes/2014-04-25-user-content-security&#34;&gt;https://developer.github.com/changes/2014-04-25-user-content-security&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;.. _CSP: &lt;a href=&#34;https://github.com/matrix-org/synapse/pull/1021&#34;&gt;https://github.com/matrix-org/synapse/pull/1021&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ideally, the homeserver should not simply be on a different subdomain, but on a completely different &lt;code&gt;registered domain&lt;/code&gt;_ (also known as top-level site or eTLD+1). This is because &lt;code&gt;some attacks&lt;/code&gt;_ are still possible as long as the two applications share the same registered domain.&lt;/p&gt; &#xA;&lt;p&gt;.. _registered domain: &lt;a href=&#34;https://tools.ietf.org/html/draft-ietf-httpbis-rfc6265bis-03#section-2.3&#34;&gt;https://tools.ietf.org/html/draft-ietf-httpbis-rfc6265bis-03#section-2.3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. _some attacks: &lt;a href=&#34;https://en.wikipedia.org/wiki/Session_fixation#Attacks_using_cross-subdomain_cookie&#34;&gt;https://en.wikipedia.org/wiki/Session_fixation#Attacks_using_cross-subdomain_cookie&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To illustrate this with an example, if your Element Web or other sensitive web application is hosted on &lt;code&gt;A.example1.com&lt;/code&gt;, you should ideally host Synapse on &lt;code&gt;example2.com&lt;/code&gt;. Some amount of protection is offered by hosting on &lt;code&gt;B.example1.com&lt;/code&gt; instead, so this is also acceptable in some scenarios. However, you should &lt;em&gt;not&lt;/em&gt; host your Synapse on &lt;code&gt;A.example1.com&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that all of the above refers exclusively to the domain used in Synapse&#39;s &lt;code&gt;public_baseurl&lt;/code&gt; setting. In particular, it has no bearing on the domain mentioned in MXIDs hosted on that server.&lt;/p&gt; &#xA;&lt;p&gt;Following this advice ensures that even if an XSS is found in Synapse, the impact to other applications will be minimal.&lt;/p&gt; &#xA;&lt;h1&gt;Upgrading an existing Synapse&lt;/h1&gt; &#xA;&lt;p&gt;The instructions for upgrading synapse are in &lt;code&gt;the upgrade notes&lt;/code&gt;_. Please check these instructions as upgrading may require extra steps for some versions of synapse.&lt;/p&gt; &#xA;&lt;p&gt;.. _the upgrade notes: &lt;a href=&#34;https://matrix-org.github.io/synapse/develop/upgrade.html&#34;&gt;https://matrix-org.github.io/synapse/develop/upgrade.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. _reverse-proxy:&lt;/p&gt; &#xA;&lt;h1&gt;Using a reverse proxy with Synapse&lt;/h1&gt; &#xA;&lt;p&gt;It is recommended to put a reverse proxy such as &lt;code&gt;nginx &amp;lt;https://nginx.org/en/docs/http/ngx_http_proxy_module.html&amp;gt;&lt;/code&gt;&lt;em&gt;, &lt;code&gt;Apache &amp;lt;https://httpd.apache.org/docs/current/mod/mod_proxy_http.html&amp;gt;&lt;/code&gt;&lt;/em&gt;, &lt;code&gt;Caddy &amp;lt;https://caddyserver.com/docs/quick-starts/reverse-proxy&amp;gt;&lt;/code&gt;&lt;em&gt;, &lt;code&gt;HAProxy &amp;lt;https://www.haproxy.org/&amp;gt;&lt;/code&gt;&lt;/em&gt; or &lt;code&gt;relayd &amp;lt;https://man.openbsd.org/relayd.8&amp;gt;&lt;/code&gt;_ in front of Synapse. One advantage of doing so is that it means that you can expose the default https port (443) to Matrix clients without needing to run Synapse with root privileges.&lt;/p&gt; &#xA;&lt;p&gt;For information on configuring one, see &lt;code&gt;&amp;lt;docs/reverse_proxy.md&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h1&gt;Identity Servers&lt;/h1&gt; &#xA;&lt;p&gt;Identity servers have the job of mapping email addresses and other 3rd Party IDs (3PIDs) to Matrix user IDs, as well as verifying the ownership of 3PIDs before creating that mapping.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;They are not where accounts or credentials are stored - these live on home servers. Identity Servers are just for mapping 3rd party IDs to matrix IDs.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This process is very security-sensitive, as there is obvious risk of spam if it is too easy to sign up for Matrix accounts or harvest 3PID data. In the longer term, we hope to create a decentralised system to manage it (&lt;code&gt;matrix-doc #712 &amp;lt;https://github.com/matrix-org/matrix-doc/issues/712&amp;gt;&lt;/code&gt;&lt;em&gt;), but in the meantime, the role of managing trusted identity in the Matrix ecosystem is farmed out to a cluster of known trusted ecosystem partners, who run &#39;Matrix Identity Servers&#39; such as &lt;code&gt;Sydent &amp;lt;https://github.com/matrix-org/sydent&amp;gt;&lt;/code&gt;&lt;/em&gt;, whose role is purely to authenticate and track 3PID logins and publish end-user public keys.&lt;/p&gt; &#xA;&lt;p&gt;You can host your own copy of Sydent, but this will prevent you reaching other users in the Matrix ecosystem via their email address, and prevent them finding you. We therefore recommend that you use one of the centralised identity servers at &lt;code&gt;https://matrix.org&lt;/code&gt; or &lt;code&gt;https://vector.im&lt;/code&gt; for now.&lt;/p&gt; &#xA;&lt;p&gt;To reiterate: the Identity server will only be used if you choose to associate an email address with your account, or send an invite to another user via their email address.&lt;/p&gt; &#xA;&lt;h1&gt;Password reset&lt;/h1&gt; &#xA;&lt;p&gt;Users can reset their password through their client. Alternatively, a server admin can reset a users password using the &lt;code&gt;admin API &amp;lt;docs/admin_api/user_admin_api.md#reset-password&amp;gt;&lt;/code&gt;_ or by directly editing the database as shown below.&lt;/p&gt; &#xA;&lt;p&gt;First calculate the hash of the new password::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ~/synapse/env/bin/hash_password&#xA;Password:&#xA;Confirm password:&#xA;$2a$12$xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then update the &lt;code&gt;users&lt;/code&gt; table in the database::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;UPDATE users SET password_hash=&#39;$2a$12$xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#39;&#xA;    WHERE name=&#39;@test:test.com&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Synapse Development&lt;/h1&gt; &#xA;&lt;p&gt;The best place to get started is our &lt;code&gt;guide for contributors &amp;lt;https://matrix-org.github.io/synapse/latest/development/contributing_guide.html&amp;gt;&lt;/code&gt;&lt;em&gt;. This is part of our larger &lt;code&gt;documentation &amp;lt;https://matrix-org.github.io/synapse/latest&amp;gt;&lt;/code&gt;&lt;/em&gt;, which includes information for synapse developers as well as synapse administrators.&lt;/p&gt; &#xA;&lt;p&gt;Developers might be particularly interested in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Synapse&#39;s database schema &amp;lt;https://matrix-org.github.io/synapse/latest/development/database_schema.html&amp;gt;&lt;/code&gt;_,&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;notes on Synapse&#39;s implementation details &amp;lt;https://matrix-org.github.io/synapse/latest/development/internal_documentation/index.html&amp;gt;&lt;/code&gt;_, and&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;how we use git &amp;lt;https://matrix-org.github.io/synapse/latest/development/git.html&amp;gt;&lt;/code&gt;_.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Alongside all that, join our developer community on Matrix: &lt;code&gt;#synapse-dev:matrix.org &amp;lt;https://matrix.to/#/#synapse-dev:matrix.org&amp;gt;&lt;/code&gt;_, featuring real humans!&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;Before setting up a development environment for synapse, make sure you have the system dependencies (such as the python header files) installed - see &lt;code&gt;Platform-specific prerequisites &amp;lt;https://matrix-org.github.io/synapse/latest/setup/installation.html#platform-specific-prerequisites&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;To check out a synapse for development, clone the git repo into a working directory of your choice::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/matrix-org/synapse.git&#xA;cd synapse&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Synapse has a number of external dependencies. We maintain a fixed development environment using &lt;code&gt;Poetry &amp;lt;https://python-poetry.org/&amp;gt;&lt;/code&gt;_. First, install poetry. We recommend::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --user pipx&#xA;pipx install poetry&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;as described &lt;code&gt;here &amp;lt;https://python-poetry.org/docs/#installing-with-pipx&amp;gt;&lt;/code&gt;&lt;em&gt;. (See &lt;code&gt;poetry&#39;s installation docs &amp;lt;https://python-poetry.org/docs/#installation&amp;gt;&lt;/code&gt;&lt;/em&gt; for other installation methods.) Then ask poetry to create a virtual environment from the project and install Synapse&#39;s dependencies::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;poetry install --extras &#34;all test&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run a process of downloading and installing all the needed dependencies into a virtual env.&lt;/p&gt; &#xA;&lt;p&gt;We recommend using the demo which starts 3 federated instances running on ports &lt;code&gt;8080&lt;/code&gt; - &lt;code&gt;8082&lt;/code&gt;::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;poetry run ./demo/start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(to stop, you can use &lt;code&gt;poetry run ./demo/stop.sh&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;code&gt;demo documentation &amp;lt;https://matrix-org.github.io/synapse/develop/development/demo.html&amp;gt;&lt;/code&gt;_ for more information.&lt;/p&gt; &#xA;&lt;p&gt;If you just want to start a single instance of the app and run it directly::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Create the homeserver.yaml config once&#xA;poetry run synapse_homeserver \&#xA;  --server-name my.domain.name \&#xA;  --config-path homeserver.yaml \&#xA;  --generate-config \&#xA;  --report-stats=[yes|no]&#xA;&#xA;# Start the app&#xA;poetry run synapse_homeserver --config-path homeserver.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the unit tests&lt;/h2&gt; &#xA;&lt;p&gt;After getting up and running, you may wish to run Synapse&#39;s unit tests to check that everything is installed correctly::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;poetry run trial tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should end with a &#39;PASSED&#39; result (note that exact numbers will differ)::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Ran 1337 tests in 716.064s&#xA;&#xA;PASSED (skips=15, successes=1322)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more tips on running the unit tests, like running a specific test or to see the logging output, see the &lt;code&gt;CONTRIBUTING doc &amp;lt;CONTRIBUTING.md#run-the-unit-tests&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;h2&gt;Running the Integration Tests&lt;/h2&gt; &#xA;&lt;p&gt;Synapse is accompanied by &lt;code&gt;SyTest &amp;lt;https://github.com/matrix-org/sytest&amp;gt;&lt;/code&gt;_, a Matrix homeserver integration testing suite, which uses HTTP requests to access the API as a Matrix client would. It is able to run Synapse directly from the source tree, so installation of the server is not required.&lt;/p&gt; &#xA;&lt;p&gt;Testing with SyTest is recommended for verifying that changes related to the Client-Server API are functioning correctly. See the &lt;code&gt;SyTest installation instructions &amp;lt;https://github.com/matrix-org/sytest#installing&amp;gt;&lt;/code&gt;_ for details.&lt;/p&gt; &#xA;&lt;h1&gt;Platform dependencies&lt;/h1&gt; &#xA;&lt;p&gt;Synapse uses a number of platform dependencies such as Python and PostgreSQL, and aims to follow supported upstream versions. See the &lt;code&gt;&amp;lt;docs/deprecation_policy.md&amp;gt;&lt;/code&gt;_ document for more details.&lt;/p&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;p&gt;Need help? Join our community support room on Matrix: &lt;code&gt;#synapse:matrix.org &amp;lt;https://matrix.to/#/#synapse:matrix.org&amp;gt;&lt;/code&gt;_&lt;/p&gt; &#xA;&lt;h2&gt;Running out of File Handles&lt;/h2&gt; &#xA;&lt;p&gt;If synapse runs out of file handles, it typically fails badly - live-locking at 100% CPU, and/or failing to accept new TCP connections (blocking the connecting client). Matrix currently can legitimately use a lot of file handles, thanks to busy rooms like #matrix:matrix.org containing hundreds of participating servers. The first time a server talks in a room it will try to connect simultaneously to all participating servers, which could exhaust the available file descriptors between DNS queries &amp;amp; HTTPS sockets, especially if DNS is slow to respond. (We need to improve the routing algorithm used to be better than full mesh, but as of March 2019 this hasn&#39;t happened yet).&lt;/p&gt; &#xA;&lt;p&gt;If you hit this failure mode, we recommend increasing the maximum number of open file handles to be at least 4096 (assuming a default of 1024 or 256). This is typically done by editing &lt;code&gt;/etc/security/limits.conf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Separately, Synapse may leak file handles if inbound HTTP requests get stuck during processing - e.g. blocked behind a lock or talking to a remote server etc. This is best diagnosed by matching up the &#39;Received request&#39; and &#39;Processed request&#39; log lines and looking for any &#39;Processed request&#39; lines which take more than a few seconds to execute. Please let us know at #synapse:matrix.org if you see this failure mode so we can help debug it, however.&lt;/p&gt; &#xA;&lt;h2&gt;Help!! Synapse is slow and eats all my RAM/CPU!&lt;/h2&gt; &#xA;&lt;p&gt;First, ensure you are running the latest version of Synapse, using Python 3 with a PostgreSQL database.&lt;/p&gt; &#xA;&lt;p&gt;Synapse&#39;s architecture is quite RAM hungry currently - we deliberately cache a lot of recent room data and metadata in RAM in order to speed up common requests. We&#39;ll improve this in the future, but for now the easiest way to either reduce the RAM usage (at the risk of slowing things down) is to set the almost-undocumented &lt;code&gt;SYNAPSE_CACHE_FACTOR&lt;/code&gt; environment variable. The default is 0.5, which can be decreased to reduce RAM usage in memory constrained enviroments, or increased if performance starts to degrade.&lt;/p&gt; &#xA;&lt;p&gt;However, degraded performance due to a low cache factor, common on machines with slow disks, often leads to explosions in memory use due backlogged requests. In this case, reducing the cache factor will make things worse. Instead, try increasing it drastically. 2.0 is a good starting value.&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;code&gt;libjemalloc &amp;lt;http://jemalloc.net/&amp;gt;&lt;/code&gt;_ can also yield a significant improvement in overall memory use, and especially in terms of giving back RAM to the OS. To use it, the library must simply be put in the LD_PRELOAD environment variable when launching Synapse. On Debian, this can be done by installing the &lt;code&gt;libjemalloc1&lt;/code&gt; package and adding this line to &lt;code&gt;/etc/default/matrix-synapse&lt;/code&gt;::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This can make a significant difference on Python 2.7 - it&#39;s unclear how much of an improvement it provides on Python 3.x.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re encountering high CPU use by the Synapse process itself, you may be affected by a bug with presence tracking that leads to a massive excess of outgoing federation requests (see &lt;code&gt;discussion &amp;lt;https://github.com/matrix-org/synapse/issues/3971&amp;gt;&lt;/code&gt;_). If metrics indicate that your server is also issuing far more outgoing federation requests than can be accounted for by your users&#39; activity, this is a likely cause. The misbehavior can be worked around by setting the following in the Synapse config file:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: yaml&lt;/p&gt; &#xA;&lt;p&gt;presence: enabled: false&lt;/p&gt; &#xA;&lt;h2&gt;People can&#39;t accept room invitations from me&lt;/h2&gt; &#xA;&lt;p&gt;The typical failure mode here is that you send an invitation to someone to join a room or direct chat, but when they go to accept it, they get an error (typically along the lines of &#34;Invalid signature&#34;). They might see something like the following in their logs::&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;2019-09-11 19:32:04,271 - synapse.federation.transport.server - 288 - WARNING - GET-11752 - authenticate_request failed: 401: Invalid signature for server &amp;lt;server&amp;gt; with key ed25519:a_EqML: Unable to verify signature for &amp;lt;server&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is normally caused by a misconfiguration in your reverse-proxy. See &lt;code&gt;&amp;lt;docs/reverse_proxy.md&amp;gt;&lt;/code&gt;_ and double-check that your settings are correct.&lt;/p&gt; &#xA;&lt;p&gt;.. |support| image:: &lt;a href=&#34;https://img.shields.io/matrix/synapse:matrix.org?label=support&amp;amp;logo=matrix&#34;&gt;https://img.shields.io/matrix/synapse:matrix.org?label=support&amp;amp;logo=matrix&lt;/a&gt; :alt: (get support on #synapse:matrix.org) :target: &lt;a href=&#34;https://matrix.to/#/#synapse:matrix.org&#34;&gt;https://matrix.to/#/#synapse:matrix.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |development| image:: &lt;a href=&#34;https://img.shields.io/matrix/synapse-dev:matrix.org?label=development&amp;amp;logo=matrix&#34;&gt;https://img.shields.io/matrix/synapse-dev:matrix.org?label=development&amp;amp;logo=matrix&lt;/a&gt; :alt: (discuss development on #synapse-dev:matrix.org) :target: &lt;a href=&#34;https://matrix.to/#/#synapse-dev:matrix.org&#34;&gt;https://matrix.to/#/#synapse-dev:matrix.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |documentation| image:: &lt;a href=&#34;https://img.shields.io/badge/documentation-%E2%9C%93-success&#34;&gt;https://img.shields.io/badge/documentation-%E2%9C%93-success&lt;/a&gt; :alt: (Rendered documentation on GitHub Pages) :target: &lt;a href=&#34;https://matrix-org.github.io/synapse/latest/&#34;&gt;https://matrix-org.github.io/synapse/latest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |license| image:: &lt;a href=&#34;https://img.shields.io/github/license/matrix-org/synapse&#34;&gt;https://img.shields.io/github/license/matrix-org/synapse&lt;/a&gt; :alt: (check license in LICENSE file) :target: LICENSE&lt;/p&gt; &#xA;&lt;p&gt;.. |pypi| image:: &lt;a href=&#34;https://img.shields.io/pypi/v/matrix-synapse&#34;&gt;https://img.shields.io/pypi/v/matrix-synapse&lt;/a&gt; :alt: (latest version released on PyPi) :target: &lt;a href=&#34;https://pypi.org/project/matrix-synapse&#34;&gt;https://pypi.org/project/matrix-synapse&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. |python| image:: &lt;a href=&#34;https://img.shields.io/pypi/pyversions/matrix-synapse&#34;&gt;https://img.shields.io/pypi/pyversions/matrix-synapse&lt;/a&gt; :alt: (supported python versions) :target: &lt;a href=&#34;https://pypi.org/project/matrix-synapse&#34;&gt;https://pypi.org/project/matrix-synapse&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>swisskyrepo/PayloadsAllTheThings</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/swisskyrepo/PayloadsAllTheThings</id>
    <link href="https://github.com/swisskyrepo/PayloadsAllTheThings" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Payloads All The Things &lt;a href=&#34;https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques ! I &lt;span&gt;❤️&lt;/span&gt; pull requests :)&lt;/p&gt; &#xA;&lt;p&gt;You can also contribute with a &lt;span&gt;🍻&lt;/span&gt; IRL, or using the sponsor button.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;📖 Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt; &#xA; &lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt; &#xA; &lt;li&gt;Images - pictures for the README.md&lt;/li&gt; &#xA; &lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/&#34;&gt;Methodology and Resources&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md&#34;&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Cloud%20-%20AWS%20Pentest.md&#34;&gt;Cloud - AWS Pentest.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Cloud%20-%20Azure%20Pentest.md&#34;&gt;Cloud - Azure Pentest.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Cobalt%20Strike%20-%20Cheatsheet.md&#34;&gt;Cobalt Strike - Cheatsheet.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md&#34;&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md&#34;&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md&#34;&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md&#34;&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md&#34;&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Network%20Discovery.md&#34;&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md&#34;&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md&#34;&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md&#34;&gt;Windows - Download and Execute.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Windows%20-%20Mimikatz.md&#34;&gt;Windows - Mimikatz.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Windows%20-%20Persistence.md&#34;&gt;Windows - Persistence.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Windows%20-%20Post%20Exploitation%20Koadic.md&#34;&gt;Windows - Post Exploitation Koadic.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Windows%20-%20Privilege%20Escalation.md&#34;&gt;Windows - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/Methodology%20and%20Resources/Windows%20-%20Using%20credentials.md&#34;&gt;Windows - Using credentials.md&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/CVE%20Exploits&#34;&gt;CVE Exploits&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You want more ? Check the &lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/BOOKS.md&#34;&gt;Books&lt;/a&gt; and &lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/YOUTUBE.md&#34;&gt;Youtube videos&lt;/a&gt; selections.&lt;/p&gt; &#xA;&lt;h2&gt;👨‍💻 Contributions&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;amp;max=36&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Thanks again for your contribution! &lt;span&gt;❤️&lt;/span&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>elastic/helm-charts</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/elastic/helm-charts</id>
    <link href="https://github.com/elastic/helm-charts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;You know, for Kubernetes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Elastic Stack Kubernetes Helm Charts&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://devops-ci.elastic.co/job/elastic+helm-charts+main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/jenkins/s/https/devops-ci.elastic.co/job/elastic+helm-charts+main.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://artifacthub.io/packages/search?repo=elastic&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/elastic&#34; alt=&#34;Artifact HUB&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/#charts&#34;&gt;Charts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/#supported-configurations&#34;&gt;Supported Configurations&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/#stack-versions&#34;&gt;Stack Versions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/#kubernetes-versions&#34;&gt;Kubernetes Versions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/#helm-versions&#34;&gt;Helm Versions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/#eck&#34;&gt;ECK&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;!-- Use this to update TOC: --&gt; &#xA;&lt;!-- docker run --entrypoint doctoc --rm -it -v $(pwd):/usr/src jorgeandrada/doctoc README.md --github --no-title --&gt; &#xA;&lt;h2&gt;Charts&lt;/h2&gt; &#xA;&lt;p&gt;These Helm charts are designed to be a lightweight way to configure Elastic official Docker images.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Configurations&lt;/h2&gt; &#xA;&lt;p&gt;We recommend that the Helm chart version is aligned to the version of the product you want to deploy. This will ensure that you are using a chart version that has been tested against the corresponding production version. This will also ensure that the documentation and examples for the chart will work with the version of the product, you are installing.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to deploy an Elasticsearch &lt;code&gt;7.7.1&lt;/code&gt; cluster, use the corresponding &lt;code&gt;7.7.1&lt;/code&gt; &lt;a href=&#34;https://github.com/elastic/helm-charts/tree/7.7.1/elasticsearch/&#34;&gt;tag&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;master&lt;/code&gt; version of these charts is intended to support the latest pre-release versions of our products, and therefore may or may not work with current released versions. Note that only the released charts coming from &lt;a href=&#34;https://helm.elastic.co&#34;&gt;Elastic Helm repo&lt;/a&gt; or &lt;a href=&#34;https://github.com/elastic/helm-charts/releases&#34;&gt;GitHub releases&lt;/a&gt; are supported.&lt;/p&gt; &#xA;&lt;h3&gt;Stack Versions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Chart&lt;/th&gt; &#xA;   &lt;th&gt;Latest 7 Version&lt;/th&gt; &#xA;   &lt;th&gt;Latest 6 Version&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/apm-server/README.md&#34;&gt;APM-Server&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/7.17/apm-server/README.md&#34;&gt;&lt;code&gt;7.17.3&lt;/code&gt;&lt;/a&gt; (Beta since 7.7.0)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/6.8/apm-server/README.md&#34;&gt;&lt;code&gt;6.8.22&lt;/code&gt;&lt;/a&gt; (Alpha)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/elasticsearch/README.md&#34;&gt;Elasticsearch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/7.17/elasticsearch/README.md&#34;&gt;&lt;code&gt;7.17.3&lt;/code&gt;&lt;/a&gt; (GA since 7.7.0)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/6.8/elasticsearch/README.md&#34;&gt;&lt;code&gt;6.8.22&lt;/code&gt;&lt;/a&gt; (Beta)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/filebeat/README.md&#34;&gt;Filebeat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/7.17/filebeat/README.md&#34;&gt;&lt;code&gt;7.17.3&lt;/code&gt;&lt;/a&gt; (GA since 7.7.0)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/6.8/filebeat/README.md&#34;&gt;&lt;code&gt;6.8.22&lt;/code&gt;&lt;/a&gt; (Beta)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/kibana/README.md&#34;&gt;Kibana&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/7.17/kibana/README.md&#34;&gt;&lt;code&gt;7.17.3&lt;/code&gt;&lt;/a&gt; (GA since 7.7.0)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/6.8/kibana/README.md&#34;&gt;&lt;code&gt;6.8.22&lt;/code&gt;&lt;/a&gt; (Beta)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/logstash/README.md&#34;&gt;Logstash&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/7.17/logstash/README.md&#34;&gt;&lt;code&gt;7.17.3&lt;/code&gt;&lt;/a&gt; (Beta since 7.5.0)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/6.8/logstash/README.md&#34;&gt;&lt;code&gt;6.8.22&lt;/code&gt;&lt;/a&gt; (Beta)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/elastic/helm-charts/main/metricbeat/README.md&#34;&gt;Metricbeat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/7.17/metricbeat/README.md&#34;&gt;&lt;code&gt;7.17.3&lt;/code&gt;&lt;/a&gt; (GA since 7.7.0)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/elastic/helm-charts/tree/6.8/metricbeat/README.md&#34;&gt;&lt;code&gt;6.8.22&lt;/code&gt;&lt;/a&gt; (Beta)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Kubernetes Versions&lt;/h3&gt; &#xA;&lt;p&gt;The charts are &lt;a href=&#34;https://devops-ci.elastic.co/job/elastic+helm-charts+main/&#34;&gt;currently tested&lt;/a&gt; against all GKE versions available. The exact versions are defined under &lt;code&gt;KUBERNETES_VERSIONS&lt;/code&gt; in &lt;a href=&#34;https://github.com/elastic/helm-charts/raw/main/helpers/matrix.yml&#34;&gt;helpers/matrix.yml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Helm Versions&lt;/h3&gt; &#xA;&lt;p&gt;While we are checking backward compatibility, the charts are only tested with Helm version mentioned in &lt;a href=&#34;https://github.com/elastic/helm-charts/raw/main/helpers/helm-tester/Dockerfile&#34;&gt;helm-tester Dockerfile&lt;/a&gt; (currently 3.8.0).&lt;/p&gt; &#xA;&lt;h2&gt;ECK&lt;/h2&gt; &#xA;&lt;p&gt;In addition to these Helm charts, Elastic also provides &lt;a href=&#34;https://github.com/elastic/cloud-on-k8s&#34;&gt;Elastic Cloud on Kubernetes&lt;/a&gt; which is based on &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&#34;&gt;Operator pattern&lt;/a&gt; and is Elastic recommended way to deploy Elasticsearch, Kibana, and APM Server on Kubernetes. There is a dedicated Helm chart for ECK which can be found &lt;a href=&#34;https://github.com/elastic/cloud-on-k8s/tree/master/deploy&#34;&gt;in ECK repo&lt;/a&gt; (&lt;a href=&#34;https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-install-helm.html&#34;&gt;documentation&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>EstrellaXD/Auto_Bangumi</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/EstrellaXD/Auto_Bangumi</id>
    <link href="https://github.com/EstrellaXD/Auto_Bangumi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AutoBangumi - 全自动追番工具，节约时间创造价值&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/EstrellaXD/Auto_Bangumi/raw/main/Docs/image/auto_bangumi_v2.png#gh-light-mode-only&#34; width=&#34;50%/&#34;&gt; &lt;img src=&#34;https://github.com/EstrellaXD/Auto_Bangumi/raw/main/Docs/image/auto_bangumi_icon_v2-dark.png#gh-dark-mode-only&#34; width=&#34;50%/&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img title=&#34;docker build version&#34; src=&#34;https://img.shields.io/docker/v/estrellaxd/auto_bangumi&#34; alt=&#34;&#34;&gt; &lt;img title=&#34;release date&#34; src=&#34;https://img.shields.io/github/release-date/estrellaxd/auto_bangumi&#34; alt=&#34;&#34;&gt; &lt;img title=&#34;docker pull&#34; src=&#34;https://img.shields.io/docker/pulls/estrellaxd/auto_bangumi&#34; alt=&#34;&#34;&gt; &lt;img title=&#34;python version&#34; src=&#34;https://img.shields.io/badge/python-3.10-blue&#34; alt=&#34;&#34;&gt; &lt;img title=&#34;platform arch&#34; src=&#34;https://img.shields.io/badge/arch-%20AMD64%20%2F%20ARM64-lightgrey&#34; alt=&#34;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;项目说明&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img title=&#34;mikan project&#34; src=&#34;https://mikanani.me/images/mikan-pic.png&#34; alt=&#34;&#34; width=&#34;10%&#34;&gt; &lt;img title=&#34;qbittorrent&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/New_qBittorrent_Logo.svg/600px-New_qBittorrent_Logo.svg.png&#34; width=&#34;10%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;本项目是基于 &lt;a href=&#34;https://mikanani.me&#34;&gt;Mikan Project&lt;/a&gt;、&lt;a href=&#34;https://qbittorrent.org&#34;&gt;qBittorrent&lt;/a&gt; 的全自动追番整理下载工具。只需要在 &lt;a href=&#34;https://mikanani.me&#34;&gt;Mikan Project&lt;/a&gt; 上订阅番剧，就可以全自动追番。并且整理完成的名称和目录可以直接被 &lt;a href=&#34;&#34;&gt;Plex&lt;/a&gt;、&lt;a href=&#34;&#34;&gt;Jellyfin&lt;/a&gt; 等媒体库软件识别，无需二次刮削。&lt;/p&gt; &#xA;&lt;p&gt;基于 &lt;a href=&#34;https://firecore.com/infuse&#34;&gt;infuse&lt;/a&gt; 与 &lt;a href=&#34;https://plex.tv&#34;&gt;Plex&lt;/a&gt; 的效果如下：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img title=&#34;plex&#34; src=&#34;https://github.com/EstrellaXD/Auto_Bangumi/blob/main/Docs/image/截屏2022-05-23%2020.47.39.png&#34; alt=&#34;&#34; width=&#34;50%&#34;&gt;&lt;img title=&#34;infuse&#34; src=&#34;https://github.com/EstrellaXD/Auto_Bangumi/blob/main/Docs/image/截屏2022-05-23%2020.48.02.png&#34; alt=&#34;&#34; width=&#34;50%&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;AutoBangumi 好在哪&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;只需单次配置&lt;/li&gt; &#xA; &lt;li&gt;修改文件名但是不破坏作种&lt;/li&gt; &#xA; &lt;li&gt;傻瓜式文件整理&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;相关文档和群组&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.craft.do/s/4viN6M3tBqigLp&#34;&gt;AutoBangumi V2 简易说明&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EstrellaXD/Auto_Bangumi/raw/main/Docs/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3.md&#34;&gt;常见 bug 和解决方法&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;更新推送：&lt;a href=&#34;https://t.me/autobangumi_update&#34;&gt;Telegram Channel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bug 反馈群：&lt;a href=&#34;https://t.me/+yNisOnDGaX5jMTM9&#34;&gt;Telegram&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;部署说明&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;安装 qBittorrent:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;用 Docker 部署 &lt;code&gt;AutoBangumi&lt;/code&gt; :&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;最简部署方法&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dash&#34;&gt;docker run -d \&#xA;  --name=AutoBangumi \&#xA;  -e DOWNLOAD_PATH=/path/downloads \&#xA;  -e RSS=&amp;lt;YOUR_RSS_ADDRESS&amp;gt; \&#xA;  --network=host \&#xA;  --dns=8.8.8.8 \&#xA;  --restart unless-stopped \&#xA;  estrellaxd/auto_bangumi:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;进阶部署:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Docker-Compose&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: &#39;3.3&#39;&#xA;services:&#xA;    auto_bangumi:&#xA;        container_name: AutoBangumi&#xA;        environment:&#xA;            - TZ=Asia/Shanghai&#xA;            - TIME=1800&#xA;            - HOST=localhost:8080&#xA;            - USER=admin&#xA;            - PASSWORD=adminadmin&#xA;            - METHOD=pn&#xA;            - GROUP_TAG=True&#xA;            - NOT_CONTAIN=720&#xA;            - DOWNLOAD_PATH=/path/downloads&#xA;            - RSS=YOUR_RSS_ADDRESS&#xA;        network_mode: host&#xA;        dns:&#xA;            - 8.8.8.8&#xA;            - 223.5.5.5&#xA;        restart: unless-stopped&#xA;        image: estrellaxd/auto_bangumi:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker-cli&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-dash&#34;&gt;docker run -d \&#xA;  --name=AutoBangumi \&#xA;  -e TZ=Asia/Shanghai \ #optional&#xA;  -e TIME=1800 \ #optional&#xA;  -e HOST=localhost:8080 \ #optional&#xA;  -e USER=admin \ #optional&#xA;  -e PASSWORD=adminadmin \ #optional&#xA;  -e METHOD=pn \ #optional&#xA;  -e GROUP_TAG=True \ #optional&#xA;  -e DOWNLOAD_PATH=/path/downloads \&#xA;  -e NOT_COTAIN=720&#xA;  -e RSS=&amp;lt;YOUR_RSS_ADDRESS&amp;gt; \&#xA;  --network=host \&#xA;  --dns=8.8.8.8 \&#xA;  --restart unless-stopped \&#xA;  estrellaxd/auto_bangumi:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;参数说明&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;环境变量&lt;/th&gt; &#xA;   &lt;th&gt;作用&lt;/th&gt; &#xA;   &lt;th&gt;参数&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;TZ&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;时区&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Asia/Shanghai&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;TIME&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;间隔时间&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1800&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;HOST&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;qBittorrent 的地址和端口号&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;USER&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;qBittorrent 的用户名&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;admin&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;PASSWORD&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;qBittorrent 的密码&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;adminadmin&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;METHOD&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;重命名方法&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pn&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;GROUP_TAG&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;是否在下载规则中添加组名&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;NOT_CONTAIN&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;正则表达式过滤器&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;720&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;DOWNLOAD_PATH&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;qBittorrent 中的下载路径&lt;/td&gt; &#xA;   &lt;td&gt;必填项&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;RSS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RSS 订阅地址&lt;/td&gt; &#xA;   &lt;td&gt;必填项&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;TIME&lt;/code&gt; : 程序运行的间隔时间，默认为 &lt;code&gt;1800&lt;/code&gt; 也就是 30 分钟，如果更新时间要求比较高可以适当降低该值。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;HOST&lt;/code&gt;, &lt;code&gt;USER&lt;/code&gt;, &lt;code&gt;PASSWORD&lt;/code&gt;: qBittorrent 的地址，用户名，密码。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;METHOD&lt;/code&gt;: 重命名规则 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pn&lt;/code&gt;: Pure Name 模式，去掉所有字幕组以及番剧额外信息，只保留名称、季度和集数。&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;normal&lt;/code&gt;: 正常模式，仅重命名会影响搜刮的非正常字符。&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GROUP_TAG&lt;/code&gt;: 开启后自动在自动下载规则中创建组名，方便管理。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DOWNLOAD_PATH&lt;/code&gt;: qBittorrent 的下载地址。&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RSS&lt;/code&gt;: Mikan Project 的个人 RSS 订阅链接&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NOT_CONTAIN&lt;/code&gt;: 可以自行添加过滤的正则表达式&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;检查 Docker 运行日志，出现：&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-other&#34;&gt;[2022-05-20 12:47:47]  RSS Already exists.&#xA;[2022-05-20 12:47:47]  add Summer Time Rendering &#xA;[2022-05-20 12:47:47]  add Paripi Koumei &#xA;[2022-05-20 12:47:47]  add Tomodachi Game &#xA;[2022-05-20 12:47:47]  add Tate no Yuusha no Nariagari S02&#xA;[2022-05-20 12:47:47]  add Shijou Saikyou no Daimaou &#xA;[2022-05-20 12:47:47]  add Yuusha, Yamemasu &#xA;[2022-05-20 12:47:47]  add Aharen-san wa Hakarenai &#xA;[2022-05-20 12:47:47]  add Kawaii dake ja Nai Shikimori-san &#xA;[2022-05-20 12:47:47]  add Kakkou no Iinazuke &#xA;[2022-05-20 12:47:47]  add SPYxFAMILY &#xA;[2022-05-20 12:47:47]  add Love Live S02&#xA;[2022-05-20 12:47:47]  add BUILD-DIVIDE &#xA;[2022-05-20 12:47:47]  add Machikado Mazoku:-choume &#xA;[2022-05-20 12:47:47]  add CUE! &#xA;[2022-05-20 12:47:47]  add Kaguya-sama wa Kokurasetai S03&#xA;[2022-05-20 12:47:47]  add Shokei Shoujo no Virgin Road &#xA;[2022-05-20 12:47:47]  add Kakkou no Iikagen &#xA;[2022-05-20 12:47:47]  Start adding rules.&#xA;[2022-05-20 12:47:47]  Finished.&#xA;[2022-05-20 12:47:47]  已完成对0个文件的检查&#xA;[2022-05-20 12:47:47]  已对其中0个文件进行重命名&#xA;[2022-05-20 12:47:47]  完成&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;说明运行成功。之后可以检查 qb 中是否建立自动下载规则。&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;安装媒体库软件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;开发中的功能：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RSS 解析器：AutoBangumi 可以自行解析分析种子无需依赖下载器。&lt;/li&gt; &#xA; &lt;li&gt;Transmission &amp;amp; Aria2 的支持。&lt;/li&gt; &#xA; &lt;li&gt;遗漏番剧下载：中间开始追番可以补全之前的剧集。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;计划开发的功能：&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Web UI&lt;/li&gt; &#xA; &lt;li&gt;更为智能细致的分类预设。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;声明&lt;/h1&gt; &#xA;&lt;p&gt;本项目的自动改名规则根据 &lt;a href=&#34;https://github.com/miracleyoo/anime_renamer&#34;&gt;miracleyoo/anime_renamer&lt;/a&gt; 项目&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zulip/zulip</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/zulip/zulip</id>
    <link href="https://github.com/zulip/zulip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Zulip server and web app—powerful open source team chat&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zulip overview&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zulip.com&#34;&gt;Zulip&lt;/a&gt; is an open-source team collaboration tool with unique &lt;a href=&#34;https://zulip.com/why-zulip/&#34;&gt;topic-based threading&lt;/a&gt; that combines the best of email and chat to make remote work productive and delightful. Fortune 500 companies, &lt;a href=&#34;https://zulip.com/case-studies/rust/&#34;&gt;leading open source projects&lt;/a&gt;, and thousands of other organizations use Zulip every day. Zulip is the only &lt;a href=&#34;https://zulip.com/features/&#34;&gt;modern team chat app&lt;/a&gt; that is designed for both live and asynchronous conversations.&lt;/p&gt; &#xA;&lt;p&gt;Zulip is built by a distributed community of developers from all around the world, with 74+ people who have each contributed 100+ commits. With over 1000 contributors merging over 500 commits a month, Zulip is the largest and fastest growing open source team chat project.&lt;/p&gt; &#xA;&lt;p&gt;Come find us on the &lt;a href=&#34;https://zulip.com/development-community/&#34;&gt;development community chat&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions build status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/zulip/zulip&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/zulip/zulip/main.svg?sanitize=true&#34; alt=&#34;coverage status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mypy-100%25-green.svg?sanitize=true&#34; alt=&#34;Mypy coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/prettier/prettier&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code_style-prettier-ff69b4.svg?sanitize=true&#34; alt=&#34;code style: prettier&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zulip/zulip/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/zulip/zulip.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zulip.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/zulip/badge/?version=latest&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://chat.zulip.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/zulip-join_chat-brightgreen.svg?sanitize=true&#34; alt=&#34;Zulip chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/zulip&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/twitter-@zulip-blue.svg?style=flat&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sponsors/zulip&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/sponsors/zulip&#34; alt=&#34;GitHub Sponsors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contributing code&lt;/strong&gt;. Check out our &lt;a href=&#34;https://zulip.readthedocs.io/en/latest/overview/contributing.html&#34;&gt;guide for new contributors&lt;/a&gt; to get started. We have invested into making Zulip’s code uniquely readable, well tested, and easy to modify. Beyond that, we have written an extraordinary 150K words of documentation on how to contribute to Zulip.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contributing non-code&lt;/strong&gt;. &lt;a href=&#34;https://zulip.readthedocs.io/en/latest/overview/contributing.html#reporting-issues&#34;&gt;Report an issue&lt;/a&gt;, &lt;a href=&#34;https://zulip.readthedocs.io/en/latest/translating/translating.html&#34;&gt;translate&lt;/a&gt; Zulip into your language, or &lt;a href=&#34;https://zulip.readthedocs.io/en/latest/overview/contributing.html#user-feedback&#34;&gt;give us feedback&lt;/a&gt;. We&#39;d love to hear from you, whether you&#39;ve been using Zulip for years, or are just trying it out for the first time.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Checking Zulip out&lt;/strong&gt;. The best way to see Zulip in action is to drop by the &lt;a href=&#34;https://zulip.com/development-community/&#34;&gt;Zulip community server&lt;/a&gt;. We also recommend reading about Zulip&#39;s &lt;a href=&#34;https://zulip.com/why-zulip/&#34;&gt;unique approach&lt;/a&gt; to organizing conversations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Running a Zulip server&lt;/strong&gt;. Self host Zulip directly on Ubuntu or Debian Linux, in &lt;a href=&#34;https://github.com/zulip/docker-zulip&#34;&gt;Docker&lt;/a&gt;, or with prebuilt images for &lt;a href=&#34;https://marketplace.digitalocean.com/apps/zulip&#34;&gt;Digital Ocean&lt;/a&gt; and &lt;a href=&#34;https://render.com/docs/deploy-zulip&#34;&gt;Render&lt;/a&gt;. Learn more about &lt;a href=&#34;https://zulip.com/self-hosting/&#34;&gt;self-hosting Zulip&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using Zulip without setting up a server&lt;/strong&gt;. Learn about &lt;a href=&#34;https://zulip.com/plans/&#34;&gt;Zulip Cloud&lt;/a&gt; hosting options. Zulip sponsors free &lt;a href=&#34;https://zulip.com/plans/&#34;&gt;Zulip Cloud Standard&lt;/a&gt; for hundreds of worthy organizations, including &lt;a href=&#34;https://zulip.com/for/open-source/&#34;&gt;fellow open-source projects&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Participating in &lt;a href=&#34;https://zulip.readthedocs.io/en/latest/overview/contributing.html#outreach-programs&#34;&gt;outreach programs&lt;/a&gt;&lt;/strong&gt; like &lt;a href=&#34;https://developers.google.com/open-source/gsoc/&#34;&gt;Google Summer of Code&lt;/a&gt; and &lt;a href=&#34;https://www.outreachy.org/&#34;&gt;Outreachy&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Supporting Zulip&lt;/strong&gt;. Advocate for your organization to use Zulip, become a &lt;a href=&#34;https://github.com/sponsors/zulip&#34;&gt;sponsor&lt;/a&gt;, write a review in the mobile app stores, or &lt;a href=&#34;https://zulip.readthedocs.io/en/latest/overview/contributing.html#help-others-find-zulip&#34;&gt;help others find Zulip&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You may also be interested in reading our &lt;a href=&#34;https://blog.zulip.org/&#34;&gt;blog&lt;/a&gt;, and following us on &lt;a href=&#34;https://twitter.com/zulip&#34;&gt;Twitter&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/company/zulip-project/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Zulip is distributed under the &lt;a href=&#34;https://github.com/zulip/zulip/raw/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>psf/black</title>
    <updated>2022-06-03T01:31:30Z</updated>
    <id>tag:github.com,2022-06-03:/psf/black</id>
    <link href="https://github.com/psf/black" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The uncompromising Python code formatter&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png&#34; alt=&#34;Black Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;The Uncompromising Code Formatter&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/psf/black/actions&#34;&gt;&lt;img alt=&#34;Actions Status&#34; src=&#34;https://github.com/psf/black/workflows/Test/badge.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://black.readthedocs.io/en/stable/?badge=stable&#34;&gt;&lt;img alt=&#34;Documentation Status&#34; src=&#34;https://readthedocs.org/projects/black/badge/?version=stable&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://coveralls.io/github/psf/black?branch=main&#34;&gt;&lt;img alt=&#34;Coverage Status&#34; src=&#34;https://coveralls.io/repos/github/psf/black/badge.svg?branch=main&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black/raw/main/LICENSE&#34;&gt;&lt;img alt=&#34;License: MIT&#34; src=&#34;https://black.readthedocs.io/en/stable/_static/license.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/black/&#34;&gt;&lt;img alt=&#34;PyPI&#34; src=&#34;https://img.shields.io/pypi/v/black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/black&#34;&gt;&lt;img alt=&#34;Downloads&#34; src=&#34;https://pepy.tech/badge/black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/black/&#34;&gt;&lt;img alt=&#34;conda-forge&#34; src=&#34;https://img.shields.io/conda/dn/conda-forge/black.svg?label=conda-forge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img alt=&#34;Code style: black&#34; src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;“Any color you like.”&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed, determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time and mental energy for more important matters.&lt;/p&gt; &#xA;&lt;p&gt;Blackened code looks the same regardless of the project you&#39;re reading. Formatting becomes transparent after a while and you can focus on the content instead.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt; &#xA;&lt;p&gt;Try it out now using the &lt;a href=&#34;https://black.vercel.app&#34;&gt;Black Playground&lt;/a&gt;. Watch the &lt;a href=&#34;https://youtu.be/esZLCuWs_2Y&#34;&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable&#34;&gt;Read the documentation on ReadTheDocs!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation and usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.6.2+ to run. If you want to format Jupyter Notebooks, install with &lt;code&gt;pip install &#39;black[jupyter]&#39;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you can&#39;t wait for the latest &lt;em&gt;hotness&lt;/em&gt; and want to install from GitHub, use:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install git+https://github.com/psf/black&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;black {source_file_or_directory}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can run &lt;em&gt;Black&lt;/em&gt; as a package if running it as a script doesn&#39;t work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m black {source_file_or_directory}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Further information can be found in our docs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/usage_and_configuration/index.html&#34;&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href=&#34;https://github.com/psf/black#used-by&#34;&gt;successfully used&lt;/a&gt; by many projects, small and big. &lt;em&gt;Black&lt;/em&gt; has a comprehensive test suite, with efficient parallel tests, and our own auto formatting and parallel Continuous Integration runner. Now that we have become stable, you should not expect large formatting to changes in the future. Stylistic changes will mostly be responses to bug reports and support for new Python syntax. For more information please refer to the &lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/index.html&#34;&gt;The Black Code Style&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also, as a safety measure which slows down processing, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still produces a valid AST that is effectively equivalent to the original (see the &lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#ast-before-and-after-formatting&#34;&gt;Pragmatism&lt;/a&gt; section for details). If you&#39;re feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a PEP 8 compliant opinionated formatter. &lt;em&gt;Black&lt;/em&gt; reformats entire files in place. Style configuration options are deliberately limited and rarely added. It doesn&#39;t take previous formatting into account (see &lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism&#34;&gt;Pragmatism&lt;/a&gt; for exceptions).&lt;/p&gt; &#xA;&lt;p&gt;Our documentation covers the current &lt;em&gt;Black&lt;/em&gt; code style, but planned changes to it are also documented. They&#39;re both worth taking a look:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Current style&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/future_style.html&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Future style&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Changes to the &lt;em&gt;Black&lt;/em&gt; code style are bound by the Stability Policy:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/index.html#stability-policy&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Stability Policy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to this document before submitting an issue. What seems like a bug might be intended behaviour.&lt;/p&gt; &#xA;&lt;h3&gt;Pragmatism&lt;/h3&gt; &#xA;&lt;p&gt;Early versions of &lt;em&gt;Black&lt;/em&gt; used to be absolutist in some respects. They took after its initial author. This was fine at the time as it made the implementation simpler and there were not many users anyway. Not many edge cases were reported. As a mature tool, &lt;em&gt;Black&lt;/em&gt; does make some exceptions to rules it otherwise holds.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism&#34;&gt;The &lt;em&gt;Black&lt;/em&gt; code style: Pragmatism&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to this document before submitting an issue just like with the document above. What seems like a bug might be intended behaviour.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt;/&lt;code&gt;--force-exclude&lt;/code&gt;/&lt;code&gt;--extend-exclude&lt;/code&gt; patterns for your project.&lt;/p&gt; &#xA;&lt;p&gt;You can find more details in our documentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file&#34;&gt;The basics: Configuration via a file&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And if you&#39;re looking for more general configuration documentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/stable/usage_and_configuration/index.html&#34;&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you&#39;re asking yourself &#34;Do I need to configure anything?&#34; the answer is &#34;No&#34;. &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults. Applying those defaults will have your code in compliance with many other &lt;em&gt;Black&lt;/em&gt; formatted projects.&lt;/p&gt; &#xA;&lt;h2&gt;Used by&lt;/h2&gt; &#xA;&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs, SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv), pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant, Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more.&lt;/p&gt; &#xA;&lt;p&gt;The following organizations use &lt;em&gt;Black&lt;/em&gt;: Facebook, Dropbox, KeepTruckin, Mozilla, Quora, Duolingo, QuantumBlack, Tesla.&lt;/p&gt; &#xA;&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt; &#xA;&lt;h2&gt;Testimonials&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mike Bayer&lt;/strong&gt;, &lt;a href=&#34;https://www.sqlalchemy.org/&#34;&gt;author of &lt;code&gt;SQLAlchemy&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;I can&#39;t think of any single tool in my entire programming career that has given me a bigger productivity increase by its introduction. I can now do refactorings in about 1% of the keystrokes that it would have taken me previously when we had no way for code to format itself.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;, &lt;a href=&#34;https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips&#34;&gt;writer&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don&#39;t have to be.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href=&#34;https://www.attrs.org/&#34;&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of Twisted and CPython:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;An auto-formatter that doesn&#39;t suck is all I want for Xmas!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; core developer:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;At least the name is good.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href=&#34;http://python-requests.org/&#34;&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://readthedocs.org/projects/pipenv/&#34;&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Show your style&lt;/h2&gt; &#xA;&lt;p&gt;Use the badge in your project&#39;s README.md:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using the badge in README.rst:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg&#xA;    :target: https://github.com/psf/black&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Looks like this: &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Welcome! Happy to see you willing to make the project better. You can get started by reading this:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/latest/contributing/the_basics.html&#34;&gt;Contributing: The basics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also take a look at the rest of the contributing docs or talk with the developers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://black.readthedocs.io/en/latest/contributing/index.html&#34;&gt;Contributing documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/RtVdv86PrH&#34;&gt;Chat on Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Change log&lt;/h2&gt; &#xA;&lt;p&gt;The log has become rather long. It moved to its own file.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://black.readthedocs.io/en/latest/change_log.html&#34;&gt;CHANGES&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p&gt;The author list is quite long nowadays, so it lives in its own file.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/psf/black/main/AUTHORS.md&#34;&gt;AUTHORS.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;Everyone participating in the &lt;em&gt;Black&lt;/em&gt; project, and in particular in the issue tracker, pull requests, and social media activity, is expected to treat other people with respect and more generally to follow the guidelines articulated in the &lt;a href=&#34;https://www.python.org/psf/codeofconduct/&#34;&gt;Python Community Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, humor is encouraged. In fact, basic familiarity with Monty Python&#39;s Flying Circus is expected. We are not savages.&lt;/p&gt; &#xA;&lt;p&gt;And if you &lt;em&gt;really&lt;/em&gt; need to slap somebody, do it with a fish while dancing.&lt;/p&gt;</summary>
  </entry>
</feed>