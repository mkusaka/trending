<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-16T01:31:38Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lxgr-linux/pokete</title>
    <updated>2022-06-16T01:31:38Z</updated>
    <id>tag:github.com,2022-06-16:/lxgr-linux/pokete</id>
    <link href="https://github.com/lxgr-linux/pokete" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A terminal based Pokemon like game&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/lxgr-linux/pokete/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/lxgr-linux/pokete/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;Wiki&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lxgr-linux/pokete/actions/workflows/main_validate.yml&#34;&gt;&lt;img src=&#34;https://github.com/lxgr-linux/pokete/actions/workflows/main_validate.yml/badge.svg?sanitize=true&#34; alt=&#34;Code-Validation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lxgr-linux/pokete/actions/workflows/documentation.yml&#34;&gt;&lt;img src=&#34;https://github.com/lxgr-linux/pokete/actions/workflows/documentation.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub-Pages Build&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;img src=&#34;https://img.shields.io/github/pipenv/locked/python-version/lxgr-linux/pokete&#34; alt=&#34;Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/lxgr-linux/pokete&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/aur/version/pokete-git&#34; alt=&#34;AUR version&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://img.shields.io/tokei/lines/github/lxgr-linux/pokete&#34; alt=&#34;Total Lines of Code&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/lxgr-linux/pokete&#34; alt=&#34;Open Issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/lxgr-linux/pokete&#34; alt=&#34;Open pull requests&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/commit-activity/m/lxgr-linux/pokete&#34; alt=&#34;commit activity&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/commits-since/lxgr-linux/pokete/latest/master?include_prereleases&#34; alt=&#34;commits since last release&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/lxgr-linux/pokete&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Pokete -- Grey Edition&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lxgr-linux/pokete/master/assets/ss/ss01.png&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lxgr-linux/pokete/master/assets/pics.md&#34;&gt;See more example pics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;p&gt;Pokete is a small terminal based game in the style of a very popular and old game by Gamefreak.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;For Linux just do this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# pip install scrap_engine&#xA;$ git clone https://github.com/lxgr-linux/pokete.git&#xA;$ ./pokete/pokete.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also install it from the AUR:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ buildaur -S pokete-git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can just run the AppImage from tge release page.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: In that case you first have to create the &lt;code&gt;~/.cache/pokete/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;For Windows and OSX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/lxgr-linux/pokete.git&#xA;pip install scrap_engine&#xA;pip install pynput&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run just execute &lt;code&gt;pokete.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The game can be run normaly by not supplying any options. For non gameplay related usage see &lt;code&gt;--help&lt;/code&gt;. Try it out &lt;a href=&#34;https://replit.com/@lxgr-linux/pokete&#34;&gt;online&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to play?&lt;/h2&gt; &#xA;&lt;p&gt;Imagine you&#39;re a Pokete-Trainer and your goal is it to run around in the world and catch/train as many Poketes as possible and to get the best trainer.&lt;/p&gt; &#xA;&lt;p&gt;First of all you get a starter Pokete (Steini), that you can use to fight battles with other Poketes. The controls are w a s d to walk around.&lt;/p&gt; &#xA;&lt;p&gt;When entering the high grass (;), you may be attacked by a wild Pokete. By pressing 1 you can choose between the attacks (as long their AP is over 0) your Pokete has, and by pressing the according number, or navigating with the &#34;*&#34;-cursor to the attack and pressing enter. The wild Pokete will fight back, you can kill it and gain XP to level up your Pokete or you can catch it to have it fight for you. To catch a Pokete you have to first weaken the enemy and then throw a Poketeball. And with a bit luck you can catch it. Pressing the &#34;1&#34; key you can take a look at your current deck, see the detailed information of your Pokete and your attacks or rearrange them. Changes will only be saved by quitting the game using the exit function.&lt;/p&gt; &#xA;&lt;p&gt;Since you&#39;re a Pokete-Trainer, you can also fight against other trainers (the other &#34;a&#34; in the middle of the landscape). He will start a fight with you when you get close enough to him. You can not escape from such a trainer fight, you either have to win, or lose. These trainer fights give double the XP.&lt;/p&gt; &#xA;&lt;p&gt;When one of your Poketes die, or is too weak, you can heal it by going into the house (Pokete-Center), talk the the person there and choose the healing option. Here you can also take a look at all of your Poketes, and not just the six in your team. The ones marked with an &#34;o&#34; are the ones in your deck.&lt;/p&gt; &#xA;&lt;p&gt;By pressing &#34;e&#34; you can get into a menu where player name, and later other settings, can be changed.&lt;/p&gt; &#xA;&lt;p&gt;The red balls all over the map are Poketeballs. You&#39;ll need these to catch Poketes. Stepping on such a ball will add it to your inventory.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/lxgr-linux/pokete/master/HowToPlay.md&#34;&gt;How to play&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Game depth&lt;/h2&gt; &#xA;&lt;p&gt;Not only are there Poketes that are stronger than others, but also Poketes with different types, which are effective against some types and ineffective against others.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Effective against&lt;/th&gt; &#xA;   &lt;th&gt;Ineffective against&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Normal&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stone&lt;/td&gt; &#xA;   &lt;td&gt;Flying, Fire&lt;/td&gt; &#xA;   &lt;td&gt;Plant&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Plant&lt;/td&gt; &#xA;   &lt;td&gt;Stone, Ground, Water&lt;/td&gt; &#xA;   &lt;td&gt;Fire, Ice&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Water&lt;/td&gt; &#xA;   &lt;td&gt;Stone, Flying, Fire&lt;/td&gt; &#xA;   &lt;td&gt;Plant, Ice&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fire&lt;/td&gt; &#xA;   &lt;td&gt;Flying, Plant, Undead, Ice&lt;/td&gt; &#xA;   &lt;td&gt;Stone, Water&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ground&lt;/td&gt; &#xA;   &lt;td&gt;Normal&lt;/td&gt; &#xA;   &lt;td&gt;Flying&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Electro&lt;/td&gt; &#xA;   &lt;td&gt;Stone, Flying&lt;/td&gt; &#xA;   &lt;td&gt;Ground&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flying&lt;/td&gt; &#xA;   &lt;td&gt;Plant&lt;/td&gt; &#xA;   &lt;td&gt;Stone&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Undead&lt;/td&gt; &#xA;   &lt;td&gt;Normal, Ground, Plant, Water&lt;/td&gt; &#xA;   &lt;td&gt;Fire&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ice&lt;/td&gt; &#xA;   &lt;td&gt;Water, Plant&lt;/td&gt; &#xA;   &lt;td&gt;Fire&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For additional information you can see &lt;a href=&#34;https://raw.githubusercontent.com/lxgr-linux/pokete/master/wiki.md&#34;&gt;wiki&lt;/a&gt; or &lt;a href=&#34;https://lxgr-linux.github.io/pokete/wiki-multi&#34;&gt;the multi-page wiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Mods&lt;/h2&gt; &#xA;&lt;p&gt;Mods can be written to extend Pokete. To load a mod, the mod has to be placed in &lt;code&gt;mods&lt;/code&gt; and mods have to be enabled in the menu. For an example mod see &lt;a href=&#34;https://raw.githubusercontent.com/lxgr-linux/pokete/master/mods/example.py&#34;&gt;example.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In conversations you can very easily skip the text printing by pressing any key&lt;/li&gt; &#xA; &lt;li&gt;When you want to see the next text in a conversation: also just press any key&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t play on full-screen; the game then starts to be overseeable&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t be offended by the other trainers; they may swear at you&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add a wizard to set name and choose starter Pokete at the start&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add More maps&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add types for attacks and Poketes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add evolving&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add more than one Pokete for trainers&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Coloured Poketes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A store to buy Poketeballs&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add potions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Intro&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add trading&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Poketedex&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Effects&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add colour codes for types&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Pokete depends on python3 and the scrap_engine module. On windows pynput has to be installed too.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lxgr-linux.github.io/pokete/doc/pokete_classes/index.html&#34;&gt;Documentation for pokete_classes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lxgr-linux.github.io/pokete/doc/pokete_data/index.html&#34;&gt;Documentation for pokete_data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lxgr-linux.github.io/pokete/doc/gen_wiki.html&#34; title=&#34;gen_wiki.py&#34;&gt;Documentation for the gen-wiki file&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lxgr-linux.github.io/pokete/doc/prepare_pages.html&#34; title=&#34;prepare_pages.py&#34;&gt;Documentation for the prepare_pages file&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lxgr-linux.github.io/pokete/doc/pokete_general_use_fns.html&#34; title=&#34;pokete_general_use_fns.py&#34;&gt;Documentation for the pokete_general_use_fns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lxgr-linux.github.io/pokete/doc/pokete.html&#34; title=&#34;pokete.py&#34;&gt;Documentation for the main file &#34;pokete.py&#34;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;p&gt;For release information see &lt;a href=&#34;https://raw.githubusercontent.com/lxgr-linux/pokete/master/Changelog.md&#34;&gt;Changelog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to contribute what ever you want to this game. New Pokete contributions are especially welcome, those are located in /pokete_data/poketes.py&lt;/p&gt; &#xA;&lt;p&gt;To see how to add more poketes/types/attacks to the game, see &lt;a href=&#34;https://raw.githubusercontent.com/lxgr-linux/pokete/master/DevGuide.md&#34;&gt;the DevGuide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After adding new Poketes and/or Attacks you may want to run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ./gen_wiki.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to regenerate the wiki and adding them to it.&lt;/p&gt; &#xA;&lt;h2&gt;Trouble shooting&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re experiencing problems on Japanese systems take a look at &lt;a href=&#34;https://gist.github.com/z80oolong/c7523367b798bdda094f859342f4c8be&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>coqui-ai/TTS</title>
    <updated>2022-06-16T01:31:38Z</updated>
    <id>tag:github.com,2022-06-16:/coqui-ai/TTS</id>
    <link href="https://github.com/coqui-ai/TTS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🐸💬 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png&#34; height=&#34;56&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;🐸TTS is a library for advanced Text-to-Speech generation. It&#39;s built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality. 🐸TTS comes with pretrained models, tools for measuring dataset quality and already used in &lt;strong&gt;20+ languages&lt;/strong&gt; for products and research projects.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/coqui-ai/TTS?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/coqui-ai/TTS.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MPL-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/TTS&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/TTS.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/master/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667&#34; alt=&#34;Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/tts&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/tts&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/265612440&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/265612440.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;img src=&#34;https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests.yml/badge.svg?sanitize=true&#34; alt=&#34;GithubActions&#34;&gt; &lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/tts/badge/?version=latest&amp;amp;style=plastic&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;📰 &lt;a href=&#34;https://coqui.ai/?subscription=true&#34;&gt;&lt;strong&gt;Subscribe to 🐸Coqui.ai Newsletter&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;📢 &lt;a href=&#34;https://erogol.github.io/ddc-samples/&#34;&gt;English Voice Samples&lt;/a&gt; and &lt;a href=&#34;https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2&#34;&gt;SoundCloud playlist&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;📄 &lt;a href=&#34;https://github.com/erogol/TTS-papers&#34;&gt;Text-to-Speech paper collection&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2&#34;&gt; &#xA;&lt;h2&gt;💬 Where to ask questions&lt;/h2&gt; &#xA;&lt;p&gt;Please use our dedicated channels for questions and discussion. Help is much more valuable if it&#39;s shared publicly so that more people can benefit from it.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Platforms&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🚨 &lt;strong&gt;Bug Reports&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🎁 &lt;strong&gt;Feature Requests &amp;amp; Ideas&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/tts/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;👩‍💻 &lt;strong&gt;Usage Questions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;Github Discussions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🗯 &lt;strong&gt;General Discussion&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/discussions&#34;&gt;Github Discussions&lt;/a&gt; or &lt;a href=&#34;https://gitter.im/coqui-ai/TTS?utm_source=share-link&amp;amp;utm_medium=link&amp;amp;utm_campaign=share-link&#34;&gt;Gitter Room&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🔗 Links and Resources&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;💼 &lt;strong&gt;Documentation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tts.readthedocs.io/en/latest/&#34;&gt;ReadTheDocs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;💾 &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/tree/dev#install-tts&#34;&gt;TTS/README.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;👩‍💻 &lt;strong&gt;Contributing&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;📌 &lt;strong&gt;Road Map&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/issues/378&#34;&gt;Main Development Plans&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🚀 &lt;strong&gt;Released Models&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/coqui-ai/TTS/releases&#34;&gt;TTS Releases&lt;/a&gt; and &lt;a href=&#34;https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models&#34;&gt;Experimental Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🥇 TTS Performance&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Underlined &#34;TTS*&#34; and &#34;Judy*&#34; are 🐸TTS models&lt;/p&gt; &#xA;&lt;!-- [Details...](https://github.com/coqui-ai/TTS/wiki/Mean-Opinion-Score-Results) --&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High-performance Deep Learning models for Text2Speech tasks. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).&lt;/li&gt; &#xA;   &lt;li&gt;Speaker Encoder to compute speaker embeddings efficiently.&lt;/li&gt; &#xA;   &lt;li&gt;Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fast and efficient model training.&lt;/li&gt; &#xA; &lt;li&gt;Detailed training logs on the terminal and Tensorboard.&lt;/li&gt; &#xA; &lt;li&gt;Support for Multi-speaker TTS.&lt;/li&gt; &#xA; &lt;li&gt;Efficient, flexible, lightweight but feature complete &lt;code&gt;Trainer API&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Released and ready-to-use models.&lt;/li&gt; &#xA; &lt;li&gt;Tools to curate Text2Speech datasets under&lt;code&gt;dataset_analysis&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Utilities to use and test your models.&lt;/li&gt; &#xA; &lt;li&gt;Modular (but not too much) code base enabling easy implementation of new ideas.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Implemented Models&lt;/h2&gt; &#xA;&lt;h3&gt;Text-to-Spectrogram&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tacotron: &lt;a href=&#34;https://arxiv.org/abs/1703.10135&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tacotron2: &lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Glow-TTS: &lt;a href=&#34;https://arxiv.org/abs/2005.11129&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Speedy-Speech: &lt;a href=&#34;https://arxiv.org/abs/2008.03802&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Align-TTS: &lt;a href=&#34;https://arxiv.org/abs/2003.01950&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastPitch: &lt;a href=&#34;https://arxiv.org/pdf/2006.06873.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastSpeech: &lt;a href=&#34;https://arxiv.org/abs/1905.09263&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;End-to-End Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VITS: &lt;a href=&#34;https://arxiv.org/pdf/2106.06103&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Attention Methods&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Guided Attention: &lt;a href=&#34;https://arxiv.org/abs/1710.08969&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Forward Backward Decoding: &lt;a href=&#34;https://arxiv.org/abs/1907.09006&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Graves Attention: &lt;a href=&#34;https://arxiv.org/abs/1910.10288&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Double Decoder Consistency: &lt;a href=&#34;https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dynamic Convolutional Attention: &lt;a href=&#34;https://arxiv.org/pdf/1910.10288.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alignment Network: &lt;a href=&#34;https://arxiv.org/abs/2108.10447&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Speaker Encoder&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GE2E: &lt;a href=&#34;https://arxiv.org/abs/1710.10467&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Angular Loss: &lt;a href=&#34;https://arxiv.org/pdf/2003.11982.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vocoders&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MelGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.06711&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MultiBandMelGAN: &lt;a href=&#34;https://arxiv.org/abs/2005.05106&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ParallelWaveGAN: &lt;a href=&#34;https://arxiv.org/abs/1910.11480&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GAN-TTS discriminators: &lt;a href=&#34;https://arxiv.org/abs/1909.11646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveRNN: &lt;a href=&#34;https://github.com/fatchord/WaveRNN/&#34;&gt;origin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WaveGrad: &lt;a href=&#34;https://arxiv.org/abs/2009.00713&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;HiFiGAN: &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;UnivNet: &lt;a href=&#34;https://arxiv.org/abs/2106.07889&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also help us implement more models.&lt;/p&gt; &#xA;&lt;h2&gt;Install TTS&lt;/h2&gt; &#xA;&lt;p&gt;🐸TTS is tested on Ubuntu 18.04 with &lt;strong&gt;python &amp;gt;= 3.7, &amp;lt; 3.11.&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are only interested in &lt;a href=&#34;https://tts.readthedocs.io/en/latest/inference.html&#34;&gt;synthesizing speech&lt;/a&gt; with the released 🐸TTS models, installing from PyPI is the easiest option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install TTS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you plan to code or train models, clone 🐸TTS and install it locally.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/coqui-ai/TTS&#xA;pip install -e .[all,dev,notebooks]  # Select the relevant extras&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Ubuntu (Debian), you can also run following commands for installation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a diffent OS.&#xA;$ make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on Windows, 👑@GuyPaddock wrote installation instructions &lt;a href=&#34;https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Use TTS&lt;/h2&gt; &#xA;&lt;h3&gt;Single Speaker Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List provided models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --list_models&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run TTS with default models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run a TTS model with its default vocoder model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run with specific TTS and vocoder models from the list:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --vocoder_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34; --output_path&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS model (Using Griffin-Lim Vocoder):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own TTS and Vocoder models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --model_path path/to/config.json --config_path path/to/model.pth --out_path output/path/speech.wav&#xA;    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multi-speaker Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;List the available speakers and choose as &amp;lt;speaker_id&amp;gt; among them:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --list_speaker_idxs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the multi-speaker TTS model with the target speaker ID:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS.&#34; --out_path output/path/speech.wav --model_name &#34;&amp;lt;language&amp;gt;/&amp;lt;dataset&amp;gt;/&amp;lt;model_name&amp;gt;&#34;  --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run your own multi-speaker TTS model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ tts --text &#34;Text for TTS&#34; --out_path output/path/speech.wav --model_path path/to/config.json --config_path path/to/model.pth --speakers_file_path path/to/speaker.json --speaker_idx &amp;lt;speaker_id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Directory Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)&#xA;|- utils/           (common utilities.)&#xA;|- TTS&#xA;    |- bin/             (folder for all the executables.)&#xA;      |- train*.py                  (train your target model.)&#xA;      |- distribute.py              (train your TTS model using Multiple GPUs.)&#xA;      |- compute_statistics.py      (compute dataset statistics for normalization.)&#xA;      |- ...&#xA;    |- tts/             (text to speech models)&#xA;        |- layers/          (model layer definitions)&#xA;        |- models/          (model definitions)&#xA;        |- utils/           (model specific utilities.)&#xA;    |- speaker_encoder/ (Speaker Encoder models.)&#xA;        |- (same)&#xA;    |- vocoder/         (Vocoder models.)&#xA;        |- (same)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>neonbjb/tortoise-tts</title>
    <updated>2022-06-16T01:31:38Z</updated>
    <id>tag:github.com,2022-06-16:/neonbjb/tortoise-tts</id>
    <link href="https://github.com/neonbjb/tortoise-tts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A multi-voice TTS system trained with an emphasis on quality&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TorToiSe&lt;/h1&gt; &#xA;&lt;p&gt;Tortoise is a text-to-speech program built with the following priorities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Strong multi-voice capabilities.&lt;/li&gt; &#xA; &lt;li&gt;Highly realistic prosody and intonation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This repo contains all the code needed to run Tortoise TTS in inference mode.&lt;/p&gt; &#xA;&lt;h3&gt;Version history&lt;/h3&gt; &#xA;&lt;h4&gt;v2.4; 2022/5/17&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed CVVP model. Found that it does not, in fact, make an appreciable difference in the output.&lt;/li&gt; &#xA; &lt;li&gt;Add better debugging support; existing tools now spit out debug files which can be used to reproduce bad runs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.3; 2022/5/12&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New CLVP-large model for further improved decoding guidance.&lt;/li&gt; &#xA; &lt;li&gt;Improvements to read.py and do_tts.py (new options)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.2; 2022/5/5&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added several new voices from the training set.&lt;/li&gt; &#xA; &lt;li&gt;Automated redaction. Wrap the text you want to use to prompt the model but not be spoken in brackets.&lt;/li&gt; &#xA; &lt;li&gt;Bug fixes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2.1; 2022/5/2&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added ability to produce totally random voices.&lt;/li&gt; &#xA; &lt;li&gt;Added ability to download voice conditioning latent via a script, and then use a user-provided conditioning latent.&lt;/li&gt; &#xA; &lt;li&gt;Added ability to use your own pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;Refactored directory structures.&lt;/li&gt; &#xA; &lt;li&gt;Performance improvements &amp;amp; bug fixes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s in a name?&lt;/h2&gt; &#xA;&lt;p&gt;I&#39;m naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model is insanely slow. It leverages both an autoregressive decoder &lt;strong&gt;and&lt;/strong&gt; a diffusion decoder; both known for their low sampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.&lt;/p&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://nonint.com/static/tortoise_v2_examples.html&#34;&gt;this page&lt;/a&gt; for a large list of example outputs.&lt;/p&gt; &#xA;&lt;h2&gt;Usage guide&lt;/h2&gt; &#xA;&lt;h3&gt;Colab&lt;/h3&gt; &#xA;&lt;p&gt;Colab is the easiest way to try this out. I&#39;ve put together a notebook you can use here: &lt;a href=&#34;https://colab.research.google.com/drive/1wVVqUPqwiDBUVeWWOUNglpGhU3hg_cbR?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1wVVqUPqwiDBUVeWWOUNglpGhU3hg_cbR?usp=sharing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Local Installation&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use this on your own computer, you must have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;First, install pytorch using these instructions: &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;. On Windows, I &lt;strong&gt;highly&lt;/strong&gt; recommend using the Conda installation path. I have been told that if you do not do this, you will spend a lot of time chasing dependency problems.&lt;/p&gt; &#xA;&lt;p&gt;Next, install TorToiSe and it&#39;s dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/neonbjb/tortoise-tts.git&#xA;cd tortoise-tts&#xA;python setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are on windows, you will also need to install pysoundfile: &lt;code&gt;conda install -c conda-forge pysoundfile&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;do_tts.py&lt;/h3&gt; &#xA;&lt;p&gt;This script allows you to speak a single phrase with one or more voices.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/do_tts.py --text &#34;I&#39;m going to speak this&#34; --voice random --preset fast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;read.py&lt;/h3&gt; &#xA;&lt;p&gt;This script provides tools for reading large amounts of text.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tortoise/read.py --textfile &amp;lt;your text to be read&amp;gt; --voice random&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will break up the textfile into sentences, and then convert them to speech one at a time. It will output a series of spoken clips as they are generated. Once all the clips are generated, it will combine them into a single file and output that as well.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes Tortoise screws up an output. You can re-generate any bad clips by re-running &lt;code&gt;read.py&lt;/code&gt; with the --regenerate argument.&lt;/p&gt; &#xA;&lt;h3&gt;API&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise can be used programmatically, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]&#xA;tts = api.TextToSpeech()&#xA;pcm_audio = tts.tts_with_preset(&#34;your text here&#34;, reference_clips, preset=&#39;fast&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Voice customization guide&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips.&lt;/p&gt; &#xA;&lt;p&gt;These reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb.&lt;/p&gt; &#xA;&lt;h3&gt;Random voice&lt;/h3&gt; &#xA;&lt;p&gt;I&#39;ve included a feature which randomly generates a voice. These voices don&#39;t actually exist and will be random every time you run it. The results are quite fascinating and I recommend you play around with it!&lt;/p&gt; &#xA;&lt;p&gt;You can use the random voice by passing in &#39;random&#39; as the voice name. Tortoise will take care of the rest.&lt;/p&gt; &#xA;&lt;p&gt;For the those in the ML space: this is created by projecting a random vector onto the voice conditioning latent space.&lt;/p&gt; &#xA;&lt;h3&gt;Provided voices&lt;/h3&gt; &#xA;&lt;p&gt;This repo comes with several pre-packaged voices. Voices prepended with &#34;train_&#34; came from the training set and perform far better than the others. If your goal is high quality speech, I recommend you pick one of them. If you want to see what Tortoise can do for zero-shot mimicing, take a look at the others.&lt;/p&gt; &#xA;&lt;h3&gt;Adding a new voice&lt;/h3&gt; &#xA;&lt;p&gt;To add new voices to Tortoise, you will need to do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section.&lt;/li&gt; &#xA; &lt;li&gt;Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing.&lt;/li&gt; &#xA; &lt;li&gt;Save the clips as a WAV file with floating point format and a 22,050 sample rate.&lt;/li&gt; &#xA; &lt;li&gt;Create a subdirectory in voices/&lt;/li&gt; &#xA; &lt;li&gt;Put your clips in that subdirectory.&lt;/li&gt; &#xA; &lt;li&gt;Run tortoise utilities with --voice=&amp;lt;your_subdirectory_name&amp;gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Picking good reference clips&lt;/h3&gt; &#xA;&lt;p&gt;As mentioned above, your reference clips have a profound impact on the output of Tortoise. Following are some tips for picking good clips:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Avoid clips with background music, noise or reverb. These clips were removed from the training dataset. Tortoise is unlikely to do well with them.&lt;/li&gt; &#xA; &lt;li&gt;Avoid speeches. These generally have distortion caused by the amplification system.&lt;/li&gt; &#xA; &lt;li&gt;Avoid clips from phone calls.&lt;/li&gt; &#xA; &lt;li&gt;Avoid clips that have excessive stuttering, stammering or words like &#34;uh&#34; or &#34;like&#34; in them.&lt;/li&gt; &#xA; &lt;li&gt;Try to find clips that are spoken in such a way as you wish your output to sound like. For example, if you want to hear your target voice read an audiobook, try to find clips of them reading a book.&lt;/li&gt; &#xA; &lt;li&gt;The text being spoken in the clips does not matter, but diverse text does seem to perform better.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Generation settings&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise is primarily an autoregressive decoder model combined with a diffusion model. Both of these have a lot of knobs that can be turned that I&#39;ve abstracted away for the sake of ease of use. I did this by generating thousands of clips using various permutations of the settings and using a metric for voice realism and intelligibility to measure their effects. I&#39;ve set the defaults to the best overall settings I was able to find. For specific use-cases, it might be effective to play with these settings (and it&#39;s very likely that I missed something!)&lt;/p&gt; &#xA;&lt;p&gt;These settings are not available in the normal scripts packaged with Tortoise. They are available, however, in the API. See &lt;code&gt;api.tts&lt;/code&gt; for a full list.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt engineering&lt;/h3&gt; &#xA;&lt;p&gt;Some people have discovered that it is possible to do prompt engineering with Tortoise! For example, you can evoke emotion by including things like &#34;I am really sad,&#34; before your text. I&#39;ve built an automated redaction system that you can use to take advantage of this. It works by attempting to redact any text in the prompt surrounded by brackets. For example, the prompt &#34;[I am really sad,] Please feed me.&#34; will only speak the words &#34;Please feed me&#34; (with a sad tonality).&lt;/p&gt; &#xA;&lt;h3&gt;Playing with the voice latent&lt;/h3&gt; &#xA;&lt;p&gt;Tortoise ingests reference clips by feeding them through individually through a small submodel that produces a point latent, then taking the mean of all of the produced latents. The experimentation I have done has indicated that these point latents are quite expressive, affecting everything from tone to speaking rate to speech abnormalities.&lt;/p&gt; &#xA;&lt;p&gt;This lends itself to some neat tricks. For example, you can combine feed two different voices to tortoise and it will output what it thinks the &#34;average&#34; of those two voices sounds like.&lt;/p&gt; &#xA;&lt;h4&gt;Generating conditioning latents from voices&lt;/h4&gt; &#xA;&lt;p&gt;Use the script &lt;code&gt;get_conditioning_latents.py&lt;/code&gt; to extract conditioning latents for a voice you have installed. This script will dump the latents to a .pth pickle file. The file will contain a single tuple, (autoregressive_latent, diffusion_latent).&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, use the api.TextToSpeech.get_conditioning_latents() to fetch the latents.&lt;/p&gt; &#xA;&lt;h4&gt;Using raw conditioning latents to generate speech&lt;/h4&gt; &#xA;&lt;p&gt;After you&#39;ve played with them, you can use them to generate speech by creating a subdirectory in voices/ with a single &#34;.pth&#34; file containing the pickled conditioning latents as a tuple (autoregressive_latent, diffusion_latent).&lt;/p&gt; &#xA;&lt;h3&gt;Send me feedback!&lt;/h3&gt; &#xA;&lt;p&gt;Probabilistic models like Tortoise are best thought of as an &#34;augmented search&#34; - in this case, through the space of possible utterances of a specific string of text. The impact of community involvement in perusing these spaces (such as is being done with GPT-3 or CLIP) has really surprised me. If you find something neat that you can do with Tortoise that isn&#39;t documented here, please report it to me! I would be glad to publish it to this page.&lt;/p&gt; &#xA;&lt;h2&gt;Tortoise-detect&lt;/h2&gt; &#xA;&lt;p&gt;Out of concerns that this model might be misused, I&#39;ve built a classifier that tells the likelihood that an audio clip came from Tortoise.&lt;/p&gt; &#xA;&lt;p&gt;This classifier can be run on any computer, usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python tortoise/is_this_from_tortoise.py --clip=&amp;lt;path_to_suspicious_audio_file&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This model has 100% accuracy on the contents of the results/ and voices/ folders in this repo. Still, treat this classifier as a &#34;strong signal&#34;. Classifiers can be fooled and it is likewise not impossible for this classifier to exhibit false positives.&lt;/p&gt; &#xA;&lt;h2&gt;Model architecture&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise TTS is inspired by OpenAI&#39;s DALLE, applied to speech data and using a better decoder. It is made up of 5 separate models that work together. I&#39;ve assembled a write-up of the system architecture here: &lt;a href=&#34;https://nonint.com/2022/04/25/tortoise-architectural-design-doc/&#34;&gt;https://nonint.com/2022/04/25/tortoise-architectural-design-doc/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;These models were trained on my &#34;homelab&#34; server with 8 RTX 3090s over the course of several months. They were trained on a dataset consisting of ~50k hours of speech data, most of which was transcribed by &lt;a href=&#34;http://www.github.com/neonbjb/ocotillo&#34;&gt;ocotillo&lt;/a&gt;. Training was done on my own &lt;a href=&#34;https://github.com/neonbjb/DL-Art-School&#34;&gt;DLAS&lt;/a&gt; trainer.&lt;/p&gt; &#xA;&lt;p&gt;I currently do not have plans to release the training configurations or methodology. See the next section..&lt;/p&gt; &#xA;&lt;h2&gt;Ethical Considerations&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise v2 works considerably better than I had planned. When I began hearing some of the outputs of the last few versions, I began wondering whether or not I had an ethically unsound project on my hands. The ways in which a voice-cloning text-to-speech system could be misused are many. It doesn&#39;t take much creativity to think up how.&lt;/p&gt; &#xA;&lt;p&gt;After some thought, I have decided to go forward with releasing this. Following are the reasons for this choice:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It is primarily good at reading books and speaking poetry. Other forms of speech do not work well.&lt;/li&gt; &#xA; &lt;li&gt;It was trained on a dataset which does not have the voices of public figures. While it will attempt to mimic these voices if they are provided as references, it does not do so in such a way that most humans would be fooled.&lt;/li&gt; &#xA; &lt;li&gt;The above points could likely be resolved by scaling up the model and the dataset. For this reason, I am currently withholding details on how I trained the model, pending community feedback.&lt;/li&gt; &#xA; &lt;li&gt;I am releasing a separate classifier model which will tell you whether a given audio clip was generated by Tortoise or not. See &lt;code&gt;tortoise-detect&lt;/code&gt; above.&lt;/li&gt; &#xA; &lt;li&gt;If I, a tinkerer with a BS in computer science with a ~$15k computer can build this, then any motivated corporation or state can as well. I would prefer that it be in the open and everyone know the kinds of things ML can do.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Diversity&lt;/h3&gt; &#xA;&lt;p&gt;The diversity expressed by ML models is strongly tied to the datasets they were trained on.&lt;/p&gt; &#xA;&lt;p&gt;Tortoise was trained primarily on a dataset consisting of audiobooks. I made no effort to balance diversity in this dataset. For this reason, Tortoise will be particularly poor at generating the voices of minorities or of people who speak with strong accents.&lt;/p&gt; &#xA;&lt;h2&gt;Looking forward&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise v2 is about as good as I think I can do in the TTS world with the resources I have access to. A phenomenon that happens when training very large models is that as parameter count increases, the communication bandwidth needed to support distributed training of the model increases multiplicatively. On enterprise-grade hardware, this is not an issue: GPUs are attached together with exceptionally wide buses that can accommodate this bandwidth. I cannot afford enterprise hardware, though, so I am stuck.&lt;/p&gt; &#xA;&lt;p&gt;I want to mention here that I think Tortoise could do be a &lt;strong&gt;lot&lt;/strong&gt; better. The three major components of Tortoise are either vanilla Transformer Encoder stacks or Decoder stacks. Both of these types of models have a rich experimental history with scaling in the NLP realm. I see no reason to believe that the same is not true of TTS.&lt;/p&gt; &#xA;&lt;p&gt;The largest model in Tortoise v2 is considerably smaller than GPT-2 large. It is 20x smaller that the original DALLE transformer. Imagine what a TTS model trained at or near GPT-3 or DALLE scale could achieve.&lt;/p&gt; &#xA;&lt;p&gt;If you are an ethical organization with computational resources to spare interested in seeing what this model could do if properly scaled out, please reach out to me! I would love to collaborate on this.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project has garnered more praise than I expected. I am standing on the shoulders of giants, though, and I want to credit a few of the amazing folks in the community that have helped make this happen:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face, who wrote the GPT model and the generate API used by Tortoise, and who hosts the model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.12092.pdf&#34;&gt;Ramesh et al&lt;/a&gt; who authored the DALLE paper, which is the inspiration behind Tortoise.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2102.09672.pdf&#34;&gt;Nichol and Dhariwal&lt;/a&gt; who authored the (revision of) the code that drives the diffusion model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.07889.pdf&#34;&gt;Jang et al&lt;/a&gt; who developed and open-sourced univnet, the vocoder this repo uses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lucidrains&#34;&gt;lucidrains&lt;/a&gt; who writes awesome open source pytorch models, many of which are used here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/patrickvonplaten&#34;&gt;Patrick von Platen&lt;/a&gt; whose guides on setting up wav2vec were invaluable to building my dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notice&lt;/h2&gt; &#xA;&lt;p&gt;Tortoise was built entirely by me using my own hardware. My employer was not involved in any facet of Tortoise&#39;s development.&lt;/p&gt; &#xA;&lt;p&gt;If you use this repo or the ideas therein for your research, please cite it! A bibtex entree can be found in the right pane on GitHub.&lt;/p&gt;</summary>
  </entry>
</feed>