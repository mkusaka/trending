<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-01T01:37:10Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/nanoGPT</title>
    <updated>2023-01-01T01:37:10Z</updated>
    <id>tag:github.com,2023-01-01:/karpathy/nanoGPT</id>
    <link href="https://github.com/karpathy/nanoGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; &#xA;&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It&#39;s a re-write of &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt;, which I think became too complicated, and which I am hesitant to now touch. Still under active development, currently working to reproduce GPT-2 on OpenWebText dataset. The code itself aims by design to be plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#39;s it.&lt;/p&gt; &#xA;&lt;h2&gt;install&lt;/h2&gt; &#xA;&lt;p&gt;Dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org&#34;&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install tiktoken&lt;/code&gt; for OpenAI&#39;s fast bpe code &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install tqdm&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;usage&lt;/h2&gt; &#xA;&lt;p&gt;To render a dataset we first tokenize some documents into one simple long 1D array of indices. E.g. for OpenWebText see:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ cd data/openwebtext&#xA;$ python prepare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download and tokenize the &lt;a href=&#34;https://huggingface.co/datasets/openwebtext&#34;&gt;OpenWebText&lt;/a&gt; dataset. This will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#39;re ready to kick off training. The training script currently by default tries to reproduce the smallest GPT-2 released by OpenAI, i.e. the 124M version of GPT-2. We can demo train as follows on a single device, though I encourage you to read the code and see all of the settings and paths up top in the file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train using PyTorch Distributed Data Parallel (DDP) run the script with torchrun. For example to train on a node with 4 GPUs run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ torchrun --standalone --nproc_per_node=4 train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To my knowledge, running this with the current script with the GPT-2 hyperparameters should reproduce the GPT-2 result, provided that OpenWebText ~= WebText. I&#39;d like to make the code more efficient before attempting to go there. Once some checkpoints are written to the output directory (e.g. &lt;code&gt;./out&lt;/code&gt; by default), we can sample from the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python sample.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training on 1 A100 40GB GPU overnight currently gets loss ~3.74, training on 4 gets ~3.60. Random chance at init is -ln(1/50257) = 10.82. Which brings us to baselines:&lt;/p&gt; &#xA;&lt;h2&gt;baselines&lt;/h2&gt; &#xA;&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python train.py eval_gpt2&#xA;$ python train.py eval_gpt2_medium&#xA;$ python train.py eval_gpt2_large&#xA;$ python train.py eval_gpt2_xl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;train loss&lt;/th&gt; &#xA;   &lt;th&gt;val loss&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2&lt;/td&gt; &#xA;   &lt;td&gt;124M&lt;/td&gt; &#xA;   &lt;td&gt;3.11&lt;/td&gt; &#xA;   &lt;td&gt;3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-medium&lt;/td&gt; &#xA;   &lt;td&gt;350M&lt;/td&gt; &#xA;   &lt;td&gt;2.85&lt;/td&gt; &#xA;   &lt;td&gt;2.84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-large&lt;/td&gt; &#xA;   &lt;td&gt;774M&lt;/td&gt; &#xA;   &lt;td&gt;2.66&lt;/td&gt; &#xA;   &lt;td&gt;2.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt2-xl&lt;/td&gt; &#xA;   &lt;td&gt;1558M&lt;/td&gt; &#xA;   &lt;td&gt;2.56&lt;/td&gt; &#xA;   &lt;td&gt;2.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;I briefly tried finetuning gpt2 a bit more on our OWT and didn&#39;t notice dramatic improvements, suggesting that OWT is not much much different from WT in terms of the data distribution, but this needs a bit more thorough attempt once the code is in a better place.&lt;/p&gt; &#xA;&lt;h2&gt;benchmarking&lt;/h2&gt; &#xA;&lt;p&gt;For model benchmarking &lt;code&gt;bench.py&lt;/code&gt; might be useful. It&#39;s identical what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; &#xA;&lt;h2&gt;efficiency notes&lt;/h2&gt; &#xA;&lt;p&gt;Code by default now uses &lt;a href=&#34;https://pytorch.org/get-started/pytorch-2.0/&#34;&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; &#xA;&lt;h2&gt;todos&lt;/h2&gt; &#xA;&lt;p&gt;A few that I&#39;m aware of, other than the ones mentioned in code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Additional optimizations to the running time&lt;/li&gt; &#xA; &lt;li&gt;Report and track other metrics e.g. PPL&lt;/li&gt; &#xA; &lt;li&gt;Eval zero-shot perplexities on PTB, WikiText, other related benchmarks&lt;/li&gt; &#xA; &lt;li&gt;Current initialization (PyTorch default) departs from GPT-2. In a very quick experiment I found it to be superior to the one suggested in the papers, but that can&#39;t be right&lt;/li&gt; &#xA; &lt;li&gt;Currently fp16 is much faster than bf16. Potentially revert back to using fp16 and re-introduce the gradient scaler?&lt;/li&gt; &#xA; &lt;li&gt;Add some finetuning dataset and guide on some dataset for demonstration.&lt;/li&gt; &#xA; &lt;li&gt;Reproduce GPT-2 results. It was estimated ~3 years ago that the training cost of 1.5B model was ~$50K&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>openai/gym</title>
    <updated>2023-01-01T01:37:10Z</updated>
    <id>tag:github.com,2023-01-01:/openai/gym</id>
    <link href="https://github.com/openai/gym" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A toolkit for developing and comparing reinforcement learning algorithms.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://pre-commit.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Important notice&lt;/h2&gt; &#xA;&lt;h3&gt;The team that has been maintaining Gym since 2021 has moved all future development to &lt;a href=&#34;https://github.com/Farama-Foundation/Gymnasium&#34;&gt;Gymnasium&lt;/a&gt;, a drop in replacement for Gym (import gymnasium as gym), and this repo isn&#39;t planned to receive any future updates. Please consider switching over to Gymnasium as you&#39;re able to do so.&lt;/h3&gt; &#xA;&lt;h2&gt;Gym&lt;/h2&gt; &#xA;&lt;p&gt;Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym&#39;s API has become the field standard for doing this.&lt;/p&gt; &#xA;&lt;p&gt;Gym documentation website is at &lt;a href=&#34;https://www.gymlibrary.dev/&#34;&gt;https://www.gymlibrary.dev/&lt;/a&gt;, and you can propose fixes and changes to it &lt;a href=&#34;https://github.com/Farama-Foundation/gym-docs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Gym also has a discord server for development purposes that you can join here: &lt;a href=&#34;https://discord.gg/nHg2JRN489&#34;&gt;https://discord.gg/nHg2JRN489&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install the base Gym library, use &lt;code&gt;pip install gym&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This does not include dependencies for all families of environments (there&#39;s a massive number, and some can be problematic to install on certain systems). You can install these dependencies for one family like &lt;code&gt;pip install gym[atari]&lt;/code&gt; or use &lt;code&gt;pip install gym[all]&lt;/code&gt; to install all dependencies.&lt;/p&gt; &#xA;&lt;p&gt;We support Python 3.7, 3.8, 3.9 and 3.10 on Linux and macOS. We will accept PRs related to Windows, but do not officially support it.&lt;/p&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;The Gym API&#39;s API models environments as simple Python &lt;code&gt;env&lt;/code&gt; classes. Creating environment instances and interacting with them is very simple- here&#39;s an example using the &#34;CartPole-v1&#34; environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym&#xA;env = gym.make(&#34;CartPole-v1&#34;)&#xA;observation, info = env.reset(seed=42)&#xA;&#xA;for _ in range(1000):&#xA;    action = env.action_space.sample()&#xA;    observation, reward, terminated, truncated, info = env.step(action)&#xA;&#xA;    if terminated or truncated:&#xA;        observation, info = env.reset()&#xA;env.close()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notable Related Libraries&lt;/h2&gt; &#xA;&lt;p&gt;Please note that this is an incomplete list, and just includes libraries that the maintainers most commonly point newcommers to when asked for recommendations.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vwxyzjn/cleanrl&#34;&gt;CleanRL&lt;/a&gt; is a learning library based on the Gym API. It is designed to cater to newer people in the field and provides very good reference implementations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thu-ml/tianshou&#34;&gt;Tianshou&lt;/a&gt; is a learning library that&#39;s geared towards very experienced users and is design to allow for ease in complex algorithm modifications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.ray.io/en/latest/rllib/index.html&#34;&gt;RLlib&lt;/a&gt; is a learning library that allows for distributed training and inferencing and supports an extraordinarily large number of features throughout the reinforcement learning space.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Farama-Foundation/PettingZoo&#34;&gt;PettingZoo&lt;/a&gt; is like Gym, but for environments with multiple agents.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Environment Versioning&lt;/h2&gt; &#xA;&lt;p&gt;Gym keeps strict versioning for reproducibility reasons. All environments end in a suffix like &#34;_v0&#34;. When changes are made to environments that might impact learning results, the number is increased by one to prevent potential confusion.&lt;/p&gt; &#xA;&lt;h2&gt;MuJoCo Environments&lt;/h2&gt; &#xA;&lt;p&gt;The latest &#34;_v4&#34; and future versions of the MuJoCo environments will no longer depend on &lt;code&gt;mujoco-py&lt;/code&gt;. Instead &lt;code&gt;mujoco&lt;/code&gt; will be the required dependency for future gym MuJoCo environment versions. Old gym MuJoCo environment versions that depend on &lt;code&gt;mujoco-py&lt;/code&gt; will still be kept but unmaintained. To install the dependencies for the latest gym MuJoCo environments use &lt;code&gt;pip install gym[mujoco]&lt;/code&gt;. Dependencies for old MuJoCo environments can still be installed by &lt;code&gt;pip install gym[mujoco_py]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;A whitepaper from when Gym just came out is available &lt;a href=&#34;https://arxiv.org/pdf/1606.01540&#34;&gt;https://arxiv.org/pdf/1606.01540&lt;/a&gt;, and can be cited with the following bibtex entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{1606.01540,&#xA;  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},&#xA;  Title = {OpenAI Gym},&#xA;  Year = {2016},&#xA;  Eprint = {arXiv:1606.01540},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Release Notes&lt;/h2&gt; &#xA;&lt;p&gt;There used to be release notes for all the new Gym versions here. New release notes are being moved to &lt;a href=&#34;https://github.com/openai/gym/releases&#34;&gt;releases page&lt;/a&gt; on GitHub, like most other libraries do. Old notes can be viewed &lt;a href=&#34;https://github.com/openai/gym/raw/31be35ecd460f670f0c4b653a14c9996b7facc6c/README.rst&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jerryjliu/gpt_index</title>
    <updated>2023-01-01T01:37:10Z</updated>
    <id>tag:github.com,2023-01-01:/jerryjliu/gpt_index</id>
    <link href="https://github.com/jerryjliu/gpt_index" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An index created by GPT to organize external information and answer queries!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üóÇÔ∏è Ô∏èGPT Index&lt;/h1&gt; &#xA;&lt;p&gt;GPT Index is a project consisting of a set of &lt;em&gt;data structures&lt;/em&gt; that are created using LLMs and can be traversed using LLMs in order to answer queries.&lt;/p&gt; &#xA;&lt;p&gt;PyPi: &lt;a href=&#34;https://pypi.org/project/gpt-index/&#34;&gt;https://pypi.org/project/gpt-index/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Documentation: &lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/&#34;&gt;https://gpt-index.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!&lt;/p&gt; &#xA;&lt;h4&gt;Context&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLMs are a phenomenonal piece of technology for knowledge generation and reasoning.&lt;/li&gt; &#xA; &lt;li&gt;A big limitation of LLMs is context size (e.g. OpenAI&#39;s &lt;code&gt;davinci&lt;/code&gt; model for GPT-3 has a &lt;a href=&#34;https://openai.com/api/pricing/&#34;&gt;limit&lt;/a&gt; of 4096 tokens. Large, but not infinite).&lt;/li&gt; &#xA; &lt;li&gt;The ability to feed &#34;knowledge&#34; to LLMs is restricted to this limited prompt size and model weights.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Thought&lt;/strong&gt;: What if LLMs can have access to potentially a much larger database of knowledge without retraining/finetuning?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Proposed Solution&lt;/h4&gt; &#xA;&lt;p&gt;That&#39;s where the &lt;strong&gt;GPT Index&lt;/strong&gt; comes in. GPT Index is a simple, flexible interface between your external data and LLMs. It resolves the following pain points:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provides simple data structures to resolve prompt size limitations.&lt;/li&gt; &#xA; &lt;li&gt;Offers data connectors to your external data sources.&lt;/li&gt; &#xA; &lt;li&gt;Offers you a comprehensive toolset trading off cost and performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;At the core of GPT Index is a &lt;strong&gt;data structure&lt;/strong&gt;. Instead of relying on world knowledge encoded in the model weights, a GPT Index data structure does the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uses a pre-trained LLM primarily for &lt;em&gt;reasoning&lt;/em&gt;/&lt;em&gt;summarization&lt;/em&gt; instead of prior knowledge.&lt;/li&gt; &#xA; &lt;li&gt;Takes as input a large corpus of text data and build a structured index over it (using an LLM or heuristics).&lt;/li&gt; &#xA; &lt;li&gt;Allow users to &lt;em&gt;query&lt;/em&gt; the index in order to synthesize an answer to the question - this requires both &lt;em&gt;traversal&lt;/em&gt; of the index as well as a synthesis of the answer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìÑ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Full documentation can be found here: &lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/&#34;&gt;https://gpt-index.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please check it out for the most up-to-date tutorials, how-to guides, references, and other resources!&lt;/p&gt; &#xA;&lt;h2&gt;üíª Example Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gpt-index&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Examples are in the &lt;code&gt;examples&lt;/code&gt; folder. Indices are in the &lt;code&gt;indices&lt;/code&gt; folder (see list of indices below).&lt;/p&gt; &#xA;&lt;p&gt;To build a tree index do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from gpt_index import GPTTreeIndex, SimpleDirectoryReader&#xA;documents = SimpleDirectoryReader(&#39;data&#39;).load_data()&#xA;index = GPTTreeIndex(documents)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To save to disk and load from disk, do&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# save to disk&#xA;index.save_to_disk(&#39;index.json&#39;)&#xA;# load from disk&#xA;index = GPTTreeIndex.load_from_disk(&#39;index.json&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To query,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;index.query(&#34;&amp;lt;question_text&amp;gt;?&#34;, child_branch_factor=1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîß Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;The main third-party package requirements are &lt;code&gt;tiktoken&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, and &lt;code&gt;langchain&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All requirements should be contained within the &lt;code&gt;setup.py&lt;/code&gt; file. To run the package locally without building the wheel, simply do &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>