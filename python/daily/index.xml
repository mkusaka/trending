<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-05T01:34:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>KdaiP/StableTTS</title>
    <updated>2024-04-05T01:34:14Z</updated>
    <id>tag:github.com,2024-04-05:/KdaiP/StableTTS</id>
    <link href="https://github.com/KdaiP/StableTTS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Next-generation TTS model using flow-matching and DiT, inspired by Stable Diffusion 3&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;StableTTS&lt;/h1&gt; &#xA; &lt;p&gt;Next-generation TTS model using flow-matching and DiT, inspired by &lt;a href=&#34;https://stability.ai/news/stable-diffusion-3&#34;&gt;Stable Diffusion 3&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;As the first open-source TTS model that tried to combine flow-matching and DiT, StableTTS is a fast and lightweight TTS model for chinese and english speech generation. It has only 10M parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Work is in progress now. Pretrained models and detailed instructions will be released soon!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;For detailed inference instructions, please refer to &lt;code&gt;inference.ipynb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Setting up and training your model with StableTTS is straightforward. Follow these steps to get started:&lt;/p&gt; &#xA;&lt;h3&gt;Preparing Your Data&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Generate Text and Audio pairs&lt;/strong&gt;: Generate the text and audio pair filelist as &lt;code&gt;./filelists/example.txt&lt;/code&gt;. Some recipes of open-source datasets could be found in &lt;code&gt;./recipes&lt;/code&gt;. (Since we use reference encoder to capture speaker identity, there is no need for a speaker ID in multispeaker synthesis and training.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Preprocessing&lt;/strong&gt;: Adjust the &lt;code&gt;DataConfig&lt;/code&gt; in &lt;code&gt;preprocess.py&lt;/code&gt; to set your input and output paths, then run the script. This will process the audio and text according to your list, outputting a JSON file with paths to resampled audios, mel features, and phonemes. &lt;strong&gt;Note: Ensure to switch &lt;code&gt;chinese=False&lt;/code&gt; in &lt;code&gt;DataConfig&lt;/code&gt; for English text processing.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Start training&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Adjust Training Configuration&lt;/strong&gt;: In &lt;code&gt;config.py&lt;/code&gt;, modify &lt;code&gt;TrainConfig&lt;/code&gt; to set your file list path and adjust training parameters as needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the Training Process&lt;/strong&gt;: Launch &lt;code&gt;train.py&lt;/code&gt; to start training your model.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Experiment with Configurations&lt;/h3&gt; &#xA;&lt;p&gt;Feel free to explore and modify settings in &lt;code&gt;config.py&lt;/code&gt; to modify the hyperparameters!&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Details&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;StableTTS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;text to mel&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Model is currently in training...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vocos&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;mel to wav&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/KdaiP/StableTTS/blob/main/vocos.pt&#34;&gt;ğŸ¤—&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model structure&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KdaiP/StableTTS/main/figures/structure.jpg&#34; height=&#34;512&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We use the Diffusion Convolution Transformer block from &lt;a href=&#34;https://github.com/sh-lee-prml/HierSpeechpp&#34;&gt;Hierspeech++&lt;/a&gt;, which is a combination of original &lt;a href=&#34;https://github.com/sh-lee-prml/HierSpeechpp&#34;&gt;DiT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1905.09263.pdf&#34;&gt;FFT&lt;/a&gt;(Feed forward Transformer from fastspeech) for better prosody.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In flow-matching decoder, we add a &lt;a href=&#34;https://arxiv.org/abs/1709.07871&#34;&gt;FiLM layer&lt;/a&gt; before DiT block to condition timestep embedding into model.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;The development of our models heavily relies on insights and code from various projects. We express our heartfelt thanks to the creators of the following:&lt;/p&gt; &#xA;&lt;h3&gt;Direct Inspirations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/shivammehta25/Matcha-TTS&#34;&gt;Matcha TTS&lt;/a&gt;: Essential flow-matching code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS&#34;&gt;Grad TTS&lt;/a&gt;: Diffusion model structure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://stability.ai/news/stable-diffusion-3&#34;&gt;Stable Diffusion 3&lt;/a&gt;: Idea of combining flow-matching and DiT.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;Vits&lt;/a&gt;: Code style and MAS insights, DistributedBucketSampler.&lt;/p&gt; &#xA;&lt;h3&gt;Additional References:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/p0p4k/pflowtts_pytorch&#34;&gt;plowtts-pytorch&lt;/a&gt;: codes of MAS in training&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VITS-fast-fine-tuning&#34;&gt;Bert-VITS2&lt;/a&gt; : numba version of MAS and modern pytorch codes of Vits&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fishaudio/fish-speech&#34;&gt;fish-speech&lt;/a&gt;: dataclass usage and mel-spectrogram transforms using torchaudio&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;gpt-sovits&lt;/a&gt;: melstyle encoder for voice clone&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openvpi/DiffSinger&#34;&gt;diffsinger&lt;/a&gt;: chinese three section phoneme scheme for chinese g2p&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Provide finetuning instructions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Japanese language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; User friendly preprocess and inference script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance documentation and citations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add chinese version of readme.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Any organization or individual is prohibited from using any technology in this repo to generate or edit someone&#39;s speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>X-PLUG/mPLUG-DocOwl</title>
    <updated>2024-04-05T01:34:14Z</updated>
    <id>tag:github.com,2024-04-05:/X-PLUG/mPLUG-DocOwl</id>
    <link href="https://github.com/X-PLUG/mPLUG-DocOwl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/mPLUG_new1.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;The Powerful Multi-modal LLM Family &lt;p&gt;for OCR-free Document Understanding&lt;/p&gt;&lt;/h2&gt;&#xA; &lt;h2&gt; &lt;strong&gt;Alibaba Group&lt;/strong&gt;&lt;p&gt;&lt;/p&gt; &lt;/h2&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ”¥ğŸ”¥ [2024.4.3] We build demos of DocOwl1.5 on both &lt;a href=&#34;https://modelscope.cn/studios/iic/mPLUG-DocOwl/&#34;&gt;ModelScope&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/modelscope.png&#34; width=&#34;20&#34;&gt; and &lt;a href=&#34;https://huggingface.co/spaces/mPLUG/DocOwl&#34;&gt;HuggingFace&lt;/a&gt; ğŸ¤—, supported by the DocOwl1.5-Omni.&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”¥[2024.3.28] We release the training data (DocStruct4M, DocDownstream-1.0, DocReason25K), codes and models (DocOwl1.5-stage1, DocOwl1.5, DocOwl1.5-Chat, DocOwl1.5-Omni) of &lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl1.5/&#34;&gt;mPLUG-DocOwl 1.5&lt;/a&gt; on both &lt;strong&gt;HuggingFace&lt;/strong&gt; ğŸ¤— and &lt;strong&gt;ModelScope&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/modelscope.png&#34; width=&#34;20&#34;&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024.3.20] We release the arxiv paper of &lt;a href=&#34;http://arxiv.org/abs/2403.12895&#34;&gt;mPLUG-DocOwl 1.5&lt;/a&gt;, a SOTA 8B Multimodal LLM on OCR-free Document Understanding (DocVQA 82.2, InfoVQA 50.7, ChartQA 70.2, TextVQA 68.6).&lt;/li&gt; &#xA; &lt;li&gt;[2024.01.13] Our Scientific Diagram Analysis dataset &lt;a href=&#34;https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl&#34;&gt;M-Paper&lt;/a&gt; has been available on both &lt;strong&gt;HuggingFace&lt;/strong&gt; ğŸ¤— and &lt;strong&gt;ModelScope&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/modelscope.png&#34; width=&#34;20&#34;&gt;, containing 447k high-resolution diagram images and corresponding paragraph analysis.&lt;/li&gt; &#xA; &lt;li&gt;[2023.10.13] Training data, models of &lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl/&#34;&gt;mPLUG-DocOwl&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/UReader/&#34;&gt;UReader&lt;/a&gt; has been open-soruced.&lt;/li&gt; &#xA; &lt;li&gt;[2023.10.10] Our paper &lt;a href=&#34;https://arxiv.org/abs/2310.05126&#34;&gt;UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model&lt;/a&gt; is accepted by EMNLP 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- * ğŸ”¥ [10.10] The source code and instruction data will be released in [UReader](https://github.com/LukeForeverYoung/UReader). --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023.07.10] The demo of mPLUG-DocOwl on &lt;a href=&#34;https://modelscope.cn/studios/damo/mPLUG-DocOwl/summary&#34;&gt;ModelScope&lt;/a&gt; is avaliable.&lt;/li&gt; &#xA; &lt;li&gt;[2023.07.07] We release the technical report and evaluation set of mPLUG-DocOwl.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl1.5/&#34;&gt;&lt;strong&gt;mPLUG-DocOwl1.5&lt;/strong&gt;&lt;/a&gt; (Arxiv 2024) - mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/PaperOwl/&#34;&gt;&lt;strong&gt;mPLUG-PaperOwl&lt;/strong&gt;&lt;/a&gt; (Arxiv 2023) - mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/UReader/&#34;&gt;&lt;strong&gt;UReader&lt;/strong&gt;&lt;/a&gt; (EMNLP 2023) - UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl/&#34;&gt;&lt;strong&gt;mPLUG-DocOwl&lt;/strong&gt;&lt;/a&gt; (Arxiv 2023) - mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;Note: The demo of HuggingFace is not as stable as ModelScope because the GPU in ZeroGPU Spaces of HuggingFace is dynamically assigned.&lt;/p&gt; &#xA;&lt;h3&gt;ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://modelscope.cn/studios/iic/mPLUG-DocOwl/summary&#34;&gt;&lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;HuggingFace&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/mPLUG/DocOwl&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/huggingface.png&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/docowl1.5_chat_case.png&#34; alt=&#34;images&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alibaba/AliceMind/tree/main/mPLUG&#34;&gt;mPLUG&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alibaba/AliceMind&#34;&gt;mPLUG-2&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/X-PLUG/mPLUG-Owl&#34;&gt;mPLUG-Owl&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>youtube-jocoding/gpt-bitcoin</title>
    <updated>2024-04-05T01:34:14Z</updated>
    <id>tag:github.com,2024-04-05:/youtube-jocoding/gpt-bitcoin</id>
    <link href="https://github.com/youtube-jocoding/gpt-bitcoin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ë¹„íŠ¸ì½”ì¸ GPT ì¸ê³µì§€ëŠ¥ AI ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ë¹„íŠ¸ì½”ì¸ GPT ì¸ê³µì§€ëŠ¥ AI ì—…ë¹„íŠ¸ ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ ë§Œë“¤ê¸°&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT APIë¥¼ í™œìš©í•˜ì—¬ íˆ¬ìë¥¼ ìë™í™” í•©ë‹ˆë‹¤. by ìœ íŠœë²„ ì¡°ì½”ë”©&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ê´€ë ¨ ë§í¬&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jocoding.net/bitcoin&#34;&gt;ìˆ˜ì—… ìë£Œ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/live/-7IVgjUw79s?feature=share&#34;&gt;1í¸ - ë¼ì´ë¸Œ í’€ë²„ì „ ë§í¬(ë©¤ë²„ì‹­ ì „ìš©)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/live/GhZenus5rww?feature=share&#34;&gt;2í¸ - ë¼ì´ë¸Œ í’€ë²„ì „ ë§í¬(ë©¤ë²„ì‹­ ì „ìš©)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/live/ORo8QAn-g74?feature=share&#34;&gt;3í¸ - ë¼ì´ë¸Œ í’€ë²„ì „ ë§í¬(ë©¤ë²„ì‹­ ì „ìš©)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;í¸ì§‘ë³¸ (ì—…ë°ì´íŠ¸ ì˜ˆì •)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ì „ëµ ì†Œê°œ&lt;/h2&gt; &#xA;&lt;h3&gt;1.autotrade.py, instruction.md&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ë°ì´í„°: ì¼(30ì¼), ì‹œê°„(24ì‹œê°„) OHLCV, Moving Averages, RSI, Stochastic Oscillator, MACD, Bollinger Bands, Orderbook Data&lt;/li&gt; &#xA; &lt;li&gt;ì „ëµ: 1ì‹œê°„ì— í•œë²ˆ íŒë‹¨í•˜ì—¬ ì „ëŸ‰ ë§¤ìˆ˜/ë§¤ë„ or í™€ë“œ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.autotrade_v2.py, instruction_v2.md&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ë°ì´í„°: ì¼(30ì¼), ì‹œê°„(24ì‹œê°„) OHLCV, Moving Averages, RSI, Stochastic Oscillator, MACD, Bollinger Bands, Orderbook Data, ìµœì‹  ë‰´ìŠ¤ ë°ì´í„°(SerpApi), ê³µí¬/íƒìš• ì§€ìˆ˜&lt;/li&gt; &#xA; &lt;li&gt;ì „ëµ: 8ì‹œê°„ì— í•œë²ˆ íŒë‹¨í•˜ì—¬ ë¶€ë¶„ ë§¤ìˆ˜/ë§¤ë„ or í™€ë“œ, íˆ¬ì ë°ì´í„° ê¸°ë¡í•˜ê³  AI ì¬ê·€ ê°œì„ &lt;/li&gt; &#xA; &lt;li&gt;ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒë¥¼ ìœ„í•œ &lt;a href=&#34;https://serpapi.com/&#34;&gt;SerpApi&lt;/a&gt; ê°€ì… ë° API KEY ë“±ë¡ í•„ìš”&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;.env íŒŒì¼ ìƒì„± ë° ì„¤ì •&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#34;YourKey&#34;&#xA;UPBIT_ACCESS_KEY=&#34;YourKey&#34;&#xA;UPBIT_SECRET_KEY=&#34;YourKey&#34;&#xA;SERPAPI_API_KEY=&#34;YourKey&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ë¡œì»¬ í™˜ê²½ ì„¤ì •&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;AWS EC2 Ubuntu ì„œë²„ ì„¤ì • ë°©ë²•&lt;/h2&gt; &#xA;&lt;h3&gt;ì—…ë¹„íŠ¸ API í—ˆìš© IP ì„¤ì •&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://upbit.com/mypage/open_api_management&#34;&gt;ì—…ë¹„íŠ¸ API í™ˆí˜ì´ì§€&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ê¸°ë³¸ ì„¸íŒ…&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;í•œêµ­ ê¸°ì¤€ìœ¼ë¡œ ì„œë²„ ì‹œê°„ ì„¤ì •&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;íŒ¨í‚¤ì§€ ëª©ë¡ ì—…ë°ì´íŠ¸&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;íŒ¨í‚¤ì§€ ëª©ë¡ ì—…ê·¸ë ˆì´ë“œ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pip3 ì„¤ì¹˜&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install python3-pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ë ˆí¬ì§€í† ë¦¬ ê°€ì ¸ì˜¤ê¸°&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/youtube-jocoding/gpt-bitcoin.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ì„œë²„ì—ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;.env íŒŒì¼ ë§Œë“¤ê³  API KEY ë„£ê¸°&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;vim .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ëª…ë ¹ì–´&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;í˜„ì¬ ê²½ë¡œ ìƒì„¸ ì¶œë ¥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ls -al&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ê²½ë¡œ ì´ë™&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ê²½ë¡œ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vim ì—ë””í„°ë¡œ íŒŒì¼ ì—´ê¸°&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;vim autotrade.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vim ì—ë””í„° ì…ë ¥: i&lt;/li&gt; &#xA; &lt;li&gt;vim ì—ë””í„° ì €ì¥: ESC + wq!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ì‹¤í–‰í•˜ê¸°&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ê·¸ëƒ¥ ì‹¤í–‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 autotrade.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;nohup python3 -u autotrade.py &amp;gt; output.log 2&amp;gt;&amp;amp;1 &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ë¡œê·¸ ë³´ê¸°&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cat output.log&#xA;tail -f output.log&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ì‹¤í–‰ í™•ì¸&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ps ax | grep .py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ì¢…ë£Œí•˜ê¸°&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kill -9 PID&#xA;ex. kill -9 13586&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ì¶”í›„ ê³„íš&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ë¹—ì¸, ë°”ì´ë‚¸ìŠ¤, ì½”ì¸ë² ì´ìŠ¤, OKX, ë°”ì´ë¹„íŠ¸ë„ ê°€ëŠ¥í•˜ë©´ ë‹¤ë£¨ê² ìŒ&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>