<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-10T06:00:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lfnovo/open-notebook</title>
    <updated>2025-08-10T06:00:47Z</updated>
    <id>tag:github.com,2025-08-10:/lfnovo/open-notebook</id>
    <link href="https://github.com/lfnovo/open-notebook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lfnovo/open-notebook/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge&#34; alt=&#34;Forks&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfnovo/open-notebook/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge&#34; alt=&#34;Stargazers&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfnovo/open-notebook/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge&#34; alt=&#34;Issues&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge&#34; alt=&#34;MIT License&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; &#xA;&lt;!-- PROJECT LOGO --&gt; &#xA;&lt;br /&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/lfnovo/open-notebook&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true&#34; alt=&#34;Logo&#34; /&gt; &lt;/a&gt; &#xA; &lt;h3 align=&#34;center&#34;&gt;Open Notebook&lt;/h3&gt; &#xA; &lt;p align=&#34;center&#34;&gt; An open source, privacy-focused alternative to Google&#39;s Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href=&#34;https://discord.gg/37XJPXfz2w&#34;&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href=&#34;https://www.open-notebook.ai&#34;&gt;&lt;strong&gt;Checkout our website ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md&#34;&gt;üìö Get Started&lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md&#34;&gt;üìñ User Guide&lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md&#34;&gt;‚ú® Features&lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md&#34;&gt;üöÄ Deploy&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üì¢ Open Notebook is under very active development&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Open Notebook is under active development! We&#39;re moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don&#39;t hesitate to reach out with any questions or suggestions. I&#39;m excited to see how you&#39;ll use it and what ideas you&#39;ll bring to the project! Let&#39;s build something amazing together! üöÄ&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;About The Project&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png&#34; alt=&#34;New Notebook&#34; /&gt;&lt;/p&gt; &#xA;&lt;p&gt;An open source, privacy-focused alternative to Google&#39;s Notebook LM. Why give Google more of our data when we can take control of our own research workflows?&lt;/p&gt; &#xA;&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think üß† and acquire new knowledge üí°, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîí &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; &#xA; &lt;li&gt;üìö &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; &#xA; &lt;li&gt;üéôÔ∏è &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; &#xA; &lt;li&gt;üîç &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; &#xA; &lt;li&gt;üí¨ &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learn more about our project at &lt;a href=&#34;https://www.open-notebook.ai&#34;&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üÜö Open Notebook vs Google Notebook LM&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;Open Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Google Notebook LM&lt;/th&gt; &#xA;   &lt;th&gt;Advantage&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Self-hosted, your data&lt;/td&gt; &#xA;   &lt;td&gt;Google cloud only&lt;/td&gt; &#xA;   &lt;td&gt;Complete data sovereignty&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; &#xA;   &lt;td&gt;Google models only&lt;/td&gt; &#xA;   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; &#xA;   &lt;td&gt;2 speakers only&lt;/td&gt; &#xA;   &lt;td&gt;Extreme flexibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3 granular levels&lt;/td&gt; &#xA;   &lt;td&gt;All-or-nothing&lt;/td&gt; &#xA;   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Custom and built-in&lt;/td&gt; &#xA;   &lt;td&gt;Limited options&lt;/td&gt; &#xA;   &lt;td&gt;Unlimited processing power&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Full REST API&lt;/td&gt; &#xA;   &lt;td&gt;No API&lt;/td&gt; &#xA;   &lt;td&gt;Complete automation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; &#xA;   &lt;td&gt;Google hosted only&lt;/td&gt; &#xA;   &lt;td&gt;Deploy anywhere&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Comprehensive with sources&lt;/td&gt; &#xA;   &lt;td&gt;Basic references&lt;/td&gt; &#xA;   &lt;td&gt;Research integrity&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Open source, fully customizable&lt;/td&gt; &#xA;   &lt;td&gt;Closed system&lt;/td&gt; &#xA;   &lt;td&gt;Unlimited extensibility&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pay only for AI usage&lt;/td&gt; &#xA;   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; &#xA;   &lt;td&gt;Transparent and controllable&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîí &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; &#xA; &lt;li&gt;üí∞ &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; &#xA; &lt;li&gt;üéôÔ∏è &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Built With&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&#34; alt=&#34;Python&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://surrealdb.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white&#34; alt=&#34;SurrealDB&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://www.langchain.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white&#34; alt=&#34;LangChain&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://streamlit.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;amp;logo=streamlit&amp;amp;logoColor=white&#34; alt=&#34;Streamlit&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Ready to try Open Notebook? Choose your preferred method:&lt;/p&gt; &#xA;&lt;h3&gt;‚ö° Instant Setup (Recommended)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a new directory for your Open Notebook installation&#xA;mkdir open-notebook&#xA;cd open-notebook&#xA;&#xA;# Using Docker - Get started in 2 minutes&#xA;docker run -d \&#xA;  --name open-notebook \&#xA;  -p 8502:8502 -p 5055:5055 \&#xA;  -v ./notebook_data:/app/data \&#xA;  -v ./surreal_data:/mydata \&#xA;  -e OPENAI_API_KEY=your_key \&#xA;  lfnovo/open_notebook:latest-single&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;open-notebook/&#xA;‚îú‚îÄ‚îÄ notebook_data/     # Your notebooks and research content&#xA;‚îî‚îÄ‚îÄ surreal_data/      # Database files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access your installation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Main Interface&lt;/strong&gt;: &lt;a href=&#34;http://localhost:8502&#34;&gt;http://localhost:8502&lt;/a&gt; (Streamlit UI)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîß API Access&lt;/strong&gt;: &lt;a href=&#34;http://localhost:5055&#34;&gt;http://localhost:5055&lt;/a&gt; (REST API)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìö API Documentation&lt;/strong&gt;: &lt;a href=&#34;http://localhost:5055/docs&#34;&gt;http://localhost:5055/docs&lt;/a&gt; (Interactive Swagger UI)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Run from a dedicated folder&lt;/strong&gt;: Create and run this from inside a new &lt;code&gt;open-notebook&lt;/code&gt; folder so your data volumes are properly organized&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Volume persistence&lt;/strong&gt;: The volumes (&lt;code&gt;-v ./notebook_data:/app/data&lt;/code&gt; and &lt;code&gt;-v ./surreal_data:/mydata&lt;/code&gt;) are essential to persist your data between container restarts. Without them, you&#39;ll lose all your notebooks and research when the container stops.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;üõ†Ô∏è Full Installation&lt;/h3&gt; &#xA;&lt;p&gt;For development or customization:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/lfnovo/open-notebook&#xA;cd open-notebook&#xA;make start-all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üìñ Need Help?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ü§ñ AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href=&#34;https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant&#34;&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md&#34;&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md&#34;&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to the &lt;a href=&#34;https://github.com/lfnovo/esperanto&#34;&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;LLM Support&lt;/th&gt; &#xA;   &lt;th&gt;Embedding Support&lt;/th&gt; &#xA;   &lt;th&gt;Speech-to-Text&lt;/th&gt; &#xA;   &lt;th&gt;Text-to-Speech&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Anthropic&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Groq&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google (GenAI)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ollama&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Perplexity&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ElevenLabs&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Azure OpenAI&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Voyage&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xAI&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenRouter&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;Core Capabilities&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîí Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üéØ Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìö Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ü§ñ Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üéôÔ∏è Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîç Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üí¨ Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìù AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;‚ö° Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîß Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üåê Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href=&#34;http://localhost:5055/docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/API-Documentation-blue?style=flat-square&#34; alt=&#34;API Docs&#34; /&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîê Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìä Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìé Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Three-Column Interface&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=D-760MlGwaI&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/D-760MlGwaI/0.jpg&#34; alt=&#34;Check out our podcast sample&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìö Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md&#34;&gt;üìñ Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md&#34;&gt;‚ö° Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md&#34;&gt;üîß Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md&#34;&gt;üéØ Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;User Guide&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md&#34;&gt;üì± Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md&#34;&gt;üìö Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md&#34;&gt;üìÑ Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md&#34;&gt;üìù Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md&#34;&gt;üí¨ Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md&#34;&gt;üîç Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Topics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md&#34;&gt;üéôÔ∏è Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md&#34;&gt;üîß Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md&#34;&gt;ü§ñ AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md&#34;&gt;üîß REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md&#34;&gt;üîê Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md&#34;&gt;üöÄ Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;üó∫Ô∏è Roadmap&lt;/h2&gt; &#xA;&lt;h3&gt;Upcoming Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;React Frontend&lt;/strong&gt;: Modern React-based frontend to replace Streamlit&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Recently Completed ‚úÖ&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/lfnovo/open-notebook/issues&#34;&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Community &amp;amp; Contributing&lt;/h2&gt; &#xA;&lt;h3&gt;Join the Community&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ &lt;strong&gt;&lt;a href=&#34;https://discord.gg/37XJPXfz2w&#34;&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; &#xA; &lt;li&gt;üêõ &lt;strong&gt;&lt;a href=&#34;https://github.com/lfnovo/open-notebook/issues&#34;&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; &#xA; &lt;li&gt;‚≠ê &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Contributing&lt;/h3&gt; &#xA;&lt;p&gt;We welcome contributions! We&#39;re especially looking for help with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help build a modern React-based UI (planned replacement for current Streamlit interface)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, SurrealDB, Streamlit&lt;br /&gt; &lt;strong&gt;Future Roadmap&lt;/strong&gt;: React frontend, enhanced real-time updates&lt;/p&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;üìû Contact&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href=&#34;https://twitter.com/lfnovo&#34;&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí¨ &lt;a href=&#34;https://discord.gg/37XJPXfz2w&#34;&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; &#xA; &lt;li&gt;üêõ &lt;a href=&#34;https://github.com/lfnovo/open-notebook/issues&#34;&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;a href=&#34;https://www.open-notebook.ai&#34;&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/lfnovo/podcast-creator&#34;&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/lfnovo/surreal-commands&#34;&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/lfnovo/content-core&#34;&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/lfnovo/esperanto&#34;&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/docling-project/docling&#34;&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;(&lt;a href=&#34;https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top&#34;&gt;back to top&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; &#xA;&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</summary>
  </entry>
  <entry>
    <title>Pointcept/Pointcept</title>
    <updated>2025-08-10T06:00:47Z</updated>
    <id>tag:github.com,2025-08-10:/Pointcept/Pointcept</id>
    <link href="https://github.com/Pointcept/Pointcept" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pointcept: Perceive the world with sparse points, a codebase for point cloud perception research. Latest works: Sonata (CVPR&#39;25 Highlight), PTv3 (CVPR&#39;24 Oral)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- pypi-strip --&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/pointcept/assets/main/pointcept/logo_dark.png&#34; /&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/pointcept/assets/main/pointcept/logo.png&#34; /&gt; &#xA;  &lt;!-- /pypi-strip --&gt; &#xA;  &lt;img alt=&#34;pointcept&#34; src=&#34;https://raw.githubusercontent.com/pointcept/assets/main/pointcept/logo.png&#34; width=&#34;400&#34; /&gt; &#xA;  &lt;!-- pypi-strip --&gt; &#xA; &lt;/picture&gt;&lt;br /&gt; &#xA; &lt;!-- /pypi-strip --&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pointcept/pointcept/actions/workflows/formatter.yml&#34;&gt;&lt;img src=&#34;https://github.com/pointcept/pointcept/actions/workflows/formatter.yml/badge.svg?sanitize=true&#34; alt=&#34;Formatter&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pointcept&lt;/strong&gt; is a powerful and flexible codebase for point cloud perception research. It is also an official implementation of the following paper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Sonata: Self-Supervised Learning of Reliable Point Representations&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, Julian Straub&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2025 - Highlight&lt;br /&gt; [ Pretrain ] [Sonata] - [ &lt;a href=&#34;https://xywu.me/sonata/&#34;&gt;Project&lt;/a&gt; ] [ &lt;a href=&#34;https://arxiv.org/abs/2503.16429&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://xywu.me/research/sonata/bib.txt&#34;&gt;Bib&lt;/a&gt; ] [ &lt;a href=&#34;https://github.com/facebookresearch/sonata&#34;&gt;Demo&lt;/a&gt; ] [ &lt;a href=&#34;https://huggingface.co/facebook/sonata&#34;&gt;Weight&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sonata&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Point Transformer V3: Simpler, Faster, Stronger&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2024 - Oral&lt;br /&gt; [ Backbone ] [PTv3] - [ &lt;a href=&#34;https://arxiv.org/abs/2312.10035&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://xywu.me/research/ptv3/bib.txt&#34;&gt;Bib&lt;/a&gt; ] [ &lt;a href=&#34;https://github.com/Pointcept/PointTransformerV3&#34;&gt;Project&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://github.com/Pointcept/PointTransformerV3&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, Jiaya Jia&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2024&lt;br /&gt; [ Backbone ] [ OA-CNNs ] - [ &lt;a href=&#34;https://arxiv.org/abs/2403.14418&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://xywu.me/research/oacnns/bib.txt&#34;&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#oa-cnns&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2024&lt;br /&gt; [ Pretrain ] [PPT] - [ &lt;a href=&#34;https://arxiv.org/abs/2308.09718&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://xywu.me/research/ppt/bib.txt&#34;&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-prompt-training-ppt&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Xin Wen, Xihui Liu, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2023&lt;br /&gt; [ Pretrain ] [ MSC ] - [ &lt;a href=&#34;https://arxiv.org/abs/2303.14191&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://xywu.me/research/msc/bib.txt&#34;&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning Context-aware Classifier for Semantic Segmentation&lt;/strong&gt; (3D Part)&lt;br /&gt; &lt;em&gt;Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai, Yixin Chen, Shu Liu, Jiaya Jia&lt;/em&gt;&lt;br /&gt; AAAI Conference on Artificial Intelligence (&lt;strong&gt;AAAI&lt;/strong&gt;) 2023 - Oral&lt;br /&gt; [ SemSeg ] [ CAC ] - [ &lt;a href=&#34;https://arxiv.org/abs/2303.11633&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://xywu.me/research/cac/bib.txt&#34;&gt;Bib&lt;/a&gt; ] [ &lt;a href=&#34;https://github.com/tianzhuotao/CAC&#34;&gt;2D Part&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#context-aware-classifier&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Point Transformer V2: Grouped Vector Attention and Partition-based Pooling&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; Conference on Neural Information Processing Systems (&lt;strong&gt;NeurIPS&lt;/strong&gt;) 2022&lt;br /&gt; [ Backbone ] [ PTv2 ] - [ &lt;a href=&#34;https://arxiv.org/abs/2210.05666&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://xywu.me/research/ptv2/bib.txt&#34;&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Point Transformer&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun&lt;/em&gt;&lt;br /&gt; IEEE International Conference on Computer Vision (&lt;strong&gt;ICCV&lt;/strong&gt;) 2021 - Oral&lt;br /&gt; [ Backbone ] [ PTv1 ] - [ &lt;a href=&#34;https://arxiv.org/abs/2012.09164&#34;&gt;arXiv&lt;/a&gt; ] [ &lt;a href=&#34;https://hszhao.github.io/papers/iccv21_pointtransformer_bib.txt&#34;&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, &lt;strong&gt;Pointcept&lt;/strong&gt; integrates the following excellent work (contain above):&lt;br /&gt; Backbone: &lt;a href=&#34;https://github.com/NVIDIA/MinkowskiEngine&#34;&gt;MinkUNet&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sparseunet&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/traveller59/spconv&#34;&gt;SpUNet&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sparseunet&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/mit-han-lab/spvnas&#34;&gt;SPVCNN&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#spvcnn&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/abs/2403.14418&#34;&gt;OACNNs&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#oa-cnns&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/abs/2012.09164&#34;&gt;PTv1&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/abs/2210.05666&#34;&gt;PTv2&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/abs/2312.10035&#34;&gt;PTv3&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/dvlab-research/Stratified-Transformer&#34;&gt;StratifiedFormer&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#stratified-transformer&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/octree-nn/octformer&#34;&gt;OctFormer&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#octformer&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/microsoft/Swin3D&#34;&gt;Swin3D&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#swin3d&#34;&gt;here&lt;/a&gt;);&lt;br /&gt; Semantic Segmentation: &lt;a href=&#34;https://github.com/kumuji/mix3d&#34;&gt;Mix3d&lt;/a&gt; (&lt;a href=&#34;https://github.com/Pointcept/Pointcept/raw/main/configs/scannet/semseg-spunet-v1m1-0-base.py#L5&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/abs/2303.11633&#34;&gt;CAC&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#context-aware-classifier&#34;&gt;here&lt;/a&gt;);&lt;br /&gt; Instance Segmentation: &lt;a href=&#34;https://github.com/dvlab-research/PointGroup&#34;&gt;PointGroup&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointgroup&#34;&gt;here&lt;/a&gt;);&lt;br /&gt; Pre-training: &lt;a href=&#34;https://github.com/facebookresearch/PointContrast&#34;&gt;PointContrast&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointcontrast&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/facebookresearch/ContrastiveSceneContexts&#34;&gt;Contrastive Scene Contexts&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#contrastive-scene-contexts&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/abs/2303.14191&#34;&gt;Masked Scene Contrast&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/abs/2308.09718&#34;&gt;Point Prompt Training&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-prompt-training-ppt&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;&#34;&gt;Sonata&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sonata&#34;&gt;here&lt;/a&gt;);&lt;br /&gt; Datasets: &lt;a href=&#34;http://www.scan-net.org/&#34;&gt;ScanNet&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#scannet-v2&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;http://www.scan-net.org/&#34;&gt;ScanNet200&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#scannet-v2&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://kaldir.vc.in.tum.de/scannetpp/&#34;&gt;ScanNet++&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#scannet&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLScDimvNMCGhy_rmBA2gHfDu3naktRm6A8BPwAWWDv-Uhm6Shw/viewform?c=0&amp;amp;w=1&#34;&gt;S3DIS&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#s3dis&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/apple/ARKitScenes&#34;&gt;ArkitScene&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#arkitscenes&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://github.com/facebookresearch/habitat-matterport3d-dataset/&#34;&gt;HM3D&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#habitat---matterport-3d-hm3d&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://niessner.github.io/Matterport/&#34;&gt;Matterport3D&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#matterport3d&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://structured3d-dataset.org/&#34;&gt;Structured3D&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#structured3d&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;http://www.semantic-kitti.org/&#34;&gt;SemanticKITTI&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#semantickitti&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://www.nuscenes.org/nuscenes&#34;&gt;nuScenes&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#nuscenes&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://modelnet.cs.princeton.edu/&#34;&gt;ModelNet40&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#modelnet&#34;&gt;here&lt;/a&gt;), &lt;a href=&#34;https://waymo.com/open/&#34;&gt;Waymo&lt;/a&gt; (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#waymo&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Apr 2025&lt;/em&gt; üöÄ: We now support &lt;code&gt;wandb&lt;/code&gt;, check the &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt; training section for more information. (Thanks @Streakfull for his contribution!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Mar 2025&lt;/em&gt; üöÄ: &lt;strong&gt;Sonata&lt;/strong&gt; is accepted by CVPR 2025 and selected as one of the &lt;strong&gt;Highlight&lt;/strong&gt; presentations (3.0% submissions)! We release the code with Pointcept v1.6.0. We release the pre-training &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sonata&#34;&gt;code&lt;/a&gt;&lt;/strong&gt; along with Pointcept v1.6.0 and provide an easy-to-use pre-trained model for inference, tuning, and visualization in our project &lt;strong&gt;&lt;a href=&#34;https://github.com/facebookresearch/sonata&#34;&gt;repository&lt;/a&gt;&lt;/strong&gt; hosted by Meta.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;May 2024&lt;/em&gt;: In v1.5.2, we redesigned the default structure for each dataset for better performance. Please &lt;strong&gt;re-preprocess&lt;/strong&gt; datasets or &lt;strong&gt;download&lt;/strong&gt; our preprocessed datasets from &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Pointcept&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Apr 2024&lt;/em&gt;: &lt;strong&gt;PTv3&lt;/strong&gt; is selected as one of the 90 &lt;strong&gt;Oral&lt;/strong&gt; papers (3.3% accepted papers, 0.78% submissions) by CVPR&#39;24!&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Mar 2024&lt;/em&gt;: We release code for &lt;strong&gt;OA-CNNs&lt;/strong&gt;, accepted by CVPR&#39;24. Issue related to &lt;strong&gt;OA-CNNs&lt;/strong&gt; can @Pbihao.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Feb 2024&lt;/em&gt;: &lt;strong&gt;PTv3&lt;/strong&gt; and &lt;strong&gt;PPT&lt;/strong&gt; are accepted by CVPR&#39;24, another &lt;strong&gt;two&lt;/strong&gt; papers by our Pointcept team have also been accepted by CVPR&#39;24 üéâüéâüéâ. We will make them publicly available soon!&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Dec 2023&lt;/em&gt;: &lt;strong&gt;PTv3&lt;/strong&gt; is released on arXiv, and the code is available in Pointcept. PTv3 is an efficient backbone model that achieves SOTA performances across indoor and outdoor scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Aug 2023&lt;/em&gt;: &lt;strong&gt;PPT&lt;/strong&gt; is released on arXiv. PPT presents a multi-dataset pre-training framework that achieves SOTA performance in both &lt;strong&gt;indoor&lt;/strong&gt; and &lt;strong&gt;outdoor&lt;/strong&gt; scenarios. It is compatible with various existing pre-training frameworks and backbones. A &lt;strong&gt;pre-release&lt;/strong&gt; version of the code is accessible; for those interested, please feel free to contact me directly for access.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Mar 2023&lt;/em&gt;: We released our codebase, &lt;strong&gt;Pointcept&lt;/strong&gt;, a highly potent tool for point cloud representation learning and perception. We welcome new work to join the &lt;em&gt;Pointcept&lt;/em&gt; family and highly recommend reading &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt; before starting your trail.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Feb 2023&lt;/em&gt;: &lt;strong&gt;MSC&lt;/strong&gt; and &lt;strong&gt;CeCo&lt;/strong&gt; accepted by CVPR 2023. &lt;em&gt;MSC&lt;/em&gt; is a highly efficient and effective pretraining framework that facilitates cross-dataset large-scale pretraining, while &lt;em&gt;CeCo&lt;/em&gt; is a segmentation method specifically designed for long-tail datasets. Both approaches are compatible with all existing backbone models in our codebase, and we will soon make the code available for public use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Jan 2023&lt;/em&gt;: &lt;strong&gt;CAC&lt;/strong&gt;, oral work of AAAI 2023, has expanded its 3D result with the incorporation of Pointcept. This addition will allow CAC to serve as a pluggable segmentor within our codebase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Sep 2022&lt;/em&gt;: &lt;strong&gt;PTv2&lt;/strong&gt; accepted by NeurIPS 2022. It is a continuation of the Point Transformer. The proposed GVA theory can apply to most existing attention mechanisms, while Grid Pooling is also a practical addition to existing pooling methods.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;em&gt;Pointcept&lt;/em&gt; useful to your research, please cite our work as encouragement. (‡©≠ÀäÍí≥‚ÄãÀã)‡©≠‚úß&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{pointcept2023,&#xA;    title={Pointcept: A Codebase for Point Cloud Perception Research},&#xA;    author={Pointcept Contributors},&#xA;    howpublished = {\url{https://github.com/Pointcept/Pointcept}},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#data-preparation&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu: 18.04 and above.&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.3 and above.&lt;/li&gt; &#xA; &lt;li&gt;PyTorch: 1.10.0 and above.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Conda Environment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 1&lt;/strong&gt;: Utilize conda &lt;code&gt;environment.yml&lt;/code&gt; to create a new environment with one line code:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create and activate conda environment named as &#39;pointcept-torch2.5.0-cu12.4&#39;&#xA;# cuda: 12.4, pytorch: 2.5.0&#xA;&#xA;# run `unset CUDA_PATH` if you have installed cuda in your local environment&#xA;conda env create -f environment.yml --verbose&#xA;conda activate pointcept-torch2.5.0-cu12.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 2&lt;/strong&gt;: Use our pre-built Docker image and refer to the supported tags &lt;a href=&#34;https://hub.docker.com/repository/docker/pointcept/pointcept/general&#34;&gt;here&lt;/a&gt;. Quickly verify the Docker image on your local machine with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus all -it --rm pointcept/pointcept:v1.6.0-pytorch2.5.0-cuda12.4-cudnn9-devel bash&#xA;git clone https://github.com/facebookresearch/sonata&#xA;cd sonata&#xA;export PYTHONPATH=./ &amp;amp;&amp;amp; python demo/0_pca.py&#xA;# Ignore the GUI error, we cannot expect a container to have its GUI, right?&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 3&lt;/strong&gt;: Manually create a conda environment:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n pointcept python=3.10 -y&#xA;conda activate pointcept&#xA;&#xA;# (Optional) If no CUDA installed&#xA;conda install nvidia/label/cuda-12.4.1::cuda conda-forge::cudnn conda-forge::gcc=13.2 conda-forge::gxx=13.2 -y&#xA;&#xA;conda install ninja -y&#xA;# Choose version you want here: https://pytorch.org/get-started/previous-versions/&#xA;conda install pytorch==2.5.0 torchvision==0.13.1 torchaudio==0.20.0 pytorch-cuda=12.4 -c pytorch -y&#xA;conda install h5py pyyaml -c anaconda -y&#xA;conda install sharedarray tensorboard tensorboardx wandb yapf addict einops scipy plyfile termcolor timm -c conda-forge -y&#xA;conda install pytorch-cluster pytorch-scatter pytorch-sparse -c pyg -y&#xA;pip install torch-geometric&#xA;&#xA;# spconv (SparseUNet)&#xA;# refer https://github.com/traveller59/spconv&#xA;pip install spconv-cu124&#xA;&#xA;# PPT (clip)&#xA;pip install ftfy regex tqdm&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&#xA;# PTv1 &amp;amp; PTv2 or precise eval&#xA;cd libs/pointops&#xA;# usual&#xA;python setup.py install&#xA;# docker &amp;amp; multi GPU arch&#xA;TORCH_CUDA_ARCH_LIST=&#34;ARCH LIST&#34; python  setup.py install&#xA;# e.g. 7.5: RTX 3000; 8.0: a100 More available in: https://developer.nvidia.com/cuda-gpus&#xA;TORCH_CUDA_ARCH_LIST=&#34;7.5 8.0&#34; python  setup.py install&#xA;cd ../..&#xA;&#xA;# Open3D (visualization, optional)&#xA;pip install open3d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;ScanNet v2&lt;/h3&gt; &#xA;&lt;p&gt;The preprocessing supports semantic and instance segmentation for both &lt;code&gt;ScanNet20&lt;/code&gt;, &lt;code&gt;ScanNet200&lt;/code&gt;, and &lt;code&gt;ScanNet Data Efficient&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the &lt;a href=&#34;http://www.scan-net.org/&#34;&gt;ScanNet&lt;/a&gt; v2 dataset.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run preprocessing code for raw ScanNet as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# RAW_SCANNET_DIR: the directory of downloaded ScanNet v2 raw dataset.&#xA;# PROCESSED_SCANNET_DIR: the directory of the processed ScanNet dataset (output dir).&#xA;python pointcept/datasets/preprocessing/scannet/preprocess_scannet.py --dataset_root ${RAW_SCANNET_DIR} --output_root ${PROCESSED_SCANNET_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) Download ScanNet Data Efficient files:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download-scannet.py is the official download script&#xA;# or follow instructions here: https://kaldir.vc.in.tum.de/scannet_benchmark/data_efficient/documentation#download&#xA;python download-scannet.py --data_efficient -o ${RAW_SCANNET_DIR}&#xA;# unzip downloads&#xA;cd ${RAW_SCANNET_DIR}/tasks&#xA;unzip limited-annotation-points.zip&#xA;unzip limited-reconstruction-scenes.zip&#xA;# copy files to processed dataset folder&#xA;mkdir ${PROCESSED_SCANNET_DIR}/tasks&#xA;cp -r ${RAW_SCANNET_DIR}/tasks/points ${PROCESSED_SCANNET_DIR}/tasks&#xA;cp -r ${RAW_SCANNET_DIR}/tasks/scenes ${PROCESSED_SCANNET_DIR}/tasks&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can be directly downloaded [&lt;a href=&#34;https://huggingface.co/datasets/Pointcept/scannet-compressed&#34;&gt;here&lt;/a&gt;], please agree the official license before download it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link processed dataset to codebase:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_SCANNET_DIR: the directory of the processed ScanNet dataset.&#xA;mkdir data&#xA;ln -s ${PROCESSED_SCANNET_DIR} ${CODEBASE_DIR}/data/scannet&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ScanNet++&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the &lt;a href=&#34;https://kaldir.vc.in.tum.de/scannetpp/&#34;&gt;ScanNet++&lt;/a&gt; dataset.&lt;/li&gt; &#xA; &lt;li&gt;Run preprocessing code for raw ScanNet++ as follows: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# RAW_SCANNETPP_DIR: the directory of downloaded ScanNet++ raw dataset.&#xA;# PROCESSED_SCANNETPP_DIR: the directory of the processed ScanNet++ dataset (output dir).&#xA;# NUM_WORKERS: the number of workers for parallel preprocessing.&#xA;python pointcept/datasets/preprocessing/scannetpp/preprocess_scannetpp.py --dataset_root ${RAW_SCANNETPP_DIR} --output_root ${PROCESSED_SCANNETPP_DIR} --num_workers ${NUM_WORKERS}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Sampling and chunking large point cloud data in train/val split as follows (only used for training): &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_SCANNETPP_DIR: the directory of the processed ScanNet++ dataset (output dir).&#xA;# NUM_WORKERS: the number of workers for parallel preprocessing.&#xA;python pointcept/datasets/preprocessing/sampling_chunking_data.py --dataset_root ${PROCESSED_SCANNETPP_DIR} --grid_size 0.01 --chunk_range 6 6 --chunk_stride 3 3 --split train --num_workers ${NUM_WORKERS}&#xA;python pointcept/datasets/preprocessing/sampling_chunking_data.py --dataset_root ${PROCESSED_SCANNETPP_DIR} --grid_size 0.01 --chunk_range 6 6 --chunk_stride 3 3 --split val --num_workers ${NUM_WORKERS}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Link processed dataset to codebase: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_SCANNETPP_DIR: the directory of the processed ScanNet dataset.&#xA;mkdir data&#xA;ln -s ${PROCESSED_SCANNETPP_DIR} ${CODEBASE_DIR}/data/scannetpp&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;S3DIS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download S3DIS data by filling this &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLScDimvNMCGhy_rmBA2gHfDu3naktRm6A8BPwAWWDv-Uhm6Shw/viewform?c=0&amp;amp;w=1&#34;&gt;Google form&lt;/a&gt;. Download the &lt;code&gt;Stanford3dDataset_v1.2.zip&lt;/code&gt; file and unzip it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Fix error in &lt;code&gt;Area_5/office_19/Annotations/ceiling&lt;/code&gt; Line 323474 (103.0ÔøΩ0000 =&amp;gt; 103.000000).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) Download Full 2D-3D S3DIS dataset (no XYZ) from &lt;a href=&#34;https://github.com/alexsax/2D-3D-Semantics&#34;&gt;here&lt;/a&gt; for parsing normal.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run preprocessing code for S3DIS as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# S3DIS_DIR: the directory of downloaded Stanford3dDataset_v1.2 dataset.&#xA;# RAW_S3DIS_DIR: the directory of Stanford2d3dDataset_noXYZ dataset. (optional, for parsing normal)&#xA;# PROCESSED_S3DIS_DIR: the directory of processed S3DIS dataset (output dir).&#xA;&#xA;# S3DIS without aligned angle&#xA;python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR}&#xA;# S3DIS with aligned angle&#xA;python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR} --align_angle&#xA;# S3DIS with normal vector (recommended, normal is helpful)&#xA;python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR} --raw_root ${RAW_S3DIS_DIR} --parse_normal&#xA;python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR} --raw_root ${RAW_S3DIS_DIR} --align_angle --parse_normal&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href=&#34;https://huggingface.co/datasets/Pointcept/s3dis-compressed&#34;&gt;here&lt;/a&gt;] (with normal vector and aligned angle), please agree with the official license before downloading it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_S3DIS_DIR: the directory of processed S3DIS dataset.&#xA;mkdir data&#xA;ln -s ${PROCESSED_S3DIS_DIR} ${CODEBASE_DIR}/data/s3dis&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ArkitScenes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download ArkitScenes 3DOD split with the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# RAW_AS_DIR: the directory of downloaded Raw ArkitScenes dataset.&#xA;git clone https://github.com/apple/ARKitScenes.git&#xA;cd ARKitScenes&#xA;python download_data.py 3dod --download_dir $RAW_AS_DIR --video_id_csv threedod/3dod_train_val_splits.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run preprocessing code for ArkitScenes as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# RAW_AS_DIR: the directory of downloaded ArkitScenes dataset.&#xA;# PROCESSED_AS_DIR: the directory of processed ArkitScenes dataset (output dir).&#xA;# NUM_WORKERS: Number for workers for preprocessing, default same as cpu count (might OOM).&#xA;cd $POINTCEPT_DIR&#xA;export PYTHONPATH=./&#xA;python pointcept/datasets/preprocessing/arkitscenes/preprocess_arkitscenes_mesh.py --dataset_root $RAW_AS_DIR --output_root $PROCESSED_AS_DIR --num_workers $NUM_WORKERS&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href=&#34;https://huggingface.co/datasets/Pointcept/arkitscenes-compressed&#34;&gt;here&lt;/a&gt;] please read and agree the official &lt;a href=&#34;https://github.com/apple/ARKitScenes?tab=License-1-ov-file#readme&#34;&gt;license&lt;/a&gt; before download it. (Unzip with the following command:&lt;br /&gt; &lt;code&gt;find ./ -name &#39;*.tar.gz&#39; | xargs -n 1 -P 8 -I {} sh -c &#39;tar -xzvf {}&#39;&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_AR_DIR: the directory of processed ArkitScenes dataset (output dir).&#xA;mkdir data&#xA;ln -s ${PROCESSED_AR_DIR} ${CODEBASE_DIR}/data/arkitscenes&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Habitat - Matterport 3D (HM3D)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download HM3D &lt;code&gt;hm3d-train-glb-v0.2.tar&lt;/code&gt; and &lt;code&gt;hm3d-val-glb-v0.2.tar&lt;/code&gt; with instuction &lt;a href=&#34;https://github.com/facebookresearch/habitat-sim/raw/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d&#34;&gt;here&lt;/a&gt; and unzip them.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run preprocessing code for HM3D as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# RAW_HM_DIR: the directory of downloaded HM3D dataset.&#xA;# PROCESSED_HM_DIR: the directory of processed HM3D dataset (output dir).&#xA;# NUM_WORKERS: Number for workers for preprocessing, default same as cpu count (might OOM).&#xA;export PYTHONPATH=./&#xA;python pointcept/datasets/preprocessing/hm3d/preprocess_hm3d.py --dataset_root $RAW_HM_DIR --output_root $PROCESSED_HM_DIR --density 0.02 --num_workers $NUM_WORKERS&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href=&#34;https://huggingface.co/datasets/Pointcept/hm3d-compressed&#34;&gt;here&lt;/a&gt;] please read and agree the official &lt;a href=&#34;https://matterport.com/legal/matterport-end-user-license-agreement-academic-use-model-data&#34;&gt;license&lt;/a&gt; before download it. (Unzip with the following command:&lt;br /&gt; &lt;code&gt;find ./ -name &#39;*.tar.gz&#39; | xargs -n 1 -P 4 -I {} sh -c &#39;tar -xzvf {}&#39;&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_HM_DIR: the directory of processed HM3D dataset (output dir).&#xA;mkdir data&#xA;ln -s ${PROCESSED_HM_DIR} ${CODEBASE_DIR}/data/hm3d&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Matterport3D&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow &lt;a href=&#34;https://niessner.github.io/Matterport/#download&#34;&gt;this page&lt;/a&gt; to request access to the dataset.&lt;/li&gt; &#xA; &lt;li&gt;Download the &#34;region_segmentation&#34; type, which represents the division of a scene into individual rooms. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download-mp.py is the official download script&#xA;# MATTERPORT3D_DIR: the directory of downloaded Matterport3D dataset.&#xA;python download-mp.py -o {MATTERPORT3D_DIR} --type region_segmentations&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Unzip the region_segmentations data &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# MATTERPORT3D_DIR: the directory of downloaded Matterport3D dataset.&#xA;python pointcept/datasets/preprocessing/matterport3d/unzip_matterport3d_region_segmentation.py --dataset_root {MATTERPORT3D_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run preprocessing code for Matterport3D as follows: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# MATTERPORT3D_DIR: the directory of downloaded Matterport3D dataset.&#xA;# PROCESSED_MATTERPORT3D_DIR: the directory of processed Matterport3D dataset (output dir).&#xA;# NUM_WORKERS: the number of workers for this preprocessing.&#xA;python pointcept/datasets/preprocessing/matterport3d/preprocess_matterport3d_mesh.py --dataset_root ${MATTERPORT3D_DIR} --output_root ${PROCESSED_MATTERPORT3D_DIR} --num_workers ${NUM_WORKERS}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Link processed dataset to codebase. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_MATTERPORT3D_DIR: the directory of processed Matterport3D dataset (output dir).&#xA;mkdir data&#xA;ln -s ${PROCESSED_MATTERPORT3D_DIR} ${CODEBASE_DIR}/data/matterport3d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Following the instruction of &lt;a href=&#34;https://github.com/ViLab-UCSD/OpenRooms&#34;&gt;OpenRooms&lt;/a&gt;, we remapped Matterport3D&#39;s categories to ScanNet 20 semantic categories with the addition of a ceiling category.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Alternative) Our preprocess data can also be downloaded &lt;a href=&#34;https://huggingface.co/datasets/Pointcept/matterport3d-compressed&#34;&gt;here&lt;/a&gt;, please agree the official license before download it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Structured3D&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download Structured3D panorama related and perspective (full) related zip files by filling this &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSc0qtvh4vHSoZaW6UvlXYy79MbcGdZfICjh4_t4bYofQIVIdw/viewform?pli=1&#34;&gt;Google form&lt;/a&gt; (no need to unzip them).&lt;/li&gt; &#xA; &lt;li&gt;Organize all downloaded zip file in one folder (&lt;code&gt;${STRUCT3D_DIR}&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Run preprocessing code for Structured3D as follows: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# STRUCT3D_DIR: the directory of downloaded Structured3D dataset.&#xA;# PROCESSED_STRUCT3D_DIR: the directory of processed Structured3D dataset (output dir).&#xA;# NUM_WORKERS: Number for workers for preprocessing, default same as cpu count (might OOM).&#xA;export PYTHONPATH=./&#xA;python pointcept/datasets/preprocessing/structured3d/preprocess_structured3d.py --dataset_root ${STRUCT3D_DIR} --output_root ${PROCESSED_STRUCT3D_DIR} --num_workers ${NUM_WORKERS} --grid_size 0.01 --fuse_prsp --fuse_pano&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Following the instruction of &lt;a href=&#34;https://arxiv.org/abs/2304.06906&#34;&gt;Swin3D&lt;/a&gt;, we keep 25 categories with frequencies of more than 0.001, out of the original 40 categories.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href=&#34;https://huggingface.co/datasets/Pointcept/structured3d-compressed&#34;&gt;here&lt;/a&gt;] (with perspective views and panorama view, 471.7G after unzipping), please agree the official license before download it. (Unzip with the following command:&lt;br /&gt; &lt;code&gt;find ./ -name &#39;*.tar.gz&#39; | xargs -n 1 -P 15 -I {} sh -c &#39;tar -xzvf {}&#39;&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_STRUCT3D_DIR: the directory of processed Structured3D dataset (output dir).&#xA;mkdir data&#xA;ln -s ${PROCESSED_STRUCT3D_DIR} ${CODEBASE_DIR}/data/structured3d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SemanticKITTI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;a href=&#34;http://www.semantic-kitti.org/dataset.html#download&#34;&gt;SemanticKITTI&lt;/a&gt; dataset.&lt;/li&gt; &#xA; &lt;li&gt;Link dataset to codebase. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# SEMANTIC_KITTI_DIR: the directory of SemanticKITTI dataset.&#xA;# |- SEMANTIC_KITTI_DIR&#xA;#   |- dataset&#xA;#     |- sequences&#xA;#       |- 00&#xA;#       |- 01&#xA;#       |- ...&#xA;&#xA;mkdir -p data&#xA;ln -s ${SEMANTIC_KITTI_DIR} ${CODEBASE_DIR}/data/semantic_kitti&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;nuScenes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the official &lt;a href=&#34;https://www.nuscenes.org/nuscenes#download&#34;&gt;NuScene&lt;/a&gt; dataset (with Lidar Segmentation) and organize the downloaded files as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NUSCENES_DIR&#xA;‚îÇ‚îÄ‚îÄ samples&#xA;‚îÇ‚îÄ‚îÄ sweeps&#xA;‚îÇ‚îÄ‚îÄ lidarseg&#xA;...&#xA;‚îÇ‚îÄ‚îÄ v1.0-trainval &#xA;‚îÇ‚îÄ‚îÄ v1.0-test&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run information preprocessing code (modified from OpenPCDet) for nuScenes as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# NUSCENES_DIR: the directory of downloaded nuScenes dataset.&#xA;# PROCESSED_NUSCENES_DIR: the directory of processed nuScenes dataset (output dir).&#xA;# MAX_SWEEPS: Max number of sweeps. Default: 10.&#xA;pip install nuscenes-devkit pyquaternion&#xA;python pointcept/datasets/preprocessing/nuscenes/preprocess_nuscenes_info.py --dataset_root ${NUSCENES_DIR} --output_root ${PROCESSED_NUSCENES_DIR} --max_sweeps ${MAX_SWEEPS} --with_camera&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess nuScenes information data can also be downloaded [&lt;a href=&#34;https://huggingface.co/datasets/Pointcept/nuscenes-compressed&#34;&gt;here&lt;/a&gt;] (only processed information, still need to download raw dataset and link to the folder), please agree the official license before download it.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link raw dataset to processed NuScene dataset folder:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# NUSCENES_DIR: the directory of downloaded nuScenes dataset.&#xA;# PROCESSED_NUSCENES_DIR: the directory of processed nuScenes dataset (output dir).&#xA;ln -s ${NUSCENES_DIR} {PROCESSED_NUSCENES_DIR}/raw&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then the processed nuscenes folder is organized as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nuscene&#xA;|‚îÄ‚îÄ raw&#xA;    ‚îÇ‚îÄ‚îÄ samples&#xA;    ‚îÇ‚îÄ‚îÄ sweeps&#xA;    ‚îÇ‚îÄ‚îÄ lidarseg&#xA;    ...&#xA;    ‚îÇ‚îÄ‚îÄ v1.0-trainval&#xA;    ‚îÇ‚îÄ‚îÄ v1.0-test&#xA;|‚îÄ‚îÄ info&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_NUSCENES_DIR: the directory of processed nuScenes dataset (output dir).&#xA;mkdir data&#xA;ln -s ${PROCESSED_NUSCENES_DIR} ${CODEBASE_DIR}/data/nuscenes&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Waymo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the official &lt;a href=&#34;https://waymo.com/open/download/&#34;&gt;Waymo&lt;/a&gt; dataset (v1.4.3) and organize the downloaded files as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;WAYMO_RAW_DIR&#xA;‚îÇ‚îÄ‚îÄ training&#xA;‚îÇ‚îÄ‚îÄ validation&#xA;‚îÇ‚îÄ‚îÄ testing&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the following dependence:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# If shows &#34;No matching distribution found&#34;, download whl directly from Pypi and install the package.&#xA;conda create -n waymo python=3.10 -y&#xA;conda activate waymo&#xA;pip install waymo-open-dataset-tf-2-12-0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the preprocessing code as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# WAYMO_DIR: the directory of the downloaded Waymo dataset.&#xA;# PROCESSED_WAYMO_DIR: the directory of the processed Waymo dataset (output dir).&#xA;# NUM_WORKERS: num workers for preprocessing&#xA;python pointcept/datasets/preprocessing/waymo/preprocess_waymo.py --dataset_root ${WAYMO_DIR} --output_root ${PROCESSED_WAYMO_DIR} --splits training validation --num_workers ${NUM_WORKERS}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Link processed dataset to the codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PROCESSED_WAYMO_DIR: the directory of the processed Waymo dataset (output dir).&#xA;mkdir data&#xA;ln -s ${PROCESSED_WAYMO_DIR} ${CODEBASE_DIR}/data/waymo&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ModelNet&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download &lt;a href=&#34;https://huggingface.co/datasets/Pointcept/modelnet40_normal_resampled-compressed&#34;&gt;modelnet40_normal_resampled.zip&lt;/a&gt; and unzip&lt;/li&gt; &#xA; &lt;li&gt;Link dataset to the codebase. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p data&#xA;ln -s ${MODELNET_DIR} ${CODEBASE_DIR}/data/modelnet40_normal_resampled&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Train from scratch.&lt;/strong&gt; The training processing is based on configs in &lt;code&gt;configs&lt;/code&gt; folder. The training script will generate an experiment folder in &lt;code&gt;exp&lt;/code&gt; folder and backup essential code in the experiment folder. Training config, log, tensorboard, and checkpoints will also be saved into the experiment folder during the training process.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}&#xA;# Script (Recommended)&#xA;sh scripts/train.sh -p ${INTERPRETER_PATH} -g ${NUM_GPU} -d ${DATASET_NAME} -c ${CONFIG_NAME} -n ${EXP_NAME}&#xA;# Direct&#xA;export PYTHONPATH=./&#xA;python tools/train.py --config-file ${CONFIG_PATH} --num-gpus ${NUM_GPU} --options save_path=${SAVE_PATH}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# By script (Recommended)&#xA;# -p is default set as python and can be ignored&#xA;sh scripts/train.sh -p python -d scannet -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;# Direct&#xA;export PYTHONPATH=./&#xA;python tools/train.py --config-file configs/scannet/semseg-pt-v2m2-0-base.py --options save_path=exp/scannet/semseg-pt-v2m2-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Resume training from checkpoint.&lt;/strong&gt; If the training process is interrupted by accident, the following script can resume training from a given checkpoint.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}&#xA;# Script (Recommended)&#xA;# simply add &#34;-r true&#34;&#xA;sh scripts/train.sh -p ${INTERPRETER_PATH} -g ${NUM_GPU} -d ${DATASET_NAME} -c ${CONFIG_NAME} -n ${EXP_NAME} -r true&#xA;# Direct&#xA;export PYTHONPATH=./&#xA;python tools/train.py --config-file ${CONFIG_PATH} --num-gpus ${NUM_GPU} --options save_path=${SAVE_PATH} resume=True weight=${CHECKPOINT_PATH}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Weights and Biases.&lt;/strong&gt; Pointcept by default enables both &lt;code&gt;tensorboard&lt;/code&gt; and &lt;code&gt;wandb&lt;/code&gt;. There are some usage notes related to &lt;code&gt;wandb&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Disable by set &lt;code&gt;enable_wandb=False&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Sync with &lt;code&gt;wandb&lt;/code&gt; remote server by &lt;code&gt;wandb login&lt;/code&gt; in the terminal or set &lt;code&gt;wandb_key=YOUR_WANDB_KEY&lt;/code&gt; in config.&lt;/li&gt; &#xA; &lt;li&gt;The project name is &#34;Pointcept&#34; by default, custom it to your research project name by setting &lt;code&gt;wandb_project=YOUR_PROJECT_NAME&lt;/code&gt; (e.g. Sonata-Dev, PointTransformerV3-Dev)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;During training, model evaluation is performed on point clouds after grid sampling (voxelization), providing an initial assessment of model performance. &lt;del&gt;However, to obtain precise evaluation results, testing is &lt;strong&gt;essential&lt;/strong&gt;&lt;/del&gt; &lt;em&gt;(now we automatically run the testing process after training with the &lt;code&gt;PreciseEvaluation&lt;/code&gt; hook)&lt;/em&gt;. The testing process involves subsampling a dense point cloud into a sequence of voxelized point clouds, ensuring comprehensive coverage of all points. These sub-results are then predicted and collected to form a complete prediction of the entire point cloud. This approach yields higher evaluation results compared to simply mapping/interpolating the prediction. In addition, our testing code supports TTA (test time augmentation) testing, which further enhances the stability of evaluation performance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# By script (Based on experiment folder created by training script)&#xA;sh scripts/test.sh -p ${INTERPRETER_PATH} -g ${NUM_GPU} -d ${DATASET_NAME} -n ${EXP_NAME} -w ${CHECKPOINT_NAME}&#xA;# Direct&#xA;export PYTHONPATH=./&#xA;python tools/test.py --config-file ${CONFIG_PATH} --num-gpus ${NUM_GPU} --options save_path=${SAVE_PATH} weight=${CHECKPOINT_PATH}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# By script (Based on experiment folder created by training script)&#xA;# -p is default set as python and can be ignored&#xA;# -w is default set as model_best and can be ignored&#xA;sh scripts/test.sh -p python -d scannet -n semseg-pt-v2m2-0-base -w model_best&#xA;# Direct&#xA;export PYTHONPATH=./&#xA;python tools/test.py --config-file configs/scannet/semseg-pt-v2m2-0-base.py --options save_path=exp/scannet/semseg-pt-v2m2-0-base weight=exp/scannet/semseg-pt-v2m2-0-base/model/model_best.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The TTA can be disabled by replace &lt;code&gt;data.test.test_cfg.aug_transform = [...]&lt;/code&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = dict(&#xA;    train = dict(...),&#xA;    val = dict(...),&#xA;    test = dict(&#xA;        ...,&#xA;        test_cfg = dict(&#xA;            ...,&#xA;            aug_transform = [&#xA;                [dict(type=&#34;RandomRotateTargetAngle&#34;, angle=[0], axis=&#34;z&#34;, center=[0, 0, 0], p=1)]&#xA;            ]&#xA;        )&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Offset&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;Offset&lt;/code&gt; is the separator of point clouds in batch data, and it is similar to the concept of &lt;code&gt;Batch&lt;/code&gt; in PyG. A visual illustration of batch and offset is as follows:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- pypi-strip --&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/pointcept/assets/main/pointcept/offset_dark.png&#34; /&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/pointcept/assets/main/pointcept/offset.png&#34; /&gt; &#xA;  &lt;!-- /pypi-strip --&gt; &#xA;  &lt;img alt=&#34;pointcept&#34; src=&#34;https://raw.githubusercontent.com/pointcept/assets/main/pointcept/offset.png&#34; width=&#34;480&#34; /&gt; &#xA;  &lt;!-- pypi-strip --&gt; &#xA; &lt;/picture&gt;&lt;br /&gt; &#xA; &lt;!-- /pypi-strip --&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;h3&gt;1. Backbones and Semantic Segmentation&lt;/h3&gt; &#xA;&lt;h4&gt;SparseUNet&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;Pointcept&lt;/em&gt; provides &lt;code&gt;SparseUNet&lt;/code&gt; implemented by &lt;code&gt;SpConv&lt;/code&gt; and &lt;code&gt;MinkowskiEngine&lt;/code&gt;. The SpConv version is recommended since SpConv is easy to install and faster than MinkowskiEngine. Meanwhile, SpConv is also widely applied in outdoor perception.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;SpConv (recommend)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The SpConv version &lt;code&gt;SparseUNet&lt;/code&gt; in the codebase was fully rewrite from &lt;code&gt;MinkowskiEngine&lt;/code&gt; version, example running script is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet val&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base&#xA;# ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base&#xA;# S3DIS&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base&#xA;# S3DIS (with normal)&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-spunet-v1m1-0-cn-base -n semseg-spunet-v1m1-0-cn-base&#xA;# SemanticKITTI&#xA;sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base&#xA;# nuScenes&#xA;sh scripts/train.sh -g 4 -d nuscenes -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base&#xA;# ModelNet40&#xA;sh scripts/train.sh -g 2 -d modelnet40 -c cls-spunet-v1m1-0-base -n cls-spunet-v1m1-0-base&#xA;&#xA;# ScanNet Data Efficient&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la20 -n semseg-spunet-v1m1-2-efficient-la20&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la50 -n semseg-spunet-v1m1-2-efficient-la50&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la100 -n semseg-spunet-v1m1-2-efficient-la100&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la200 -n semseg-spunet-v1m1-2-efficient-la200&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr1 -n semseg-spunet-v1m1-2-efficient-lr1&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr5 -n semseg-spunet-v1m1-2-efficient-lr5&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr10 -n semseg-spunet-v1m1-2-efficient-lr10&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr20 -n semseg-spunet-v1m1-2-efficient-lr20&#xA;&#xA;# Profile model run time&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-0-enable-profiler -n semseg-spunet-v1m1-0-enable-profiler&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MinkowskiEngine&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The MinkowskiEngine version &lt;code&gt;SparseUNet&lt;/code&gt; in the codebase was modified from the original MinkowskiEngine repo, and example running scripts are as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install MinkowskiEngine, refer &lt;a href=&#34;https://github.com/NVIDIA/MinkowskiEngine&#34;&gt;https://github.com/NVIDIA/MinkowskiEngine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Uncomment &#34;# from .sparse_unet import *&#34; in &#34;pointcept/models/__init__.py&#34;&#xA;# Uncomment &#34;# from .mink_unet import *&#34; in &#34;pointcept/models/sparse_unet/__init__.py&#34;&#xA;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base&#xA;# ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base&#xA;# S3DIS&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base&#xA;# SemanticKITTI&#xA;sh scripts/train.sh -g 2 -d semantic_kitti -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;OA-CNNs&lt;/h4&gt; &#xA;&lt;p&gt;Introducing Omni-Adaptive 3D CNNs (&lt;strong&gt;OA-CNNs&lt;/strong&gt;), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost. Without any self-attention modules, &lt;strong&gt;OA-CNNs&lt;/strong&gt; favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Issue related to &lt;strong&gt;OA-CNNs&lt;/strong&gt; can @Pbihao.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-oacnns-v1m1-0-base -n semseg-oacnns-v1m1-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Point Transformers&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;PTv3&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.10035&#34;&gt;PTv3&lt;/a&gt; is an efficient backbone model that achieves SOTA performances across indoor and outdoor scenarios. The full PTv3 relies on FlashAttention, while FlashAttention relies on CUDA 11.6 and above, make sure your local Pointcept environment satisfies the requirements.&lt;/p&gt; &#xA;&lt;p&gt;If you can not upgrade your local environment to satisfy the requirements (CUDA &amp;gt;= 11.6), then you can disable FlashAttention by setting the model parameter &lt;code&gt;enable_flash&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; and reducing the &lt;code&gt;enc_patch_size&lt;/code&gt; and &lt;code&gt;dec_patch_size&lt;/code&gt; to a level (e.g. 128).&lt;/p&gt; &#xA;&lt;p&gt;FlashAttention force disables RPE and forces the accuracy reduced to fp16. If you require these features, please disable &lt;code&gt;enable_flash&lt;/code&gt; and adjust &lt;code&gt;enable_rpe&lt;/code&gt;, &lt;code&gt;upcast_attention&lt;/code&gt; and&lt;code&gt;upcast_softmax&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Detailed instructions and experiment records (containing weights) are available on the &lt;a href=&#34;https://github.com/Pointcept/PointTransformerV3&#34;&gt;project repository&lt;/a&gt;. Example running scripts are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Scratched ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base&#xA;# PPT joint training (ScanNet + Structured3D) and evaluate in ScanNet&#xA;sh scripts/train.sh -g 8 -d scannet -c semseg-pt-v3m1-1-ppt-extreme -n semseg-pt-v3m1-1-ppt-extreme&#xA;&#xA;# Scratched ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base&#xA;# Fine-tuning from  PPT joint training (ScanNet + Structured3D) with ScanNet200&#xA;# PTV3_PPT_WEIGHT_PATH: Path to model weight trained by PPT multi-dataset joint training&#xA;# e.g. exp/scannet/semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v3m1-1-ppt-ft -n semseg-pt-v3m1-1-ppt-ft -w ${PTV3_PPT_WEIGHT_PATH}&#xA;&#xA;# Scratched ScanNet++&#xA;sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base&#xA;# Scratched ScanNet++ test&#xA;sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v3m1-1-submit -n semseg-pt-v3m1-1-submit&#xA;&#xA;&#xA;# Scratched S3DIS&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base&#xA;# an example for disbale flash_attention and enable rpe.&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v3m1-1-rpe -n semseg-pt-v3m1-0-rpe&#xA;# PPT joint training (ScanNet + S3DIS + Structured3D) and evaluate in ScanNet&#xA;sh scripts/train.sh -g 8 -d s3dis -c semseg-pt-v3m1-1-ppt-extreme -n semseg-pt-v3m1-1-ppt-extreme&#xA;# S3DIS 6-fold cross validation&#xA;# 1. The default configs are evaluated on Area_5, modify the &#34;data.train.split&#34;, &#34;data.val.split&#34;, and &#34;data.test.split&#34; to make the config evaluated on Area_1 ~ Area_6 respectively.&#xA;# 2. Train and evaluate the model on each split of areas and gather result files located in &#34;exp/s3dis/EXP_NAME/result/Area_x.pth&#34; in one single folder, noted as RECORD_FOLDER.&#xA;# 3. Run the following script to get S3DIS 6-fold cross validation performance:&#xA;export PYTHONPATH=./&#xA;python tools/test_s3dis_6fold.py --record_root ${RECORD_FOLDER}&#xA;&#xA;# Scratched nuScenes&#xA;sh scripts/train.sh -g 4 -d nuscenes -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base&#xA;# Scratched Waymo&#xA;sh scripts/train.sh -g 4 -d waymo -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base&#xA;&#xA;# More configs and exp records for PTv3 will be available soon.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Indoor semantic segmentation&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Additional Data&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Num GPUs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Val mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tensorboard&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Exp Record&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PTv3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ScanNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úó&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.6%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Pointcept/Pointcept/raw/main/configs/scannet/semseg-pt-v3m1-0-base.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tensorboard&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tree/main/scannet-semseg-pt-v3m1-0-base&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PTv3 + PPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ScanNet&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Pointcept/Pointcept/raw/main/configs/scannet/semseg-pt-v3m1-1-ppt-extreme.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tensorboard&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tree/main/scannet-semseg-pt-v3m1-1-ppt-extreme&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PTv3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ScanNet200&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úó&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Pointcept/Pointcept/raw/main/configs/scannet200/semseg-pt-v3m1-0-base.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tensorboard&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tree/main/scannet200-semseg-pt-v3m1-0-base&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PTv3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;S3DIS (Area5)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úó&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.6%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Pointcept/Pointcept/raw/main/configs/s3dis/semseg-pt-v3m1-0-rpe.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tensorboard&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tree/main/s3dis-semseg-pt-v3m1-0-rpe&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PTv3 + PPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;S3DIS (Area5)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úì&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.4%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Pointcept/Pointcept/raw/main/configs/s3dis/semseg-pt-v3m1-1-ppt-extreme.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tensorboard&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Pointcept/PointTransformerV3/tree/main/s3dis-semseg-pt-v3m1-1-ppt-extreme&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;*Released model weights are trained for v1.5.1, weights for v1.5.2 and later is still ongoing.&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;PTv2 mode2&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The original PTv2 was trained on 4 * RTX a6000 (48G memory). Even enabling AMP, the memory cost of the original PTv2 is slightly larger than 24G. Considering GPUs with 24G memory are much more accessible, I tuned the PTv2 on the latest Pointcept and made it runnable on 4 * RTX 3090 machines.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;PTv2 Mode2&lt;/code&gt; enables AMP and disables &lt;em&gt;Position Encoding Multiplier&lt;/em&gt; &amp;amp; &lt;em&gt;Grouped Linear&lt;/em&gt;. During our further research, we found that precise coordinates are not necessary for point cloud understanding (Replacing precise coordinates with grid coordinates doesn&#39;t influence the performance. Also, SparseUNet is an example). As for Grouped Linear, my implementation of Grouped Linear seems to cost more memory than the Linear layer provided by PyTorch. Benefiting from the codebase and better parameter tuning, we also relieve the overfitting problem. The reproducing performance is even better than the results reported in our paper.&lt;/p&gt; &#xA;&lt;p&gt;Example running scripts are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ptv2m2: PTv2 mode2, disable PEM &amp;amp; Grouped Linear, GPU memory cost &amp;lt; 24G (recommend)&#xA;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m2-3-lovasz -n semseg-pt-v2m2-3-lovasz&#xA;&#xA;# ScanNet test&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m2-1-submit -n semseg-pt-v2m2-1-submit&#xA;# ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;# ScanNet++&#xA;sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;# ScanNet++ test&#xA;sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v2m2-1-submit -n semseg-pt-v2m2-1-submit&#xA;# S3DIS&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;# SemanticKITTI&#xA;sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;# nuScenes&#xA;sh scripts/train.sh -g 4 -d nuscenes -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;PTv2 mode1&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;PTv2 mode1&lt;/code&gt; is the original PTv2 we reported in our paper, example running scripts are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ptv2m1: PTv2 mode1, Original PTv2, GPU memory cost &amp;gt; 24G&#xA;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m1-0-base -n semseg-pt-v2m1-0-base&#xA;# ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v2m1-0-base -n semseg-pt-v2m1-0-base&#xA;# S3DIS&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v2m1-0-base -n semseg-pt-v2m1-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;PTv1&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The original PTv1 is also available in our Pointcept codebase. I haven&#39;t run PTv1 for a long time, but I have ensured that the example running script works well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v1-0-base -n semseg-pt-v1-0-base&#xA;# ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v1-0-base -n semseg-pt-v1-0-base&#xA;# S3DIS&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v1-0-base -n semseg-pt-v1-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Stratified Transformer&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Additional requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch-points3d&#xA;# Fix dependence, caused by installing torch-points3d &#xA;pip uninstall SharedArray&#xA;pip install SharedArray==3.2.1&#xA;&#xA;cd libs/pointops2&#xA;python setup.py install&#xA;cd ../..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Uncomment &lt;code&gt;# from .stratified_transformer import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Refer &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/installation&#34;&gt;Optional Installation&lt;/a&gt; to install dependence.&lt;/li&gt; &#xA; &lt;li&gt;Training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# stv1m1: Stratified Transformer mode1, Modified from the original Stratified Transformer code.&#xA;# PTv2m2: Stratified Transformer mode2, My rewrite version (recommend).&#xA;&#xA;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-st-v1m2-0-refined -n semseg-st-v1m2-0-refined&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-st-v1m1-0-origin -n semseg-st-v1m1-0-origin&#xA;# ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-st-v1m2-0-refined -n semseg-st-v1m2-0-refined&#xA;# S3DIS&#xA;sh scripts/train.sh -g 4 -d s3dis -c semseg-st-v1m2-0-refined -n semseg-st-v1m2-0-refined&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;SPVCNN&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;SPVCNN&lt;/code&gt; is a baseline model of &lt;a href=&#34;https://github.com/mit-han-lab/spvnas&#34;&gt;SPVNAS&lt;/a&gt;, it is also a practical baseline for outdoor datasets.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install torchsparse:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# refer https://github.com/mit-han-lab/torchsparse&#xA;# install method without sudo apt install&#xA;conda install google-sparsehash -c bioconda&#xA;export C_INCLUDE_PATH=${CONDA_PREFIX}/include:$C_INCLUDE_PATH&#xA;export CPLUS_INCLUDE_PATH=${CONDA_PREFIX}/include:CPLUS_INCLUDE_PATH&#xA;pip install --upgrade git+https://github.com/mit-han-lab/torchsparse.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# SemanticKITTI&#xA;sh scripts/train.sh -g 2 -d semantic_kitti -c semseg-spvcnn-v1m1-0-base -n semseg-spvcnn-v1m1-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;OctFormer&lt;/h4&gt; &#xA;&lt;p&gt;OctFormer from &lt;em&gt;OctFormer: Octree-based Transformers for 3D Point Clouds&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Additional requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd libs&#xA;git clone https://github.com/octree-nn/dwconv.git&#xA;pip install ./dwconv&#xA;pip install ocnn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Uncomment &lt;code&gt;# from .octformer import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-octformer-v1m1-0-base -n semseg-octformer-v1m1-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Swin3D&lt;/h4&gt; &#xA;&lt;p&gt;Swin3D from &lt;em&gt;Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Additional requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. Install MinkEngine v0.5.4, follow readme in https://github.com/NVIDIA/MinkowskiEngine;&#xA;# 2. Install Swin3D, mainly for cuda operation:&#xA;cd libs&#xA;git clone https://github.com/microsoft/Swin3D.git&#xA;cd Swin3D&#xA;pip install ./&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Uncomment &lt;code&gt;# from .swin3d import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Pre-Training with the following example scripts (Structured3D preprocessing refer &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#structured3d&#34;&gt;here&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Structured3D + Swin-S&#xA;sh scripts/train.sh -g 4 -d structured3d -c semseg-swin3d-v1m1-0-small -n semseg-swin3d-v1m1-0-small&#xA;# Structured3D + Swin-L&#xA;sh scripts/train.sh -g 4 -d structured3d -c semseg-swin3d-v1m1-1-large -n semseg-swin3d-v1m1-1-large&#xA;&#xA;# Addition&#xA;# Structured3D + SpUNet&#xA;sh scripts/train.sh -g 4 -d structured3d -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base&#xA;# Structured3D + PTv2&#xA;sh scripts/train.sh -g 4 -d structured3d -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Fine-tuning with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet + Swin-S&#xA;sh scripts/train.sh -g 4 -d scannet -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-0-small -n semseg-swin3d-v1m1-0-small&#xA;# ScanNet + Swin-L&#xA;sh scripts/train.sh -g 4 -d scannet -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-1-large -n semseg-swin3d-v1m1-1-large&#xA;&#xA;# S3DIS + Swin-S (here we provide config support S3DIS normal vector)&#xA;sh scripts/train.sh -g 4 -d s3dis -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-0-small -n semseg-swin3d-v1m1-0-small&#xA;# S3DIS + Swin-L (here we provide config support S3DIS normal vector)&#xA;sh scripts/train.sh -g 4 -d s3dis -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-1-large -n semseg-swin3d-v1m1-1-large&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Context-Aware Classifier&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;Context-Aware Classifier&lt;/code&gt; is a segmentor that can further boost the performance of each backbone, as a replacement for &lt;code&gt;Default Segmentor&lt;/code&gt;. Training with the following example scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-cac-v1m1-0-spunet-base -n semseg-cac-v1m1-0-spunet-base&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-cac-v1m1-1-spunet-lovasz -n semseg-cac-v1m1-1-spunet-lovasz&#xA;sh scripts/train.sh -g 4 -d scannet -c semseg-cac-v1m1-2-ptv2-lovasz -n semseg-cac-v1m1-2-ptv2-lovasz&#xA;&#xA;# ScanNet200&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-cac-v1m1-0-spunet-base -n semseg-cac-v1m1-0-spunet-base&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-cac-v1m1-1-spunet-lovasz -n semseg-cac-v1m1-1-spunet-lovasz&#xA;sh scripts/train.sh -g 4 -d scannet200 -c semseg-cac-v1m1-2-ptv2-lovasz -n semseg-cac-v1m1-2-ptv2-lovasz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Instance Segmentation&lt;/h3&gt; &#xA;&lt;h4&gt;PointGroup&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dvlab-research/PointGroup&#34;&gt;PointGroup&lt;/a&gt; is a baseline framework for point cloud instance segmentation.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Additional requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c bioconda google-sparsehash &#xA;cd libs/pointgroup_ops&#xA;python setup.py install --include_dirs=${CONDA_PREFIX}/include&#xA;cd ../..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Uncomment &lt;code&gt;# from .point_group import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 4 -d scannet -c insseg-pointgroup-v1m1-0-spunet-base -n insseg-pointgroup-v1m1-0-spunet-base&#xA;# S3DIS&#xA;sh scripts/train.sh -g 4 -d scannet -c insseg-pointgroup-v1m1-0-spunet-base -n insseg-pointgroup-v1m1-0-spunet-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Pre-training&lt;/h3&gt; &#xA;&lt;h4&gt;Sonata&lt;/h4&gt; &#xA;&lt;p&gt;Follow the instruction &lt;a href=&#34;https://github.com/Pointcept/Pointcept/tree/main/pointcept/models/sonata&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Masked Scene Contrast (MSC)&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pre-training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 8 -d scannet -c pretrain-msc-v1m1-0-spunet-base -n pretrain-msc-v1m1-0-spunet-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Fine-tuning with the following example scripts:&lt;br /&gt; enable PointGroup (&lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointgroup&#34;&gt;here&lt;/a&gt;) before fine-tuning on instance segmentation task.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet20 Semantic Segmentation&#xA;sh scripts/train.sh -g 8 -d scannet -w exp/scannet/pretrain-msc-v1m1-0-spunet-base/model/model_last.pth -c semseg-spunet-v1m1-4-ft -n semseg-msc-v1m1-0f-spunet-base&#xA;# ScanNet20 Instance Segmentation (enable PointGroup before running the script)&#xA;sh scripts/train.sh -g 4 -d scannet -w exp/scannet/pretrain-msc-v1m1-0-spunet-base/model/model_last.pth -c insseg-pointgroup-v1m1-0-spunet-base -n insseg-msc-v1m1-0f-pointgroup-spunet-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Example log and weight: [&lt;a href=&#34;https://connecthkuhk-my.sharepoint.com/:u:/g/personal/wuxy_connect_hku_hk/EYvNV4XUJ_5Mlk-g15RelN4BW_P8lVBfC_zhjC_BlBDARg?e=UoGFWH&#34;&gt;Pretrain&lt;/a&gt;] [&lt;a href=&#34;https://connecthkuhk-my.sharepoint.com/:u:/g/personal/wuxy_connect_hku_hk/EQkDiv5xkOFKgCpGiGtAlLwBon7i8W6my3TIbGVxuiTttQ?e=tQFnbr&#34;&gt;Semseg&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Point Prompt Training (PPT)&lt;/h4&gt; &#xA;&lt;p&gt;PPT presents a multi-dataset pre-training framework, and it is compatible with various existing pre-training frameworks and backbones.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;PPT supervised joint training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet + Structured3d, validate on ScanNet (S3DIS might cause long data time, w/o S3DIS for a quick validation) &amp;gt;= 3090 * 8 &#xA;sh scripts/train.sh -g 8 -d scannet -c semseg-ppt-v1m1-0-sc-st-spunet -n semseg-ppt-v1m1-0-sc-st-spunet&#xA;sh scripts/train.sh -g 8 -d scannet -c semseg-ppt-v1m1-1-sc-st-spunet-submit -n semseg-ppt-v1m1-1-sc-st-spunet-submit&#xA;# ScanNet + S3DIS + Structured3d, validate on S3DIS (&amp;gt;= a100 * 8)&#xA;sh scripts/train.sh -g 8 -d s3dis -c semseg-ppt-v1m1-0-s3-sc-st-spunet -n semseg-ppt-v1m1-0-s3-sc-st-spunet&#xA;# SemanticKITTI + nuScenes + Waymo, validate on SemanticKITTI (bs12 &amp;gt;= 3090 * 4 &amp;gt;= 3090 * 8, v1m1-0 is still on tuning)&#xA;sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-ppt-v1m1-0-nu-sk-wa-spunet -n semseg-ppt-v1m1-0-nu-sk-wa-spunet&#xA;sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-ppt-v1m2-0-sk-nu-wa-spunet -n semseg-ppt-v1m2-0-sk-nu-wa-spunet&#xA;sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-ppt-v1m2-1-sk-nu-wa-spunet-submit -n semseg-ppt-v1m2-1-sk-nu-wa-spunet-submit&#xA;# SemanticKITTI + nuScenes + Waymo, validate on nuScenes (bs12 &amp;gt;= 3090 * 4; bs24 &amp;gt;= 3090 * 8, v1m1-0 is still on tuning))&#xA;sh scripts/train.sh -g 4 -d nuscenes -c semseg-ppt-v1m1-0-nu-sk-wa-spunet -n semseg-ppt-v1m1-0-nu-sk-wa-spunet&#xA;sh scripts/train.sh -g 4 -d nuscenes -c semseg-ppt-v1m2-0-nu-sk-wa-spunet -n semseg-ppt-v1m2-0-nu-sk-wa-spunet&#xA;sh scripts/train.sh -g 4 -d nuscenes -c semseg-ppt-v1m2-1-nu-sk-wa-spunet-submit -n semseg-ppt-v1m2-1-nu-sk-wa-spunet-submit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;PointContrast&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Preprocess and link ScanNet-Pair dataset (pair-wise matching with ScanNet raw RGB-D frame, ~1.5T):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# RAW_SCANNET_DIR: the directory of downloaded ScanNet v2 raw dataset.&#xA;# PROCESSED_SCANNET_PAIR_DIR: the directory of processed ScanNet pair dataset (output dir).&#xA;python pointcept/datasets/preprocessing/scannet/scannet_pair/preprocess.py --dataset_root ${RAW_SCANNET_DIR} --output_root ${PROCESSED_SCANNET_PAIR_DIR}&#xA;ln -s ${PROCESSED_SCANNET_PAIR_DIR} ${CODEBASE_DIR}/data/scannet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Pre-training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 8 -d scannet -c pretrain-msc-v1m1-1-spunet-pointcontrast -n pretrain-msc-v1m1-1-spunet-pointcontrast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Fine-tuning refer &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc&#34;&gt;MSC&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Contrastive Scene Contexts&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Preprocess and link ScanNet-Pair dataset (refer &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointcontrast&#34;&gt;PointContrast&lt;/a&gt;):&lt;/li&gt; &#xA; &lt;li&gt;Pre-training with the following example scripts:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ScanNet&#xA;sh scripts/train.sh -g 8 -d scannet -c pretrain-msc-v1m2-0-spunet-csc -n pretrain-msc-v1m2-0-spunet-csc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Fine-tuning refer &lt;a href=&#34;https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc&#34;&gt;MSC&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Pointcept&lt;/em&gt; is designed by &lt;a href=&#34;https://xywu.me/&#34;&gt;Xiaoyang&lt;/a&gt;, named by &lt;a href=&#34;https://github.com/yxlao&#34;&gt;Yixing&lt;/a&gt; and the logo is created by &lt;a href=&#34;https://julianjuaner.github.io/&#34;&gt;Yuechen&lt;/a&gt;. It is derived from &lt;a href=&#34;https://hszhao.github.io/&#34;&gt;Hengshuang&lt;/a&gt;&#39;s &lt;a href=&#34;https://github.com/hszhao/semseg&#34;&gt;Semseg&lt;/a&gt; and inspirited by several repos, e.g., &lt;a href=&#34;https://github.com/NVIDIA/MinkowskiEngine&#34;&gt;MinkowskiEngine&lt;/a&gt;, &lt;a href=&#34;https://github.com/charlesq34/pointnet2&#34;&gt;pointnet2&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmcv/tree/master/mmcv&#34;&gt;mmcv&lt;/a&gt;, and &lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;Detectron2&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dataease/SQLBot</title>
    <updated>2025-08-10T06:00:47Z</updated>
    <id>tag:github.com,2025-08-10:/dataease/SQLBot</id>
    <link href="https://github.com/dataease/SQLBot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png&#34; alt=&#34;SQLBot&#34; width=&#34;300&#34; /&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/dataease/SQLBot/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/dataease/SQLBot&#34; alt=&#34;Latest release&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dataease/SQLBot&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square&#34; alt=&#34;Stars&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/dataease/SQLbot&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads&#34; alt=&#34;Download&#34; /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; &#xA;&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; &#xA;&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨„ÄÇ&lt;br /&gt; Âú®ËøêË°å SQLBot ÂâçÔºåËØ∑Á°Æ‰øùÂ∑≤ÂÆâË£ÖÂ•Ω &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Docker&lt;/a&gt; Âíå &lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker Compose&lt;/a&gt;„ÄÇ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ÂàõÂª∫ÁõÆÂΩï&#xA;mkdir -p /opt/sqlbot&#xA;cd /opt/sqlbot&#xA;&#xA;# ‰∏ãËΩΩ docker-compose.yaml&#xA;curl -o docker-compose.yaml https://raw.githubusercontent.com/dataease/SQLBot/main/docker-compose.yaml&#xA;&#xA;# ÂêØÂä®ÊúçÂä°&#xA;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href=&#34;https://apps.fit2cloud.com/1panel&#34;&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBotÔºõ&lt;/p&gt; &#xA;&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; &#xA; &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; &#xA; &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; &#xA;&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; &#xA;&lt;img width=&#34;396&#34; height=&#34;396&#34; alt=&#34;contact_me_qr&#34; src=&#34;https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030&#34; /&gt; &#xA;&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  &#xA;&lt;img alt=&#34;q&amp;amp;a&#34; src=&#34;https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280&#34; /&gt;  &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#dataease/sqlbot&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dataease/dataease/&#34;&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1panel-dev/1panel/&#34;&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/1panel-dev/MaxKB/&#34;&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jumpserver/jumpserver/&#34;&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/halo-dev/halo/&#34;&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/metersphere/metersphere/&#34;&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href=&#34;https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE&#34;&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt;</summary>
  </entry>
</feed>