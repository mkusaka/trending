<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-10T01:43:41Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TabbyML/tabby</title>
    <updated>2023-04-10T01:43:41Z</updated>
    <id>tag:github.com,2023-04-10:/TabbyML/tabby</id>
    <link href="https://github.com/TabbyML/tabby" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Self-hosted AI coding assistant&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üêæ Tabby&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/TabbyML/tabby/docker.yml?label=docker%20image%20build&#34; alt=&#34;Docker build status&#34;&gt; &lt;a href=&#34;https://hub.docker.com/r/tabbyml/tabby&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/tabbyml/tabby&#34; alt=&#34;Docker pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/388154/229353706-230d70e1-7d09-48e2-a884-4da768bccf6f.png&#34; alt=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Self-hosted AI coding assistant. An opensource / on-prem alternative to GitHub Copilot.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; Tabby is still in the alpha phase&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Self-contained, with no need for a DBMS or cloud service&lt;/li&gt; &#xA; &lt;li&gt;Web UI for visualizing and configuration models and MLOps.&lt;/li&gt; &#xA; &lt;li&gt;OpenAPI interface, easy to integrate with existing infrastructure (e.g Cloud IDE).&lt;/li&gt; &#xA; &lt;li&gt;Consumer level GPU supports (FP-16 weight loading with various optimization).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/TabbyML/tabby&#34;&gt;&lt;img alt=&#34;Open in Spaces&#34; src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-md.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Demo&#34; src=&#34;https://user-images.githubusercontent.com/388154/230440226-9bc01d05-9f57-478b-b04d-81184eba14ca.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Tabby requires &lt;a href=&#34;https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/&#34;&gt;Pascal or newer&lt;/a&gt; NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;Before running Tabby, ensure the installation of the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;. We suggest using NVIDIA drivers that are compatible with CUDA version 11.8 or higher.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create data dir and grant owner to 1000 (Tabby run as uid 1000 in container)&#xA;mkdir -p data/hf_cache &amp;amp;&amp;amp; chown -R 1000 data&#xA;&#xA;docker run \&#xA;  --gpus all \&#xA;  -it --rm \&#xA;  -v &#34;./data:/data&#34; \&#xA;  -v &#34;./data/hf_cache:/home/app/.cache/huggingface&#34; \&#xA;  -p 5000:5000 \&#xA;  -e MODEL_NAME=TabbyML/J-350M \&#xA;  -e MODEL_BACKEND=triton \&#xA;  --name=tabby \&#xA;  tabbyml/tabby&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then query the server using &lt;code&gt;/v1/completions&lt;/code&gt; endpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST http://localhost:5000/v1/completions -H &#39;Content-Type: application/json&#39; --data &#39;{&#xA;    &#34;prompt&#34;: &#34;def binarySearch(arr, left, right, x):\n    mid = (left +&#34;&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provides an interactive playground in admin panel &lt;a href=&#34;http://localhost:5000/_admin&#34;&gt;localhost:5000/_admin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Skypilot&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/TabbyML/tabby/main/deployment/skypilot/README.md&#34;&gt;deployment/skypilot/README.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;API documentation&lt;/h2&gt; &#xA;&lt;p&gt;Tabby opens an FastAPI server at &lt;a href=&#34;https://localhost:5000&#34;&gt;localhost:5000&lt;/a&gt;, which embeds an OpenAPI documentation of the HTTP API.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Go to &lt;code&gt;development&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make dev-triton # Turn on triton backend (for cuda env developers)&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jiawei-ren/diffmimic</title>
    <updated>2023-04-10T01:43:41Z</updated>
    <id>tag:github.com,2023-04-10:/jiawei-ren/diffmimic</id>
    <link href="https://github.com/jiawei-ren/diffmimic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICLR 2023] DiffMimic: Efficient Motion Mimicking with Differentiable Physics https://arxiv.org/abs/2304.03274&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;DiffMimic: &lt;br&gt; Efficient Motion Mimicking with Differentiable Physics&lt;/h1&gt; &#xA; &lt;div&gt;&#xA;   Jiawei Ren&#xA;  &lt;sup&gt;*&lt;/sup&gt;‚ÄÉCunjun Yu&#xA;  &lt;sup&gt;*&lt;/sup&gt;‚ÄÉSiwei Chen‚ÄÉXiao Ma‚ÄÉLiang Pan‚ÄÉZiwei Liu&#xA;  &lt;sup&gt;‚Ä†&lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;   S-Lab, Nanyang Technological University‚ÄÉ National University of Singapore ‚ÄÉ&#xA;  &lt;br&gt; &#xA;  &lt;sup&gt;*&lt;/sup&gt;equal contribution &#xA;  &lt;br&gt; &#xA;  &lt;sup&gt;‚Ä†&lt;/sup&gt;corresponding author &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;strong&gt;ICLR 2023&lt;/strong&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/jiawei-ren/diffmimic/main/asset/teaser.gif&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;hr&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://diffmimic.github.io/&#34; target=&#34;_blank&#34;&gt;[Project Page]&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://openreview.net/forum?id=06mk-epSwZ&#34; target=&#34;_blank&#34;&gt;[Paper]&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://diffmimic-demo-main-g7h0i8.streamlit.app/&#34; target=&#34;_blank&#34;&gt;[Demo]&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://youtu.be/B0unbsvGsLc&#34; target=&#34;_blank&#34;&gt;[Video]&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;We implement DiffMimic with &lt;a href=&#34;https://github.com/google/brax&#34;&gt;Brax&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;img src=&#34;https://github.com/google/brax/raw/main/docs/img/brax_logo.gif&#34; width=&#34;158&#34; height=&#34;40&#34; alt=&#34;BRAX&#34;&gt; &#xA; &lt;p&gt;Brax is a fast and fully differentiable physics engine used for research and development of robotics, human perception, materials science, reinforcement learning, and other simulation-heavy applications.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;An environment &lt;code&gt;mimic_env&lt;/code&gt; is implemented for training and benchmarking. &lt;code&gt;mimic_env&lt;/code&gt; now includes the following characters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiawei-ren/diffmimic/main/diffmimic/mimic_envs/system_configs/HUMANOID.py&#34;&gt;HUMANOID&lt;/a&gt;: &lt;a href=&#34;https://github.com/nv-tlabs/ASE/raw/main/ase/data/assets/mjcf/amp_humanoid.xml&#34;&gt;AMP&lt;/a&gt;-formatted humanoid, used for acrobatics skills.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiawei-ren/diffmimic/main/diffmimic/mimic_envs/system_configs/SMPL.py&#34;&gt;SMPL&lt;/a&gt;: &lt;a href=&#34;https://smpl.is.tue.mpg.de/&#34;&gt;SMPL&lt;/a&gt;-formatted humanoid, used for mocap data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jiawei-ren/diffmimic/main/diffmimic/mimic_envs/system_configs/SWORDSHIELD.py&#34;&gt;SWORDSHIELD&lt;/a&gt;: &lt;a href=&#34;https://github.com/nv-tlabs/ASE/raw/main/ase/data/assets/mjcf/amp_humanoid_sword_shield.xml&#34;&gt;ASE&lt;/a&gt;-formatted humanoid, used for REALLUSION sword-shield motion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More characters are on the way.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n diffmimic python==3.9&#xA;conda activate diffmimic&#xA;&#xA;pip install --upgrade pip&#xA;pip install --upgrade &#34;jax[cuda]==0.3.20&#34; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html&#xA;pip install brax==0.0.15&#xA;pip install flax==0.6.0&#xA;pip install streamlit  &#xA;pip install tensorflow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python mimic.py --config configs/AMP/backflip.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Visualize&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;streamlit run visualize.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful for your research, please consider citing the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{ren2023diffmimic,&#xA;  author    = {Ren, Jiawei and Yu, Cunjun and Chen, Siwei and Ma, Xiao and Pan, Liang and Liu, Ziwei},&#xA;  title     = {DiffMimic: Efficient Motion Mimicking with Differentiable Physics},&#xA;  journal   = {ICLR},&#xA;  year      = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Differentiable physics simulation is done by &lt;a href=&#34;https://github.com/google/brax&#34;&gt;Brax&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Early version of the code is heavily based on &lt;a href=&#34;https://github.com/sail-sg/ILD&#34;&gt;Imitation Learning via Differentiable Physics (ILD)&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Motion files are borrowed from &lt;a href=&#34;https://github.com/xbpeng/DeepMimic&#34;&gt;DeepMimic&lt;/a&gt;, &lt;a href=&#34;https://github.com/nv-tlabs/ASE&#34;&gt;ASE&lt;/a&gt;, &lt;a href=&#34;https://amass.is.tue.mpg.de/&#34;&gt;AMASS&lt;/a&gt;, and &lt;a href=&#34;https://google.github.io/aistplusplus_dataset/factsfigures.html&#34;&gt;AIST++&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Characters are borrowed from &lt;a href=&#34;https://github.com/xbpeng/DeepMimic&#34;&gt;DeepMimic&lt;/a&gt; and &lt;a href=&#34;https://github.com/nv-tlabs/ASE&#34;&gt;ASE&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The work is inspired by valuable insights from &lt;a href=&#34;https://montreal.ubisoft.com/en/supertrack-motion-tracking-for-physically-simulated-characters-using-supervised-learning/&#34;&gt;SuperTrack&lt;/a&gt; and &lt;a href=&#34;https://milkpku.github.io/project/spacetime.html&#34;&gt;Spacetime Bound&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>logspace-ai/langflow</title>
    <updated>2023-04-10T01:43:41Z</updated>
    <id>tag:github.com,2023-04-10:/logspace-ai/langflow</id>
    <link href="https://github.com/logspace-ai/langflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚õìÔ∏è LangFlow is a UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‚õìÔ∏è LangFlow&lt;/h1&gt; &#xA;&lt;p&gt;~ A User Interface For &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; ~&lt;/p&gt; &#xA;&lt;p&gt; &lt;img alt=&#34;GitHub Contributors&#34; src=&#34;https://img.shields.io/github/contributors/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Last Commit&#34; src=&#34;https://img.shields.io/github/last-commit/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;&#34; src=&#34;https://img.shields.io/github/repo-size/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Issues&#34; src=&#34;https://img.shields.io/github/issues/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;GitHub Pull Requests&#34; src=&#34;https://img.shields.io/github/issues-pr/logspace-ai/langflow&#34;&gt; &lt;img alt=&#34;Github License&#34; src=&#34;https://img.shields.io/github/license/logspace-ai/langflow&#34;&gt; &lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/logspace-ai/langflow&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://github.com/logspace-ai/langflow/raw/main/img/langflow-demo.gif?raw=true&#34;&gt;&lt;/a&gt; &#xA;&lt;p&gt;LangFlow is a GUI for &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt;, designed with &lt;a href=&#34;https://github.com/wbkd/react-flow&#34;&gt;react-flow&lt;/a&gt; to provide an effortless way to experiment and prototype flows with drag-and-drop components and a chat box.&lt;/p&gt; &#xA;&lt;h2&gt;üì¶ Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can install LangFlow from pip:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install langflow&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Next, run:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;langflow&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üé® Creating Flows&lt;/h2&gt; &#xA;&lt;p&gt;Creating flows with LangFlow is easy. Simply drag sidebar components onto the canvas and connect them together to create your pipeline. LangFlow provides a range of &lt;a href=&#34;https://langchain.readthedocs.io/en/latest/reference.html&#34;&gt;LangChain components&lt;/a&gt; to choose from, including LLMs, prompt serializers, agents, and chains.&lt;/p&gt; &#xA;&lt;p&gt;Explore by editing prompt parameters, link chains and agents, track an agent&#39;s thought process, and export your flow.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re done, you can export your flow as a JSON file to use with LangChain. To do so, click the &#34;Export&#34; button in the top right corner of the canvas, then in Python, you can load the flow with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langflow import load_flow_from_json&#xA;&#xA;flow = load_flow_from_json(&#34;path/to/flow.json&#34;)&#xA;# Now you can use it like any chain&#xA;flow(&#34;Hey, have you heard of LangFlow?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üëã Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from developers of all levels to our open-source project on GitHub. If you&#39;d like to contribute, please check our &lt;a href=&#34;https://raw.githubusercontent.com/logspace-ai/langflow/dev/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; and help make LangFlow more accessible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#logspace-ai/langflow&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=logspace-ai/langflow&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;LangFlow is released under the MIT License. See the LICENSE file for details.&lt;/p&gt;</summary>
  </entry>
</feed>