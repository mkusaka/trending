<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-02T01:37:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lucidrains/make-a-video-pytorch</title>
    <updated>2022-10-02T01:37:24Z</updated>
    <id>tag:github.com,2022-10-02:/lucidrains/make-a-video-pytorch</id>
    <link href="https://github.com/lucidrains/make-a-video-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Make-A-Video, new SOTA text to video generator from Meta AI, in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/make-a-video-pytorch/main/make-a-video.png&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Make-A-Video - Pytorch (wip)&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://makeavideo.studio/&#34;&gt;Make-A-Video&lt;/a&gt;, new SOTA text to video generator from Meta AI, in Pytorch. They combine pseudo-3d convolutions (axial convolutions) and temporal attention and show much better temporal fusion.&lt;/p&gt; &#xA;&lt;p&gt;The pseudo-3d convolutions isn&#39;t a new concept. It has been explored before in other contexts, say for protein contact prediction as &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.08.04.502748v2.full&#34;&gt;&#34;dimensional hybrid residual networks&#34;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The gist of the paper comes down to, take a SOTA text-to-image model (here they use DALL-E2, but the same learning points would easily apply to Imagen), make a few minor modifications for &lt;a href=&#34;https://arxiv.org/abs/2204.03458&#34;&gt;attention across time&lt;/a&gt; and other ways to skimp on the compute cost, do frame interpolation correctly, get a great video model out.&lt;/p&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{Singer2022,&#xA;    author  = {Uriel Singer},&#xA;    url     = {https://makeavideo.studio/Make-A-Video.pdf}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>nv-tlabs/GET3D</title>
    <updated>2022-10-02T01:37:24Z</updated>
    <id>tag:github.com,2022-10-02:/nv-tlabs/GET3D</id>
    <link href="https://github.com/nv-tlabs/GET3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images (NeurIPS 2022)&lt;br&gt;&lt;sub&gt;Official PyTorch implementation &lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/docs/assets/get3d_model.png&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;http://www.cs.toronto.edu/~jungao/&#34;&gt;Jun Gao&lt;/a&gt; , &lt;a href=&#34;http://www.cs.toronto.edu/~shenti11/&#34;&gt;Tianchang Shen&lt;/a&gt; , &lt;a href=&#34;http://www.cs.toronto.edu/~zianwang/&#34;&gt;Zian Wang&lt;/a&gt;, &lt;a href=&#34;http://www.cs.toronto.edu/~wenzheng/&#34;&gt;Wenzheng Chen&lt;/a&gt;, &lt;a href=&#34;https://kangxue.org/&#34;&gt;Kangxue Yin&lt;/a&gt; , &lt;a href=&#34;https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&amp;amp;hl=en&#34;&gt;Daiqing Li&lt;/a&gt;, &lt;a href=&#34;https://orlitany.github.io/&#34;&gt;Or Litany&lt;/a&gt;, &lt;a href=&#34;https://zgojcic.github.io/&#34;&gt;Zan Gojcic&lt;/a&gt;, &lt;a href=&#34;https://www.cs.toronto.edu/~fidler/&#34;&gt;Sanja Fidler&lt;/a&gt; &lt;br&gt; &lt;strong&gt;&lt;a href=&#34;https://nv-tlabs.github.io/GET3D/assets/paper.pdf&#34;&gt;Paper&lt;/a&gt; , &lt;a href=&#34;https://nv-tlabs.github.io/GET3D/&#34;&gt;Project Page&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: &lt;em&gt;As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/docs/assets/teaser_result.jpg&#34; alt=&#34;Teaser Results&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2022-09-29: Code released!&lt;/li&gt; &#xA; &lt;li&gt;2022-09-22: Code will be uploaded next week!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We recommend Linux for performance and compatibility reasons.&lt;/li&gt; &#xA; &lt;li&gt;8 high-end NVIDIA GPUs. We have done all testing and development using V100 or A100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;64-bit Python 3.8 and PyTorch 1.9.0. See &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt; for PyTorch install instructions.&lt;/li&gt; &#xA; &lt;li&gt;CUDA toolkit 11.1 or later. (Why is a separate CUDA toolkit installation required? We use the custom CUDA extensions from the StyleGAN3 repo. Please see &lt;a href=&#34;https://github.com/NVlabs/stylegan3/raw/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary&#34;&gt;Troubleshooting&lt;/a&gt;) .&lt;/li&gt; &#xA; &lt;li&gt;We also recommend to install Nvdiffrast following instructions from &lt;a href=&#34;https://github.com/NVlabs/nvdiffrast&#34;&gt;official repo&lt;/a&gt;, and install &lt;a href=&#34;https://github.com/NVIDIAGameWorks/kaolin&#34;&gt;Kaolin&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/install_get3d.sh&#34;&gt;script&lt;/a&gt; to install packages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Server usage through Docker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build Docker image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;chmod +x make_image.sh&#xA;./make_image.sh get3d:v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start an interactive docker container: &lt;code&gt;docker run --gpus device=all -it --rm -v YOUR_LOCAL_FOLDER:MOUNT_FOLDER -it get3d:v1 bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparing datasets&lt;/h2&gt; &#xA;&lt;p&gt;GET3D is trained on synthetic dataset. We provide rendering scripts for Shapenet. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/render_shapenet_data/README.md&#34;&gt;readme&lt;/a&gt; to download shapenet dataset and render it.&lt;/p&gt; &#xA;&lt;h2&gt;Train the model&lt;/h2&gt; &#xA;&lt;h4&gt;Clone the gitlab code and necessary files:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd YOUR_CODE_PARH&#xA;git clone git@github.com:nv-tlabs/GET3D.git&#xA;cd GET3D; mkdir cache; cd cache&#xA;wget https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train the model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd YOUR_CODE_PATH &#xA;export PYTHONPATH=$PWD:$PYTHONPATH&#xA;export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train on the unified generator on cars, motorbikes or chair (Improved generator in Appendix):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=400 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If want to train on seperate generators (main Figure in the paper):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=80 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 0&#xA;python train_3d.py --outdir=PATH_TO_LOG --data=PATH_TO_RENDER_IMG --camera_path PATH_TO_RENDER_CAMERA --gpus=8 --batch=32 --gamma=3200 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If want to debug the model first, reduce the number of gpus to 1 and batch size to 4 via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--gpus=1 --batch=4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Inference on a pretrained model for visualization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inference could operate on a single GPU with 16 GB memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_3d.py --outdir=save_inference_results/shapenet_car  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_car  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH&#xA;python train_3d.py --outdir=save_inference_results/shapenet_chair  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_chair  --dmtet_scale 0.8  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH&#xA;python train_3d.py --outdir=save_inference_results/shapenet_motorbike  --gpus=1 --batch=4 --gamma=40 --data_camera_mode shapenet_motorbike  --dmtet_scale 1.0  --use_shapenet_split 1  --one_3d_generator 1  --fp32 0 --inference_vis 1 --resume_pretrain MODEL_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To generate mesh with textures, add one option to the inference command: &lt;code&gt;--inference_to_generate_textured_mesh 1&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To generate the results with latent code interpolation, add one option to the inference command: &lt;code&gt;--inference_save_interpolation 1&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evluation metrics&lt;/h3&gt; &#xA;&lt;h5&gt;Compute FID&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To evaluate the model with FID metric, add one option to the inference command: &lt;code&gt;--inference_compute_fid 1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Compute COV &amp;amp; MMD scores for LFD &amp;amp; CD&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First generate 3D objects for evaluation, add one option to the inference command: &lt;code&gt;--inference_generate_geo 1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Following &lt;a href=&#34;https://raw.githubusercontent.com/nv-tlabs/GET3D/master/evaluation_scripts/README.md&#34;&gt;README&lt;/a&gt; to compute metrics.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright © 2022, NVIDIA Corporation &amp;amp; affiliates. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;This work is made available under the &lt;a href=&#34;https://github.com/nv-tlabs/GET3D/raw/master/LICENSE.txt&#34;&gt;Nvidia Source Code License&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;Broader Information&lt;/h2&gt; &#xA;&lt;p&gt;GET3D builds upon several previous works:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nv-tlabs.github.io/DefTet/&#34;&gt;Learning Deformable Tetrahedral Meshes for 3D Reconstruction (NeurIPS 2020)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nv-tlabs.github.io/DMTet/&#34;&gt;Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis (NeurIPS 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nvlabs.github.io/nvdiffrec/&#34;&gt;Extracting Triangular 3D Models, Materials, and Lighting From Images (CVPR 2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nv-tlabs.github.io/DIBRPlus/&#34;&gt;DIB-R++: Learning to Predict Lighting and Material with a Hybrid Differentiable Renderer (NeurIPS 2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nvlabs.github.io/nvdiffrast/&#34;&gt;Nvdiffrast – Modular Primitives for High-Performance Differentiable Rendering (SIGRAPH Asia 2020)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@inproceedings{gao2022get3d,&#xA;title={GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images},&#xA;author={Jun Gao and Tianchang Shen and Zian Wang and Wenzheng Chen and Kangxue Yin&#xA;and Daiqing Li and Or Litany and Zan Gojcic and Sanja Fidler},&#xA;booktitle={Advances In Neural Information Processing Systems},&#xA;year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Ildaron/Laser_control</title>
    <updated>2022-10-02T01:37:24Z</updated>
    <id>tag:github.com,2022-10-02:/Ildaron/Laser_control</id>
    <link href="https://github.com/Ildaron/Laser_control" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-Source Laser for control mosquito, weed, and pest&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Laser device for neutralizing - mosquitoes, asian hornet, weeds and pests (Open-source)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/intent/tweet?text=Laser%20for%20%20control%20mosquitoes%20%20and%20any%20insect%20&amp;amp;url=https://github.com/Ildaron/Laser_control&amp;amp;hashtags=laser,mosquitoes,python,opensource&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/Ildaron/Laser_control/raw/master/license.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Licence-FREE-blue&#34; alt=&#34;Hardware demonstrations&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://youtu.be/2BKtM5cxOik&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Youtube-view-red&#34; alt=&#34;Hardware demonstrations&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here I will post information for creating a laser device.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/gen_view.JPG&#34; alt=&#34;alt tag&#34; title=&#34;general view&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#a-warning&#34;&gt;A warning!!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control/raw/master/README.md#how-it-works&#34;&gt;How It Works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#general-information&#34;&gt;General information&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#dimensions&#34;&gt;Dimensions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#galvanometer-setting&#34;&gt;Galvanometer setting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control/raw/master/README.md#determining-the-coordinates-of-an-object&#34;&gt;Determining the coordinates of an object&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#demonstrations&#34;&gt;Demonstrations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#we-need-more-fps&#34;&gt;We need more FPS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control/raw/master/README.md#security-questions&#34;&gt;Security questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#contacts&#34;&gt;Discussion and Future work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#publication-and-citation&#34;&gt;Publication and Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ildaron/Laser_control#contacts&#34;&gt;Contacts&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;A warning!!&lt;/h3&gt; &#xA;&lt;h4&gt;Don&#39;t use the power laser!&lt;/h4&gt; &#xA;&lt;p&gt;The main limiting factor in the development of this technology is the danger of the laser may damage the eyes. The laser can enter a blood vessel and clog it, it can get into a blind spot where nerves from all over the eye go to the brain, you can burn out a line of &#34;pixels&#34; And then the damaged retina can begin to flake off, and this is the path to complete and irreversible loss of vision. This is dangerous because a person may not notice at the beginning of damage from a laser hit: there are no pain receptors there, the brain completes objects in damaged areas (remapping of dead pixels), and only when the damaged area becomes large enough person starts to notice that some objects not visible. We can develop additional security systems, such as human detection, audio sensors, etc. But in any case, we are not able to make the installation 100% safe, since even a laser can be reflected and damage the eye of a person who is not in the field of view of the device and at a distant distance. Therefore, this technology should not be used at home. My strong recommendation - don&#39;t use the power laser! I recommend making a device that will track an object using a safe laser pointer.&lt;/p&gt; &#xA;&lt;h4&gt;How It Works&lt;/h4&gt; &#xA;&lt;p&gt;To detect x,y coordinates initially we used Haar cascades in RaspberryPI after that yolov4-tiny in Jetson nano. For Y coordinates - stereo vision.&lt;br&gt; Calculation necessary value for the angle of mirrors.&lt;br&gt; RaspberryPI/JetsonNano by SPI sends a command for galvanometer via DAC mcp4922. Electrical scheme (&lt;a href=&#34;https://github.com/Ildaron/Laser_control/tree/master/2.Jetson_code/2.1_mirror_control&#34;&gt;here&lt;/a&gt;). From mcp4922 bibolar analog signal go to amplifair. Finally, we have -12 and + 12 V for control positions of the mirrors.&lt;/p&gt; &#xA;&lt;h4&gt;General information&lt;/h4&gt; &#xA;&lt;p&gt;The principle of operation&lt;br&gt; &lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/scheme...bmp&#34; alt=&#34;alt tag&#34; title=&#34;general view&#34;&gt;&lt;br&gt; Single board computer to processes the digital signal from the camera and determines positioning to the object, and transmits the digital signal to the analog display - 3, where digital-to-analog converts the signal to the range of 0-5V. Using a board with an operational amplifier, we get a bipolar voltage, from which the boards with the motor driver for the galvanometer are powered - 4, from where the signal goes to galvanometers -7. The galvanometer uses mirrors to change the direction of the laser - 6. The system is powered by the power supply - 5. Cameras 2 determine the distance to the object. The camera detects mosquito and transmits data to the galvanometer, which sets the mirrors in the correct position, and then the laser turns on.&lt;/p&gt; &#xA;&lt;h3&gt;Dimensions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/dimension.bmp&#34; alt=&#34;alt tag&#34; title=&#34;general view&#34;&gt;&lt;br&gt; 1 - PI cameras, 2 - galvanometer, 3 - Jetson nano, 4 - adjusting the position to the object, 5 - laser device, 6 - power supply, 7 - galvanometer driver boards, 8 - analog conversion boards&lt;/p&gt; &#xA;&lt;h4&gt;Galvanometer setting&lt;/h4&gt; &#xA;&lt;p&gt;In practice, the maximum deflection angle of the mirrors is set at the factory, but before use, it is necessary to check, for example, according to the documentation, our galvanometer had a step width of 30, but as it turned out we have only 20 &lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/galv_angle.bmp&#34; alt=&#34;alt tag&#34; title=&#34;general view&#34;&gt;&lt;br&gt; Maximum and minimum positions of galvanometer mirrors:&lt;br&gt; a - lower position - 350 for x mirror;&lt;br&gt; b - upper position - 550 for x mirror;&lt;br&gt; c - lower position - 00 for y mirror;&lt;br&gt; d - upper position - 250 for y mirror;&lt;/p&gt; &#xA;&lt;h3&gt;Determining the coordinates of an object&lt;/h3&gt; &#xA;&lt;h4&gt;X,Y - coordinate&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/detect_moment.bmp&#34; alt=&#34;alt tag&#34; title=&#34;Example of result for Fast Fourier  transform&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Z-coordinate&lt;/h4&gt; &#xA;&lt;p&gt;We created GUI, source &lt;a href=&#34;https://github.com/Ildaron/OpenCV-stereovision-tuner-for-windows&#34;&gt;here&lt;/a&gt;. At the expense of computer vision, the position of the object in the X, Y plane is determined - based on which its ROI area is taken. Then we use stereo vision to compile a depth map and for a given ROI with the NumPy library tool - np.average we calculated the average value for the pixels of this area, which will allow us to calculate the distance to the object.&lt;br&gt; &lt;img src=&#34;https://github.com/Ildaron/OpenCV-stereovision-tuner-for-windows/raw/master/pic.2.bmp&#34; alt=&#34;alt tag&#34; title=&#34;Example of result for Fast Fourier  transform&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can find more detail in the published paper in preprint - &lt;a href=&#34;https://www.preprints.org/manuscript/202104.0282/v1&#34;&gt;Low-Cost Stereovision System (Disparity Map) For Few Dollars&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Determining the angle of galvanometer mirror&lt;/h3&gt; &#xA;&lt;h4&gt;angle of galvanometer mirror theory&lt;/h4&gt; &#xA;&lt;p&gt;The laser beam obeys all the optical laws of physics, therefore, depending on the design of the galvanometer, the required angle of inclination of the mirror – α, can be calculated through the geometrical formulas. In our case, through the tangent of the angle α, where it is equal to the ratio of the opposing side – X(Y) (position calculated by deep learning) to the adjacent side - Z (calculated by stereo vision).&lt;br&gt; &lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/Z_position.bmp&#34; alt=&#34;alt tag&#34; title=&#34;general view&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;angle of galvanometer mirror practice&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/z_practice.bmp&#34; alt=&#34;alt tag&#34; title=&#34;general view&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;We need more FPS&lt;/h4&gt; &#xA;&lt;p&gt;For single boards, computers are actual problems with FPS. For one object with Jetson was reached the next result for the Yolov4-tiny model.&lt;/p&gt; &#xA;&lt;p&gt;Framework&lt;br&gt; with Keras: 4-5 FPS&lt;br&gt; with Darknet: 12-15 FPS&lt;br&gt; with Darknet Tensor RT: 24-27 FPS&lt;br&gt; with Darknet DeepStream: 23-26 FPS&lt;br&gt; with tkDNN: 30-35 FPS&lt;/p&gt; &#xA;&lt;p&gt;You can find more detail in the published paper in arxiv - &lt;a href=&#34;https://arxiv.org/abs/2107.12148&#34;&gt;Increasing FPS for single board computers and embedded computers in 2021 (Jetson nano and YOVOv4-tiny). Practice and review&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demonstrations&lt;/h3&gt; &#xA;&lt;p&gt;In this video - a laser (the red point) tries to catch a yellow LED. It is an adjusting process but in fact, instead, a yellow LED can be a mosquito, and instead, the red laser can be a powerful laser.&lt;br&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=2BKtM5cxOik&#34;&gt;&lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/demonstration.bmp&#34; alt=&#34;Hardware demonstrations&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the video below, you can see, how the YoloV4tiny detected a cockroach and after that - turn on the laser and send a signal to the galvanometer to set the position of the mirror. When using a more powerful laser, the efficiency is much higher. But the video cannot be shot with a powerful laser, the light is too bright.&lt;br&gt; &lt;a href=&#34;https://youtu.be/VwZunUqHbiU&#34;&gt;&lt;img src=&#34;https://github.com/Ildaron/Laser_control/raw/master/Supplementary%20files/coac1.bmp&#34; alt=&#34;Hardware demonstrations&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Security questions&lt;/h3&gt; &#xA;&lt;p&gt;An additional device - a security module that will turn off the laser:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use additional cameras to fix people&lt;/li&gt; &#xA; &lt;li&gt;Audio sensors to capture voice and noise&lt;/li&gt; &#xA; &lt;li&gt;To mechanically shoot down the laser&lt;/li&gt; &#xA; &lt;li&gt;To use a thermal camera if there is any warm effect, turn it off - this is probably also possible to protect against fires consider not to overheat.&lt;/li&gt; &#xA; &lt;li&gt;Teach the system to record the process of laser reflection from any random glass or other mirror surfaces (maybe before turning on the power laser - for checking turn on the simple laser).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Discussion and Future work&lt;/h3&gt; &#xA;&lt;p&gt;We can try light up the room to reflect the mosquito - and then use the library functions - OpenCV in range or haar cascades to detect the object. With a bright background, they will be detected without problems. This is for low-power single-board computers - Raspberry, Orange, Banana, etc. For jetson nano, we can use yolov4-tiny which, using the tkDNN library, is able to give 30-35 FPS&lt;/p&gt; &#xA;&lt;h4&gt;Research laser effect&lt;/h4&gt; &#xA;&lt;p&gt;Use a lower the laser power as much as possible. The laser should burn the wings of mosquitos but should be safe for the eyes. That is to do research on the topic of laser power, laser wavelength, and their efficiency for mosquitos. This is for safety, the lower the power, the better.&lt;/p&gt; &#xA;&lt;h4&gt;Remote control&lt;/h4&gt; &#xA;&lt;p&gt;Laser control on a stationary computer. The IP camera installed next to the laser only transmits video to the computer, and the computer already analyzes it on a powerful processor video card and transmits back coordinates for the laser via Wi-Fi. In this case, we can use very powerful computing processors.&lt;/p&gt; &#xA;&lt;h4&gt;PCB boards&lt;/h4&gt; &#xA;&lt;p&gt;Make the device completely on our electronic boards. It is a galvanometer for a laser show and changes positions 20,000 times per second, which is why there are such powerful and big drivers for motors. It is useful to make a small PCB board to change the position of the laser only 200 times per second. In finally, so to speak, the pocket version.&lt;/p&gt; &#xA;&lt;h4&gt;Publication and Citation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ildar, R. (2021). Machine vision for low-cost remote control of mosquitoes by power laser. Journal of Real-Time Image Processing&lt;br&gt; availabe &lt;a href=&#34;https://www.researchgate.net/publication/349226713_Machine_vision_for_low-cost_remote_control_of_mosquitoes_by_power_laser&#34;&gt;here&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.1007/s11554-021-01079-x&#34;&gt;https://doi.org/10.1007/s11554-021-01079-x&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rakhmatulin I, Andreasen C. (2020). A Concept of a Compact and Inexpensive Device for Controlling Weeds with Laser Beams. Agronomy&lt;br&gt; availabe &lt;a href=&#34;https://www.mdpi.com/2073-4395/10/10/1616&#34;&gt;here&lt;/a&gt; &lt;a href=&#34;https://doi.org/10.3390/agronomy10101616&#34;&gt;https://doi.org/10.3390/agronomy10101616&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rakhmatuiln I, Kamilaris A, Andreasen C. Deep Neural Networks to Detect Weeds from Crops in Agricultural Environments in Real-Time: A Review. Remote Sensing. 2021; 13(21):4486. &lt;a href=&#34;https://doi.org/10.3390/rs13214486&#34;&gt;https://doi.org/10.3390/rs13214486&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Contacts&lt;/h4&gt; &#xA;&lt;p&gt;For any questions write to me by mail - &lt;a href=&#34;mailto:ildarr2016@gmail.com&#34;&gt;ildarr2016@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>