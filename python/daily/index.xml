<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-24T01:35:26Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>meituan/YOLOv6</title>
    <updated>2022-06-24T01:35:26Z</updated>
    <id>tag:github.com,2022-06-24:/meituan/YOLOv6</id>
    <link href="https://github.com/meituan/YOLOv6" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YOLOv6: a single-stage object detection framework dedicated to industrial application.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YOLOv6&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;YOLOv6 is a single-stage object detection framework dedicated to industrial application, with hardware-friendly efficient design and high performance.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/assets/picture.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;YOLOv6-nano achieves 35.0 mAP on COCO val2017 dataset with 1242 FPS on T4 using TensorRT FP16 for bs32 inference, and YOLOv6-s achieves 43.1 mAP on COCO val2017 dataset with 520 FPS on T4 using TensorRT FP16 for bs32 inference.&lt;/p&gt; &#xA;&lt;p&gt;YOLOv6 is composed of the following methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hardware-friendly Design for Backbone and Neck&lt;/li&gt; &#xA; &lt;li&gt;Efficient Decoupled Head with SIoU Loss&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Coming soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOv6 m/l/x model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Deployment for MNN/TNN/NCNN/CoreML...&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Quantization tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/meituan/YOLOv6&#xA;cd YOLOv6&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;First, download a pretrained model from the YOLOv6 release&lt;/p&gt; &#xA;&lt;p&gt;Second, run inference with &lt;code&gt;tools/infer.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/infer.py --weights yolov6s.pt --source [img.jpg / imgdir]&#xA;                                yolov6n.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Single GPU&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/train.py --batch 256 --conf configs/yolov6s.py --data data/coco.yaml --device 0&#xA;                                         configs/yolov6n.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Multi GPUs (DDP mode recommended)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m torch.distributed.launch --nproc_per_node 8 tools/train.py --batch 256 --conf configs/yolov6s.py --data data/coco.yaml --device 0,1,2,3,4,5,6,7&#xA;                                                                                        configs/yolov6n.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;conf: select config file to specify network/optimizer/hyperparameters&lt;/li&gt; &#xA; &lt;li&gt;data: prepare &lt;a href=&#34;http://cocodataset.org&#34;&gt;COCO&lt;/a&gt; dataset and specify dataset paths in data.yaml&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Reproduce mAP on COCO val2017 dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/eval.py --data data/coco.yaml  --batch 32 --weights yolov6s.pt --task val&#xA;                                                                 yolov6n.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/deploy/ONNX&#34;&gt;ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/deploy/OpenVINO&#34;&gt;OpenVINO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tutorials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/docs/Train_custom_data.md&#34;&gt;Train custom data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/docs/Test_speed.md&#34;&gt;Test speed&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Speed&lt;sup&gt;V100&lt;br&gt;fp16 b32 &lt;br&gt;(ms)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Speed&lt;sup&gt;V100&lt;br&gt;fp32 b32 &lt;br&gt;(ms)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;sup&gt;T4&lt;br&gt;trt fp16 b1 &lt;br&gt;(fps)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Speed&lt;sup&gt;T4&lt;br&gt;trt fp16 b32 &lt;br&gt;(fps)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Params&lt;br&gt;&lt;sup&gt; (M)&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Flops&lt;br&gt;&lt;sup&gt; (G)&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/download/0.1.0/yolov6n.pt&#34;&gt;&lt;strong&gt;YOLOv6-n&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;416&lt;br&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;30.8&lt;br&gt;35.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.3&lt;br&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.4&lt;br&gt;0.7&lt;/td&gt; &#xA;   &lt;td&gt;1100&lt;br&gt;788&lt;/td&gt; &#xA;   &lt;td&gt;2716&lt;br&gt;1242&lt;/td&gt; &#xA;   &lt;td&gt;4.3&lt;br&gt;4.3&lt;/td&gt; &#xA;   &lt;td&gt;4.7&lt;br&gt;11.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/download/0.1.0/yolov6t.pt&#34;&gt;&lt;strong&gt;YOLOv6-tiny&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;41.3&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;0.9&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td&gt;425&lt;/td&gt; &#xA;   &lt;td&gt;602&lt;/td&gt; &#xA;   &lt;td&gt;15.0&lt;/td&gt; &#xA;   &lt;td&gt;36.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6/releases/download/0.1.0/yolov6s.pt&#34;&gt;&lt;strong&gt;YOLOv6-s&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;43.1&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.7&lt;/td&gt; &#xA;   &lt;td&gt;373&lt;/td&gt; &#xA;   &lt;td&gt;520&lt;/td&gt; &#xA;   &lt;td&gt;17.2&lt;/td&gt; &#xA;   &lt;td&gt;44.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comparisons of the mAP and speed of different object detectors are tested on &lt;a href=&#34;https://cocodataset.org/#download&#34;&gt;COCO val2017&lt;/a&gt; dataset.&lt;/li&gt; &#xA; &lt;li&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/meituan/YOLOv6/main/docs/Test_speed.md&#34;&gt;Test speed&lt;/a&gt; tutorial to reproduce the speed results of YOLOv6.&lt;/li&gt; &#xA; &lt;li&gt;Params and Flops of YOLOv6 are estimated on deployed model.&lt;/li&gt; &#xA; &lt;li&gt;Speed results of other methods are tested in our environment using official codebase and model if not found from the corresponding official release.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>GreaterWMS/GreaterWMS</title>
    <updated>2022-06-24T01:35:26Z</updated>
    <id>tag:github.com,2022-06-24:/GreaterWMS/GreaterWMS</id>
    <link href="https://github.com/GreaterWMS/GreaterWMS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This Inventory management system is the currently Ford Asia Pacific after-sales logistics warehousing supply chain process . After I leave Ford , I start this project . In order to help some who need it . OneAPP Type . Support scanner PDA, mobile APP, desktop exe, website as well .&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/GreaterWMS/GreaterWMS/master/static/img/logo.png&#34; alt=&#34;GreaterWMS logo&#34; width=&#34;200&#34; height=&#34;auto&#34;&gt; &#xA; &lt;h1&gt;GreaterWMS&lt;/h1&gt; &#xA; &lt;p&gt;Open Source Inventory Management System &lt;/p&gt; &#xA; &lt;!-- Badges --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/Singosgu/GreaterWMS&#34; alt=&#34;License: GPLv3&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/Singosgu/GreaterWMS?color=orange&amp;amp;include_prereleases&#34; alt=&#34;Release Version (latest Version)&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/QR--Code-Support-orange.svg?sanitize=true&#34; alt=&#34;QR Code Support&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Docker-Support-orange.svg?sanitize=true&#34; alt=&#34;Docker Support&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/i18n-Support-orange.svg?sanitize=true&#34; alt=&#34;i18n Support&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/repo-size/Singosgu/GreaterWMS&#34; alt=&#34;repo size&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/commit-activity/m/Singosgu/GreaterWMS&#34; alt=&#34;GitHub commit activity&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/Singosgu/GreaterWMS?color=blue&#34; alt=&#34;Contributors&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/GreaterWMS?style=social&#34; alt=&#34;GitHub Org&#39;s stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/followers/Singosgu?style=social&#34; alt=&#34;GitHub Follows&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/GreaterWMS/GreaterWMS?style=social&#34; alt=&#34;GitHub Forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/watchers/GreaterWMS/GreaterWMS?style=social&#34; alt=&#34;GitHub Watch&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.9.5-yellowgreen&#34; alt=&#34;Python&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Django-3.1.14-yellowgreen&#34; alt=&#34;Django&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Quasar/cli-1.2.1-yellowgreen&#34; alt=&#34;Quasar Cli&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Vue-2.6.0-yellowgreen&#34; alt=&#34;Vue&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/NodeJS-14.19.3-yellowgreen&#34; alt=&#34;NodeJS&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCPW1wciGMIEh7CYOdLnsloA&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/subscribers/UCPW1wciGMIEh7CYOdLnsloA?color=red&amp;amp;label=YouTube&amp;amp;logo=youtube&amp;amp;style=for-the-badge&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;&lt;span&gt;🚀&lt;/span&gt; Link US&lt;/h2&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/&#34;&gt;Home Page&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.youtube.com/channel/UCPW1wciGMIEh7CYOdLnsloA&#34;&gt;Video Tutorials&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://github.com/GreaterWMS/GreaterwMS/issues/new?template=bug_report.md&amp;amp;title=%5BBUG%5D&#34;&gt;Report Bug&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://github.com/GreaterWMS/GreaterWMS/issues/new?template=feature_request.md&amp;amp;title=%5BFR%5D&#34;&gt;Request Feature&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://github.com/GreaterWMS/GreaterWMS/raw/master/README_CN.md&#34;&gt;中文文档&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h2&gt;&lt;span&gt;🌟&lt;/span&gt; About the Project&lt;/h2&gt; &#xA;&lt;p&gt;This Inventory management system is the currently Ford Asia Pacific after-sales logistics warehousing supply chain process . After I leave Ford , I start this project . In order to help some who need it . OneAPP Type . Support scanner PDA, mobile APP, desktop exe, website as well .&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;🎯&lt;/span&gt; Function&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supplier Management&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Customer Management&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Scanner PDA&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Cycle Count&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Order Management&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Stock Control&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Safety Stock Show&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; API Documents&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; IOS APP Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Android APP Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Electron APP Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Auto Update&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; i18n Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; API Documents&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;👀&lt;/span&gt; Where is APIs Documents:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;After installed you can find APIs Documents from url /docs/&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;example: http://127.0.0.1:8008/docs/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;🧭&lt;/span&gt; Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/GreaterWMS/GreaterWMS.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd GreaterWMS/&#xA;docker-compose up -d&#xA;# Change Front Request Baseurl&#xA;# baseurl GreaterWMS/templates/public/statics/baseurl.js&#xA;# change the baseurl and wsurl&#xA;docker-compose restart&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/3/&#34;&gt;Windows X64&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/4/&#34;&gt;Centos 7&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/5/&#34;&gt;Ubuntu 20&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/6/&#34;&gt;IOS Environment&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/7/&#34;&gt;Android Environment&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/8/&#34;&gt;Android APK Signed&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/11/&#34;&gt;Electron Environment&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h2&gt;&lt;span&gt;🛠&lt;/span&gt; How To Run Development Server:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Webside Dev Run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd templates&#xA;quasar d # http://localhost:8080&#xA;or&#xA;quasar dev # http://localhost:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Electron APP Dev Run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd templates&#xA;quasar d -m electron&#xA;or&#xA;quasar dev -m electron&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mobile APP Dev Run&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You should connect your mobile . Sometime it need you choose the ip , the ip is your PC&#39;s internal ip . The Android APP installed on the mobile phone is the mobile page, and the installation on the scanning device is the scanning page.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd templates/src-cordova&#xA;cordova platform add [ios or android]&#xA;cd .. # back to templates&#xA;quasar d -m cordova -T [ios or android]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;🎺&lt;/span&gt; How To Publish Your APP:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Webside Build:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd templates&#xA;quasar build # /templates/dist/spa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Electron APP Build:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;quasar build -m electron -P always # /templates/dist/electron&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mobile APP Build:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;quasar build -m cordova -T [ios or android] # /templates/dist/cordova&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;💻&lt;/span&gt; How To Deploy Server:&lt;/h2&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/9/&#34;&gt;Supervisor Process Guarded&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;h4&gt; &lt;a href=&#34;https://www.56yhz.com/en-us/docs/2/10/&#34;&gt;Nginx Config&lt;/a&gt; &lt;/h4&gt; &#xA;&lt;p&gt;If the server has SSL enabled, please use https and wss, if SSL is not enabled, use http and ws&lt;/p&gt; &#xA;&lt;p&gt;The front-end code needs to be rebuilt after modification&lt;/p&gt; &#xA;&lt;!-- Sponsor --&gt; &#xA;&lt;h2&gt;&lt;span&gt;💸&lt;/span&gt; Sponsor&lt;/h2&gt; &#xA;&lt;p&gt;If you use GreaterWMS and find it to be useful, please consider making a donation toward its continued development.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paypal.me/singosgu&#34;&gt;Donate via PayPal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- License --&gt; &#xA;&lt;h2&gt;&lt;span&gt;⚠&lt;/span&gt; License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.html&#34;&gt;GPL v3&lt;/a&gt; License. See &lt;a href=&#34;https://github.com/Singosgu/GreaterWMS/raw/master/LICENSE&#34;&gt;LICENSE.txt&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shenweichen/DeepCTR</title>
    <updated>2022-06-24T01:35:26Z</updated>
    <id>tag:github.com,2022-06-24:/shenweichen/DeepCTR</id>
    <link href="https://github.com/shenweichen/DeepCTR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easy-to-use,Modular and Extendible package of deep-learning based CTR models .&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepCTR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/deepctr&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/deepctr.svg?sanitize=true&#34; alt=&#34;Python Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/deepctr&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/TensorFlow-1.4+/2.0+-blue.svg?sanitize=true&#34; alt=&#34;TensorFlow Versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/deepctr&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/deepctr&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/deepctr&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/deepctr.svg?sanitize=true&#34; alt=&#34;PyPI Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shenweichen/deepctr/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/shenweichen/deepctr.svg?sanitize=true&#34; alt=&#34;GitHub Issues&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![Activity](https://img.shields.io/github/last-commit/shenweichen/deepctr.svg)](https://github.com/shenweichen/DeepCTR/commits/master) --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://deepctr-doc.readthedocs.io/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/deepctr-doc/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/shenweichen/deepctr/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI status&#34;&gt; &lt;a href=&#34;https://codecov.io/gh/shenweichen/DeepCTR&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/shenweichen/DeepCTR/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.codacy.com/gh/shenweichen/DeepCTR?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=shenweichen/DeepCTR&amp;amp;utm_campaign=Badge_Grade&#34;&gt;&lt;img src=&#34;https://api.codacy.com/project/badge/Grade/d4099734dc0e4bab91d332ead8c0bdd0&#34; alt=&#34;Codacy Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/shenweichen/DeepCTR/master/README.md#DisscussionGroup&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-wechat-brightgreen?style=flat&#34; alt=&#34;Disscussion&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shenweichen/deepctr/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/shenweichen/deepctr.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![Gitter](https://badges.gitter.im/DeepCTR/community.svg)](https://gitter.im/DeepCTR/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge) --&gt; &#xA;&lt;p&gt;DeepCTR is a &lt;strong&gt;Easy-to-use&lt;/strong&gt;,&lt;strong&gt;Modular&lt;/strong&gt; and &lt;strong&gt;Extendible&lt;/strong&gt; package of deep-learning based CTR models along with lots of core components layers which can be used to easily build custom models.You can use any complex model with &lt;code&gt;model.fit()&lt;/code&gt; ，and &lt;code&gt;model.predict()&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Provide &lt;code&gt;tf.keras.Model&lt;/code&gt; like interface for &lt;strong&gt;quick experiment&lt;/strong&gt; . &lt;a href=&#34;https://deepctr-doc.readthedocs.io/en/latest/Quick-Start.html#getting-started-4-steps-to-deepctr&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Provide &lt;code&gt;tensorflow estimator&lt;/code&gt; interface for &lt;strong&gt;large scale data&lt;/strong&gt; and &lt;strong&gt;distributed training&lt;/strong&gt; . &lt;a href=&#34;https://deepctr-doc.readthedocs.io/en/latest/Quick-Start.html#getting-started-4-steps-to-deepctr-estimator-with-tfrecord&#34;&gt;example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;It is compatible with both &lt;code&gt;tf 1.x&lt;/code&gt; and &lt;code&gt;tf 2.x&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some related projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeepMatch: &lt;a href=&#34;https://github.com/shenweichen/DeepMatch&#34;&gt;https://github.com/shenweichen/DeepMatch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DeepCTR-Torch: &lt;a href=&#34;https://github.com/shenweichen/DeepCTR-Torch&#34;&gt;https://github.com/shenweichen/DeepCTR-Torch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s &lt;a href=&#34;https://deepctr-doc.readthedocs.io/en/latest/Quick-Start.html&#34;&gt;&lt;strong&gt;Get Started!&lt;/strong&gt;&lt;/a&gt;(&lt;a href=&#34;https://zhuanlan.zhihu.com/p/53231955&#34;&gt;Chinese Introduction&lt;/a&gt;) and &lt;a href=&#34;https://raw.githubusercontent.com/shenweichen/DeepCTR/master/CONTRIBUTING.md&#34;&gt;welcome to join us!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Models List&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Paper&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Convolutional Click Prediction Model&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[CIKM 2015]&lt;a href=&#34;http://ir.ia.ac.cn/bitstream/173211/12337/1/A%20Convolutional%20Click%20Prediction%20Model.pdf&#34;&gt;A Convolutional Click Prediction Model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Factorization-supported Neural Network&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[ECIR 2016]&lt;a href=&#34;https://arxiv.org/pdf/1601.02376.pdf&#34;&gt;Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Product-based Neural Network&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[ICDM 2016]&lt;a href=&#34;https://arxiv.org/pdf/1611.00144.pdf&#34;&gt;Product-based neural networks for user response prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Wide &amp;amp; Deep&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[DLRS 2016]&lt;a href=&#34;https://arxiv.org/pdf/1606.07792.pdf&#34;&gt;Wide &amp;amp; Deep Learning for Recommender Systems&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepFM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[IJCAI 2017]&lt;a href=&#34;http://www.ijcai.org/proceedings/2017/0239.pdf&#34;&gt;DeepFM: A Factorization-Machine based Neural Network for CTR Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Piece-wise Linear Model&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[arxiv 2017]&lt;a href=&#34;https://arxiv.org/abs/1704.05194&#34;&gt;Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deep &amp;amp; Cross Network&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[ADKDD 2017]&lt;a href=&#34;https://arxiv.org/abs/1708.05123&#34;&gt;Deep &amp;amp; Cross Network for Ad Click Predictions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Attentional Factorization Machine&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[IJCAI 2017]&lt;a href=&#34;http://www.ijcai.org/proceedings/2017/435&#34;&gt;Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Neural Factorization Machine&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[SIGIR 2017]&lt;a href=&#34;https://arxiv.org/pdf/1708.05027.pdf&#34;&gt;Neural Factorization Machines for Sparse Predictive Analytics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;xDeepFM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[KDD 2018]&lt;a href=&#34;https://arxiv.org/pdf/1803.05170.pdf&#34;&gt;xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deep Interest Network&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[KDD 2018]&lt;a href=&#34;https://arxiv.org/pdf/1706.06978.pdf&#34;&gt;Deep Interest Network for Click-Through Rate Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AutoInt&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[CIKM 2019]&lt;a href=&#34;https://arxiv.org/abs/1810.11921&#34;&gt;AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deep Interest Evolution Network&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[AAAI 2019]&lt;a href=&#34;https://arxiv.org/pdf/1809.03672.pdf&#34;&gt;Deep Interest Evolution Network for Click-Through Rate Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FwFM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[WWW 2018]&lt;a href=&#34;https://arxiv.org/pdf/1806.03514.pdf&#34;&gt;Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ONN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[arxiv 2019]&lt;a href=&#34;https://arxiv.org/pdf/1904.12579.pdf&#34;&gt;Operation-aware Neural Networks for User Response Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FGCNN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[WWW 2019]&lt;a href=&#34;https://arxiv.org/pdf/1904.04447&#34;&gt;Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deep Session Interest Network&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[IJCAI 2019]&lt;a href=&#34;https://arxiv.org/abs/1905.06482&#34;&gt;Deep Session Interest Network for Click-Through Rate Prediction &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FiBiNET&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[RecSys 2019]&lt;a href=&#34;https://arxiv.org/pdf/1905.09433.pdf&#34;&gt;FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FLEN&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[arxiv 2019]&lt;a href=&#34;https://arxiv.org/pdf/1911.04690.pdf&#34;&gt;FLEN: Leveraging Field for Scalable CTR Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BST&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[DLP-KDD 2019]&lt;a href=&#34;https://arxiv.org/pdf/1905.06874.pdf&#34;&gt;Behavior sequence transformer for e-commerce recommendation in Alibaba&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;IFM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[IJCAI 2019]&lt;a href=&#34;https://www.ijcai.org/Proceedings/2019/0203.pdf&#34;&gt;An Input-aware Factorization Machine for Sparse Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DCN V2&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[arxiv 2020]&lt;a href=&#34;https://arxiv.org/abs/2008.13535&#34;&gt;DCN V2: Improved Deep &amp;amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DIFM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[IJCAI 2020]&lt;a href=&#34;https://www.ijcai.org/Proceedings/2020/0434.pdf&#34;&gt;A Dual Input-aware Factorization Machine for CTR Prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FEFM and DeepFEFM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[arxiv 2020]&lt;a href=&#34;https://arxiv.org/abs/2009.09931&#34;&gt;Field-Embedded Factorization Machines for Click-through rate prediction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SharedBottom&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[arxiv 2017]&lt;a href=&#34;https://arxiv.org/pdf/1706.05098.pdf&#34;&gt;An Overview of Multi-Task Learning in Deep Neural Networks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ESMM&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[SIGIR 2018]&lt;a href=&#34;https://arxiv.org/abs/1804.07931&#34;&gt;Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MMOE&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[KDD 2018]&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3219819.3220007&#34;&gt;Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PLE&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;[RecSys 2020]&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3383313.3412236&#34;&gt;Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Weichen Shen. (2017). DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models. &lt;a href=&#34;https://github.com/shenweichen/deepctr&#34;&gt;https://github.com/shenweichen/deepctr&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you find this code useful in your research, please cite it using the following BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{shen2017deepctr,&#xA;  author = {Weichen Shen},&#xA;  title = {DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models},&#xA;  year = {2017},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub Repository},&#xA;  howpublished = {\url{https://github.com/shenweichen/deepctr}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;DisscussionGroup&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shenweichen/DeepCTR/discussions&#34;&gt;Github Discussions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Wechat Discussions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;公众号：浅梦学习笔记&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;微信：deepctrbot&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;学习小组 &lt;a href=&#34;https://t.zsxq.com/026UJEuzv&#34;&gt;加入&lt;/a&gt; &lt;a href=&#34;https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MjM5MzY4NzE3MA==&amp;amp;action=getalbum&amp;amp;album_id=1361647041096843265&amp;amp;scene=126#wechat_redirect&#34;&gt;主题集合&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/shenweichen/AlgoNotes&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shenweichen/DeepCTR/master/docs/pics/code.png&#34; alt=&#34;公众号&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/shenweichen/AlgoNotes&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shenweichen/DeepCTR/master/docs/pics/deepctrbot.png&#34; alt=&#34;微信&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://t.zsxq.com/026UJEuzv&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shenweichen/DeepCTR/master/docs/pics/planet_github.png&#34; alt=&#34;学习小组&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Main contributors(&lt;a href=&#34;https://raw.githubusercontent.com/shenweichen/DeepCTR/master/CONTRIBUTING.md&#34;&gt;welcome to join us!&lt;/a&gt;)&lt;/h2&gt; &#xA;&lt;table border=&#34;0&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt; ​ &lt;a href=&#34;https://github.com/shenweichen&#34;&gt;&lt;img width=&#34;70&#34; height=&#34;70&#34; src=&#34;https://github.com/shenweichen.png?s=40&#34; alt=&#34;pic&#34;&gt;&lt;/a&gt;&lt;br&gt; ​ &lt;a href=&#34;https://github.com/shenweichen&#34;&gt;Shen Weichen&lt;/a&gt; ​ &lt;p&gt; Alibaba Group &lt;/p&gt;​ &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/zanshuxun&#34;&gt;&lt;img width=&#34;70&#34; height=&#34;70&#34; src=&#34;https://github.com/zanshuxun.png?s=40&#34; alt=&#34;pic&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/zanshuxun&#34;&gt;Zan Shuxun&lt;/a&gt; ​ &lt;p&gt;Alibaba Group &lt;/p&gt;​ &lt;/td&gt; &#xA;   &lt;td&gt; ​ &lt;a href=&#34;https://github.com/pandeconscious&#34;&gt;&lt;img width=&#34;70&#34; height=&#34;70&#34; src=&#34;https://github.com/pandeconscious.png?s=40&#34; alt=&#34;pic&#34;&gt;&lt;/a&gt;&lt;br&gt; ​ &lt;a href=&#34;https://github.com/pandeconscious&#34;&gt;Harshit Pande&lt;/a&gt; &lt;p&gt; Amazon &lt;/p&gt;​ &lt;/td&gt; &#xA;   &lt;td&gt; ​ &lt;a href=&#34;https://github.com/morningsky&#34;&gt;&lt;img width=&#34;70&#34; height=&#34;70&#34; src=&#34;https://github.com/morningsky.png?s=40&#34; alt=&#34;pic&#34;&gt;&lt;/a&gt;&lt;br&gt; ​ &lt;a href=&#34;https://github.com/morningsky&#34;&gt;Lai Mincai&lt;/a&gt; &lt;p&gt; ByteDance &lt;/p&gt;​ &lt;/td&gt; &#xA;   &lt;td&gt; ​ &lt;a href=&#34;https://github.com/codewithzichao&#34;&gt;&lt;img width=&#34;70&#34; height=&#34;70&#34; src=&#34;https://github.com/codewithzichao.png?s=40&#34; alt=&#34;pic&#34;&gt;&lt;/a&gt;&lt;br&gt; ​ &lt;a href=&#34;https://github.com/codewithzichao&#34;&gt;Li Zichao&lt;/a&gt; &lt;p&gt; ByteDance &lt;/p&gt;​ &lt;/td&gt; &#xA;   &lt;td&gt; ​ &lt;a href=&#34;https://github.com/TanTingyi&#34;&gt;&lt;img width=&#34;70&#34; height=&#34;70&#34; src=&#34;https://github.com/TanTingyi.png?s=40&#34; alt=&#34;pic&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/TanTingyi&#34;&gt;Tan Tingyi&lt;/a&gt; &lt;p&gt; Chongqing University &lt;br&gt; of Posts and &lt;br&gt; Telecommunications &lt;/p&gt;​ &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>