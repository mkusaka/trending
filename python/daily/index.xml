<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-08T01:45:26Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/shap-e</title>
    <updated>2023-05-08T01:45:26Z</updated>
    <id>tag:github.com,2023-05-08:/openai/shap-e</id>
    <link href="https://github.com/openai/shap-e" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generate 3D objects conditioned on text or images&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Shap-E&lt;/h1&gt; &#xA;&lt;p&gt;This is the official code and model release for &lt;a href=&#34;https://arxiv.org/abs/2305.02463&#34;&gt;Shap-E: Generating Conditional 3D Implicit Functions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/shap-e/main/#usage&#34;&gt;Usage&lt;/a&gt; for guidance on how to use this repository.&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/shap-e/main/#samples&#34;&gt;Samples&lt;/a&gt; for examples of what our text-conditional model can generate.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Samples&lt;/h1&gt; &#xA;&lt;p&gt;Here are some highlighted samples from our text-conditional model. For random samples on selected prompts, see &lt;a href=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples.md&#34;&gt;samples.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/a_chair_that_looks_like_an_avocado/2.gif&#34; alt=&#34;A chair that looks like an avocado&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/an_airplane_that_looks_like_a_banana/3.gif&#34; alt=&#34;An airplane that looks like a banana&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/a_spaceship/0.gif&#34; alt=&#34;A spaceship&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A chair that looks&lt;br&gt;like an avocado&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;An airplane that looks&lt;br&gt;like a banana&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A spaceship&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/a_birthday_cupcake/3.gif&#34; alt=&#34;A birthday cupcake&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/a_chair_that_looks_like_a_tree/2.gif&#34; alt=&#34;A chair that looks like a tree&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/a_green_boot/3.gif&#34; alt=&#34;A green boot&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A birthday cupcake&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A chair that looks&lt;br&gt;like a tree&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A green boot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/a_penguin/1.gif&#34; alt=&#34;A penguin&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/ube_ice_cream_cone/3.gif&#34; alt=&#34;Ube ice cream cone&#34;&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/openai/shap-e/main/samples/a_bowl_of_vegetables/2.gif&#34; alt=&#34;A bowl of vegetables&#34;&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A penguin&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Ube ice cream cone&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A bowl of vegetables&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;&#xA;&lt;h1&gt;Usage&lt;/h1&gt;&#xA;&lt;p&gt;Install with &lt;code&gt;pip install -e .&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;To get started with examples, see the following notebooks:&lt;/p&gt;&#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/shap-e/main/shap_e/examples/sample_text_to_3d.ipynb&#34;&gt;sample_text_to_3d.ipynb&lt;/a&gt; - sample a 3D model, conditioned on a text prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/shap-e/main/shap_e/examples/sample_image_to_3d.ipynb&#34;&gt;sample_image_to_3d.ipynb&lt;/a&gt; - sample a 3D model, conditioned on an synthetic view image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openai/shap-e/main/shap_e/examples/encode_model.ipynb&#34;&gt;encode_model.ipynb&lt;/a&gt; - loads a 3D model or a trimesh, creates a batch of multiview renders and a point cloud, encodes them into a latent, and renders it back. For this to work, install Blender version 3.3.1 or higher, and set the environment variable &lt;code&gt;BLENDER_PATH&lt;/code&gt; to the path of the Blender executable.&lt;/li&gt; &#xA;&lt;/ul&gt;&#xA;&lt;table&gt;     &#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>ZrrSkywalker/Personalize-SAM</title>
    <updated>2023-05-08T01:45:26Z</updated>
    <id>tag:github.com,2023-05-08:/ZrrSkywalker/Personalize-SAM</id>
    <link href="https://github.com/ZrrSkywalker/Personalize-SAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Personalize Segment Anything Model (SAM) with 1 shot in 10 seconds&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Personalize Segment Anything with 1 Shot in 10 Seconds&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation of &lt;a href=&#34;https://arxiv.org/pdf/2305.03048.pdf&#34;&gt;&#39;Personalize Segment Anything Model with One Shot&#39;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;TODO&lt;/strong&gt;: Release the PerSAM-assisted &lt;a href=&#34;https://arxiv.org/pdf/2208.12242.pdf&#34;&gt;Dreambooth&lt;/a&gt; for better fine-tuning &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; ðŸ“Œ.&lt;/li&gt; &#xA; &lt;li&gt;We release the code of PerSAM and PerSAM-F ðŸ”¥. Check our &lt;a href=&#34;https://www.youtube.com/watch?v=QlunvXpYQXM&#34;&gt;demo&lt;/a&gt; here!&lt;/li&gt; &#xA; &lt;li&gt;We release a new dataset for personalized segmentation, &lt;a href=&#34;https://drive.google.com/file/d/18TbrwhZtAPY5dlaoEqkPa5h08G9Rjcio/view?usp=sharing&#34;&gt;PerSeg&lt;/a&gt; ðŸ”¥.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;How to customize SAM to automatically segment your pet dog in a photo album?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this project, we propose a training-free &lt;strong&gt;Per&lt;/strong&gt;sonalization approach for &lt;a href=&#34;https://ai.facebook.com/research/publications/segment-anything/&#34;&gt;Segment Anything Model (SAM)&lt;/a&gt;, termed as &lt;strong&gt;PerSAM&lt;/strong&gt;. Given only a single image with a reference mask, PerSAM can segment specific visual concepts, e.g., your pet dog, within other images or videos without any training. For better performance, we further present an efficient one-shot fine-tuning variant, &lt;strong&gt;PerSAM-F&lt;/strong&gt;. We freeze the entire SAM and introduce two learnable mask weights, which only trains &lt;strong&gt;2 parameters&lt;/strong&gt; within &lt;strong&gt;10 seconds&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ZrrSkywalker/Personalize-SAM/main/figs/fig_persam.png&#34; width=&#34;97%&#34;&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Besides, our approach can be utilized to assist &lt;a href=&#34;https://arxiv.org/pdf/2208.12242.pdf&#34;&gt;DreamBooth&lt;/a&gt; in fine-tuning better &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for personalized image synthesis. We adopt PerSAM to segment the target object in the user-provided few-shot images, which eliminates the &lt;strong&gt;background disturbance&lt;/strong&gt; and benefits the target representation learning.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ZrrSkywalker/Personalize-SAM/main/figs/fig_db.png&#34; width=&#34;97%&#34;&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repo and create a conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ZrrSkywalker/Personalize-SAM.git&#xA;cd Personalize-SAM&#xA;&#xA;conda create -n persam python=3.8&#xA;conda activate persam&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similar to Segment Anything, our code requires &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies.&lt;/p&gt; &#xA;&lt;h3&gt;Preparation&lt;/h3&gt; &#xA;&lt;p&gt;Please download our constructed dataset &lt;strong&gt;PerSeg&lt;/strong&gt; for personalized segmentation from &lt;a href=&#34;https://drive.google.com/file/d/18TbrwhZtAPY5dlaoEqkPa5h08G9Rjcio/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1X-czD-FYW0ELlk2x90eTLg&#34;&gt;Baidu Yun&lt;/a&gt; (code &lt;code&gt;222k&lt;/code&gt;), and the pre-trained weights of SAM from &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#34;&gt;here&lt;/a&gt;. Then, unzip the dataset file and organize them as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;data/&#xA;|â€“â€“ Annotations/&#xA;|â€“â€“ Images/&#xA;sam_vit_h_4b8939.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Personalized Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;For the training-free ðŸ§Š &lt;strong&gt;PerSAM&lt;/strong&gt;, just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python persam.py --outdir &amp;lt;output filename&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For 10-second fine-tuning of ðŸš€ &lt;strong&gt;PerSAM-F&lt;/strong&gt;, just run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python persam_f.py --outdir &amp;lt;output filename&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running, the output masks and visualzations will be stored at &lt;code&gt;outputs/&amp;lt;output filename&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, for mIoU evaluation, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval_miou.py --pred_path &amp;lt;output filename&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Personalized Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;Our approach can enhance DreamBooth to better personalize Stable Diffusion for text-to-image generation.&lt;/p&gt; &#xA;&lt;p&gt;Comming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@misc{zhang2023personalize,&#xA;      title={Personalize Segment Anything Model with One Shot}, &#xA;      author={Renrui Zhang and Zhengkai Jiang and Ziyu Guo and Shilin Yan and Junting Pan and Hao Dong and Peng Gao and Hongsheng Li},&#xA;      year={2023},&#xA;      eprint={2305.03048},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This repo benefits from &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; and &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion&#34;&gt;DreamBooth&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question about this project, please feel free to contact &lt;a href=&#34;mailto:zhangrenrui@pjlab.org.cn&#34;&gt;zhangrenrui@pjlab.org.cn&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alejandro-ao/langchain-ask-pdf</title>
    <updated>2023-05-08T01:45:26Z</updated>
    <id>tag:github.com,2023-05-08:/alejandro-ao/langchain-ask-pdf</id>
    <link href="https://github.com/alejandro-ao/langchain-ask-pdf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI-app that allows you to upload a PDF and ask questions about it. It uses OpenAI&#39;s LLMs to generate a response.&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>