<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-27T01:33:29Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>maitrix-org/Pandora</title>
    <updated>2024-05-27T01:33:29Z</updated>
    <id>tag:github.com,2024-05-27:/maitrix-org/Pandora</id>
    <link href="https://github.com/maitrix-org/Pandora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pandora: Towards General World Model with Natural Language Actions and Video States&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/maitrix-org/Pandora/main/assets/logo.png&#34; width=&#34;250&#34;&gt; &lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; Pandora: Towards General World Model with Natural Language Actions and Video States&lt;/h2&gt; &#xA;&lt;p&gt;We introduce Pandora, a step towards a General World Model (GWM) that:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Simulates world states by generating videos across any domains&lt;/li&gt; &#xA; &lt;li&gt;Allows any-time control with actions expressed in natural language&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/maitrix-org/Pandora/main/world-model.ai&#34;&gt;world-model.ai&lt;/a&gt; for results.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://world-model.maitrix.org/&#34;&gt;[Website]&lt;/a&gt; &lt;a href=&#34;https://world-model.maitrix.org/assets/pandora.pdf&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/maitrix-org/Pandora&#34;&gt;[Model]&lt;/a&gt; &lt;a href=&#34;https://world-model.maitrix.org/gallery.html&#34;&gt;[Gallery]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/maitrix-org/Pandora/main/assets/architecture.png&#34; width=&#34;780&#34; alt=&#34;struct&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024/05/23]&lt;/strong&gt; Release the model and inference code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024/05/23]&lt;/strong&gt; Launch the website and release the paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n pandora python=3.12.3 nvidia/label/cuda-12.1.0::cuda-toolkit -y&#xA;conda activate pandora&#xA;pip install torch torchvision torchaudio&#xA;bash build_envs.sh  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your GPU doesn&#39;t support CUDA 12.1, you can also install with CUDA 11.8:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n pandora python=3.12.3 nvidia/label/cuda-11.8.0::cuda-toolkit -y &#xA;conda activate pandora&#xA;pip install torch torchvision torchaudio&#xA;bash build_envs.sh  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the model checkpoint from &lt;a href=&#34;https://huggingface.co/maitrix-org/Pandora&#34;&gt;Hugging Face&lt;/a&gt;. (&lt;em&gt;&lt;strong&gt;We currently hide the model weights due to data license issue. We will re-open the weights soon after we figure this out.&lt;/strong&gt;&lt;/em&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Run the commands on your terminal&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES={cuda_id} python gradio_app.py  --ckpt_path {path_to_ckpt}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can interact with the model through gradio interface.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{xiang2024pandora,&#xA;  title={Pandora: Towards General World Model with Natural Language Actions and Video States},&#xA;  author={Jiannan Xiang and Guangyi Liu and Yi Gu and Qiyue Gao and Yuting Ning and Yuheng Zha and Zeyu Feng and Tianhua Tao and Shibo Hao and Yemin Shi and Zhengzhong Liu and Eric P. Xing and Zhiting Hu},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Dataherald/dataherald</title>
    <updated>2024-05-27T01:33:29Z</updated>
    <id>tag:github.com,2024-05-27:/Dataherald/dataherald</id>
    <link href="https://github.com/Dataherald/dataherald" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Interact with your SQL database, Natural Language to SQL using LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dataherald monorepo&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://dataherald.com&#34;&gt;&lt;img src=&#34;https://files.dataherald.com/logos/dataherald.png&#34; alt=&#34;Dataherald logo&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Query your relational data in natural language&lt;/b&gt;. &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/A59Uxyy2k9&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1138593282184716441&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Dataherald/dataherald/main/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=license&amp;amp;message=Apache 2.0&amp;amp;color=white&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; | &lt;a href=&#34;https://dataherald.readthedocs.io/&#34; target=&#34;_blank&#34;&gt; Docs &lt;/a&gt; | &lt;a href=&#34;https://www.dataherald.com/&#34; target=&#34;_blank&#34;&gt; Homepage &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Dataherald is a natural language-to-SQL engine built for enterprise-level question answering over relational data. It allows you to set up an API from your database that can answer questions in plain English. You can use Dataherald to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Allow business users to get insights from the data warehouse without going through a data analyst&lt;/li&gt; &#xA; &lt;li&gt;Enable Q+A from your production DBs inside your SaaS application&lt;/li&gt; &#xA; &lt;li&gt;Create a ChatGPT plug-in from your proprietary data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repository contains four components under &lt;code&gt;/services&lt;/code&gt; which can be used together to set up an end-to-end Dataherald deployment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Engine: The core natural language-to-SQL engine. If you would like to use the dataherald API without users or authentication, running the engine will suffice.&lt;/li&gt; &#xA; &lt;li&gt;Enterprise: The application API layer which adds authentication, organizations and users, and other business logic to Dataherald.&lt;/li&gt; &#xA; &lt;li&gt;Admin-console: The front-end component of Dataherald which allows a GUI for configuration and observability. You will need to run both engine and enterprise for the admin-console to work.&lt;/li&gt; &#xA; &lt;li&gt;Slackbot: A slackbot which allows users from a slack channel to interact with dataherald. Requires both engine and enterprise to run.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information on each component, please take a look at their &lt;code&gt;README.md&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;h2&gt;Running locally&lt;/h2&gt; &#xA;&lt;p&gt;Each component in the &lt;code&gt;/services&lt;/code&gt; directory has its own &lt;code&gt;docker-compose.yml&lt;/code&gt; file. To set up the environment, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Set Environment Variables&lt;/strong&gt;: Each service requires specific environment variables. Refer to the &lt;code&gt;.env.example&lt;/code&gt; file in each service directory and create a &lt;code&gt;.env&lt;/code&gt; file with the necessary values. &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;For the Next.js front-end app is &lt;code&gt;.env.local&lt;/code&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run Services&lt;/strong&gt;: You can run all the services using a single script located in the root directory. This script creates a common Docker network and runs each service in detached mode.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Run the script to start all services:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh docker-run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open-source project in a rapidly developing field, we are open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.&lt;/p&gt; &#xA;&lt;p&gt;For detailed information on how to contribute, see &lt;a href=&#34;https://raw.githubusercontent.com/Dataherald/dataherald/main/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>eloialonso/diamond</title>
    <updated>2024-05-27T01:33:29Z</updated>
    <id>tag:github.com,2024-05-27:/eloialonso/diamond</id>
    <link href="https://github.com/eloialonso/diamond" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DIAMOND (DIffusion As a Model Of eNvironment Dreams) is a reinforcement learning agent trained in a diffusion world model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Diffusion for World Modeling: Visual Details Matter in Atari&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; We introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Autoregressive imagination with DIAMOND on a subset of Atari games &#xA; &lt;img alt=&#34;DIAMOND agent in WM&#34; src=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/assets/main.gif&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Quick install to try our &lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#try&#34;&gt;pretrained world models&lt;/a&gt; using &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/miniconda-install/&#34;&gt;miniconda&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:eloialonso/diamond.git&#xA;cd diamond&#xA;conda create -n diamond python=3.10&#xA;conda activate diamond&#xA;pip install -r requirements.txt&#xA;python src/play.py --pretrained&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Alternatively, if you do not have miniconda installed you can use python &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;venv&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:eloialonso/diamond.git&#xA;cd diamond&#xA;python3 -m venv diamond_env&#xA;source activate ./diamond_env/bin/&#xA;pip install -r requirements.txt&#xA;python src/play.py --pretrained&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;And press &lt;code&gt;m&lt;/code&gt; to take control (the policy is playing by default)!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Atari ROMs will be downloaded with the dependencies, which means that you acknowledge that you have the license to use them.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;quick_links&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#try&#34;&gt;Try our playable diffusion world models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#launch&#34;&gt;Launch a training run&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#visualization&#34;&gt;Visualization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#play_mode&#34;&gt;Play mode (default)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#dataset_mode&#34;&gt;Dataset mode (add &lt;code&gt;-d&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#other_options&#34;&gt;Other options, common to play/dataset modes&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#structure&#34;&gt;Run folder structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#credits&#34;&gt;Credits&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a name=&#34;try&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Try our playable diffusion world models&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/play.py --pretrained&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then select a game, and world model and policy pretrained on Atari 100k will be downloaded from our &lt;a href=&#34;https://huggingface.co/eloialonso/diamond&#34;&gt;repository on Hugging Face Hub 🤗&lt;/a&gt; and cached on your machine.&lt;/p&gt; &#xA;&lt;p&gt;Some things you might want to try:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Press &lt;code&gt;m&lt;/code&gt; to change the policy between the agent and human (the policy is playing by default).&lt;/li&gt; &#xA; &lt;li&gt;Press &lt;code&gt;↑/↓&lt;/code&gt; to change the imagination horizon (default is 50 for playing).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To adjust the sampling parameters (number of denoising steps, stochasticity, order, etc) of the trained diffusion world model, for instance to trade off sampling speed and quality, edit the section &lt;code&gt;world_model_env.diffusion_sampler&lt;/code&gt; in the file &lt;code&gt;config/trainer.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#visualization&#34;&gt;Visualization&lt;/a&gt; for more details about the available commands and options.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;launch&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Launch a training run&lt;/h2&gt; &#xA;&lt;p&gt;To train with the hyperparameters used in the paper, launch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/main.py env.train.id=BreakoutNoFrameskip-v4 common.device=cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This creates a new folder for your run, located in &lt;code&gt;outputs/YYYY-MM-DD/hh-mm-ss/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To resume a run that crashed, navigate to the fun folder and launch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/resume.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;configuration&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Configuration&lt;/h2&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/facebookresearch/hydra&#34;&gt;Hydra&lt;/a&gt; for configuration management.&lt;/p&gt; &#xA;&lt;p&gt;All configuration files are located in the &lt;code&gt;config&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config/trainer.yaml&lt;/code&gt;: main configuration file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config/agent/default.yaml&lt;/code&gt;: architecture hyperparameters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config/env/atari.yaml&lt;/code&gt;: environment hyperparameters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can turn on logging to &lt;a href=&#34;https://wandb.ai&#34;&gt;weights &amp;amp; biases&lt;/a&gt; in the &lt;code&gt;wandb&lt;/code&gt; section of &lt;code&gt;config/trainer.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Set &lt;code&gt;training.model_free=true&lt;/code&gt; in the file &lt;code&gt;config/trainer.yaml&lt;/code&gt; to &#34;unplug&#34; the world model and perform standard model-free reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;visualization&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Visualization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a name=&#34;play_mode&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Play mode (default)&lt;/h3&gt; &#xA;&lt;p&gt;To visualize your last checkpoint, launch &lt;strong&gt;from the run folder&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/play.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, you visualize the policy playing in the world model. To play yourself, or switch to the real environment, use the controls described below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;Controls (play mode)&#xA;&#xA;(Game-specific commands will be printed on start up)&#xA;&#xA;⏎   : reset environment&#xA;&#xA;m   : switch controller (policy/human)&#xA;↑/↓ : imagination horizon (+1/-1)&#xA;←/→ : next environment [world model ←→ real env (test) ←→ real env (train)]&#xA;&#xA;.   : pause/unpause&#xA;e   : step-by-step (when paused)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add &lt;code&gt;-r&lt;/code&gt; to toggle &#34;recording mode&#34; (works only in play mode). Every completed episode will be saved in &lt;code&gt;dataset/rec_&amp;lt;env_name&amp;gt;_&amp;lt;controller&amp;gt;&lt;/code&gt;. For instance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;dataset/rec_wm_π&lt;/code&gt;: Policy playing in world model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dataset/rec_wm_H&lt;/code&gt;: Human playing in world model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dataset/rec_test_H&lt;/code&gt;: Human playing in test real environment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can then use the &#34;dataset mode&#34; described in the next section to replay the stored episodes.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;dataset_mode&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Dataset mode (add &lt;code&gt;-d&lt;/code&gt;)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;In the run folder&lt;/strong&gt;, to visualize the datasets contained in the &lt;code&gt;dataset&lt;/code&gt; subfolder, add &lt;code&gt;-d&lt;/code&gt; to switch to &#34;dataset mode&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python src/play.py -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use the controls described below to navigate the datasets and episodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;Controls (dataset mode)&#xA;&#xA;m   : next dataset (if multiple datasets, like recordings, etc)&#xA;↑/↓ : next/previous episode&#xA;←/→ : next/previous timestep in episodes&#xA;PgUp: +10 timesteps&#xA;PgDn: -10 timesteps&#xA;⏎   : back to first timestep&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;other_options&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Other options, common to play/dataset modes&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;--fps FPS             Target frame rate (default 15).&#xA;--size SIZE           Window size (default 800).&#xA;--no-header           Remove header.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;structure&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Run folder structure&lt;/h2&gt; &#xA;&lt;p&gt;Each new run is located at &lt;code&gt;outputs/YYYY-MM-DD/hh-mm-ss/&lt;/code&gt;. This folder is structured as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;outputs/YYYY-MM-DD/hh-mm-ss/&#xA;│&#xA;└─── checkpoints&#xA;│   │   state.pt  # full training state&#xA;│   │&#xA;│   └─── agent_versions&#xA;│       │   ...&#xA;│       │   agent_epoch_00999.pt&#xA;│       │   agent_epoch_01000.pt  # agent weights only&#xA;│&#xA;└─── config&#xA;│   |   trainer.yaml&#xA;|&#xA;└─── dataset&#xA;│   │&#xA;│   └─── train&#xA;│   |   │   info.pt&#xA;│   |   │   ...&#xA;|   |&#xA;│   └─── test&#xA;│       │   info.pt&#xA;│       │   ...&#xA;│&#xA;└─── scripts&#xA;│   │   resume.sh&#xA;|   |   ...&#xA;|&#xA;└─── src&#xA;|   |   main.py&#xA;|   |   ...&#xA;|&#xA;└─── wandb&#xA;    |   ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;results&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Results&lt;/h2&gt; &#xA;&lt;p&gt;The file &lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/results/data/DIAMOND.json&#34;&gt;results/data/DIAMOND.json&lt;/a&gt; contains the results for each game and seed used in the paper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a name=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick-links&#34;&gt;⬆️&lt;/a&gt; Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@misc{alonso2024diffusion,&#xA;      title={Diffusion for World Modeling: Visual Details Matter in Atari},&#xA;      author={Eloi Alonso and Adam Jelley and Vincent Micheli and Anssi Kanervisto and Amos Storkey and Tim Pearce and François Fleuret},&#xA;      year={2024},&#xA;      eprint={2405.12399},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a name=&#34;credits&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/eloialonso/diamond/main/#quick_links&#34;&gt;⬆️&lt;/a&gt; Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/crowsonkb/k-diffusion/&#34;&gt;https://github.com/crowsonkb/k-diffusion/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/huggingface_hub&#34;&gt;https://github.com/huggingface/huggingface_hub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/rliable&#34;&gt;https://github.com/google-research/rliable&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;https://github.com/pytorch/pytorch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>