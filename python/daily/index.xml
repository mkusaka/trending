<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-09T01:37:51Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ashawkey/stable-dreamfusion</title>
    <updated>2022-10-09T01:37:51Z</updated>
    <id>tag:github.com,2022-10-09:/ashawkey/stable-dreamfusion</id>
    <link href="https://github.com/ashawkey/stable-dreamfusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable-Dreamfusion&lt;/h1&gt; &#xA;&lt;p&gt;A pytorch implementation of the text-to-3D model &lt;strong&gt;Dreamfusion&lt;/strong&gt;, powered by the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; text-to-2D model.&lt;/p&gt; &#xA;&lt;p&gt;The original paper&#39;s project page: &lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;&lt;em&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Colab notebook for usage: &lt;a href=&#34;https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Examples generated from text prompt &lt;code&gt;a high quality photo of a pineapple&lt;/code&gt; viewed with the GUI in real time:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/25863658/194241493-f3e68f78-aefe-479e-a4a8-001424a61b37.mp4&#34;&gt;https://user-images.githubusercontent.com/25863658/194241493-f3e68f78-aefe-479e-a4a8-001424a61b37.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion/issues/1&#34;&gt;Gallery&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ashawkey/stable-dreamfusion/main/assets/update_logs.md&#34;&gt;Update Logs&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h1&gt;Important Notice&lt;/h1&gt; &#xA;&lt;p&gt;This project is a &lt;strong&gt;work-in-progress&lt;/strong&gt;, and contains lots of differences from the paper. Also, many features are still not implemented now. &lt;strong&gt;The current generation quality cannot match the results from the original paper, and many prompts still fail badly!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notable differences from the paper&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since the Imagen model is not publicly available, we use &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; to replace it (implementation from &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;). Different from Imagen, Stable-Diffusion is a latent diffusion model, which diffuses in a latent space instead of the original image space. Therefore, we need the loss to propagate back from the VAE&#39;s encoder part too, which introduces extra time cost in training. Currently, 15000 training steps take about 5 hours to train on a V100.&lt;/li&gt; &#xA; &lt;li&gt;We use the &lt;a href=&#34;https://github.com/NVlabs/instant-ngp/&#34;&gt;multi-resolution grid encoder&lt;/a&gt; to implement the NeRF backbone (implementation from &lt;a href=&#34;https://github.com/ashawkey/torch-ngp&#34;&gt;torch-ngp&lt;/a&gt;), which enables much faster rendering (~10FPS at 800x800).&lt;/li&gt; &#xA; &lt;li&gt;We use the Adam optimizer with a larger initial learning rate.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The normal evaluation &amp;amp; shading part.&lt;/li&gt; &#xA; &lt;li&gt;Better mesh (improve the surface quality).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ashawkey/stable-dreamfusion.git&#xA;cd stable-dreamfusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: To download the Stable Diffusion model checkpoint, you should provide your &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;access token&lt;/a&gt;. You could choose either of the following ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and enter your token.&lt;/li&gt; &#xA; &lt;li&gt;Create a file called &lt;code&gt;TOKEN&lt;/code&gt; under this directory (i.e., &lt;code&gt;stable-dreamfusion/TOKEN&lt;/code&gt;) and copy your token into it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install with pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&#xA;# (optional) install nvdiffrast for exporting textured mesh (--save_mesh)&#xA;pip install git+https://github.com/NVlabs/nvdiffrast/&#xA;&#xA;# (optional) install the tcnn backbone if using --tcnn&#xA;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&#xA;&#xA;# (optional) install CLIP guidance for the dreamfield setting&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build extension (optional)&lt;/h3&gt; &#xA;&lt;p&gt;By default, we use &lt;a href=&#34;https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load&#34;&gt;&lt;code&gt;load&lt;/code&gt;&lt;/a&gt; to build the extension at runtime. We also provide the &lt;code&gt;setup.py&lt;/code&gt; to build each extension:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install all extension modules&#xA;bash scripts/install_ext.sh&#xA;&#xA;# if you want to install manually, here is an example:&#xA;pip install ./raymarching # install to python path (you still need the raymarching/ folder, since this only installs the built extension.)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tested environments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 22 with torch 1.12 &amp;amp; CUDA 11.6 on a V100.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;First time running will take some time to compile the CUDA extensions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;### stable-dreamfusion setting&#xA;## train with text prompt&#xA;# `-O` equals `--cuda_ray --fp16 --dir_text`&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial -O&#xA;&#xA;## after the training is finished:&#xA;# test (exporting 360 video, and an obj mesh with png texture)&#xA;python main.py --workspace trial -O --test&#xA;&#xA;# test with a GUI (free view control!)&#xA;python main.py --workspace trial -O --test --gui&#xA;&#xA;### dreamfields (CLIP) setting&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial_clip -O --guidance clip&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial_clip -O --test --gui --guidance clip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Code organization &amp;amp; Advanced tips&lt;/h1&gt; &#xA;&lt;p&gt;This is a simple description of the most important implementation details. If you are interested in improving this repo, this might be a starting point. Any contribution would be greatly appreciated!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The SDS loss is located at &lt;code&gt;./nerf/sd.py &amp;gt; StableDiffusion &amp;gt; train_step&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. we need to interpolate the NeRF rendering to 512x512, to feed it to SD&#39;s VAE.&#xA;pred_rgb_512 = F.interpolate(pred_rgb, (512, 512), mode=&#39;bilinear&#39;, align_corners=False)&#xA;# 2. image (512x512) --- VAE --&amp;gt; latents (64x64), this is SD&#39;s difference from Imagen.&#xA;latents = self.encode_imgs(pred_rgb_512)&#xA;... # timestep sampling, noise adding and UNet noise predicting&#xA;# 3. the SDS loss, since UNet part is ignored and cannot simply audodiff, we manually set the grad for latents.&#xA;w = (1 - self.scheduler.alphas_cumprod[t]).to(self.device)&#xA;grad = w * (noise_pred - noise)&#xA;latents.backward(gradient=grad, retain_graph=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Other regularizations are in &lt;code&gt;./nerf/utils.py &amp;gt; Trainer &amp;gt; train_step&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The generation seems quite sensitive to regularizations on weights_sum (alphas for each ray). The original opacity loss tends to make NeRF disappear (zero density everywhere), so we use an entropy loss to replace it for now (encourages alpha to be either 0 or 1).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;NeRF Rendering core function: &lt;code&gt;./nerf/renderer.py &amp;gt; NeRFRenderer &amp;gt; run_cuda&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;the occupancy grid based training acceleration (instant-ngp like, enabled by &lt;code&gt;--cuda_ray&lt;/code&gt;) may harm the generation progress, since once a grid cell is marked as empty, rays won&#39;t pass it later...&lt;/li&gt; &#xA;   &lt;li&gt;Not using &lt;code&gt;--cuda_ray&lt;/code&gt; also works now: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# `-O2` equals `--fp16 --dir_text`&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial -O2 # faster training, but slower rendering&#xA;&lt;/code&gt;&lt;/pre&gt; Training is faster if only sample 128 points uniformly per ray (5h --&amp;gt; 2.5h). More testing is needed...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Shading &amp;amp; normal evaluation: &lt;code&gt;./nerf/network*.py &amp;gt; NeRFNetwork &amp;gt; forward&lt;/code&gt;. Current implementation harms training and is disabled. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;use &lt;code&gt;--albedo_iters 1000&lt;/code&gt; to enable random shading mode after 1000 steps from albedo, lambertian, and textureless.&lt;/li&gt; &#xA;   &lt;li&gt;light direction: current implementation use a plane light source, instead of a point light source...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;View-dependent prompting: &lt;code&gt;./nerf/provider.py &amp;gt; get_view_direction&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ues &lt;code&gt;--angle_overhead, --angle_front&lt;/code&gt; to set the border. How to better divide front/back/side regions?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Network backbone (&lt;code&gt;./nerf/network*.py&lt;/code&gt;) can be chosen by the &lt;code&gt;--backbone&lt;/code&gt; option, but &lt;code&gt;tcnn&lt;/code&gt; and &lt;code&gt;vanilla&lt;/code&gt; are not well tested.&lt;/li&gt; &#xA; &lt;li&gt;Spatial density bias (gaussian density blob): &lt;code&gt;./nerf/network*.py &amp;gt; NeRFNetwork &amp;gt; gaussian&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The amazing original work: &lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;&lt;em&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@article{poole2022dreamfusion,&#xA;    author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},&#xA;    title = {DreamFusion: Text-to-3D using 2D Diffusion},&#xA;    journal = {arXiv},&#xA;    year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Huge thanks to the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; and the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;    title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;    author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;    year={2021},&#xA;    eprint={2112.10752},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{von-platen-etal-2022-diffusers,&#xA;    author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},&#xA;    title = {Diffusers: State-of-the-art diffusion models},&#xA;    year = {2022},&#xA;    publisher = {GitHub},&#xA;    journal = {GitHub repository},&#xA;    howpublished = {\url{https://github.com/huggingface/diffusers}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The GUI is developed with &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui&#34;&gt;DearPyGui&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aerdem4/lofo-importance</title>
    <updated>2022-10-09T01:37:51Z</updated>
    <id>tag:github.com,2022-10-09:/aerdem4/lofo-importance</id>
    <link href="https://github.com/aerdem4/lofo-importance" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Leave One Feature Out Importance&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aerdem4/lofo-importance/master/docs/lofo_logo.png?raw=true&#34; alt=&#34;alt text&#34; title=&#34;Title&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;LOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric.&lt;/p&gt; &#xA;&lt;p&gt;LOFO first evaluates the performance of the model with all the input features included, then iteratively removes one feature at a time, retrains the model, and evaluates its performance on a validation set. The mean and standard deviation (across the folds) of the importance of each feature is then reported.&lt;/p&gt; &#xA;&lt;p&gt;If a model is not passed as an argument to LOFO Importance, it will run LightGBM as a default model.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;LOFO Importance can be installed using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install lofo-importance&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Advantages of LOFO Importance&lt;/h2&gt; &#xA;&lt;p&gt;LOFO has several advantages compared to other importance types:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It does not favor granular features&lt;/li&gt; &#xA; &lt;li&gt;It generalises well to unseen test sets&lt;/li&gt; &#xA; &lt;li&gt;It is model agnostic&lt;/li&gt; &#xA; &lt;li&gt;It gives negative importance to features that hurt performance upon inclusion&lt;/li&gt; &#xA; &lt;li&gt;It can group the features. Especially useful for high dimensional features like TFIDF or OHE features.&lt;/li&gt; &#xA; &lt;li&gt;It can automatically group highly correlated features to avoid underestimating their importance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Example on Kaggle&#39;s Microsoft Malware Prediction Competition&lt;/h2&gt; &#xA;&lt;p&gt;In this Kaggle competition, Microsoft provides a malware dataset to predict whether or not a machine will soon be hit with malware. One of the features, Centos_OSVersion is very predictive on the training set, since some OS versions are probably more prone to bugs and failures than others. However, upon splitting the data out of time, we obtain validation sets with OS versions that have not occurred in the training set. Therefore, the model will not have learned the relationship between the target and this seasonal feature. By evaluating this feature&#39;s importance using other importance types, Centos_OSVersion seems to have high importance, because its importance was evaluated using only the training set. However, LOFO Importance depends on a validation scheme, so it will not only give this feature low importance, but even negative importance.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.model_selection import KFold&#xA;from lofo import LOFOImportance, Dataset, plot_importance&#xA;%matplotlib inline&#xA;&#xA;# import data&#xA;train_df = pd.read_csv(&#34;../input/train.csv&#34;, dtype=dtypes)&#xA;&#xA;# extract a sample of the data&#xA;sample_df = train_df.sample(frac=0.01, random_state=0)&#xA;sample_df.sort_values(&#34;AvSigVersion&#34;, inplace=True)&#xA;&#xA;# define the validation scheme&#xA;cv = KFold(n_splits=4, shuffle=False, random_state=0)&#xA;&#xA;# define the binary target and the features&#xA;dataset = Dataset(df=sample_df, target=&#34;HasDetections&#34;, features=[col for col in train_df.columns if col != target])&#xA;&#xA;# define the validation scheme and scorer. The default model is LightGBM&#xA;lofo_imp = LOFOImportance(dataset, cv=cv, scoring=&#34;roc_auc&#34;)&#xA;&#xA;# get the mean and standard deviation of the importances in pandas format&#xA;importance_df = lofo_imp.get_importance()&#xA;&#xA;# plot the means and standard deviations of the importances&#xA;plot_importance(importance_df, figsize=(12, 20))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aerdem4/lofo-importance/master/docs/plot_importance.png?raw=true&#34; alt=&#34;alt text&#34; title=&#34;Title&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Another Example: Kaggle&#39;s TReNDS Competition&lt;/h2&gt; &#xA;&lt;p&gt;In this Kaggle competition, pariticipants are asked to predict some cognitive properties of patients. Independent component features (IC) from sMRI and very high dimensional correlation features (FNC) from 3D fMRIs are provided. LOFO can group the fMRI correlation features into one.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;def get_lofo_importance(target):&#xA;    cv = KFold(n_splits=7, shuffle=True, random_state=17)&#xA;&#xA;    dataset = Dataset(df=df[df[target].notnull()], target=target, features=loading_features,&#xA;                      feature_groups={&#34;fnc&#34;: df[df[target].notnull()][fnc_features].values&#xA;                      })&#xA;&#xA;    model = Ridge(alpha=0.01)&#xA;    lofo_imp = LOFOImportance(dataset, cv=cv, scoring=&#34;neg_mean_absolute_error&#34;, model=model)&#xA;&#xA;    return lofo_imp.get_importance()&#xA;&#xA;plot_importance(get_lofo_importance(target=&#34;domain1_var1&#34;), figsize=(8, 8), kind=&#34;box&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aerdem4/lofo-importance/master/docs/plot_importance_box.png?raw=true&#34; alt=&#34;alt text&#34; title=&#34;Title&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Flofo Importance&lt;/h2&gt; &#xA;&lt;p&gt;If running the LOFO Importance package is too time-costly for you, you can use Fast LOFO. Fast LOFO, or FLOFO takes, as inputs, an already trained model and a validation set, and does a pseudo-random permutation on the values of each feature, one by one, then uses the trained model to make predictions on the validation set. The mean of the FLOFO importance is then the difference in the performance of the model on the validation set over several randomised permutations. The difference between FLOFO importance and permutation importance is that the permutations on a feature&#39;s values are done within groups, where groups are obtained by grouping the validation set by k=2 features. These k features are chosen at random n=10 times, and the mean and standard deviation of the FLOFO importance are calculated based on these n runs. The reason this grouping makes the measure of importance better is that permuting a feature&#39;s value is no longer completely random. In fact, the permutations are done within groups of similar samples, so the permutations are equivalent to noising the samples. This ensures that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The permuted feature values are very unlikely to be replaced by unrealistic values.&lt;/li&gt; &#xA; &lt;li&gt;A feature that is predictable by features among the chosen n*k features will be replaced by very similar values during permutation. Therefore, it will only slightly affect the model performance (and will yield a small FLOFO importance). This solves the correlated feature overestimation problem.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>t3l3machus/eviltree</title>
    <updated>2022-10-09T01:37:51Z</updated>
    <id>tag:github.com,2022-10-09:/t3l3machus/eviltree</id>
    <link href="https://github.com/t3l3machus/eviltree" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A python3 remake of the classic &#34;tree&#34; command with the additional feature of searching for user provided keywords/regex in files, highlighting those that contain matches.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EvilTree&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.x-yellow.svg?sanitize=true&#34; alt=&#34;Python 3.x&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/t3l3machus/eviltree/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-BSD-red.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/Maintained%3F-Yes-96c40f&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&#34; alt=&#34;Linux&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white&#34; alt=&#34;Windows&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A standalone python3 remake of the classic &#34;tree&#34; command with the additional feature of searching for user provided keywords/regex in files, highlighting those that contain matches. Created for two main reasons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;While searching for secrets in files of nested directory structures, being able to visualize which files contain user provided keywords/regex patterns and where those files are located in the hierarchy of folders, provides a significant advantage.&lt;/li&gt; &#xA; &lt;li&gt;&#34;tree&#34; is an amazing tool for analyzing directory structures. It&#39;s really handy to have a standalone alternative of the command for post-exploitation enumeration as it is not pre-installed on every linux distro and is kind of limited on Windows (compared to the UNIX version).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example #1&lt;/strong&gt;: Running a regex that essentially matches strings similar to: &lt;code&gt;password = something&lt;/code&gt; against &lt;code&gt;/var/www&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/75489922/193536337-188b1f0d-46ad-4680-b068-a4f1772734da.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example #2&lt;/strong&gt;: Using comma separated keywords instead of regex:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/75489922/193478656-a184ab55-0b3b-4f54-ada4-e658406503c1.png&#34; alt=&#34;image&#34;&gt;&lt;br&gt; &lt;strong&gt;Disclaimer&lt;/strong&gt;: Only tested on Windows 10 Pro.&lt;/p&gt; &#xA;&lt;h2&gt;Further Options &amp;amp; Usage Tips&lt;/h2&gt; &#xA;&lt;p&gt;Notable features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Regex &lt;code&gt;-x&lt;/code&gt; search actually returns a unique list of all matched patterns in a file. Be careful when combining it with &lt;code&gt;-v&lt;/code&gt; (--verbose), try to be specific and limit the length of chars to match.&lt;/li&gt; &#xA; &lt;li&gt;You can search keywords/regex in binary files as well by providing option &lt;code&gt;-b&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can use this tool as the classic &#34;tree&#34; command if you do not provide keywords &lt;code&gt;-k&lt;/code&gt; and regex &lt;code&gt;-x&lt;/code&gt; values. This is useful in case you have gained a limited shell on a machine and want to have &#34;tree&#34; with colored output to look around.&lt;/li&gt; &#xA; &lt;li&gt;There&#39;s a list variable &lt;code&gt;filetype_blacklist&lt;/code&gt; in &lt;code&gt;eviltree.py&lt;/code&gt; which can be used to exclude certain file extensions from content search. By default, it excludes the following: &lt;code&gt;gz, zip, tar, rar, 7z, bz2, xz, deb, img, iso, vmdk, dll, ovf, ova&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A quite useful feature is the &lt;code&gt;-i&lt;/code&gt; (--interesting-only) option. It instructs eviltree to list only files with matching keywords/regex content, significantly reducing the output length:&lt;br&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/75489922/193540467-7fa13d73-0893-491f-9b1b-89b34cae8ad7.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Useful keywords/regex patterns&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Regex to look for passwords: &lt;code&gt;-x &#34;.{0,3}passw.{0,3}[=]{1}.{0,18}&#34;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Keywords to look for sensitive info: &lt;code&gt;-k passw,db_,admin,account,user,token&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>