<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-25T01:36:55Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DeepLabCut/DeepLabCut</title>
    <updated>2024-10-25T01:36:55Z</updated>
    <id>tag:github.com,2024-10-25:/DeepLabCut/DeepLabCut</id>
    <link href="https://github.com/DeepLabCut/DeepLabCut" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of DeepLabCut: Markerless pose estimation of user-defined features with deep learning for all animals incl. humans&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1628250004229-KVYD7JJVHYEFDJ32L9VJ/DLClogo2021.jpg?format=1000w&#34; width=&#34;95%&#34;&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1665060917309-V0YVY2UKVLKSS6O18XDI/MousereachGIF.gif?format=1000w?format=180w&#34; height=&#34;150&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/daed7f16-527f-4150-8bdd-cbb20e267451/cheetah-ezgif.com-video-to-gif-converter.gif?format=180w&#34; height=&#34;150&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1534797521117-EIEUED03C68241QZ4KCK/ke17ZwdGBToddI8pDm48kAx9qLOWpcHWRGxWsJQSczRZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwdr4GYy30vFzf31Oe7KAPZKkqgaiEgc5jBNdhZmDPlzxdkDSclo6ofuXZm6YCEhUo/MATHIS_2018_fly.gif?format=180w&#34; height=&#34;150&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1619609897110-TKSTWKEM6HTGXID9D489/ke17ZwdGBToddI8pDm48kAvjv6tW_eojYQmNU0ncbllZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVHBSTXHtjUKlhRtWJ1Vo6l1B2bxJtByvWSjL6Vz3amc5yb8BodarTVrzIWCp72ioWw/triMouseDLC.gif?format=180w&#34; height=&#34;150&#34;&gt; &lt;/p&gt;&#xA; &lt;p&gt;&lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut/README.html&#34;&gt;üìöDocumentation&lt;/a&gt; | &lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut/docs/installation.html&#34;&gt;üõ†Ô∏è Installation&lt;/a&gt; | &lt;a href=&#34;https://www.deeplabcut.org&#34;&gt;üåé Home Page&lt;/a&gt; | &lt;a href=&#34;http://www.mackenziemathislab.org/deeplabcut/&#34;&gt;üêøüê¥üêÅüêòüêÜ Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut/README.html#news-and-in-the-news&#34;&gt;üö® News&lt;/a&gt; | &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/issues&#34;&gt;ü™≤ Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut/README.html#be-part-of-the-dlc-community&#34;&gt;ü´∂ Getting Assistance&lt;/a&gt; | &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/raw/master/DLCcourse.md&#34;&gt;‚àû DeepLabCut Online Course&lt;/a&gt; | &lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut/README.html#references&#34;&gt;üìù Publications&lt;/a&gt; | &lt;a href=&#34;https://www.deeplabcutairesidency.org/&#34;&gt;üë©üèæ‚Äçüíªüë®‚Äçüíª DeepLabCut AI Residency&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/python_version-3.10-purple&#34; alt=&#34;Vesion&#34;&gt; &lt;a href=&#34;https://pepy.tech/project/deeplabcut&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/deeplabcut&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/deeplabcut&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/deeplabcut/month&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/deeplabcut&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/deeplabcut.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/DeepLabCut/DeepLabCut/workflows/Python%20package/badge.svg?sanitize=true&#34; alt=&#34;Python package&#34;&gt; &lt;a href=&#34;https://www.gnu.org/licenses/lgpl-3.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-LGPL%20v3-blue.svg?sanitize=true&#34; alt=&#34;License: LGPL v3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img alt=&#34;Code style: black&#34; src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/DeepLabCut/DeepLabCut.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/deeplabcut/deeplabcut&#34; title=&#34;Average time to resolve an issue&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/resolution/deeplabcut/deeplabcut.svg?sanitize=true&#34; alt=&#34;Average time to resolve an issue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://isitmaintained.com/project/deeplabcut/deeplabcut&#34; title=&#34;Percentage of issues still open&#34;&gt;&lt;img src=&#34;http://isitmaintained.com/badge/open/deeplabcut/deeplabcut.svg?sanitize=true&#34; alt=&#34;Percentage of issues still open&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forum.image.sc/tag/deeplabcut&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;amp;query=%24.topic_list.tags.0.topic_count&amp;amp;colorB=brightgreen&amp;amp;&amp;amp;suffix=%20topics&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC&#34; alt=&#34;Image.sc forum&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/DeepLabCut/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/DeepLabCut/community.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/DeepLabCut&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&amp;amp;style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/DeepLabCut/DeepLabCut/main/CONTRIBUTING.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;Generic badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://czi.co/EOSS&#34;&gt;&lt;img src=&#34;https://chanzuckerberg.github.io/open-science/badges/CZI-EOSS.svg?sanitize=true&#34; alt=&#34;CZI&#39;s Essential Open Source Software for Science&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Welcome! üëã&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepLabCut‚Ñ¢Ô∏è&lt;/strong&gt; is a toolbox for state-of-the-art markerless pose estimation of animals performing various behaviors. As long as you can see (label) what you want to track, you can use this toolbox, as it is animal and object agnostic. &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut#why-use-deeplabcut&#34;&gt;Read a short development and application summary below&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut/docs/installation.html&#34;&gt;Installation: how to install DeepLabCut&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Please click the link above for all the information you need to get started! Please note that currently we support only Python 3.10+ (see conda files for guidance).&lt;/p&gt; &#xA;&lt;p&gt;Developers Stable Release:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Very quick start: You need to have TensorFlow installed (up to v2.10 supported across platforms) &lt;code&gt;pip install &#34;deeplabcut[gui,tf]&#34;&lt;/code&gt; that includes all functions plus GUIs, or &lt;code&gt;pip install deeplabcut[tf]&lt;/code&gt; (headless version with PyTorch and TensorFlow).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Developers Alpha Release:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We also have an alpha release of PyTorch DeepLabCut available! &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/raw/pytorch_docs/docs/pytorch/user_guide.md&#34;&gt;Please see here for instructions and information&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We recommend using our conda file, see &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/raw/main/conda-environments/README.md&#34;&gt;here&lt;/a&gt; or the new &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/tree/main/docker&#34;&gt;&lt;code&gt;deeplabcut-docker&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut&#34;&gt;Documentation: The DeepLabCut Process&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Our docs walk you through using DeepLabCut, and key API points. For an overview of the toolbox and workflow for project management, see our step-by-step at &lt;a href=&#34;https://doi.org/10.1038/s41596-019-0176-0&#34;&gt;Nature Protocols paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a deeper understanding and more resources for you to get started with Python and DeepLabCut, please check out our free online course! &lt;a href=&#34;http://DLCcourse.deeplabcut.org&#34;&gt;http://DLCcourse.deeplabcut.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1609244903687-US1SN063QIFJS4BP4IJD/ke17ZwdGBToddI8pDm48kFG9xAYub2PPnmh56PTVg7gUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcAju5e7u9RZJEVbVQPZRu9xb_m-kUO2M3I1IeDqD4l8YcGqu2nZPx1UhKV8wc1ELN/dlc_overview_whitebkgrnd.png?format=2500w&#34; width=&#34;95%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DeepLabCut/DeepLabCut/main/examples&#34;&gt;DEMO the code&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;üê≠ pose tracking of single animals demo &lt;a href=&#34;https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_DEMO_mouse_openfield.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üê≠üê≠üê≠ pose tracking of multiple animals demo &lt;a href=&#34;https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_3miceDemo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/raw/main/examples/README.md&#34;&gt;more demos here&lt;/a&gt;. We provide data and several Jupyter Notebooks: one that walks you through a demo dataset to test your installation, and another Notebook to run DeepLabCut from the beginning on your own data. We also show you how to use the code in Docker, and on Google Colab.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Why use DeepLabCut?&lt;/h1&gt; &#xA;&lt;p&gt;In 2018, we demonstrated the capabilities for &lt;a href=&#34;https://vnmurthylab.org/&#34;&gt;trail tracking&lt;/a&gt;, &lt;a href=&#34;http://www.mousemotorlab.org/&#34;&gt;reaching in mice&lt;/a&gt; and various Drosophila behaviors during egg-laying (see &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0209-y&#34;&gt;Mathis et al.&lt;/a&gt; for details). There is, however, nothing specific that makes the toolbox only applicable to these tasks and/or species. The toolbox has already been successfully applied (by us and others) to &lt;a href=&#34;http://www.mousemotorlab.org/deeplabcut&#34;&gt;rats&lt;/a&gt;, humans, various fish species, bacteria, leeches, various robots, cheetahs, &lt;a href=&#34;http://www.mousemotorlab.org/deeplabcut&#34;&gt;mouse whiskers&lt;/a&gt; and &lt;a href=&#34;http://www.mousemotorlab.org/deeplabcut&#34;&gt;race horses&lt;/a&gt;. DeepLabCut utilized the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut, which inspired the name for our toolbox (see references below). Since this time, the package has changed substantially. The code has been re-tooled and re-factored since 2.1+: We have added faster and higher performance variants with MobileNetV2s, EfficientNets, and our own DLCRNet backbones (see &lt;a href=&#34;https://arxiv.org/abs/1909.11229&#34;&gt;Pretraining boosts out-of-domain robustness for pose estimation&lt;/a&gt; and &lt;a href=&#34;https://www.nature.com/articles/s41592-022-01443-0&#34;&gt;Lauer et al 2022&lt;/a&gt;). Additionally, we have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support. In v3.0+ we have changed the backend to support PyTorch. This brings not only an easier installation process for users, but performance gains, developer flexibility, and a lot of new tools! Importantly, the high-level API stays the same, so it will be a seamless transition for users üíú! We currently provide state-of-the-art performance for animal pose estimation and the labs (M. Mathis Lab and A. Mathis Group) have both top journal and computer vision conference papers.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3e47258a922d548c483247/1547585339819/ErrorvsTrainingsetSize.png?format=750w&#34; height=&#34;160&#34;&gt; &lt;img src=&#34;https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3e469d8a922d548c4828fa/1547585194560/compressionrobustness.png?format=750w&#34; height=&#34;160&#34;&gt; &lt;img src=&#34;https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3fbed74fa51acecd63deeb/1547681534736/MouseLocomotion_warren.gif?format=500w&#34; height=&#34;160&#34;&gt; &lt;img src=&#34;https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3fc1c6758d46950ce7eec7/1547682383595/cheetah.png?format=750w&#34; height=&#34;160&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Left:&lt;/strong&gt; Due to transfer learning it requires &lt;strong&gt;little training data&lt;/strong&gt; for multiple, challenging behaviors (see &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0209-y&#34;&gt;Mathis et al. 2018&lt;/a&gt; for details). &lt;strong&gt;Mid Left:&lt;/strong&gt; The feature detectors are robust to video compression (see &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/10/30/457242&#34;&gt;Mathis/Warren&lt;/a&gt; for details). &lt;strong&gt;Mid Right:&lt;/strong&gt; It allows 3D pose estimation with a single network and camera (see &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/10/30/457242&#34;&gt;Mathis/Warren&lt;/a&gt;). &lt;strong&gt;Right:&lt;/strong&gt; It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods (see &lt;a href=&#34;https://doi.org/10.1038/s41596-019-0176-0&#34;&gt;Nath* and Mathis* et al. 2019&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepLabCut&lt;/strong&gt; is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. Moreover, many new tools are being actively developed. See &lt;a href=&#34;https://github.com/DeepLabCut/DLCutils&#34;&gt;DLC-Utils&lt;/a&gt; for some helper code.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588292233203-FD1DVKAQYNV2TU91CO7R/ke17ZwdGBToddI8pDm48kIX24IsDPzy6M4KUaihfICJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIxtGUdkzp028KVNnpOijF3PweOM5su6FUQHO6Wkh72Nw/dlc_eco.gif?format=1000w&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Code contributors:&lt;/h2&gt; &#xA;&lt;p&gt;DLC code was originally developed by &lt;a href=&#34;https://github.com/AlexEMG&#34;&gt;Alexander Mathis&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://github.com/MMathisLab&#34;&gt;Mackenzie Mathis&lt;/a&gt;, and was extended in 2.0 with the core dev team consisting of &lt;a href=&#34;https://github.com/meet10may&#34;&gt;Tanmay Nath&lt;/a&gt; (2.0-2.1), and currently (2.1+) with &lt;a href=&#34;https://github.com/jeylau&#34;&gt;Jessy Lauer&lt;/a&gt; and (2.3+) &lt;a href=&#34;https://github.com/n-poulsen&#34;&gt;Niels Poulsen&lt;/a&gt;. DeepLabCut is an open-source tool and has benefited from suggestions and edits by many individuals including Mert Yuksekgonul, Tom Biasi, Richard Warren, Ronny Eichler, Hao Wu, Federico Claudi, Gary Kane and Jonny Saunders as well as the &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/graphs/contributors&#34;&gt;100+ contributors&lt;/a&gt;. Please see &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/raw/master/AUTHORS&#34;&gt;AUTHORS&lt;/a&gt; for more details!&lt;/p&gt; &#xA;&lt;p&gt;This is an actively developed package and we welcome community development and involvement.&lt;/p&gt; &#xA;&lt;h1&gt;Get Assistance &amp;amp; be part of the DLC Community‚ú®:&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;üöâ Platform&lt;/th&gt; &#xA;   &lt;th&gt;üéØ Goal&lt;/th&gt; &#xA;   &lt;th&gt;‚è±Ô∏è Estimated Response Time&lt;/th&gt; &#xA;   &lt;th&gt;üì¢ Support Squad&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://forum.image.sc/tag/deeplabcut&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;amp;query=%24.topic_list.tags.0.topic_count&amp;amp;colorB=brightgreen&amp;amp;&amp;amp;suffix=%20topics&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC&#34; alt=&#34;Image.sc forum&#34;&gt;&lt;/a&gt; &lt;br&gt; üê≠Tag: DeepLabCut&lt;/td&gt; &#xA;   &lt;td&gt;To ask help and support questionsüëã&lt;/td&gt; &#xA;   &lt;td&gt;Promptlyüî•&lt;/td&gt; &#xA;   &lt;td&gt;DLC Team and The DLC Community&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GitHub DeepLabCut/&lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/issues&#34;&gt;Issues&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;To report bugs and code issuesüêõ (we encourage you to search issues first)&lt;/td&gt; &#xA;   &lt;td&gt;2-3 days&lt;/td&gt; &#xA;   &lt;td&gt;DLC Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gitter.im/DeepLabCut/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/DeepLabCut/community.svg?sanitize=true&#34; alt=&#34;Gitter&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;To discuss with other users, share ideas and collaborateüí°&lt;/td&gt; &#xA;   &lt;td&gt;2 days&lt;/td&gt; &#xA;   &lt;td&gt;The DLC Community&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GitHub DeepLabCut/&lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/raw/master/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;To contribute your expertise and experienceüôèüíØ&lt;/td&gt; &#xA;   &lt;td&gt;Promptlyüî•&lt;/td&gt; &#xA;   &lt;td&gt;DLC Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üöß GitHub DeepLabCut/&lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/raw/master/docs/roadmap.md&#34;&gt;Roadmap&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;To learn more about our journey‚úàÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/DeepLabCut&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&amp;amp;style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;To keep up with our latest news and updates üì¢&lt;/td&gt; &#xA;   &lt;td&gt;Daily&lt;/td&gt; &#xA;   &lt;td&gt;DLC Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;The DeepLabCut &lt;a href=&#34;https://www.deeplabcutairesidency.org/&#34;&gt;AI Residency Program&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;To come and work with us next summerüëè&lt;/td&gt; &#xA;   &lt;td&gt;Annually&lt;/td&gt; &#xA;   &lt;td&gt;DLC Team&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;References:&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code or data we kindly ask that you please &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0209-y&#34;&gt;cite Mathis et al, 2018&lt;/a&gt; and, if you use the Python package (DeepLabCut2.x) please also cite &lt;a href=&#34;https://doi.org/10.1038/s41596-019-0176-0&#34;&gt;Nath, Mathis et al, 2019&lt;/a&gt;. If you utilize the MobileNetV2s or EfficientNets please cite &lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2021/papers/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.pdf&#34;&gt;Mathis, Biasi et al. 2021&lt;/a&gt;. If you use versions 2.2beta+ or 2.2rc1+, please cite &lt;a href=&#34;https://www.nature.com/articles/s41592-022-01443-0&#34;&gt;Lauer et al. 2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;DOIs (#ProTip, for helping you find citations for software, check out &lt;a href=&#34;http://citeas.org/&#34;&gt;CiteAs.org&lt;/a&gt;!):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mathis et al 2018: &lt;a href=&#34;https://doi.org/10.1038/s41593-018-0209-y&#34;&gt;10.1038/s41593-018-0209-y&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nath, Mathis et al 2019: &lt;a href=&#34;https://doi.org/10.1038/s41596-019-0176-0&#34;&gt;10.1038/s41596-019-0176-0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Lauer et al 2022: &lt;a href=&#34;https://doi.org/10.1038/s41592-022-01443-0&#34;&gt;10.1038/s41592-022-01443-0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please check out the following references for more details:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{Mathisetal2018,&#xA;    title = {DeepLabCut: markerless pose estimation of user-defined body parts with deep learning},&#xA;    author = {Alexander Mathis and Pranav Mamidanna and Kevin M. Cury and Taiga Abe  and Venkatesh N. Murthy and Mackenzie W. Mathis and Matthias Bethge},&#xA;    journal = {Nature Neuroscience},&#xA;    year = {2018},&#xA;    url = {https://www.nature.com/articles/s41593-018-0209-y}}&#xA;&#xA; @article{NathMathisetal2019,&#xA;    title = {Using DeepLabCut for 3D markerless pose estimation across species and behaviors},&#xA;    author = {Nath*, Tanmay and Mathis*, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie W},&#xA;    journal = {Nature Protocols},&#xA;    year = {2019},&#xA;    url = {https://doi.org/10.1038/s41596-019-0176-0}}&#xA;    &#xA;@InProceedings{Mathis_2021_WACV,&#xA;    author    = {Mathis, Alexander and Biasi, Thomas and Schneider, Steffen and Yuksekgonul, Mert and Rogers, Byron and Bethge, Matthias and Mathis, Mackenzie W.},&#xA;    title     = {Pretraining Boosts Out-of-Domain Robustness for Pose Estimation},&#xA;    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},&#xA;    month     = {January},&#xA;    year      = {2021},&#xA;    pages     = {1859-1868}}&#xA;    &#xA;@article{Lauer2022MultianimalPE,&#xA;    title={Multi-animal pose estimation, identification and tracking with DeepLabCut},&#xA;    author={Jessy Lauer and Mu Zhou and Shaokai Ye and William Menegas and Steffen Schneider and Tanmay Nath and Mohammed Mostafizur Rahman and     Valentina Di Santo and Daniel Soberanes and Guoping Feng and Venkatesh N. Murthy and George Lauder and Catherine Dulac and M. Mathis and Alexander Mathis},&#xA;    journal={Nature Methods},&#xA;    year={2022},&#xA;    volume={19},&#xA;    pages={496 - 504}}&#xA;&#xA;@article{insafutdinov2016eccv,&#xA;    title = {DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model},&#xA;    author = {Eldar Insafutdinov and Leonid Pishchulin and Bjoern Andres and Mykhaylo Andriluka and Bernt Schiele},&#xA;    booktitle = {ECCV&#39;16},&#xA;    url = {http://arxiv.org/abs/1605.03170}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Review &amp;amp; Educational articles:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{Mathis2020DeepLT,&#xA;    title={Deep learning tools for the measurement of animal behavior in neuroscience},&#xA;    author={Mackenzie W. Mathis and Alexander Mathis},&#xA;    journal={Current Opinion in Neurobiology},&#xA;    year={2020},&#xA;    volume={60},&#xA;    pages={1-11}}&#xA;&#xA;@article{Mathis2020Primer,&#xA;    title={A Primer on Motion Capture with Deep Learning: Principles, Pitfalls, and Perspectives},&#xA;    author={Alexander Mathis and Steffen Schneider and Jessy Lauer and Mackenzie W. Mathis},&#xA;    journal={Neuron},&#xA;    year={2020},&#xA;    volume={108},&#xA;    pages={44-65}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other open-access pre-prints related to our work on DeepLabCut:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{MathisWarren2018speed,&#xA;    author = {Mathis, Alexander and Warren, Richard A.},&#xA;    title = {On the inference speed and video-compression robustness of DeepLabCut},&#xA;    year = {2018},&#xA;    doi = {10.1101/457242},&#xA;    publisher = {Cold Spring Harbor Laboratory},&#xA;    URL = {https://www.biorxiv.org/content/early/2018/10/30/457242},&#xA;    eprint = {https://www.biorxiv.org/content/early/2018/10/30/457242.full.pdf},&#xA;    journal = {bioRxiv}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License:&lt;/h2&gt; &#xA;&lt;p&gt;This project is primarily licensed under the GNU Lesser General Public License v3.0. Note that the software is provided &#34;as is&#34;, without warranty of any kind, express or implied. If you use the code or data, please cite us! Note, artwork (DeepLabCut logo) and images are copyrighted; please do not take or use these images without written permission.&lt;/p&gt; &#xA;&lt;p&gt;SuperAnimal models are provided for research use only (non-commercial use).&lt;/p&gt; &#xA;&lt;h2&gt;Major Versions:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For all versions, please see &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/releases&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;VERSION 3.0: A whole new experience with PyTorchüî•. While the high-level API remains the same, the backend and developer friendliness have greatly improved, along with performance gains!&lt;/p&gt; &#xA;&lt;p&gt;VERSION 2.3: Model Zoo SuperAnimals, and a whole new GUI experience.&lt;/p&gt; &#xA;&lt;p&gt;VERSION 2.2: Multi-animal pose estimation, identification, and tracking with DeepLabCut is supported (as well as single-animal projects).&lt;/p&gt; &#xA;&lt;p&gt;VERSION 2.0-2.1: This is the &lt;strong&gt;Python package&lt;/strong&gt; of &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0209-y&#34;&gt;DeepLabCut&lt;/a&gt; that was originally released in Oct 2018 with our &lt;a href=&#34;https://doi.org/10.1038/s41596-019-0176-0&#34;&gt;Nature Protocols&lt;/a&gt; paper (preprint &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/476531v1&#34;&gt;here&lt;/a&gt;). This package includes graphical user interfaces to label your data, and take you from data set creation to automatic behavioral analysis. It also introduces an active learning framework to efficiently use DeepLabCut on large experimental projects, and data augmentation tools that improve network performance, especially in challenging cases (see &lt;a href=&#34;https://camo.githubusercontent.com/77c92f6b89d44ca758d815bdd7e801247437060b/68747470733a2f2f737461746963312e73717561726573706163652e636f6d2f7374617469632f3537663664353163396637343536366635356563663237312f742f3563336663316336373538643436393530636537656563372f313534373638323338333539352f636865657461682e706e673f666f726d61743d37353077&#34;&gt;panel b&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;VERSION 1.0: The initial, Nature Neuroscience version of &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0209-y&#34;&gt;DeepLabCut&lt;/a&gt; can be found in the history of git, or here: &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/releases/tag/1.11&#34;&gt;https://github.com/DeepLabCut/DeepLabCut/releases/tag/1.11&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;News (and in the news):&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span&gt;üíú&lt;/span&gt; We released a major update, moving from 2.x --&amp;gt; 3.x with the backend change to PyTorch&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üíú&lt;/span&gt; The DeepLabCut Model Zoo launches SuperAnimals, see more &lt;a href=&#34;http://www.mackenziemathislab.org/dlc-modelzoo/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üíú&lt;/span&gt; &lt;strong&gt;DeepLabCut supports multi-animal pose estimation!&lt;/strong&gt; maDLC is out of beta/rc mode and beta is deprecated, thanks to the testers out there for feedback! Your labeled data will be backwards compatible, but not all other steps. Please see the &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/releases&#34;&gt;new &lt;code&gt;2.2+&lt;/code&gt; releases&lt;/a&gt; for what&#39;s new &amp;amp; how to install it, please see our new &lt;a href=&#34;https://www.nature.com/articles/s41592-022-01443-0&#34;&gt;paper, Lauer et al 2022&lt;/a&gt;, and the &lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut&#34;&gt;new docs&lt;/a&gt; on how to use it!&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üíú&lt;/span&gt; We support multi-animal re-identification, see &lt;a href=&#34;https://www.nature.com/articles/s41592-022-01443-0&#34;&gt;Lauer et al 2022&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üíú&lt;/span&gt; We have a &lt;strong&gt;real-time&lt;/strong&gt; package available! &lt;a href=&#34;http://DLClive.deeplabcut.org&#34;&gt;http://DLClive.deeplabcut.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;June 2024: Our second DLC paper &lt;a href=&#34;https://www.nature.com/articles/s41596-019-0176-0&#34;&gt;&#39;Using DeepLabCut for 3D markerless pose estimation across species and behaviors&#39;&lt;/a&gt; in Nature Protocols has surpassed 1,000 Google Scholar citations!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;May 2024: DeepLabCut was featured in Nature: &lt;a href=&#34;https://www.nature.com/articles/d41586-024-01474-x&#34;&gt;&#39;DeepLabCut: the motion-tracking tool that went viral&#39;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;January 2024: Our original paper &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0209-y&#34;&gt;&#39;DeepLabCut: markerless pose estimation of user-defined body parts with deep learning&#39;&lt;/a&gt; in Nature Neuroscience has surpassed 3,000 Google Scholar citations!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;December 2023: DeepLabCut hit 600,000 downloads!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;October 2023: DeepLabCut celebrates a milestone with 4,000 üåü in Github!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;July 2023: The user forum is very active with more than 1k questions and answers: &lt;a href=&#34;https://forum.image.sc/tag/deeplabcut&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;amp;query=%24.topic_list.tags.0.topic_count&amp;amp;colorB=brightgreen&amp;amp;&amp;amp;suffix=%20topics&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC&#34; alt=&#34;Image.sc forum&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;May 2023: The Model Zoo is now fully integrated into the DeepLabCut GUI, making it easier than ever to access a variety of pre-trained models. Check out the accompanying paper: &lt;a href=&#34;https://arxiv.org/abs/2203.07436&#34;&gt;SuperAnimal pretrained pose estimation models for behavioral analysis by Ye et al.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;December 2022: DeepLabCut hits 450,000 downloads and 2.3 is the new stable release&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;August 2022: DeepLabCut hit 400,000 downloads&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;August 2021: 2.2 becomes the new stable release for DeepLabCut.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;July 2021: Docs are now at &lt;a href=&#34;https://deeplabcut.github.io/DeepLabCut&#34;&gt;https://deeplabcut.github.io/DeepLabCut&lt;/a&gt; and we now include TensorFlow 2 support!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;May 2021: DeepLabCut hit 200,000 downloads! Also, Our preprint on 2.2, multi-animal DeepLabCut is released!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jan 2021: &lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html&#34;&gt;Pretraining boosts out-of-domain robustness for pose estimation&lt;/a&gt; published in the IEEE Winter Conference on Applications of Computer Vision. We also added EfficientNet backbones to DeepLabCut, those are best trained with cosine decay (see paper). To use them, just pass &#34;&lt;code&gt;efficientnet-b0&lt;/code&gt;&#34; to &#34;&lt;code&gt;efficientnet-b6&lt;/code&gt;&#34; when creating the trainingset!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Dec 2020: We released a real-time package that allows for online pose estimation and real-time feedback. See &lt;a href=&#34;http://DLClive.deeplabcut.org&#34;&gt;DLClive.deeplabcut.org&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;5/22 2020: We released 2.2beta5. This beta release has some of the features of DeepLabCut 2.2, whose major goal is to integrate multi-animal pose estimation to DeepLabCut.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mar 2020: Inspired by suggestions we heard at this weeks CZI&#39;s Essential Open Source Software meeting in Berkeley, CA we updated our &lt;a href=&#34;https://raw.githubusercontent.com/DeepLabCut/DeepLabCut/main/docs/UseOverviewGuide.md&#34;&gt;docs&lt;/a&gt;. Let us know what you think!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Feb 2020: Our &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0959438819301151&#34;&gt;review on animal pose estimation is published!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Nov 2019: DeepLabCut was recognized by the Chan Zuckerberg Initiative (CZI) with funding to support this project. Read more in the &lt;a href=&#34;https://news.harvard.edu/gazette/story/newsplus/harvard-researchers-awarded-czi-open-source-award/&#34;&gt;Harvard Gazette&lt;/a&gt;, on &lt;a href=&#34;https://chanzuckerberg.com/eoss/proposals/&#34;&gt;CZI&#39;s Essential Open Source Software for Science site&lt;/a&gt; and in their &lt;a href=&#34;https://medium.com/@cziscience/how-open-source-software-contributors-are-accelerating-biomedicine-1a5f50f6846a&#34;&gt;Medium post&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Oct 2019: DLC 2.1 released with lots of updates. In particular, a Project Manager GUI, MobileNetsV2, and augmentation packages (Imgaug and Tensorpack). For detailed updates see &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/releases&#34;&gt;releases&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Sept 2019: We published two preprints. One showing that &lt;a href=&#34;https://arxiv.org/abs/1909.11229&#34;&gt;ImageNet pretraining contributes to robustness&lt;/a&gt; and a &lt;a href=&#34;https://arxiv.org/abs/1909.13868&#34;&gt;review on animal pose estimation&lt;/a&gt;. Check them out!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jun 2019: DLC 2.0.7 released with lots of updates. For updates see &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut/releases&#34;&gt;releases&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Feb 2019: DeepLabCut joined &lt;a href=&#34;https://twitter.com/deeplabcut&#34;&gt;twitter&lt;/a&gt; &lt;a href=&#34;https://twitter.com/DeepLabCut&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&amp;amp;style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jan 2019: We hosted workshops for DLC in Warsaw, Munich and Cambridge. The materials are available &lt;a href=&#34;https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jan 2019: We joined the Image Source Forum for user help: &lt;a href=&#34;https://forum.image.sc/tag/deeplabcut&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;amp;query=%24.topic_list.tags.0.topic_count&amp;amp;colorB=brightgreen&amp;amp;&amp;amp;suffix=%20topics&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC&#34; alt=&#34;Image.sc forum&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Nov 2018: We posted a detailed guide for DeepLabCut 2.0 on &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/11/24/476531&#34;&gt;BioRxiv&lt;/a&gt;. It also contains a case study for 3D pose estimation in cheetahs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Nov 2018: Various (post-hoc) analysis scripts contributed by users (and us) will be gathered at &lt;a href=&#34;https://github.com/DeepLabCut/DLCutils&#34;&gt;DLCutils&lt;/a&gt;. Feel free to contribute! In particular, there is a script guiding you through importing a project into the new data format for DLC 2.0&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Oct 2018: new pre-print on the speed video-compression and robustness of DeepLabCut on &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/10/30/457242&#34;&gt;BioRxiv&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Sept 2018: Nature Lab Animal covers DeepLabCut: &lt;a href=&#34;https://www.nature.com/articles/s41684-018-0164-y&#34;&gt;Behavior tracking cuts deep&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Kunlin Wei &amp;amp; Konrad Kording write a very nice News &amp;amp; Views on our paper: &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0215-0&#34;&gt;Behavioral Tracking Gets Real&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;August 2018: Our &lt;a href=&#34;https://arxiv.org/abs/1804.03142&#34;&gt;preprint&lt;/a&gt; appeared in &lt;a href=&#34;https://www.nature.com/articles/s41593-018-0209-y&#34;&gt;Nature Neuroscience&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;August 2018: NVIDIA AI Developer News: &lt;a href=&#34;https://news.developer.nvidia.com/ai-enables-markerless-animal-tracking/&#34;&gt;AI Enables Markerless Animal Tracking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;July 2018: Ed Yong covered DeepLabCut and interviewed several users for the &lt;a href=&#34;https://www.theatlantic.com/science/archive/2018/07/deeplabcut-tracking-animal-movements/564338&#34;&gt;Atlantic&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;April 2018: first DeepLabCut preprint on &lt;a href=&#34;https://arxiv.org/abs/1804.03142&#34;&gt;arXiv.org&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Funding&lt;/h2&gt; &lt;p&gt;We are grateful for the follow support over the years! This software project was supported in part by the Essential Open Source Software for Science (EOSS) program at Chan Zuckerberg Initiative (cycles 1, 3, 3-DEI, 6). We also thank the Rowland Institute at Harvard for funding from 2017-2020, and EPFL from 2020-present.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>