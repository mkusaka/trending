<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-15T02:56:32Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>everythingishacked/Semaphore</title>
    <updated>2023-04-15T02:56:32Z</updated>
    <id>tag:github.com,2023-04-15:/everythingishacked/Semaphore</id>
    <link href="https://github.com/everythingishacked/Semaphore" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A full-body keyboard using gestures to type through computer vision&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Semaphore&lt;/h1&gt; &#xA;&lt;h3&gt;A full-body keyboard&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/everythingishacked/Semaphore/main/demo.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;View a fuller demo and more background on the project at &lt;a href=&#34;https://youtu.be/h376W93gQq4&#34;&gt;https://youtu.be/h376W93gQq4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Semaphore uses &lt;a href=&#34;https://github.com/opencv/opencv-python&#34;&gt;OpenCV&lt;/a&gt; and MediaPipe&#39;s &lt;a href=&#34;https://google.github.io/mediapipe/solutions/pose.html#python-solution-api&#34;&gt;Pose detection&lt;/a&gt; to perform real-time detection of body landmarks from video input. From there, relative differences are calculated to determine specific positions and translate those into keys and commands sent via &lt;a href=&#34;https://github.com/boppreh/keyboard&#34;&gt;keyboard&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary input is to &#34;type&#34; letters, digits, and symbols via &lt;a href=&#34;https://en.wikipedia.org/wiki/Flag_semaphore&#34;&gt;flag semaphore&lt;/a&gt; by extending both arms at various angles. Rather than waiting a set time after every signal, you can jump to repeat the last sent symbol.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;code&gt;SEMAPHORES&lt;/code&gt; dictionary in the code for a full set of angles, which mostly conform to standard US semaphore with some custom additions. Most of the rest of the keyboard is included as other modifier gestures, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;shift&lt;/code&gt;: open both hands, instead of fists&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;backspace&lt;/code&gt;: both hands over mouth&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;digits&lt;/code&gt; and other extra symbols: squat while signaling&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;command&lt;/code&gt;: lift left leg to ~horizontal thigh&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;control&lt;/code&gt;: lift right leg to ~horizontal thigh&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;arrow left/right/up/down&lt;/code&gt;: cross arms and raise each straight leg &lt;code&gt;LEG_ARROW_ANGLE&lt;/code&gt; degrees&lt;/li&gt; &#xA; &lt;li&gt;repeat previous letter/command: jump&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Running on latest MacOS from Terminal, toggle the following for keyboard access: System Settings -&amp;gt; Privacy &amp;amp; Security -&amp;gt; Accessibility -&amp;gt; Terminal -&amp;gt; slide to allow&lt;/p&gt; &#xA;&lt;p&gt;For Mac, this uses a &lt;a href=&#34;https://github.com/everythingishacked/keyboard&#34;&gt;custom keyboard library&lt;/a&gt;. This is built for a Mac keyboard, but you can also swap e.g. Windows key for Command simply enough.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/consistency_models</title>
    <updated>2023-04-15T02:56:32Z</updated>
    <id>tag:github.com,2023-04-15:/openai/consistency_models</id>
    <link href="https://github.com/openai/consistency_models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repo for consistency models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Consistency Models&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the codebase for &lt;a href=&#34;https://arxiv.org/abs/2303.01469&#34;&gt;Consistency Models&lt;/a&gt;, implemented using PyTorch for conducting large-scale experiments on ImageNet-64, LSUN Bedroom-256, and LSUN Cat-256. We have based our repository on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;openai/guided-diffusion&lt;/a&gt;, which was initially released under the MIT license. Our modifications have enabled support for consistency distillation, consistency training, as well as several sampling and editing algorithms discussed in the paper.&lt;/p&gt; &#xA;&lt;p&gt;The repository for CIFAR-10 experiments is in JAX and will be released separately.&lt;/p&gt; &#xA;&lt;h1&gt;Pre-trained models&lt;/h1&gt; &#xA;&lt;p&gt;We have released checkpoints for the main models in the paper. Before using these models, please review the corresponding &lt;a href=&#34;https://raw.githubusercontent.com/openai/consistency_models/main/model-card.md&#34;&gt;model card&lt;/a&gt; to understand the intended use and limitations of these models.&lt;/p&gt; &#xA;&lt;p&gt;Here are the download links for each model checkpoint:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EDM on ImageNet-64: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/edm_imagenet64_ema.pt&#34;&gt;edm_imagenet64_ema.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CD on ImageNet-64 with l2 metric: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/cd_imagenet64_l2.pt&#34;&gt;cd_imagenet64_l2.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CD on ImageNet-64 with LPIPS metric: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/cd_imagenet64_lpips.pt&#34;&gt;cd_imagenet64_lpips.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CT on ImageNet-64: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/ct_imagenet64.pt&#34;&gt;ct_imagenet64.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;EDM on LSUN Bedroom-256: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/edm_bedroom256_ema.pt&#34;&gt;edm_bedroom256_ema.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CD on LSUN Bedroom-256 with l2 metric: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/cd_bedroom256_l2.pt&#34;&gt;cd_bedroom256_l2.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CD on LSUN Bedroom-256 with LPIPS metric: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/cd_bedroom256_lpips.pt&#34;&gt;cd_bedroom256_lpips.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CT on LSUN Bedroom-256: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/ct_bedroom256.pt&#34;&gt;ct_bedroom256.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;EDM on LSUN Cat-256: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/edm_cat256_ema.pt&#34;&gt;edm_cat256_ema.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CD on LSUN Cat-256 with l2 metric: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/cd_cat256_l2.pt&#34;&gt;cd_cat256_l2.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CD on LSUN Cat-256 with LPIPS metric: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/cd_cat256_lpips.pt&#34;&gt;cd_cat256_lpips.pt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CT on LSUN Cat-256: &lt;a href=&#34;https://openaipublic.blob.core.windows.net/consistency/ct_cat256.pt&#34;&gt;ct_cat256.pt&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Dependencies&lt;/h1&gt; &#xA;&lt;p&gt;To install all packages in this codebase along with their dependencies, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Model training and sampling&lt;/h1&gt; &#xA;&lt;p&gt;We provide examples of EDM training, consistency distillation, consistency training, single-step generation, and multistep generation in &lt;a href=&#34;https://raw.githubusercontent.com/openai/consistency_models/main/scripts/launch.sh&#34;&gt;cm/scripts/launch.sh&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Evaluations&lt;/h1&gt; &#xA;&lt;p&gt;To compare different generative models, we use FID, Precision, Recall, and Inception Score. These metrics can all be calculated using batches of samples stored in &lt;code&gt;.npz&lt;/code&gt; (numpy) files. One can evaluate samples with &lt;a href=&#34;https://raw.githubusercontent.com/openai/consistency_models/main/evaluations/evaluator.py&#34;&gt;cm/evaluations/evaluator.py&lt;/a&gt; in the same way as described in &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;openai/guided-diffusion&lt;/a&gt;, with reference dataset batches provided therein.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find this method and/or code useful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{song2023consistency,&#xA;  title={Consistency Models},&#xA;  author={Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},&#xA;  journal={arXiv preprint arXiv:2303.01469},&#xA;  year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>anuragxel/salt</title>
    <updated>2023-04-15T02:56:32Z</updated>
    <id>tag:github.com,2023-04-15:/anuragxel/salt</id>
    <link href="https://github.com/anuragxel/salt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Segment Anything Labelling Tool&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Segment Anything Labelling Tool (SALT)&lt;/h1&gt; &#xA;&lt;p&gt;Uses the Segment-Anything Model By Meta AI and adds a barebones interface to label images and saves the masks in the COCO format.&lt;/p&gt; &#xA;&lt;p&gt;Under active development, apologies for rough edges and bugs. Use at your own risk.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment Anything&lt;/a&gt; on any machine with a GPU. (Need not be the labelling machine.)&lt;/li&gt; &#xA; &lt;li&gt;Create a conda environment using &lt;code&gt;conda env create -f environment.yaml&lt;/code&gt; on the labelling machine (Need not have GPU).&lt;/li&gt; &#xA; &lt;li&gt;(Optional) Install &lt;a href=&#34;https://github.com/trsvchn/coco-viewer&#34;&gt;coco-viewer&lt;/a&gt; to scroll through your annotations quickly.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup your dataset in the following format &lt;code&gt;&amp;lt;dataset_name&amp;gt;/images/*&lt;/code&gt; and create empty folder &lt;code&gt;&amp;lt;dataset_name&amp;gt;/embeddings&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Annotations will be saved in &lt;code&gt;&amp;lt;dataset_name&amp;gt;/annotations.json&lt;/code&gt; by default.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Copy the &lt;code&gt;helpers&lt;/code&gt; scripts to the base folder of your &lt;code&gt;segment-anything&lt;/code&gt; folder. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Call &lt;code&gt;extract_embeddings.py&lt;/code&gt; to extract embeddings for your images.&lt;/li&gt; &#xA;   &lt;li&gt;Call &lt;code&gt;generate_onnx.py&lt;/code&gt; generate &lt;code&gt;*.onnx&lt;/code&gt; files in models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Copy the models in &lt;code&gt;models&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Symlink your dataset in the SALT&#39;s root folder as &lt;code&gt;&amp;lt;dataset_name&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Call &lt;code&gt;segment_anything_annotator.py&lt;/code&gt; with argument &lt;code&gt;&amp;lt;dataset_name&amp;gt;&lt;/code&gt; and categories &lt;code&gt;cat1,cat2,cat3..&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;There are a few keybindings that make the annotation process fast.&lt;/li&gt; &#xA;   &lt;li&gt;Click on the object using left clicks and right click (to indicate outside object boundary).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n&lt;/code&gt; adds predicted mask into your annotations. (Add button)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;r&lt;/code&gt; rejects the predicted mask. (Reject button)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;a&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; to cycle through images in your your set. (Next and Prev)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;l&lt;/code&gt; and &lt;code&gt;k&lt;/code&gt; to increase and decrease the transparency of the other annotations.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Ctrl + S&lt;/code&gt; to save progress to the COCO-style annotations file.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/trsvchn/coco-viewer&#34;&gt;coco-viewer&lt;/a&gt; to view your annotations. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;python cocoviewer.py -i &amp;lt;dataset&amp;gt; -a &amp;lt;dataset&amp;gt;/annotations.json&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/anuragxel/salt/raw/main/assets/how-it-works.gif&#34; alt=&#34;How it Works Gif!&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Follow these guidelines to ensure that your contributions can be reviewed and merged. Need a lot of help in making the UI better.&lt;/p&gt; &#xA;&lt;p&gt;If you have found a bug or have an idea for an improvement or new feature, please create an issue on GitHub. Before creating a new issue, please search existing issues to see if your issue has already been reported.&lt;/p&gt; &#xA;&lt;p&gt;When creating an issue, please include as much detail as possible, including steps to reproduce the issue if applicable.&lt;/p&gt; &#xA;&lt;p&gt;Create a pull request (PR) to the original repository. Please use &lt;code&gt;black&lt;/code&gt; formatter when making code changes.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code is licensed under the MIT License. By contributing to SALT, you agree to license your contributions under the same license as the project. See LICENSE for more information.&lt;/p&gt;</summary>
  </entry>
</feed>