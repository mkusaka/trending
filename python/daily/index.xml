<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-20T01:42:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jerryjliu/llama_index</title>
    <updated>2023-03-20T01:42:35Z</updated>
    <id>tag:github.com,2023-03-20:/jerryjliu/llama_index</id>
    <link href="https://github.com/jerryjliu/llama_index" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM&#39;s with external data.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üóÇÔ∏è LlamaIndex ü¶ô (GPT Index)&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;NOTE&lt;/strong&gt;: We are rebranding GPT Index as LlamaIndex! We will carry out this transition gradually.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;2/25/2023&lt;/strong&gt;: By default, our docs/notebooks/instructions now reference &#34;LlamaIndex&#34; instead of &#34;GPT Index&#34;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;2/19/2023&lt;/strong&gt;: By default, our docs/notebooks/instructions now use the &lt;code&gt;llama-index&lt;/code&gt; package. However the &lt;code&gt;gpt-index&lt;/code&gt; package still exists as a duplicate!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;2/16/2023&lt;/strong&gt;: We have a duplicate &lt;code&gt;llama-index&lt;/code&gt; pip package. Simply replace all imports of &lt;code&gt;gpt_index&lt;/code&gt; with &lt;code&gt;llama_index&lt;/code&gt; if you choose to &lt;code&gt;pip install llama-index&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM&#39;s with external data.&lt;/p&gt; &#xA;&lt;p&gt;PyPi:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LlamaIndex: &lt;a href=&#34;https://pypi.org/project/llama-index/&#34;&gt;https://pypi.org/project/llama-index/&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;GPT Index (duplicate): &lt;a href=&#34;https://pypi.org/project/gpt-index/&#34;&gt;https://pypi.org/project/gpt-index/&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Documentation: &lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/&#34;&gt;https://gpt-index.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Twitter: &lt;a href=&#34;https://twitter.com/gpt_index&#34;&gt;https://twitter.com/gpt_index&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Discord: &lt;a href=&#34;https://discord.gg/dGcwcsnxhU&#34;&gt;https://discord.gg/dGcwcsnxhU&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;LlamaHub (community library of data loaders): &lt;a href=&#34;https://llamahub.ai&#34;&gt;https://llamahub.ai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!&lt;/p&gt; &#xA;&lt;h3&gt;Context&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.&lt;/li&gt; &#xA; &lt;li&gt;How do we best augment LLMs with our own private data?&lt;/li&gt; &#xA; &lt;li&gt;One paradigm that has emerged is &lt;em&gt;in-context&lt;/em&gt; learning (the other is finetuning), where we insert context into the input prompt. That way, we take advantage of the LLM&#39;s reasoning capabilities to generate a response.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To perform LLM&#39;s data augmentation in a performant, efficient, and cheap manner, we need to solve two components:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data Ingestion&lt;/li&gt; &#xA; &lt;li&gt;Data Indexing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Proposed Solution&lt;/h3&gt; &#xA;&lt;p&gt;That&#39;s where the &lt;strong&gt;LlamaIndex&lt;/strong&gt; comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Offers &lt;strong&gt;data connectors&lt;/strong&gt; to your existing data sources and data formats (API&#39;s, PDF&#39;s, docs, SQL, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Provides &lt;strong&gt;indices&lt;/strong&gt; over your unstructured and structured data for use with LLM&#39;s. These indices help to abstract away common boilerplate and pain points for in-context learning: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Storing context in an easy-to-access format for prompt insertion.&lt;/li&gt; &#xA;   &lt;li&gt;Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big.&lt;/li&gt; &#xA;   &lt;li&gt;Dealing with text splitting.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Provides users an interface to &lt;strong&gt;query&lt;/strong&gt; the index (feed in an input prompt) and obtain a knowledge-augmented output.&lt;/li&gt; &#xA; &lt;li&gt;Offers you a comprehensive toolset trading off cost and performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí° Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Interesting in contributing? See our &lt;a href=&#34;https://raw.githubusercontent.com/jerryjliu/llama_index/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Full documentation can be found here: &lt;a href=&#34;https://gpt-index.readthedocs.io/en/latest/&#34;&gt;https://gpt-index.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please check it out for the most up-to-date tutorials, how-to guides, references, and other resources!&lt;/p&gt; &#xA;&lt;h2&gt;üíª Example Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-index&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Examples are in the &lt;code&gt;examples&lt;/code&gt; folder. Indices are in the &lt;code&gt;indices&lt;/code&gt; folder (see list of indices below).&lt;/p&gt; &#xA;&lt;p&gt;To build a simple vector store index:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#39;YOUR_OPENAI_API_KEY&#39;&#xA;&#xA;from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader&#xA;documents = SimpleDirectoryReader(&#39;data&#39;).load_data()&#xA;index = GPTSimpleVectorIndex(documents)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To save to and load from disk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# save to disk&#xA;index.save_to_disk(&#39;index.json&#39;)&#xA;# load from disk&#xA;index = GPTSimpleVectorIndex.load_from_disk(&#39;index.json&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To query:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;index.query(&#34;&amp;lt;question_text&amp;gt;?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîß Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;The main third-party package requirements are &lt;code&gt;tiktoken&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, and &lt;code&gt;langchain&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All requirements should be contained within the &lt;code&gt;setup.py&lt;/code&gt; file. To run the package locally without building the wheel, simply run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Citation&lt;/h2&gt; &#xA;&lt;p&gt;Reference to cite if you use LlamaIndex in a paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{Liu_LlamaIndex_2022,&#xA;author = {Liu, Jerry},&#xA;doi = {10.5281/zenodo.1234},&#xA;month = {11},&#xA;title = {{LlamaIndex}},&#xA;url = {https://github.com/jerryjliu/gpt_index},&#xA;year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pytube/pytube</title>
    <updated>2023-03-20T01:42:35Z</updated>
    <id>tag:github.com,2023-03-20:/pytube/pytube</id>
    <link href="https://github.com/pytube/pytube" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A lightweight, dependency-free Python library (and command-line utility) for downloading YouTube Videos.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytube/pytube/master/#&#34;&gt;&lt;img src=&#34;https://assets.nickficano.com/gh-pytube.min.svg?sanitize=true&#34; width=&#34;456&#34; height=&#34;143&#34; alt=&#34;pytube logo&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/pytube/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/pytube?style=flat-square&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytube.io/en/latest/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/python-pytube/badge/?version=latest&amp;amp;style=flat-square&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pytube/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pytube?style=flat-square&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Actively soliciting contributors!&lt;/h3&gt; &#xA;&lt;p&gt;Have ideas for how pytube can be improved? Feel free to open an issue or a pull request!&lt;/p&gt; &#xA;&lt;h1&gt;pytube&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;pytube&lt;/em&gt; is a genuine, lightweight, dependency-free Python library (and command-line utility) for downloading YouTube videos.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Detailed documentation about the usage of the library can be found at &lt;a href=&#34;https://pytube.io&#34;&gt;pytube.io&lt;/a&gt;. This is recommended for most cases. If you want to hastily download a single video, the &lt;a href=&#34;https://raw.githubusercontent.com/pytube/pytube/master/#Quickstart&#34;&gt;quick start&lt;/a&gt; guide below might be what you&#39;re looking for.&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;YouTube is the most popular video-sharing platform in the world and as a hacker, you may encounter a situation where you want to script something to download videos. For this, I present to you: &lt;em&gt;pytube&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;pytube&lt;/em&gt; is a lightweight library written in Python. It has no third-party dependencies and aims to be highly reliable.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;pytube&lt;/em&gt; also makes pipelining easy, allowing you to specify callback functions for different download events, such as &lt;code&gt;on progress&lt;/code&gt; or &lt;code&gt;on complete&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Furthermore, &lt;em&gt;pytube&lt;/em&gt; includes a command-line utility, allowing you to download videos right from the terminal.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for both progressive &amp;amp; DASH streams&lt;/li&gt; &#xA; &lt;li&gt;Support for downloading the complete playlist&lt;/li&gt; &#xA; &lt;li&gt;Easily register &lt;code&gt;on_download_progress&lt;/code&gt; &amp;amp; &lt;code&gt;on_download_complete&lt;/code&gt; callbacks&lt;/li&gt; &#xA; &lt;li&gt;Command-line interfaced included&lt;/li&gt; &#xA; &lt;li&gt;Caption track support&lt;/li&gt; &#xA; &lt;li&gt;Outputs caption tracks to .srt format (SubRip Subtitle)&lt;/li&gt; &#xA; &lt;li&gt;Ability to capture thumbnail URL&lt;/li&gt; &#xA; &lt;li&gt;Extensively documented source code&lt;/li&gt; &#xA; &lt;li&gt;No third-party dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;This guide covers the most basic usage of the library. For more detailed information, please refer to &lt;a href=&#34;https://pytube.io&#34;&gt;pytube.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Pytube requires an installation of Python 3.6 or greater, as well as pip. (Pip is typically bundled with Python &lt;a href=&#34;https://python.org/downloads&#34;&gt;installations&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;p&gt;To install from PyPI with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python -m pip install pytube&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sometimes, the PyPI release becomes slightly outdated. To install from the source with pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python -m pip install git+https://github.com/pytube/pytube&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using pytube in a Python script&lt;/h3&gt; &#xA;&lt;p&gt;To download a video using the library in a script, you&#39;ll need to import the YouTube class from the library and pass an argument of the video URL. From there, you can access the streams and download them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; &amp;gt;&amp;gt;&amp;gt; from pytube import YouTube&#xA; &amp;gt;&amp;gt;&amp;gt; YouTube(&#39;https://youtu.be/2lAe1cqCOXo&#39;).streams.first().download()&#xA; &amp;gt;&amp;gt;&amp;gt; yt = YouTube(&#39;http://youtube.com/watch?v=2lAe1cqCOXo&#39;)&#xA; &amp;gt;&amp;gt;&amp;gt; yt.streams&#xA;  ... .filter(progressive=True, file_extension=&#39;mp4&#39;)&#xA;  ... .order_by(&#39;resolution&#39;)&#xA;  ... .desc()&#xA;  ... .first()&#xA;  ... .download()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using the command-line interface&lt;/h3&gt; &#xA;&lt;p&gt;Using the CLI is remarkably straightforward as well. To download a video at the highest progressive quality, you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pytube https://youtube.com/watch?v=2lAe1cqCOXo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also do the same for a playlist:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pytube https://www.youtube.com/playlist?list=PLS1QulWo1RIaJECMeUT4LFwJ-ghgoSH6n&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>PiotrNawrot/nanoT5</title>
    <updated>2023-03-20T01:42:35Z</updated>
    <id>tag:github.com,2023-03-20:/PiotrNawrot/nanoT5</id>
    <link href="https://github.com/PiotrNawrot/nanoT5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast &amp; Simple repository for pre-training and fine-tuning T5-style models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoT5 (Encoder-Decoder / Pre-training + Fine-Tuning)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/nanoT5.png&#34; alt=&#34;nanoT5&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/#tldr&#34;&gt;&lt;strong&gt;TLDR&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/#motivation&#34;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/#setup&#34;&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/#references&#34;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/#conclusions&#34;&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/#issues&#34;&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TLDR:&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains the code to reproduce the pre-training of a &#34;Large Language Model&#34; (T5) under a limited budget (1xA100 GPU, ~20 hours) in PyTorch. We start from the randomly initialised T5-base-v1.1 (248M parameters) model implemented in HuggingFace. Next, we pre-train it on the English subset of the C4 dataset and then fine-tune it on Super-Natural Instructions (SNI).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In ~20 hours on a single GPU, we achieve ~40 RougeL on the SNI test set, compared to ~42 RougeL of the original model available on HuggingFace Hub and pretrained through &#34;a combination of model and data parallelism [...] on slices of Cloud TPU Pods&#34;, each with 1024 TPUs.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our core contribution is not the T5 model itself, which follows the HuggingFace implementation. Instead, we optimise everything else in the training pipeline to offer you a user-friendly starting template for your NLP application/research.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;Despite the continuously increasing size of pretrained &lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34;&gt;Transformers&lt;/a&gt;, the research community still needs easy-to-reproduce and up-to-date baselines to test new research hypotheses fast and at a small scale.&lt;/p&gt; &#xA;&lt;p&gt;A recent effort from Andrej Karpathy, the &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt; repository, enables researchers to pre-train and fine-tune GPT-style (Decoder-only) language models. On the other hand, &lt;a href=&#34;https://github.com/JonasGeiping/cramming&#34;&gt;Cramming&lt;/a&gt; implements the optimal BERT-style (Encoder-only) pre-training for limited-compute settings.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;a href=&#34;https://github.com/PiotrNawrot/nanoT5&#34;&gt;nanoT5&lt;/a&gt;, we want to fill a gap (Community requests: &lt;a href=&#34;https://github.com/huggingface/transformers/issues/18030&#34;&gt;#1&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookresearch/fairseq/issues/1899&#34;&gt;#2&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer/issues/172&#34;&gt;#3&lt;/a&gt; &lt;a href=&#34;https://discuss.huggingface.co/t/example-of-how-to-pretrain-t5/4129&#34;&gt;#4&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/transformers/issues/5079&#34;&gt;#5&lt;/a&gt;) of an accessible research template to pre-train and fine-tune T5-style (Encoder-Decoder) model. &lt;strong&gt;To the best of our knowledge, it is the first attempt to reproduce T5 v1.1 pre-training in PyTorch (previously available implementations are in Jax/Flax).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We created this repository for people who want to pre-train T5-style models by themselves and evaluate their performance on downstream tasks.&lt;/strong&gt; This could be for a variety of reasons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You are a researcher in academia with limited compute (like me), and you came up with a promising idea to modify the T5 model, so you need a pipeline to evaluate it;&lt;/li&gt; &#xA; &lt;li&gt;You have an in-house dataset that you think is more appropriate than the original pre-training data;&lt;/li&gt; &#xA; &lt;li&gt;You want to experiment with continued pre-training or want to build on the T5 pre-training objective.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you don&#39;t need to pre-train the T5 model, you&#39;d be better off downloading the weights from HuggingFace Hub. Our checkpoints are worse because we work under limited compute.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p&gt;In this project, we expose (for research purposes) and optimise everything in the training pipeline of T5 except from model implementation. &lt;strong&gt;Most importantly, we base our code on PyTorch, since access to TPUs is limited.&lt;/strong&gt; Among others:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; Downloading and preprocessing of the C4 dataset happens in parallel with the training of the model. The C4 dataset is &amp;gt; 300GB, so it takes a couple of hours to download it and even longer to preprocess it. This codebase does it on the fly without any detrimental effect on the training loss (we haven&#39;t observed it, although it might happen with an old CPU (&amp;lt; 8 core) or a slow internet connection). &lt;strong&gt;As a result, you can start pre-training right after downloading and setting up this repository.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Optimizer / LR Scheduler:&lt;/strong&gt; The original T5 uses a memory-efficient Adafactor optimizer. &lt;a href=&#34;https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models&#34;&gt;A study on pre-training T5&lt;/a&gt;, on the other hand, reports that training does not converge with AdamW. We analysed the source of this discrepancy with several ablations. Although there are many subtle differences between Adafactor and AdamW, what ensures the Adafactor convergence is &lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/src/transformers/optimization.py#L595&#34;&gt;matrix-wise LR scaling by its root mean square (RMS)&lt;/a&gt;. We augmented the AdamW implementation by RMS scaling and observed that it becomes &lt;strong&gt;more stable during pre-training, achieves better validation loss, and is faster&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exposure and simplicity:&lt;/strong&gt; We try to balance the implementation of the training pipeline by keeping it customisable while retaining a sufficient level of abstraction. We use the &lt;a href=&#34;https://huggingface.co/docs/accelerate/index&#34;&gt;HuggingFace Accelerator&lt;/a&gt; to implement operations like Checkpoint Saving, Gradient Accumulation and moving tensors to the correct devices. We use &lt;a href=&#34;https://neptune.ai&#34;&gt;neptune.ai&lt;/a&gt; for experiment tracking and &lt;a href=&#34;https://hydra.cc/docs/intro/&#34;&gt;hydra&lt;/a&gt; for hyperparameter search. Apart from this, we expose the training loop, data preprocessing, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; We enable TF32 operations (Ampere GPUs) by default, use PyTorch 2.0 compile, and utilise all optimisations listed in established optimisation tutorials &lt;a href=&#34;https://huggingface.co/docs/transformers/perf_train_gpu_one&#34;&gt;#1&lt;/a&gt; &lt;a href=&#34;https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html&#34;&gt;#2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Environment &amp;amp; Hardware:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/PiotrNawrot/nanoT5.git&#xA;cd nanoT5&#xA;conda create -n nanoT5 python=3.8&#xA;conda activate nanoT5&#xA;pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following commands result in the following &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/env_dump/pip_freeze.txt&#34;&gt;pip freeze&lt;/a&gt; as of 15.03.2023.&lt;/p&gt; &#xA;&lt;p&gt;We also include our &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/env_dump/lscpu.txt&#34;&gt;lscpu&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/env_dump/nvidia_smi.txt&#34;&gt;nvidia-smi&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-training:&lt;/h3&gt; &#xA;&lt;h4&gt;Reference:&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/pdf/2002.05202.pdf&#34;&gt;T5 v1.1&lt;/a&gt; authors report &lt;strong&gt;1.942&lt;/strong&gt; negative log-likelihood (NLL) on the held-out set after after 2^16 steps.&lt;/p&gt; &#xA;&lt;h4&gt;Legacy Optimizer (Adafactor) &amp;amp; LR Schedule (Inverse-Square-Root)&lt;/h4&gt; &#xA;&lt;p&gt;We follow the original experimental setup for pre-training, including &lt;a href=&#34;https://github.com/PiotrNawrot/nanoT5/raw/main/nanoT5/utils/model_utils.py#L58&#34;&gt;Dataset (C4)&lt;/a&gt;, &lt;a href=&#34;https://github.com/PiotrNawrot/nanoT5/raw/main/nanoT5/utils/copied_utils.py#L16&#34;&gt;Training Objective (Span Filling)&lt;/a&gt;, &lt;a href=&#34;https://github.com/PiotrNawrot/nanoT5/raw/main/nanoT5/configs/default.yaml#L12&#34;&gt;Model Architecture (T5-Base)&lt;/a&gt;, &lt;a href=&#34;https://github.com/PiotrNawrot/nanoT5/raw/main/nanoT5/utils/model_utils.py#L236&#34;&gt;Optimizer (Adafactor)&lt;/a&gt;, and &lt;a href=&#34;https://github.com/PiotrNawrot/nanoT5/raw/main/nanoT5/utils/model_utils.py#L276&#34;&gt;LR Schedule (Inverse-Square-Root)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Our negative log-likelihood on the held-out set is &lt;strong&gt;1.995&lt;/strong&gt;, slightly worse than the reference.&lt;/p&gt; &#xA;&lt;h4&gt;AdamW with RMS scaling Optimizer &amp;amp; Cosine LR Schedule&lt;/h4&gt; &#xA;&lt;p&gt;We also experiment with the AdamW optimizer (instead of the original Adafactor) as it offers more stability during training. Instead of using a low-rank approximation for the second moment of the gradients, it estimates it directly by storing the moving average for each parameter in memory. However, training diverges with AdamW, similar to &lt;a href=&#34;https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models&#34;&gt;this study on T5 pre-training&lt;/a&gt;. Through several ablations, we found that &lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/src/transformers/optimization.py#L595&#34;&gt;matrix-wise LR scaling by its root mean square (RMS)&lt;/a&gt; is responsible for the convergence of Adafactor. We augmented the AdamW implementation by RMS scaling and observed that &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/pt_loss.png&#34;&gt;it converges, becomes more stable during pre-training&lt;/a&gt; and is slightly faster (it retrieves the second moment from memory instead of approximating it via matrix multiplications).&lt;/p&gt; &#xA;&lt;p&gt;However, AdamW, when paired with the Inverse-Square-Root LR schedule, performs worse than Adafactor. For our final experiment, we replace ISR with Cosine LR Schedule. We achieve &lt;strong&gt;1.953&lt;/strong&gt; negative log-likelihood on the held-out set and significantly outperform Adafactor with ISR schedule.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Inverse-Square-Root&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Cosine&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Adafactor&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.995&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.993&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;AdamW&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2.040&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.953&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Increased BS (128 -&amp;gt; 144) to maximise GPU Utilization&lt;/h4&gt; &#xA;&lt;p&gt;We notice that with the original Batch Size of 128, we use 60GB / 80GB GPU memory. To maximise the GPU Utilization by allowing for more parallelism, we increase the Batch Size to 144 and consider it &lt;strong&gt;our default pre-training config&lt;/strong&gt;. This achieves &lt;strong&gt;1.932&lt;/strong&gt; negative log-likelihood on the held-out set, improving upon all previous experiments.&lt;/p&gt; &#xA;&lt;h4&gt;Training loss of experiments with different optimisers, schedulers, and batch sizes&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/pt_loss.png&#34; alt=&#34;pt_loss&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;When not indicated in the plot, the batch size is 128.&lt;/p&gt; &#xA;&lt;h4&gt;Examples&lt;/h4&gt; &#xA;&lt;p&gt;To reproduce our default pre-training config experiment, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m nanoT5.main&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reproduce any of the experiments mentioned above choose any combination of hyperparameters as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m nanoT5.main \&#xA;    optim.name={adafactor,adamwscale} \&#xA;    optim.batch_size={128,144} \&#xA;    optim.lr_scheduler={legacy,cosine}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend adding &lt;code&gt;model.compile=true&lt;/code&gt; flag for pre-training, if you are able to install PyTorch 2.0. In our case it results in ~1.33x speedup.&lt;/p&gt; &#xA;&lt;p&gt;Suppose you don&#39;t have access to a 80GB GPU. In that case, you can increase the number of gradient accumulation steps by &lt;code&gt;optim.grad_acc=steps&lt;/code&gt;, In where &lt;code&gt;batch_size&lt;/code&gt; has to be divisible by &lt;code&gt;steps&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The summary of the optimization process is printed every 100 steps in the following format. For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[train] Step 100 out of 65536 | Loss --&amp;gt; 59.881 | Grad_l2 --&amp;gt; 61.126 | Weights_l2 --&amp;gt; 7042.931 | Lr --&amp;gt; 0.010 | Seconds_per_step --&amp;gt; 1.385 |&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning:&lt;/h3&gt; &#xA;&lt;p&gt;To fine-tune our model, we use the popular meta-dataset called &lt;strong&gt;Super Natural-Instructions (SNI)&lt;/strong&gt;, which aggregates datasets for many tasks. This meta-datasets was used to fine-tune many of the recent LLMs, e.g. &lt;a href=&#34;https://arxiv.org/pdf/2210.11416.pdf&#34;&gt;FlanT5&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2211.05100.pdf&#34;&gt;BLOOM&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/pdf/2204.07705.pdf&#34;&gt;Tk-Instruct&lt;/a&gt;. While FlanT5 and BLOOM use other corpora in addition to SNI, Tk-Instruct&#39;s pipeline consists of starting from a pre-trained T5 model and fine-tuning it solely on SNI.&lt;/p&gt; &#xA;&lt;p&gt;In this repository, we reproduce the Tk-Instruct fine-tuning results and use their pipeline to evaluate our pre-training config.&lt;/p&gt; &#xA;&lt;h4&gt;Download the Super-Natural Instructions data:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/allenai/natural-instructions.git data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Run fine-tuning:&lt;/h4&gt; &#xA;&lt;p&gt;We strictly follow the fine-tuning &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/nanoT5/configs/task/ft.yaml&#34;&gt;config&lt;/a&gt; of Tk-Instruct. It remains unclear whether Tk-Instruct was initialised from a regular checkpoint (&lt;em&gt;google/t5-v1_1-base&lt;/em&gt;) or the one adapted explicitly for Language Modelling with continued training (&lt;em&gt;google/t5-base-lm-adapt&lt;/em&gt;). Therefore, we decided to evaluate both. Run the following command to reproduce the Tk-Instruct experiments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m adaptive.moe task=ft \&#xA;    model.name={google/t5-v1_1-base,google/t5-base-lm-adapt} \&#xA;    model.random_init={true,false} \&#xA;    model.checkpoint_path={&#34;&#34;,&#34;/path/to/pytorch_model.bin&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Setting &lt;code&gt;model.random_init=false model.checkpoint_path=&#34;&#34;&lt;/code&gt; corresponds to downloading pre-trained weights from HuggingFace Hub.&lt;/p&gt; &#xA;&lt;p&gt;Setting &lt;code&gt;model.random_init=false model.checkpoint_path=&#34;/path/to/pytorch_model.bin&#34;&lt;/code&gt; corresponds to using the weights &lt;a href=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/#pre-training&#34;&gt;&lt;strong&gt;pre-trained&lt;/strong&gt;&lt;/a&gt; with nanoT5.&lt;/p&gt; &#xA;&lt;p&gt;Setting &lt;code&gt;model.random_init=true model.checkpoint_path=&#34;&#34;&lt;/code&gt; corresponds to a random initialisation.&lt;/p&gt; &#xA;&lt;h4&gt;Fine-tuning loss curves:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/ft_loss.png&#34; alt=&#34;ft_loss&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Rouge-L on the held-out test-set:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PiotrNawrot/nanoT5/main/assets/ft_rougeL.png&#34; alt=&#34;ft_rougeL&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Efficiency statistics:&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Pre-training&lt;/strong&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;One training step&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;~1.05s&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;~0.175s&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;65536&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;18830&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Full training&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;~19h&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;~1h&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;For pre-training we compile our model with PyTorch 2.0 using &lt;code&gt;model.compile=true&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;h2&gt;Conclusions:&lt;/h2&gt; &#xA;&lt;p&gt;We show that it is possible to successfully pre-train a &#34;Large Language Model&#34; (T5) under a limited budget (1xA100 GPU, ~20 hours) in PyTorch. We make our codebase, configs and training logs publicly available to enhance the accessibility of NLP research. We are keen to hear your suggestions to improve the codebase further.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements:&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://ducdauge.github.io&#34;&gt;Edoardo Maria Ponti&lt;/a&gt; for his feedback!&lt;/p&gt; &#xA;&lt;h2&gt;References:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1910.10683.pdf&#34;&gt;T5 paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2002.05202.pdf&#34;&gt;T5 v1.1 paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2204.07705.pdf&#34;&gt;Super-Natural Instructions paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/examples/flax/language-modeling/run_t5_mlm_flax.py&#34;&gt;HuggingFace Flax Script&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;Karpathy&#39;s nanoGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yizhongw/Tk-Instruct&#34;&gt;Instruct-GPT codebase (Super-Natural Instructions)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/yhavinga/pre-training-dutch-t5-models&#34;&gt;Blog about pre-training Dutch T5 in HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Issues:&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to raise a Github issue or contact me directly at: &lt;a href=&#34;mailto:piotr.nawrot@ed.ac.uk&#34;&gt;piotr.nawrot@ed.ac.uk&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>