<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-06T01:34:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>layerdiffusion/sd-forge-layerdiffuse</title>
    <updated>2024-03-06T01:34:07Z</updated>
    <id>tag:github.com,2024-03-06:/layerdiffusion/sd-forge-layerdiffuse</id>
    <link href="https://github.com/layerdiffusion/sd-forge-layerdiffuse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[WIP] Layer Diffusion for WebUI (via Forge)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sd-forge-layerdiffuse&lt;/h1&gt; &#xA;&lt;p&gt;Transparent Image Layer Diffusion using Latent Transparency&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/36598904-ae5f-4578-87d3-4b496e11dcc5&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a WIP extension for SD WebUI &lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge&#34;&gt;(via Forge)&lt;/a&gt; to generate transparent images and layers.&lt;/p&gt; &#xA;&lt;p&gt;The image generating and basic layer functionality is working now, but &lt;strong&gt;the transparent img2img is not finished yet (will finish in about one week)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This code base is highly dynamic and may change a lot in the next month. If you are from professional content creation studio and need all previous results to be strictly reproduced, you may consider backup files during each update.&lt;/p&gt; &#xA;&lt;h1&gt;Before You Start&lt;/h1&gt; &#xA;&lt;p&gt;Because many people may be curious about how the latent preview looks like during a transparent diffusion process, I recorded a video so that you can see it before you download the models and extensions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e93b71d1-3560-48e2-a970-0b8efbfebb42&#34;&gt;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e93b71d1-3560-48e2-a970-0b8efbfebb42&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can see that the native transparent diffusion can process transparent glass, semi-transparent glowing effects, etc, that are not possible with simple background removal methods. Native transparent diffusion also gives you detailed fur, hair, whiskers, and detailed structure like that skeleton.&lt;/p&gt; &#xA;&lt;h1&gt;Model Notes&lt;/h1&gt; &#xA;&lt;p&gt;Note that all currently released models are for SDXL. Models for SD1.5 may be provided later if demanded.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that in this extension, all model downloads/selections are fully automatic. In fact most users can just skip this section.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below models are released:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_transparent_attn.safetensors&lt;/code&gt; This is a rank-256 LoRA to turn a SDXL into a transparent image generator. It will change the latent distribution of the model to a &#34;transparent latent space&#34; that can be decoded by the special VAE pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_transparent_conv.safetensors&lt;/code&gt; This is an alternative model to turn your SDXL into a transparent image generator. This safetensors file includes an offset of all conv layers (and actually, all layers that are not q,k,v of any attention layers). These offsets can be merged to any XL model to change the latent distribution to transparent images. Because we excluded the offset training of any q,k,v layers, the prompt understanding of SDXL should be perfectly preserved. However, in practice, I find the &lt;code&gt;layer_xl_transparent_attn.safetensors&lt;/code&gt; will lead to better results. This &lt;code&gt;layer_xl_transparent_conv.safetensors&lt;/code&gt; is still included for some special use cases that needs special prompt understanding. Also, this model may introduce a strong style influence to the base model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_fg2ble.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on foregrounds, and generates blended compositions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_fgble2bg.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on foregrounds and blended compositions, and generates backgrounds.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_bg2ble.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on backgrounds, and generates blended compositions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_bgble2fg.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on backgrounds and blended compositions, and generates foregrounds.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vae_transparent_encoder.safetensors&lt;/code&gt; This is an image encoder to extract a latent offset from pixel space. The offset can be added to latent images to help the diffusion of transparency. Note that in the paper we used a relatively heavy model with exactly same amount of parameters as the SD VAE. The released model is more light weighted, requires much less vram, and does not influence result quality in my tests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vae_transparent_decoder.safetensors&lt;/code&gt; This is an image decoder that takes SD VAE outputs and latent image as inputs, and outputs a real PNG image. The model architecture is also more lightweight than the paper version to reduce VRAM requirement. I have made sure that the reduced parameters does not influence result quality.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;(Update Mar 4) Below models will be released soon:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Joint foreground-background generating model. The model will be 3x slower and requires 3.5x more VRAM, but will generate foregrounds and backgrounds together in one single pass.&lt;/li&gt; &#xA; &lt;li&gt;One step foreground-conditioned background model. The model will be 2x slower and requires 2.5x more VRAM, but will generate cleaner backgrounds in one single pass (compared to the released two-step models).&lt;/li&gt; &#xA; &lt;li&gt;One step background-conditioned foreground model. The model will be 2x slower and requires 2.5x more VRAM, but will generate cleaner foregrounds in one single pass (compared to the released two-step models).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Below models may be released soon (if necessary):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A model that can generate foreground and background together (using attention sharing similar to AnimateDiff). I put this model on hold because of these reasons: (1) the other released models can already achieve all functionalities and this model does not bring more functionalities. (2) the inference speed of this model is 3x slower than others and requires 4x more VRAM than other released model, and I am working on reducing the VRAM of this model if necessary. (3) This model will involve more hyperparameters and if demanded, I will investigate the best practice for inference/training before release it. &lt;strong&gt;this model is confirmed to be released soon with joint layer generating and one-step bg/fg-condition, after we finish the final VRAM optimization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;The current background-conditioned foreground model may be a bit too lightweight. I will probably release a heavier one with more parameters and different behaviors (see also the discussions later).&lt;/li&gt; &#xA; &lt;li&gt;Because the difference between diffusers training and k-diffusion inference, I can observe some mystical problems like sometimes DPM++ will give artifacts but Euler A will fix it. I am looking into it and may provide some revised model that works better with all A1111 samplers.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Sanity Check&lt;/h1&gt; &#xA;&lt;p&gt;We highly encourage you to go through the sanity check and get exactly same results (so that if any problem occurs, we will know if the problem is on our side).&lt;/p&gt; &#xA;&lt;p&gt;The two used models are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://civitai.com/models/133005?modelVersionId=198530&#34;&gt;https://civitai.com/models/133005?modelVersionId=198530&lt;/a&gt; Juggernaut XL V6 (note that the used one is &lt;strong&gt;V6&lt;/strong&gt;, not v7 or v8 or V9)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://civitai.com/models/261336?modelVersionId=295158&#34;&gt;https://civitai.com/models/261336?modelVersionId=295158&lt;/a&gt; anima_pencil-XL 1.0.0 (note that the used one is &lt;strong&gt;1.0.0&lt;/strong&gt;, not 1.5.0)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We will first test transparent image generating. Set your extension to this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/5b85b383-89c0-403e-aa07-d6e43ff3b8ae&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;an apple, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 5, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;Make sure that you get this apple&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/376fa8bc-547e-4cd7-b658-7d60f2e37f1d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/16efc57b-4da8-4227-a257-f45f3dfeaddc&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/38ace070-6530-43c9-9ca1-c98aa5b7a0ed&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;woman, messy hair, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 5, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;Make sure that you get the woman with hair as messy as this&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/17c86ba5-eb29-45d4-b708-caf7e836b509&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/6f1ef595-255c-4162-bdf9-c8e4eb321f31&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;a cup made of glass, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 5, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;Make sure that you get this cup&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/a99177e6-72ed-447b-b2a5-6ca0fe1dc105&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/3b7df3f3-c6c1-401d-afa8-5a1c404165c9&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;glowing effect, book of magic, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: True, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;make sure that you get this glowing book&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/c093c862-17a3-4604-8e23-6c7f3a0eb4b3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/fa0b02b0-b530-48ed-a8ec-17bd9cccfc87&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OK then lets move on to a bit longer prompt:&lt;/p&gt; &#xA;&lt;p&gt;(this prompt is from &lt;a href=&#34;https://civitai.com/images/3160575&#34;&gt;https://civitai.com/images/3160575&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;photograph close up portrait of Female boxer training, serious, stoic cinematic 4k epic detailed 4k epic detailed photograph shot on kodak detailed bokeh cinematic hbo dark moody&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/845c0e35-0096-484b-be2c-d443b4dc63cd&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/47ee7ba1-7f64-4e27-857f-c82c9d2bbb14&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Anime model test:&lt;/p&gt; &#xA;&lt;p&gt;girl in dress, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: nsfw, bad, ugly, text, watermark&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 7ed8da12d9, Model: animaPencilXL_v100, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/fcec8ea5-32de-44af-847a-d66dd62b95d1&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/53d84e56-4061-4d91-982f-8f1e927f68b7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(I am not very good at writing prompts in the AnimagineXL format, and perhaps you can get better results with better prompts)&lt;/p&gt; &#xA;&lt;h3&gt;Background Condition&lt;/h3&gt; &#xA;&lt;p&gt;First download this image:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e7e2d80e-ffbe-4724-812a-5139a88027e3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then set the interface with&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/99a7e648-a83f-4ea5-bff6-66a1c624c0bd&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then set the parameters with&lt;/p&gt; &#xA;&lt;p&gt;old man sitting, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: From Background to Blending, layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: True, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/4dd5022a-d9fd-4436-83b8-775e2456bfc6&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then set the interface with (you first change the mode and then drag the image from result to interface)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/8277399c-fc9b-43fd-a9bb-1c7a8dcebb3f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then change the sampler to Euler A or UniPC or some other sampler that is not dpm (This is probably because of some difference between diffusers training script and webui&#39;s k-diffusion. I am still looking into this and may revise my training script and model very soon so that this step will be removed.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/2c7124c5-e5d4-40cf-b106-e55c33e40003&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;FAQ:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;OK. But how can I get a background image like this?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use the Foreground Condition to get a background like this. We will describe it in the next section.&lt;/p&gt; &#xA;&lt;p&gt;Or you can use old inpainting tech to perform foreground removal on any image to get a background like this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Wait. Why you generate it with two steps? Can I generate it with one pass?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Two steps allows for more flexible editing. We will release the one-step model soon if necessary, but that model is 2x larger and requires 4x larger VRAM, and we are still working on reducing the computation requirement of that model. (But in my tests, the current solution is better than that model in most cases.)&lt;/p&gt; &#xA;&lt;p&gt;Also you can see that the current model is about 680MB and in particular I think it is a bit too lightweight and will soon release a relatively heavier model for potential stronger structure understanding (but that is still under experiments).&lt;/p&gt; &#xA;&lt;h1&gt;Foreground Condition&lt;/h1&gt; &#xA;&lt;p&gt;First we generate a dog&lt;/p&gt; &#xA;&lt;p&gt;a dog sitting, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/dd515df4-cc58-47e0-8fe0-89e21e8320c4&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e2785fd4-c168-4062-ae2f-010540ff0991&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then change to &lt;code&gt;From Foreground to Blending&lt;/code&gt; and drag the transparent image to foreground input.&lt;/p&gt; &#xA;&lt;p&gt;Note that you drag the real transparent image, not the visualization with checkboard background. Make sure tou see this&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/b912e1e8-7511-4afc-aa61-4bb31d6949f7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then do this&lt;/p&gt; &#xA;&lt;p&gt;a dog sitting in room, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: From Foreground to Blending, layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/0b2abb76-56b9-448d-8f2a-8572a18c759b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then change mode, drag your image, so that&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/48667cbf-e460-4037-b059-a30580841bcd&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Note that here I set stop at as 0.5 to get better results since I do not need the bg to be exactly same)&lt;/p&gt; &#xA;&lt;p&gt;Then change the sampler to Euler A or UniPC or some other sampler that is not dpm (This is probably because of some difference between diffusers training script and webui&#39;s k-diffusion. I am still looking into this and may revise my training script and model very soon so that this step will be removed.)&lt;/p&gt; &#xA;&lt;p&gt;then do this&lt;/p&gt; &#xA;&lt;p&gt;room, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: UniPC, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: From Foreground and Blending to Background, layerdiffusion_weight: 1, layerdiffusion_ending_step: 0.5, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: True, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/5f5a5b6a-7dd2-4e16-9571-1458a9ef465d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huchenlei/ComfyUI-layerdiffuse</title>
    <updated>2024-03-06T01:34:07Z</updated>
    <id>tag:github.com,2024-03-06:/huchenlei/ComfyUI-layerdiffuse</id>
    <link href="https://github.com/huchenlei/ComfyUI-layerdiffuse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Layer Diffusion custom nodes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI-layerdiffuse&lt;/h1&gt; &#xA;&lt;p&gt;ComfyUI implementation of &lt;a href=&#34;https://github.com/layerdiffusion/LayerDiffuse&#34;&gt;https://github.com/layerdiffusion/LayerDiffuse&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the repository and unpack into the custom_nodes folder in the ComfyUI installation directory.&lt;/p&gt; &#xA;&lt;p&gt;Or clone via GIT, starting from ComfyUI installation directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd custom_nodes&#xA;git clone git@github.com:huchenlei/ComfyUI-layerdiffuse.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install python dependencies. You might experience version conflict on diffusers if you have other extensions that depends on other versions of diffusers. In this case, it is recommended to setup separate Python venvs.&lt;/p&gt; &#xA;&lt;h2&gt;Workflows&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/raw/main/examples/layer_diffusion_fg_example_rgba.json&#34;&gt;Generate foreground&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/5e6085e5-d997-4a0a-b589-257d65eb1eb2&#34; alt=&#34;rgba&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/raw/main/examples/layer_diffusion_fg_example.json&#34;&gt;Generate foreground (RGB + alpha)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you want more control of getting RGB image and alpha channel mask separately, you can use this workflow. &lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/4825b81c-7089-4806-bce7-777229421707&#34; alt=&#34;readme1&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/raw/main/examples/layer_diffusion_cond_example.json&#34;&gt;Blending (FG/BG)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Blending given FG &lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/7f7dee80-6e57-4570-b304-d1f7e5dc3aad&#34; alt=&#34;fg_cond&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Blending given BG &lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/e3a79218-6123-453b-a54b-2f338db1c12d&#34; alt=&#34;bg_cond&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/raw/main/examples/layer_diffusion_diff_fg.json&#34;&gt;Extract FG from Blended + BG&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/45c7207d-72ff-4fb0-9c91-687040781837&#34; alt=&#34;diff_bg&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/raw/main/examples/layer_diffusion_diff_bg.json&#34;&gt;Extract BG from Blended + FG&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/layerdiffuse/sd-forge-layerdiffuse#sanity-check&#34;&gt;Forge impl&#39;s sanity check&lt;/a&gt; sets &lt;code&gt;Stop at&lt;/code&gt; to 0.5 to get better quality BG. This workflow might be inferior comparing to other object removal workflows. &lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/05a10add-68b0-473a-acee-5853e4720322&#34; alt=&#34;diff_fg&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/raw/main/examples/layer_diffusion_diff_bg_stop_at.json&#34;&gt;Extract BG from Blended + FG (Stop at 0.5)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://github.com/layerdiffuse/sd-forge-layerdiffuse&#34;&gt;SD Forge impl&lt;/a&gt;, there is a &lt;code&gt;stop at&lt;/code&gt; param that determines when layer diffuse should stop in the denosing process. In the background, what this param does is unapply the LoRA and c_concat cond after a certain step threshold. This is hard/risky to implement directly in ComfyUI as it requires manually load a model that has every changes except the layer diffusion change applied. A workaround in ComfyUI is to have another img2img pass on the layer diffuse result to simulate the effect of &lt;code&gt;stop at&lt;/code&gt; param. &lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/e383c9d3-2d47-40c2-b764-b0bd48243ee8&#34; alt=&#34;diff_fg_stop_at&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/raw/main/examples/layer_diffusion_cond_fg_all.json&#34;&gt;Generate FG from BG combined&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Combines previous workflows to generate blended and FG given BG. We found that there are some color variations in the extracted FG. Need to confirm with layer diffusion authors on whether this is expected. &lt;img src=&#34;https://github.com/huchenlei/ComfyUI-layerdiffuse/assets/20929282/f4c18585-961a-473a-a616-aa3776bacd41&#34; alt=&#34;fg_all&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Note&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Currently only SDXL is supported. See &lt;a href=&#34;https://github.com/layerdiffuse/sd-forge-layerdiffuse#model-notes&#34;&gt;https://github.com/layerdiffuse/sd-forge-layerdiffuse#model-notes&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Foreground conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Background conditioning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Blended + foreground =&amp;gt; background&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Blended + background =&amp;gt; foreground&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aappleby/hancho</title>
    <updated>2024-03-06T01:34:07Z</updated>
    <id>tag:github.com,2024-03-06:/aappleby/hancho</id>
    <link href="https://github.com/aappleby/hancho" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple pleasant build system in Python.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/aappleby/hancho/main/hancho_small.png&#34; alt=&#34;Logo&#34;&gt; Hancho&lt;/h1&gt; &#xA;&lt;p&gt;&#34;班長, hanchō - &#34;Squad leader”, from 19th c. Mandarin 班長 (bānzhǎng, “team leader”)&#34;&lt;/p&gt; &#xA;&lt;p&gt;Hancho is a simple, pleasant build system with few moving parts.&lt;/p&gt; &#xA;&lt;p&gt;Hancho fits comfortably in 500 lines* of Python and requires no installation, just copy-paste it into your source tree.&lt;/p&gt; &#xA;&lt;p&gt;Hancho is inspired by Ninja (for speed and simplicity) and Bazel (for syntax and extensibility).&lt;/p&gt; &#xA;&lt;p&gt;Like Ninja, it knows nothing about your build tools and is only trying to assemble and run commands as fast as possible.&lt;/p&gt; &#xA;&lt;p&gt;Unlike Ninja, you don&#39;t need a separate build rule invocation for every single output file.&lt;/p&gt; &#xA;&lt;p&gt;Like Bazel, you invoke build rules by calling them as if they were functions with keyword arguments.&lt;/p&gt; &#xA;&lt;p&gt;Unlike Bazel, you can create build rules that call arbitary Python code (for better or worse).&lt;/p&gt; &#xA;&lt;p&gt;Hancho should suffice for small to medium sized projects.&lt;/p&gt; &#xA;&lt;p&gt;(* - Autoformatting added some whitespace, but it&#39;s still under 500 lines of actual code :D )&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aappleby/hancho/main/tutorial&#34;&gt;Tutorial Here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aappleby/hancho/main/docs&#34;&gt;Some Additional Documentation Here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024-03-04 - Cleaned up pylint &amp;amp; formatting issues in hancho.py and test.py. Hancho.py is now over 500 lines if you include whitespace and comments :D.&lt;/li&gt; &#xA; &lt;li&gt;2024-03-04 - Unrecognized &#39;--key=value&#39; command line flags are now merged into the global config object. This allows you to do things like &#34;hancho.py --build_dir=some/other/dir&#34; which could be annoying otherwise.&lt;/li&gt; &#xA; &lt;li&gt;2024-03-02 - Initial release. Some test cases yet to be written.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;user@host:~$ wget https://raw.githubusercontent.com/aappleby/hancho/main/hancho.py&#xA;user@host:~$ chmod +x hancho.py&#xA;user@host:~$ ./hancho.py --help&#xA;usage: hancho.py [-h] [-C CHDIR] [-j JOBS] [-v] [-q] [-n] [-d] [-f] [filename]&#xA;&#xA;positional arguments:&#xA;  filename              The name of the .hancho file to build&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  -C CHDIR, --chdir CHDIR&#xA;                        Change directory first&#xA;  -j JOBS, --jobs JOBS  Run N jobs in parallel (default = cpu_count, 0 = infinity)&#xA;  -v, --verbose         Print verbose build info&#xA;  -q, --quiet           Mute all output&#xA;  -n, --dryrun          Do not run commands&#xA;  -d, --debug           Print debugging information&#xA;  -f, --force           Force rebuild of everything&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Simple Example&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# examples/hello_world/build.hancho&#xA;&#xA;compile = Rule(&#xA;  desc      = &#34;Compile {files_in} -&amp;gt; {files_out}&#34;,&#xA;  command   = &#34;g++ -MMD -c {files_in} -o {files_out}&#34;,&#xA;  files_out = &#34;{swap_ext(files_in, &#39;.o&#39;)}&#34;,&#xA;  depfile   = &#34;{swap_ext(files_out, &#39;.d&#39;)}&#34;,&#xA;)&#xA;&#xA;link = Rule(&#xA;  desc      = &#34;Link {files_in} -&amp;gt; {files_out}&#34;,&#xA;  command   = &#34;g++ {files_in} -o {files_out}&#34;,&#xA;)&#xA;&#xA;main_o = compile(&#34;main.cpp&#34;)&#xA;main_app = link(main_o, &#34;app&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cpp&#34;&gt;// examples/hello_world/main.cpp&#xA;#include &amp;lt;stdio.h&amp;gt;&#xA;&#xA;int main(int argc, char** argv) {&#xA;  printf(&#34;Hello World\n&#34;);&#xA;  return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;user@host:~/hancho/examples/hello_world$ ../../hancho.py --verbose&#xA;[1/2] Compile main.cpp -&amp;gt; build/main.o&#xA;Reason: Rebuilding [&#39;build/main.o&#39;] because some are missing&#xA;g++ -MMD -c main.cpp -o build/main.o&#xA;[2/2] Link build/main.o -&amp;gt; build/app&#xA;Reason: Rebuilding [&#39;build/app&#39;] because some are missing&#xA;g++ build/main.o -o build/app&#xA;hancho: BUILD PASSED&#xA;&#xA;user@host:~/hancho/examples/hello_world$ build/app&#xA;Hello World&#xA;&#xA;user@host:~/hancho/examples/hello_world$ ../../hancho.py --verbose&#xA;hancho: BUILD CLEAN&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>