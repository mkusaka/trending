<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-23T01:41:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>majacinka/crewai-experiments</title>
    <updated>2024-01-23T01:41:24Z</updated>
    <id>tag:github.com,2024-01-23:/majacinka/crewai-experiments</id>
    <link href="https://github.com/majacinka/crewai-experiments" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Experiments with local as well as models available through an api&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CREWAI experiments&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For my experiments with CrewAI, I decided to try 3 different projects, starting from the easiest to the most complex. The aim of the experiments was to have a team of AI agents do following work for me:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Examine my Startup idea&lt;/li&gt; &#xA; &lt;li&gt;Build AI newsletter with Google SERP&lt;/li&gt; &#xA; &lt;li&gt;Build AI newsletter with Reddit Scraper&lt;/li&gt; &#xA; &lt;li&gt;Email classifier [WIP]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;For my experiements I&#39;ve tried following LLMs:&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Available through API calls:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;OpenAI -&amp;gt; GPT-4&lt;/li&gt; &#xA; &lt;li&gt;Gemini Pro&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Local Models through Ollama + rating of how they performed:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mistral 7B&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nice, coherent results&lt;/li&gt; &#xA; &lt;li&gt;Didn&#39;t understand that it should use scraping tool for the output&lt;/li&gt; &#xA; &lt;li&gt;Result is a bunch of generic text from training data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mistral 7B instruct&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nice, coherent results, a lot of emojis&lt;/li&gt; &#xA; &lt;li&gt;Didn&#39;t use any scraping tool for the output&lt;/li&gt; &#xA; &lt;li&gt;Result is a bunch of generic text from training data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open Chat 3.5 7B&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The best and most &#34;newsletter-y&#34; results&lt;/li&gt; &#xA; &lt;li&gt;But again, didn&#39;t use any tool, so generic content&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Nous Hermes 7B&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ok results&lt;/li&gt; &#xA; &lt;li&gt;didn&#39;t use any tool&lt;/li&gt; &#xA; &lt;li&gt;generic content&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open Hermes 2.5 7B&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The tone and style of writing is great&lt;/li&gt; &#xA; &lt;li&gt;but generic content&lt;/li&gt; &#xA; &lt;li&gt;didn&#39;t understand that it needs to use tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Starling 7B&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ok results&lt;/li&gt; &#xA; &lt;li&gt;didn&#39;t use any tool&lt;/li&gt; &#xA; &lt;li&gt;generic content&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama 2 13B&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The only model that &#34;understood&#34; what the task is&lt;/li&gt; &#xA; &lt;li&gt;but the text wasn&#39;t coherent enough, didn&#39;s sound like a newsletter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama 2 13B chat&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Didn&#39;t understand the task or produce any output&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama 2 13B text&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Didn&#39;t understand the task or produce any output&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama 2 7B&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;not coherent&lt;/li&gt; &#xA; &lt;li&gt;didn&#39;t use any tool&lt;/li&gt; &#xA; &lt;li&gt;no output&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;11&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama 2 7B text&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No actual output&lt;/li&gt; &#xA; &lt;li&gt;didn&#39;t use any tool&lt;/li&gt; &#xA; &lt;li&gt;generic content&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;12&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Llama 2 7B chat&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Didn&#39;t use any tool&lt;/li&gt; &#xA; &lt;li&gt;generic content&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;13&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Phi-2&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The smallest model ran into biggest problems&lt;/li&gt; &#xA; &lt;li&gt;Lost track of what it&#39;s suppose to do, no output&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>gabrielchua/RAGxplorer</title>
    <updated>2024-01-23T01:41:24Z</updated>
    <id>tag:github.com,2024-01-23:/gabrielchua/RAGxplorer</id>
    <link href="https://github.com/gabrielchua/RAGxplorer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Visualise and explore your RAG documents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RAGxplorer ü¶ôü¶∫&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/gabrielchua/RAGxplorer/main/images/logo.png&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;RAGxplorer is an interactive streamlit tool to support the building of Retrieval Augmented Generation (RAG) applications by visualizing document chunks and the queries in the embedding space.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] I will be re-factoring the code massively to be a standalone package, instead of being within a streamlit application. Until then, I appreciate your patience. Further suggestions will be most appreciated &lt;a href=&#34;https://github.com/gabrielchua/RAGxplorer/issues/3&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Demo üîé&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://rag-xplorer.streamlit.app/&#34;&gt;&lt;img src=&#34;https://static.streamlit.io/badges/streamlit_badge_black_white.svg?sanitize=true&#34; alt=&#34;Streamlit App&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ö†Ô∏è &lt;em&gt;Due to &lt;a href=&#34;https://discuss.streamlit.io/t/is-there-streamlit-app-limitations-such-as-usage-time-users-etc/42800&#34;&gt;infra limitations&lt;/a&gt;, this freely hosted demo may occassionaly go down. The best experience is to clone this repo, and run it locally.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/gabrielchua/RAGxplorer/main/images/example.png&#34; width=&#34;650&#34;&gt; &#xA;&lt;h2&gt;Features ‚ú®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Document Upload&lt;/strong&gt;: Users can upload PDF documents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chunk Configuration&lt;/strong&gt;: Options to configure the chunk size and overlap&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Choice of embedding model&lt;/strong&gt;: &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt; or &lt;code&gt;text-embedding-ada-002&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vector Database Creation&lt;/strong&gt;: Builds a vector database using Chroma&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query Expansion&lt;/strong&gt;: Generates sub-questions and hypothetical answers to enhance the retrieval process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interactive Visualization&lt;/strong&gt;: Utilizes Plotly to visualise the chunks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Local Installation ‚öôÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;To run RAGxplorer, ensure you have Python installed, and then install the necessary dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements-local-deployment.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] ‚ö†Ô∏è Do not use &lt;code&gt;requirements.txt&lt;/code&gt;. That is so the free streamlit deployment can run. That file includes an additional &lt;code&gt;pysqlite3-binary&lt;/code&gt; dependency.&lt;/p&gt; &#xA; &lt;p&gt;‚ö†Ô∏è If it helps with troubleshooting, this application was built using Python 3.11&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Usage üèéÔ∏è&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (required) and &lt;code&gt;ANYSCALE_API_KEY&lt;/code&gt; (if you need anyscale). Copy the &lt;code&gt;.streamlit/secrets.example.toml&lt;/code&gt; file to &lt;code&gt;.streamlit/secrets.toml&lt;/code&gt; and fill in the values.&lt;/li&gt; &#xA; &lt;li&gt;To start the application, run: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;You may need to comment out/remove line 5-7 in &lt;code&gt;app.py&lt;/code&gt;. &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__import__(&#39;pysqlite3&#39;)&#xA;import sys&#xA;sys.modules[&#39;sqlite3&#39;] = sys.modules.pop(&#39;pysqlite3&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] This repo is currently linked to the streamlit demo, and these lines were added due to the runtime in the free streamlit deployment env. See &lt;a href=&#34;https://discuss.streamlit.io/t/issues-with-chroma-and-sqlite/47950&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contributing üëã&lt;/h2&gt; &#xA;&lt;p&gt;Contributions to RAGxplorer are welcome. Please read our &lt;a href=&#34;https://raw.githubusercontent.com/gabrielchua/RAGxplorer/main/.github/CONTRIBUTING.md&#34;&gt;contributing guidelines (WIP)&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;License üëÄ&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT license - see the LICENSE file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments üíô&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeepLearning.AI and Chroma for the inspiration and code labs in their &lt;a href=&#34;https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai/&#34;&gt;Advanced Retrival&lt;/a&gt; course.&lt;/li&gt; &#xA; &lt;li&gt;The Streamlit community for the support and resources.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>IncomeStreamSurfer/chatgptassistantautoblogger</title>
    <updated>2024-01-23T01:41:24Z</updated>
    <id>tag:github.com,2024-01-23:/IncomeStreamSurfer/chatgptassistantautoblogger</id>
    <link href="https://github.com/IncomeStreamSurfer/chatgptassistantautoblogger" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This autoblogger uses the GPT assistant&#39;s API to autoblog on ecommerce websites&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;How to autoblog using the ChatGPT Assistant&#39;s API&lt;/h2&gt; &#xA;&lt;p&gt;Firstly check out Income Stream Surfers on YouTube. There will be a few videos on how to use this script.&lt;/p&gt; &#xA;&lt;p&gt;This is really powerful because we can use retrieval with some relevant files and upload them at the start of each thread, we can then use these files to create contextually relevant blog posts with very little effort, but with products and internal links already inside.&lt;/p&gt; &#xA;&lt;h2&gt;Step 1 - Products&lt;/h2&gt; &#xA;&lt;p&gt;You need to firstly get a few things, if you&#39;re on Shopify you&#39;re in luck and this will work instantly, however other CMS may need some tweaking. Go to your sitemap yourwebsite.com/sitemap.xml on Shopify - and then go to your products and right click download it as .xml file. You then need to get Visual Studio Code, and put the .xml inside a new folder. Make sure you have Python and the latest version of OpenAI (pip install openai --upgrade). Then you need to use 2mentest.py in order to pick 200 random products from the sitemap. These should then be put into a .txt file.&lt;/p&gt; &#xA;&lt;h2&gt;Step 2 - Keywords&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll need a csv file formatted like this, Topic Cluster,Topic,Type,Blog Post Ideas,Keywords,Word Count&lt;/p&gt; &#xA;&lt;p&gt;You can put your niche into this into ChatGPT and ask for some stuff, give it some prompting, eventually it&#39;ll come out with something workable. You can even add your products to it to get better results.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve got that, make sure it&#39;s formatted in the same way, sometimes formatting can get weird, so just ask for it in markdown and the same formatting as the original document (2men_it_blog_content_plan_expanded (1).csv) this is an example&lt;/p&gt; &#xA;&lt;h2&gt;Step 3 Internal links&lt;/h2&gt; &#xA;&lt;p&gt;My blogging strategy personally is geared towards pushing to our /collections/ on Shopify. Now I know that might not be true for everyone, so you can do this next part as you like. But you basically just need to clean some data, go to your collections sitemap on Shopify and either use &lt;a href=&#34;https://chromewebstore.google.com/detail/sitemaptoclipboard/ilephboodpbklhnfckkkcilidgeclfhm?pli=1&#34;&gt;sitemaptoclipboard&lt;/a&gt;, or use Google Sheets to clean the data. This file should be called internallinks.txt and should be a list of your internal links.&lt;/p&gt; &#xA;&lt;h2&gt;Step 4 - OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;Add your Secret Key from OpenAI, change the propmts if you want they are these parts of the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;instructions=&#34;Never use sources or footnotes Read internallinks.txt and products.txt - You always choose 5 strictly relevant product images and internal links for the articles. You do not use sources in the outline, you just pick 5 product images that are highly relevant to the article. First you read the attached files and understand them completely, then you create a detailed outline on the blog post topic, including a maximum of 5 HIGHLY relevant internal collection links and product image links. These will finally be used to write an article.&#34;,&#xA;get_request = f&#34;Choose 5 internal links and 5 product image urls that are relevant to {blog_post_idea}. For example for exotic leather shoes look for crocodile shoes etc. For suit articles look for suits.&#39;.&#34;&#xA;outline_request = f&#34;use the product images and internal links from {get_internal_links} and use them to create an outline for an article about {blog_post_idea}&#39; In the outline do not use sources or footnotes, but just add a relevant product image in a relevant section, and a relevant internal link in a relevant section. There is no need for a lot of sources, each article needs a maximum of 5 product images and internal links.&#34;&#xA;article_request = f&#34;Choose 5 internal links and 5 product images that are relevant to an article and then Write a detailed article based on the following outline:\n{outline}. Include the product images and internal links naturally and with relevance inside the article. Use markdown formatting and ensure to use tables and lists to add to formatting. Use 3 relevant product images and internal links maximum. Never invent any internal links.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You don&#39;t really have to change the prompts I don&#39;t think, but feel free to tinker with them to get some better output.&lt;/p&gt; &#xA;&lt;h2&gt;Step 5 - The Content&lt;/h2&gt; &#xA;&lt;p&gt;You should see good results, around 700-1000 words, with product images, internal links, tables, lists, etc. You can use ChatGPT 3.5 in order to format them easily either using automation, or just simply while you&#39;re waiting for the script to run. If you run it all night you can probably achieve 500 articles in a night. I personally don&#39;t think that using automated uploading works that well and I think Google doesn&#39;t like it, so I&#39;m not including any automations in the uploading of the articles in this particular autoblogger that I&#39;ve made.&lt;/p&gt;</summary>
  </entry>
</feed>