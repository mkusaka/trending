<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-12T01:45:35Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Safiullah-Rahu/CSV-AI</title>
    <updated>2023-06-12T01:45:35Z</updated>
    <id>tag:github.com,2023-06-12:/Safiullah-Rahu/CSV-AI</id>
    <link href="https://github.com/Safiullah-Rahu/CSV-AI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CSV-AI is the ultimate app powered by LangChain, OpenAI, and Streamlit that allows you to unlock hidden insights in your CSV files. With CSV-AI, you can effortlessly interact with, summarize, and analyze your CSV files in one convenient place.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CSV-AI ðŸ§ &lt;/h1&gt; &#xA;&lt;p&gt;CSV-AI is the ultimate app powered by LangChain, OpenAI, and Streamlit that allows you to unlock hidden insights in your CSV files. With CSV-AI, you can effortlessly interact with, summarize, and analyze your CSV files in one convenient place.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;CSV-AI offers the following key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interact:&lt;/strong&gt; Easily navigate through your CSV files and interact with the data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarize:&lt;/strong&gt; Generate descriptive summaries for your CSV data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analyze:&lt;/strong&gt; Perform advanced data analysis on your CSV files, including filtering, sorting, and visualizing the data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To run CSV-AI, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository to your local machine.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the project directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Safiullah-Rahu/CSV-AI.git&#xA;cd csv-ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the required packages using &lt;code&gt;pip&lt;/code&gt; with the provided &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To start CSV-AI, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will launch the CSV-AI app in your default web browser. You can then start exploring and analyzing your CSV files.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Safiullah-Rahu/CSV-AI/main/CSV_AI.PNG&#34; alt=&#34;CSV-AI App Homepage&#34;&gt; &#xA;&lt;h2&gt;Feedback and Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you have any feedback, suggestions, or issues related to CSV-AI, please open an issue on the GitHub repository. Contributions are also welcome! If you would like to contribute to CSV-AI, please follow the guidelines outlined in the Contribution Guidelines.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;CSV-AI is licensed under the MIT License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/audiocraft</title>
    <updated>2023-06-12T01:45:35Z</updated>
    <id>tag:github.com,2023-06-12:/facebookresearch/audiocraft</id>
    <link href="https://github.com/facebookresearch/audiocraft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Audiocraft&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_docs/badge.svg?sanitize=true&#34; alt=&#34;docs badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_linter/badge.svg?sanitize=true&#34; alt=&#34;linter badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/audiocraft/workflows/audiocraft_tests/badge.svg?sanitize=true&#34; alt=&#34;tests badge&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Audiocraft is a PyTorch library for deep learning research on audio generation. At the moment, it contains the code for MusicGen, a state-of-the-art controllable text-to-music model.&lt;/p&gt; &#xA;&lt;h2&gt;MusicGen&lt;/h2&gt; &#xA;&lt;p&gt;Audiocraft provides the code and models for MusicGen, &lt;a href=&#34;https://arxiv.org/abs/2306.05284&#34;&gt;a simple and controllable model for music generation&lt;/a&gt;. MusicGen is a single stage auto-regressive Transformer model trained over a 32kHz &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec tokenizer&lt;/a&gt; with 4 codebooks sampled at 50 Hz. Unlike existing methods like &lt;a href=&#34;https://arxiv.org/abs/2301.11325&#34;&gt;MusicLM&lt;/a&gt;, MusicGen doesn&#39;t require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio. Check out our &lt;a href=&#34;https://ai.honu.io/papers/musicgen/&#34;&gt;sample page&lt;/a&gt; or test the available demo!&lt;/p&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/drive/1-Xe9NCdIs2sCUbiSmwHXozK6AAhMm7_i?usp=sharing&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://huggingface.co/spaces/facebook/MusicGen&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;Open in HugginFace&#34;&gt; &lt;/a&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;We use 20K hours of licensed music to train MusicGen. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Audiocraft requires Python 3.9, PyTorch 2.0.0, and a GPU with at least 16 GB of memory (for the medium-sized model). To install Audiocraft, you can run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Best to make sure you have torch installed first, in particular before installing xformers.&#xA;# Don&#39;t run this if you already have PyTorch installed.&#xA;pip install &#39;torch&amp;gt;=2.0&#39;&#xA;# Then proceed to one of the following&#xA;pip install -U audiocraft  # stable release&#xA;pip install -U git+https://git@github.com/facebookresearch/audiocraft#egg=audiocraft  # bleeding edge&#xA;pip install -e .  # or if you cloned the repo locally&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We offer a number of way to interact with MusicGen:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can play with MusicGen by running the jupyter notebook at &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/demo.ipynb&#34;&gt;&lt;code&gt;demo.ipynb&lt;/code&gt;&lt;/a&gt; locally, or use the provided &lt;a href=&#34;https://colab.research.google.com/drive/1fxGqfg96RBUvGxZ1XXN07s3DthrKUl4-?usp=sharing&#34;&gt;colab notebook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can use the gradio demo locally by running &lt;code&gt;python app.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A demo is also available on the &lt;a href=&#34;https://huggingface.co/spaces/facebook/MusicGen&#34;&gt;&lt;code&gt;facebook/MusicGen&lt;/code&gt; HuggingFace Space&lt;/a&gt; (huge thanks to all the HF team for their support).&lt;/li&gt; &#xA; &lt;li&gt;Finally, you can run the &lt;a href=&#34;https://colab.research.google.com/drive/1-Xe9NCdIs2sCUbiSmwHXozK6AAhMm7_i?usp=sharing&#34;&gt;Gradio demo with a Colab GPU&lt;/a&gt;, as adapted from &lt;a href=&#34;https://github.com/camenduru/MusicGen-colab&#34;&gt;@camenduru Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;API&lt;/h2&gt; &#xA;&lt;p&gt;We provide a simple API and 4 pre-trained models. The pre trained models are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;small&lt;/code&gt;: 300M model, text to music only - &lt;a href=&#34;https://huggingface.co/facebook/musicgen-small&#34;&gt;ðŸ¤— Hub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;medium&lt;/code&gt;: 1.5B model, text to music only - &lt;a href=&#34;https://huggingface.co/facebook/musicgen-medium&#34;&gt;ðŸ¤— Hub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;melody&lt;/code&gt;: 1.5B model, text to music and text+melody to music - &lt;a href=&#34;https://huggingface.co/facebook/musicgen-melody&#34;&gt;ðŸ¤— Hub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;large&lt;/code&gt;: 3.3B model, text to music only - &lt;a href=&#34;https://huggingface.co/facebook/musicgen-large&#34;&gt;ðŸ¤— Hub&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We observe the best trade-off between quality and compute with the &lt;code&gt;medium&lt;/code&gt; or &lt;code&gt;melody&lt;/code&gt; model. In order to use MusicGen locally &lt;strong&gt;you must have a GPU&lt;/strong&gt;. We recommend 16GB of memory, but smaller GPUs will be able to generate short sequences, or longer sequences with the &lt;code&gt;small&lt;/code&gt; model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please make sure to have &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;ffmpeg&lt;/a&gt; installed when using newer version of &lt;code&gt;torchaudio&lt;/code&gt;. You can install it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See after a quick example for using the API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torchaudio&#xA;from audiocraft.models import MusicGen&#xA;from audiocraft.data.audio import audio_write&#xA;&#xA;model = MusicGen.get_pretrained(&#39;melody&#39;)&#xA;model.set_generation_params(duration=8)  # generate 8 seconds.&#xA;wav = model.generate_unconditional(4)    # generates 4 unconditional audio samples&#xA;descriptions = [&#39;happy rock&#39;, &#39;energetic EDM&#39;, &#39;sad jazz&#39;]&#xA;wav = model.generate(descriptions)  # generates 3 samples.&#xA;&#xA;melody, sr = torchaudio.load(&#39;./assets/bach.mp3&#39;)&#xA;# generates using the melody from the given audio and the provided descriptions.&#xA;wav = model.generate_with_chroma(descriptions, melody[None].expand(3, -1, -1), sr)&#xA;&#xA;for idx, one_wav in enumerate(wav):&#xA;    # Will save under {idx}.wav, with loudness normalization at -14 db LUFS.&#xA;    audio_write(f&#39;{idx}&#39;, one_wav.cpu(), model.sample_rate, strategy=&#34;loudness&#34;, loudness_compressor=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/MODEL_CARD.md&#34;&gt;the model card page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Will the training code be released?&lt;/h4&gt; &#xA;&lt;p&gt;Yes. We will soon release the training code for MusicGen and EnCodec.&lt;/p&gt; &#xA;&lt;h4&gt;I need help on Windows&lt;/h4&gt; &#xA;&lt;p&gt;@FurkanGozukara made a complete tutorial for &lt;a href=&#34;https://youtu.be/v-YpvPkhdO4&#34;&gt;Audiocraft/MusicGen on Windows&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{copet2023simple,&#xA;      title={Simple and Controllable Music Generation},&#xA;      author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre DÃ©fossez},&#xA;      year={2023},&#xA;      journal={arXiv preprint arXiv:2306.05284},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code in this repository is released under the MIT license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE&#34;&gt;LICENSE file&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The weights in this repository are released under the CC-BY-NC 4.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/audiocraft/main/LICENSE_weights&#34;&gt;LICENSE_weights file&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>deep-diver/LLM-As-Chatbot</title>
    <updated>2023-06-12T01:45:35Z</updated>
    <id>tag:github.com,2023-06-12:/deep-diver/LLM-As-Chatbot</id>
    <link href="https://github.com/deep-diver/LLM-As-Chatbot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM as a Chatbot Service&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ’¬ðŸš€ LLM as a Chatbot Service&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/gW7yKj9/2023-05-26-3-31-06.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The purpose of this repository is to let people to use lots of open sourced instruction-following fine-tuned LLM models as a Chatbot service. Because different models behave differently, and different models require differently formmated prompts, I made a very simple library &lt;a href=&#34;https://github.com/deep-diver/PingPong&#34;&gt;&lt;code&gt;Ping Pong&lt;/code&gt;&lt;/a&gt; for model agnostic conversation and context managements. Also, I made &lt;a href=&#34;https://github.com/deep-diver/gradio-chat&#34;&gt;&lt;code&gt;GradioChat&lt;/code&gt;&lt;/a&gt; UI looking similar to &lt;a href=&#34;https://huggingface.co/chat/&#34;&gt;HuggingChat&lt;/a&gt; but entirely built in Gradio. Those two projects are fully integrated to power this project.&lt;/p&gt; &#xA;&lt;h3&gt;Context management&lt;/h3&gt; &#xA;&lt;p&gt;Different model might have different strategies to manage context, so if you want to know the exact strategies applied to each model, take a look at the &lt;a href=&#34;https://github.com/deep-diver/LLM-As-Chatbot/tree/main/chats&#34;&gt;&lt;code&gt;chats&lt;/code&gt;&lt;/a&gt; directory. However, here are the basic ideas that I have come up with initially. I have found long prompts will slow down the generation process a lot eventually, so I thought the prompts should be kept as short as possible while as concise as possible at the same time. In the previous version, I have accumulated all the past conversations, and that didn&#39;t go well.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In every turn of the conversation, the past &lt;code&gt;N&lt;/code&gt; conversations will be kept. Think about the &lt;code&gt;N&lt;/code&gt; as a hyper-parameter. As an experiment, currently the past 2-3 conversations are only kept for all models.&lt;/li&gt; &#xA; &lt;li&gt;(TBD) In every turn of the conversation, it summarizes or extract information. The summarized information will be given in the every next turn of conversation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Currently supported models&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Checkout the list of models&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b&#34;&gt;tloen/alpaca-lora-7b&lt;/a&gt;: the original 7B Alpaca-LoRA checkpoint by tloen (updated by 4/4/2022)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Alpaca-LoRA-7B-elina&#34;&gt;LLMs/Alpaca-LoRA-7B-elina&lt;/a&gt;: the 7B Alpaca-LoRA checkpoint by Chansung (updated by 5/1/2022)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Alpaca-LoRA-13B-elina&#34;&gt;LLMs/Alpaca-LoRA-13B-elina&lt;/a&gt;: the 13B Alpaca-LoRA checkpoint by Chansung (updated by 5/1/2022)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Alpaca-LoRA-30B-elina&#34;&gt;LLMs/Alpaca-LoRA-30B-elina&lt;/a&gt;: the 30B Alpaca-LoRA checkpoint by Chansung (updated by 5/1/2022)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Alpaca-LoRA-65B-elina&#34;&gt;LLMs/Alpaca-LoRA-65B-elina&lt;/a&gt;: the 65B Alpaca-LoRA checkpoint by Chansung (updated by 5/1/2022)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/AlpacaGPT4-LoRA-7B-elina&#34;&gt;LLMs/AlpacaGPT4-LoRA-7B-elina&lt;/a&gt;: the 7B Alpaca-LoRA checkpoint trained on GPT4 generated Alpaca style dataset by Chansung (updated by 5/1/2022)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/AlpacaGPT4-LoRA-13B-elina&#34;&gt;LLMs/AlpacaGPT4-LoRA-13B-elina&lt;/a&gt;: the 13B Alpaca-LoRA checkpoint trained on GPT4 generated Alpaca style dataset by Chansung (updated by 5/1/2022)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b&#34;&gt;stabilityai/stablelm-tuned-alpha-7b&lt;/a&gt;: StableLM based fine-tuned model&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B&#34;&gt;beomi/KoAlpaca-Polyglot-12.8B&lt;/a&gt;: &lt;a href=&#34;https://github.com/EleutherAI/polyglot&#34;&gt;Polyglot&lt;/a&gt; based Alpaca style instruction fine-tuned model&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/declare-lab/flan-alpaca-xl&#34;&gt;declare-lab/flan-alpaca-xl&lt;/a&gt;: Flan XL(3B) based Alpaca style instruction fine-tuned model.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/declare-lab/flan-alpaca-xxl&#34;&gt;declare-lab/flan-alpaca-xxl&lt;/a&gt;: Flan XXL(11B) based Alpaca style instruction fine-tuned model.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/OpenAssistant/stablelm-7b-sft-v7-epoch-3&#34;&gt;OpenAssistant/stablelm-7b-sft-v7-epoch-3&lt;/a&gt;: StableLM(7B) based OpenAssistant&#39;s oasst1 instruction fine-tuned model.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Writer/camel-5b-hf&#34;&gt;Writer/camel-5b-hf&lt;/a&gt;: Palmyra-base based instruction fine-tuned model. The foundation model and the data are from its creator, &lt;a href=&#34;https://dev.writer.com&#34;&gt;Writer&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lmsys/fastchat-t5-3b-v1.0&#34;&gt;lmsys/fastchat-t5-3b-v1.0&lt;/a&gt;: T5(3B) based Vicuna style instruction fine-tuned model on SharedGPT by &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;lm-sys&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Stable-Vicuna-13B&#34;&gt;LLMs/Stable-Vicuna-13B&lt;/a&gt;: Stable Vicuna(13B) from Carpel AI and Stability AI. This is not a delta weight, so use it at your own risk. I will make this repo as private soon and add Hugging Face token field.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Vicuna-7b-v1.1&#34;&gt;LLMs/Vicuna-7b-v1.1&lt;/a&gt;: Vicuna(7B) from FastChat. This is not a delta weight, so use it at your own risk. I will make this repo as private soon and add Hugging Face token field.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Vicuna-13b-v1.1&#34;&gt;LLMs/Vicuna-13b-v1.1&lt;/a&gt;: Vicuna(13B) from FastChat. This is not a delta weight, so use it at your own risk. I will make this repo as private soon and add Hugging Face token field.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-7B-v0.1&#34;&gt;togethercomputer/RedPajama-INCITE-Chat-7B-v0.1&lt;/a&gt;: RedPajama INCITE Chat(7B) from Together.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b-chat&#34;&gt;mosaicml/mpt-7b-chat&lt;/a&gt;: MPT-7B from MOSAIC ML.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/teknium/llama-deus-7b-v3-lora&#34;&gt;teknium/llama-deus-7b-v3-lora&lt;/a&gt;: LLaMA 7B based Alpaca style instruction fine-tuned model. The only difference between Alpaca is that this model is fine-tuned on more data including Alpaca dataset, GPTeacher, General Instruct, Code Instruct, Roleplay Instruct, Roleplay V2 Instruct, GPT4-LLM Uncensored, Unnatural Instructions, WizardLM Uncensored, CamelAI&#39;s 20k Biology, 20k Physics, 20k Chemistry, 50k Math GPT4 Datasets, and CodeAlpaca&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceH4/starchat-alpha&#34;&gt;HuggingFaceH4/starchat-alpha&lt;/a&gt;: Starcoder 15.5B based instruction fine-tuned model. This model is particularly good at answering questions about coding.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Vicuna-LoRA-EvolInstruct-7B&#34;&gt;LLMs/Vicuna-LoRA-EvolInstruct-7B&lt;/a&gt;: LLaMA 7B based Vicuna style instruction fine-tuned model. The dataset to fine-tune this model is from WizardLM&#39;s Evol Instruction dataset.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/LLMs/Vicuna-LoRA-EvolInstruct-13B&#34;&gt;LLMs/Vicuna-LoRA-EvolInstruct-13B&lt;/a&gt;: LLaMA 13B based Vicuna style instruction fine-tuned model. The dataset to fine-tune this model is from WizardLM&#39;s Evol Instruction dataset.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/project-baize/baize-v2-7b&#34;&gt;project-baize/baize-v2-7b&lt;/a&gt;: LLaMA 7B based Baize&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/project-baize/baize-v2-7b&#34;&gt;project-baize/baize-v2-13b&lt;/a&gt;: LLaMA 13B based Baize&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/timdettmers/guanaco-7b&#34;&gt;timdettmers/guanaco-7b&lt;/a&gt;: LLaMA 7B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in &#34;QLoRA: Efficient Finetuning of Quantized LLMs&#34; paper.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/timdettmers/guanaco-13b&#34;&gt;timdettmers/guanaco-13b&lt;/a&gt;: LLaMA 13B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in &#34;QLoRA: Efficient Finetuning of Quantized LLMs&#34; paper.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/timdettmers/guanaco-33b-merged&#34;&gt;timdettmers/guanaco-33b-merged&lt;/a&gt;: LLaMA 30B based Guanaco which is fine-tuned on OASST1 dataset with QLoRA techniques introduced in &#34;QLoRA: Efficient Finetuning of Quantized LLMs&#34; paper.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-7b-instruct&#34;&gt;tiiuae/falcon-7b-instruct&lt;/a&gt;: Falcon 7B based instruction fine-tuned model on Baize, GPT4All, GPTeacher, and RefinedWeb-English datasets.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b-instruct&#34;&gt;tiiuae/falcon-40b-instruct&lt;/a&gt;: Falcon 40B based instruction fine-tuned model on Baize and RefinedWeb-English datasets.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Prerequisites&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note that the code only works &lt;code&gt;Python &amp;gt;= 3.9&lt;/code&gt; and &lt;code&gt;gradio &amp;gt;= 3.32.0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ conda create -n llm-serve python=3.9&#xA;$ conda activate llm-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies. &lt;code&gt;flash-attn&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt; are included to support &lt;code&gt;MPT&lt;/code&gt; models, If you don&#39;t want to use &lt;code&gt;MPT&lt;/code&gt;, comment them out, otherwise you will face two &lt;code&gt;module not found errors&lt;/code&gt;, then you will have to install &lt;code&gt;packaging&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt; packages while facing the errors.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ cd LLM-As-Chatbot&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run Gradio application&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to plugin your own model&lt;/h2&gt; &#xA;&lt;p&gt;You need to follow the following steps to bring your own models in this project.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add your model spec in &lt;a href=&#34;https://github.com/deep-diver/LLM-As-Chatbot/raw/main/model_cards.json&#34;&gt;&lt;code&gt;model_cards.json&lt;/code&gt;&lt;/a&gt;. If you don&#39;t have thumnail image, just leave it as blank string(&lt;code&gt;&#34;&#34;&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Add the button for your model in &lt;a href=&#34;https://github.com/deep-diver/LLM-As-Chatbot/raw/2efbb004a1989483cbdbd57a6d2b808f966f516a/app.py#L405&#34;&gt;&lt;code&gt;app.py&lt;/code&gt;&lt;/a&gt;. Don&#39;t forget to give it a name in the &lt;code&gt;gr.Button&lt;/code&gt; and &lt;code&gt;gr.Markdown&lt;/code&gt;. For placeholders, their names are omitted. Assign the &lt;code&gt;gr.Button&lt;/code&gt; to a variable with the name of your choice.&lt;/li&gt; &#xA; &lt;li&gt;Add the button variable to the &lt;a href=&#34;https://github.com/deep-diver/LLM-As-Chatbot/raw/2efbb004a1989483cbdbd57a6d2b808f966f516a/app.py#L559&#34;&gt;button list&lt;/a&gt; in the &lt;code&gt;app.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Determine the model type in &lt;a href=&#34;https://github.com/deep-diver/LLM-As-Chatbot/raw/2efbb004a1989483cbdbd57a6d2b808f966f516a/global_vars.py#L12&#34;&gt;&lt;code&gt;global_vars.py&lt;/code&gt;&lt;/a&gt;. If you think your model is similar to one of the existings, just add a filtering rules(&lt;code&gt;if-else&lt;/code&gt;) and give it the same name.&lt;/li&gt; &#xA; &lt;li&gt;(Optional) if your model is totally new one, you need to give a new &lt;code&gt;model_type&lt;/code&gt; in &lt;code&gt;global_vars.py&lt;/code&gt;, and make changes accordingly in &lt;code&gt;utils.py&lt;/code&gt;, and &lt;code&gt;chats/central.py&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Todos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio components to control the configurations of the generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;code&gt;Flan based Alpaca&lt;/code&gt; models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multiple conversation management&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement server only option w/ FastAPI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ChatGPT&#39;s plugin like features&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;I am thankful to &lt;a href=&#34;https://jarvislabs.ai/&#34;&gt;Jarvislabs.ai&lt;/a&gt; who generously provided free GPU resources to experiment with Alpaca-LoRA deployment and share it to communities to try out.&lt;/li&gt; &#xA; &lt;li&gt;I am thankful to &lt;a href=&#34;https://comcom.ai/ko/&#34;&gt;Common Computer&lt;/a&gt; who generously provided A100(40G) x 8 DGX workstation for fine-tuning the models.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>