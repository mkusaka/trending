<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-11T01:39:06Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ElectricAlexis/NotaGen</title>
    <updated>2025-03-11T01:39:06Z</updated>
    <id>tag:github.com,2025-03-11:/ElectricAlexis/NotaGen</id>
    <link href="https://github.com/ElectricAlexis/NotaGen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üéµ NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- ArXiv --&gt; &lt;a href=&#34;https://arxiv.org/abs/2502.18008&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/NotaGen_Paper-ArXiv-%23B31B1B?logo=arxiv&amp;amp;logoColor=white&#34; alt=&#34;Paper&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &#xA; &lt;!-- GitHub --&gt; &lt;a href=&#34;https://github.com/ElectricAlexis/NotaGen&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/NotaGen_Code-GitHub-%23181717?logo=github&amp;amp;logoColor=white&#34; alt=&#34;GitHub&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &#xA; &lt;!-- HuggingFace --&gt; &lt;a href=&#34;https://huggingface.co/ElectricAlexis/NotaGen&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/NotaGen_Weights-HuggingFace-%23FFD21F?logo=huggingface&amp;amp;logoColor=white&#34; alt=&#34;Weights&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &#xA; &lt;!-- Web Demo --&gt; &lt;a href=&#34;https://electricalexis.github.io/notagen-demo/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/NotaGen_Demo-Web-%23007ACC?logo=google-chrome&amp;amp;logoColor=white&#34; alt=&#34;Demo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ElectricAlexis/NotaGen/main/notagen.png&#34; alt=&#34;NotaGen&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üìñ Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NotaGen&lt;/strong&gt; is a symbolic music generation model that explores the potential of producing &lt;strong&gt;high-quality classical sheet music&lt;/strong&gt;. Inspired by the success of Large Language Models (LLMs), NotaGen adopts a three-stage training paradigm:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Pre-training&lt;/strong&gt; on 1.6M musical pieces&lt;/li&gt; &#xA; &lt;li&gt;üéØ &lt;strong&gt;Fine-tuning&lt;/strong&gt; on ~9K classical compositions with &lt;code&gt;period-composer-instrumentation&lt;/code&gt; prompts&lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;strong&gt;Reinforcement Learning&lt;/strong&gt; using our novel &lt;strong&gt;CLaMP-DPO&lt;/strong&gt; method (no human annotations or pre-defined rewards required.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check our &lt;a href=&#34;https://electricalexis.github.io/notagen-demo/&#34;&gt;demo page&lt;/a&gt; and enjoy music composed by NotaGen!&lt;/p&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Environment Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name notagen python=3.10&#xA;conda activate notagen&#xA;conda install pytorch==2.3.0 pytorch-cuda=11.8 -c pytorch -c nvidia&#xA;pip install accelerate&#xA;pip install optimum&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üèãÔ∏è NotaGen Model Weights&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-training&lt;/h3&gt; &#xA;&lt;p&gt;We provide pre-trained weights of different scales:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Patch-level Decoder Layers&lt;/th&gt; &#xA;   &lt;th&gt;Character-level Decoder Layers&lt;/th&gt; &#xA;   &lt;th&gt;Hidden Size&lt;/th&gt; &#xA;   &lt;th&gt;Patch Length (Context Length)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ElectricAlexis/NotaGen/blob/main/weights_notagen_pretrain_p_size_16_p_length_2048_p_layers_12_c_layers_3_h_size_768_lr_0.0002_batch_8.pth&#34;&gt;NotaGen-small&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110M&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ElectricAlexis/NotaGen/blob/main/weights_notagen_pretrain_p_size_16_p_length_2048_p_layers_16_c_layers_3_h_size_1024_lr_0.0001_batch_4.pth&#34;&gt;NotaGen-medium&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;244M&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ElectricAlexis/NotaGen/blob/main/weights_notagen_pretrain_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_0.0001_batch_4.pth&#34;&gt;NotaGen-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;516M&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;1280&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tuned NotaGen-large on a corpus of approximately 9k classical pieces. You can download the weights &lt;a href=&#34;https://huggingface.co/ElectricAlexis/NotaGen/blob/main/weights_notagen_pretrain-finetune_p_size_16_p_length_1024_p_layers_c_layers_6_20_h_size_1280_lr_1e-05_batch_1.pth&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Reinforcement-Learning&lt;/h3&gt; &#xA;&lt;p&gt;After pre-training and fine-tuning, we optimized NotaGen-large with 3 iterations of CLaMP-DPO. You can download the weights &lt;a href=&#34;https://huggingface.co/ElectricAlexis/NotaGen/blob/main/weights_notagen_pretrain-finetune-RL3_beta_0.1_lambda_10_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-06_batch_1.pth&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;üåü NotaGen-X&lt;/h3&gt; &#xA;&lt;p&gt;Inspired by Deepseek-R1, we further optimized the training procedures of NotaGen and released a better version --- &lt;a href=&#34;https://huggingface.co/ElectricAlexis/NotaGen/blob/main/weights_notagenx_p_size_16_p_length_1024_p_layers_20_h_size_1280.pth&#34;&gt;NotaGen-X&lt;/a&gt;. Compared to the version in the paper, NotaGen-X incorporates the following improvements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We introduced a post-training stage between pre-training and fine-tuning, refining the model with a classical-style subset of the pre-training dataset.&lt;/li&gt; &#xA; &lt;li&gt;We removed the key augmentation in the Fine-tune stage, making the instrument range of the generated compositions more reasonable.&lt;/li&gt; &#xA; &lt;li&gt;After RL, we utilized the resulting checkpoint to gather a new set of post-training data. Starting from the pre-trained checkpoint, we conducted another round of post-training, fine-tuning, and reinforcement learning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üéπ Gradio Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Local Gradio Demo&lt;/h3&gt; &#xA;&lt;p&gt;We developed a local Gradio demo for NotaGen-X. You can input &lt;strong&gt;&#34;Period-Composer-Instrumentation&#34;&lt;/strong&gt; as the prompt to have NotaGen generate musicÔºÅ&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ElectricAlexis/NotaGen/main/gradio/illustration.png&#34; alt=&#34;NotaGen Gradio Demo&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Deploying NotaGen-X inference locally requires at least 24GB of GPU memory. For implementation details, please view &lt;a href=&#34;https://github.com/ElectricAlexis/NotaGen/raw/main/gradio/README.md&#34;&gt;gradio/README.md&lt;/a&gt;. We are also working on developing an online demo.&lt;/p&gt; &#xA;&lt;h3&gt;Online Colab Notebook&lt;/h3&gt; &#xA;&lt;p&gt;Thanks for &lt;a href=&#34;https://github.com/deeplearn-art/NotaGen&#34;&gt;@deeplearn-art&lt;/a&gt;&#39;s contribution of a &lt;a href=&#34;https://colab.research.google.com/drive/1yJA1wG0fiwNeehdQxAUw56i4bTXzoVVv?usp=sharing&#34;&gt;Google Colab notebook for NotaGen&lt;/a&gt;! You can run it and access to a Gradio public link to play with this demo.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Data Pre-processing &amp;amp; Post-processing&lt;/h2&gt; &#xA;&lt;p&gt;For converting &lt;strong&gt;ABC notation&lt;/strong&gt; files from / to &lt;strong&gt;MusicXML&lt;/strong&gt; files, please view &lt;a href=&#34;https://github.com/ElectricAlexis/NotaGen/raw/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt; for instructions.&lt;/p&gt; &#xA;&lt;p&gt;To illustrate the specific data format, we provide a small dataset of &lt;strong&gt;Schubert&#39;s lieder&lt;/strong&gt; compositions from the &lt;a href=&#34;https://github.com/OpenScore/Lieder&#34;&gt;OpenScore Lieder&lt;/a&gt;, which includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üóÇÔ∏è Interleaved ABC folders&lt;/li&gt; &#xA; &lt;li&gt;üóÇÔ∏è Augmented ABC folders&lt;/li&gt; &#xA; &lt;li&gt;üìÑ Data index files for training and evaluation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can download it &lt;a href=&#34;https://drive.google.com/drive/folders/1iVLkcywzXGcHFodce9nDQyEmK4UDmBtY?usp=sharing&#34;&gt;here&lt;/a&gt; and put it under &lt;code&gt;data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In the instructions of &lt;strong&gt;Fine-tuning&lt;/strong&gt; and &lt;strong&gt;Reinforcement Learning&lt;/strong&gt; below, we will use this dataset as an example of our implementation. &lt;strong&gt;It won&#39;t include the &#34;period-composer-instrumentation&#34; conditioning&lt;/strong&gt;, just for showing how to adapt the pretrained NotaGen to a specific music style.&lt;/p&gt; &#xA;&lt;h2&gt;üß† Pre-train&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use your own data to pre-train a blank &lt;strong&gt;NotaGen&lt;/strong&gt; model, please:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Preprocess the data and generate the data index files following the instructions in &lt;a href=&#34;https://github.com/ElectricAlexis/NotaGen/raw/main/data/README.md&#34;&gt;data/README.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Modify the parameters in &lt;code&gt;pretrain/config.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Use this command for pre-training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd pretrain/&#xA;accelerate launch --multi_gpu --mixed_precision fp16 train-gen.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üéØ Fine-tune&lt;/h2&gt; &#xA;&lt;p&gt;Here we give an example on fine-tuning &lt;strong&gt;NotaGen-large&lt;/strong&gt; with the &lt;strong&gt;Schubert&#39;s lieder&lt;/strong&gt; data mentioned above.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notice:&lt;/strong&gt; The use of &lt;strong&gt;NotaGen-large&lt;/strong&gt; requires at least &lt;strong&gt;24GB of GPU memory&lt;/strong&gt; for training and inference. Alternatively, you may use &lt;strong&gt;NotaGen-small&lt;/strong&gt; or &lt;strong&gt;NotaGen-medium&lt;/strong&gt; and change the configuration of models in &lt;code&gt;finetune/config.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;finetune/config.py&lt;/code&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Modify the &lt;code&gt;DATA_TRAIN_INDEX_PATH&lt;/code&gt; and &lt;code&gt;DATA_EVAL_INDEX_PATH&lt;/code&gt;: &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Configuration for the data&#xA;DATA_TRAIN_INDEX_PATH = &#34;../data/schubert_augmented_train.jsonl&#34; &#xA;DATA_EVAL_INDEX_PATH  = &#34;../data/schubert_augmented_eval.jsonl&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Download pre-trained NotaGen weights, and modify the &lt;code&gt;PRETRAINED_PATH&lt;/code&gt;: &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;PRETRAINED_PATH = &#34;../pretrain/weights_notagen_pretrain_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_0.0001_batch_4.pth&#34;  # Use NotaGen-large&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;EXP_TAG&lt;/code&gt; is for differentiating the models. It will be integrated into the ckpt&#39;s name. Here we set it to &lt;code&gt;schubert&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;You can also modify other parameters like the learning rate.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Execution&lt;/h3&gt; &#xA;&lt;p&gt;Use this command for fine-tuning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd finetune/&#xA;CUDA_VISIBLE_DEVICES=0 python train-gen.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Reinforcement Learning (CLaMP-DPO)&lt;/h2&gt; &#xA;&lt;p&gt;Here we give an example on how to use &lt;strong&gt;CLaMP-DPO&lt;/strong&gt; to enhance the model fine-tuned with &lt;strong&gt;Schubert&#39;s lieder&lt;/strong&gt; data.&lt;/p&gt; &#xA;&lt;h3&gt;‚öôÔ∏è &lt;a href=&#34;https://github.com/sanderwood/clamp2&#34;&gt;CLaMP 2&lt;/a&gt; Setup&lt;/h3&gt; &#xA;&lt;p&gt;Download model weights and put them under the &lt;code&gt;clamp2/&lt;/code&gt;folder:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/sander-wood/clamp2/blob/main/weights_clamp2_h_size_768_lr_5e-05_batch_128_scale_1_t_length_128_t_model_FacebookAI_xlm-roberta-base_t_dropout_True_m3_True.pth&#34;&gt;CLaMP 2 Model Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/sander-wood/clamp2/blob/main/weights_m3_p_size_64_p_length_512_t_layers_3_p_layers_12_h_size_768_lr_0.0001_batch_16_mask_0.45.pth&#34;&gt;M3 Model Weights&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üîç Extract Ground Truth Features&lt;/h3&gt; &#xA;&lt;p&gt;Modify &lt;code&gt;input_dir&lt;/code&gt; and &lt;code&gt;output_dir&lt;/code&gt; in &lt;code&gt;clamp2/extract_clamp2.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_dir = &#39;../data/schubert_interleaved&#39;  # interleaved abc folder&#xA;output_dir = &#39;feature/schubert_interleaved&#39;  # feature folder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extract the features:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd clamp2/&#xA;python extract_clamp2.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üîÑ CLaMP-DPO&lt;/h3&gt; &#xA;&lt;p&gt;Here we give an example of an iteration of &lt;strong&gt;CLaMP-DPO&lt;/strong&gt; from the initial model fine-tuned on &lt;strong&gt;Schubert&#39;s lieder&lt;/strong&gt; data.&lt;/p&gt; &#xA;&lt;h4&gt;1. Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify the &lt;code&gt;INFERENCE_WEIGHTS_PATH&lt;/code&gt; to path of the fine-tuned weights and &lt;code&gt;NUM_SAMPLES&lt;/code&gt; to generate in &lt;code&gt;inference/config.py&lt;/code&gt;: &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  INFERENCE_WEIGHTS_PATH = &#39;../finetune/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1.pth&#39;              &#xA;  NUM_SAMPLES = 1000                                               &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Inference: &lt;pre&gt;&lt;code&gt;cd inference/&#xA;python inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; This will generate an &lt;code&gt;output/&lt;/code&gt;folder with two subfolders: &lt;code&gt;original&lt;/code&gt; and &lt;code&gt;interleaved&lt;/code&gt;. The &lt;code&gt;original/&lt;/code&gt; subdirectory stores the raw inference outputs from the model, while the &lt;code&gt;interleaved/&lt;/code&gt; subdirectory contains data post-processed with rest measure completion, compatible with CLaMP 2. Each of these subdirectories will contain a model-specific folder, named as a combination of the model&#39;s name and its sampling parameters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Extract Generated Data Features&lt;/h4&gt; &#xA;&lt;p&gt;Modify &lt;code&gt;input_dir&lt;/code&gt; and &lt;code&gt;output_dir&lt;/code&gt; in &lt;code&gt;clamp2/extract_clamp2.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_dir = &#39;../output/interleaved/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1_k_9_p_0.9_temp_1.2&#39;  # interleaved abc folder&#xA;output_dir = &#39;feature/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1_k_9_p_0.9_temp_1.2&#39;  # feature folder&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Extract the features:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd clamp2/&#xA;python extract_clamp2.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Statistics on Averge CLaMP 2 Score (Optional)&lt;/h4&gt; &#xA;&lt;p&gt;If you&#39;re interested in the &lt;strong&gt;Average CLaMP 2 Score&lt;/strong&gt; of the current model, modify the parameters in &lt;code&gt;clamp2/statistics.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gt_feature_folder = &#39;feature/schubert_interleaved&#39;&#xA;output_feature_folder = &#39;feature/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1_k_9_p_0.9_temp_1.2&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run this script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd clamp2/&#xA;python statistics.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Construct Preference Data&lt;/h4&gt; &#xA;&lt;p&gt;Modify the parameters in &lt;code&gt;RL/data.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gt_feature_folder = &#39;../clamp2/feature/schubert_interleaved&#39;&#xA;output_feature_folder = &#39;../clamp2/feature/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1_k_9_p_0.9_temp_1.2&#39;&#xA;output_original_abc_folder = &#39;../output/original/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1_k_9_p_0.9_temp_1.2&#39;&#xA;output_interleaved_abc_folder = &#39;../output/interleaved/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1_k_9_p_0.9_temp_1.2&#39;&#xA;data_index_path = &#39;schubert_RL1.json&#39;  # Data for the first iteration of RL&#xA;data_select_portion = 0.1              &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this script, the &lt;strong&gt;CLaMP 2 Score&lt;/strong&gt; of each generated piece will be calculated and sorted. The portion of data in the chosen and rejected sets is determined by &lt;code&gt;data_select_portion&lt;/code&gt;. Additionally, there are also three rules to exclude problematic sheets from the chosen set:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sheets with duration alignment problems are excluded;&lt;/li&gt; &#xA; &lt;li&gt;Sheets that may plagiarize from ground truth data (ld_sim&amp;gt;0.95) are excluded;&lt;/li&gt; &#xA; &lt;li&gt;Sheets where staves for the same instrument are not grouped together are excluded.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The prefence data file will be names as &lt;code&gt;data_index_path&lt;/code&gt;, which records the file paths in chosen and rejected sets.&lt;/p&gt; &#xA;&lt;p&gt;Run this script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd RL/&#xA;python data.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;5. DPO Training&lt;/h4&gt; &#xA;&lt;p&gt;Modify the parameters in &lt;code&gt;RL/config.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DATA_INDEX_PATH = &#39;schubert_RL1.json&#39;  # Preference data path&#xA;PRETRAINED_PATH = &#39;../finetune/weights_notagen_schubert_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-05_batch_1.pth&#39;  # The model to go through DPO optimization&#xA;EXP_TAG = &#39;schubert-RL1&#39;              # Model tag for differentiation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also modify other parameters like &lt;code&gt;OPTIMATION_STEPS&lt;/code&gt; and DPO hyper-parameters.&lt;/p&gt; &#xA;&lt;p&gt;Run this script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd RL/&#xA;CUDA_VISIBLE_DEVICES=0 python train.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After training, a model named &lt;code&gt;weights_notagen_schubert-RL1_beta_0.1_lambda_10_p_size_16_p_length_1024_p_layers_20_c_layers_6_h_size_1280_lr_1e-06.pth&lt;/code&gt; will be saved under &lt;code&gt;RL/&lt;/code&gt;. For the second round of CLaMP-DPO, please go back to the first inference stage, and let the new model to generate pieces.&lt;/p&gt; &#xA;&lt;p&gt;For this small experiment on &lt;strong&gt;Schubert&#39;s lieder&lt;/strong&gt; data, we post our &lt;strong&gt;Average CLaMP 2 Score&lt;/strong&gt; here for the fine-tuned model and models after each iteration of CLaMP-DPO, as a reference:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;CLaMP-DPO Iteration (K)&lt;/th&gt; &#xA;   &lt;th&gt;Average CLaMP 2 Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0 (fine-tuned)&lt;/td&gt; &#xA;   &lt;td&gt;0.324&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;0.579&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;0.778&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;If you are interested in this method, have a try on your own style-specific dataset :D&lt;/p&gt; &#xA;&lt;h2&gt;üìö Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;strong&gt;NotaGen&lt;/strong&gt; or &lt;strong&gt;CLaMP-DPO&lt;/strong&gt; useful in your work, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{wang2025notagenadvancingmusicalitysymbolic,&#xA;      title={NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms}, &#xA;      author={Yashan Wang and Shangda Wu and Jianhuai Hu and Xingjian Du and Yueqi Peng and Yongxin Huang and Shuai Fan and Xiaobing Li and Feng Yu and Maosong Sun},&#xA;      year={2025},&#xA;      eprint={2502.18008},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SD},&#xA;      url={https://arxiv.org/abs/2502.18008}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>thalissonvs/pydoll</title>
    <updated>2025-03-11T01:39:06Z</updated>
    <id>tag:github.com,2025-03-11:/thalissonvs/pydoll</id>
    <link href="https://github.com/thalissonvs/pydoll" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pydoll is a library for automating chromium-based browsers without a WebDriver, offering realistic interactions. It supports Python&#39;s asynchronous features, enhancing performance and enabling event capturing and simultaneous web scraping.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1&gt;üöÄ Pydoll: Async Web Automation in Python!&lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/c4615101-d932-4e79-8a08-f50fbc686e3b&#34; alt=&#34;Alt text&#34;&gt; &lt;br&gt;&lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://codecov.io/github/thalissonvs/pydoll/graph/badge.svg?token=40I938OGM9&#34;&gt; &lt;img src=&#34;https://github.com/thalissonvs/pydoll/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt; &lt;img src=&#34;https://github.com/thalissonvs/pydoll/actions/workflows/ruff-ci.yml/badge.svg?sanitize=true&#34; alt=&#34;Ruff CI&#34;&gt; &lt;img src=&#34;https://github.com/thalissonvs/pydoll/actions/workflows/release.yml/badge.svg?sanitize=true&#34; alt=&#34;Release&#34;&gt; &lt;img src=&#34;https://tokei.rs/b1/github/thalissonvs/pydoll&#34; alt=&#34;Total lines&#34;&gt; &lt;img src=&#34;https://tokei.rs/b1/github/thalissonvs/pydoll?category=files&#34; alt=&#34;Files&#34;&gt; &lt;img src=&#34;https://tokei.rs/b1/github/thalissonvs/pydoll?category=comments&#34; alt=&#34;Comments&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/thalissonvs/pydoll?label=Issues&#34; alt=&#34;GitHub issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-closed/thalissonvs/pydoll?label=Closed%20issues&#34; alt=&#34;GitHub closed issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/thalissonvs/pydoll/bug?label=Bugs&amp;amp;color=red&#34; alt=&#34;GitHub bug issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/thalissonvs/pydoll/enhancement?label=Enhancements&amp;amp;color=purple&#34; alt=&#34;GitHub enhancement issues&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/13125&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/13125&#34; alt=&#34;thalissonvs%2Fpydoll | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Pydoll is an innovative Python library that&#39;s redefining Chromium browser automation! Unlike other solutions, Pydoll &lt;strong&gt;completely eliminates the need for webdrivers&lt;/strong&gt;, providing a much more fluid and reliable automation experience.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê Extraordinary Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero Webdrivers!&lt;/strong&gt; Say goodbye to webdriver compatibility and configuration headaches&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Native Captcha Bypass!&lt;/strong&gt; Naturally passes through Cloudflare Turnstile and reCAPTCHA v3 *&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt; thanks to native asynchronous programming&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Realistic Interactions&lt;/strong&gt; that simulate human behavior&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced Event System&lt;/strong&gt; for complex and reactive automations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: for cloudflare captcha, you have to perform a click in the checkbox. Just find a div containing the iframe and use the &lt;code&gt;.click()&lt;/code&gt; method. Automatic detection and click coming soon!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#-installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#-quick-start&#34;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#-core-components&#34;&gt;Core Components&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#browser-interface&#34;&gt;Browser Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#page-interface&#34;&gt;Page Interface&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#webelement-interface&#34;&gt;WebElement Interface&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#-advanced-features&#34;&gt;Advanced Features&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#event-system&#34;&gt;Event System&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#concurrent-scraping&#34;&gt;Concurrent Scraping&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/#proxy-configuration&#34;&gt;Proxy Configuration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üî• Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pydoll-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;See how simple it is to get started - no webdriver configuration needed!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from pydoll.browser.chrome import Chrome&#xA;from pydoll.constants import By&#xA;&#xA;async def main():&#xA;    # Start the browser with no additional webdriver configuration!&#xA;    async with Chrome() as browser:&#xA;        await browser.start()&#xA;        page = await browser.get_page()&#xA;        &#xA;        # Navigate through captcha-protected sites without worry&#xA;        await page.go_to(&#39;https://example-with-cloudflare.com&#39;)&#xA;        button = await page.find_element(By.CSS_SELECTOR, &#39;button&#39;)&#xA;        await button.click()&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üéØ Core Components&lt;/h2&gt; &#xA;&lt;h3&gt;Browser Interface&lt;/h3&gt; &#xA;&lt;p&gt;Powerful interface for global browser control:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def browser_examples():&#xA;    async with Chrome() as browser:&#xA;        await browser.start()&#xA;        # Control multiple pages with incredible ease&#xA;        pages = [await browser.get_page() for _ in range(3)]&#xA;        &#xA;        # Advanced settings with a simple command&#xA;        await browser.set_window_maximized()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Page Interface&lt;/h3&gt; &#xA;&lt;p&gt;Individual page control with surgical precision:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def page_examples():&#xA;    page = await browser.get_page()&#xA;    &#xA;    # Smooth navigation, even on protected sites&#xA;    await page.go_to(&#39;https://site-with-recaptcha.com&#39;)&#xA;    &#xA;    # Capture perfect screenshots&#xA;    await page.get_screenshot(&#39;/screenshots/evidence.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;WebElement Interface&lt;/h3&gt; &#xA;&lt;p&gt;Interact with elements like a real user:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def element_examples():&#xA;    # Natural and precise interactions&#xA;    input_field = await page.find_element(By.CSS_SELECTOR, &#39;input&#39;)&#xA;    await input_field.type_keys(&#39;Hello World&#39;)  # Realistic typing!&#xA;    &#xA;    # Intuitive chained operations&#xA;    dropdown = await page.find_element(By.CSS_SELECTOR, &#39;select&#39;)&#xA;    await dropdown.select_option(&#39;value&#39;)&#xA;&#xA;    # Realistic clicks with offset&#xA;    button = await page.find_element(By.CSS_SELECTOR, &#39;button&#39;)&#xA;    await button.click(x_offset=5, y_offset=10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Advanced Features&lt;/h2&gt; &#xA;&lt;h3&gt;Event System&lt;/h3&gt; &#xA;&lt;p&gt;Powerful event system for intelligent automation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pydoll.events.page import PageEvents&#xA;&#xA;async def event_example():&#xA;    await page.enable_page_events()&#xA;    # React to events in real-time!&#xA;    await page.on(PageEvents.PAGE_LOADED, &#xA;                  lambda e: print(&#39;Page loaded successfully!&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Concurrent Scraping&lt;/h3&gt; &#xA;&lt;p&gt;Scrape multiple pages simultaneously with extraordinary performance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def concurrent_example():&#xA;    pages = [await browser.get_page() for _ in range(10)]&#xA;    # Parallel scraping with intelligent resource management&#xA;    results = await asyncio.gather(&#xA;        *(scrape_page(page) for page in pages)&#xA;    )&#xA;    # Just declare the scrape_page method and see the magic happens!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Proxy Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Robust proxy support, including authentication:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def proxy_example():&#xA;    options = Options()&#xA;    # Private or public proxies, you choose!&#xA;    options.add_argument(&#39;--proxy-server=username:password@ip:port&#39;)&#xA;    &#xA;    async with Chrome(options=options) as browser:&#xA;        await browser.start()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For exploring all available methods and additional features, check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Browser interface: &lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/pydoll/browser/base.py&#34;&gt;pydoll/browser/base.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Page interface: &lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/pydoll/browser/page.py&#34;&gt;pydoll/browser/page.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;WebElement interface: &lt;a href=&#34;https://raw.githubusercontent.com/thalissonvs/pydoll/main/pydoll/element.py&#34;&gt;pydoll/element.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chrome options: &lt;a href=&#34;https://peter.sh/experiments/chromium-command-line-switches/&#34;&gt;Chromium Command Line Switches&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üéâ Start Now!&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to use, open issues and contributing!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kijai/ComfyUI-HunyuanVideoWrapper</title>
    <updated>2025-03-11T01:39:06Z</updated>
    <id>tag:github.com,2025-03-11:/kijai/ComfyUI-HunyuanVideoWrapper</id>
    <link href="https://github.com/kijai/ComfyUI-HunyuanVideoWrapper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI wrapper nodes for &lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;Update 5&lt;/h1&gt; &#xA;&lt;p&gt;So I know I said I&#39;d stop working on this, but with all the new stuff out I wanted to work on those and have included the official I2V, it&#39;s &#34;fixed&#34; version 2 and the &lt;a href=&#34;https://huggingface.co/Kijai/HunyuanVideo_comfy/blob/main/hyvid_I2V_lora_embrace.safetensors&#34;&gt;LoRAs&lt;/a&gt; they included in the release&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/8ce4b1ee-fb63-49a2-83b4-ba8ef1a8b842&#34;&gt;https://github.com/user-attachments/assets/8ce4b1ee-fb63-49a2-83b4-ba8ef1a8b842&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;and the &lt;a href=&#34;https://github.com/dashtoon/hunyuan-video-keyframe-control-lora&#34;&gt;dashtoon keyframe LoRA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/2b6e32e4-470f-4feb-b299-5a453e2b4fa1&#34;&gt;https://github.com/user-attachments/assets/2b6e32e4-470f-4feb-b299-5a453e2b4fa1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Also because there&#39;s been so much trouble in using the transformer model for text encoding, I figured a way to use the text embeds from native ComfyUI text encoding, like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/80b23087-a66d-4937-bb2c-d15d5a20304b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Not that it does give somewhat different results and using these nodes like that can&#39;t be considered as original implementation wrapper anymore.&lt;/p&gt; &#xA;&lt;h1&gt;Update 4, the non-update:&lt;/h1&gt; &#xA;&lt;p&gt;As the native implementation exists, and has support for most features by now, I will mostly stop working on these nodes for anything but it&#39;s main purpose: early access and testing of potential new features that are difficult (at least for me) to implement natively.&lt;/p&gt; &#xA;&lt;h2&gt;Some resources for native workflows:&lt;/h2&gt; &#xA;&lt;p&gt;Flowedit and enhance-a-video can be found from these nodes: &lt;a href=&#34;https://github.com/logtd/ComfyUI-HunyuanLoom&#34;&gt;https://github.com/logtd/ComfyUI-HunyuanLoom&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;TeaCache equilevant FirstBlockCache, as well as torch.compile with LoRA support: &lt;a href=&#34;https://github.com/chengzeyi/Comfy-WaveSpeed&#34;&gt;https://github.com/chengzeyi/Comfy-WaveSpeed&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sageattention can be enabled by &lt;code&gt;--use-sage-attention&lt;/code&gt; startup argument for ComfyUI, or with a patcher node found in &lt;a href=&#34;https://github.com/kijai/ComfyUI-KJNodes&#34;&gt;KJNodes&lt;/a&gt; as well as some other node packs.&lt;/p&gt; &#xA;&lt;p&gt;Leapfusion I2V can be used with my patcher node found in the KJNodes as well, example workflow: &lt;a href=&#34;https://github.com/kijai/ComfyUI-KJNodes/raw/main/example_workflows/leapfusion_hunyuuanvideo_i2v_native_testing.json&#34;&gt;https://github.com/kijai/ComfyUI-KJNodes/blob/main/example_workflows/leapfusion_hunyuuanvideo_i2v_native_testing.json&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;What remains missing from native implementation currently:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;context windowing&lt;/li&gt; &#xA; &lt;li&gt;direct image embed support through IP2V&lt;/li&gt; &#xA; &lt;li&gt;manual memory management&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Update 3:&lt;/h1&gt; &#xA;&lt;p&gt;It&#39;s been hectic couple of weeks with this model, I&#39;ve lost track of what has happened since the start, but I&#39;ll try to present some of the more important updates:&lt;/p&gt; &#xA;&lt;h2&gt;Official scaled fp8 weights were released:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/tencent/HunyuanVideo/blob/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8.pt&#34;&gt;https://huggingface.co/tencent/HunyuanVideo/blob/main/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8.pt&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Even if this file is .pt it&#39;s completely safe and it is loaded with weights_only, the scale map is included with the nodes. To use this model you have to use the &lt;code&gt;fp8_scaled&lt;/code&gt; -quantization option in the model loader. The quality of these weights is much closer to the original bf16, downside is that they do not currently support fp8 fast mode, or LoRAs.&lt;/p&gt; &#xA;&lt;h2&gt;Almost free quality increase with &lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/Enhance-A-Video&#34;&gt;Enhance-A-Video&lt;/a&gt;:&lt;/h2&gt; &#xA;&lt;p&gt;This has a very slight hit on inference speed and zero hit on memory use, initial tests indicate it&#39;s absolutely worth using.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/68f0b5eb-aa23-49e1-a48f-fd3c4b1108ed&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/e19b30e1-5f67-4e75-9c73-716d4569c319&#34;&gt;https://github.com/user-attachments/assets/e19b30e1-5f67-4e75-9c73-716d4569c319&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/083353a2-e9aa-43e9-a916-ff3af1d581c1&#34;&gt;https://github.com/user-attachments/assets/083353a2-e9aa-43e9-a916-ff3af1d581c1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Update 2: Experimental IP2V - Image Prompting to Video via VLM by @Dango233&lt;/h1&gt; &#xA;&lt;h2&gt;WORK IN PROGRESS - But it should work now!&lt;/h2&gt; &#xA;&lt;p&gt;Now you can feed image to the VLM as condition of generations! This is different from image2video where the image become the first frame of the video. IP2V uses image as a part of the prompt, to extract the concept and style of the image. So - very much like IPAdapter - but VLM will do the heavy lifting for you!&lt;/p&gt; &#xA;&lt;p&gt;Now this is a tuning free approach but with further task specific tuning we can expand the use scenarios.&lt;/p&gt; &#xA;&lt;h2&gt;Guide to Using &lt;code&gt;xtuner/llava-llama-3-8b-v1_1-transformers&lt;/code&gt; for Image-Text Tasks&lt;/h2&gt; &#xA;&lt;h2&gt;Step 1: Model Selection&lt;/h2&gt; &#xA;&lt;p&gt;Use the original &lt;code&gt;xtuner/llava-llama-3-8b-v1_1-transformers&lt;/code&gt; model which includes the vision tower. You have two options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the model and place it in the &lt;code&gt;models/LLM&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Rely on the auto-download mechanism.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; It&#39;s recommended to offload the text encoder since the vision tower requires additional VRAM.&lt;/p&gt; &#xA;&lt;h2&gt;Step 2: Load and Connect Image&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use the comfy native node to load the image.&lt;/li&gt; &#xA; &lt;li&gt;Connect the loaded image to the &lt;code&gt;Hunyuan TextImageEncode&lt;/code&gt; node. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can connect up to 2 images to this node.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Step 3: Prompting with Images&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reference the image in your prompt by including &lt;code&gt;&amp;lt;image&amp;gt;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The number of &lt;code&gt;&amp;lt;image&amp;gt;&lt;/code&gt; tags should match the number of images provided to the sampler. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Example prompt: &lt;code&gt;Describe this &amp;lt;image&amp;gt; in great detail.&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also choose to give CLIP a prompt that does not reference the image separately.&lt;/p&gt; &#xA;&lt;h2&gt;Step 4: Advanced Configuration - &lt;code&gt;image_token_selection_expression&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This expression is for advanced users and serves as a boolean mask to select which part of the image hidden state will be used for conditioning. Here are some details and recommendations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The hidden state sequence length (or number of tokens) per image in llava-llama-3 is 576.&lt;/li&gt; &#xA; &lt;li&gt;The default setting is &lt;code&gt;::4&lt;/code&gt;, meaning every four tokens, one token goes into conditioning, interleaved, resulting in 144 tokens per image.&lt;/li&gt; &#xA; &lt;li&gt;Generally, more tokens lean more towards the conditional image.&lt;/li&gt; &#xA; &lt;li&gt;However, too many tokens (especially if the overall token count exceeds 256) will degrade generation quality. It&#39;s recommended not to use more than half the tokens (&lt;code&gt;::2&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Interleaved tokens generally perform better, but you might also want to try the following expressions: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;:128&lt;/code&gt; - First 128 tokens.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;-128:&lt;/code&gt; - Last 128 tokens.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;:128, -128:&lt;/code&gt; - First 128 tokens and last 128 tokens.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;With a proper prompting strategy, even not passing in any image tokens (leaving the expression blank) can yield decent effects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Update&lt;/h1&gt; &#xA;&lt;p&gt;Scaled dot product attention (sdpa) should now be working (only tested on Windows, torch 2.5.1+cu124 on 4090), sageattention is still recommended for speed, but should not be necessary anymore making installation much easier.&lt;/p&gt; &#xA;&lt;p&gt;Vid2vid test: &lt;a href=&#34;https://www.pexels.com/video/a-4x4-vehicle-speeding-on-a-dirt-road-during-a-competition-15604814/&#34;&gt;source video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/12940721-4168-4e2b-8a71-31b4b0432314&#34;&gt;https://github.com/user-attachments/assets/12940721-4168-4e2b-8a71-31b4b0432314&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;text2vid (old test):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/3750da65-9753-4bd2-aae2-a688d2b86115&#34;&gt;https://github.com/user-attachments/assets/3750da65-9753-4bd2-aae2-a688d2b86115&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Transformer and VAE (single files, no autodownload):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Kijai/HunyuanVideo_comfy/tree/main&#34;&gt;https://huggingface.co/Kijai/HunyuanVideo_comfy/tree/main&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to the usual ComfyUI folders (diffusion_models and vae)&lt;/p&gt; &#xA;&lt;p&gt;LLM text encoder (has autodownload):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Kijai/llava-llama-3-8b-text-encoder-tokenizer&#34;&gt;https://huggingface.co/Kijai/llava-llama-3-8b-text-encoder-tokenizer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Files go to &lt;code&gt;ComfyUI/models/LLM/llava-llama-3-8b-text-encoder-tokenizer&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clip text encoder (has autodownload)&lt;/p&gt; &#xA;&lt;p&gt;Either use any Clip_L model supported by ComfyUI by disabling the clip_model in the text encoder loader and plugging in ClipLoader to the text encoder node, or allow the autodownloader to fetch the original clip model from:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14&#34;&gt;https://huggingface.co/openai/clip-vit-large-patch14&lt;/a&gt;, (only need the .safetensor from the weights, and all the config files) to:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ComfyUI/models/clip/clip-vit-large-patch14&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Memory use is entirely dependant on resolution and frame count, don&#39;t expect to be able to go very high even on 24GB.&lt;/p&gt; &#xA;&lt;p&gt;Good news is that the model can do functional videos even at really low resolutions.&lt;/p&gt;</summary>
  </entry>
</feed>