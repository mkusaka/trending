<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-21T01:33:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>thuml/Time-Series-Library</title>
    <updated>2024-08-21T01:33:07Z</updated>
    <id>tag:github.com,2024-08-21:/thuml/Time-Series-Library</id>
    <link href="https://github.com/thuml/Time-Series-Library" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Library for Advanced Deep Time Series Models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Time Series Library (TSLib)&lt;/h1&gt; &#xA;&lt;p&gt;TSLib is an open-source library for deep learning researchers, especially for deep time series analysis.&lt;/p&gt; &#xA;&lt;p&gt;We provide a neat code base to evaluate advanced deep time series models or develop your model, which covers five mainstream tasks: &lt;strong&gt;long- and short-term forecasting, imputation, anomaly detection, and classification.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2024.07) We wrote a comprehensive survey of &lt;a href=&#34;https://arxiv.org/abs/2407.13278&#34;&gt;[Deep Time Series Models]&lt;/a&gt; with a rigorous benchmark based on TSLib. In this paper, we summarized the design principles of current time series models supported by insightful experiments, hoping to be helpful to future research.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2024.04) Many thanks for the great work from &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/pull/378&#34;&gt;frecklebars&lt;/a&gt;. The famous sequenctial model &lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;Mamba&lt;/a&gt; has been included in our library. See &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Mamba.py&#34;&gt;this file&lt;/a&gt;, where you need to install &lt;code&gt;mamba_ssm&lt;/code&gt; with pip at first.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2024.03) Given the inconsistent look-back length of various papers, we split the long-term forecasting in the leaderboard into two categories: Look-Back-96 and Look-Back-Searching. We recommend researchers read &lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;TimeMixer&lt;/a&gt;, which includes both look-back length settings in experiments for scientific rigor.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2023.10) We add an implementation to &lt;a href=&#34;https://arxiv.org/abs/2310.06625&#34;&gt;iTransformer&lt;/a&gt;, which is the state-of-the-art model for long-term forecasting. The official code and complete scripts of iTransformer can be found &lt;a href=&#34;https://github.com/thuml/iTransformer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2023.09) We added a detailed &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/tutorial/TimesNet_tutorial.ipynb&#34;&gt;tutorial&lt;/a&gt; for &lt;a href=&#34;https://openreview.net/pdf?id=ju_Uqw384Oq&#34;&gt;TimesNet&lt;/a&gt; and this library, which is quite friendly to beginners of deep time series analysis.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üö©&lt;/span&gt;&lt;strong&gt;News&lt;/strong&gt; (2023.02) We release the TSlib as a comprehensive benchmark and code base for time series models, which is extended from our previous GitHub repository &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Leaderboard for Time Series Analysis&lt;/h2&gt; &#xA;&lt;p&gt;Till March 2024, the top three models for five different tasks are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;br&gt;Ranking&lt;/th&gt; &#xA;   &lt;th&gt;Long-term&lt;br&gt;Forecasting&lt;br&gt;Look-Back-96&lt;/th&gt; &#xA;   &lt;th&gt;Long-term&lt;br&gt;Forecasting&lt;br&gt;Look-Back-Searching&lt;/th&gt; &#xA;   &lt;th&gt;Short-term&lt;br&gt;Forecasting&lt;/th&gt; &#xA;   &lt;th&gt;Imputation&lt;/th&gt; &#xA;   &lt;th&gt;Classification&lt;/th&gt; &#xA;   &lt;th&gt;Anomaly&lt;br&gt;Detection&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•á 1st&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.06625&#34;&gt;iTransformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;TimeMixer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•à 2nd&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;TimeMixer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yuqinie98/PatchTST&#34;&gt;PatchTST&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Nonstationary_Transformers&#34;&gt;Non-stationary&lt;br&gt;Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MAZiqing/FEDformer&#34;&gt;FEDformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ü•â 3rd&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;TimesNet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.13504.pdf&#34;&gt;DLinear&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MAZiqing/FEDformer&#34;&gt;FEDformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhouhaoyi/Informer2020&#34;&gt;Informer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;Autoformer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: We will keep updating this leaderboard.&lt;/strong&gt; If you have proposed advanced and awesome models, you can send us your paper/code link or raise a pull request. We will add them to this repo and update the leaderboard as soon as possible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compared models of this leaderboard.&lt;/strong&gt; ‚òë means that their codes have already been included in this repo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TimeMixer&lt;/strong&gt; - TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=7oLshfEIC2&#34;&gt;[ICLR 2024]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TimeMixer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TSMixer&lt;/strong&gt; - TSMixer: An All-MLP Architecture for Time Series Forecasting &lt;a href=&#34;https://arxiv.org/pdf/2303.06053.pdf&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TSMixer.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;iTransformer&lt;/strong&gt; - iTransformer: Inverted Transformers Are Effective for Time Series Forecasting &lt;a href=&#34;https://arxiv.org/abs/2310.06625&#34;&gt;[ICLR 2024]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/iTransformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;PatchTST&lt;/strong&gt; - A Time Series is Worth 64 Words: Long-term Forecasting with Transformers &lt;a href=&#34;https://openreview.net/pdf?id=Jbdc0vTOcol&#34;&gt;[ICLR 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/PatchTST.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TimesNet&lt;/strong&gt; - TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis &lt;a href=&#34;https://openreview.net/pdf?id=ju_Uqw384Oq&#34;&gt;[ICLR 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TimesNet.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;DLinear&lt;/strong&gt; - Are Transformers Effective for Time Series Forecasting? &lt;a href=&#34;https://arxiv.org/pdf/2205.13504.pdf&#34;&gt;[AAAI 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/DLinear.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;LightTS&lt;/strong&gt; - Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures &lt;a href=&#34;https://arxiv.org/abs/2207.01186&#34;&gt;[arXiv 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/LightTS.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;ETSformer&lt;/strong&gt; - ETSformer: Exponential Smoothing Transformers for Time-series Forecasting &lt;a href=&#34;https://arxiv.org/abs/2202.01381&#34;&gt;[arXiv 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/ETSformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Non-stationary Transformer&lt;/strong&gt; - Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=ucNDIDRNjjv&#34;&gt;[NeurIPS 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Nonstationary_Transformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FEDformer&lt;/strong&gt; - FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting &lt;a href=&#34;https://proceedings.mlr.press/v162/zhou22g.html&#34;&gt;[ICML 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FEDformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Pyraformer&lt;/strong&gt; - Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=0EXmFzUn5I&#34;&gt;[ICLR 2022]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Pyraformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Autoformer&lt;/strong&gt; - Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=I55UqU-M11y&#34;&gt;[NeurIPS 2021]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Autoformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Informer&lt;/strong&gt; - Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132&#34;&gt;[AAAI 2021]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Informer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Reformer&lt;/strong&gt; - Reformer: The Efficient Transformer &lt;a href=&#34;https://openreview.net/forum?id=rkgNKkHtvB&#34;&gt;[ICLR 2020]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Reformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Transformer&lt;/strong&gt; - Attention is All You Need &lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;[NeurIPS 2017]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Transformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See our latest paper &lt;a href=&#34;https://arxiv.org/abs/2210.02186&#34;&gt;[TimesNet]&lt;/a&gt; for the comprehensive benchmark. We will release a real-time updated online version soon.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Newly added baselines.&lt;/strong&gt; We will add them to the leaderboard after a comprehensive evaluation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Mamba&lt;/strong&gt; - Mamba: Linear-Time Sequence Modeling with Selective State Spaces &lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Mamba.py&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;SegRNN&lt;/strong&gt; - SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting &lt;a href=&#34;https://arxiv.org/abs/2308.11200.pdf&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/SegRNN.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Koopa&lt;/strong&gt; - Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors &lt;a href=&#34;https://arxiv.org/pdf/2305.18803.pdf&#34;&gt;[NeurIPS 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Koopa.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FreTS&lt;/strong&gt; - Frequency-domain MLPs are More Effective Learners in Time Series Forecasting &lt;a href=&#34;https://arxiv.org/pdf/2311.06184.pdf&#34;&gt;[NeurIPS 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FreTS.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TiDE&lt;/strong&gt; - Long-term Forecasting with TiDE: Time-series Dense Encoder &lt;a href=&#34;https://arxiv.org/pdf/2304.08424.pdf&#34;&gt;[arXiv 2023]&lt;/a&gt; &lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TiDE.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;FiLM&lt;/strong&gt; - FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting &lt;a href=&#34;https://openreview.net/forum?id=zTQdHSQUQWc&#34;&gt;[NeurIPS 2022]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/FiLM.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;MICN&lt;/strong&gt; - MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=zt53IDUR1U&#34;&gt;[ICLR 2023]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/MICN.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;Crossformer&lt;/strong&gt; - Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting &lt;a href=&#34;https://openreview.net/pdf?id=vSVLM2j9eie&#34;&gt;[ICLR 2023]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/Crossformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;TFT&lt;/strong&gt; - Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting &lt;a href=&#34;https://arxiv.org/abs/1912.09363&#34;&gt;[arXiv 2019]&lt;/a&gt;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/raw/main/models/TemporalFusionTransformer.py&#34;&gt;[Code]&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Python 3.8. For convenience, execute the following command.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Prepare Data. You can obtain the well pre-processed datasets from &lt;a href=&#34;https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing&#34;&gt;[Google Drive]&lt;/a&gt; or&amp;nbsp;&lt;a href=&#34;https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy&#34;&gt;[Baidu Drive]&lt;/a&gt;, Then place the downloaded data in the folder&lt;code&gt;./dataset&lt;/code&gt;. Here is a summary of supported datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/thuml/Time-Series-Library/main/.%5Cpic%5Cdataset.png&#34; height=&#34;200&#34; alt=&#34;&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Train and evaluate model. We provide the experiment scripts for all benchmarks under the folder &lt;code&gt;./scripts/&lt;/code&gt;. You can reproduce the experiment results as the following examples:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# long-term forecast&#xA;bash ./scripts/long_term_forecast/ETT_script/TimesNet_ETTh1.sh&#xA;# short-term forecast&#xA;bash ./scripts/short_term_forecast/TimesNet_M4.sh&#xA;# imputation&#xA;bash ./scripts/imputation/ETT_script/TimesNet_ETTh1.sh&#xA;# anomaly detection&#xA;bash ./scripts/anomaly_detection/PSM/TimesNet.sh&#xA;# classification&#xA;bash ./scripts/classification/TimesNet.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Develop your own model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add the model file to the folder &lt;code&gt;./models&lt;/code&gt;. You can follow the &lt;code&gt;./models/Transformer.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Include the newly added model in the &lt;code&gt;Exp_Basic.model_dict&lt;/code&gt; of &lt;code&gt;./exp/exp_basic.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create the corresponding scripts under the folder &lt;code&gt;./scripts&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: The original code for the classification task can be found &lt;a href=&#34;https://github.com/thuml/Flowformer/tree/main/Flowformer_TimeSeries&#34;&gt;here&lt;/a&gt;. It is hard to fuse all five tasks in one library. We are still working on this task.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repo useful, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{wu2023timesnet,&#xA;  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},&#xA;  author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},&#xA;  booktitle={International Conference on Learning Representations},&#xA;  year={2023},&#xA;}&#xA;&#xA;@article{wang2024tssurvey,&#xA;  title={Deep Time Series Models: A Comprehensive Survey and Benchmark},&#xA;  author={Yuxuan Wang and Haixu Wu and Jiaxiang Dong and Yong Liu and Mingsheng Long and Jianmin Wang},&#xA;  booktitle={arXiv preprint arXiv:2407.13278},&#xA;  year={2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, feel free to contact our maintenance team:&lt;/p&gt; &#xA;&lt;p&gt;Current:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Haixu Wu (Ph.D. student, &lt;a href=&#34;mailto:wuhx23@mails.tsinghua.edu.cn&#34;&gt;wuhx23@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Yong Liu (Ph.D. student, &lt;a href=&#34;mailto:liuyong21@mails.tsinghua.edu.cn&#34;&gt;liuyong21@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Yuxuan Wang (Ph.D. student, &lt;a href=&#34;mailto:wangyuxu22@mails.tsinghua.edu.cn&#34;&gt;wangyuxu22@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Huikun Weng (Undergraduate, &lt;a href=&#34;mailto:wenghk22@mails.tsinghua.edu.cn&#34;&gt;wenghk22@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Previous:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tengge Hu (Master student, &lt;a href=&#34;mailto:htg21@mails.tsinghua.edu.cn&#34;&gt;htg21@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Haoran Zhang (Master student, &lt;a href=&#34;mailto:z-hr20@mails.tsinghua.edu.cn&#34;&gt;z-hr20@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Jiawei Guo (Undergraduate, &lt;a href=&#34;mailto:guo-jw21@mails.tsinghua.edu.cn&#34;&gt;guo-jw21@mails.tsinghua.edu.cn&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Or describe it in Issues.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is supported by the National Key R&amp;amp;D Program of China (2021YFB1715200).&lt;/p&gt; &#xA;&lt;p&gt;This library is constructed based on the following repos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Forecasting: &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;https://github.com/thuml/Autoformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Anomaly Detection: &lt;a href=&#34;https://github.com/thuml/Anomaly-Transformer&#34;&gt;https://github.com/thuml/Anomaly-Transformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classification: &lt;a href=&#34;https://github.com/thuml/Flowformer&#34;&gt;https://github.com/thuml/Flowformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All the experiment datasets are public, and we obtain them from the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Long-term Forecasting and Imputation: &lt;a href=&#34;https://github.com/thuml/Autoformer&#34;&gt;https://github.com/thuml/Autoformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Short-term Forecasting: &lt;a href=&#34;https://github.com/ServiceNow/N-BEATS&#34;&gt;https://github.com/ServiceNow/N-BEATS&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Anomaly Detection: &lt;a href=&#34;https://github.com/thuml/Anomaly-Transformer&#34;&gt;https://github.com/thuml/Anomaly-Transformer&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Classification: &lt;a href=&#34;https://www.timeseriesclassification.com/&#34;&gt;https://www.timeseriesclassification.com/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;All Thanks To Our Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/thuml/Time-Series-Library/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=thuml/Time-Series-Library&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>YvanYin/Metric3D</title>
    <updated>2024-08-21T01:33:07Z</updated>
    <id>tag:github.com,2024-08-21:/YvanYin/Metric3D</id>
    <link href="https://github.com/YvanYin/Metric3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The repo for &#34;Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image&#34; and &#34;Metric3Dv2: A Versatile Monocular Geometric Foundation Model...&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄ Metric3D Project üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Official PyTorch implementation of Metric3Dv1 and Metric3Dv2:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://arxiv.org/abs/2307.10984&#34;&gt;Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/abs/2404.15506&#34;&gt;Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jugghm.github.io/Metric3Dv2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/project%20page-@Metric3D-yellow.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.10984&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-@Metric3Dv1-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.15506&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv-@Metric3Dv2-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/JUGGHM/Metric3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2?p=metric3d-v2-a-versatile-monocular-geometric-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/metric3d-v2-a-versatile-monocular-geometric-1/monocular-depth-estimation-on-nyu-depth-v2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/monocular-depth-estimation-on-kitti-eigen?p=metric3d-v2-a-versatile-monocular-geometric-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/metric3d-v2-a-versatile-monocular-geometric-1/monocular-depth-estimation-on-kitti-eigen&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/surface-normals-estimation-on-nyu-depth-v2-1?p=metric3d-v2-a-versatile-monocular-geometric-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/metric3d-v2-a-versatile-monocular-geometric-1/surface-normals-estimation-on-nyu-depth-v2-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/surface-normals-estimation-on-ibims-1?p=metric3d-v2-a-versatile-monocular-geometric-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/metric3d-v2-a-versatile-monocular-geometric-1/surface-normals-estimation-on-ibims-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/surface-normals-estimation-on-scannetv2?p=metric3d-v2-a-versatile-monocular-geometric-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/metric3d-v2-a-versatile-monocular-geometric-1/surface-normals-estimation-on-scannetv2&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üèÜ &lt;strong&gt;Champion in &lt;a href=&#34;https://jspenmar.github.io/MDEC&#34;&gt;CVPR2023 Monocular Depth Estimation Challenge&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/8]&lt;/code&gt; Metric3Dv2 is accepted by TPAMI!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/7/5]&lt;/code&gt; Our stable-diffusion alternative GeoWizard has now been accepted by ECCV 2024! Check NOW the &lt;a href=&#34;https://github.com/fuxiao0719/GeoWizard&#34;&gt;repository&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2403.12013&#34;&gt;paper&lt;/a&gt; for the finest-grained geometry ever! üéâüéâüéâ&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/6/25]&lt;/code&gt; Json files for KITTI datasets now available! Refer to &lt;a href=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/training/README.md&#34;&gt;Training&lt;/a&gt; for more details&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/6/3]&lt;/code&gt; ONNX is supported! We appreciate &lt;a href=&#34;https://github.com/xenova&#34;&gt;@xenova&lt;/a&gt; for their remarkable efforts!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/4/25]&lt;/code&gt; Weights for ViT-giant2 model released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/4/11]&lt;/code&gt; Training codes are released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/3/18]&lt;/code&gt; &lt;a href=&#34;https://huggingface.co/spaces/JUGGHM/Metric3D&#34;&gt;HuggingFace ü§ó&lt;/a&gt; GPU version updated!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/3/18]&lt;/code&gt; &lt;a href=&#34;https://jugghm.github.io/Metric3Dv2/&#34;&gt;Project page&lt;/a&gt; released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024/3/18]&lt;/code&gt; Metric3D V2 models released, supporting metric depth and surface normal now!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2023/8/10]&lt;/code&gt; Inference codes, pre-trained weights, and demo released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2023/7]&lt;/code&gt; Metric3D accepted by ICCV 2023!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2023/4]&lt;/code&gt; The Champion of &lt;a href=&#34;https://jspenmar.github.io/MDEC&#34;&gt;2nd Monocular Depth Estimation Challenge&lt;/a&gt; in CVPR 2023&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåº Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Metric3D is a strong and robust geometry foundation model for high-quality and zero-shot &lt;strong&gt;metric depth&lt;/strong&gt; and &lt;strong&gt;surface normal&lt;/strong&gt; estimation from a single image. It excels at solving in-the-wild scene reconstruction. It can directly help you measure the size of structures from a single image. Now it achieves SOTA performance on over 10 depth and normal benchmarks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/media/screenshots/depth_normal.jpg&#34; alt=&#34;depth_normal&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/media/screenshots/metrology.jpg&#34; alt=&#34;metrology&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìù Benchmarks&lt;/h2&gt; &#xA;&lt;h3&gt;Metric Depth&lt;/h3&gt; &#xA;&lt;p&gt;Our models rank 1st on the routing KITTI and NYU benchmarks.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Backbone&lt;/th&gt; &#xA;   &lt;th&gt;KITTI Œ¥1 ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;KITTI Œ¥2 ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;KITTI AbsRel ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;KITTI RMSE ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;KITTI RMS_log ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;NYU Œ¥1 ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;NYU Œ¥2 ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;NYU AbsRel ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;NYU RMSE ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;NYU log10 ‚Üì&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ZoeDepth&lt;/td&gt; &#xA;   &lt;td&gt;ViT-Large&lt;/td&gt; &#xA;   &lt;td&gt;0.971&lt;/td&gt; &#xA;   &lt;td&gt;0.995&lt;/td&gt; &#xA;   &lt;td&gt;0.053&lt;/td&gt; &#xA;   &lt;td&gt;2.281&lt;/td&gt; &#xA;   &lt;td&gt;0.082&lt;/td&gt; &#xA;   &lt;td&gt;0.953&lt;/td&gt; &#xA;   &lt;td&gt;0.995&lt;/td&gt; &#xA;   &lt;td&gt;0.077&lt;/td&gt; &#xA;   &lt;td&gt;0.277&lt;/td&gt; &#xA;   &lt;td&gt;0.033&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ZeroDepth&lt;/td&gt; &#xA;   &lt;td&gt;ResNet-18&lt;/td&gt; &#xA;   &lt;td&gt;0.968&lt;/td&gt; &#xA;   &lt;td&gt;0.996&lt;/td&gt; &#xA;   &lt;td&gt;0.057&lt;/td&gt; &#xA;   &lt;td&gt;2.087&lt;/td&gt; &#xA;   &lt;td&gt;0.083&lt;/td&gt; &#xA;   &lt;td&gt;0.954&lt;/td&gt; &#xA;   &lt;td&gt;0.995&lt;/td&gt; &#xA;   &lt;td&gt;0.074&lt;/td&gt; &#xA;   &lt;td&gt;0.269&lt;/td&gt; &#xA;   &lt;td&gt;0.103&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IEBins&lt;/td&gt; &#xA;   &lt;td&gt;SwinT-Large&lt;/td&gt; &#xA;   &lt;td&gt;0.978&lt;/td&gt; &#xA;   &lt;td&gt;0.998&lt;/td&gt; &#xA;   &lt;td&gt;0.050&lt;/td&gt; &#xA;   &lt;td&gt;2.011&lt;/td&gt; &#xA;   &lt;td&gt;0.075&lt;/td&gt; &#xA;   &lt;td&gt;0.936&lt;/td&gt; &#xA;   &lt;td&gt;0.992&lt;/td&gt; &#xA;   &lt;td&gt;0.087&lt;/td&gt; &#xA;   &lt;td&gt;0.314&lt;/td&gt; &#xA;   &lt;td&gt;0.031&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DepthAnything&lt;/td&gt; &#xA;   &lt;td&gt;ViT-Large&lt;/td&gt; &#xA;   &lt;td&gt;0.982&lt;/td&gt; &#xA;   &lt;td&gt;0.998&lt;/td&gt; &#xA;   &lt;td&gt;0.046&lt;/td&gt; &#xA;   &lt;td&gt;1.985&lt;/td&gt; &#xA;   &lt;td&gt;0.069&lt;/td&gt; &#xA;   &lt;td&gt;0.984&lt;/td&gt; &#xA;   &lt;td&gt;0.998&lt;/td&gt; &#xA;   &lt;td&gt;0.056&lt;/td&gt; &#xA;   &lt;td&gt;0.206&lt;/td&gt; &#xA;   &lt;td&gt;0.024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours&lt;/td&gt; &#xA;   &lt;td&gt;ViT-Large&lt;/td&gt; &#xA;   &lt;td&gt;0.985&lt;/td&gt; &#xA;   &lt;td&gt;0.998&lt;/td&gt; &#xA;   &lt;td&gt;0.044&lt;/td&gt; &#xA;   &lt;td&gt;1.985&lt;/td&gt; &#xA;   &lt;td&gt;0.064&lt;/td&gt; &#xA;   &lt;td&gt;0.989&lt;/td&gt; &#xA;   &lt;td&gt;0.998&lt;/td&gt; &#xA;   &lt;td&gt;0.047&lt;/td&gt; &#xA;   &lt;td&gt;0.183&lt;/td&gt; &#xA;   &lt;td&gt;0.020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours&lt;/td&gt; &#xA;   &lt;td&gt;ViT-giant2&lt;/td&gt; &#xA;   &lt;td&gt;0.989&lt;/td&gt; &#xA;   &lt;td&gt;0.998&lt;/td&gt; &#xA;   &lt;td&gt;0.039&lt;/td&gt; &#xA;   &lt;td&gt;1.766&lt;/td&gt; &#xA;   &lt;td&gt;0.060&lt;/td&gt; &#xA;   &lt;td&gt;0.987&lt;/td&gt; &#xA;   &lt;td&gt;0.997&lt;/td&gt; &#xA;   &lt;td&gt;0.045&lt;/td&gt; &#xA;   &lt;td&gt;0.187&lt;/td&gt; &#xA;   &lt;td&gt;0.015&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Affine-invariant Depth&lt;/h3&gt; &#xA;&lt;p&gt;Even compared to recent affine-invariant depth methods (Marigold and Depth Anything), our metric-depth (and normal) models still show superior performance.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;#Data for Pretrain and Train&lt;/th&gt; &#xA;   &lt;th&gt;KITTI Absrel ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;KITTI Œ¥1 ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;NYUv2 AbsRel ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;NYUv2 Œ¥1 ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;DIODE-Full AbsRel ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;DIODE-Full Œ¥1 ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;Eth3d AbsRel ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;Eth3d Œ¥1 ‚Üë&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OmniData (v2, ViT-L)&lt;/td&gt; &#xA;   &lt;td&gt;1.3M + 12.2M&lt;/td&gt; &#xA;   &lt;td&gt;0.069&lt;/td&gt; &#xA;   &lt;td&gt;0.948&lt;/td&gt; &#xA;   &lt;td&gt;0.074&lt;/td&gt; &#xA;   &lt;td&gt;0.945&lt;/td&gt; &#xA;   &lt;td&gt;0.149&lt;/td&gt; &#xA;   &lt;td&gt;0.835&lt;/td&gt; &#xA;   &lt;td&gt;0.166&lt;/td&gt; &#xA;   &lt;td&gt;0.778&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MariGold (LDMv2)&lt;/td&gt; &#xA;   &lt;td&gt;5B + 74K&lt;/td&gt; &#xA;   &lt;td&gt;0.099&lt;/td&gt; &#xA;   &lt;td&gt;0.916&lt;/td&gt; &#xA;   &lt;td&gt;0.055&lt;/td&gt; &#xA;   &lt;td&gt;0.961&lt;/td&gt; &#xA;   &lt;td&gt;0.308&lt;/td&gt; &#xA;   &lt;td&gt;0.773&lt;/td&gt; &#xA;   &lt;td&gt;0.127&lt;/td&gt; &#xA;   &lt;td&gt;0.960&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DepthAnything (ViT-L)&lt;/td&gt; &#xA;   &lt;td&gt;142M + 63M&lt;/td&gt; &#xA;   &lt;td&gt;0.076&lt;/td&gt; &#xA;   &lt;td&gt;0.947&lt;/td&gt; &#xA;   &lt;td&gt;0.043&lt;/td&gt; &#xA;   &lt;td&gt;0.981&lt;/td&gt; &#xA;   &lt;td&gt;0.277&lt;/td&gt; &#xA;   &lt;td&gt;0.759&lt;/td&gt; &#xA;   &lt;td&gt;0.065&lt;/td&gt; &#xA;   &lt;td&gt;0.882&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours (ViT-L)&lt;/td&gt; &#xA;   &lt;td&gt;142M + 16M&lt;/td&gt; &#xA;   &lt;td&gt;0.042&lt;/td&gt; &#xA;   &lt;td&gt;0.979&lt;/td&gt; &#xA;   &lt;td&gt;0.042&lt;/td&gt; &#xA;   &lt;td&gt;0.980&lt;/td&gt; &#xA;   &lt;td&gt;0.141&lt;/td&gt; &#xA;   &lt;td&gt;0.882&lt;/td&gt; &#xA;   &lt;td&gt;0.042&lt;/td&gt; &#xA;   &lt;td&gt;0.987&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours (ViT-g)&lt;/td&gt; &#xA;   &lt;td&gt;142M + 16M&lt;/td&gt; &#xA;   &lt;td&gt;0.043&lt;/td&gt; &#xA;   &lt;td&gt;0.982&lt;/td&gt; &#xA;   &lt;td&gt;0.043&lt;/td&gt; &#xA;   &lt;td&gt;0.981&lt;/td&gt; &#xA;   &lt;td&gt;0.136&lt;/td&gt; &#xA;   &lt;td&gt;0.895&lt;/td&gt; &#xA;   &lt;td&gt;0.042&lt;/td&gt; &#xA;   &lt;td&gt;0.983&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Surface Normal&lt;/h3&gt; &#xA;&lt;p&gt;Our models also show powerful performance on normal benchmarks.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;NYU 11.25¬∞ ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;NYU Mean ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;NYU RMS ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;ScanNet 11.25¬∞ ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;ScanNet Mean ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;ScanNet RMS ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;iBims 11.25¬∞ ‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;iBims Mean ‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;iBims RMS ‚Üì&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EESNU&lt;/td&gt; &#xA;   &lt;td&gt;0.597&lt;/td&gt; &#xA;   &lt;td&gt;16.0&lt;/td&gt; &#xA;   &lt;td&gt;24.7&lt;/td&gt; &#xA;   &lt;td&gt;0.711&lt;/td&gt; &#xA;   &lt;td&gt;11.8&lt;/td&gt; &#xA;   &lt;td&gt;20.3&lt;/td&gt; &#xA;   &lt;td&gt;0.585&lt;/td&gt; &#xA;   &lt;td&gt;20.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IronDepth&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;0.431&lt;/td&gt; &#xA;   &lt;td&gt;25.3&lt;/td&gt; &#xA;   &lt;td&gt;37.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PolyMax&lt;/td&gt; &#xA;   &lt;td&gt;0.656&lt;/td&gt; &#xA;   &lt;td&gt;13.1&lt;/td&gt; &#xA;   &lt;td&gt;20.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours (ViT-L)&lt;/td&gt; &#xA;   &lt;td&gt;0.688&lt;/td&gt; &#xA;   &lt;td&gt;12.0&lt;/td&gt; &#xA;   &lt;td&gt;19.2&lt;/td&gt; &#xA;   &lt;td&gt;0.760&lt;/td&gt; &#xA;   &lt;td&gt;9.9&lt;/td&gt; &#xA;   &lt;td&gt;16.4&lt;/td&gt; &#xA;   &lt;td&gt;0.694&lt;/td&gt; &#xA;   &lt;td&gt;19.4&lt;/td&gt; &#xA;   &lt;td&gt;34.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours (ViT-g)&lt;/td&gt; &#xA;   &lt;td&gt;0.662&lt;/td&gt; &#xA;   &lt;td&gt;13.2&lt;/td&gt; &#xA;   &lt;td&gt;20.2&lt;/td&gt; &#xA;   &lt;td&gt;0.778&lt;/td&gt; &#xA;   &lt;td&gt;9.2&lt;/td&gt; &#xA;   &lt;td&gt;15.3&lt;/td&gt; &#xA;   &lt;td&gt;0.697&lt;/td&gt; &#xA;   &lt;td&gt;19.6&lt;/td&gt; &#xA;   &lt;td&gt;35.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üåà DEMOs&lt;/h2&gt; &#xA;&lt;h3&gt;Zero-shot monocular metric depth &amp;amp; surface normal&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/media/gifs/demo_1.gif&#34; width=&#34;600&#34; height=&#34;337&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/media/gifs/demo_12.gif&#34; width=&#34;600&#34; height=&#34;337&#34;&gt; &#xA;&lt;h3&gt;Zero-shot metric 3D recovery&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/media/gifs/demo_2.gif&#34; width=&#34;600&#34; height=&#34;337&#34;&gt; &#xA;&lt;h3&gt;Improving monocular SLAM&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/media/gifs/demo_22.gif&#34; width=&#34;600&#34; height=&#34;337&#34;&gt; &#xA;&lt;h2&gt;üî® Installation&lt;/h2&gt; &#xA;&lt;h3&gt;One-line Installation&lt;/h3&gt; &#xA;&lt;p&gt;For the ViT models, use the following environmentÔºö&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_v2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ConvNeXt-L, it is&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements_v1.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;dataset annotation components&lt;/h3&gt; &#xA;&lt;p&gt;With off-the-shelf depth datasets, we need to generate json annotaions in compatible with this dataset, which is organized by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dict(&#xA;&#x9;&#39;files&#39;:list(&#xA;&#x9;&#x9;dict(&#xA;&#x9;&#x9;&#x9;&#39;rgb&#39;: &#39;data/kitti_demo/rgb/xxx.png&#39;,&#xA;&#x9;&#x9;&#x9;&#39;depth&#39;: &#39;data/kitti_demo/depth/xxx.png&#39;,&#xA;&#x9;&#x9;&#x9;&#39;depth_scale&#39;: 1000.0 # the depth scale of gt depth img.&#xA;&#x9;&#x9;&#x9;&#39;cam_in&#39;: [fx, fy, cx, cy],&#xA;&#x9;&#x9;),&#xA;&#xA;&#x9;&#x9;dict(&#xA;&#x9;&#x9;&#x9;...&#xA;&#x9;&#x9;),&#xA;&#xA;&#x9;&#x9;...&#xA;&#x9;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate such annotations, please refer to the &#34;Inference&#34; section.&lt;/p&gt; &#xA;&lt;h3&gt;configs&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;mono/configs&lt;/code&gt; we provide different config setups.&lt;/p&gt; &#xA;&lt;p&gt;Intrinsics of the canonical camera is set bellow:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    canonical_space = dict(&#xA;        img_size=(512, 960),&#xA;        focal_length=1000.0,&#xA;    ),&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where cx and cy is set to be half of the image size.&lt;/p&gt; &#xA;&lt;p&gt;Inference settings are defined as&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    depth_range=(0, 1),&#xA;    depth_normalize=(0.3, 150),&#xA;    crop_size = (512, 1088),&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where the images will be first resized as the &lt;code&gt;crop_size&lt;/code&gt; and then fed into the model.&lt;/p&gt; &#xA;&lt;h2&gt;‚úàÔ∏è Training&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/training/README.md&#34;&gt;training/README.md&lt;/a&gt;. Now we provide complete json files for KITTI fine-tuning.&lt;/p&gt; &#xA;&lt;h2&gt;‚úàÔ∏è Inference&lt;/h2&gt; &#xA;&lt;h3&gt;News: Improved ONNX support with dynamic shapes (Feature owned by &lt;a href=&#34;https://github.com/xenova&#34;&gt;@xenova&lt;/a&gt;. Appreciate for this outstanding contribution üö©üö©üö©)&lt;/h3&gt; &#xA;&lt;p&gt;Now the onnx supports are availble for all three models with varying shapes. Refer to &lt;a href=&#34;https://github.com/YvanYin/Metric3D/issues/117&#34;&gt;issue117&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Improved ONNX Checkpoints Available now&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Decoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v2-S-ONNX&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DINO2reg-ViT-Small&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RAFT-4iter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/onnx-community/metric3d-vit-small&#34;&gt;Download ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v2-L-ONNX&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DINO2reg-ViT-Large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RAFT-8iter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/onnx-community/metric3d-vit-large&#34;&gt;Download ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v2-g-ONNX&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DINO2reg-ViT-giant2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RAFT-8iter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/onnx-community/metric3d-vit-giant2&#34;&gt;Download ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;One additional &lt;a href=&#34;https://github.com/YvanYin/Metric3D/issues/143#issue-2444506808&#34;&gt;reminder&lt;/a&gt; for using these onnx models is reported by @norbertlink.&lt;/p&gt; &#xA;&lt;h3&gt;News: Pytorch Hub is supported&lt;/h3&gt; &#xA;&lt;p&gt;Now you can use Metric3D via Pytorch Hub with just few lines of code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;model = torch.hub.load(&#39;yvanyin/metric3d&#39;, &#39;metric3d_vit_small&#39;, pretrain=True)&#xA;pred_depth, confidence, output_dict = model.inference({&#39;input&#39;: rgb})&#xA;pred_normal = output_dict[&#39;prediction_normal&#39;][:, :3, :, :] # only available for Metric3Dv2 i.e., ViT models&#xA;normal_confidence = output_dict[&#39;prediction_normal&#39;][:, 3, :, :] # see https://arxiv.org/abs/2109.09881 for details&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Supported models: &lt;code&gt;metric3d_convnext_tiny&lt;/code&gt;, &lt;code&gt;metric3d_convnext_large&lt;/code&gt;, &lt;code&gt;metric3d_vit_small&lt;/code&gt;, &lt;code&gt;metric3d_vit_large&lt;/code&gt;, &lt;code&gt;metric3d_vit_giant2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provided a minimal working example in &lt;a href=&#34;https://github.com/YvanYin/Metric3D/raw/main/hubconf.py#L145&#34;&gt;hubconf.py&lt;/a&gt;, which hopefully makes everything clearer.&lt;/p&gt; &#xA;&lt;h3&gt;News: ONNX Exportation and Inference are supported&lt;/h3&gt; &#xA;&lt;p&gt;We also provided a flexible working example in &lt;a href=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/onnx/metric3d_onnx_export.py&#34;&gt;metric3d_onnx_export.py&lt;/a&gt; to export the Pytorch Hub model to ONNX format. We could test with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Export the model to ONNX model&#xA;python3 onnx/metric_3d_onnx_export.py metric3d_vit_small # metric3d_vit_large/metric3d_convnext_large&#xA;&#xA;# Test the inference of the ONNX model&#xA;python3 onnx/test_onnx.py metric3d_vit_small.onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Owen-Liuyuxuan/ros2_vision_inference&#34;&gt;ros2_vision_inference&lt;/a&gt; provides a Python example, showcasing a pipeline from image to point clouds and integrated into ROS2 systems.&lt;/p&gt; &#xA;&lt;h3&gt;Download Checkpoint&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Decoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v1-T&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-Tiny&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hourglass-Decoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/JUGGHM/Metric3D/blob/main/convtiny_hourglass_v1.pth&#34;&gt;Download ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v1-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ConvNeXt-Large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hourglass-Decoder&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1KVINiBkVpJylx_6z1lAC7CQ4kmn-RJRN/view?usp=drive_link&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v2-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DINO2reg-ViT-Small&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RAFT-4iter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1YfmvXwpWmhLg3jSxnhT7LvY0yawlXcr_/view?usp=drive_link&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v2-L&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DINO2reg-ViT-Large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RAFT-8iter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1eT2gG-kwsVzNy5nJrbm4KC-9DbNKyLnr/view?usp=drive_link&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;v2-g&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DINO2reg-ViT-giant2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;RAFT-8iter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/JUGGHM/Metric3D/blob/main/metric_depth_vit_giant2_800k.pth&#34;&gt;Download ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Dataset Mode&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;put the trained ckpt file &lt;code&gt;model.pth&lt;/code&gt; in &lt;code&gt;weight/&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;generate data annotation by following the code &lt;code&gt;data/gene_annos_kitti_demo.py&lt;/code&gt;, which includes &#39;rgb&#39;, (optional) &#39;intrinsic&#39;, (optional) &#39;depth&#39;, (optional) &#39;depth_scale&#39;.&lt;/li&gt; &#xA; &lt;li&gt;change the &#39;test_data_path&#39; in &lt;code&gt;test_*.sh&lt;/code&gt; to the &lt;code&gt;*.json&lt;/code&gt; path.&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;source test_kitti.sh&lt;/code&gt; or &lt;code&gt;source test_nyu.sh&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;In-the-Wild Mode&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;put the trained ckpt file &lt;code&gt;model.pth&lt;/code&gt; in &lt;code&gt;weight/&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;change the &#39;test_data_path&#39; in &lt;code&gt;test.sh&lt;/code&gt; to the image folder path.&lt;/li&gt; &#xA; &lt;li&gt;run &lt;code&gt;source test_vit.sh&lt;/code&gt; for transformers and &lt;code&gt;source test.sh&lt;/code&gt; for convnets. As no intrinsics are provided, we provided by default 9 settings of focal length.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Metric3D and Droid-Slam&lt;/h3&gt; &#xA;&lt;p&gt;If you are interested in combining metric3D and monocular visual slam system to achieve the metric slam, you can refer to this &lt;a href=&#34;https://github.com/Jianxff/droid_metric&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚ùì Q &amp;amp; A&lt;/h2&gt; &#xA;&lt;h3&gt;Q1: Why depth maps look good but pointclouds are distorted?&lt;/h3&gt; &#xA;&lt;p&gt;Because the focal length is not properly set! Please find a proper focal length by modifying codes &lt;a href=&#34;https://raw.githubusercontent.com/YvanYin/Metric3D/main/mono/utils/do_test.py#309&#34;&gt;here&lt;/a&gt; yourself.&lt;/p&gt; &#xA;&lt;h3&gt;Q2: Why the pointclouds are too slow to be generated?&lt;/h3&gt; &#xA;&lt;p&gt;Because the images are too large! Use smaller ones instead.&lt;/p&gt; &#xA;&lt;h3&gt;Q3: Why predicted depth maps are not satisfactory?&lt;/h3&gt; &#xA;&lt;p&gt;First be sure all black padding regions at image boundaries are cropped out. Then please try again. Besides, metric 3D is not almighty. Some objects (chandeliers, drones...) / camera views (aerial view, bev...) do not occur frequently in the training datasets. We will going deeper into this and release more powerful solutions.&lt;/p&gt; &#xA;&lt;h2&gt;üìß Citation&lt;/h2&gt; &#xA;&lt;!-- ```&#xA;@article{hu2024metric3dv2,&#xA;  title={Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation},&#xA;  author={Hu, Mu and Yin, Wei and Zhang, Chi and Cai, Zhipeng and Long, Xiaoxiao and Chen, Hao and Wang, Kaixuan and Yu, Gang and Shen, Chunhua and Shen, Shaojie},&#xA;  journal={arXiv preprint arXiv:2404.15506},&#xA;  year={2024}&#xA;}&#xA;``` --&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{hu2024metric3d,&#xA;  title={Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation},&#xA;  author={Hu, Mu and Yin, Wei and Zhang, Chi and Cai, Zhipeng and Long, Xiaoxiao and Chen, Hao and Wang, Kaixuan and Yu, Gang and Shen, Chunhua and Shen, Shaojie},&#xA;  journal={arXiv preprint arXiv:2404.15506},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{yin2023metric,&#xA;  title={Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image},&#xA;  author={Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, Chunhua Shen},&#xA;  booktitle={ICCV},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License and Contact&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;em&gt;Metric 3D&lt;/em&gt; code is under a 2-clause BSD License. For further commercial inquiries, please contact Dr. Wei Yin [&lt;a href=&#34;mailto:yvanwy@outlook.com&#34;&gt;yvanwy@outlook.com&lt;/a&gt;] and Mr. Mu Hu [&lt;a href=&#34;mailto:mhuam@connect.ust.hk&#34;&gt;mhuam@connect.ust.hk&lt;/a&gt;].&lt;/p&gt;</summary>
  </entry>
</feed>