<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-28T01:39:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>spla-tam/SplaTAM</title>
    <updated>2024-01-28T01:39:50Z</updated>
    <id>tag:github.com,2024-01-28:/spla-tam/SplaTAM</id>
    <link href="https://github.com/spla-tam/SplaTAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;SplaTAM: Splat, Track &amp;amp; Map 3D Gaussians for Dense RGB-D SLAM&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://nik-v9.github.io/&#34;&gt;&lt;strong&gt;Nikhil Keetha&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://jaykarhade.github.io/&#34;&gt;&lt;strong&gt;Jay Karhade&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://krrish94.github.io/&#34;&gt;&lt;strong&gt;Krishna Murthy Jatavallabhula&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://gengshan-y.github.io/&#34;&gt;&lt;strong&gt;Gengshan Yang&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://theairlab.org/team/sebastian/&#34;&gt;&lt;strong&gt;Sebastian Scherer&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://www.cs.cmu.edu/~deva/&#34;&gt;&lt;strong&gt;Deva Ramanan&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://www.vision.rwth-aachen.de/person/216/&#34;&gt;&lt;strong&gt;Jonathon Luiten&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.02126.pdf&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/jWLI-OFp3qU&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://spla-tam.github.io/&#34;&gt;Project Page&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/assets/1.gif&#34; alt=&#34;Logo&#34; width=&#34;100%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Coming Soon: Stay Tuned for Faster, Better and Stronger SplaTAM V2 Update!&lt;/h2&gt; &#xA;&lt;!-- TABLE OF CONTENTS --&gt; &#xA;&lt;details open style=&#34;padding: 10px; border-radius:5px 30px 30px 5px; border-style: solid; border-width: 1px;&#34;&gt; &#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#installation&#34;&gt;Installation&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#demo&#34;&gt;Online Demo&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#usage&#34;&gt;Usage&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#downloads&#34;&gt;Downloads&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#benchmarking&#34;&gt;Benchmarking&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#citation&#34;&gt;Citation&lt;/a&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;a href=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/#developers&#34;&gt;Developers&lt;/a&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h5&gt;(Recommended)&lt;/h5&gt; &#xA;&lt;p&gt;SplaTAM has been tested on python 3.10, CUDA&amp;gt;=11.6. The simplest way to install all dependences is to use &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;anaconda&lt;/a&gt; and &lt;a href=&#34;https://pypi.org/project/pip/&#34;&gt;pip&lt;/a&gt; in the following steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n splatam python=3.10&#xA;conda activate splatam&#xA;conda install -c &#34;nvidia/label/cuda-11.6.0&#34; cuda-toolkit&#xA;conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.6 -c pytorch -c conda-forge&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, we also provide a conda environment.yml file :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;conda activate splatam&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;For installation on Windows using Git bash, please refer to the &lt;a href=&#34;https://github.com/spla-tam/SplaTAM/issues/9#issuecomment-1848348403&#34;&gt;instructions shared in Issue#9&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Docker and Singularity Setup&lt;/h4&gt; &#xA;&lt;p&gt;We also provide a docker image. We recommend using a venv to run the code inside a docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull nkeetha/splatam:v1&#xA;bash bash_scripts/docker_start.bash&#xA;cd /SplaTAM/&#xA;pip install virtualenv --user&#xA;mkdir venv&#xA;cd venv&#xA;virtualenv --system-site-packages splatam&#xA;source ./splatam/bin/activate&#xA;pip install -r venv_requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Setting up a singularity container is similar:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd &amp;lt;/path/to/singularity/folder/&amp;gt;&#xA;singularity pull splatam.sif docker://nkeetha/splatam:v1&#xA;singularity instance start --nv splatam.sif splatam&#xA;singularity run --nv instance://splatam&#xA;cd &amp;lt;path/to/SplaTAM/&amp;gt;&#xA;pip install virtualenv --user&#xA;mkdir venv&#xA;cd venv&#xA;virtualenv --system-site-packages splatam&#xA;source ./splatam/bin/activate&#xA;pip install -r venv_requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Online&lt;/h3&gt; &#xA;&lt;p&gt;You can SplaTAM your own environment with an iPhone or LiDAR-equipped Apple device by downloading and using the &lt;a href=&#34;https://apps.apple.com/au/app/nerfcapture/id6446518379&#34;&gt;NeRFCapture&lt;/a&gt; app.&lt;/p&gt; &#xA;&lt;p&gt;Make sure that your iPhone and PC are connected to the same WiFi network, and then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash bash_scripts/online_demo.bash configs/iphone/online_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On the app, keep clicking send for successive frames. Once the capturing of frames is done, the app will disconnect from the PC and check out SplaTAM&#39;s interactive rendering of the reconstruction on your PC! Here are some cool example results:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/spla-tam/SplaTAM/main/assets/collage.gif&#34; alt=&#34;Logo&#34; width=&#34;75%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Offline&lt;/h3&gt; &#xA;&lt;p&gt;You can also first capture the dataset and then run SplaTAM offline on the dataset with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash bash_scripts/nerfcapture.bash configs/iphone/nerfcapture.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dataset Collection&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to only capture your own iPhone dataset using the NeRFCapture app, please use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash bash_scripts/nerfcapture2dataset.bash configs/iphone/dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We will use the iPhone dataset as an example to show how to use SplaTAM. The following steps are similar for other datasets.&lt;/p&gt; &#xA;&lt;p&gt;To run SplaTAM, please use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/splatam.py configs/iphone/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To visualize the final interactive SplaTAM reconstruction, please use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python viz_scripts/final_recon.py configs/iphone/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To visualize the SplaTAM reconstruction in an online fashion, please use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python viz_scripts/online_recon.py configs/iphone/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run 3D Gaussian Splatting on the SplaTAM reconstruction, please use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/post_splatam_opt.py configs/iphone/post_splatam_opt.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run 3D Gaussian Splatting on a dataset using ground truth poses, please use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/gaussian_splatting.py configs/iphone/gaussian_splatting.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Downloads&lt;/h2&gt; &#xA;&lt;p&gt;DATAROOT is &lt;code&gt;./data&lt;/code&gt; by default. Please change the &lt;code&gt;input_folder&lt;/code&gt; path in the scene-specific config files if datasets are stored somewhere else on your machine.&lt;/p&gt; &#xA;&lt;h3&gt;Replica&lt;/h3&gt; &#xA;&lt;p&gt;Download the data as below, and the data is saved into the &lt;code&gt;./data/Replica&lt;/code&gt; folder. Note that the Replica data is generated by the authors of iMAP (but hosted by the authors of NICE-SLAM). Please cite iMAP if you use the data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash bash_scripts/download_replica.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TUM-RGBD&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash bash_scripts/download_tum.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ScanNet&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the data downloading procedure on the &lt;a href=&#34;http://www.scan-net.org/&#34;&gt;ScanNet&lt;/a&gt; website, and extract color/depth frames from the &lt;code&gt;.sens&lt;/code&gt; file using this &lt;a href=&#34;https://github.com/ScanNet/ScanNet/raw/master/SensReader/python/reader.py&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Directory structure of ScanNet (click to expand)]&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;  DATAROOT&#xA;  └── scannet&#xA;        └── scene0000_00&#xA;            └── frames&#xA;                ├── color&#xA;                │   ├── 0.jpg&#xA;                │   ├── 1.jpg&#xA;                │   ├── ...&#xA;                │   └── ...&#xA;                ├── depth&#xA;                │   ├── 0.png&#xA;                │   ├── 1.png&#xA;                │   ├── ...&#xA;                │   └── ...&#xA;                ├── intrinsic&#xA;                └── pose&#xA;                    ├── 0.txt&#xA;                    ├── 1.txt&#xA;                    ├── ...&#xA;                    └── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;We use the following sequences:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;scene0000_00&#xA;scene0059_00&#xA;scene0106_00&#xA;scene0181_00&#xA;scene0207_00&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ScanNet++&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the data downloading and image undistortion procedure on the &lt;a href=&#34;https://kaldir.vc.in.tum.de/scannetpp/&#34;&gt;ScanNet++&lt;/a&gt; website. Additionally for undistorting the DSLR depth images, we use our &lt;a href=&#34;https://github.com/Nik-V9/scannetpp&#34;&gt;own variant of the official ScanNet++ processing code&lt;/a&gt;. We will open a pull request to the official ScanNet++ repository soon.&lt;/p&gt; &#xA;&lt;p&gt;We use the following sequences:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;8b5caf3398&#xA;b20a261fdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For b20a261fdf, we use the first 360 frames, due to an abrupt jump/teleportation in the trajectory post frame 360. Please note that ScanNet++ was primarily intended as a NeRF Training &amp;amp; Novel View Synthesis dataset.&lt;/p&gt; &#xA;&lt;h3&gt;Replica-V2&lt;/h3&gt; &#xA;&lt;p&gt;We use the Replica-V2 dataset from vMAP to evaluate novel view synthesis. Please download the pre-generated replica sequences from &lt;a href=&#34;https://github.com/kxhit/vMAP&#34;&gt;vMAP&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarking&lt;/h2&gt; &#xA;&lt;p&gt;For running SplaTAM, we recommend using &lt;a href=&#34;https://wandb.ai/&#34;&gt;weights and biases&lt;/a&gt; for the logging. This can be turned on by setting the &lt;code&gt;wandb&lt;/code&gt; flag to True in the configs file. Also make sure to specify the path &lt;code&gt;wandb_folder&lt;/code&gt;. If you don&#39;t have a wandb account, first create one. Please make sure to change the &lt;code&gt;entity&lt;/code&gt; config to your wandb account. Each scene has a config folder, where the &lt;code&gt;input_folder&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; paths need to be specified.&lt;/p&gt; &#xA;&lt;p&gt;Below, we show some example run commands for one scene from each dataset. After SLAM, the trajectory error will be evaluated along with the rendering metrics. The results will be saved to &lt;code&gt;./experiments&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;h3&gt;Replica&lt;/h3&gt; &#xA;&lt;p&gt;To run SplaTAM on the &lt;code&gt;room0&lt;/code&gt; scene, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/splatam.py configs/replica/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run SplaTAM-S on the &lt;code&gt;room0&lt;/code&gt; scene, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/splatam.py configs/replica/splatam_s.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other scenes, please modify the &lt;code&gt;configs/replica/splatam.py&lt;/code&gt; file or use &lt;code&gt;configs/replica/replica.bash&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;TUM-RGBD&lt;/h3&gt; &#xA;&lt;p&gt;To run SplaTAM on the &lt;code&gt;freiburg1_desk&lt;/code&gt; scene, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/splatam.py configs/tum/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other scenes, please modify the &lt;code&gt;configs/tum/splatam.py&lt;/code&gt; file or use &lt;code&gt;configs/tum/tum.bash&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ScanNet&lt;/h3&gt; &#xA;&lt;p&gt;To run SplaTAM on the &lt;code&gt;scene0000_00&lt;/code&gt; scene, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/splatam.py configs/scannet/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other scenes, please modify the &lt;code&gt;configs/scannet/splatam.py&lt;/code&gt; file or use &lt;code&gt;configs/scannet/scannet.bash&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ScanNet++&lt;/h3&gt; &#xA;&lt;p&gt;To run SplaTAM on the &lt;code&gt;8b5caf3398&lt;/code&gt; scene, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/splatam.py configs/scannetpp/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run Novel View Synthesis on the &lt;code&gt;8b5caf3398&lt;/code&gt; scene, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/eval_novel_view.py configs/scannetpp/eval_novel_view.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other scenes, please modify the &lt;code&gt;configs/scannetpp/splatam.py&lt;/code&gt; file or use &lt;code&gt;configs/scannetpp/scannetpp.bash&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ReplicaV2&lt;/h3&gt; &#xA;&lt;p&gt;To run SplaTAM on the &lt;code&gt;room0&lt;/code&gt; scene, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/splatam.py configs/replica_v2/splatam.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run Novel View Synthesis on the &lt;code&gt;room0&lt;/code&gt; scene post SplaTAM, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/eval_novel_view.py configs/replica_v2/eval_novel_view.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other scenes, please modify the config files.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank the authors of the following repositories for their open-source code:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;3D Gaussians &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/JonathonLuiten/Dynamic3DGaussians&#34;&gt;Dynamic 3D Gaussians&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;3D Gaussian Splating&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Dataloaders &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/gradslam/gradslam/tree/conceptfusion&#34;&gt;GradSLAM &amp;amp; ConceptFusion&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Baselines &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/cvg/nice-slam&#34;&gt;Nice-SLAM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/eriksandstroem/Point-SLAM&#34;&gt;Point-SLAM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful, please cite us:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{keetha2023splatam,&#xA;    author    = {Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan, Deva and Luiten, Jonathan}&#xA;    title     = {SplaTAM: Splat, Track &amp;amp; Map 3D Gaussians for Dense RGB-D SLAM},&#xA;    journal   = {arXiv},&#xA;    year      = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Developers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Nik-V9&#34;&gt;Nik-V9&lt;/a&gt; (&lt;a href=&#34;https://nik-v9.github.io/&#34;&gt;Nikhil Keetha&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JayKarhade&#34;&gt;JayKarhade&lt;/a&gt; (&lt;a href=&#34;https://jaykarhade.github.io/&#34;&gt;Jay Karhade&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JonathonLuiten&#34;&gt;JonathonLuiten&lt;/a&gt; (&lt;a href=&#34;https://www.vision.rwth-aachen.de/person/216/&#34;&gt;Jonathan Luiten&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/krrish94&#34;&gt;krrish94&lt;/a&gt; (&lt;a href=&#34;https://krrish94.github.io/&#34;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gengshan-y&#34;&gt;gengshan-y&lt;/a&gt; (&lt;a href=&#34;https://gengshan-y.github.io/&#34;&gt;Gengshan Yang&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>state-spaces/mamba</title>
    <updated>2024-01-28T01:39:50Z</updated>
    <id>tag:github.com,2024-01-28:/state-spaces/mamba</id>
    <link href="https://github.com/state-spaces/mamba" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mamba&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/state-spaces/mamba/main/assets/selection.png&#34; alt=&#34;Mamba&#34; title=&#34;Selective State Space&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Mamba: Linear-Time Sequence Modeling with Selective State Spaces&lt;/strong&gt;&lt;br&gt; Albert Gu*, Tri Dao*&lt;br&gt; Paper: &lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;https://arxiv.org/abs/2312.00752&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling, where previous subquadratic models fall short of Transformers. It is based on the line of progress on &lt;a href=&#34;https://github.com/state-spaces/s4&#34;&gt;structured state space models&lt;/a&gt;, with an efficient hardware-aware design and implementation in the spirit of &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install causal-conv1d&amp;gt;=1.1.0&lt;/code&gt;: an efficient implementation of a simple causal Conv1d layer used inside the Mamba block.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip install mamba-ssm&lt;/code&gt;: the core Mamba package.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It can also be built from source with &lt;code&gt;pip install .&lt;/code&gt; from this repository.&lt;/p&gt; &#xA;&lt;p&gt;If &lt;code&gt;pip&lt;/code&gt; complains about PyTorch versions, try passing &lt;code&gt;--no-build-isolation&lt;/code&gt; to &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Other requirements:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU&lt;/li&gt; &#xA; &lt;li&gt;PyTorch 1.12+&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.6+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;We expose several levels of interface with the Mamba model.&lt;/p&gt; &#xA;&lt;h3&gt;Selective SSM&lt;/h3&gt; &#xA;&lt;p&gt;Mamba is based on a selective SSM layer, which is the focus of the paper (Section 3; Algorithm 2).&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/mamba/main/mamba_ssm/ops/selective_scan_interface.py&#34;&gt;ops/selective_scan_interface.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Mamba Block&lt;/h3&gt; &#xA;&lt;p&gt;The main module of this repository is the Mamba architecture block wrapping the selective SSM.&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/mamba/main/mamba_ssm/modules/mamba_simple.py&#34;&gt;modules/mamba_simple.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;from mamba_ssm import Mamba&#xA;&#xA;batch, length, dim = 2, 64, 16&#xA;x = torch.randn(batch, length, dim).to(&#34;cuda&#34;)&#xA;model = Mamba(&#xA;    # This module uses roughly 3 * expand * d_model^2 parameters&#xA;    d_model=dim, # Model dimension d_model&#xA;    d_state=16,  # SSM state expansion factor&#xA;    d_conv=4,    # Local convolution width&#xA;    expand=2,    # Block expansion factor&#xA;).to(&#34;cuda&#34;)&#xA;y = model(x)&#xA;assert y.shape == x.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mamba Language Model&lt;/h3&gt; &#xA;&lt;p&gt;Finally, we provide an example of a complete language model: a deep sequence model backbone (with repeating Mamba blocks) + language model head.&lt;/p&gt; &#xA;&lt;p&gt;Source: &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/mamba/main/mamba_ssm/models/mixer_seq_simple.py&#34;&gt;models/mixer_seq_simple.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is an example of how to integrate Mamba into an end-to-end neural network. This example is used in the generation scripts below.&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;Pretrained models are uploaded to &lt;a href=&#34;https://huggingface.co/state-spaces&#34;&gt;Hugging Face&lt;/a&gt;: &lt;code&gt;mamba-130m&lt;/code&gt;, &lt;code&gt;mamba-370m&lt;/code&gt;, &lt;code&gt;mamba-790m&lt;/code&gt;, &lt;code&gt;mamba-1.4b&lt;/code&gt;, &lt;code&gt;mamba-2.8b&lt;/code&gt;, trained on 300B tokens on the Pile, as well as &lt;code&gt;mamba-2.8b-slimpj&lt;/code&gt; (trained on 600B tokens on the SlimPajama dataset).&lt;/p&gt; &#xA;&lt;p&gt;The models will be autodownloaded by the generation script below.&lt;/p&gt; &#xA;&lt;p&gt;These models were trained on the &lt;a href=&#34;https://huggingface.co/datasets/EleutherAI/pile&#34;&gt;Pile&lt;/a&gt;, and follow the standard model dimensions described by GPT-3 and followed by many open source models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameters&lt;/th&gt; &#xA;   &lt;th&gt;Layers&lt;/th&gt; &#xA;   &lt;th&gt;Model dim.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;130M&lt;/td&gt; &#xA;   &lt;td&gt;24&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;370M&lt;/td&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;790M&lt;/td&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;   &lt;td&gt;1536&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.4B&lt;/td&gt; &#xA;   &lt;td&gt;48&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.8B&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;   &lt;td&gt;2560&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;(The layer count of Mamba doubles that of a Transformer with similar size, as two Mamba blocks are needed for each &#34;layer&#34; (MHA block + MLP block) of a Transformer.)&lt;/p&gt; &#xA;&lt;p&gt;Note: these are base models trained only for 300B tokens, without any form of downstream modification (instruction tuning, etc.). Performance is expected to be comparable or better than other architectures trained on similar data, but not to match larger or fine-tuned models.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;To run zero-shot evaluations of models (corresponding to Table 3 of the paper), we use the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor&#34;&gt;lm-evaluation-harness&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pull the &lt;code&gt;lm-evaluation-harness&lt;/code&gt; repo by &lt;code&gt;git submodule update --init --recursive&lt;/code&gt;. We use the &lt;code&gt;big-refactor&lt;/code&gt; branch.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;lm-evaluation-harness&lt;/code&gt;: &lt;code&gt;pip install -e 3rdparty/lm-evaluation-harness&lt;/code&gt;. On Python 3.10 you might need to manually install the latest version of &lt;code&gt;promptsource&lt;/code&gt;: &lt;code&gt;pip install git+https://github.com/bigscience-workshop/promptsource.git&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run evaluation with (more documentation at the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor&#34;&gt;lm-evaluation-harness&lt;/a&gt; repo):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-130m --tasks lambada_openai,hellaswag,piqa,arc_easy,arc_challenge,winogrande --device cuda --batch_size 64&#xA;python evals/lm_harness_eval.py --model hf --model_args pretrained=EleutherAI/pythia-160m --tasks lambada_openai,hellaswag,piqa,arc_easy,arc_challenge,winogrande --device cuda --batch_size 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reproduce the results on the &lt;code&gt;mamba-2.8b-slimpj&lt;/code&gt; model reported in the blogposts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-2.8b-slimpj --tasks boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa,race,truthfulqa_mc2 --device cuda --batch_size 64&#xA;python evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-2.8b-slimpj --tasks mmlu --num_fewshot 5 --device cuda --batch_size 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the result of each task might differ from reported values by 0.1-0.3 due to noise in the evaluation process.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;The script &lt;a href=&#34;https://raw.githubusercontent.com/state-spaces/mamba/main/benchmarks/benchmark_generation_mamba_simple.py&#34;&gt;benchmarks/benchmark_generation_mamba_simple.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;autoloads a model from the Hugging Face Hub,&lt;/li&gt; &#xA; &lt;li&gt;generates completions of a user-specified prompt,&lt;/li&gt; &#xA; &lt;li&gt;benchmarks the inference speed of this generation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Other configurable options include the top-p (nucleus sampling) probability, and the softmax temperature.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;To test generation latency (e.g. batch size = 1) with different sampling strategies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmarks/benchmark_generation_mamba_simple.py --model-name &#34;state-spaces/mamba-2.8b&#34; --prompt &#34;My cat wrote all this CUDA code for a new language model and&#34; --topp 0.9 --temperature 0.7 --repetition-penalty 1.2&#xA;python benchmarks/benchmark_generation_mamba_simple.py --model-name &#34;EleutherAI/pythia-2.8b&#34; --prompt &#34;My cat wrote all this CUDA code for a new language model and&#34; --topp 0.9 --temperature 0.7 --repetition-penalty 1.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test generation throughput with random prompts (e.g. large batch size):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmarks/benchmark_generation_mamba_simple.py --model-name &#34;state-spaces/mamba-2.8b&#34; --batch 128&#xA;python benchmarks/benchmark_generation_mamba_simple.py --model-name &#34;EleutherAI/pythia-2.8b&#34; --batch 128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;h3&gt;Precision&lt;/h3&gt; &#xA;&lt;p&gt;Our models were trained using PyTorch &lt;a href=&#34;https://pytorch.org/docs/stable/amp.html&#34;&gt;AMP&lt;/a&gt; for mixed precision. AMP keeps model parameters in float32 and casts to half precision when necessary. On the other hand, other frameworks like DeepSpeed store parameters in float16 and upcasts when necessary (e.g. for optimizer accumulation).&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve observed that higher precision for the main model parameters may be necessary, because SSMs are sensitive to their recurrent dynamics. If you are experiencing instabilities, as a first step please try a framework storing parameters in fp32 (such as AMP).&lt;/p&gt; &#xA;&lt;h3&gt;Initialization&lt;/h3&gt; &#xA;&lt;p&gt;Some parts of the model have initializations inherited from prior work on S4 models. For &lt;a href=&#34;https://github.com/state-spaces/mamba/raw/f0affcf69f06d1d06cef018ff640bf080a11c421/mamba_ssm/modules/mamba_simple.py#L102&#34;&gt;example&lt;/a&gt;, the $\Delta$ parameter has a targeted range by initializing the bias of its linear projection. However, some frameworks may have post-initialization hooks (e.g. setting all bias terms in &lt;code&gt;nn.Linear&lt;/code&gt; modules to zero). If this is the case, you may have to add custom logic (e.g. this &lt;a href=&#34;https://github.com/state-spaces/mamba/raw/f0affcf69f06d1d06cef018ff640bf080a11c421/mamba_ssm/modules/mamba_simple.py#L104&#34;&gt;line&lt;/a&gt; turns off re-initializing in our trainer, but would be a no-op in any other framework) that is specific to the training framework.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite Mamba:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{mamba,&#xA;  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},&#xA;  author={Gu, Albert and Dao, Tri},&#xA;  journal={arXiv preprint arXiv:2312.00752},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>TaskingAI/TaskingAI</title>
    <updated>2024-01-28T01:39:50Z</updated>
    <id>tag:github.com,2024-01-28:/TaskingAI/TaskingAI</id>
    <link href="https://github.com/TaskingAI/TaskingAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The open source platform for AI-native application development.&lt;/p&gt;&lt;hr&gt;&lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TaskingAI/TaskingAI/master/static/img/logo.png&#34; alt=&#34;https://www.tasking.ai&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;TaskingAI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.tasking.ai&#34;&gt;TaskingAI&lt;/a&gt; brings Firebase&#39;s simplicity to &lt;strong&gt;AI-native app development&lt;/strong&gt;. The platform enables the creation of GPTs-like multi-tenant applications using a wide range of LLMs from various providers. It features distinct, modular functions such as Inference, Retrieval, Assistant, and Tool, seamlessly integrated to enhance the development process. TaskingAI’s cohesive design ensures an efficient, intelligent, and user-friendly experience in AI application development.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;All-In-One LLM Platform&lt;/strong&gt;: Access hundreds of AI models with unified APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intuitive UI Console&lt;/strong&gt;: Simplifies project management and allows in-console workflow testing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;BaaS-Inspired Workflow&lt;/strong&gt;: Separate AI logic (server-side) from product development (client-side), offering a clear pathway from console-based prototyping to scalable solutions using RESTful APIs and client SDKs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable Integration&lt;/strong&gt;: Enhance LLM functionalities with customizable &lt;strong&gt;tools&lt;/strong&gt; and advanced &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt; (RAG) system&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Asynchronous Efficiency&lt;/strong&gt;: Harness Python FastAPI&#39;s asynchronous features for high-performance, concurrent computation, enhancing the responsiveness and scalability of the applications.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TaskingAI/TaskingAI/master/static/img/console.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Please give us a &lt;strong&gt;FREE STAR🌟&lt;/strong&gt; on GitHub if you find it useful.&lt;/p&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TaskingAI/TaskingAI/master/static/img/star.gif&#34; alt=&#34;&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;What Can You Build with TaskingAI?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;&lt;strong&gt;Interactive Application Demos&lt;/strong&gt;: Quickly create and deploy engaging application demos using TaskingAI&#39;s UI Console. It’s an ideal environment for demonstrating the potential of AI-native apps with real-time interaction and user engagement.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;&lt;strong&gt;AI Agents for Team Collaboration&lt;/strong&gt;: Develop AI agents that harness collective knowledge and tools, enhancing teamwork and efficiency. TaskingAI facilitates the creation of shared AI resources that streamline collaboration and support within your organization.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;&lt;strong&gt;Multi-Tenant AI-Native Applications for Business&lt;/strong&gt;: With TaskingAI, build robust, multi-tenant AI-native applications that are ready for production. It&#39;s perfectly suited for handling varied client needs while maintaining individual customization, security, and scalability.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why TaskingAI?&lt;/h2&gt; &#xA;&lt;h3&gt;Problems with existing products&lt;/h3&gt; &#xA;&lt;p&gt;The Assistant API from OpenAI, while robust for GPTs-like functionalities, has limitations due to its design where key functions like tools and documentation retrieval are tied to individual assistants. This structure can restrict flexibility in multi-tenant applications, where shared data is essential.&lt;/p&gt; &#xA;&lt;h3&gt;How TaskingAI solves the problem&lt;/h3&gt; &#xA;&lt;p&gt;TaskingAI overcomes these obstacles by decoupling key modules, offering broader model support and an open-source framework. Its adaptability makes it a superior choice for developers needing more versatile, data-sharing capable AI solutions, particularly for complex, customizable projects.&lt;/p&gt; &#xA;&lt;h3&gt;Comparisons&lt;/h3&gt; &#xA;&lt;p&gt;Here is a comparison table among the mainstream agent development frameworks and TaskingAI:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;LangChain&lt;/th&gt; &#xA;   &lt;th&gt;OpenAI &lt;br&gt; Assistant API&lt;/th&gt; &#xA;   &lt;th&gt;TaskingAI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Ecosystem&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Open Source&lt;/td&gt; &#xA;   &lt;td&gt;Proprietary&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;LLM Providers&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multiple providers&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI only&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Multiple providers&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Retrieval System&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Requires 3rd-party&lt;/td&gt; &#xA;   &lt;td&gt;Tied with Assistant&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Decoupled; flexible&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Tool Integration&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Requires 3rd-party&lt;/td&gt; &#xA;   &lt;td&gt;Tied with Assistant&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Decoupled; flexible&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Agent Memory&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Configurable&lt;/td&gt; &#xA;   &lt;td&gt;Opaque&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Customizable&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Development Method&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python-based SDK&lt;/td&gt; &#xA;   &lt;td&gt;RESTful APIs &amp;amp; SDKs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;RESTful APIs &amp;amp; SDKs&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Async Support&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Selective model support&lt;/td&gt; &#xA;   &lt;td&gt;Limited to OpenAI models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Multi-Tenant Support&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Complex setup&lt;/td&gt; &#xA;   &lt;td&gt;Not available&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Simplified setup&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;TaskingAI&#39;s architecture is designed with modularity and flexibility at its core, enabling compatibility with a wide spectrum of LLMs. This adaptability allows it to effortlessly support a variety of applications, from straightforward demos to sophisticated, multi-tenant AI systems. Constructed on a foundation of open-source principles, TaskingAI incorporates numerous open-source tools, ensuring that the platform is not only versatile but also customizable.&lt;/p&gt; &#xA;&lt;p&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TaskingAI/TaskingAI/master/static/img/architecture.png&#34; alt=&#34;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.nginx.com/&#34;&gt;Nginx&lt;/a&gt;&lt;/strong&gt;: Functions as the frontend web server, efficiently routing traffic to the designated services within the architecture.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Frontend (&lt;a href=&#34;https://www.typescriptlang.org/&#34;&gt;TypeScript&lt;/a&gt; + &lt;a href=&#34;https://react.dev/&#34;&gt;React&lt;/a&gt;)&lt;/strong&gt;: This interactive and responsive user interface is built with TypeScript and React, allowing users to smoothly interact with backend APIs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Backend (&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; + &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt;)&lt;/strong&gt;: The backend, engineered with Python and FastAPI, offers high performance stemming from its asynchronous design. It manages business logic, data processing, and serves as the conduit between the frontend and AI inference services. Python&#39;s widespread use invites broader contributions, fostering a collaborative environment for continuous improvement and innovation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/taskingai/taskingai-inference&#34;&gt;TaskingAI-Inference&lt;/a&gt;&lt;/strong&gt;: Dedicated to AI model inference, this component adeptly handles tasks such as response generation and natural language input processing. It&#39;s another standout project within TaskingAI&#39;s suite of open-source offerings.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TaskingAI Core Services&lt;/strong&gt;: Comprises various services including Model, Assistant, Retrieval, and Tool, each integral to the platform&#39;s operation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; + &lt;a href=&#34;https://github.com/pgvector/pgvector&#34;&gt;PGVector&lt;/a&gt;&lt;/strong&gt;: Serves as the primary database, with PGVector enhancing vector operations for embedding comparisons, crucial for AI functionalities.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.redis.com/&#34;&gt;Redis&lt;/a&gt;&lt;/strong&gt;: Delivers high-performance data caching, crucial for expediting response times and bolstering data retrieval efficiency.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart with Docker&lt;/h2&gt; &#xA;&lt;p&gt;A simple way to initiate self-hosted TaskingAI community edition is through &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker and Docker Compose installed on your machine.&lt;/li&gt; &#xA; &lt;li&gt;Git installed for cloning the repository.&lt;/li&gt; &#xA; &lt;li&gt;Python environment (above Python 3.8) for running the client SDK.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;First, clone the TaskingAI (community edition) repository from GitHub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/taskingai/taskingai.git&#xA;cd taskingai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Inside the cloned repository, go to the docker directory and launch the services using Docker Compose.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd docker&#xA;docker-compose -p taskingai up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the service is up, access the TaskingAI console through your browser with the URL &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;. The default username and password are &lt;code&gt;admin&lt;/code&gt; and &lt;code&gt;TaskingAI321&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;TaskingAI UI Console&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/4A5uQoawETU&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/4A5uQoawETU/maxresdefault.jpg&#34; alt=&#34;TaskingAI Console Demo&#34;&gt;&lt;/a&gt; &lt;em&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p style=&#34;text-align: center; font-size: small; &#34;&gt;&lt;em&gt;&lt;strong&gt;Click the image above for TaskingAI Console Demo Video&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TaskingAI Client SDK&lt;/h3&gt; &#xA;&lt;p&gt;Once the console is up, you can programmatically interact with the TaskingAI server using the TaskingAI client SDK.&lt;/p&gt; &#xA;&lt;p&gt;Ensure you have Python 3.8 or above installed, and set up a virtual environment (optional but recommended). Install the TaskingAI Python client SDK using pip.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install taskingai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is a client code example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import taskingai&#xA;from taskingai.assistant.memory import AssistantNaiveMemory&#xA;&#xA;taskingai.init(api_key=&#39;YOUR_API_KEY&#39;, host=&#39;http://localhost:8080&#39;)&#xA;&#xA;# Create a new assistant&#xA;assistant = taskingai.assistant.create_assistant(&#xA;    model_id=&#34;YOUR_MODEL_ID&#34;,&#xA;    memory=AssistantNaiveMemory(),&#xA;)&#xA;&#xA;# Create a new chat&#xA;chat = taskingai.assistant.create_chat(&#xA;    assistant_id=assistant.assistant_id,&#xA;)&#xA;&#xA;# Send a user message&#xA;taskingai.assistant.create_message(&#xA;    assistant_id=assistant.assistant_id,&#xA;    chat_id=chat.chat_id,&#xA;    text=&#34;Hello!&#34;,&#xA;)&#xA;&#xA;# generate assistant response&#xA;assistant_message = taskingai.assistant.generate_message(&#xA;    assistant_id=assistant.assistant_id,&#xA;    chat_id=chat.chat_id,&#xA;)&#xA;&#xA;print(assistant_message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;YOUR_API_KEY&lt;/code&gt; and &lt;code&gt;YOUR_MODEL_ID&lt;/code&gt; should be replaced with the actual API key and chat completion model ID you created in the console.&lt;/p&gt; &#xA;&lt;p&gt;You can learn more in the &lt;a href=&#34;https://docs.tasking.ai/docs/guide/getting_started/self_hosting/overview&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.tasking.ai&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.tasking.ai/api&#34;&gt;API Reference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tasking.ai/contact-us&#34;&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community and Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/TaskingAI/TaskingAI/master/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt; for how to contribute to the project.&lt;/p&gt; &#xA;&lt;h2&gt;License and Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;TaskingAI is released under a specific &lt;a href=&#34;https://raw.githubusercontent.com/TaskingAI/TaskingAI/master/LICENSE&#34;&gt;TaskingAI Open Source License&lt;/a&gt;. By contributing to this project, you agree to abide by its terms.&lt;/p&gt; &#xA;&lt;h2&gt;Support and Contact&lt;/h2&gt; &#xA;&lt;p&gt;For support, please refer to our &lt;a href=&#34;https://docs.tasking.ai&#34;&gt;documentation&lt;/a&gt; or contact us at &lt;a href=&#34;mailto:support@tasking.ai&#34;&gt;support@tasking.ai&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>