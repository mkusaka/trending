<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-24T01:42:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>horizon3ai/CVE-2022-39952</title>
    <updated>2023-02-24T01:42:58Z</updated>
    <id>tag:github.com,2023-02-24:/horizon3ai/CVE-2022-39952</id>
    <link href="https://github.com/horizon3ai/CVE-2022-39952" rel="alternate"></link>
    <summary type="html">&lt;p&gt;POC for CVE-2022-39952&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2022-39952&lt;/h1&gt; &#xA;&lt;p&gt;POC for CVE-2022-39952 affecting Fortinet FortiNAC&lt;/p&gt; &#xA;&lt;p&gt;The default configuration of this exploit writes a cron job to create a reverse shell. Be sure to change the &lt;code&gt;payload&lt;/code&gt; file to suite your environment.&lt;/p&gt; &#xA;&lt;h2&gt;Technical Analysis&lt;/h2&gt; &#xA;&lt;p&gt;A technical root cause analysis of the vulnerability and indicators of compromise can be found on our blog: &lt;a href=&#34;https://www.horizon3.ai/fortinet-fortinac-cve-2022-39952-deep-dive-and-iocs&#34;&gt;https://www.horizon3.ai/fortinet-fortinac-cve-2022-39952-deep-dive-and-iocs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Summary&lt;/h2&gt; &#xA;&lt;p&gt;This POC abuses the keyUpload.jsp endpoint to achieve an arbitrary file write.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;root@kali:~/CVE-2022-39952# python3 CVE-2022-39952.py --target 10.0.40.85 --file payload&#xA;[+] Wrote payload to /etc/cron.d/payload&#xA;[+] Payload successfully delivered&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If using a cron based payload, make sure the payload file has the appropriate permissions and owner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo chown root:root payload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo chmod 0644 payload &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Mitigations&lt;/h2&gt; &#xA;&lt;p&gt;Update to the latest version by following the instructions within the PSIRT &lt;a href=&#34;https://www.fortiguard.com/psirt/FG-IR-22-300&#34;&gt;https://www.fortiguard.com/psirt/FG-IR-22-300&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Follow the Horizon3.ai Attack Team on Twitter for the latest security research:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/Horizon3Attack&#34;&gt;Horizon3 Attack Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/JamesHorseman2&#34;&gt;James Horseman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/hacks_zach&#34;&gt;Zach Hanley&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This software has been created purely for the purposes of academic research and for the development of effective defensive techniques, and is not intended to be used to attack systems except where explicitly authorized. Project maintainers are not responsible or liable for misuse of the software. Use responsibly.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lupantech/ScienceQA</title>
    <updated>2023-02-24T01:42:58Z</updated>
    <id>tag:github.com,2023-02-24:/lupantech/ScienceQA</id>
    <link href="https://github.com/lupantech/ScienceQA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Data and code for NeurIPS 2022 Paper &#34;Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering&#34;.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ScienceQA: Science Question Answering&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/Task-VQA-orange&#34; alt=&#34;VQA&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Task-Science_Problems-orange&#34; alt=&#34;Science Problems&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Dataset-ScienceQA-blue&#34; alt=&#34;ScienceQA&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Model-Chain_of_Thought-green&#34; alt=&#34;Chain-of-Thought&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Model-GPT--3-green&#34; alt=&#34;GPT-3&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Model-LLMs-green&#34; alt=&#34;LLMs&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Data and code for NeurIPS 2022 Paper &#34;&lt;a href=&#34;http://lupantech.github.io/papers/neurips22_scienceqa.pdf&#34;&gt;Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering&lt;/a&gt;&#34;.&lt;/p&gt; &#xA;&lt;p&gt;For more details, please refer to the project page with dataset exploration and visualization tools: &lt;a href=&#34;https://scienceqa.github.io&#34;&gt;https://scienceqa.github.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸ””&lt;/span&gt; If you have any questions or suggestions, please don&#39;t hesitate to let us know. You can directly email &lt;a href=&#34;https://lupantech.github.io/&#34;&gt;Pan Lu&lt;/a&gt; at UCLA using the email address &lt;a href=&#34;mailto:lupantech@gmail.com&#34;&gt;lupantech@gmail.com&lt;/a&gt;, comment on the &lt;a href=&#34;https://twitter.com/lupantech/status/1570828580346802178&#34;&gt;Twitter&lt;/a&gt;, or post an issue on this repository.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;ðŸ”¥&lt;/span&gt; Leaderboard &lt;span&gt;ðŸ”¥&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;ðŸ””&lt;/span&gt; The leaderboard is continuously being updated. If you have any new results to contribute, please feel free to reach out to us.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Sources&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Date&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;NAT&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;SOC&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;LAN&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;TXT&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;IMG&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;NO&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;G1-6&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;G7-12&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Avg&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;Random Chance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.09513&#34;&gt;Pan et al., NeurIPS 2022&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sep-2022&lt;/td&gt; &#xA;   &lt;td&gt;40.28&lt;/td&gt; &#xA;   &lt;td&gt;46.13&lt;/td&gt; &#xA;   &lt;td&gt;29.25&lt;/td&gt; &#xA;   &lt;td&gt;47.45&lt;/td&gt; &#xA;   &lt;td&gt;40.08&lt;/td&gt; &#xA;   &lt;td&gt;33.66&lt;/td&gt; &#xA;   &lt;td&gt;39.35&lt;/td&gt; &#xA;   &lt;td&gt;40.67&lt;/td&gt; &#xA;   &lt;td&gt;39.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;Human Average&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.09513&#34;&gt;Pan et al., NeurIPS 2022&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sep-2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;90.23&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84.97&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;87.48&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;89.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;87.50&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;88.10&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;91.59&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;82.42&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;88.40&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-3.5&lt;/strong&gt; (QCMâ†’A, 2-shot)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.09513&#34;&gt;Pan et al., NeurIPS 2022&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sep-2022&lt;/td&gt; &#xA;   &lt;td&gt;74.64&lt;/td&gt; &#xA;   &lt;td&gt;69.74&lt;/td&gt; &#xA;   &lt;td&gt;76.00&lt;/td&gt; &#xA;   &lt;td&gt;74.44&lt;/td&gt; &#xA;   &lt;td&gt;67.28&lt;/td&gt; &#xA;   &lt;td&gt;77.42&lt;/td&gt; &#xA;   &lt;td&gt;76.80&lt;/td&gt; &#xA;   &lt;td&gt;68.89&lt;/td&gt; &#xA;   &lt;td&gt;73.97&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-3.5&lt;/strong&gt; (QCMâ†’A, zero-shot)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.09513&#34;&gt;Pan et al., NeurIPS 2022&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sep-2022&lt;/td&gt; &#xA;   &lt;td&gt;75.04&lt;/td&gt; &#xA;   &lt;td&gt;66.59&lt;/td&gt; &#xA;   &lt;td&gt;78.00&lt;/td&gt; &#xA;   &lt;td&gt;74.24&lt;/td&gt; &#xA;   &lt;td&gt;65.74&lt;/td&gt; &#xA;   &lt;td&gt;79.58&lt;/td&gt; &#xA;   &lt;td&gt;76.36&lt;/td&gt; &#xA;   &lt;td&gt;69.87&lt;/td&gt; &#xA;   &lt;td&gt;74.04&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-3.5 w/ CoT&lt;/strong&gt; (QCMâ†’A, 2-shot)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.09513&#34;&gt;Pan et al., NeurIPS 2022&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sep-2022&lt;/td&gt; &#xA;   &lt;td&gt;76.60&lt;/td&gt; &#xA;   &lt;td&gt;65.92&lt;/td&gt; &#xA;   &lt;td&gt;77.55&lt;/td&gt; &#xA;   &lt;td&gt;75.51&lt;/td&gt; &#xA;   &lt;td&gt;66.09&lt;/td&gt; &#xA;   &lt;td&gt;79.58&lt;/td&gt; &#xA;   &lt;td&gt;78.49&lt;/td&gt; &#xA;   &lt;td&gt;67.63&lt;/td&gt; &#xA;   &lt;td&gt;74.61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;GPT-3.5 w/ CoT&lt;/strong&gt; (QCMâ†’ALE, 2-shot)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.09513&#34;&gt;Pan et al., NeurIPS 2022&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sep-2022&lt;/td&gt; &#xA;   &lt;td&gt;75.44&lt;/td&gt; &#xA;   &lt;td&gt;70.87&lt;/td&gt; &#xA;   &lt;td&gt;78.09&lt;/td&gt; &#xA;   &lt;td&gt;74.68&lt;/td&gt; &#xA;   &lt;td&gt;67.43&lt;/td&gt; &#xA;   &lt;td&gt;79.93&lt;/td&gt; &#xA;   &lt;td&gt;78.23&lt;/td&gt; &#xA;   &lt;td&gt;69.68&lt;/td&gt; &#xA;   &lt;td&gt;75.17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Multimodal-CoT&lt;/strong&gt; (Base)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.00923&#34;&gt;Zhang et al., arXiv 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Feb-2023&lt;/td&gt; &#xA;   &lt;td&gt;87.52&lt;/td&gt; &#xA;   &lt;td&gt;77.17&lt;/td&gt; &#xA;   &lt;td&gt;85.82&lt;/td&gt; &#xA;   &lt;td&gt;87.88&lt;/td&gt; &#xA;   &lt;td&gt;82.90&lt;/td&gt; &#xA;   &lt;td&gt;86.83&lt;/td&gt; &#xA;   &lt;td&gt;84.65&lt;/td&gt; &#xA;   &lt;td&gt;85.37&lt;/td&gt; &#xA;   &lt;td&gt;84.91&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Multimodal-CoT&lt;/strong&gt; (Large)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.00923&#34;&gt;Zhang et al., arXiv 2023&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Feb-2023&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;95.91&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;82.00&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;90.82&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;95.26&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;88.80&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;92.89&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;92.44&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;90.31&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;91.68&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;About ScienceQA&lt;/h2&gt; &#xA;&lt;p&gt;We present &lt;strong&gt;Science Question Answering (ScienceQA)&lt;/strong&gt;, a new benchmark that consists of 21,208 multimodal multiple choice questions with a diverse set of &lt;em&gt;science&lt;/em&gt; topics and annotations of their answers with corresponding &lt;em&gt;lectures&lt;/em&gt; and &lt;em&gt;explanations&lt;/em&gt;. The lecture and explanation provide general external knowledge and specific reasons, respectively, for arriving at the correct answer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lupantech/ScienceQA/main/data/scienceqa.png&#34; alt=&#34;scienceqa&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ScienceQA, in contrast to previous datasets, has richer domain diversity from three subjects: &lt;strong&gt;natural science&lt;/strong&gt;, &lt;strong&gt;language science&lt;/strong&gt;, and &lt;strong&gt;social science&lt;/strong&gt;. ScienceQA features 26 topics, 127 categories, and 379 skills that cover a wide range of domains.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lupantech/ScienceQA/main/data/domains.png&#34; alt=&#34;domains&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We further design language models to learn to generate lectures and explanations as &lt;strong&gt;the chain of thought (CoT)&lt;/strong&gt; to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA.&lt;/p&gt; &#xA;&lt;p&gt;For more details, you can find our project page &lt;a href=&#34;https://scienceqa.github.io/&#34;&gt;here&lt;/a&gt; and our paper &lt;a href=&#34;https://lupantech.github.io/papers/neurips22_scienceqa.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download the dataset&lt;/h2&gt; &#xA;&lt;p&gt;The text part of the &lt;strong&gt;ScienceQA&lt;/strong&gt; dataset is provided in &lt;a href=&#34;https://github.com/lupantech/ScienceQA/raw/main/data/scienceqa/problems.json&#34;&gt;data/scienceqa/problems.json&lt;/a&gt;. You can download the image data of ScienceQA by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;. tools/download.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can download &lt;strong&gt;ScienceQA&lt;/strong&gt; from &lt;a href=&#34;https://drive.google.com/drive/folders/1w8imCXWYn2LxajmGeGH_g5DaL2rabHev?usp=sharing&#34;&gt;Google Drive&lt;/a&gt; and unzip the images under &lt;code&gt;root_dir/data&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python==3.8.10&#xA;huggingface-hub==0.0.12&#xA;nltk==3.5&#xA;numpy==1.23.2&#xA;openai==0.23.0&#xA;pandas==1.4.3&#xA;rouge==1.0.1&#xA;sentence-transformers==2.2.2&#xA;torch==1.12.1+cu113&#xA;transformers==4.21.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install all required python dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run the GPT-3 (CoT) Model for ScienceQA&lt;/h2&gt; &#xA;&lt;h3&gt;Generate the image captions&lt;/h3&gt; &#xA;&lt;p&gt;We use the image captioning model to generate the text content for images in ScienceQA. The pre-generated image captions are provided in &lt;a href=&#34;https://github.com/lupantech/ScienceQA/raw/main/data/captions.json&#34;&gt;data/captions.json&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;(Optionally) You can generate the image captions with user-specific arguments with the following command, which will save the caption data in &lt;code&gt;data/captions_user.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd tools&#xA;python generate_caption.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run the model&lt;/h3&gt; &#xA;&lt;p&gt;We build a few-shot GPT-3 model via chain-of-thought (CoT) prompting to generate the answer followed by the lecture and the explanation (&lt;strong&gt;QCMâ†’ALE&lt;/strong&gt;). The prompt instruction encoding for the test example in GPT-3 (CoT) is defined as below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lupantech/ScienceQA/main/data/prompt.png&#34; alt=&#34;scienceqa&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In our final model, we develp GPT-3 (CoT) prompted with two in-context examples and evalute it on the ScienceQA test split:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd models&#xA;python run_gpt3.py \&#xA;--label exp1 \&#xA;--test_split test \&#xA;--test_number -1 \&#xA;--shot_number 2 \&#xA;--prompt_format QCM-ALE \&#xA;--seed 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluate the results&lt;/h3&gt; &#xA;&lt;p&gt;Our final GPT-3 (CoT) model achieves a state-of-the-art accuracy of 75.17% on the test split. One prediction example is visualized bellow. We can see that GPT-3 (CoT) predicts the correct answer and generates a reasonable lecture and explanation to mimic the human thought process.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lupantech/ScienceQA/main/data/prediction.png&#34; alt=&#34;scienceqa&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can get the accuracy metrics on average and across different question classes by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd tools&#xA;python evaluate_acc.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can run the following command to evaluate the generated lectures and explanations automatically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd tools&#xA;python evaluate_explaination.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Try different prompt templates&lt;/h3&gt; &#xA;&lt;p&gt;You can try other prompt templates. For example, if you want the model to take the question, the context, and the multiple options as input, and output the answer after the lecture and explanation (QCMâ†’LEA), you can run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd models&#xA;python run_gpt3.py \&#xA;--label exp1 \&#xA;--test_split test \&#xA;--test_number -1 \&#xA;--shot_number 2 \&#xA;--prompt_format QCM-LEA \&#xA;--seed 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lbesson.mit-license.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This work is licensed under a &lt;a href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/bdc6a3b8963aa99ff57dfd6e1e4b937bd2e752bcb1f1936f90368e5c3a38f670/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d434325323042592d2d5341253230342e302d6c69676874677265792e737667&#34; alt=&#34;License: CC BY-SA 4.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The ScienceQA dataset is licensed under a &lt;a href=&#34;http://creativecommons.org/licenses/by-nc-sa/4.0/&#34;&gt;Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If the paper, codes, or the dataset inspire you, please cite us:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@inproceedings{lu2022learn,&#xA;    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},&#xA;    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},&#xA;    booktitle={The 36th Conference on Neural Information Processing Systems (NeurIPS)},&#xA;    year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/PromptCraft-Robotics</title>
    <updated>2023-02-24T01:42:58Z</updated>
    <id>tag:github.com,2023-02-24:/microsoft/PromptCraft-Robotics</id>
    <link href="https://github.com/microsoft/PromptCraft-Robotics" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Community for applying LLMs to robotics and a robot simulator with ChatGPT integration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PromptCraft-Robotics&lt;/h1&gt; &#xA;&lt;p&gt;The PromptCraft-Robotics repository serves as a community for people to test and share interesting prompting examples for large language models (LLMs) within the robotics domain. We also provide a sample &lt;a href=&#34;https://github.com/microsoft/PromptCraft-Robotics/tree/main/chatgpt_airsim&#34;&gt;robotics simulator&lt;/a&gt; (built on Microsoft AirSim) with ChatGPT integration for users to get started.&lt;/p&gt; &#xA;&lt;p&gt;We currently focus on OpenAI&#39;s &lt;a href=&#34;https://openai.com/blog/chatgpt/&#34;&gt;ChatGPT&lt;/a&gt;, but we also welcome examples from other LLMs (for example open-sourced models or others with API access such as &lt;a href=&#34;https://openai.com/api/&#34;&gt;GPT-3&lt;/a&gt; and Codex).&lt;/p&gt; &#xA;&lt;p&gt;Users can contribute to this repository by submitting interesting prompt examples to the &lt;a href=&#34;https://github.com/microsoft/PromptCraft-Robotics/discussions&#34;&gt;Discussions&lt;/a&gt; section of this repository. A prompt can be submitted within different robotics categories such as &lt;a href=&#34;https://github.com/microsoft/PromptCraft-Robotics/discussions/categories/llm-manipulation&#34;&gt;Manipulation&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/PromptCraft-Robotics/discussions/categories/llm-home-robots&#34;&gt;Home Robotics&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/PromptCraft-Robotics/discussions/categories/llm-physical-reasoning&#34;&gt;Physical Reasoning&lt;/a&gt;, among many others. Once submitted, the prompt will be reviewed by the community (upvote your favorites!) and added to the repository by a team of admins if it is deemed interesting and useful. We encourage users to submit prompts that are interesting, fun, or useful. We also encourage users to submit prompts that are not necessarily &#34;correct&#34; or &#34;optimal&#34; but are interesting nonetheless.&lt;/p&gt; &#xA;&lt;p&gt;We encourage prompt submissions formatted as markdown, so that they can be easily transferred to the main repository. Please specify which LLM you used, and if possible provide other visuals of the model in action such as videos and pictures.&lt;/p&gt; &#xA;&lt;h2&gt;Paper, videos and citations&lt;/h2&gt; &#xA;&lt;p&gt;Blog post: &lt;a href=&#34;https://aka.ms/ChatGPT-Robotics&#34; target=&#34;_blank&#34;&gt;aka.ms/ChatGPT-Robotics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf&#34; target=&#34;_blank&#34;&gt;ChatGPT for Robotics: Design Principles and Model Abilities&lt;/a&gt;&lt;/p&gt;&#xA;&lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf&#34; target=&#34;_blank&#34;&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf&#34; target=&#34;_blank&#34;&gt;Video: &lt;/a&gt;&lt;a href=&#34;https://youtu.be/NYd0QcZcS6Q&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://youtu.be/NYd0QcZcS6Q&#34;&gt;https://youtu.be/NYd0QcZcS6Q&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you use this repository in your research, please cite the following paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@techreport{vemprala2023chatgpt,&#xA;author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},&#xA;title = {ChatGPT for Robotics: Design Principles and Model Abilities},&#xA;institution = {Microsoft},&#xA;year = {2023},&#xA;month = {February},&#xA;url = {https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/},&#xA;number = {MSR-TR-2023-8},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ChatGPT Prompting Guides &amp;amp; Examples&lt;/h2&gt; &#xA;&lt;p&gt;The list below contains links to the different robotics categories and their corresponding prompt examples. We welcome contributions to this repository to add more robotics categories and examples. Please submit prompt examples to the &lt;a href=&#34;https://github.com/microsoft/PromptCraft-Robotics/discussions&#34;&gt;Discussions&lt;/a&gt; page, or submit a pull request with your category and examples.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embodied agent &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/embodied_agents/visual_language_navigation_1.md&#34;&gt;ChatGPT - Habitat, closed loop object navigation 1&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/embodied_agents/visual_language_navigation_2.md&#34;&gt;ChatGPT - Habitat, closed loop object navigation 2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/embodied_agents/airsim_objectnavigation.md&#34;&gt;ChatGPT - AirSim, object navigation using RGBD&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Aerial robotics &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/aerial_robotics/tello_example.md&#34;&gt;ChatGPT - Real robot: Tello deployment&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/i5wZJFb4dyA&#34;&gt;Video Link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/aerial_robotics/airsim_turbine_inspection.md&#34;&gt;ChatGPT - AirSim turbine Inspection&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/38lA3U2J43w&#34;&gt;Video Link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/aerial_robotics/airsim_solarpanel_inspection.md&#34;&gt;ChatGPT - AirSim solar panel Inspection&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/aerial_robotics/airsim_obstacleavoidance.md&#34;&gt;ChatGPT - AirSim obstacle avoidance&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/Vn6NapLlHPE&#34;&gt;Video Link&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Manipulation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/manipulation/pick_stack_msft_logo.md&#34;&gt;ChatGPT - Real robot: Picking, stacking, and building the MSFT logo&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/wLOChUtdqoA&#34;&gt;Video Link&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/manipulation/manipulation_zeroshot.md&#34;&gt;ChatGPT - Manipulation tasks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Spatial-temporal reasoning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/examples/spatial_temporal_reasoning/visual_servoing_basketball.md&#34;&gt;ChatGPT - Visual servoing with basketball&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ChatGPT + Robotics Simulator&lt;/h2&gt; &#xA;&lt;p&gt;We provice a sample &lt;a href=&#34;https://github.com/microsoft/AirSim&#34;&gt;AirSim&lt;/a&gt; environment for users to test their ChatGPT prompts. The environment is a binary containing a sample inspection environment with assets such as wind turbines, electric towers, solar panels etc. The environment comes with a drone and interfaces with ChatGPT such that users can easily send commands in natural language. &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/PromptCraft-Robotics/main/chatgpt_airsim/README.md&#34;&gt;[Simulator Link]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We welcome contributions to this repository to add more robotics simulators and environments. Please submit a pull request with your simulator and environment.&lt;/p&gt; &#xA;&lt;h2&gt;Related resources&lt;/h2&gt; &#xA;&lt;p&gt;Beyond the prompt examples here, we leave useful and related links to the use of large language models below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/api/&#34;&gt;Read about the OpenAI APIs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/products/cognitive-services/openai-service&#34;&gt;Azure OpenAI service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/opt&#34;&gt;OPT language model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>