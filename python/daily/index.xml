<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-08T01:34:31Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gusye1234/nano-graphrag</title>
    <updated>2024-12-08T01:34:31Z</updated>
    <id>tag:github.com,2024-12-08:/gusye1234/nano-graphrag</id>
    <link href="https://github.com/gusye1234/nano-graphrag" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple, easy-to-hack GraphRAG implementation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/gusye1234/nano-graphrag&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://assets.memodb.io/nano-graphrag-dark.png&#34;&gt; &#xA;   &lt;img alt=&#34;Shows the MemoDB logo&#34; src=&#34;https://assets.memodb.io/nano-graphrag.png&#34; width=&#34;512&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA; &lt;p&gt;&lt;strong&gt;A simple, easy-to-hack GraphRAG implementation&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt; &lt;img src=&#34;https://img.shields.io/badge/python-%3E=3.9.11-blue&#34;&gt; &lt;a href=&#34;https://pypi.org/project/nano-graphrag/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/nano-graphrag.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://codecov.io/github/gusye1234/nano-graphrag&#34;&gt; &lt;img src=&#34;https://codecov.io/github/gusye1234/nano-graphrag/graph/badge.svg?token=YFPMj9uQo7&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/nano-graphrag&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/badge/nano-graphrag/month&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt; &lt;a href=&#34;https://discord.gg/sqCVzAhUY6&#34;&gt; &lt;img src=&#34;https://dcbadge.limes.pink/api/server/sqCVzAhUY6?style=flat&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/gusye1234/nano-graphrag/issues/8&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%E7%BE%A4%E8%81%8A-wechat-green&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;üò≠ &lt;a href=&#34;https://arxiv.org/pdf/2404.16130&#34;&gt;GraphRAG&lt;/a&gt; is good and powerful, but the official &lt;a href=&#34;https://github.com/microsoft/graphrag/tree/main&#34;&gt;implementation&lt;/a&gt; is difficult/painful to &lt;strong&gt;read or hack&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üòä This project provides a &lt;strong&gt;smaller, faster, cleaner GraphRAG&lt;/strong&gt;, while remaining the core functionality(see &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/#benchmark&#34;&gt;benchmark&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/#Issues&#34;&gt;issues&lt;/a&gt; ).&lt;/p&gt; &#xA;&lt;p&gt;üéÅ Excluding &lt;code&gt;tests&lt;/code&gt; and prompts, &lt;code&gt;nano-graphrag&lt;/code&gt; is about &lt;strong&gt;1100 lines of code&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üëå Small yet &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/#Components&#34;&gt;&lt;strong&gt;portable&lt;/strong&gt;&lt;/a&gt;(faiss, neo4j, ollama...), &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/#Async&#34;&gt;&lt;strong&gt;asynchronous&lt;/strong&gt;&lt;/a&gt; and fully typed.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install from source&lt;/strong&gt; (recommend)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# clone this repo first&#xA;cd nano-graphrag&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install from PyPi&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install nano-graphrag&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Please set OpenAI API key in environment: &lt;code&gt;export OPENAI_API_KEY=&#34;sk-...&#34;&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you&#39;re using Azure OpenAI API, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/.env.example.azure&#34;&gt;.env.example&lt;/a&gt; to set your azure openai. Then pass &lt;code&gt;GraphRAG(...,using_azure_openai=True,...)&lt;/code&gt; to enable.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you&#39;re using Amazon Bedrock API, please ensure your credentials are properly set through commands like &lt;code&gt;aws configure&lt;/code&gt;. Then enable it by configuring like this: &lt;code&gt;GraphRAG(...,using_amazon_bedrock=True, best_model_id=&#34;us.anthropic.claude-3-sonnet-20240229-v1:0&#34;, cheap_model_id=&#34;us.anthropic.claude-3-haiku-20240307-v1:0&#34;,...)&lt;/code&gt;. Refer to an &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/using_amazon_bedrock.py&#34;&gt;example script&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;If you don&#39;t have any key, check out this &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/no_openai_key_at_all.py&#34;&gt;example&lt;/a&gt; that using &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;ollama&lt;/code&gt; . If you like to use another LLM or Embedding Model, check &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/#Advances&#34;&gt;Advances&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;download a copy of A Christmas Carol by Charles Dickens:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &amp;gt; ./book.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use the below python snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nano_graphrag import GraphRAG, QueryParam&#xA;&#xA;graph_func = GraphRAG(working_dir=&#34;./dickens&#34;)&#xA;&#xA;with open(&#34;./book.txt&#34;) as f:&#xA;    graph_func.insert(f.read())&#xA;&#xA;# Perform global graphrag search&#xA;print(graph_func.query(&#34;What are the top themes in this story?&#34;))&#xA;&#xA;# Perform local graphrag search (I think is better and more scalable one)&#xA;print(graph_func.query(&#34;What are the top themes in this story?&#34;, param=QueryParam(mode=&#34;local&#34;)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next time you initialize a &lt;code&gt;GraphRAG&lt;/code&gt; from the same &lt;code&gt;working_dir&lt;/code&gt;, it will reload all the contexts automatically.&lt;/p&gt; &#xA;&lt;h4&gt;Batch Insert&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;graph_func.insert([&#34;TEXT1&#34;, &#34;TEXT2&#34;,...])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Incremental Insert&lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; supports incremental insert, no duplicated computation or data will be added:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&#34;./book.txt&#34;) as f:&#xA;    book = f.read()&#xA;    half_len = len(book) // 2&#xA;    graph_func.insert(book[:half_len])&#xA;    graph_func.insert(book[half_len:])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; use md5-hash of the content as the key, so there is no duplicated chunk.&lt;/p&gt; &#xA;  &lt;p&gt;However, each time you insert, the communities of graph will be re-computed and the community reports will be re-generated&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Naive RAG&lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; supports naive RAG insert and query as well:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;graph_func = GraphRAG(working_dir=&#34;./dickens&#34;, enable_naive_rag=True)&#xA;...&#xA;# Query&#xA;print(rag.query(&#xA;      &#34;What are the top themes in this story?&#34;,&#xA;      param=QueryParam(mode=&#34;naive&#34;)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Async&lt;/h3&gt; &#xA;&lt;p&gt;For each method &lt;code&gt;NAME(...)&lt;/code&gt; , there is a corresponding async method &lt;code&gt;aNAME(...)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;await graph_func.ainsert(...)&#xA;await graph_func.aquery(...)&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Available Parameters&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;GraphRAG&lt;/code&gt; and &lt;code&gt;QueryParam&lt;/code&gt; are &lt;code&gt;dataclass&lt;/code&gt; in Python. Use &lt;code&gt;help(GraphRAG)&lt;/code&gt; and &lt;code&gt;help(QueryParam)&lt;/code&gt; to see all available parameters! Or check out the &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/#Advances&#34;&gt;Advances&lt;/a&gt; section to see some options.&lt;/p&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;p&gt;Below are the components you can use:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;What&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Where&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Amazon Bedrock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Embedding&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Amazon Bedrock&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sentence-transformers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Vector DataBase&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/gusye1234/nano-vectordb&#34;&gt;&lt;code&gt;nano-vectordb&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/nmslib/hnswlib&#34;&gt;&lt;code&gt;hnswlib&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in, &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/milvus-io/milvus-lite&#34;&gt;&lt;code&gt;milvus-lite&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/faiss?tab=readme-ov-file&#34;&gt;faiss&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Graph Storage&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://networkx.org/documentation/stable/index.html&#34;&gt;&lt;code&gt;networkx&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://neo4j.com/&#34;&gt;&lt;code&gt;neo4j&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in(&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/docs/use_neo4j_for_graphrag.md&#34;&gt;doc&lt;/a&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Visualization&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;graphml&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chunking&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;by token size&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;by text splitter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Built-in&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Built-in&lt;/code&gt; means we have that implementation inside &lt;code&gt;nano-graphrag&lt;/code&gt;. &lt;code&gt;examples&lt;/code&gt; means we have that implementation inside an tutorial under &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/benchmarks&#34;&gt;examples/benchmarks&lt;/a&gt; to see few comparisons between components.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Always welcome to contribute more components.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Advances&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Some setup options&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;GraphRAG(...,always_create_working_dir=False,...)&lt;/code&gt; will skip the dir-creating step. Use it if you switch all your components to non-file storages.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Only query the related context&lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;graph_func.query&lt;/code&gt; return the final answer without streaming.&lt;/p&gt; &#xA; &lt;p&gt;If you like to interagte &lt;code&gt;nano-graphrag&lt;/code&gt; in your project, you can use &lt;code&gt;param=QueryParam(..., only_need_context=True,...)&lt;/code&gt;, which will only return the retrieved context from graph, something like:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;# Local mode&#xA;-----Reports-----&#xA;```csv&#xA;id,&#x9;content&#xA;0,&#x9;# FOX News and Key Figures in Media and Politics...&#xA;1, ...&#xA;```&#xA;...&#xA;&#xA;# Global mode&#xA;----Analyst 3----&#xA;Importance Score: 100&#xA;Donald J. Trump: Frequently discussed in relation to his political activities...&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can integrate that context into your customized prompt.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Prompt&lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; use prompts from &lt;code&gt;nano_graphrag.prompt.PROMPTS&lt;/code&gt; dict object. You can play with it and replace any prompt inside.&lt;/p&gt; &#xA; &lt;p&gt;Some important prompts:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;PROMPTS[&#34;entity_extraction&#34;]&lt;/code&gt; is used to extract the entities and relations from a text chunk.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;PROMPTS[&#34;community_report&#34;]&lt;/code&gt; is used to organize and summary the graph cluster&#39;s description.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;PROMPTS[&#34;local_rag_response&#34;]&lt;/code&gt; is the system prompt template of the local search generation.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;PROMPTS[&#34;global_reduce_rag_response&#34;]&lt;/code&gt; is the system prompt template of the global search generation.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;PROMPTS[&#34;fail_response&#34;]&lt;/code&gt; is the fallback response when nothing is related to the user query.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Customize Chunking&lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; allow you to customize your own chunking method, check out the &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/using_custom_chunking_method.py&#34;&gt;example&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Switch to the built-in text splitter chunking method:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nano_graphrag._op import chunking_by_seperators&#xA;&#xA;GraphRAG(...,chunk_func=chunking_by_seperators,...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;LLM Function&lt;/summary&gt; &#xA; &lt;p&gt;In &lt;code&gt;nano-graphrag&lt;/code&gt;, we requires two types of LLM, a great one and a cheap one. The former is used to plan and respond, the latter is used to summary. By default, the great one is &lt;code&gt;gpt-4o&lt;/code&gt; and the cheap one is &lt;code&gt;gpt-4o-mini&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;You can implement your own LLM function (refer to &lt;code&gt;_llm.gpt_4o_complete&lt;/code&gt;):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;async def my_llm_complete(&#xA;    prompt, system_prompt=None, history_messages=[], **kwargs&#xA;) -&amp;gt; str:&#xA;  # pop cache KV database if any&#xA;  hashing_kv: BaseKVStorage = kwargs.pop(&#34;hashing_kv&#34;, None)&#xA;  # the rest kwargs are for calling LLM, for example, `max_tokens=xxx`&#xA;&#x9;...&#xA;  # YOUR LLM calling&#xA;  response = await call_your_LLM(messages, **kwargs)&#xA;  return response&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Replace the default one with:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Adjust the max token size or the max async requests if needed&#xA;GraphRAG(best_model_func=my_llm_complete, best_model_max_token_size=..., best_model_max_async=...)&#xA;GraphRAG(cheap_model_func=my_llm_complete, cheap_model_max_token_size=..., cheap_model_max_async=...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can refer to this &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/using_deepseek_as_llm.py&#34;&gt;example&lt;/a&gt; that use &lt;a href=&#34;https://platform.deepseek.com/api-docs/&#34;&gt;&lt;code&gt;deepseek-chat&lt;/code&gt;&lt;/a&gt; as the LLM model&lt;/p&gt; &#xA; &lt;p&gt;You can refer to this &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/using_ollama_as_llm.py&#34;&gt;example&lt;/a&gt; that use &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/a&gt; as the LLM model&lt;/p&gt; &#xA; &lt;h4&gt;Json Output&lt;/h4&gt; &#xA; &lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; will use &lt;code&gt;best_model_func&lt;/code&gt; to output JSON with params &lt;code&gt;&#34;response_format&#34;: {&#34;type&#34;: &#34;json_object&#34;}&lt;/code&gt;. However there are some open-source model maybe produce unstable JSON.&lt;/p&gt; &#xA; &lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; introduces a post-process interface for you to convert the response to JSON. This func&#39;s signature is below:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def YOUR_STRING_TO_JSON_FUNC(response: str) -&amp;gt; dict:&#xA;  &#34;Convert the string response to JSON&#34;&#xA;  ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;And pass your own func by &lt;code&gt;GraphRAG(...convert_response_to_json_func=YOUR_STRING_TO_JSON_FUNC,...)&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For example, you can refer to &lt;a href=&#34;https://github.com/mangiucugna/json_repair&#34;&gt;json_repair&lt;/a&gt; to repair the JSON string returned by LLM.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Embedding Function&lt;/summary&gt; &#xA; &lt;p&gt;You can replace the default embedding functions with any &lt;code&gt;_utils.EmbedddingFunc&lt;/code&gt; instance.&lt;/p&gt; &#xA; &lt;p&gt;For example, the default one is using OpenAI embedding API:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@wrap_embedding_func_with_attrs(embedding_dim=1536, max_token_size=8192)&#xA;async def openai_embedding(texts: list[str]) -&amp;gt; np.ndarray:&#xA;    openai_async_client = AsyncOpenAI()&#xA;    response = await openai_async_client.embeddings.create(&#xA;        model=&#34;text-embedding-3-small&#34;, input=texts, encoding_format=&#34;float&#34;&#xA;    )&#xA;    return np.array([dp.embedding for dp in response.data])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Replace default embedding function with:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;GraphRAG(embedding_func=your_embed_func, embedding_batch_num=..., embedding_func_max_async=...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can refer to an &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/using_local_embedding_model.py&#34;&gt;example&lt;/a&gt; that use &lt;code&gt;sentence-transformer&lt;/code&gt; to locally compute embeddings.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Storage Component&lt;/summary&gt; &#xA; &lt;p&gt;You can replace all storage-related components to your own implementation, &lt;code&gt;nano-graphrag&lt;/code&gt; mainly uses three kinds of storage:&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;code&gt;base.BaseKVStorage&lt;/code&gt; for storing key-json pairs of data&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;By default we use disk file storage as the backend.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;GraphRAG(.., key_string_value_json_storage_cls=YOURS,...)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;code&gt;base.BaseVectorStorage&lt;/code&gt; for indexing embeddings&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;By default we use &lt;a href=&#34;https://github.com/gusye1234/nano-vectordb&#34;&gt;&lt;code&gt;nano-vectordb&lt;/code&gt;&lt;/a&gt; as the backend.&lt;/li&gt; &#xA;  &lt;li&gt;We have a built-in &lt;a href=&#34;https://github.com/nmslib/hnswlib&#34;&gt;&lt;code&gt;hnswlib&lt;/code&gt;&lt;/a&gt; storage also, check out this &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/using_hnsw_as_vectorDB.py&#34;&gt;example&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Check out this &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/using_milvus_as_vectorDB.py&#34;&gt;example&lt;/a&gt; that implements &lt;a href=&#34;https://github.com/milvus-io/milvus-lite&#34;&gt;&lt;code&gt;milvus-lite&lt;/code&gt;&lt;/a&gt; as the backend (not available in Windows).&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;GraphRAG(.., vector_db_storage_cls=YOURS,...)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;code&gt;base.BaseGraphStorage&lt;/code&gt; for storing knowledge graph&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;By default we use &lt;a href=&#34;https://github.com/networkx/networkx&#34;&gt;&lt;code&gt;networkx&lt;/code&gt;&lt;/a&gt; as the backend.&lt;/li&gt; &#xA;  &lt;li&gt;We have a built-in &lt;code&gt;Neo4jStorage&lt;/code&gt; for graph, check out this &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/docs/use_neo4j_for_graphrag.md&#34;&gt;tutorial&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;GraphRAG(.., graph_storage_cls=YOURS,...)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;You can refer to &lt;code&gt;nano_graphrag.base&lt;/code&gt; to see detailed interfaces for each components.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;FQA&lt;/h2&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/docs/FAQ.md&#34;&gt;FQA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/docs/ROADMAP.md&#34;&gt;ROADMAP.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; is open to any kind of contribution. Read &lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/docs/CONTRIBUTING.md&#34;&gt;this&lt;/a&gt; before you contribute.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/docs/benchmark-en.md&#34;&gt;benchmark for English&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/docs/benchmark-zh.md&#34;&gt;benchmark for Chinese&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/examples/benchmarks/eval_naive_graphrag_on_multi_hop.ipynb&#34;&gt;An evaluation&lt;/a&gt; notebook on a &lt;a href=&#34;https://github.com/yixuantt/MultiHop-RAG&#34;&gt;multi-hop RAG task&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Projects that used &lt;code&gt;nano-graphrag&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MedicineToken/Medical-Graph-RAG&#34;&gt;Medical Graph RAG&lt;/a&gt;: Graph RAG for the Medical Data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HKUDS/LightRAG&#34;&gt;LightRAG&lt;/a&gt;: Simple and Fast Retrieval-Augmented Generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/circlemind-ai/fast-graphrag&#34;&gt;fast-graphrag&lt;/a&gt;: RAG that intelligently adapts to your use case, data, and queries&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Welcome to pull requests if your project uses &lt;code&gt;nano-graphrag&lt;/code&gt;, it will help others to trust this repo‚ù§Ô∏è&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; didn&#39;t implement the &lt;code&gt;covariates&lt;/code&gt; feature of &lt;code&gt;GraphRAG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; implements the global search different from the original. The original use a map-reduce-like style to fill all the communities into context, while &lt;code&gt;nano-graphrag&lt;/code&gt; only use the top-K important and central communites (use &lt;code&gt;QueryParam.global_max_consider_community&lt;/code&gt; to control, default to 512 communities).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>evolutionaryscale/esm</title>
    <updated>2024-12-08T01:34:31Z</updated>
    <id>tag:github.com,2024-12-08:/evolutionaryscale/esm</id>
    <link href="https://github.com/evolutionaryscale/esm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#installation-&#34;&gt;Installation &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#esm-c-&#34;&gt;ESM C &lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#using-esm-c-300m-and-600m-via-github&#34;&gt;Using ESM C 300M and 600M via GitHub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#using-esm-c-6b-via-forge-api&#34;&gt;Using ESM C 6B via Forge API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#using-esm-c-6b-via-sagemaker&#34;&gt;Using ESM C 6B via SageMaker&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#esm-3--&#34;&gt;ESM 3 &lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#quickstart-for-esm3-open&#34;&gt;Quickstart for ESM3-open&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#forge-access-to-larger-esm3-models&#34;&gt;Forge: Access to larger ESM3 models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#responsible-development-&#34;&gt;Responsible Development &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#licenses--&#34;&gt;Licenses &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation &lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To get started with ESM, install the library using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install esm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ESM C &lt;a name=&#34;esm-c&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.evolutionaryscale.ai/blog/esm-cambrian&#34;&gt;ESM Cambrian&lt;/a&gt; is a parallel model family to our flagship ESM3 generative models. While ESM3 focuses on controllable generation of proteins for therapeutic and many other applications, ESM C focuses on creating representations of the underlying biology of proteins.&lt;/p&gt; &#xA;&lt;p&gt;ESM C comes with major performance benefits over ESM2. The 300M parameter ESM C delivers similar performance to ESM2 650M with dramatically reduced memory requirements and faster inference. The 600M parameter ESM C rivals the 3B parameter ESM2 and approaches the capabilities of the 15B model, delivering frontier performance with far greater efficiency. The 6B parameter ESM C sets a new benchmark, outperforming the best ESM2 models by a wide margin.&lt;/p&gt; &#xA;&lt;p&gt;ESM C models are available immediately for academic and commercial use under a new license structure designed to promote openness and enable scientists and builders. You can find the high level take-away of the license structure in the &lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/#licenses&#34;&gt;Licenses&lt;/a&gt; section of this page, and the full license structure in the &lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;You can use the following guides to start using ESM C models today, either &lt;a href=&#34;https://huggingface.co/EvolutionaryScale&#34;&gt;running the model locally&lt;/a&gt;, &lt;a href=&#34;https://forge.evolutionaryscale.ai/&#34;&gt;the Forge API&lt;/a&gt; and &lt;a href=&#34;https://aws.amazon.com/marketplace/seller-profile?id=seller-iw2nbscescndm&#34;&gt;AWS SageMaker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using ESM C 300M and 600M via GitHub&lt;/h3&gt; &#xA;&lt;p&gt;ESM C model weights are stored on the HuggingFace hub under &lt;a href=&#34;https://huggingface.co/EvolutionaryScale/&#34;&gt;https://huggingface.co/EvolutionaryScale/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from esm.models.esmc import ESMC&#xA;from esm.sdk.api import ESMProtein, LogitsConfig&#xA;&#xA;protein = ESMProtein(sequence=&#34;AAAAA&#34;)&#xA;client = ESMC.from_pretrained(&#34;esmc_300m&#34;).to(&#34;cuda&#34;) # or &#34;cpu&#34;&#xA;protein_tensor = client.encode(protein)&#xA;logits_output = client.logits(&#xA;   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)&#xA;)&#xA;print(logits_output.logits, logits_output.embeddings)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using ESM C 6B via Forge API&lt;/h3&gt; &#xA;&lt;p&gt;ESM C models, including ESMC 6B, are accessible via EvolutionaryScale Forge. You can request access and utilize these models through forge.evolutionaryscale.ai, as demonstrated in the example below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from esm.sdk.forge import ESM3ForgeInferenceClient&#xA;from esm.sdk.api import ESMProtein, LogitsConfig&#xA;&#xA;# Apply for forge access and get an access token&#xA;forge_client = ESM3ForgeInferenceClient(model=&#34;esmc-6b-2024-12&#34;, url=&#34;https://forge.evolutionaryscale.ai&#34;, token=&#34;&amp;lt;your forge token&amp;gt;&#34;)&#xA;protein_tensor = forge_client.encode(protein)&#xA;logits_output = forge_client.logits(&#xA;   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)&#xA;)&#xA;print(logits_output.logits, logits_output.embeddings)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using ESM C 6B via SageMaker&lt;/h3&gt; &#xA;&lt;p&gt;ESM C models are also available on Amazon SageMaker. They function similarly to the ESM3 model family, and you can refer to the sample notebooks provided in this repository for examples.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll need an admin AWS access to an AWS account to follow these instructions. To deploy, first we need to deploy the AWS package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find the ESM C model version you want to subscribe to. All of our offerings are visible &lt;a href=&#34;https://aws.amazon.com/marketplace/seller-profile?id=seller-iw2nbscescndm&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click the name of the model version you are interested in, review pricing information and the end user license agreement (EULA), then click &#34;Continue to Subscribe&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Once you have subscribed, you should be able to see our model under your &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/marketplace/home#/subscriptions&#34;&gt;marketplace subscriptions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click the product name and then from the &#34;Actions&#34; dropdown select &#34;Configure&#34;.&lt;/li&gt; &#xA; &lt;li&gt;You will next see the &#34;Configure and Launch&#34; UI. There are multiple deployment paths - we recommend using &#34;AWS CloudFormation&#34;.&lt;/li&gt; &#xA; &lt;li&gt;The default value for &#34;Service Access&#34; may or may not work. We recommend clicking &#34;Create and use a new service role&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Launch CloudFormation Template&#34;. This takes 15 to 25 minutes depending on model size.&lt;/li&gt; &#xA; &lt;li&gt;On the &#34;Quick create stack&#34; page, ensure the stack name and endpoint names are not already used. You can check existing stack names &lt;a href=&#34;https://console.aws.amazon.com/cloudformation/home/stacks&#34;&gt;here&lt;/a&gt; and existing endpoint names &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The Sagemaker deployment of the model now lives on a dedicated GPU instance inside your AWS environment, and will be billed directly to your AWS account. Make sure to remember to shut down the instance after you stop using it. Find the CloudFormation stack you created &lt;a href=&#34;https://us-east-1.console.aws.amazon.com/cloudformation/home&#34;&gt;here&lt;/a&gt;, select it, and then click &#34;Delete&#34; to clean up all resources.&lt;/p&gt; &#xA;&lt;p&gt;After creating the endpoint, you can create a sagemaker client and use it the same way as a forge client. They share the same API.&lt;/p&gt; &#xA;&lt;p&gt;Ensure that the code below runs in an environment that has AWS credentials available for the account which provisioned SageMaker resources. Learn more about general AWS credential options &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-authentication.html#cli-chap-authentication-precedence&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from esm.sdk.sagemaker import ESM3SageMakerClient&#xA;from esm.sdk.api import ESMProtein, LogitsConfig&#xA;&#xA;sagemaker_client = ESM3SageMakerClient(&#xA;   # E.g. &#34;Endpoint-ESMC-6B-1&#34;&#xA;   endpoint_name=SAGE_ENDPOINT_NAME,&#xA;   # E.g. &#34;esmc-6b-2024-12&#34;. Same model names as in Forge.&#xA;   model=MODEL_NAME,&#xA;)&#xA;&#xA;protein = ESMProtein(sequence=&#34;AAAAA&#34;)&#xA;protein_tensor = sagemaker_client.encode(protein)&#xA;logits_output = sagemaker_client.logits(&#xA;   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)&#xA;)&#xA;print(logits_output.logits, logits_output.embeddings)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ESM 3 &lt;a name=&#34;esm3&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.evolutionaryscale.ai/papers/esm3-simulating-500-million-years-of-evolution-with-a-language-model&#34;&gt;ESM3&lt;/a&gt; is a frontier generative model for biology, able to jointly reason across three fundamental biological properties of proteins: sequence, structure, and function. These three data modalities are represented as tracks of discrete tokens at the input and output of ESM3. You can present the model with a combination of partial inputs across the tracks, and ESM3 will provide output predictions for all the tracks.&lt;/p&gt; &#xA;&lt;p&gt;ESM3 is a &lt;em&gt;generative&lt;/em&gt; masked language model. You can prompt it with partial sequence, structure, and function keywords, and iteratively sample masked positions until all positions are unmasked. This iterative sampling is what the &lt;code&gt;.generate()&lt;/code&gt; function does.&lt;/p&gt; &#xA;&lt;!--![ESM3 Diagram](_assets/esm3_diagram.png)--&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/_assets/esm3_diagram.png&#34; alt=&#34;ESM3 Diagram&#34; width=&#34;400&#34;&gt; &#xA;&lt;p&gt;The ESM3 architecture is highly scalable due to its transformer backbone and all-to-all reasoning over discrete token sequences. At its largest scale, ESM3 was trained with 1.07e24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters. Learn more by reading the &lt;a href=&#34;https://www.evolutionaryscale.ai/blog/esm3-release&#34;&gt;blog post&lt;/a&gt; and &lt;a href=&#34;https://www.evolutionaryscale.ai/papers/esm3-simulating-500-million-years-of-evolution-with-a-language-model&#34;&gt;the pre-print (Hayes et al., 2024)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here we present &lt;code&gt;esm3-open-small&lt;/code&gt;. With 1.4B parameters it is the smallest and fastest model in the family. ESM3-open is available under the &lt;a href=&#34;https://www.evolutionaryscale.ai/policies/cambrian-non-commercial-license-agreement&#34;&gt;Cambrian non-commercial license agreement&lt;/a&gt;, as outlined in &lt;code&gt;LICENSE.md&lt;/code&gt; (note: updated with ESM C release). Visit our &lt;a href=&#34;https://github.com/evolutionaryscale/esm/discussions&#34;&gt;Discussions page&lt;/a&gt; to get in touch, provide feedback, ask questions or share your experience with ESM3!&lt;/p&gt; &#xA;&lt;h3&gt;Quickstart for ESM3-open&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install esm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to download the weights, we require users to accept our non-commercial license. The weights are stored on HuggingFace Hub under &lt;a href=&#34;https://huggingface.co/EvolutionaryScale/esm3&#34;&gt;HuggingFace/EvolutionaryScale/esm3&lt;/a&gt;. Please create an account and accept the license.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from huggingface_hub import login&#xA;from esm.models.esm3 import ESM3&#xA;from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig&#xA;&#xA;# Will instruct you how to get an API key from huggingface hub, make one with &#34;Read&#34; permission.&#xA;login()&#xA;&#xA;# This will download the model weights and instantiate the model on your machine.&#xA;model: ESM3InferenceClient = ESM3.from_pretrained(&#34;esm3-open&#34;).to(&#34;cuda&#34;) # or &#34;cpu&#34;&#xA;&#xA;# Generate a completion for a partial Carbonic Anhydrase (2vvb)&#xA;prompt = &#34;___________________________________________________DQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPP___________________________________________________________&#34;&#xA;protein = ESMProtein(sequence=prompt)&#xA;# Generate the sequence, then the structure. This will iteratively unmask the sequence track.&#xA;protein = model.generate(protein, GenerationConfig(track=&#34;sequence&#34;, num_steps=8, temperature=0.7))&#xA;# We can show the predicted structure for the generated sequence.&#xA;protein = model.generate(protein, GenerationConfig(track=&#34;structure&#34;, num_steps=8))&#xA;protein.to_pdb(&#34;./generation.pdb&#34;)&#xA;# Then we can do a round trip design by inverse folding the sequence and recomputing the structure&#xA;protein.sequence = None&#xA;protein = model.generate(protein, GenerationConfig(track=&#34;sequence&#34;, num_steps=8))&#xA;protein.coordinates = None&#xA;protein = model.generate(protein, GenerationConfig(track=&#34;structure&#34;, num_steps=8))&#xA;protein.to_pdb(&#34;./round_tripped.pdb&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Congratulations, you just generated your first proteins with ESM3! Let&#39;s explore some more advanced prompting with the help of our &lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/examples/&#34;&gt;notebooks and scripts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;generate.ipynb&lt;/code&gt; will walk through two prompting examples (scaffolding and secondary structure editing) using the open model: &lt;a href=&#34;https://colab.research.google.com/github/evolutionaryscale/esm/blob/main/examples/generate.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;gfp_design.ipynb&lt;/code&gt; will walk through the more complex generation procedure we used to design esmGFP: &lt;a href=&#34;https://colab.research.google.com/github/evolutionaryscale/esm/blob/main/examples/gfp_design.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also provide example scripts that show common workflows under &lt;code&gt;examples/&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/examples/local_generate.py&#34;&gt;local_generate.py&lt;/a&gt; shows how simple and elegant common tasks are: it shows folding, inverse folding and chain of thought generation, all by calling just &lt;code&gt;model.generate()&lt;/code&gt; for iterative decoding.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/examples/seqfun_struct.py&#34;&gt;seqfun_struct.py&lt;/a&gt; shows direct use of the model as a standard pytorch model with a simple model &lt;code&gt;forward&lt;/code&gt; call.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Forge: Access to larger ESM3 models&lt;/h3&gt; &#xA;&lt;p&gt;You can apply for beta access to the full family of larger and higher capability ESM3 models at &lt;a href=&#34;https://forge.evolutionaryscale.ai&#34;&gt;EvolutionaryScale Forge&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We encourage users to interact with the Forge API through the python &lt;code&gt;esm&lt;/code&gt; library instead of the command line. The python interface enables you to interactively load proteins, build prompts, and inspect generated proteins with the &lt;code&gt;ESMProtein&lt;/code&gt; and config classes used to interact with the local model.&lt;/p&gt; &#xA;&lt;p&gt;In any example script try to replace a local &lt;code&gt;ESM3&lt;/code&gt; model with a Forge API client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# Instead of loading the model locally on your machine:&#xA;model: ESM3InferenceClient = ESM3.from_pretrained(&#34;esm3_sm_open_v1&#34;).to(&#34;cuda&#34;) # or &#34;cpu&#34;&#xA;# just replace the line with this:&#xA;model: ESM3InferenceClient = esm.sdk.client(&#34;esm3-medium-2024-08&#34;, token=&#34;&amp;lt;your forge token&amp;gt;&#34;)&#xA;# and now you&#39;re interfacing with the model running on our remote servers.&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and the exact same code will work. This enables a seamless transition from smaller and faster models, to our large 98B protein language models for protein design work.&lt;/p&gt; &#xA;&lt;h2&gt;Responsible Development &lt;a name=&#34;responsible-development&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;EvolutionaryScale is a public benefit company. Our mission is to develop artificial intelligence to understand biology for the benefit of human health and society, through partnership with the scientific community, and open, safe, and responsible research. Inspired by the history of our field as well as &lt;a href=&#34;https://responsiblebiodesign.ai/&#34;&gt;new principles and recommendations&lt;/a&gt;, we have created a Responsible Development Framework to guide our work towards our mission with transparency and clarity.&lt;/p&gt; &#xA;&lt;p&gt;The core tenets of our framework are&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We will communicate the benefits and risks of our research&lt;/li&gt; &#xA; &lt;li&gt;We will proactively and rigorously evaluate the risk of our models before public deployment&lt;/li&gt; &#xA; &lt;li&gt;We will adopt risk mitigation strategies and precautionary guardrails&lt;/li&gt; &#xA; &lt;li&gt;We will work with stakeholders in government, policy, and civil society to keep them informed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With this in mind, we have performed a variety of mitigations for &lt;code&gt;esm3-sm-open-v1&lt;/code&gt;, detailed in our &lt;a href=&#34;https://www.evolutionaryscale.ai/papers/esm3-simulating-500-million-years-of-evolution-with-a-language-model&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Licenses &lt;a name=&#34;licenses&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The code and model weights of ESM3 and ESM C are available under a mixture of non-commercial and more permissive licenses, fully outlined in &lt;a href=&#34;https://raw.githubusercontent.com/evolutionaryscale/esm/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>