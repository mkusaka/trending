<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-09-14T01:37:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nianticlabs/simplerecon</title>
    <updated>2022-09-14T01:37:58Z</updated>
    <id>tag:github.com,2022-09-14:/nianticlabs/simplerecon</id>
    <link href="https://github.com/nianticlabs/simplerecon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ECCV 2022] SimpleRecon: 3D Reconstruction Without 3D Convolutions&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SimpleRecon: 3D Reconstruction Without 3D Convolutions&lt;/h1&gt; &#xA;&lt;p&gt;This is the reference PyTorch implementation for training and testing MVS depth estimation models using the method described in&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;SimpleRecon: 3D Reconstruction Without 3D Convolutions&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nianticlabs/simplerecon/main/masayed.com&#34;&gt;Mohamed Sayed&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/john-e-gibson-ii/&#34;&gt;John Gibson&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jamie-watson-544825127/&#34;&gt;Jamie Whatson&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~victor/&#34;&gt;Victor Adrian Prisacariu&lt;/a&gt;, &lt;a href=&#34;http://www.michaelfirman.co.uk&#34;&gt;Michael Firman&lt;/a&gt;, and &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/C.Godard/&#34;&gt;Cl√©ment Godard&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.14743&#34;&gt;Paper, ECCV 2022 (arXiv pdf)&lt;/a&gt;, &lt;a href=&#34;https://nianticlabs.github.io/simplerecon/resources/SimpleRecon_supp.pdf&#34;&gt;Supplemental Material&lt;/a&gt;, &lt;a href=&#34;https://nianticlabs.github.io/simplerecon/&#34;&gt;Project Page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/nianticlabs/simplerecon/main/media/teaser.jpeg&#34; alt=&#34;example output&#34; width=&#34;720&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/14994206/187824325-e1878d77-2f14-4ea9-bc02-3fded1bb1851.mp4&#34;&gt;https://user-images.githubusercontent.com/14994206/187824325-e1878d77-2f14-4ea9-bc02-3fded1bb1851.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/14994206/189788536-5fa8a1b5-ae8b-4f64-92d6-1ff1abb03eaf.mp4&#34;&gt;https://user-images.githubusercontent.com/14994206/189788536-5fa8a1b5-ae8b-4f64-92d6-1ff1abb03eaf.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This code is for non-commercial use; please see the &lt;a href=&#34;https://raw.githubusercontent.com/nianticlabs/simplerecon/main/LICENSE&#34;&gt;license file&lt;/a&gt; for terms. If you do find any part of this codebase helpful, please cite our paper using the BibTex below and link this repo. Thanks!&lt;/p&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Setup&lt;/h2&gt; &#xA;&lt;p&gt;Assuming a fresh &lt;a href=&#34;https://www.anaconda.com/download/&#34;&gt;Anaconda&lt;/a&gt; distribution, you can install dependencies with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f simplerecon_env.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We ran our experiments with PyTorch 1.10, CUDA 11.3, Python 3.9.7 and Debian GNU/Linux 10.&lt;/p&gt; &#xA;&lt;h2&gt;üì¶ Models&lt;/h2&gt; &#xA;&lt;p&gt;Download a pretrained model into the &lt;code&gt;weights/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;We provide the following models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;--config&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Abs Diff‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;Sq Rel‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;delta &amp;lt; 1.05‚Üë&lt;/th&gt; &#xA;   &lt;th&gt;Chamfer‚Üì&lt;/th&gt; &#xA;   &lt;th&gt;F-Score‚Üë&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1qGkROKhVOyHtBqkSlmM-illihYdr8sZg/view?usp=sharing&#34;&gt;&lt;code&gt;hero_model.yaml&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Metadata + Resnet Matching&lt;/td&gt; &#xA;   &lt;td&gt;0.0885&lt;/td&gt; &#xA;   &lt;td&gt;0.0125&lt;/td&gt; &#xA;   &lt;td&gt;73.16&lt;/td&gt; &#xA;   &lt;td&gt;5.81&lt;/td&gt; &#xA;   &lt;td&gt;0.671&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1qk469gs3kLRi1f5Usx11hVpJDScuegBu/view?usp=sharing&#34;&gt;&lt;code&gt;dot_product_model.yaml&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Dot Product + Resnet Matching&lt;/td&gt; &#xA;   &lt;td&gt;0.0941&lt;/td&gt; &#xA;   &lt;td&gt;0.0139&lt;/td&gt; &#xA;   &lt;td&gt;70.48&lt;/td&gt; &#xA;   &lt;td&gt;6.29&lt;/td&gt; &#xA;   &lt;td&gt;0.642&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;hero_model&lt;/code&gt; is the one we use in the paper as &lt;strong&gt;Ours&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Speed&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;--config&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Inference Speed (&lt;code&gt;--batch_size 1&lt;/code&gt;)&lt;/th&gt; &#xA;   &lt;th&gt;Inference GPU memory&lt;/th&gt; &#xA;   &lt;th&gt;Approximate training time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;hero_model&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Hero, Metadata + Resnet&lt;/td&gt; &#xA;   &lt;td&gt;130ms / 70ms (speed optimized)&lt;/td&gt; &#xA;   &lt;td&gt;2.6GB / 5.7GB (speed optimized)&lt;/td&gt; &#xA;   &lt;td&gt;36 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dot_product_model&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Dot Product + Resnet&lt;/td&gt; &#xA;   &lt;td&gt;80ms&lt;/td&gt; &#xA;   &lt;td&gt;2.6GB&lt;/td&gt; &#xA;   &lt;td&gt;36 hours&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;With larger batches speed increases considerably. With batch size 8 on the non-speed optimized model, the latency drops to ~40ms.&lt;/p&gt; &#xA;&lt;h2&gt;üìù TODOs:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Simple scan for folks to quickly try the code, instead of downloading the ScanNetv2 test scenes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; FPN model weights.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üèÉ Running out of the box!&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve now included two scans for people to try out immediately with the code. You can download these scans &lt;a href=&#34;https://drive.google.com/file/d/1x-auV7vGCMdu5yZUMPcoP83p77QOuasT/view?usp=sharing&#34;&gt;from here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download weights for the &lt;code&gt;hero_model&lt;/code&gt; into the weights directory.&lt;/li&gt; &#xA; &lt;li&gt;Download the scans and unzip them to a directory of your choosing.&lt;/li&gt; &#xA; &lt;li&gt;Modify the value for the option &lt;code&gt;dataset_path&lt;/code&gt; in &lt;code&gt;configs/data/vdr_dense.yaml&lt;/code&gt; to the base path of the unzipped vdr folder.&lt;/li&gt; &#xA; &lt;li&gt;You should be able to run it! Something like this will work:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python test.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/vdr_dense.yaml \&#xA;            --num_workers 8 \&#xA;            --batch_size 2 \&#xA;            --fast_cost_volume \&#xA;            --run_fusion \&#xA;            --depth_fuser open3d \&#xA;            --fuse_color \&#xA;            --dump_depth_visualization;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will output meshes, quick depth viz, and socres when benchmarked against LiDAR depth under &lt;code&gt;OUTPUT_PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This command uses &lt;code&gt;vdr_dense.yaml&lt;/code&gt; which will generate depths for every frame and fuse them into a mesh. In the paper we report scores with fused keyframes instead, and you can run those using &lt;code&gt;vdr_default.yaml&lt;/code&gt;. You can also use &lt;code&gt;dense_offline&lt;/code&gt; tuples by instead using &lt;code&gt;vdr_dense_offline.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the section below on testing and evaluation. Make sure to use the correct config flags for datasets.&lt;/p&gt; &#xA;&lt;h2&gt;üíæ ScanNetv2 Dataset&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the instructions &lt;a href=&#34;https://github.com/ScanNet/ScanNet&#34;&gt;here&lt;/a&gt; to download the dataset. This dataset is quite big (&amp;gt;2TB), so make sure you have enough space, especially for extracting files.&lt;/p&gt; &#xA;&lt;p&gt;Once downloaded, use this &lt;a href=&#34;https://github.com/ScanNet/ScanNet/tree/master/SensReader/python&#34;&gt;script&lt;/a&gt; to export raw sensor data to images and depth files.&lt;/p&gt; &#xA;&lt;p&gt;You should change the &lt;code&gt;dataset_path&lt;/code&gt; config argument for ScanNetv2 data configs at &lt;code&gt;configs/data/&lt;/code&gt; to match where your dataset is.&lt;/p&gt; &#xA;&lt;p&gt;The codebase expects ScanNetv2 to be in the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dataset_path&#xA;    scans_test (test scans)&#xA;        scene0707&#xA;            scene0707_00_vh_clean_2.ply (gt mesh)&#xA;            sensor_data&#xA;                frame-000261.pose.txt&#xA;                frame-000261.color.jpg &#xA;                frame-000261.color.512.png (optional, image at 512x384)&#xA;                frame-000261.color.640.png (optional, image at 640x480)&#xA;                frame-000261.depth.png (full res depth, stored scale *1000)&#xA;                frame-000261.depth.256.png (optional, depth at 256x192 also&#xA;                                            scaled)&#xA;            scene0707.txt (scan metadata and intrinsics)&#xA;        ...&#xA;    scans (val and train scans)&#xA;        scene0000_00&#xA;            (see above)&#xA;        scene0000_01&#xA;        ....&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example &lt;code&gt;scene0707.txt&lt;/code&gt; should contain the scan&#39;s metadata and intrinsics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    colorHeight = 968&#xA;    colorToDepthExtrinsics = 0.999263 -0.010031 0.037048 ........&#xA;    colorWidth = 1296&#xA;    depthHeight = 480&#xA;    depthWidth = 640&#xA;    fx_color = 1170.187988&#xA;    fx_depth = 570.924255&#xA;    fy_color = 1170.187988&#xA;    fy_depth = 570.924316&#xA;    mx_color = 647.750000&#xA;    mx_depth = 319.500000&#xA;    my_color = 483.750000&#xA;    my_depth = 239.500000&#xA;    numColorFrames = 784&#xA;    numDepthFrames = 784&#xA;    numIMUmeasurements = 1632&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;frame-000261.pose.txt&lt;/code&gt; should contain pose in the form:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    -0.384739 0.271466 -0.882203 4.98152&#xA;    0.921157 0.0521417 -0.385682 1.46821&#xA;    -0.0587002 -0.961035 -0.270124 1.51837&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;frame-000261.color.512.png&lt;/code&gt; and &lt;code&gt;frame-000261.color.640.png&lt;/code&gt; are precached resized versions of the original image to save load and compute time during training and testing. &lt;code&gt;frame-000261.depth.256.png&lt;/code&gt; is also a precached resized version of the depth map.&lt;/p&gt; &#xA;&lt;p&gt;All resized precached versions of depth and images are nice to have but not required. If they don&#39;t exist, the full resolution versions will be loaded, and downsampled on the fly.&lt;/p&gt; &#xA;&lt;h2&gt;üñºÔ∏èüñºÔ∏èüñºÔ∏è Frame Tuples&lt;/h2&gt; &#xA;&lt;p&gt;By default, we estimate a depth map for each keyframe in a scan. We use DeepVideoMVS&#39;s heuristic for keyframe separation and construct tuples to match. We use the depth maps at these keyframes for depth fusion. For each keyframe, we associate a list of source frames that will be used to build the cost volume. We also use dense tuples, where we predict a depth map for each frame in the data, and not just at specific keyframes; these are mostly used for visualization.&lt;/p&gt; &#xA;&lt;p&gt;We generate and export a list of tuples across all scans that act as the dataset&#39;s elements. We&#39;ve precomputed these lists and they are available at &lt;code&gt;data_splits&lt;/code&gt; under each dataset&#39;s split. For ScanNet&#39;s test scans they are at &lt;code&gt;data_splits/ScanNetv2/standard_split&lt;/code&gt;. Our core depth numbers are computed using &lt;code&gt;data_splits/ScanNetv2/standard_split/test_eight_view_deepvmvs.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a quick taxonamy of the type of tuples for test:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;default&lt;/code&gt;: a tuple for every keyframe following DeepVideoMVS where all source frames are in the past. Used for all depth and mesh evaluation unless stated otherwise. For ScanNet use &lt;code&gt;data_splits/ScanNetv2/standard_split/test_eight_view_deepvmvs.txt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;offline&lt;/code&gt;: a tuple for every frame in the scan where source frames can be both in the past and future relative to the current frame. These are useful when a scene is captured offline, and you want the best accuracy possible. With online tuples, the cost volume will contain empty regions as the camera moves away and all source frames lag behind; however with offline tuples, the cost volume is full on both ends, leading to a better scale (and metric) estimate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dense&lt;/code&gt;: an online tuple (like default) for every frame in the scan where all source frames are in the past. For ScanNet this would be &lt;code&gt;data_splits/ScanNetv2/standard_split/test_eight_view_deepvmvs_dense.txt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;offline&lt;/code&gt;: an offline tuple for every keyframefor every keyframe in the scan.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For the train and validation sets, we follow the same tuple augmentation strategy as in DeepVideoMVS and use the same core generation script.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to generate these tuples yourself, you can use the scripts at &lt;code&gt;data_scripts/generate_train_tuples.py&lt;/code&gt; for train tuples and &lt;code&gt;data_scripts/generate_test_tuples.py&lt;/code&gt; for test tuples. These follow the same config format as &lt;code&gt;test.py&lt;/code&gt; and will use whatever dataset class you build to read pose informaiton.&lt;/p&gt; &#xA;&lt;p&gt;Example for test:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# default tuples&#xA;python ./data_scripts/generate_test_tuples.py &#xA;    --data_config configs/data/scannet_default_test.yaml&#xA;    --num_workers 16&#xA;&#xA;# dense tuples&#xA;python ./data_scripts/generate_test_tuples.py &#xA;    --data_config configs/data/scannet_dense_test.yaml&#xA;    --num_workers 16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Examples for train:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# train&#xA;python ./data_scripts/generate_train_tuples.py &#xA;    --data_config configs/data/scannet_default_train.yaml&#xA;    --num_workers 16&#xA;&#xA;# val&#xA;python ./data_scripts/generate_val_tuples.py &#xA;    --data_config configs/data/scannet_default_val.yaml&#xA;    --num_workers 16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These scripts will first check each frame in the dataset to make sure it has an existing RGB frame, an existing depth frame (if appropriate for the dataset), and also an existing and valid pose file. It will save these &lt;code&gt;valid_frames&lt;/code&gt; in a text file in each scan&#39;s folder, but if the directory is read only, it will ignore saving a &lt;code&gt;valid_frames&lt;/code&gt; file and generate tuples anyway.&lt;/p&gt; &#xA;&lt;h2&gt;üìä Testing and Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;test.py&lt;/code&gt; for inferring and evaluating depth maps and fusing meshes.&lt;/p&gt; &#xA;&lt;p&gt;All results will be stored at a base results folder (results_path) at:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;opts.output_base_path/opts.name/opts.dataset/opts.frame_tuple_type/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where opts is the &lt;code&gt;options&lt;/code&gt; class. For example, when &lt;code&gt;opts.output_base_path&lt;/code&gt; is &lt;code&gt;./results&lt;/code&gt;, &lt;code&gt;opts.name&lt;/code&gt; is &lt;code&gt;HERO_MODEL&lt;/code&gt;, &lt;code&gt;opts.dataset&lt;/code&gt; is &lt;code&gt;scannet&lt;/code&gt;, and &lt;code&gt;opts.frame_tuple_type&lt;/code&gt; is &lt;code&gt;default&lt;/code&gt;, the output directory will be&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./results/HERO_MODEL/scannet/default/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure to set &lt;code&gt;--opts.output_base_path&lt;/code&gt; to a directory suitable for you to store results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--frame_tuple_type&lt;/code&gt; is the type of image tuple used for MVS. A selection should be provided in the &lt;code&gt;data_config&lt;/code&gt; file you used.&lt;/p&gt; &#xA;&lt;p&gt;By default &lt;code&gt;test.py&lt;/code&gt; will attempt to compute depth scores for each frame and provide both frame averaged and scene averaged metrics. The script will save these scores (per scene and totals) under &lt;code&gt;results_path/scores&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve done our best to ensure that a torch batching bug through the matching encoder is fixed for (&amp;lt;10^-4) accurate testing by disabling image batching through that encoder. Run &lt;code&gt;--batch_size 4&lt;/code&gt; at most if in doubt, and if you&#39;re looking to get as stable as possible numbers and avoid PyTorch gremlins, use &lt;code&gt;--batch_size 1&lt;/code&gt; for comparison evaluation.&lt;/p&gt; &#xA;&lt;p&gt;If you want to use this for speed, set &lt;code&gt;--fast_cost_volume&lt;/code&gt; to True. This will enable batching through the matching encoder and will enable an einops optimized feature volume.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example command to just compute scores &#xA;CUDA_VISIBLE_DEVICES=0 python test.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_default_test.yaml \&#xA;            --num_workers 8 \&#xA;            --batch_size 4;&#xA;&#xA;# If you&#39;d like to get a super fast version use:&#xA;CUDA_VISIBLE_DEVICES=0 python test.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_default_test.yaml \&#xA;            --num_workers 8 \&#xA;            --fast_cost_volume \&#xA;            --batch_size 2;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script can also be used to perform a few different auxiliary tasks, including:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TSDF Fusion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run TSDF fusion provide the &lt;code&gt;--run_fusion&lt;/code&gt; flag. You have two choices for fusers&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;--depth_fuser ours&lt;/code&gt; (default) will use our fuser, whose meshes are used in most visualizations and for scores. This fuser does not support color. We&#39;ve provided a custom branch of scikit-image with our custom implementation of &lt;code&gt;measure.matching_cubes&lt;/code&gt; that allows single walled. We use single walled meshes for evaluation. If this is isn&#39;t important to you, you can set the export_single_mesh to &lt;code&gt;False&lt;/code&gt; for call to &lt;code&gt;export_mesh&lt;/code&gt; in &lt;code&gt;test.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--depth_fuser open3d&lt;/code&gt; will use the open3d depth fuser. This fuser supports color and you can enable this by using the &lt;code&gt;--fuse_color&lt;/code&gt; flag.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;By default, depth maps will be clipped to 3m for fusion and a tsdf resolution of 0.04m&lt;sup&gt;3&lt;/sup&gt; will be used, but you can change that by changing both &lt;code&gt;--max_fusion_depth&lt;/code&gt; and &lt;code&gt;--fusion_resolution&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can optionnally ask for predicted depths used for fusion to be masked when no vaiid MVS information exists using &lt;code&gt;--mask_pred_depths&lt;/code&gt;. This is not enabled by default.&lt;/p&gt; &#xA;&lt;p&gt;You can also fuse the best guess depths from the cost volume before the cost volume encoder-decoder that introduces a strong image prior. You can do this by using &lt;code&gt;--fusion_use_raw_lowest_cost&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Meshes will be stored under &lt;code&gt;results_path/meshes/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example command to fuse depths to get meshes&#xA;CUDA_VISIBLE_DEVICES=0 python test.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_default_test.yaml \&#xA;            --num_workers 8 \&#xA;            --run_fusion \&#xA;            --batch_size 8;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cache depths&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can optionally store depths by providing the &lt;code&gt;--cache_depths&lt;/code&gt; flag. They will be stored at &lt;code&gt;results_path/depths&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example command to compute scores and cache depths&#xA;CUDA_VISIBLE_DEVICES=0 python test.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_default_test.yaml \&#xA;            --num_workers 8 \&#xA;            --cache_depths \&#xA;            --batch_size 8;&#xA;&#xA;# Example command to fuse depths to get color meshes&#xA;CUDA_VISIBLE_DEVICES=0 python test.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_default_test.yaml \&#xA;            --num_workers 8 \&#xA;            --run_fusion \&#xA;            --depth_fuser open3d \&#xA;            --fuse_color \&#xA;            --batch_size 4;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick viz&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are other scripts for deeper visualizations of output depths and fusion, but for quick export of depth map visualization you can use &lt;code&gt;--dump_depth_visualization&lt;/code&gt;. Visualizations will be stored at &lt;code&gt;results_path/viz/quick_viz/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example command to output quick depth visualizations&#xA;CUDA_VISIBLE_DEVICES=0 python test.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_default_test.yaml \&#xA;            --num_workers 8 \&#xA;            --dump_depth_visualization \&#xA;            --batch_size 4;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üëâ‚òÅÔ∏è Point Cloud Fusion&lt;/h2&gt; &#xA;&lt;p&gt;We also allow point cloud fusion of depth maps using the fuser from 3DVNet&#39;s &lt;a href=&#34;https://github.com/alexrich021/3dvnet/raw/main/mv3d/eval/pointcloudfusion_custom.py&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example command to fuse depths into point clouds.&#xA;CUDA_VISIBLE_DEVICES=0 python pc_fusion.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_dense_test.yaml \&#xA;            --num_workers 8 \&#xA;            --batch_size 4;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change &lt;code&gt;configs/data/scannet_dense_test.yaml&lt;/code&gt; to &lt;code&gt;configs/data/scannet_default_test.yaml&lt;/code&gt; to use keyframes only if you don&#39;t want to wait too long.&lt;/p&gt; &#xA;&lt;h2&gt;üìä Mesh Metrics&lt;/h2&gt; &#xA;&lt;p&gt;We use TransformerFusion&#39;s &lt;a href=&#34;https://github.com/AljazBozic/TransformerFusion/raw/main/src/evaluation/eval.py&#34;&gt;mesh evaluation&lt;/a&gt; for our main results table but set the seed to a fixed value for consistency when randomly sampling meshes. We also report mesh metrics using NeuralRecon&#39;s &lt;a href=&#34;https://github.com/zju3dv/NeuralRecon/raw/master/tools/evaluation.py&#34;&gt;evaluation&lt;/a&gt; in the supplemental material.&lt;/p&gt; &#xA;&lt;p&gt;For point cloud evaluation, we use TransformerFusion&#39;s code but load in a point cloud in place of sampling a mesh&#39;s surface.&lt;/p&gt; &#xA;&lt;h2&gt;‚è≥ Training&lt;/h2&gt; &#xA;&lt;p&gt;By default models and tensorboard event files are saved to &lt;code&gt;~/tmp/tensorboard/&amp;lt;model_name&amp;gt;&lt;/code&gt;. This can be changed with the &lt;code&gt;--log_dir&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;p&gt;We train with a batch_size of 16 with 16-bit precision on two A100s on the default ScanNetv2 split.&lt;/p&gt; &#xA;&lt;p&gt;Example command to train with two GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python train.py --name HERO_MODEL \&#xA;            --log_dir logs \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --data_config configs/data/scannet_default_train.yaml \&#xA;            --gpus 2 \&#xA;            --batch_size 16;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The code supports any number of GPUs for training. You can specify which GPUs to use with the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment.&lt;/p&gt; &#xA;&lt;p&gt;All our training runs were performed on two NVIDIA A100s.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Different dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can train on a custom MVS dataset by writing a new dataloader class which inherits from &lt;code&gt;GenericMVSDataset&lt;/code&gt; at &lt;code&gt;datasets/generic_mvs_dataset.py&lt;/code&gt;. See the &lt;code&gt;ScannetDataset&lt;/code&gt; class in &lt;code&gt;datasets/scannet_dataset.py&lt;/code&gt; or indeed any other class in &lt;code&gt;datasets&lt;/code&gt; for an example.&lt;/p&gt; &#xA;&lt;h3&gt;üéõÔ∏è Finetuning a pretrained model&lt;/h3&gt; &#xA;&lt;p&gt;To finetune, simple load a checkpoint (not resume!) and train from there:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 python train.py --config configs/models/hero_model.yaml&#xA;                --data_config configs/data/scannet_default_train.yaml &#xA;                --load_weights_from_checkpoint weights/hero_model.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change the data configs to whatever dataset you want to finetune to.&lt;/p&gt; &#xA;&lt;h2&gt;üîß Other training and testing options&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;code&gt;options.py&lt;/code&gt; for the range of other training options, such as learning rates and ablation settings, and testing options.&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Visualization&lt;/h2&gt; &#xA;&lt;p&gt;Other than quick depth visualization in the &lt;code&gt;test.py&lt;/code&gt; script, there are two scripts for visualizing depth output.&lt;/p&gt; &#xA;&lt;p&gt;The first is &lt;code&gt;visualization_scripts/visualize_scene_depth_output.py&lt;/code&gt;. This will produce a video with color images of the reference and source frames, depth prediction, cost volume estimate, GT depth, and estimated normals from depth. The script assumes you have cached depth output using &lt;code&gt;test.py&lt;/code&gt; and accepts the same command template format as &lt;code&gt;test.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Example command to get visualizations for dense frames&#xA;CUDA_VISIBLE_DEVICES=0 python ./visualization_scripts/visualize_scene_depth_output.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --data_config configs/data/scannet_dense_test.yaml \&#xA;            --num_workers 8;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;OUTPUT_PATH&lt;/code&gt; is the base results directory for SimpleRecon (what you used for test to begin with). You could optionally run &lt;code&gt;.visualization_scripts/generate_gt_min_max_cache.py&lt;/code&gt; before this script to get a scene average for the min and max depth values used for colormapping; if those aren&#39;t available, the script will use 0m and 5m for colomapping min and max.&lt;/p&gt; &#xA;&lt;p&gt;The second allows a live visualization of meshing. This script will use cached depth maps if available, otherwise it will use the model to predict them before fusion. The script will iteratively load in a depth map, fuse it, save a mesh file at this step, and render this mesh alongside a camera marker for the birdseye video, and from the point of view of the camera for the fpv video.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Example command to get live visualizations for mesh reconstruction&#xA;CUDA_VISIBLE_DEVICES=0 python visualize_live_meshing.py --name HERO_MODEL \&#xA;            --output_base_path OUTPUT_PATH \&#xA;            --config_file configs/models/hero_model.yaml \&#xA;            --load_weights_from_checkpoint weights/hero_model.ckpt \&#xA;            --data_config configs/data/scannet_dense_test.yaml \&#xA;            --num_workers 8;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default the script will save meshes to an intermediate location, and you can optionally load those meshes to save time when visualizing the same meshes again by passing &lt;code&gt;--use_precomputed_partial_meshes&lt;/code&gt;. All intermediate meshes will have had to be computed on the previous run for this to work.&lt;/p&gt; &#xA;&lt;h2&gt;üìùüßÆüë©‚Äçüíª Notation for Transformation Matrices&lt;/h2&gt; &#xA;&lt;p&gt;This repo uses the notation &#34;cam_T_world&#34; to denote a transformation from world to camera points (extrinsics). The intention is to make it so that the coordinate frame names would match on either side of the variable when used in multiplication:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cam_points = cam_T_world @ world_points&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;world_T_cam&lt;/code&gt; denotes camera pose (from cam to world coords). &lt;code&gt;ref_T_src&lt;/code&gt; denotes a transformation from a source to a reference view.&lt;/p&gt; &#xA;&lt;h2&gt;üó∫Ô∏è World Coordinate System&lt;/h2&gt; &#xA;&lt;p&gt;This repo is geared towards ScanNet, so while its functionality should allow for any coordinate system (signaled via input flags), the model weights we provide assume a ScanNet coordinate system. This is important since we include ray information as part of metadata. Other datasets used with these weights should be transformed to the ScanNet system. The dataset classes we include will perform the appropriate transforms.&lt;/p&gt; &#xA;&lt;h2&gt;üôè Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank Alja≈æ Bo≈æiƒç of &lt;a href=&#34;https://github.com/AljazBozic/TransformerFusion&#34;&gt;TransformerFusion&lt;/a&gt;, Jiaming Sun of &lt;a href=&#34;https://zju3dv.github.io/neuralrecon/&#34;&gt;Neural Recon&lt;/a&gt;, and Arda D√ºz√ßeker of &lt;a href=&#34;https://github.com/ardaduz/deep-video-mvs&#34;&gt;DeepVideoMVS&lt;/a&gt; for quickly providing useful information to help with baselines and for making their codebases readily available, especially on short notice.&lt;/p&gt; &#xA;&lt;p&gt;The tuple generation scripts make heavy use of a modified version of DeepVideoMVS&#39;s &lt;a href=&#34;https://github.com/ardaduz/deep-video-mvs/raw/master/dvmvs/keyframe_buffer.py&#34;&gt;Keyframe buffer&lt;/a&gt; (thanks again Arda and co!).&lt;/p&gt; &#xA;&lt;p&gt;The PyTorch point cloud fusion module at &lt;code&gt;torch_point_cloud_fusion&lt;/code&gt; code is borrowed from 3DVNet&#39;s &lt;a href=&#34;https://github.com/alexrich021/3dvnet/raw/main/mv3d/eval/pointcloudfusion_custom.py&#34;&gt;repo&lt;/a&gt;. Thanks Alexander Rich!&lt;/p&gt; &#xA;&lt;p&gt;We&#39;d also like to thank Niantic&#39;s infrastructure team for quick actions when we needed them. Thanks folks!&lt;/p&gt; &#xA;&lt;p&gt;Mohamed is funded by a Microsoft Research PhD Scholarship (MRL 2018-085).&lt;/p&gt; &#xA;&lt;h2&gt;üìú BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{sayed2022simplerecon,&#xA;  title={SimpleRecon: 3D Reconstruction Without 3D Convolutions},&#xA;  author={Sayed, Mohamed and Gibson, John and Watson, Jamie and Prisacariu, Victor and Firman, Michael and Godard, Cl{\&#39;e}ment},&#xA;  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},&#xA;  year={2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üë©‚Äç‚öñÔ∏è License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright ¬© Niantic, Inc. 2022. Patent Pending. All rights reserved. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/nianticlabs/simplerecon/main/LICENSE&#34;&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>drpancake/chard</title>
    <updated>2022-09-14T01:37:58Z</updated>
    <id>tag:github.com,2022-09-14:/drpancake/chard</id>
    <link href="https://github.com/drpancake/chard" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple async/await task queue for Django. One process, no threads, no other dependencies.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chard&lt;/h1&gt; &#xA;&lt;p&gt;Chard is a very simple async/await background task queue for Django. One process, no threads, no other dependencies.&lt;/p&gt; &#xA;&lt;p&gt;It uses Django&#39;s ORM to keep track of tasks. Not very efficient or battle tested. PRs are welcome!&lt;/p&gt; &#xA;&lt;p&gt;üîó &lt;a href=&#34;https://github.com/drpancake/chard-django-example&#34;&gt;Check the example Django project&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8+&lt;/li&gt; &#xA; &lt;li&gt;Django 4.1+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install django-chard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;First add &lt;code&gt;chard&lt;/code&gt; anywhere in your &lt;code&gt;INSTALLED_APPS&lt;/code&gt; setting and then run the migrations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python manage.py migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a file called &lt;code&gt;tasks.py&lt;/code&gt; in one of your apps and define a task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import chard&#xA;import httpx&#xA;from asgiref.sync import sync_to_async&#xA;&#xA;from .models import MyModel&#xA;&#xA;@chard.task&#xA;async def my_task(country_code):&#xA;    url = f&#34;https://somewhere.com/some-api.json?country_code={country_code}&#34;&#xA;    async with httpx.AsyncClient() as client:&#xA;        resp = await client.get(url)&#xA;        obj = resp.json()&#xA;    for item in obj[&#34;items&#34;]:&#xA;        await sync_to_async(MyModel.objects.create)(&#xA;          country_code=country_code,&#xA;          item=item&#xA;        )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To fire a task for the worker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Note that all arguments must be JSON serializable.&#xA;my_task.send(&#34;gb&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the worker process and it will watch for new pending tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python manage.py chardworker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;You can optionally place these in your &lt;code&gt;settings.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# How many tasks can run concurrently (default: 10)&#xA;CHARD_MAX_CONCURRENT_TASKS = 50&#xA;&#xA;# How long a task can run until it is forcibly canceled - setting this to&#xA;# to 0 means no timeout (default: 60)&#xA;CHARD_TIMEOUT = 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Return &lt;code&gt;Task.id&lt;/code&gt; when firing a task so that you can check its status&lt;/li&gt; &#xA; &lt;li&gt;Figure out how to continually check for new pending tasks without hammering the DB too much&lt;/li&gt; &#xA; &lt;li&gt;Error reporting interface to hook up e.g. Sentry&lt;/li&gt; &#xA; &lt;li&gt;Simple monitoring dashboard&lt;/li&gt; &#xA; &lt;li&gt;Task scheduling&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>httpie/httpie</title>
    <updated>2022-09-14T01:37:58Z</updated>
    <id>tag:github.com,2022-09-14:/httpie/httpie</id>
    <link href="https://github.com/httpie/httpie" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü•ß HTTPie for Terminal ‚Äî modern, user-friendly command-line HTTP client for the API era. JSON support, colors, sessions, downloads, plugins &amp; more.&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://httpie.io&#34; target=&#34;blank_&#34;&gt; &lt;img height=&#34;100&#34; alt=&#34;HTTPie&#34; src=&#34;https://raw.githubusercontent.com/httpie/httpie/master/docs/httpie-logo.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;br&gt; HTTPie: human-friendly CLI HTTP client for the API era &lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://httpie.io/product&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=HTTPie&amp;amp;message=for%20Desktop&amp;amp;color=4B78E6&#34; alt=&#34;HTTPie for Desktop&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://httpie.io/app&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=HTTPie&amp;amp;message=for%20Web%20%26%20Mobile&amp;amp;color=73DC8C&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://httpie.io/cli&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=HTTPie&amp;amp;message=for%20Terminal&amp;amp;color=FA9BFA&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/httpie&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/httpie?style=flat&amp;amp;color=%234B78E6&amp;amp;logoColor=%234B78E6&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://httpie.io/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/725351238698270761?style=flat&amp;amp;label=Chat%20on%20Discord&amp;amp;color=%23FA9BFA&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://httpie.org/docs/cli&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable%20docs-httpie.io%2Fdocs%2Fcli-brightgreen?style=flat&amp;amp;color=%2373DC8C&amp;amp;label=Docs&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/httpie&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/httpie.svg?style=flat&amp;amp;label=Latest&amp;amp;color=%234B78E6&amp;amp;logo=&amp;amp;logoColor=white&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/httpie/httpie/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/httpie/httpie/Build?color=%23FA9BFA&amp;amp;label=Build&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/httpie/httpie&#34;&gt;&lt;img src=&#34;https://img.shields.io/codecov/c/github/httpie/httpie?style=flat&amp;amp;label=Coverage&amp;amp;color=%2373DC8C&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;HTTPie (pronounced &lt;em&gt;aitch-tee-tee-pie&lt;/em&gt;) is a command-line HTTP client. Its goal is to make CLI interaction with web services as human-friendly as possible. HTTPie is designed for testing, debugging, and generally interacting with APIs &amp;amp; HTTP servers. The &lt;code&gt;http&lt;/code&gt; &amp;amp; &lt;code&gt;https&lt;/code&gt; commands allow for creating and sending arbitrary HTTP requests. They use simple and natural syntax and provide formatted and colorized output.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/httpie/httpie/master/docs/httpie-animation.gif&#34; alt=&#34;HTTPie in action&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;We lost 54k GitHub stars&lt;/h2&gt; &#xA;&lt;p&gt;Please note we recently accidentally made this repo private for a moment, and GitHub deleted our community that took a decade to build. Read the full story here: &lt;a href=&#34;https://httpie.io/blog/stardust&#34;&gt;https://httpie.io/blog/stardust&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/httpie/httpie/master/docs/stardust.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://httpie.io/docs#installation&#34;&gt;Installation instructions ‚Üí&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://httpie.io/docs&#34;&gt;Full documentation ‚Üí&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Expressive and intuitive syntax&lt;/li&gt; &#xA; &lt;li&gt;Formatted and colorized terminal output&lt;/li&gt; &#xA; &lt;li&gt;Built-in JSON support&lt;/li&gt; &#xA; &lt;li&gt;Forms and file uploads&lt;/li&gt; &#xA; &lt;li&gt;HTTPS, proxies, and authentication&lt;/li&gt; &#xA; &lt;li&gt;Arbitrary request data&lt;/li&gt; &#xA; &lt;li&gt;Custom headers&lt;/li&gt; &#xA; &lt;li&gt;Persistent sessions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wget&lt;/code&gt;-like downloads&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://httpie.io/docs&#34;&gt;See all features ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Hello World:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ https httpie.io/hello&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Custom &lt;a href=&#34;https://httpie.io/docs#http-method&#34;&gt;HTTP method&lt;/a&gt;, &lt;a href=&#34;https://httpie.io/docs#http-headers&#34;&gt;HTTP headers&lt;/a&gt; and &lt;a href=&#34;https://httpie.io/docs#json&#34;&gt;JSON&lt;/a&gt; data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ http PUT pie.dev/put X-API-Token:123 name=John&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build and print a request without sending it using &lt;a href=&#34;https://httpie.io/docs#offline-mode&#34;&gt;offline mode&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ http --offline pie.dev/post hello=offline&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://developer.github.com/v3/issues/comments/#create-a-comment&#34;&gt;GitHub API&lt;/a&gt; to post a comment on an &lt;a href=&#34;https://github.com/httpie/httpie/issues/83&#34;&gt;Issue&lt;/a&gt; with &lt;a href=&#34;https://httpie.io/docs#authentication&#34;&gt;authentication&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ http -a USERNAME POST https://api.github.com/repos/httpie/httpie/issues/83/comments body=&#39;HTTPie is awesome! &lt;span&gt;‚ù§Ô∏è&lt;/span&gt;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://httpie.io/docs#examples&#34;&gt;See more examples ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community &amp;amp; support&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visit the &lt;a href=&#34;https://httpie.io&#34;&gt;HTTPie website&lt;/a&gt; for full documentation and useful links.&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://httpie.io/discord&#34;&gt;Discord server&lt;/a&gt; is to ask questions, discuss features, and for general API chat.&lt;/li&gt; &#xA; &lt;li&gt;Tweet at &lt;a href=&#34;https://twitter.com/httpie&#34;&gt;@httpie&lt;/a&gt; on Twitter.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://stackoverflow.com/questions/tagged/httpie&#34;&gt;StackOverflow&lt;/a&gt; to ask questions and include a &lt;code&gt;httpie&lt;/code&gt; tag.&lt;/li&gt; &#xA; &lt;li&gt;Create &lt;a href=&#34;https://github.com/httpie/httpie/issues&#34;&gt;GitHub Issues&lt;/a&gt; for bug reports and feature requests.&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to the &lt;a href=&#34;https://httpie.io&#34;&gt;HTTPie newsletter&lt;/a&gt; for occasional updates.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Have a look through existing &lt;a href=&#34;https://github.com/httpie/httpie/issues&#34;&gt;Issues&lt;/a&gt; and &lt;a href=&#34;https://github.com/httpie/httpie/pulls&#34;&gt;Pull Requests&lt;/a&gt; that you could help with. If you&#39;d like to request a feature or report a bug, please &lt;a href=&#34;https://github.com/httpie/httpie/issues&#34;&gt;create a GitHub Issue&lt;/a&gt; using one of the templates provided.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/httpie/httpie/raw/master/CONTRIBUTING.md&#34;&gt;See contribution guide ‚Üí&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>