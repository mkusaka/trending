<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-07T01:35:01Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-edge-platform/anomalib</title>
    <updated>2025-08-07T01:35:01Z</updated>
    <id>tag:github.com,2025-08-07:/open-edge-platform/anomalib</id>
    <link href="https://github.com/open-edge-platform/anomalib" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/docs/source/_static/images/logos/anomalib-wide-blue.png&#34; width=&#34;600px&#34; alt=&#34;Anomalib Logo - A deep learning library for anomaly detection&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;A library for benchmarking, developing and deploying deep learning anomaly detection algorithms&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/#key-features&#34;&gt;Key Features&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://anomalib.readthedocs.io/en/latest/&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/examples/notebooks&#34;&gt;Notebooks&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10%2B-green&#34; alt=&#34;python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pytorch-2.0%2B-orange&#34; alt=&#34;pytorch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/lightning-2.2%2B-blue&#34; alt=&#34;lightning&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/openvino-2024.0%2B-purple&#34; alt=&#34;openvino&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/open-edge-platform/anomalib/actions/workflows/pre_merge.yml&#34;&gt;&lt;img src=&#34;https://github.com/open-edge-platform/anomalib/actions/workflows/pre_merge.yml/badge.svg?sanitize=true&#34; alt=&#34;Pre-Merge Checks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-edge-platform/anomalib&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-edge-platform/anomalib/branch/main/graph/badge.svg?token=Z6A07N1BZK&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/anomalib&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/anomalib?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=green&amp;amp;left_text=PyPI%20Downloads&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://snyk.io/advisor/python/anomalib&#34;&gt;&lt;img src=&#34;https://snyk.io/advisor/python/anomalib/badge.svg?sanitize=true&#34; alt=&#34;snyk&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.bestpractices.dev/projects/8330&#34;&gt;&lt;img src=&#34;https://www.bestpractices.dev/projects/8330/badge&#34; alt=&#34;OpenSSF Best Practices&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://anomalib.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/anomalib/badge/?version=latest&#34; alt=&#34;ReadTheDocs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gurubase.io/g/anomalib&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gurubase-Ask%20Anomalib%20Guru-006BFF&#34; alt=&#34;Anomalib - Gurubase docs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/6030&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/6030&#34; alt=&#34;open-edge-platform%2Fanomalib | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üåü &lt;strong&gt;Announcing v2.1.0 Release!&lt;/strong&gt; üåü&lt;/p&gt; &#xA; &lt;p&gt;We&#39;re excited to announce the release of Anomalib v2.1.0! This version brings several state-of-the-art models and anomaly detection datasets. Key features include:&lt;/p&gt; &#xA; &lt;p&gt;New models :&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;üñºÔ∏è UniNet (CVPR 2025)&lt;/strong&gt;: A contrastive learning-guided unified framework with feature selection for anomaly detection.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;üñºÔ∏è Dinomaly (CVPR 2025)&lt;/strong&gt;: A &#39;less is more philosophy&#39; encoder-decoder architecture model leveraging pre-trained foundational models.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;üé• Fuvas (ICASSP 2025)&lt;/strong&gt;: Few-shot unsupervised video anomaly segmentation via low-rank factorization of spatio-temporal features.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;New datasets:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;MVTec AD 2&lt;/strong&gt; : A new version of the MVTec AD dataset with 8 categories of industrial anomaly detection.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;MVTec LOCO AD&lt;/strong&gt; : MVTec logical constraints anomaly detection dataset that includes both structural and logical anomalies.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Real-IAD&lt;/strong&gt; : A real-world multi-view dataset for benchmarking versatile industrial anomaly detection.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;VAD dataset&lt;/strong&gt; : Valeo Anomaly Dataset (VAD) showcasing a diverse range of defects, from highly obvious to extremely subtle.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;We value your input! Please share feedback via &lt;a href=&#34;https://github.com/open-edge-platform/anomalib/issues&#34;&gt;GitHub Issues&lt;/a&gt; or our &lt;a href=&#34;https://github.com/open-edge-platform/anomalib/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;üëã Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib is a deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets. Anomalib provides several ready-to-use implementations of anomaly detection algorithms described in the recent literature, as well as a set of tools that facilitate the development and implementation of custom models. The library has a strong focus on visual anomaly detection, where the goal of the algorithm is to detect and/or localize anomalies within images or videos in a dataset. Anomalib is constantly updated with new algorithms and training/inference extensions, so keep checking!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/docs/source/_static/images/readme.png&#34; width=&#34;1000&#34; alt=&#34;A prediction made by anomalib&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Key features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple and modular API and CLI for training, inference, benchmarking, and hyperparameter optimization.&lt;/li&gt; &#xA; &lt;li&gt;The largest public collection of ready-to-use deep learning anomaly detection algorithms and benchmark datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lightning.ai/&#34;&gt;&lt;strong&gt;Lightning&lt;/strong&gt;&lt;/a&gt; based model implementations to reduce boilerplate code and limit the implementation efforts to the bare essentials.&lt;/li&gt; &#xA; &lt;li&gt;The majority of models can be exported to &lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html&#34;&gt;&lt;strong&gt;OpenVINO&lt;/strong&gt;&lt;/a&gt; Intermediate Representation (IR) for accelerated inference on Intel hardware.&lt;/li&gt; &#xA; &lt;li&gt;A set of &lt;a href=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/tools&#34;&gt;inference tools&lt;/a&gt; for quick and easy deployment of the standard or custom anomaly detection models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üì¶ Installation&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib can be installed from PyPI. We recommend using a virtual environment and a modern package installer like &lt;code&gt;uv&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Install&lt;/h2&gt; &#xA;&lt;p&gt;For a standard installation, you can use &lt;code&gt;uv&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt;. This will install the latest version of Anomalib with its core dependencies. PyTorch will be installed based on its default behavior, which usually works for CPU and standard CUDA setups.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# With uv&#xA;uv pip install anomalib&#xA;&#xA;# Or with pip&#xA;pip install anomalib&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more control over the installation, such as specifying the PyTorch backend (e.g., XPU, CUDA and ROCm) or installing extra dependencies for specific models, see the advanced options below.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;üí° Advanced Installation: Specify Hardware Backend&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;To ensure compatibility with your hardware, you can specify a backend during installation. This is the recommended approach for production environments and for hardware other than CPU or standard CUDA.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Using &lt;code&gt;uv&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# CPU support (default, works on all platforms)&#xA;uv pip install &#34;anomalib[cpu]&#34;&#xA;&#xA;# CUDA 12.4 support (Linux/Windows with NVIDIA GPU)&#xA;uv pip install &#34;anomalib[cu124]&#34;&#xA;&#xA;# CUDA 12.1 support (Linux/Windows with NVIDIA GPU)&#xA;uv pip install &#34;anomalib[cu121]&#34;&#xA;&#xA;# CUDA 11.8 support (Linux/Windows with NVIDIA GPU)&#xA;uv pip install &#34;anomalib[cu118]&#34;&#xA;&#xA;# ROCm support (Linux with AMD GPU)&#xA;uv pip install &#34;anomalib[rocm]&#34;&#xA;&#xA;# Intel XPU support (Linux with Intel GPU)&#xA;uv pip install &#34;anomalib[xpu]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Using &lt;code&gt;pip&lt;/code&gt;:&lt;/strong&gt; The same extras can be used with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;anomalib[cu124]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;üß© Advanced Installation: Additional Dependencies&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Anomalib includes most dependencies by default. For specialized features, you may need additional optional dependencies. Remember to include your hardware-specific extra.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example: Install with OpenVINO support and CUDA 12.4&#xA;uv pip install &#34;anomalib[openvino,cu124]&#34;&#xA;&#xA;# Example: Install all optional dependencies for a CPU-only setup&#xA;uv pip install &#34;anomalib[full,cpu]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Here is a list of available optional dependency groups:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Extra&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Purpose&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;code&gt;[openvino]&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Intel OpenVINO optimization&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;For accelerated inference on Intel hardware&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;code&gt;[clip]&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Vision-language models&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;code&gt;winclip&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;code&gt;[vlm]&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Vision-language model backends&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Advanced VLM features&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;code&gt;[loggers]&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Experiment tracking (wandb, comet, etc.)&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;For experiment management&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;code&gt;[notebooks]&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Jupyter notebook support&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;For running example notebooks&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;&lt;code&gt;[full]&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;All optional dependencies&lt;/td&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;All optional features&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;üîß Advanced Installation: Install from Source&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;p&gt;For contributing to &lt;code&gt;anomalib&lt;/code&gt; or using a development version, you can install from source.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Using &lt;code&gt;uv&lt;/code&gt;:&lt;/strong&gt; This is the recommended method for developers as it uses the project&#39;s lock file for reproducible environments.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/open-edge-platform/anomalib.git&#xA;cd anomalib&#xA;&#xA;# Create the virtual environment&#xA;uv venv&#xA;&#xA;# Sync with the lockfile for a specific backend (e.g., CPU)&#xA;uv sync --extra cpu&#xA;&#xA;# Or for a different backend like CUDA 12.4&#xA;uv sync --extra cu124&#xA;&#xA;# To set up a full development environment&#xA;uv sync --extra dev --extra cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Using &lt;code&gt;pip&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/open-edge-platform/anomalib.git&#xA;cd anomalib&#xA;&#xA;# Install in editable mode with a specific backend&#xA;pip install -e &#34;.[cpu]&#34;&#xA;&#xA;# Install with development dependencies&#xA;pip install -e &#34;.[dev,cpu]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;üß† Training&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib supports both API and CLI-based training approaches:&lt;/p&gt; &#xA;&lt;h2&gt;üîå Python API&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from anomalib.data import MVTecAD&#xA;from anomalib.models import Patchcore&#xA;from anomalib.engine import Engine&#xA;&#xA;# Initialize components&#xA;datamodule = MVTecAD()&#xA;model = Patchcore()&#xA;engine = Engine()&#xA;&#xA;# Train the model&#xA;engine.fit(datamodule=datamodule, model=model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚å®Ô∏è Command Line&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Train with default settings&#xA;anomalib train --model Patchcore --data anomalib.data.MVTecAD&#xA;&#xA;# Train with custom category&#xA;anomalib train --model Patchcore --data anomalib.data.MVTecAD --data.category transistor&#xA;&#xA;# Train with config file&#xA;anomalib train --config path/to/config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;ü§ñ Inference&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib provides multiple inference options including Torch, Lightning, Gradio, and OpenVINO. Here&#39;s how to get started:&lt;/p&gt; &#xA;&lt;h2&gt;üîå Python API&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load model and make predictions&#xA;predictions = engine.predict(&#xA;    datamodule=datamodule,&#xA;    model=model,&#xA;    ckpt_path=&#34;path/to/checkpoint.ckpt&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚å®Ô∏è Command Line&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Basic prediction&#xA;anomalib predict --model anomalib.models.Patchcore \&#xA;                 --data anomalib.data.MVTecAD \&#xA;                 --ckpt_path path/to/model.ckpt&#xA;&#xA;# Prediction with results&#xA;anomalib predict --model anomalib.models.Patchcore \&#xA;                 --data anomalib.data.MVTecAD \&#xA;                 --ckpt_path path/to/model.ckpt \&#xA;                 --return_predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üìò &lt;strong&gt;Note:&lt;/strong&gt; For advanced inference options including Gradio and OpenVINO, check our &lt;a href=&#34;https://anomalib.readthedocs.io&#34;&gt;Inference Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Training on Intel GPUs&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] Currently, only single GPU training is supported on Intel GPUs. These commands were tested on Arc 750 and Arc 770.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Ensure that you have PyTorch with XPU support installed. For more information, please refer to the &lt;a href=&#34;https://pytorch.org/docs/stable/notes/get_start_xpu.html&#34;&gt;PyTorch XPU documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üîå API&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from anomalib.data import MVTecAD&#xA;from anomalib.engine import Engine, SingleXPUStrategy, XPUAccelerator&#xA;from anomalib.models import Stfpm&#xA;&#xA;engine = Engine(&#xA;    strategy=SingleXPUStrategy(),&#xA;    accelerator=XPUAccelerator(),&#xA;)&#xA;engine.train(Stfpm(), datamodule=MVTecAD())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚å®Ô∏è CLI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;anomalib train --model Padim --data MVTecAD --trainer.accelerator xpu --trainer.strategy xpu_single&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;‚öôÔ∏è Hyperparameter Optimization&lt;/h1&gt; &#xA;&lt;p&gt;Anomalib supports hyperparameter optimization (HPO) using &lt;a href=&#34;https://wandb.ai/&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; and &lt;a href=&#34;https://www.comet.com/&#34;&gt;Comet.ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run HPO with Weights &amp;amp; Biases&#xA;anomalib hpo --backend WANDB --sweep_config tools/hpo/configs/wandb.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üìò &lt;strong&gt;Note:&lt;/strong&gt; For detailed HPO configuration, check our &lt;a href=&#34;https://open-edge-platform.github.io/anomalib/tutorials/hyperparameter_optimization.html&#34;&gt;HPO Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;üß™ Experiment Management&lt;/h1&gt; &#xA;&lt;p&gt;Track your experiments with popular logging platforms through &lt;a href=&#34;https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html&#34;&gt;PyTorch Lightning loggers&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìä Weights &amp;amp; Biases&lt;/li&gt; &#xA; &lt;li&gt;üìà Comet.ml&lt;/li&gt; &#xA; &lt;li&gt;üìâ TensorBoard&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Enable logging in your config file to track:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hyperparameters&lt;/li&gt; &#xA; &lt;li&gt;Metrics&lt;/li&gt; &#xA; &lt;li&gt;Model graphs&lt;/li&gt; &#xA; &lt;li&gt;Test predictions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üìò &lt;strong&gt;Note:&lt;/strong&gt; For logging setup, see our &lt;a href=&#34;https://open-edge-platform.github.io/anomalib/tutorials/logging.html&#34;&gt;Logging Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;üìä Benchmarking&lt;/h1&gt; &#xA;&lt;p&gt;Evaluate and compare model performance across different datasets:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run benchmarking with default configuration&#xA;anomalib benchmark --config tools/experimental/benchmarking/sample.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí° &lt;strong&gt;Tip:&lt;/strong&gt; Check individual model performance in their respective README files:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/src/anomalib/models/image/patchcore/README.md#mvtec-ad-dataset&#34;&gt;Patchcore Results&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/src/anomalib/models/&#34;&gt;Other Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;‚úçÔ∏è Reference&lt;/h1&gt; &#xA;&lt;p&gt;If you find Anomalib useful in your research or work, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{akcay2022anomalib,&#xA;  title={Anomalib: A deep learning library for anomaly detection},&#xA;  author={Akcay, Samet and Ameln, Dick and Vaidya, Ashwin and Lakshmanan, Barath and Ahuja, Nilesh and Genc, Utku},&#xA;  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},&#xA;  pages={1706--1710},&#xA;  year={2022},&#xA;  organization={IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üë• Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We welcome contributions! Check out our &lt;a href=&#34;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/open-edge-platform/anomalib/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=open-edge-platform/anomalib&#34; alt=&#34;Contributors to open-edge-platform/anomalib&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt;Thank you to all our contributors!&lt;/b&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JudgmentLabs/judgeval</title>
    <updated>2025-08-07T01:35:01Z</updated>
    <id>tag:github.com,2025-08-07:/JudgmentLabs/judgeval</id>
    <link href="https://github.com/JudgmentLabs/judgeval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The open source post-building layer for agents. Our traces + evals power agent post-training (RL, SFT), monitoring, and regression testing.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/new_lightmode.svg#gh-light-mode-only&#34; alt=&#34;Judgment Logo&#34; width=&#34;400&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/new_darkmode.svg#gh-dark-mode-only&#34; alt=&#34;Judgment Logo&#34; width=&#34;400&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;div style=&#34;font-size: 1.5em;&#34;&gt;&#xA;   Enable self-learning agents with traces, evals, and environment data. &#xA; &lt;/div&gt; &#xA; &lt;h2&gt;&lt;a href=&#34;https://docs.judgmentlabs.ai/&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://app.judgmentlabs.ai/register&#34;&gt;Judgment Cloud&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.judgmentlabs.ai/documentation/self-hosting/get-started&#34;&gt;Self-Host&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://judgmentlabs.ai/&#34;&gt;Landing Page&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=1S4LixpVbcc&#34;&gt;Demo&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/JudgmentLabs/judgeval/issues&#34;&gt;Bug Reports&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.judgmentlabs.ai/changelog/2025-04-21&#34;&gt;Changelog&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;We&#39;re hiring! Join us in our mission to enable self-learning agents by providing the data and signals needed for monitoring and post-training.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://x.com/JudgmentLabs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-X/Twitter-000?logo=x&amp;amp;logoColor=white&#34; alt=&#34;X&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/company/judgmentlabs&#34;&gt;&lt;img src=&#34;https://custom-icon-badges.demolab.com/badge/LinkedIn%20-0A66C2?logo=linkedin-white&amp;amp;logoColor=fff&#34; alt=&#34;LinkedIn&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/tGVFf8UBUY&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Discord-5865F2?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/product_shot.png&#34; alt=&#34;Judgment Platform&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Judgeval offers &lt;strong&gt;open-source tooling&lt;/strong&gt; for tracing and evaluating autonomous, stateful agents. It &lt;strong&gt;provides runtime data from agent-environment interactions&lt;/strong&gt; for continuous learning and self-improvement.&lt;/p&gt; &#xA;&lt;h2&gt;üé¨ See Judgeval in Action&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/JudgmentLabs/judgment-cookbook/tree/main/cookbooks/agents/multi-agent&#34;&gt;Multi-Agent System&lt;/a&gt; with complete observability:&lt;/strong&gt; (1) A multi-agent system spawns agents to research topics on the internet. (2) With just &lt;strong&gt;3 lines of code&lt;/strong&gt;, Judgeval traces every input/output + environment response across all agent tool calls for debugging. (3) After completion, (4) export all interaction data to enable further environment-specific learning and optimization.&lt;/p&gt; &#xA;&lt;table style=&#34;width: 100%; max-width: 800px; table-layout: fixed;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;padding: 8px; width: 50%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/agent.gif&#34; alt=&#34;Agent Demo&#34; style=&#34;width: 100%; max-width: 350px; height: auto;&#34;&gt; &lt;br&gt;&lt;strong&gt;ü§ñ Agents Running&lt;/strong&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;padding: 8px; width: 50%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/trace.gif&#34; alt=&#34;Trace Demo&#34; style=&#34;width: 100%; max-width: 350px; height: auto;&#34;&gt; &lt;br&gt;&lt;strong&gt;üìä Real-time Tracing&lt;/strong&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;padding: 8px; width: 50%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/document.gif&#34; alt=&#34;Agent Completed Demo&#34; style=&#34;width: 100%; max-width: 350px; height: auto;&#34;&gt; &lt;br&gt;&lt;strong&gt;‚úÖ Agents Completed Running&lt;/strong&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; style=&#34;padding: 8px; width: 50%;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/data.gif&#34; alt=&#34;Data Export Demo&#34; style=&#34;width: 100%; max-width: 350px; height: auto;&#34;&gt; &lt;br&gt;&lt;strong&gt;üì§ Exporting Agent Environment Data&lt;/strong&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/#%EF%B8%8F-installation&#34;&gt;üõ†Ô∏è Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/#-quickstarts&#34;&gt;üèÅ Quickstarts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/#-features&#34;&gt;‚ú® Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/#-self-hosting&#34;&gt;üè¢ Self-Hosting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/#-cookbooks&#34;&gt;üìö Cookbooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/#-development-with-cursor&#34;&gt;üíª Development with Cursor&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; &#xA;&lt;p&gt;Get started with Judgeval by installing our SDK using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install judgeval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure you have your &lt;code&gt;JUDGMENT_API_KEY&lt;/code&gt; and &lt;code&gt;JUDGMENT_ORG_ID&lt;/code&gt; environment variables set to connect to the &lt;a href=&#34;https://app.judgmentlabs.ai/&#34;&gt;Judgment Platform&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export JUDGMENT_API_KEY=...&#xA;export JUDGMENT_ORG_ID=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you don&#39;t have keys, &lt;a href=&#34;https://app.judgmentlabs.ai/register&#34;&gt;create an account&lt;/a&gt; on the platform!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèÅ Quickstarts&lt;/h2&gt; &#xA;&lt;h3&gt;üõ∞Ô∏è Tracing&lt;/h3&gt; &#xA;&lt;p&gt;Create a file named &lt;code&gt;agent.py&lt;/code&gt; with the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from judgeval.tracer import Tracer, wrap&#xA;from openai import OpenAI&#xA;&#xA;client = wrap(OpenAI())  # tracks all LLM calls&#xA;judgment = Tracer(project_name=&#34;my_project&#34;)&#xA;&#xA;@judgment.observe(span_type=&#34;tool&#34;)&#xA;def format_question(question: str) -&amp;gt; str:&#xA;    # dummy tool&#xA;    return f&#34;Question : {question}&#34;&#xA;&#xA;@judgment.observe(span_type=&#34;function&#34;)&#xA;def run_agent(prompt: str) -&amp;gt; str:&#xA;    task = format_question(prompt)&#xA;    response = client.chat.completions.create(&#xA;        model=&#34;gpt-4.1&#34;,&#xA;        messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: task}]&#xA;    )&#xA;    return response.choices[0].message.content&#xA;    &#xA;run_agent(&#34;What is the capital of the United States?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll see your trace exported to the Judgment Platform:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/online_eval.png&#34; alt=&#34;Judgment Platform Trace Example&#34; width=&#34;1500&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.judgmentlabs.ai/documentation/tracing/introduction&#34;&gt;Click here&lt;/a&gt; for a more detailed explanation.&lt;/p&gt; &#xA;&lt;!-- Created by https://github.com/ekalinin/github-markdown-toc --&gt; &#xA;&lt;h2&gt;‚ú® Features&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;h3&gt;üîç Tracing&lt;/h3&gt;Automatic agent tracing integrated with common frameworks (LangGraph, OpenAI, Anthropic). &lt;strong&gt;Tracks inputs/outputs, agent tool calls, latency, cost, and custom metadata&lt;/strong&gt; at every step.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Useful for:&lt;/strong&gt;&lt;br&gt;‚Ä¢ üêõ Debugging agent runs &lt;br&gt;‚Ä¢ üìã Collecting agent environment data &lt;br&gt;‚Ä¢ üî¨ Pinpointing performance bottlenecks&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/agent_trace_example.png&#34; alt=&#34;Tracing visualization&#34; width=&#34;1200&#34;&gt;&lt;/p&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;h3&gt;üß™ Evals&lt;/h3&gt;Build custom evaluators on top of your agents. Judgeval supports LLM-as-a-judge, manual labeling, and code-based evaluators that connect with our metric-tracking infrastructure. &lt;br&gt;&lt;br&gt;&lt;strong&gt;Useful for:&lt;/strong&gt;&lt;br&gt;‚Ä¢ ‚ö†Ô∏è Unit-testing &lt;br&gt;‚Ä¢ üî¨ A/B testing &lt;br&gt;‚Ä¢ üõ°Ô∏è Online guardrails&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/test.png&#34; alt=&#34;Evaluation metrics&#34; width=&#34;800&#34;&gt;&lt;/p&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;h3&gt;üì° Monitoring&lt;/h3&gt;Get Slack alerts for agent failures in production. Add custom hooks to address production regressions.&lt;br&gt;&lt;br&gt; &lt;strong&gt;Useful for:&lt;/strong&gt; &lt;br&gt;‚Ä¢ üìâ Identifying degradation early &lt;br&gt;‚Ä¢ üìà Visualizing performance trends across agent versions and time&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/errors.png&#34; alt=&#34;Monitoring Dashboard&#34; width=&#34;1200&#34;&gt;&lt;/p&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;h3&gt;üìä Datasets&lt;/h3&gt;Export traces and test cases to datasets for scaled analysis and optimization. Move datasets to/from Parquet, S3, etc. &lt;br&gt;&lt;br&gt;Run evals on datasets as unit tests or to A/B test different agent configurations, enabling continuous learning from production interactions. &lt;br&gt;&lt;br&gt; &lt;strong&gt;Useful for:&lt;/strong&gt;&lt;br&gt;‚Ä¢ üóÉÔ∏è Agent environment interaction data for optimization&lt;br&gt;‚Ä¢ üîÑ Scaled analysis for A/B tests&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JudgmentLabs/judgeval/main/assets/datasets_preview_screenshot.png&#34; alt=&#34;Dataset management&#34; width=&#34;1200&#34;&gt;&lt;/p&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üè¢ Self-Hosting&lt;/h2&gt; &#xA;&lt;p&gt;Run Judgment on your own infrastructure: we provide comprehensive self-hosting capabilities that give you full control over the backend and data plane that Judgeval interfaces with.&lt;/p&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deploy Judgment on your own AWS account&lt;/li&gt; &#xA; &lt;li&gt;Store data in your own Supabase instance&lt;/li&gt; &#xA; &lt;li&gt;Access Judgment through your own custom domain&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Check out our &lt;a href=&#34;https://docs.judgmentlabs.ai/documentation/self-hosting/get-started&#34;&gt;self-hosting documentation&lt;/a&gt; for detailed setup instructions, along with how your self-hosted instance can be accessed&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://docs.judgmentlabs.ai/documentation/developer-tools/judgment-cli/installation&#34;&gt;Judgment CLI&lt;/a&gt; to deploy your self-hosted environment&lt;/li&gt; &#xA; &lt;li&gt;After your self-hosted instance is setup, make sure the &lt;code&gt;JUDGMENT_API_URL&lt;/code&gt; environmental variable is set to your self-hosted backend endpoint&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üìö Cookbooks&lt;/h2&gt; &#xA;&lt;p&gt;Have your own? We&#39;re happy to feature it if you create a PR or message us on &lt;a href=&#34;https://discord.gg/tGVFf8UBUY&#34;&gt;Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can access our repo of cookbooks &lt;a href=&#34;https://github.com/JudgmentLabs/judgment-cookbook&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Development with Cursor&lt;/h2&gt; &#xA;&lt;p&gt;Building agents and LLM workflows in Cursor works best when your coding assistant has the proper context about Judgment integration. The Cursor rules file contains the key information needed for your assistant to implement Judgment features effectively.&lt;/p&gt; &#xA;&lt;p&gt;Refer to the official &lt;a href=&#34;https://docs.judgmentlabs.ai/documentation/developer-tools/cursor/cursor-rules&#34;&gt;documentation&lt;/a&gt; for access to the rules file and more information on integrating this rules file with your codebase.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠ê Star Us on GitHub&lt;/h2&gt; &#xA;&lt;p&gt;If you find Judgeval useful, please consider giving us a star on GitHub! Your support helps us grow our community and continue improving the repository.&lt;/p&gt; &#xA;&lt;h2&gt;‚ù§Ô∏è Contributors&lt;/h2&gt; &#xA;&lt;p&gt;There are many ways to contribute to Judgeval:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Submit &lt;a href=&#34;https://github.com/JudgmentLabs/judgeval/issues&#34;&gt;bug reports&lt;/a&gt; and &lt;a href=&#34;https://github.com/JudgmentLabs/judgeval/issues&#34;&gt;feature requests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Review the documentation and submit &lt;a href=&#34;https://github.com/JudgmentLabs/judgeval/pulls&#34;&gt;Pull Requests&lt;/a&gt; to improve it&lt;/li&gt; &#xA; &lt;li&gt;Speaking or writing about Judgment and letting us know!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Contributors collage --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/JudgmentLabs/judgeval/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contributors-img.web.app/image?repo=JudgmentLabs/judgeval&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Judgeval is created and maintained by &lt;a href=&#34;https://judgmentlabs.ai/&#34;&gt;Judgment Labs&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hao-ai-lab/FastVideo</title>
    <updated>2025-08-07T01:35:01Z</updated>
    <id>tag:github.com,2025-08-07:/hao-ai-lab/FastVideo</id>
    <link href="https://github.com/hao-ai-lab/FastVideo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A unified inference and post-training framework for accelerated video generation.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/logo.png&#34; width=&#34;30%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastVideo is a unified post-training and inference framework for accelerated video generation.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;FastVideo features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html&#34;&gt;&lt;b&gt; Quick Start&lt;/b&gt;&lt;/a&gt; | ü§ó &lt;a href=&#34;https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;FastWan2.1&lt;/b&gt;&lt;/a&gt; | ü§ó &lt;a href=&#34;https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;FastWan2.2&lt;/b&gt;&lt;/a&gt; | üü£üí¨ &lt;a href=&#34;https://join.slack.com/t/fastvideo/shared_invite/zt-38u6p1jqe-yDI1QJOCEnbtkLoaI5bjZQ&#34; target=&#34;_blank&#34;&gt; &lt;b&gt;Slack&lt;/b&gt; &lt;/a&gt; | üü£üí¨ &lt;a href=&#34;https://ibb.co/qqPzbrw&#34; target=&#34;_blank&#34;&gt; &lt;b&gt; WeChat &lt;/b&gt; &lt;/a&gt; | &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/fastwan.png&#34; width=&#34;90%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;NEWS&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2025/08/04&lt;/code&gt;: Release &lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html&#34;&gt;FastWan&lt;/a&gt; models and &lt;a href=&#34;https://hao-ai-lab.github.io/blogs/fastvideo_post_training/&#34;&gt;Sparse-Distillation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025/06/14&lt;/code&gt;: Release finetuning and inference code for &lt;a href=&#34;https://arxiv.org/pdf/2505.13389&#34;&gt;VSA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025/04/24&lt;/code&gt;: &lt;a href=&#34;https://hao-ai-lab.github.io/blogs/fastvideo/&#34;&gt;FastVideo V1&lt;/a&gt; is released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2025/02/18&lt;/code&gt;: Release the inference code for &lt;a href=&#34;https://hao-ai-lab.github.io/blogs/sta/&#34;&gt;Sliding Tile Attention&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;p&gt;FastVideo has the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;End-to-end post-training support: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://hao-ai-lab.github.io/blogs/fastvideo_post_training/&#34;&gt;Sparse distillation&lt;/a&gt; for Wan2.1 and Wan2.2 to achineve &amp;gt;50x denoising speedup&lt;/li&gt; &#xA;   &lt;li&gt;Data preprocessing pipeline for video data&lt;/li&gt; &#xA;   &lt;li&gt;Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs&lt;/li&gt; &#xA;   &lt;li&gt;Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;State-of-the-art performance optimizations for inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.13389&#34;&gt;Video Sparse Attention&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.04507&#34;&gt;Sliding Tile Attention&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.19108&#34;&gt;TeaCache&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.02367&#34;&gt;Sage Attention&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Diverse hardware and OS support &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support H100, A100, 4090&lt;/li&gt; &#xA;   &lt;li&gt;Support Linux, Windows, MacOS&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using an environment manager such as &lt;code&gt;Conda&lt;/code&gt; to create a clean environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create and activate a new conda environment&#xA;conda create -n fastvideo python=3.12&#xA;conda activate fastvideo&#xA;&#xA;# Install FastVideo&#xA;pip install fastvideo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html&#34;&gt;docs&lt;/a&gt; for more detailed installation instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Sparse Distillation&lt;/h2&gt; &#xA;&lt;p&gt;For our sparse distillation techniques, please see our &lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html&#34;&gt;distillation docs&lt;/a&gt; and check out our &lt;a href=&#34;https://hao-ai-lab.github.io/blogs/fastvideo_post_training/&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See below for recipes and datasets:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sparse Distillation&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers&#34;&gt;FastWan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P&#34;&gt;Recipe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/FastVideo/Wan-Syn_77x448x832_600k&#34;&gt;FastVideo Synthetic Wan2.1 480P&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/FastVideo/FastWan2.1-T2V-14B-Diffusers&#34;&gt;FastWan2.1-T2V-14B-Preview&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Coming soon!&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/FastVideo/Wan-Syn_77x768x1280_250k&#34;&gt;FastVideo Synthetic Wan2.1 720P&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers&#34;&gt;FastWan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free&#34;&gt;Recipe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/FastVideo/Wan2.2-Syn-121x704x1280_32k&#34;&gt;FastVideo Synthetic Wan2.2 720P&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Generating Your First Video&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s a minimal example to generate a video using the default settings. Create a file called &lt;code&gt;example.py&lt;/code&gt; with the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastvideo import VideoGenerator&#xA;&#xA;def main():&#xA;    # Create a video generator with a pre-trained model&#xA;    generator = VideoGenerator.from_pretrained(&#xA;        &#34;FastVideo/FastWan2.1-T2V-1.3B-Diffusers&#34;,&#xA;        num_gpus=1,  # Adjust based on your hardware&#xA;    )&#xA;&#xA;    # Define a prompt for your video&#xA;    prompt = &#34;A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest.&#34;&#xA;&#xA;    # Generate the video&#xA;    video = generator.generate_video(&#xA;        prompt,&#xA;        return_frames=True,  # Also return frames from this call (defaults to False)&#xA;        output_path=&#34;my_videos/&#34;,  # Controls where videos are saved&#xA;        save_video=True&#xA;    )&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    main()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the script with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python example.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a more detailed guide, please see our &lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html&#34;&gt;inference quick start&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other docs:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/design/overview.html&#34;&gt;Design Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html&#34;&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Distillation and Finetuning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html&#34;&gt;Distillation Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - [Finetuning Guide](https://hao-ai-lab.github.io/FastVideo/training/finetune.html) --&gt; &#xA;&lt;h2&gt;üìë Development Plan&lt;/h2&gt; &#xA;&lt;!-- - More distillation methods --&gt; &#xA;&lt;!-- - [ ] Add Distribution Matching Distillation --&gt; &#xA;&lt;p&gt;More FastWan Models Coming Soon!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add FastWan2.1-T2V-14B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add FastWan2.2-T2V-14B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add FastWan2.2-I2V-14B&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - Optimization features&#xA;- Code updates --&gt; &#xA;&lt;!-- - [ ] fp8 support --&gt; &#xA;&lt;!-- - [ ] faster load model and save model support --&gt; &#xA;&lt;p&gt;See details in &lt;a href=&#34;https://github.com/hao-ai-lab/FastVideo/issues/468&#34;&gt;development roadmap&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome all contributions. Please check out our guide &lt;a href=&#34;https://hao-ai-lab.github.io/FastVideo/contributing/overview.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We learned and reused code from the following projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Wan-Video&#34;&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HazyResearch/ThunderKittens&#34;&gt;ThunderKittens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/triton-lang/triton&#34;&gt;Triton&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tianweiy/DMD2&#34;&gt;DMD2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xdit-project/xDiT&#34;&gt;xDiT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;SGLang&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We thank &lt;a href=&#34;https://ifm.mbzuai.ac.ae/&#34;&gt;MBZUAI&lt;/a&gt;, &lt;a href=&#34;https://www.anyscale.com/&#34;&gt;Anyscale&lt;/a&gt;, and &lt;a href=&#34;https://www.gmicloud.ai/&#34;&gt;GMI Cloud&lt;/a&gt; for their support throughout this project.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find FastVideo useful, please considering citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{fastvideo2024,&#xA;  title        = {FastVideo: A Unified Framework for Accelerated Video Generation},&#xA;  author       = {The FastVideo Team},&#xA;  url          = {https://github.com/hao-ai-lab/FastVideo},&#xA;  month        = apr,&#xA;  year         = {2024},&#xA;}&#xA;&#xA;@article{zhang2025vsa,&#xA;  title={VSA: Faster Video Diffusion with Trainable Sparse Attention},&#xA;  author={Zhang, Peiyuan and Huang, Haofeng and Chen, Yongqi and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},&#xA;  journal={arXiv preprint arXiv:2505.13389},&#xA;  year={2025}&#xA;}&#xA;&#xA;@article{zhang2025fast,&#xA;  title={Fast video generation with sliding tile attention},&#xA;  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},&#xA;  journal={arXiv preprint arXiv:2502.04507},&#xA;  year={2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>