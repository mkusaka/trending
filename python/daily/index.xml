<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-01T01:37:11Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>qnguyen3/chat-with-mlx</title>
    <updated>2024-03-01T01:37:11Z</updated>
    <id>tag:github.com,2024-03-01:/qnguyen3/chat-with-mlx</id>
    <link href="https://github.com/qnguyen3/chat-with-mlx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with your data natively on Apple Silicon using MLX Framework.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Native RAG on MacOS and Apple Silicon with MLX üßë‚Äçüíª&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/chat-with-mlx&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/chat-with-mlx.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/chat-with-mlx&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/chat-with-mlx&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/qnguyen3/chat-with-mlx/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/chat-with-mlx&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/chat-with-mlx&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/chat-with-mlx&#34; alt=&#34;python-version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This repository showcases a Retrieval-augmented Generation (RAG) chat interface with support for multiple open-source models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/qnguyen3/chat-with-mlx/main/assets/chat-w-mlx.gif&#34; alt=&#34;chat_with_mlx&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat with your Data&lt;/strong&gt;: &lt;code&gt;doc(x), pdf, txt&lt;/code&gt; and YouTube video via URL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;: Chinese üá®üá≥, Englishüè¥, Frenchüá´üá∑, Germanüá©üá™, HindiüáÆüá≥, ItalianüáÆüáπ, JapaneseüáØüáµ,Koreanüá∞üá∑, Spanishüá™üá∏, Turkishüáπüá∑ and Vietnameseüáªüá≥&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy Integration&lt;/strong&gt;: Easy integrate any HuggingFace and MLX Compatible Open-Source Model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation and Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Easy Setup&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Pip&lt;/li&gt; &#xA; &lt;li&gt;Install: &lt;code&gt;pip install chat-with-mlx&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Note: Setting up this way is really hard if you want to add your own model (which I will let you add later in the UI), but it is a fast way to test the app.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Manual Pip Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/qnguyen3/chat-with-mlx.git&#xA;cd chat-with-mlx&#xA;python -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Manual Conda Installation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/qnguyen3/chat-with-mlx.git&#xA;cd chat-with-mlx&#xA;conda create -n mlx-chat python=3.11&#xA;conda activate mlx-chat&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start the app: &lt;code&gt;chat-with-mlx&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Google Gemma-7b-it, Gemma-2b-it&lt;/li&gt; &#xA; &lt;li&gt;Mistral-7B-Instruct, OpenHermes-2.5-Mistral-7B, NousHermes-2-Mistral-7B-DPO&lt;/li&gt; &#xA; &lt;li&gt;Mixtral-8x7B-Instruct-v0.1, Nous-Hermes-2-Mixtral-8x7B-DPO&lt;/li&gt; &#xA; &lt;li&gt;Quyen-SE (0.5B), Quyen (4B)&lt;/li&gt; &#xA; &lt;li&gt;StableLM 2 Zephyr (1.6B)&lt;/li&gt; &#xA; &lt;li&gt;Vistral-7B-Chat, VBD-Llama2-7b-chat, vinallama-7b-chat&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Add Your Own Models&lt;/h2&gt; &#xA;&lt;h3&gt;Solution 1&lt;/h3&gt; &#xA;&lt;p&gt;This solution only requires you to add your own model with a simple .yaml config file in &lt;code&gt;chat_with_mlx/models/configs&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;examlple.yaml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;original_repo: google/gemma-2b-it # The original HuggingFace Repo, this helps with displaying&#xA;mlx-repo: mlx-community/quantized-gemma-2b-it # The MLX models Repo, most are available through `mlx-community`&#xA;quantize: 4bit # Optional: [4bit, 8bit]&#xA;default_language: multi # Optional: [en, es, zh, vi, multi]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After adding the .yaml config, you can go and load the model inside the app (for now you need to keep track the download through your Terminal/CLI)&lt;/p&gt; &#xA;&lt;h3&gt;Solution 2&lt;/h3&gt; &#xA;&lt;p&gt;Do the same as Solution 1. Sometimes, the &lt;code&gt;download_snapshot&lt;/code&gt; method that is used to download the models are slow, and you would like to download it by your own.&lt;/p&gt; &#xA;&lt;p&gt;After the adding the .yaml config, you can download the repo by yourself and add it to &lt;code&gt;chat_with_mlx/models/download&lt;/code&gt;. The folder name MUST be the same as the orginal repo name without the username (so &lt;code&gt;google/gemma-2b-it&lt;/code&gt; -&amp;gt; &lt;code&gt;gemma-2b-it&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;A complete model should have the following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;merges.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model.safetensors.index.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;special_tokens_map.json&lt;/code&gt; - this is optinal by model&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tokenizer.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vocab.json&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You HAVE TO unload a model before loading in a new model. Otherwise, you would need to restart the app to use a new model, it would stuck at the old one.&lt;/li&gt; &#xA; &lt;li&gt;When the model is downloading by Solution 1, the only way to stop it is to hit &lt;code&gt;control + C&lt;/code&gt; on your Terminal.&lt;/li&gt; &#xA; &lt;li&gt;If you want to switch the file, you have to manually hit STOP INDEXING. Otherwise, the vector database would add the second document to the current database.&lt;/li&gt; &#xA; &lt;li&gt;You have to choose a dataset mode (Document or YouTube) in order for it to work.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;WHY MLX?&lt;/h2&gt; &#xA;&lt;p&gt;MLX is an array framework for machine learning research on Apple silicon, brought to you by Apple machine learning research.&lt;/p&gt; &#xA;&lt;p&gt;Some key features of MLX include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Familiar APIs&lt;/strong&gt;: MLX has a Python API that closely follows NumPy. MLX also has fully featured C++, &lt;a href=&#34;https://github.com/ml-explore/mlx-c&#34;&gt;C&lt;/a&gt;, and &lt;a href=&#34;https://github.com/ml-explore/mlx-swift/&#34;&gt;Swift&lt;/a&gt; APIs, which closely mirror the Python API. MLX has higher-level packages like &lt;code&gt;mlx.nn&lt;/code&gt; and &lt;code&gt;mlx.optimizers&lt;/code&gt; with APIs that closely follow PyTorch to simplify building more complex models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Composable function transformations&lt;/strong&gt;: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lazy computation&lt;/strong&gt;: Computations in MLX are lazy. Arrays are only materialized when needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dynamic graph construction&lt;/strong&gt;: Computation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-device&lt;/strong&gt;: Operations can run on any of the supported devices (currently the CPU and the GPU).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified memory&lt;/strong&gt;: A notable difference from MLX and other frameworks is the &lt;em&gt;unified memory model&lt;/em&gt;. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;I would like to send my many thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Apple Machine Learning Research team for the amazing MLX library.&lt;/li&gt; &#xA; &lt;li&gt;LangChain and ChromaDB for such easy RAG Implementation&lt;/li&gt; &#xA; &lt;li&gt;People from Nous, VinBigData and Qwen team that helped me during the implementation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#qnguyen3/chat-with-mlx&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=qnguyen3/chat-with-mlx&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>evo-design/evo</title>
    <updated>2024-03-01T01:37:11Z</updated>
    <id>tag:github.com,2024-03-01:/evo-design/evo</id>
    <link href="https://github.com/evo-design/evo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DNA foundation modeling from molecular to genome scale&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Evo: DNA foundation modeling from molecular to genome scale&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/evo-design/evo/main/evo.jpg&#34; alt=&#34;Evo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Evo is a biological foundation model capable of long-context modeling and design. Evo uses the &lt;a href=&#34;https://github.com/togethercomputer/stripedhyena&#34;&gt;StripedHyena architecture&lt;/a&gt; to enable modeling of sequences at a single-nucleotide, byte-level resolution with near-linear scaling of compute and memory relative to context length. Evo has 7 billion parameters and is trained on OpenGenome, a prokaryotic whole-genome dataset containing ~300 billion tokens.&lt;/p&gt; &#xA;&lt;p&gt;We describe Evo in the the paper &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1&#34;&gt;‚ÄúSequence modeling and design from molecular to genome scale with Evo‚Äù&lt;/a&gt; and in the &lt;a href=&#34;https://arcinstitute.org/news/blog/evo&#34;&gt;accompanying blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide the following model checkpoints:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Checkpoint Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;evo-1-8k-base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model pretrained with 8,192 context. We use this model as the base model for molecular-scale finetuning tasks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;evo-1-131k-base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model pretrained with 131,072 context using &lt;code&gt;evo-1-8k-base&lt;/code&gt; as the base model. We use this model to reason about and generate sequences at the genome scale.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#huggingface&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#together-api&#34;&gt;Together API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Evo is based on &lt;a href=&#34;https://github.com/togethercomputer/stripedhyena/tree/main&#34;&gt;StripedHyena&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Evo uses &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;, which may not work on all GPU architectures. Please consult the &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention#installation-and-features&#34;&gt;FlashAttention GitHub repository&lt;/a&gt; for the current list of supported GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to install the correct &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch version&lt;/a&gt; on your system.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;You can install Evo using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install evo-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or directly from the GitHub source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/evo-design/evo.git&#xA;cd evo/&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend that you install the PyTorch library first, before installing all other dependencies (due to dependency issues of the &lt;code&gt;flash-attn&lt;/code&gt; library; see, e.g., this &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/issues/246&#34;&gt;issue&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;One of our &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/&#34;&gt;example scripts&lt;/a&gt;, demonstrating how to go from generating sequences with Evo to folding proteins (&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generation_to_folding.py&#34;&gt;scripts/generation_to_folding.py&lt;/a&gt;), further requires the installation of &lt;code&gt;prodigal&lt;/code&gt;. We have created an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/environment.yml&#34;&gt;environment.yml&lt;/a&gt; file for this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;conda activate evo-design&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Below is an example of how to download Evo and use it locally through the Python API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from evo import Evo&#xA;import torch&#xA;&#xA;device = &#39;cuda:0&#39;&#xA;&#xA;evo_model = Evo(&#39;evo-1-131k-base&#39;)&#xA;model, tokenizer = evo_model.model, evo_model.tokenizer&#xA;model.to(device)&#xA;model.eval()&#xA;&#xA;sequence = &#39;ACGT&#39;&#xA;input_ids = torch.tensor(&#xA;    tokenizer.tokenize(sequence),&#xA;    dtype=torch.int,&#xA;).to(device).unsqueeze(0)&#xA;logits, _ = model(input_ids) # (batch, length, vocab)&#xA;&#xA;print(&#39;Logits: &#39;, logits)&#xA;print(&#39;Shape (batch, length, vocab): &#39;, logits.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of batched inference can be found in &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/example_inference.py&#34;&gt;&lt;code&gt;scripts/example_inference.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generate.py&#34;&gt;example script&lt;/a&gt; for how to prompt the model and sample a set of sequences given the prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m scripts.generate \&#xA;    --model-name &#39;evo-1-131k-base&#39; \&#xA;    --prompt ACGT \&#xA;    --n-samples 10 \&#xA;    --n-tokens 100 \&#xA;    --temperature 1. \&#xA;    --top-k 4 \&#xA;    --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generate.py&#34;&gt;example script&lt;/a&gt; for using the model to score the log-likelihoods of a set of sequences.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m scripts.score \&#xA;    --input-fasta examples/example_seqs.fasta \&#xA;    --output-tsv scores.tsv \&#xA;    --model-name &#39;evo-1-131k-base&#39; \&#xA;    --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;Evo is integrated with &lt;a href=&#34;https://huggingface.co/togethercomputer/evo-1-131k-base&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoConfig, AutoModelForCausalLM&#xA;&#xA;model_name = &#39;togethercomputer/evo-1-8k-base&#39;&#xA;&#xA;model_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)&#xA;model_config.use_cache = True&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    config=model_config,&#xA;    trust_remote_code=True,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Together API&lt;/h2&gt; &#xA;&lt;p&gt;Evo will also be soon available via an API by &lt;a href=&#34;https://www.together.ai/&#34;&gt;TogetherAI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import os&#xA;&#xA;# Fill in your API information here.&#xA;client = openai.OpenAI(&#xA;  api_key=TOGETHER_API_KEY,&#xA;  base_url=&#39;https://api.together.xyz&#39;,&#xA;)&#xA;&#xA;chat_completion = client.chat.completions.create(&#xA;  messages=[&#xA;    {&#xA;      &#34;role&#34;: &#34;system&#34;,&#xA;      &#34;content&#34;: &#34;&#34;&#xA;    },&#xA;    {&#xA;      &#34;role&#34;: &#34;user&#34;,&#xA;      &#34;content&#34;: &#34;ACGT&#34;, # Prompt the model with a sequence.&#xA;    }&#xA;  ],&#xA;  model=&#34;togethercomputer/evo-1-131k-base&#34;,&#xA;  max_tokens=128, # Sample some number of new tokens.&#xA;  logprobs=True&#xA;)&#xA;print(&#xA;    chat_completion.choices[0].logprobs.token_logprobs,&#xA;    chat_completion.choices[0].message.content&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the following preprint when referencing Evo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article {nguyen2024sequence,&#xA;&#x9;author = {Eric Nguyen and Michael Poli and Matthew G Durrant and Armin W Thomas and Brian Kang and Jeremy Sullivan and Madelena Y Ng and Ashley Lewis and Aman Patel and Aaron Lou and Stefano Ermon and Stephen A Baccus and Tina Hernandez-Boussard and Christopher R√© and Patrick D Hsu and Brian L Hie},&#xA;&#x9;title = {Sequence modeling and design from molecular to genome scale with Evo},&#xA;&#x9;year = {2024},&#xA;&#x9;doi = {10.1101/2024.02.27.582234},&#xA;&#x9;publisher = {Cold Spring Harbor Laboratory},&#xA;&#x9;URL = {https://www.biorxiv.org/content/early/2024/02/27/2024.02.27.582234},&#xA;&#x9;journal = {bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Sinaptik-AI/pandas-ai</title>
    <updated>2024-03-01T01:37:11Z</updated>
    <id>tag:github.com,2024-03-01:/Sinaptik-AI/pandas-ai</id>
    <link href="https://github.com/Sinaptik-AI/pandas-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with your data (SQL, CSV, pandas, polars, noSQL, etc). PandasAI makes data analysis conversational using LLMs (GPT 3.5 / 4, Anthropic, VertexAI) and RAG.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sinaptik-AI/pandas-ai/main/images/logo.png&#34; alt=&#34;PandasAI&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pandasai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pandasai?label=Release&amp;amp;style=flat-square&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg&#34;&gt;&lt;img src=&#34;https://github.com/gventuri/pandas-ai/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gventuri/pandas-ai/actions/workflows/cd.yml/badge.svg&#34;&gt;&lt;img src=&#34;https://github.com/gventuri/pandas-ai/actions/workflows/cd.yml/badge.svg?sanitize=true&#34; alt=&#34;CD&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/gventuri/pandas-ai&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/gventuri/pandas-ai/branch/main/graph/badge.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pandas-ai.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pandas-ai/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/kF7FqH2FwS&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;amp;compact=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/pandasai&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/pandasai&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PandasAI is a Python library that makes it easy to ask questions to your data in natural language. It helps you to explore, clean, and analyze your data using generative AI.&lt;/p&gt; &#xA;&lt;h1&gt;üîß Getting started&lt;/h1&gt; &#xA;&lt;p&gt;The documentation for PandasAI to use it with specific LLMs, vector stores and connectors, can be found &lt;a href=&#34;https://pandas-ai.readthedocs.io/en/latest/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üì¶ Installation&lt;/h2&gt; &#xA;&lt;p&gt;With pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pandasai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With poetry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry add pandasai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîç Demo&lt;/h2&gt; &#xA;&lt;p&gt;Try out PandasAI yourself in your browser:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üöÄ Deploying PandasAI&lt;/h1&gt; &#xA;&lt;p&gt;PandasAI can be deployed in a variety of ways. You can easily use it in your Jupyter notebooks or streamlit apps, or you can deploy it as a REST API such as with FastAPI or Flask.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, take a look at &lt;a href=&#34;https://pandas-ai.com&#34;&gt;our website&lt;/a&gt; or &lt;a href=&#34;https://zcal.co/gventuri/pandas-ai-demo&#34;&gt;book a meeting with us&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Ask questions&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;from pandasai import SmartDataframe&#xA;&#xA;# Sample DataFrame&#xA;sales_by_country = pd.DataFrame({&#xA;    &#34;country&#34;: [&#34;United States&#34;, &#34;United Kingdom&#34;, &#34;France&#34;, &#34;Germany&#34;, &#34;Italy&#34;, &#34;Spain&#34;, &#34;Canada&#34;, &#34;Australia&#34;, &#34;Japan&#34;, &#34;China&#34;],&#xA;    &#34;sales&#34;: [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]&#xA;})&#xA;&#xA;# Instantiate a LLM&#xA;from pandasai.llm import OpenAI&#xA;llm = OpenAI(api_token=&#34;YOUR_API_TOKEN&#34;)&#xA;&#xA;df = SmartDataframe(sales_by_country, config={&#34;llm&#34;: llm})&#xA;df.chat(&#39;Which are the top 5 countries by sales?&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;China, United States, Japan, Germany, Australia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Or you can ask more complex questions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.chat(&#xA;    &#34;What is the total sales for the top 3 countries by sales?&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;The total sales for the top 3 countries by sales is 16500.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualize charts&lt;/h3&gt; &#xA;&lt;p&gt;You can also ask PandasAI to generate charts for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.chat(&#xA;    &#34;Plot the histogram of countries showing for each the gdp, using different colors for each bar&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sinaptik-AI/pandas-ai/main/images/histogram-chart.png?raw=true&#34; alt=&#34;Chart&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multiple DataFrames&lt;/h3&gt; &#xA;&lt;p&gt;You can also pass in multiple dataframes to PandasAI and ask questions relating them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd&#xA;from pandasai import SmartDatalake&#xA;from pandasai.llm import OpenAI&#xA;&#xA;employees_data = {&#xA;    &#39;EmployeeID&#39;: [1, 2, 3, 4, 5],&#xA;    &#39;Name&#39;: [&#39;John&#39;, &#39;Emma&#39;, &#39;Liam&#39;, &#39;Olivia&#39;, &#39;William&#39;],&#xA;    &#39;Department&#39;: [&#39;HR&#39;, &#39;Sales&#39;, &#39;IT&#39;, &#39;Marketing&#39;, &#39;Finance&#39;]&#xA;}&#xA;&#xA;salaries_data = {&#xA;    &#39;EmployeeID&#39;: [1, 2, 3, 4, 5],&#xA;    &#39;Salary&#39;: [5000, 6000, 4500, 7000, 5500]&#xA;}&#xA;&#xA;employees_df = pd.DataFrame(employees_data)&#xA;salaries_df = pd.DataFrame(salaries_data)&#xA;&#xA;&#xA;llm = OpenAI()&#xA;dl = SmartDatalake([employees_df, salaries_df], config={&#34;llm&#34;: llm})&#xA;dl.chat(&#34;Who gets paid the most?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find more examples in the &lt;a href=&#34;https://raw.githubusercontent.com/Sinaptik-AI/pandas-ai/main/examples&#34;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;üîí Privacy &amp;amp; Security&lt;/h2&gt; &#xA;&lt;p&gt;In order to generate the Python code to run, we take some random samples from the dataframe, we randomize it (using random generation for sensitive data and shuffling for non-sensitive data) and send just the randomized head to the LLM.&lt;/p&gt; &#xA;&lt;p&gt;If you want to enforce further your privacy you can instantiate PandasAI with &lt;code&gt;enforce_privacy = True&lt;/code&gt; which will not send the head (but just column names) to the LLM.&lt;/p&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;PandasAI is available under the MIT expat license, except for the &lt;code&gt;pandasai/ee&lt;/code&gt; directory (which has it&#39;s &lt;a href=&#34;https://github.com/Sinaptik-AI/pandas-ai/raw/master/pandasai/ee/LICENSE&#34;&gt;license here&lt;/a&gt; if applicable.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, take a look at &lt;a href=&#34;https://pandas-ai.com&#34;&gt;our website&lt;/a&gt; or &lt;a href=&#34;https://zcal.co/gventuri/pandas-ai-demo&#34;&gt;book a meeting with us&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pandas-ai.readthedocs.io/en/latest/&#34;&gt;Docs&lt;/a&gt; for comprehensive documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Sinaptik-AI/pandas-ai/main/examples&#34;&gt;Examples&lt;/a&gt; for example notebooks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/kF7FqH2FwS&#34;&gt;Discord&lt;/a&gt; for discussion with the community and PandasAI team&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Please check the outstanding issues and feel free to open a pull request. For more information, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/Sinaptik-AI/pandas-ai/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Thank you!&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gventuri/pandas-ai/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=gventuri/pandas-ai&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>