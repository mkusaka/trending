<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-30T01:35:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepseek-ai/ESFT</title>
    <updated>2025-01-30T01:35:58Z</updated>
    <id>tag:github.com,2025-01-30:/deepseek-ai/ESFT</id>
    <link href="https://github.com/deepseek-ai/ESFT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Expert Specialized Fine-Tuning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Expert-Specialized Fine-Tuning&lt;/h1&gt; &#xA;&lt;p&gt;Official Repo for paper &lt;a href=&#34;https://arxiv.org/abs/2407.01906&#34;&gt;Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models&lt;/a&gt; by &lt;a href=&#34;https://zihanwang314.github.io&#34;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&#34;https://victorchen96.github.io/chendeli.io/&#34;&gt;Deli Chen&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=8b-ysf0NWVoC&amp;amp;hl=zh-CN&#34;&gt;Damai Dai&lt;/a&gt;, &lt;a href=&#34;https://runxinxu.github.io/aboutme/&#34;&gt;Runxin Xu&lt;/a&gt;, &lt;a href=&#34;http://www.idi.zju.edu.cn/member/3053.html&#34;&gt;Zhuoshu Li&lt;/a&gt; and Y. Wu.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ESFT&lt;/strong&gt; aims to efficiently customize Large Language Models (LLMs) with Mixture-of-Experts (MoE) architecture by adjusting only task-relevant parts, improving efficiency and performance while using fewer resources and storage.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“° News&lt;/h2&gt; &#xA;&lt;p&gt;ğŸ“… &lt;strong&gt;2024.9.20:&lt;/strong&gt; Glad to announce that ESFT has been accepted to the &lt;strong&gt;EMNLP 2024 Main Conference&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;ğŸ“… &lt;strong&gt;2024.8.11:&lt;/strong&gt; We now release the &lt;strong&gt;ESFT training code&lt;/strong&gt;! âœ¨ You can now try it with your own models and dataset!&lt;/p&gt; &#xA;&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation and Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/deepseek-ai/ESFT.git&#xA;cd esft&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install required dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers torch safetensors accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download necessary adapters&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/download_adapters.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ”§Key Scripts&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;eval_multigpu.py&lt;/strong&gt; This script evaluates the performance of the model on various datasets. See &lt;strong&gt;scripts/eval.sh&lt;/strong&gt; for detailed configs and explanations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval_multigpu.py \&#xA;    --eval_dataset=translation \&#xA;    --base_model_path=deepseek-ai/ESFT-vanilla-lite \&#xA;    --adapter_dir=all_models/adapters/token/translation \&#xA;    --output_path=results/completions/token/translation.jsonl \&#xA;    --openai_api_key=YOUR_OPENAI_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;get_expert_scores.py&lt;/strong&gt; This script calculates the scores for each expert based on the evaluation datasets. &lt;strong&gt;Usage:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/expert/get_expert_scores.py \&#xA;    --eval_dataset=translation \&#xA;    --base_model_path=deepseek-ai/ESFT-vanilla-lite \&#xA;    --output_dir=results/expert_scores/translation \&#xA;    --n_sample_tokens=131072 \&#xA;    --world_size=4 \&#xA;    --gpus_per_rank=2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;generate_expert_config.py&lt;/strong&gt; This script generates the configuration to convert a MoE model with only task-relevant tasks trained based on evaluation scores. &lt;strong&gt;Usage:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/expert/generate_expert_config.py \&#xA;    --eval_datasets=intent,summary,law,translation \&#xA;    --expert_scores_dir=results/expert_scores \&#xA;    --output_dir=results/expert_configs \&#xA;    --score_function=token \&#xA;    --top_p=0.2 # the scoring function and top_p are hyperparameters&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;train.py&lt;/strong&gt; and &lt;strong&gt;train_ep.py&lt;/strong&gt; This script trains the model with the expert configuration generated by the previous script. The train_ep.py file uses expert parallel and has been optimized for multi-GPU training. &lt;strong&gt;Usage:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py \&#xA;    --base_model_path=deepseek-ai/ESFT-vanilla-lite \&#xA;    --expert_config=results/expert_configs/intent.json \&#xA;    --train_dataset=intent \&#xA;    --train_config=configs/base.yaml \&#xA;    --output_dir=results/checkpoints/intent&#xA;    &#xA;torchrun --nproc-per-node=8 train_ep.py \&#xA;    --base_model_path=deepseek-ai/ESFT-vanilla-lite \&#xA;    --expert_config=results/expert_configs/translation.json \&#xA;    --train_dataset=translation \&#xA;    --train_config=configs/base.yaml \&#xA;    --output_dir=results/checkpoints/translation&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact and Support&lt;/h2&gt; &#xA;&lt;p&gt;For bug reports, feature requests, and general inquiries, please open an issue on our GitHub Issues page. Make sure to include as much detail as possible to help us address your issue quickly.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸŒŸTodo list&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;â˜‘ï¸ ğŸ“ Update models, evaluation scripts, and expert selection scripts&lt;/li&gt; &#xA; &lt;li&gt;â˜‘ï¸ ğŸ”§ Update training scripts&lt;/li&gt; &#xA; &lt;li&gt;ğŸ”² ğŸš€ More...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ“šCitation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our code or paper useful, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{wang2024letexpertsticklast,&#xA;      title={Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models}, &#xA;      author={Zihan Wang and Deli Chen and Damai Dai and Runxin Xu and Zhuoshu Li and Y. Wu},&#xA;      year={2024},&#xA;      eprint={2407.01906},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      url={https://arxiv.org/abs/2407.01906}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>