<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-06T01:31:45Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Megvii-BaseDetection/YOLOX</title>
    <updated>2022-07-06T01:31:45Z</updated>
    <id>tag:github.com,2022-07-06:/Megvii-BaseDetection/YOLOX</id>
    <link href="https://github.com/Megvii-BaseDetection/YOLOX" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/logo.png&#34; width=&#34;350&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/demo.png&#34;&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;YOLOX is an anchor-free version of YOLO, with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details, please refer to our &lt;a href=&#34;https://arxiv.org/abs/2107.08430&#34;&gt;report on Arxiv&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This repo is an implementation of PyTorch version YOLOX, there is also a &lt;a href=&#34;https://github.com/MegEngine/YOLOX&#34;&gt;MegEngine implementation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/git_fig.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Updates!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;【2022/04/14】 We suport jit compile op.&lt;/li&gt; &#xA; &lt;li&gt;【2021/08/19】 We optimize the training process with &lt;strong&gt;2x&lt;/strong&gt; faster training and &lt;strong&gt;~1%&lt;/strong&gt; higher performance! See &lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/docs/updates_note.md&#34;&gt;notes&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;【2021/08/05】 We release &lt;a href=&#34;https://github.com/MegEngine/YOLOX&#34;&gt;MegEngine version YOLOX&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;【2021/07/28】 We fix the fatal error of &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/issues/103&#34;&gt;memory leak&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;【2021/07/26】 We now support &lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/tree/main/demo/MegEngine&#34;&gt;MegEngine&lt;/a&gt; deployment.&lt;/li&gt; &#xA; &lt;li&gt;【2021/07/20】 We have released our technical report on &lt;a href=&#34;https://arxiv.org/abs/2107.08430&#34;&gt;Arxiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Coming soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; YOLOX-P6 and larger model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Objects365 pretrain.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Transformer modules.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More features in need.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;h4&gt;Standard Models.&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;test&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Speed V100&lt;br&gt;(ms)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;br&gt;(M)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;br&gt;(G)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_s.py&#34;&gt;YOLOX-s&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_m.py&#34;&gt;YOLOX-m&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_m.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_l.py&#34;&gt;YOLOX-l&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;155.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_l.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_x.py&#34;&gt;YOLOX-x&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;99.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;281.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_x.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolov3.py&#34;&gt;YOLOX-Darknet53&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;185.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_darknet.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Legacy models&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;size&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;test&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Speed V100&lt;br&gt;(ms)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Params&lt;br&gt;(M)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;FLOPs&lt;br&gt;(G)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_s.py&#34;&gt;YOLOX-s&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;39.6&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;26.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EW62gmO2vnNNs5npxjzunVwB9p307qqygaCkXdTO88BLUg?e=NMTQYw&#34;&gt;onedrive&lt;/a&gt;/&lt;a href=&#34;https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_m.py&#34;&gt;YOLOX-m&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;46.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;12.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;73.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/ERMTP7VFqrVBrXKMU7Vl4TcBQs0SUeCT7kvc-JdIbej4tQ?e=1MDo9y&#34;&gt;onedrive&lt;/a&gt;/&lt;a href=&#34;https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_m.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_l.py&#34;&gt;YOLOX-l&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;14.5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;54.2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;155.6&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EWA8w_IEOzBKvuueBqfaZh0BeoG5sVzR-XYbOJO4YlOkRw?e=wHWOBE&#34;&gt;onedrive&lt;/a&gt;/&lt;a href=&#34;https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_l.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_x.py&#34;&gt;YOLOX-x&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.2&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;17.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;99.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;281.9&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EdgVPHBziOVBtGAXHfeHI5kBza0q9yyueMGdT0wXZfI1rQ?e=tABO5u&#34;&gt;onedrive&lt;/a&gt;/&lt;a href=&#34;https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_x.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolov3.py&#34;&gt;YOLOX-Darknet53&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;47.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;11.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;63.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;185.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EZ-MV1r_fMFPkPrNjvbJEMoBLOLAnXH-XKEB77w8LhXL6Q?e=mf6wOc&#34;&gt;onedrive&lt;/a&gt;/&lt;a href=&#34;https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_darknet53.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Light Models.&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;br&gt;(M)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;br&gt;(G)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/nano.py&#34;&gt;YOLOX-Nano&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_nano.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_tiny.py&#34;&gt;YOLOX-Tiny&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;416&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Legacy models&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;size&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;mAP&lt;sup&gt;val&lt;br&gt;0.5:0.95&lt;/sup&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Params&lt;br&gt;(M)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;FLOPs&lt;br&gt;(G)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;weights&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/nano.py&#34;&gt;YOLOX-Nano&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;416&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;25.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.91&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.08&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_nano.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/exps/default/yolox_tiny.py&#34;&gt;YOLOX-Tiny&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;416&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;32.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5.06&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;6.45&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_tiny_32dot8.pth&#34;&gt;github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Installation&lt;/summary&gt; &#xA; &lt;p&gt;Step1. Install YOLOX from source.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:Megvii-BaseDetection/YOLOX.git&#xA;cd YOLOX&#xA;pip3 install -v -e .  # or  python3 setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Demo&lt;/summary&gt; &#xA; &lt;p&gt;Step1. Download a pretrained model from the benchmark table.&lt;/p&gt; &#xA; &lt;p&gt;Step2. Use either -n or -f to specify your detector&#39;s config. For example:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/demo.py image -n yolox-s -c /path/to/your/yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;or&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/demo.py image -f exps/default/yolox_s.py -c /path/to/your/yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Demo for video:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/demo.py video -n yolox-s -c /path/to/your/yolox_s.pth --path /path/to/your/video --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Reproduce our results on COCO&lt;/summary&gt; &#xA; &lt;p&gt;Step1. Prepare COCO dataset&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd &amp;lt;YOLOX_HOME&amp;gt;&#xA;ln -s /path/to/your/COCO ./datasets/COCO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Step2. Reproduce our results on COCO by specifying -n:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m yolox.tools.train -n yolox-s -d 8 -b 64 --fp16 -o [--cache]&#xA;                               yolox-m&#xA;                               yolox-l&#xA;                               yolox-x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;-d: number of gpu devices&lt;/li&gt; &#xA;  &lt;li&gt;-b: total batch size, the recommended number for -b is num-gpu * 8&lt;/li&gt; &#xA;  &lt;li&gt;--fp16: mixed precision training&lt;/li&gt; &#xA;  &lt;li&gt;--cache: caching imgs into RAM to accelarate training, which need large system RAM.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;When using -f, the above commands are equivalent to:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m yolox.tools.train -f exps/default/yolox_s.py -d 8 -b 64 --fp16 -o [--cache]&#xA;                               exps/default/yolox_m.py&#xA;                               exps/default/yolox_l.py&#xA;                               exps/default/yolox_x.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Multi Machine Training&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;We also support multi-nodes training. Just add the following args:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;--num_machines: num of your total training nodes&lt;/li&gt; &#xA;  &lt;li&gt;--machine_rank: specify the rank of each node&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Suppose you want to train YOLOX on 2 machines, and your master machines&#39;s IP is 123.123.123.123, use port 12312 and TCP.&lt;br&gt; On master machine, run&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/train.py -n yolox-s -b 128 --dist-url tcp://123.123.123.123:12312 --num_machines 2 --machine_rank 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;On the second machine, run&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/train.py -n yolox-s -b 128 --dist-url tcp://123.123.123.123:12312 --num_machines 2 --machine_rank 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Logging to Weights &amp;amp; Biases&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;To log metrics, predictions and model checkpoints to &lt;a href=&#34;https://docs.wandb.ai/guides/integrations/other/yolox&#34;&gt;W&amp;amp;B&lt;/a&gt; use the command line argument &lt;code&gt;--logger wandb&lt;/code&gt; and use the prefix &#34;wandb-&#34; to specify arguments for initializing the wandb run.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/train.py -n yolox-s -d 8 -b 64 --fp16 -o [--cache] --logger wandb wandb-project &amp;lt;project name&amp;gt;&#xA;                         yolox-m&#xA;                         yolox-l&#xA;                         yolox-x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;An example wandb dashboard is available &lt;a href=&#34;https://wandb.ai/manan-goel/yolox-nano/runs/3pzfeom0&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Others&lt;/strong&gt;&lt;br&gt; See more information with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m yolox.tools.train --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Evaluation&lt;/summary&gt; &#xA; &lt;p&gt;We support batch testing for fast evaluation:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m yolox.tools.eval -n  yolox-s -c yolox_s.pth -b 64 -d 8 --conf 0.001 [--fp16] [--fuse]&#xA;                               yolox-m&#xA;                               yolox-l&#xA;                               yolox-x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;--fuse: fuse conv and bn&lt;/li&gt; &#xA;  &lt;li&gt;-d: number of GPUs used for evaluation. DEFAULT: All GPUs available will be used.&lt;/li&gt; &#xA;  &lt;li&gt;-b: total batch size across on all GPUs&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;To reproduce speed test, we use the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m yolox.tools.eval -n  yolox-s -c yolox_s.pth -b 1 -d 1 --conf 0.001 --fp16 --fuse&#xA;                               yolox-m&#xA;                               yolox-l&#xA;                               yolox-x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tutorials&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/docs/train_custom_data.md&#34;&gt;Training on custom data&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/docs/manipulate_training_image_size.md&#34;&gt;Manipulating training image size&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/docs/freeze_module.md&#34;&gt;Freezing model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/demo/MegEngine&#34;&gt;MegEngine in C++ and Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/demo/ONNXRuntime&#34;&gt;ONNX export and an ONNXRuntime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/demo/TensorRT&#34;&gt;TensorRT in C++ and Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/demo/ncnn&#34;&gt;ncnn in C++ and Java&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/demo/OpenVINO&#34;&gt;OpenVINO in C++ and Python&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Third-party resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;YOLOX for streaming perception: &lt;a href=&#34;https://github.com/yancie-yjr/StreamYOLO&#34;&gt;StreamYOLO (CVPR 2022 Oral)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces 🤗&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/Sultannn/YOLOX-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The ncnn android app with video support: &lt;a href=&#34;https://github.com/FeiGeChuanShu/ncnn-android-yolox&#34;&gt;ncnn-android-yolox&lt;/a&gt; from &lt;a href=&#34;https://github.com/FeiGeChuanShu&#34;&gt;FeiGeChuanShu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOX with Tengine support: &lt;a href=&#34;https://github.com/OAID/Tengine/raw/tengine-lite/examples/tm_yolox.cpp&#34;&gt;Tengine&lt;/a&gt; from &lt;a href=&#34;https://github.com/BUG1989&#34;&gt;BUG1989&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOX + ROS2 Foxy: &lt;a href=&#34;https://github.com/Ar-Ray-code/YOLOX-ROS&#34;&gt;YOLOX-ROS&lt;/a&gt; from &lt;a href=&#34;https://github.com/Ar-Ray-code&#34;&gt;Ar-Ray&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOX Deploy DeepStream: &lt;a href=&#34;https://github.com/nanmi/YOLOX-deepstream&#34;&gt;YOLOX-deepstream&lt;/a&gt; from &lt;a href=&#34;https://github.com/nanmi&#34;&gt;nanmi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLOX MNN/TNN/ONNXRuntime: &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/mnn/cv/mnn_yolox.cpp&#34;&gt;YOLOX-MNN&lt;/a&gt;、&lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/tnn/cv/tnn_yolox.cpp&#34;&gt;YOLOX-TNN&lt;/a&gt; and &lt;a href=&#34;https://github.com/DefTruth/lite.ai.toolkit/raw/main/lite/ort/cv/yolox.cpp&#34;&gt;YOLOX-ONNXRuntime C++&lt;/a&gt; from &lt;a href=&#34;https://github.com/DefTruth&#34;&gt;DefTruth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Converting darknet or yolov5 datasets to COCO format for YOLOX: &lt;a href=&#34;https://github.com/RapidAI/YOLO2COCO&#34;&gt;YOLO2COCO&lt;/a&gt; from &lt;a href=&#34;https://github.com/znsoftm&#34;&gt;Daniel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite YOLOX&lt;/h2&gt; &#xA;&lt;p&gt;If you use YOLOX in your research, please cite our work by using the following BibTeX entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt; @article{yolox2021,&#xA;  title={YOLOX: Exceeding YOLO Series in 2021},&#xA;  author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},&#xA;  journal={arXiv preprint arXiv:2107.08430},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;In memory of Dr. Jian Sun&lt;/h2&gt; &#xA;&lt;p&gt;Without the guidance of &lt;a href=&#34;http://www.jiansun.org/&#34;&gt;Dr. Sun Jian&lt;/a&gt;, YOLOX would not have been released and open sourced to the community. The passing away of Dr. Sun Jian is a great loss to the Computer Vision field. We have added this section here to express our remembrance and condolences to our captain Dr. Sun. It is hoped that every AI practitioner in the world will stick to the concept of &#34;continuous innovation to expand cognitive boundaries, and extraordinary technology to achieve product value&#34; and move forward all the way.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/sunjian.png&#34; width=&#34;200&#34;&gt;&#xA;&lt;/div&gt; 没有孙剑博士的指导，YOLOX也不会问世并开源给社区使用。 孙剑博士的离去是CV领域的一大损失，我们在此特别添加了这个部分来表达对我们的“船长”孙老师的纪念和哀思。 希望世界上的每个AI从业者秉持着“持续创新拓展认知边界，非凡科技成就产品价值”的观念，一路向前。</summary>
  </entry>
  <entry>
    <title>babysor/MockingBird</title>
    <updated>2022-07-06T01:31:45Z</updated>
    <id>tag:github.com,2022-07-06:/babysor/MockingBird</id>
    <link href="https://github.com/babysor/MockingBird" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🚀AI拟声: 5秒内克隆您的声音并生成任意语音内容 Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg&#34; alt=&#34;mockingbird&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://choosealicense.com/licenses/mit/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?style=flat&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/babysor/MockingBird/main/README-CN.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;🌍 &lt;strong&gt;Chinese&lt;/strong&gt; supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.&lt;/p&gt; &#xA;&lt;p&gt;🤩 &lt;strong&gt;PyTorch&lt;/strong&gt; worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060&lt;/p&gt; &#xA;&lt;p&gt;🌍 &lt;strong&gt;Windows + Linux&lt;/strong&gt; run in both Windows OS and linux OS (even in M1 MACOS)&lt;/p&gt; &#xA;&lt;p&gt;🤩 &lt;strong&gt;Easy &amp;amp; Awesome&lt;/strong&gt; effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder&lt;/p&gt; &#xA;&lt;p&gt;🌍 &lt;strong&gt;Webserver Ready&lt;/strong&gt; to serve your result with remote calling&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV17Q4y1B7mY/&#34;&gt;DEMO VIDEO&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Ongoing Works(Helps Needed)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Major upgrade on GUI/Client and unifying web and toolbox [X] Init framework &lt;code&gt;./mkgui&lt;/code&gt; and &lt;a href=&#34;https://vaj2fgg8yn.feishu.cn/docs/doccnvotLWylBub8VJIjKzoEaee&#34;&gt;tech design&lt;/a&gt; [X] Add demo part of Voice Cloning and Conversion [X] Add preprocessing and training for Voice Conversion [ ] Add preprocessing and training for Encoder/Synthesizer/Vocoder&lt;/li&gt; &#xA; &lt;li&gt;Major upgrade on model backend based on ESPnet2(not yet started)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install Requirements&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Follow the original repo to test if you got all environment ready. **Python 3.7 or higher ** is needed to run the toolbox.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you get an &lt;code&gt;ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )&lt;/code&gt; This error is probably due to a low version of python, try using 3.9 and it will install successfully&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://ffmpeg.org/download.html#get-packages&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the remaining necessary packages.&lt;/li&gt; &#xA; &lt;li&gt;Install webrtcvad &lt;code&gt;pip install webrtcvad-wheels&lt;/code&gt;(If you need)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note that we are using the pretrained encoder/vocoder but synthesizer, since the original model is incompatible with the Chinese sympols. It means the demo_cli is not working at this moment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2. Prepare your models&lt;/h3&gt; &#xA;&lt;p&gt;You can either train your models or use existing ones:&lt;/p&gt; &#xA;&lt;h4&gt;2.1 Train encoder with your dataset (Optional)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python encoder_preprocess.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the encoder: &lt;code&gt;python encoder_train.py my_run &amp;lt;datasets_root&amp;gt;/SV2TTS/encoder&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For training, the encoder uses visdom. You can disable it with &lt;code&gt;--no_visdom&lt;/code&gt;, but it&#39;s nice to have. Run &#34;visdom&#34; in a separate CLI/process to start your visdom server.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.2 Train synthesizer with your dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download dataset and unzip: make sure you can access all .wav in folder&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python pre.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the synthesizer: &lt;code&gt;python synthesizer_train.py mandarin &amp;lt;datasets_root&amp;gt;/SV2TTS/synthesizer&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to next step when you see attention line show and loss meet your need in training folder &lt;em&gt;synthesizer/saved_models/&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2.3 Use pretrained model of synthesizer&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Thanks to the community, some models will be shared:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;author&lt;/th&gt; &#xA;   &lt;th&gt;Download link&lt;/th&gt; &#xA;   &lt;th&gt;Preview Video&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;Baidu&lt;/a&gt; 4j5d&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;75k steps trained by multiple datasets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;Baidu&lt;/a&gt; code：om7f&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25k steps trained by multiple datasets, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@FawenYo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&#34;&gt;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&lt;/a&gt; &lt;a href=&#34;https://u.teknik.io/AYxWf.pt&#34;&gt;https://u.teknik.io/AYxWf.pt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3&#34;&gt;input&lt;/a&gt; &lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/export.wav&#34;&gt;output&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;200k steps with local accent of Taiwan, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@miven&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&#34;&gt;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&lt;/a&gt; code: 2021 &lt;a href=&#34;https://www.aliyundrive.com/s/AwPsbo8mcSP&#34;&gt;https://www.aliyundrive.com/s/AwPsbo8mcSP&lt;/a&gt; code: z2m0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1uh411B7AD/&#34;&gt;https://www.bilibili.com/video/BV1uh411B7AD/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.4 Train vocoder (Optional)&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;note: vocoder has little difference in effect, so you may not need to train a new one.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Preprocess the data: &lt;code&gt;python vocoder_preprocess.py &amp;lt;datasets_root&amp;gt; -m &amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; replace with your dataset root，&lt;code&gt;&amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;replace with directory of your best trained models of sythensizer, e.g. &lt;em&gt;sythensizer\saved_mode\xxx&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the wavernn vocoder: &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the hifigan vocoder &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt; hifigan&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Launch&lt;/h3&gt; &#xA;&lt;h4&gt;3.1 Using the web server&lt;/h4&gt; &#xA;&lt;p&gt;You can then try to run:&lt;code&gt;python web.py&lt;/code&gt; and open it in browser, default as &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2 Using the Toolbox&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the toolbox: &lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3 Using the command line&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the command: &lt;code&gt;python gen_voice.py &amp;lt;text_file.txt&amp;gt; your_wav_file.wav&lt;/code&gt; you may need to install cn2an by &#34;pip install cn2an&#34; for better digital number result.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repository is forked from &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning&#34;&gt;Real-Time-Voice-Cloning&lt;/a&gt; which only support English.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;Designation&lt;/th&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Implementation source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1803.09017&#34;&gt;1803.09017&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GlobalStyleToken (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;2010.05646&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HiFi-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02297&#34;&gt;2106.02297&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN: Adversarial Frequency-consistent Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.08435.pdf&#34;&gt;1802.08435&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;1703.10135&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10467.pdf&#34;&gt;1710.10467&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GE2E (encoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;F Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;h4&gt;1.Where can I download the dataset?&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Original Source&lt;/th&gt; &#xA;   &lt;th&gt;Alternative Sources&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aidatatang_200zh&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/62/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;magicdata&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/68/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing&#34;&gt;Google Drive (Dev set)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aishell3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/93/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;data_aishell&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/33/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;After unzip aidatatang_200zh, you need to unzip all the files under &lt;code&gt;aidatatang_200zh\corpus\train&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.What is&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;p&gt;If the dataset path is &lt;code&gt;D:\data\aidatatang_200zh&lt;/code&gt;,then &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is&lt;code&gt;D:\data&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.Not enough VRAM&lt;/h4&gt; &#xA;&lt;p&gt;Train the synthesizer：adjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  12),   #&#xA;                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  12)],  # lr = learning rate&#xA;//After&#xA;tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  8),   #&#xA;                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  8)],  # lr = learning rate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Preprocess the data：adjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.&#xA;//After&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Train the vocoder：adjust the batch_size in &lt;code&gt;vocoder/wavernn/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;# Training&#xA;voc_batch_size = 100&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad = 2&#xA;&#xA;//After&#xA;# Training&#xA;voc_batch_size = 6&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad =2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4.If it happens &lt;code&gt;RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to issue &lt;a href=&#34;https://github.com/babysor/MockingBird/issues/37&#34;&gt;#37&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5. How to improve CPU and GPU occupancy rate?&lt;/h4&gt; &#xA;&lt;p&gt;Adjust the batch_size as appropriate to improve&lt;/p&gt; &#xA;&lt;h4&gt;6. What if it happens &lt;code&gt;the page file is too small to complete the operation&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to this &lt;a href=&#34;https://www.youtube.com/watch?v=Oh6dga-Oy10&amp;amp;ab_channel=CodeProf&#34;&gt;video&lt;/a&gt; and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.&lt;/p&gt; &#xA;&lt;h4&gt;7. When should I stop during training?&lt;/h4&gt; &#xA;&lt;p&gt;FYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps. &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png&#34; alt=&#34;attention_step_20500_sample_1&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png&#34; alt=&#34;step-135500-mel-spectrogram_sample_1&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tobymao/sqlglot</title>
    <updated>2022-07-06T01:31:45Z</updated>
    <id>tag:github.com,2022-07-06:/tobymao/sqlglot</id>
    <link href="https://github.com/tobymao/sqlglot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python SQL Parser and Transpiler&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SQLGlot&lt;/h1&gt; &#xA;&lt;p&gt;SQLGlot is a no dependency Python SQL parser, transpiler, and optimizer. It can be used to format SQL or translate between different dialects like Presto, Spark, and Hive. It aims to read a wide variety of SQL inputs and output syntatically correct SQL in the targeted dialects.&lt;/p&gt; &#xA;&lt;p&gt;It is currently the &lt;a href=&#34;https://raw.githubusercontent.com/tobymao/sqlglot/main/#benchmarks&#34;&gt;fastest&lt;/a&gt; pure-Python SQL parser.&lt;/p&gt; &#xA;&lt;p&gt;You can easily customize the parser to support UDF&#39;s across dialects as well through the transform API.&lt;/p&gt; &#xA;&lt;p&gt;Syntax errors are highlighted and dialect incompatibilities can warn or raise depending on configurations.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;From PyPI&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install sqlglot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or with a local checkout&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Easily translate from one dialect to another. For example, date/time functions vary from dialects and can be hard to deal with.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sqlglot&#xA;sqlglot.transpile(&#34;SELECT EPOCH_MS(1618088028295)&#34;, read=&#39;duckdb&#39;, write=&#39;hive&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT TO_UTC_TIMESTAMP(FROM_UNIXTIME(1618088028295 / 1000, &#39;yyyy-MM-dd HH:mm:ss&#39;), &#39;UTC&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;SQLGlot can even translate custom time formats.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sqlglot&#xA;sqlglot.transpile(&#34;SELECT STRFTIME(x, &#39;%y-%-m-%S&#39;)&#34;, read=&#39;duckdb&#39;, write=&#39;hive&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT DATE_FORMAT(x, &#39;yy-M-ss&#39;)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Formatting and Transpiling&lt;/h3&gt; &#xA;&lt;p&gt;Read in a SQL statement with a CTE and CASTING to a REAL and then transpiling to Spark.&lt;/p&gt; &#xA;&lt;p&gt;Spark uses backticks as identifiers and the REAL type is transpiled to FLOAT.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sqlglot&#xA;&#xA;sql = &#34;&#34;&#34;WITH baz AS (SELECT a, c FROM foo WHERE a = 1) SELECT f.a, b.b, baz.c, CAST(&#34;b&#34;.&#34;a&#34; AS REAL) d FROM foo f JOIN bar b ON f.a = b.a LEFT JOIN baz ON f.a = baz.a&#34;&#34;&#34;&#xA;sqlglot.transpile(sql, write=&#39;spark&#39;, identify=True, pretty=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;WITH `baz` AS (&#xA;  SELECT&#xA;    `a`,&#xA;    `c`&#xA;  FROM `foo`&#xA;  WHERE&#xA;    `a` = 1&#xA;)&#xA;SELECT&#xA;  `f`.`a`,&#xA;  `b`.`b`,&#xA;  `baz`.`c`,&#xA;  CAST(`b`.`a` AS FLOAT) AS `d`&#xA;FROM `foo` AS `f`&#xA;JOIN `bar` AS `b`&#xA;  ON `f`.`a` = `b`.`a`&#xA;LEFT JOIN `baz`&#xA;  ON `f`.`a` = `baz`.`a`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Metadata&lt;/h3&gt; &#xA;&lt;p&gt;You can explore SQL with expression helpers to do things like find columns and tables.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlglot import parse_one, exp&#xA;&#xA;# print all column references (a and b)&#xA;for column in parse_one(&#34;SELECT a, b + 1 AS c FROM d&#34;).find_all(exp.Column):&#xA;  print(column.alias_or_name)&#xA;&#xA;# find all projections in select statements (a and c)&#xA;for select in parse_one(&#34;SELECT a, b + 1 AS c FROM d&#34;).find_all(exp.Select):&#xA;  for projection in select.args[&#34;expressions&#34;]:&#xA;    print(projection.alias_or_name)&#xA;&#xA;# find all tables (x, y, z)&#xA;for table in parse_one(&#34;SELECT * FROM x JOIN y JOIN z&#34;).find_all(exp.Table):&#xA;  print(table.name)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Parser Errors&lt;/h3&gt; &#xA;&lt;p&gt;A syntax error will result in a parser error.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transpile(&#34;SELECT foo( FROM bar&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;sqlglot.errors.ParseError: Expecting ). Line 1, Col: 13.&#xA;  SELECT foo( FROM bar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Unsupported Errors&lt;/h3&gt; &#xA;&lt;p&gt;Presto APPROX_DISTINCT supports the accuracy argument which is not supported in Spark.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transpile(&#xA;    &#39;SELECT APPROX_DISTINCT(a, 0.1) FROM foo&#39;,&#xA;    read=&#39;presto&#39;,&#xA;    write=&#39;spark&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;WARNING:root:APPROX_COUNT_DISTINCT does not support accuracy&#xA;&#xA;SELECT APPROX_COUNT_DISTINCT(a) FROM foo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build and Modify SQL&lt;/h3&gt; &#xA;&lt;p&gt;SQLGlot supports incrementally building sql expressions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlglot import select, condition&#xA;&#xA;where = condition(&#34;x=1&#34;).and_(&#34;y=1&#34;)&#xA;select(&#34;*&#34;).from_(&#34;y&#34;).where(where).sql()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT * FROM y WHERE x = 1 AND y = 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also modify a parsed tree:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlglot import parse_one&#xA;&#xA;parse_one(&#34;SELECT x FROM y&#34;).from_(&#34;z&#34;).sql()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT x FROM y, z&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There is also a way to recursively transform the parsed tree by applying a mapping function to each tree node:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlglot import exp, parse_one&#xA;&#xA;expression_tree = parse_one(&#34;SELECT a FROM x&#34;)&#xA;&#xA;def transformer(node):&#xA;    if isinstance(node, exp.Column) and node.name == &#34;a&#34;:&#xA;        return parse_one(&#34;FUN(a)&#34;)&#xA;    return node&#xA;&#xA;transformed_tree = expression_tree.transform(transformer)&#xA;transformed_tree.sql()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which outputs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT FUN(a) FROM x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL Optimizer&lt;/h3&gt; &#xA;&lt;p&gt;SQLGlot can rewrite queries into an &#34;optimized&#34; form. It performs a variety of &lt;a href=&#34;https://raw.githubusercontent.com/tobymao/sqlglot/main/sqlglot/optimizer/optimizer.py&#34;&gt;techniques&lt;/a&gt; to create a new canonical AST. This AST can be used to standaradize queries or provide the foundations for implementing an actual engine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import sqlglot&#xA;from sqlglot.optimizer import optimize&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;optimize(&#xA;    sqlglot.parse_one(&#34;&#34;&#34;&#xA;    SELECT A OR (B OR (C AND D))&#xA;    FROM x&#xA;    WHERE Z = date &#39;2021-01-01&#39; + INTERVAL &#39;1&#39; month OR 1 = 0&#xA;    &#34;&#34;&#34;),&#xA;    schema={&#34;x&#34;: {&#34;A&#34;: &#34;INT&#34;, &#34;B&#34;: &#34;INT&#34;, &#34;C&#34;: &#34;INT&#34;, &#34;D&#34;: &#34;INT&#34;, &#34;Z&#34;: &#34;STRING&#34;}}&#xA;).sql(pretty=True)&#xA;&#xA;&#34;&#34;&#34;&#xA;SELECT&#xA;  (&#xA;    &#34;x&#34;.&#34;A&#34;&#xA;    OR &#34;x&#34;.&#34;B&#34;&#xA;    OR &#34;x&#34;.&#34;C&#34;&#xA;  )&#xA;  AND (&#xA;    &#34;x&#34;.&#34;A&#34;&#xA;    OR &#34;x&#34;.&#34;B&#34;&#xA;    OR &#34;x&#34;.&#34;D&#34;&#xA;  ) AS &#34;_col_0&#34;&#xA;FROM &#34;x&#34; AS &#34;x&#34;&#xA;WHERE&#xA;  &#34;x&#34;.&#34;Z&#34; = CAST(&#39;2021-02-01&#39; AS DATE)&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SQL Annotations&lt;/h3&gt; &#xA;&lt;p&gt;SQLGlot supports annotations in the sql expression. This is an experimental feature that is not part of any of the SQL standards but it can be useful when needing to annotate what a selected field is supposed to be. Below is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT&#xA;  user #primary_key,&#xA;  country&#xA;FROM users&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Customization&lt;/h3&gt; &#xA;&lt;h4&gt;Custom Types&lt;/h4&gt; &#xA;&lt;p&gt;A simple transform on types can be accomplished by providing a corresponding mapping:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from sqlglot import *&#xA;&#xA;transpile(&#34;SELECT CAST(a AS INT) FROM x&#34;, type_mapping={exp.DataType.Type.INT: &#34;SPECIAL INT&#34;})[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT CAST(a AS SPECIAL INT) FROM x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More complicated transforms can be accomplished by using the Tokenizer, Parser, and Generator directly.&lt;/p&gt; &#xA;&lt;h4&gt;Custom Functions&lt;/h4&gt; &#xA;&lt;p&gt;In this example, we want to parse a UDF SPECIAL_UDF and then output another version called SPECIAL_UDF_INVERSE with the arguments switched.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlglot import *&#xA;from sqlglot.expressions import Func&#xA;&#xA;class SpecialUdf(Func):&#xA;    arg_types = {&#39;a&#39;: True, &#39;b&#39;: True}&#xA;&#xA;tokens = Tokenizer().tokenize(&#34;SELECT SPECIAL_UDF(a, b) FROM x&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is the output of the tokenizer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;    &amp;lt;Token token_type: TokenType.SELECT, text: SELECT, line: 0, col: 0&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.VAR, text: SPECIAL_UDF, line: 0, col: 7&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.L_PAREN, text: (, line: 0, col: 18&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.VAR, text: a, line: 0, col: 19&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.COMMA, text: ,, line: 0, col: 20&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.VAR, text: b, line: 0, col: 22&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.R_PAREN, text: ), line: 0, col: 23&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.FROM, text: FROM, line: 0, col: 25&amp;gt;,&#xA;    &amp;lt;Token token_type: TokenType.VAR, text: x, line: 0, col: 30&amp;gt;,&#xA;]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;expression = Parser(functions={&#xA;    **SpecialUdf.default_parser_mappings(),&#xA;}).parse(tokens)[0]&#xA;&#xA;repr(expression)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The expression tree produced by the parser:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(SELECT distinct: False, expressions:&#xA;  (SPECIALUDF a:&#xA;    (COLUMN this:&#xA;      (IDENTIFIER this: a, quoted: False)), b:&#xA;    (COLUMN this:&#xA;      (IDENTIFIER this: b, quoted: False))), from:&#xA;  (FROM expressions:&#xA;    (TABLE this:&#xA;      (IDENTIFIER this: x, quoted: False))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally generating the new SQL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Generator(transforms={&#xA;    SpecialUdf: lambda self, e: f&#34;SPECIAL_UDF_INVERSE({self.sql(e, &#39;b&#39;)}, {self.sql(e, &#39;a&#39;)})&#34;&#xA;}).generate(expression)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT SPECIAL_UDF_INVERSE(b, a) FROM x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tobymao/sqlglot/main/benchmarks&#34;&gt;Benchmarks&lt;/a&gt; run on Python 3.9.6 in seconds.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Query&lt;/th&gt; &#xA;   &lt;th&gt;sqlglot&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/andialbrecht/sqlparse&#34;&gt;sqlparse&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/klahnakoski/mo-sql-parsing&#34;&gt;moz_sql_parser&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/wseaton/sqloxide/&#34;&gt;sqloxide&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;short&lt;/td&gt; &#xA;   &lt;td&gt;0.00038&lt;/td&gt; &#xA;   &lt;td&gt;0.00104&lt;/td&gt; &#xA;   &lt;td&gt;0.00174&lt;/td&gt; &#xA;   &lt;td&gt;0.000060&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;long&lt;/td&gt; &#xA;   &lt;td&gt;0.00508&lt;/td&gt; &#xA;   &lt;td&gt;0.01522&lt;/td&gt; &#xA;   &lt;td&gt;0.02162&lt;/td&gt; &#xA;   &lt;td&gt;0.000597&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;crazy&lt;/td&gt; &#xA;   &lt;td&gt;0.01871&lt;/td&gt; &#xA;   &lt;td&gt;3.49415&lt;/td&gt; &#xA;   &lt;td&gt;0.35346&lt;/td&gt; &#xA;   &lt;td&gt;0.003104&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Run Tests and Lint&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;./format_code.sh&#xA;./run_checks.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Optional Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;SQLGlot uses &lt;a href=&#34;https://github.com/dateutil/dateutil&#34;&gt;dateutil&lt;/a&gt; to simplify literal timedelta expressions. The optimizer will not simplify expressions like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;x + interval &#39;1&#39; month&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if the module cannot be found.&lt;/p&gt;</summary>
  </entry>
</feed>