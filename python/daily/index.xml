<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-07-19T01:34:20Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wenet-e2e/wenet</title>
    <updated>2024-07-19T01:34:20Z</updated>
    <id>tag:github.com,2024-07-19:/wenet-e2e/wenet</id>
    <link href="https://github.com/wenet-e2e/wenet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Production First and Production Ready End-to-End Speech Recognition Toolkit&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WeNet&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Python-3.7%7C3.8-brightgreen&#34; alt=&#34;Python-Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet/issues/1683&#34;&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://wenet-e2e.github.io/wenet/papers.html&#34;&gt;&lt;strong&gt;Papers&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime&#34;&gt;&lt;strong&gt;Runtime&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/pretrained_models.md&#34;&gt;&lt;strong&gt;Pretrained Models&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/wenet/wenet_demo&#34;&gt;&lt;strong&gt;HuggingFace&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;We&lt;/strong&gt; share &lt;strong&gt;Net&lt;/strong&gt; together.&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Production first and production ready&lt;/strong&gt;: The core design principle, WeNet provides full stack production solutions for speech recognition.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accurate&lt;/strong&gt;: WeNet achieves SOTA results on a lot of public speech datasets.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Light weight&lt;/strong&gt;: WeNet is easy to install, easy to use, well designed, and well documented.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Install python package&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://github.com/wenet-e2e/wenet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Command-line usage&lt;/strong&gt; (use &lt;code&gt;-h&lt;/code&gt; for parameters):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wenet --language chinese audio.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python programming usage&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import wenet&#xA;&#xA;model = wenet.load_model(&#39;chinese&#39;)&#xA;result = model.transcribe(&#39;audio.wav&#39;)&#xA;print(result[&#39;text&#39;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer &lt;a href=&#34;https://raw.githubusercontent.com/wenet-e2e/wenet/main/docs/python_package.md&#34;&gt;python usage&lt;/a&gt; for more command line and python programming usage.&lt;/p&gt; &#xA;&lt;h3&gt;Install for training &amp;amp; deployment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the repo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/wenet-e2e/wenet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Conda: please see &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create Conda env:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda create -n wenet python=3.10&#xA;conda activate wenet&#xA;conda install conda-forge::sox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install CUDA: please follow this &lt;a href=&#34;https://icefall.readthedocs.io/en/latest/installation/index.html#id1&#34;&gt;link&lt;/a&gt;, It&#39;s recomended to install CUDA 12.1&lt;/li&gt; &#xA; &lt;li&gt;Install torch and torchaudio, It&#39;s recomended to use 2.2.2+cu121:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install torch==2.2.2+cu121 torchaudio==2.2.2+cu121 -f https://download.pytorch.org/whl/torch_stable.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install other python packages&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;pre-commit install  # for clean and tidy code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Frequently Asked Questions (FAQs)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# If you encounter sox compatibility issues&#xA;RuntimeError: set_buffer_size requires sox extension which is not available.&#xA;# ubuntu&#xA;sudo apt-get install sox libsox-dev&#xA;# centos&#xA;sudo yum install sox sox-devel&#xA;# conda env&#xA;conda install  conda-forge::sox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Build for deployment&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optionally, if you want to use x86 runtime or language model(LM), you have to build the runtime as follows. Otherwise, you can just ignore this step.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# runtime build requires cmake 3.14 or above&#xA;cd runtime/libtorch&#xA;mkdir build &amp;amp;&amp;amp; cd build &amp;amp;&amp;amp; cmake -DGRAPH_TOOLS=ON .. &amp;amp;&amp;amp; cmake --build .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://github.com/wenet-e2e/wenet/tree/main/runtime&#34;&gt;doc&lt;/a&gt; for building runtime on more platforms and OS.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion &amp;amp; Communication&lt;/h2&gt; &#xA;&lt;p&gt;You can directly discuss on &lt;a href=&#34;https://github.com/wenet-e2e/wenet/issues&#34;&gt;Github Issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Chinese users, you can aslo scan the QR code on the left to follow our offical account of WeNet. We created a WeChat group for better discussion and quicker response. Please scan the personal QR code on the right, and the guy is responsible for inviting you to the chat group.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/wenet.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;img src=&#34;https://github.com/robin1001/qr/raw/master/binbin.jpeg&#34; width=&#34;250px&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledge&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPnet&lt;/a&gt; for transformer based modeling.&lt;/li&gt; &#xA; &lt;li&gt;We borrowed a lot of code from &lt;a href=&#34;http://kaldi-asr.org/&#34;&gt;Kaldi&lt;/a&gt; for WFST based decoding for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred &lt;a href=&#34;https://github.com/srvk/eesen&#34;&gt;EESEN&lt;/a&gt; for building TLG based graph for LM integration.&lt;/li&gt; &#xA; &lt;li&gt;We referred to &lt;a href=&#34;https://github.com/ZhengkunTian/OpenTransformer/&#34;&gt;OpenTransformer&lt;/a&gt; for python batch inference of e2e models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{yao2021wenet,&#xA;title={WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit},&#xA;author={Yao, Zhuoyuan and Wu, Di and Wang, Xiong and Zhang, Binbin and Yu, Fan and Yang, Chao and Peng, Zhendong and Chen, Xiaoyu and Xie, Lei and Lei, Xin},&#xA;  booktitle={Proc. Interspeech},&#xA;  year={2021},&#xA;  address={Brno, Czech Republic },&#xA;  organization={IEEE}&#xA;}&#xA;&#xA;@article{zhang2022wenet,&#xA;  title={WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit},&#xA;  author={Zhang, Binbin and Wu, Di and Peng, Zhendong and Song, Xingchen and Yao, Zhuoyuan and Lv, Hang and Xie, Lei and Yang, Chao and Pan, Fuping and Niu, Jianwei},&#xA;  journal={arXiv preprint arXiv:2203.15455},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>