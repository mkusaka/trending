<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-01T01:33:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bin123apple/AutoCoder</title>
    <updated>2024-06-01T01:33:52Z</updated>
    <id>tag:github.com,2024-06-01:/bin123apple/AutoCoder</id>
    <link href="https://github.com/bin123apple/AutoCoder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We introduced a new model designed for the Code generation task. Its test accuracy on the HumanEval base dataset surpasses that of GPT-4 Turbo (April 2024) and GPT-4o.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoCoder&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We introduced a new model designed for the Code generation task. Its test accuracy on the HumanEval base dataset surpasses that of GPT-4 Turbo (April 2024). (&lt;strong&gt;90.9% vs 90.2%&lt;/strong&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Additionally, compared to previous open-source models, AutoCoder offers a new feature: it can &lt;strong&gt;automatically install the required packages&lt;/strong&gt; and attempt to run the code until it deems there are no issues, &lt;strong&gt;whenever the user wishes to execute the code&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Difference between the code interpreter of AutoCoder and the GPT-4 Turbo:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Below are the video demos for the code interpreter comparision between GPT-4 Turbo and AutoCoder:&lt;/p&gt; &#xA;&lt;p&gt;GPT-4o can not access the external library.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bin123apple/AutoCoder/assets/99925255/be47b449-4e8a-4b77-981b-ec79b15970cc&#34;&gt;GPT-4o&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AutoCoder can automatically install the required packages. This feature expands the scope of code interpreter&#39;s application.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bin123apple/AutoCoder/assets/99925255/1893f904-c1f2-4f59-9ec5-45b69efcc26a&#34;&gt;AutoCoder&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Difference between the code interpreter of AutoCoder and the current open-source code interpreter &lt;a href=&#34;https://opencodeinterpreter.github.io/&#34;&gt;OpenCodeInterpreter&lt;/a&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The code interpreter of AutoCoder, like GPT-4 Turbo, is only called when the user has a need to verify the code, while OpenCodeInterpreter runs all generated python code.&lt;/p&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;The Model is avaliable on Huggingface: &lt;a href=&#34;https://huggingface.co/Bin12345/AutoCoder&#34;&gt;AutoCoder (33B)&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/Bin12345/AutoCoder_S_6.7B&#34;&gt;AutoCoder-S (6.7B)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The base model is deepseeker-coder.&lt;/p&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create the conda env&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n AutoCoder python=3.11&#xA;conda activate AutoCoder&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Test on HumanEval &lt;strong&gt;90.9% on base, 78.0% on base + extra&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Evaluation&#xA;python test_humaneval.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will receive a file named AutoCoder_HumanEval+.jsonl, which follows the EvalPlus format, after this step.&lt;/p&gt; &#xA;&lt;p&gt;Then follow the testing framework of the &lt;a href=&#34;https://github.com/evalplus/evalplus&#34;&gt;EvalPlus GitHub&lt;/a&gt;. You will see the results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Don&#39;t forget to use evalplus&#39;s &lt;code&gt;evalplus.sanitize&lt;/code&gt; to post-process the code.&lt;/li&gt; &#xA; &lt;li&gt;If you don&#39;t use the greedy method (for example set the &lt;code&gt;do_sample=True&lt;/code&gt;) for the code generation. You will probably see the different results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Test on MBPP &lt;strong&gt;82.5% on base, 70.6% on base + extra&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_humaneval.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Post-process to delete the nature language for testing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python postprocess_mbpp.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your will get a AutoCoder_Mbpp+-sanitized.jsonl file after this step, it extracted all the code blocks. Then, directly test it by using &lt;a href=&#34;https://github.com/evalplus/evalplus&#34;&gt;EvalPlus GitHub&lt;/a&gt; (You don&#39;t need to use to use evalplus&#39;s &lt;code&gt;evalplus.sanitize&lt;/code&gt; to post-process the code this time).&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Test on DS-1000.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_ds1000.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your will get a jsonl file after this step, it extracted all the code blocks. Then, directly test it by using &lt;a href=&#34;https://github.com/xlang-ai/DS-1000&#34;&gt;DS-1000 GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Web demo (Include code interpreter)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Install gradio related pakcages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd /Web_demo&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python chatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Currently the model will only start the code interpreter if you ask it to &lt;strong&gt;verify&lt;/strong&gt; its code. I am still finetuning it on a instructed dataset, which will give it the ability to enable the code interpreter upon a user request to &lt;strong&gt;run&lt;/strong&gt; code. I will update the model when it is finished.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We suggest to set &lt;code&gt;do_sample = True&lt;/code&gt; (default setting here) while using the code interpreter.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any inquiries, please feel free to raise an issue or reach out to &lt;a href=&#34;mailto:leib2765@gmail.com&#34;&gt;leib2765@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lei2024autocoder,&#xA;      title={AutoCoder: Enhancing Code Large Language Model with \textsc{AIEV-Instruct}}, &#xA;      author={Bin Lei and Yuchen Li and Qiuwu Chen},&#xA;      year={2024},&#xA;      eprint={2405.14906},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to Tianyu Zheng, the first author of the &lt;a href=&#34;https://opencodeinterpreter.github.io/&#34;&gt;OpenCodeInterpreter&lt;/a&gt;, for guidance on some technical details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>outlines-dev/outlines</title>
    <updated>2024-06-01T01:33:52Z</updated>
    <id>tag:github.com,2024-06-01:/outlines-dev/outlines</id>
    <link href="https://github.com/outlines-dev/outlines" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Structured Text Generation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; style=&#34;margin-bottom: 1em;&#34;&gt; &#xA; &lt;h1&gt;Outlines „Ä∞Ô∏è&lt;/h1&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/outlines-dev/outlines/main/docs/assets/images/logo.png&#34; alt=&#34;Outlines Logo&#34; width=&#34;300&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://twitter.com/dottxtai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/dottxtai?style=social&#34; alt=&#34;.txt Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/OutlinesOSS&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/OutlinesOSS?style=social&#34; alt=&#34;Outlines Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/outlines-dev/outlines/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/outlines-dev/outlines?style=flat-square&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;color=ECEFF4&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/outlines&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/outlines?color=89AC6B&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/R9DSu34mGd&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1182316225284554793?color=81A1C1&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Robust (structured) text generation.&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;Made with ‚ù§üë∑Ô∏è by the team at &lt;a href=&#34;https://dottxt.co&#34;&gt;.txt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install outlines&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;First time here? Go to our &lt;a href=&#34;https://outlines-dev.github.io/outlines/welcome&#34;&gt;setup guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ü§ñ &lt;a href=&#34;https://outlines-dev.github.io/outlines/installation&#34;&gt;Multiple model integrations&lt;/a&gt;: OpenAI, transformers, llama.cpp, exllama2, mamba&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üñçÔ∏è Simple and powerful prompting primitives based on the &lt;a href=&#34;https://jinja.palletsprojects.com/&#34;&gt;Jinja templating engine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üöÑ &lt;a href=&#34;https://raw.githubusercontent.com/outlines-dev/outlines/main/#multiple-choices&#34;&gt;Multiple choices&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/outlines-dev/outlines/main/#type-constraint&#34;&gt;type constraints&lt;/a&gt; and dynamic stopping&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ‚ö° Fast &lt;a href=&#34;https://raw.githubusercontent.com/outlines-dev/outlines/main/#efficient-regex-structured-generation&#34;&gt;regex-structured generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üî• Fast &lt;a href=&#34;https://raw.githubusercontent.com/outlines-dev/outlines/main/#efficient-json-generation-following-a-pydantic-model&#34;&gt;JSON generation&lt;/a&gt; following a JSON schema or a Pydantic model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üìù &lt;a href=&#34;https://raw.githubusercontent.com/outlines-dev/outlines/main/#using-context-free-grammars-to-guide-generation&#34;&gt;Grammar-structured generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üêç Interleave completions with loops, conditionals, and custom Python functions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üíæ Caching of generations&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üóÇÔ∏è Batch inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üé≤ Sample with the greedy, multinomial and beam search algorithms (and more to come!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; üöÄ &lt;a href=&#34;https://outlines-dev.github.io/outlines/reference/vllm&#34;&gt;Serve with vLLM&lt;/a&gt;, with official Docker image, &lt;a href=&#34;https://hub.docker.com/r/outlinesdev/outlines&#34;&gt;&lt;code&gt;outlinesdev/outlines&lt;/code&gt;&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Outlines „Ä∞ has new releases and features coming every week. Make sure to ‚≠ê star and üëÄ watch this repository, follow &lt;a href=&#34;https://twitter.com/dottxtai&#34;&gt;@dottxtai&lt;/a&gt; to stay up to date!&lt;/p&gt; &#xA;&lt;h2&gt;Why should I use structured generation?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It doesn&#39;t add any overhead during inference (cost-free)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://blog.dottxt.co/coalescence.html&#34;&gt;It speeds up inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://blog.dottxt.co/performance-gsm8k.html&#34;&gt;It improves the performance of base models (GSM8K)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://predibase.com/blog/lorax-outlines-better-json-extraction-with-structured-generation-and-lora&#34;&gt;It improves the performance of finetuned models (CoNNL)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/evaluation-structured-outputs&#34;&gt;It improves model efficiency (less examples needed)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;.txt company&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/outlines-dev/outlines/main/docs/assets/images/dottxt.png&#34; alt=&#34;Outlines Logo&#34; width=&#34;100&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We started a company to keep pushing the boundaries of structured generation. Learn more about &lt;a href=&#34;https://twitter.com/dottxtai&#34;&gt;.txt&lt;/a&gt;, and &lt;a href=&#34;https://h1xbpbfsf0w.typeform.com/to/ZgBCvJHF&#34;&gt;give our .json API a try&lt;/a&gt; if you need a hosted solution ‚ú®&lt;/p&gt; &#xA;&lt;h2&gt;Structured generation&lt;/h2&gt; &#xA;&lt;p&gt;The first step towards reliability of systems that include large language models is to ensure that there is a well-defined interface between their output and user-defined code. &lt;strong&gt;Outlines&lt;/strong&gt; provides ways to control the generation of language models to make their output more predictable.&lt;/p&gt; &#xA;&lt;h3&gt;Multiple choices&lt;/h3&gt; &#xA;&lt;p&gt;You can reduce the completion to a choice between multiple possibilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines&#xA;&#xA;model = outlines.models.transformers(&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;)&#xA;&#xA;prompt = &#34;&#34;&#34;You are a sentiment-labelling assistant.&#xA;Is the following review positive or negative?&#xA;&#xA;Review: This restaurant is just awesome!&#xA;&#34;&#34;&#34;&#xA;&#xA;generator = outlines.generate.choice(model, [&#34;Positive&#34;, &#34;Negative&#34;])&#xA;answer = generator(prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Type constraint&lt;/h3&gt; &#xA;&lt;p&gt;You can instruct the model to only return integers or floats:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines&#xA;&#xA;model = outlines.models.transformers(&#34;WizardLM/WizardMath-7B-V1.1&#34;)&#xA;&#xA;prompt = &#34;&amp;lt;s&amp;gt;result of 9 + 9 = 18&amp;lt;/s&amp;gt;&amp;lt;s&amp;gt;result of 1 + 2 = &#34;&#xA;answer = outlines.generate.format(model, int)(prompt)&#xA;print(answer)&#xA;# 3&#xA;&#xA;prompt = &#34;sqrt(2)=&#34;&#xA;generator = outlines.generate.format(model, float)&#xA;answer = generator(prompt, max_tokens=10)&#xA;print(answer)&#xA;# 1.41421356&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Efficient regex-structured generation&lt;/h3&gt; &#xA;&lt;p&gt;Outlines also comes with fast regex-structured generation. In fact, the &lt;code&gt;choice&lt;/code&gt; and &lt;code&gt;format&lt;/code&gt; functions above all use regex-structured generation under the hood:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines&#xA;&#xA;model = outlines.models.transformers(&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;)&#xA;&#xA;prompt = &#34;What is the IP address of the Google DNS servers? &#34;&#xA;&#xA;generator = outlines.generate.text(model)&#xA;unstructured = generator(prompt, max_tokens=30)&#xA;&#xA;generator = outlines.generate.regex(&#xA;    model,&#xA;    r&#34;((25[0-5]|2[0-4]\d|[01]?\d\d?)\.){3}(25[0-5]|2[0-4]\d|[01]?\d\d?)&#34;,&#xA;)&#xA;structured = generator(prompt, max_tokens=30)&#xA;&#xA;print(unstructured)&#xA;# What is the IP address of the Google DNS servers?&#xA;#&#xA;# Passive DNS servers are at DNS servers that are private.&#xA;# In other words, both IP servers are private. The database&#xA;# does not contain Chelsea Manning&#xA;&#xA;print(structured)&#xA;# What is the IP address of the Google DNS servers?&#xA;# 2.2.6.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unlike other libraries, regex-structured generation in Outlines is almost as fast as non-structured generation.&lt;/p&gt; &#xA;&lt;h3&gt;Efficient JSON generation following a Pydantic model&lt;/h3&gt; &#xA;&lt;p&gt;Outlines „Ä∞ allows to guide the generation process so the output is &lt;em&gt;guaranteed&lt;/em&gt; to follow a &lt;a href=&#34;https://json-schema.org/&#34;&gt;JSON schema&lt;/a&gt; or &lt;a href=&#34;https://docs.pydantic.dev/latest/&#34;&gt;Pydantic model&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from enum import Enum&#xA;from pydantic import BaseModel, constr&#xA;&#xA;import outlines&#xA;import torch&#xA;&#xA;&#xA;class Weapon(str, Enum):&#xA;    sword = &#34;sword&#34;&#xA;    axe = &#34;axe&#34;&#xA;    mace = &#34;mace&#34;&#xA;    spear = &#34;spear&#34;&#xA;    bow = &#34;bow&#34;&#xA;    crossbow = &#34;crossbow&#34;&#xA;&#xA;&#xA;class Armor(str, Enum):&#xA;    leather = &#34;leather&#34;&#xA;    chainmail = &#34;chainmail&#34;&#xA;    plate = &#34;plate&#34;&#xA;&#xA;&#xA;class Character(BaseModel):&#xA;    name: constr(max_length=10)&#xA;    age: int&#xA;    armor: Armor&#xA;    weapon: Weapon&#xA;    strength: int&#xA;&#xA;&#xA;model = outlines.models.transformers(&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;)&#xA;&#xA;# Construct structured sequence generator&#xA;generator = outlines.generate.json(model, Character)&#xA;&#xA;# Draw a sample&#xA;rng = torch.Generator(device=&#34;cuda&#34;)&#xA;rng.manual_seed(789001)&#xA;&#xA;character = generator(&#34;Give me a character description&#34;, rng=rng)&#xA;&#xA;print(repr(character))&#xA;# Character(name=&#39;Anderson&#39;, age=28, armor=&amp;lt;Armor.chainmail: &#39;chainmail&#39;&amp;gt;, weapon=&amp;lt;Weapon.sword: &#39;sword&#39;&amp;gt;, strength=8)&#xA;&#xA;character = generator(&#34;Give me an interesting character description&#34;, rng=rng)&#xA;&#xA;print(repr(character))&#xA;# Character(name=&#39;Vivian Thr&#39;, age=44, armor=&amp;lt;Armor.plate: &#39;plate&#39;&amp;gt;, weapon=&amp;lt;Weapon.crossbow: &#39;crossbow&#39;&amp;gt;, strength=125)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The method works with union types, optional types, arrays, nested schemas, etc. Some field constraints are &lt;a href=&#34;https://github.com/outlines-dev/outlines/issues/215&#34;&gt;not supported yet&lt;/a&gt;, but everything else should work.&lt;/p&gt; &#xA;&lt;h3&gt;Efficient JSON generation following a JSON Schema&lt;/h3&gt; &#xA;&lt;p&gt;Sometimes you just want to be able to pass a JSON Schema instead of a Pydantic model. We&#39;ve got you covered:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines&#xA;&#xA;schema = &#39;&#39;&#39;{&#xA;    &#34;title&#34;: &#34;Character&#34;,&#xA;    &#34;type&#34;: &#34;object&#34;,&#xA;    &#34;properties&#34;: {&#xA;        &#34;name&#34;: {&#xA;            &#34;title&#34;: &#34;Name&#34;,&#xA;            &#34;maxLength&#34;: 10,&#xA;            &#34;type&#34;: &#34;string&#34;&#xA;        },&#xA;        &#34;age&#34;: {&#xA;            &#34;title&#34;: &#34;Age&#34;,&#xA;            &#34;type&#34;: &#34;integer&#34;&#xA;        },&#xA;        &#34;armor&#34;: {&#34;$ref&#34;: &#34;#/definitions/Armor&#34;},&#xA;        &#34;weapon&#34;: {&#34;$ref&#34;: &#34;#/definitions/Weapon&#34;},&#xA;        &#34;strength&#34;: {&#xA;            &#34;title&#34;: &#34;Strength&#34;,&#xA;            &#34;type&#34;: &#34;integer&#34;&#xA;        }&#xA;    },&#xA;    &#34;required&#34;: [&#34;name&#34;, &#34;age&#34;, &#34;armor&#34;, &#34;weapon&#34;, &#34;strength&#34;],&#xA;    &#34;definitions&#34;: {&#xA;        &#34;Armor&#34;: {&#xA;            &#34;title&#34;: &#34;Armor&#34;,&#xA;            &#34;description&#34;: &#34;An enumeration.&#34;,&#xA;            &#34;enum&#34;: [&#34;leather&#34;, &#34;chainmail&#34;, &#34;plate&#34;],&#xA;            &#34;type&#34;: &#34;string&#34;&#xA;        },&#xA;        &#34;Weapon&#34;: {&#xA;            &#34;title&#34;: &#34;Weapon&#34;,&#xA;            &#34;description&#34;: &#34;An enumeration.&#34;,&#xA;            &#34;enum&#34;: [&#34;sword&#34;, &#34;axe&#34;, &#34;mace&#34;, &#34;spear&#34;, &#34;bow&#34;, &#34;crossbow&#34;],&#xA;            &#34;type&#34;: &#34;string&#34;&#xA;        }&#xA;    }&#xA;}&#39;&#39;&#39;&#xA;&#xA;model = outlines.models.transformers(&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;)&#xA;generator = outlines.generate.json(model, schema)&#xA;character = generator(&#34;Give me a character description&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using context-free grammars to guide generation&lt;/h3&gt; &#xA;&lt;p&gt;Formal grammars rule the world, and Outlines makes them rule LLMs too. You can pass any context-free grammar in the EBNF format and Outlines will generate an output that is valid to this grammar:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines&#xA;&#xA;arithmetic_grammar = &#34;&#34;&#34;&#xA;    ?start: expression&#xA;&#xA;    ?expression: term ((&#34;+&#34; | &#34;-&#34;) term)*&#xA;&#xA;    ?term: factor ((&#34;*&#34; | &#34;/&#34;) factor)*&#xA;&#xA;    ?factor: NUMBER&#xA;           | &#34;-&#34; factor&#xA;           | &#34;(&#34; expression &#34;)&#34;&#xA;&#xA;    %import common.NUMBER&#xA;&#34;&#34;&#34;&#xA;&#xA;model = outlines.models.transformers(&#34;WizardLM/WizardMath-7B-V1.1&#34;)&#xA;generator = outlines.generate.cfg(model, arithmetic_grammar)&#xA;sequence = generator(&#34;Alice had 4 apples and Bob ate 2. Write an expression for Alice&#39;s apples:&#34;)&#xA;&#xA;print(sequence)&#xA;# (8-2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This was a very simple grammar, and you can use &lt;code&gt;outlines.generate.cfg&lt;/code&gt; to generate syntactically valid Python, SQL, and much more than this. Any kind of structured text, really. All you have to do is search for &#34;X EBNF grammar&#34; on the web, and take a look at the &lt;a href=&#34;https://github.com/outlines-dev/outlines/tree/main/outlines/grammars&#34;&gt;Outlines &lt;code&gt;grammars&lt;/code&gt; module&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Open functions&lt;/h3&gt; &#xA;&lt;p&gt;Outlines can infer the structure of the output from the signature of a function. The result is a dictionary, and can be passed directly to the function using the usual dictionary expansion syntax &lt;code&gt;**&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines&#xA;&#xA;&#xA;def add(a: int, b: int):&#xA;    return a + b&#xA;&#xA;model = outlines.models.transformers(&#34;WizardLM/WizardMath-7B-V1.1&#34;)&#xA;generator = outlines.generate.json(model, add)&#xA;result = generator(&#34;Return json with two integers named a and b respectively. a is odd and b even.&#34;)&#xA;&#xA;print(add(**result))&#xA;# 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A great advantage of passing functions directly to specify the structure is that the structure of the LLM will change with the function&#39;s definition. No need to change the code at several places!&lt;/p&gt; &#xA;&lt;h2&gt;Prompting&lt;/h2&gt; &#xA;&lt;p&gt;Building prompts can get messy. &lt;strong&gt;Outlines&lt;/strong&gt; makes it easier to write and manage prompts by encapsulating templates inside &#34;template functions&#34;.&lt;/p&gt; &#xA;&lt;p&gt;These functions make it possible to neatly separate the prompt logic from the general program logic; they can be imported from other modules and libraries.&lt;/p&gt; &#xA;&lt;p&gt;Template functions require no superfluous abstraction, they use the Jinja2 templating engine to help build complex prompts in a concise manner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import outlines&#xA;&#xA;examples = [&#xA;    (&#34;The food was disgusting&#34;, &#34;Negative&#34;),&#xA;    (&#34;We had a fantastic night&#34;, &#34;Positive&#34;),&#xA;    (&#34;Recommended&#34;, &#34;Positive&#34;),&#xA;    (&#34;The waiter was rude&#34;, &#34;Negative&#34;)&#xA;]&#xA;&#xA;@outlines.prompt&#xA;def labelling(to_label, examples):&#xA;    &#34;&#34;&#34;You are a sentiment-labelling assistant.&#xA;&#xA;    {% for example in examples %}&#xA;    {{ example[0] }} // {{ example[1] }}&#xA;    {% endfor %}&#xA;    {{ to_label }} //&#xA;    &#34;&#34;&#34;&#xA;&#xA;model = outlines.models.transformers(&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;)&#xA;prompt = labelling(&#34;Just awesome&#34;, examples)&#xA;answer = outlines.generate.text(model)(prompt, max_tokens=100)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Join us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üí° &lt;strong&gt;Have an idea?&lt;/strong&gt; Come chat with us on &lt;a href=&#34;https://discord.gg/R9DSu34mGd&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî® &lt;strong&gt;Want to contribute?&lt;/strong&gt; Consult our &lt;a href=&#34;https://outlines-dev.github.io/outlines/community/contribute/&#34;&gt;contribution guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üêû &lt;strong&gt;Found a bug?&lt;/strong&gt; Open an &lt;a href=&#34;https://github.com/outlines-dev/outlines/issues&#34;&gt;issue&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite Outlines&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{willard2023efficient,&#xA;  title={Efficient Guided Generation for LLMs},&#xA;  author={Willard, Brandon T and Louf, R{\&#39;e}mi},&#xA;  journal={arXiv preprint arXiv:2307.09702},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>TMElyralab/MusePose</title>
    <updated>2024-06-01T01:33:52Z</updated>
    <id>tag:github.com,2024-06-01:/TMElyralab/MusePose</id>
    <link href="https://github.com/TMElyralab/MusePose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MusePose&lt;/h1&gt; &#xA;&lt;p&gt;MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation.&lt;/p&gt; &#xA;&lt;p&gt;Zhengyan Tong, Chao Li, Zhaokang Chen, Bin Wu&lt;sup&gt;‚Ä†&lt;/sup&gt;, Wenjiang Zhou (&lt;sup&gt;‚Ä†&lt;/sup&gt;Corresponding Author, &lt;a href=&#34;mailto:benbinwu@tencent.com&#34;&gt;benbinwu@tencent.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Lyra Lab, Tencent Music Entertainment&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/TMElyralab/MusePose&#34;&gt;github&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/TMElyralab/MusePose&#34;&gt;huggingface&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;space (comming soon)&lt;/strong&gt; &lt;strong&gt;Project (comming soon)&lt;/strong&gt; &lt;strong&gt;Technical report (comming soon)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TMElyralab/MusePose&#34;&gt;MusePose&lt;/a&gt; is an image-to-video generation framework for virtual human under control signal such as pose.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;MusePose&lt;/code&gt; is the last building block of &lt;strong&gt;the Muse opensource serie&lt;/strong&gt;. Together with &lt;a href=&#34;https://github.com/TMElyralab/MuseV&#34;&gt;MuseV&lt;/a&gt; and &lt;a href=&#34;https://github.com/TMElyralab/MuseTalk&#34;&gt;MuseTalk&lt;/a&gt;, we hope the community can join us and march towards the vision where a virtual human can be generated end2end with native ability of full body movement and interaction. Please stay tuned for our next milestone!&lt;/p&gt; &#xA;&lt;p&gt;We really appreciate &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;AnimateAnyone&lt;/a&gt; for their academic paper and &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt; for their code base, which have significantly expedited the development of the AIGC community and &lt;a href=&#34;https://github.com/TMElyralab/MusePose&#34;&gt;MusePose&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Update:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We support &lt;a href=&#34;https://github.com/TMElyralab/Comfyui-MusePose&#34;&gt;Comfyui-MusePose&lt;/a&gt; now!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/TMElyralab/MusePose&#34;&gt;MusePose&lt;/a&gt; is a diffusion-based and pose-guided virtual human video generation framework.&lt;br&gt; Our main contributions could be summarized as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The released model can generate dance videos of the human character in a reference image under the given pose sequence. The result quality exceeds almost all current open source models within the same topic.&lt;/li&gt; &#xA; &lt;li&gt;We release the &lt;code&gt;pose align&lt;/code&gt; algorithm so that users could align arbitrary dance videos to arbitrary reference images, which &lt;strong&gt;SIGNIFICANTLY&lt;/strong&gt; improved inference performance and enhanced model usability.&lt;/li&gt; &#xA; &lt;li&gt;We have fixed several important bugs and made some improvement based on the code of &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/bb52ca3e-8a5c-405a-8575-7ab42abca248&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/6667c9ae-8417-49a1-bbbb-fe1695404c23&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/7f7a3aaf-2720-4b50-8bca-3257acce4733&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/c56f7e9c-d94d-494e-88e6-62a4a3c1e016&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/00a9faec-2453-4834-ad1f-44eb0ec8247d&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/41ad26b3-d477-4975-bf29-73a3c9ed0380&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/2bbebf98-6805-4f1b-b769-537f69cc0e4b&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/TMElyralab/MusePose/assets/47803475/1b2b97d0-0ae9-49a6-83ba-b3024ae64f08&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[05/27/2024] Release &lt;code&gt;MusePose&lt;/code&gt; and pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;[05/31/2024] Support &lt;a href=&#34;https://github.com/TMElyralab/Comfyui-MusePose&#34;&gt;Comfyui-MusePose&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Todo:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release our trained models and inference codes of MusePose.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; release pose align algorithm.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Comfyui-MusePose&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; training guidelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Huggingface Gradio demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; a improved architecture and model (may take longer).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;We provide a detailed tutorial about the installation and the basic usage of MusePose for new users:&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To prepare the Python environment and install additional packages such as opencv, diffusers, mmcv, etc., please follow the steps below:&lt;/p&gt; &#xA;&lt;h3&gt;Build environment&lt;/h3&gt; &#xA;&lt;p&gt;We recommend a python version &amp;gt;=3.10 and cuda version =11.7. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;mmlab packages&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-cache-dir -U openmim &#xA;mim install mmengine &#xA;mim install &#34;mmcv&amp;gt;=2.0.1&#34; &#xA;mim install &#34;mmdet&amp;gt;=3.1.0&#34; &#xA;mim install &#34;mmpose&amp;gt;=1.1.0&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;You can download weights manually as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/TMElyralab/MusePose&#34;&gt;weights&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the weights of other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/unet&#34;&gt;sd-image-variations-diffusers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/yzd-v/DWPose/tree/main&#34;&gt;dwpose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_l_8x8_300e_coco/yolox_l_8x8_300e_coco_20211126_140236-d3bd2b23.pth&#34;&gt;yolox&lt;/a&gt; - Make sure to rename to &lt;code&gt;yolox_l_8x8_300e_coco.pth&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be organized in &lt;code&gt;pretrained_weights&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./pretrained_weights/&#xA;|-- MusePose&#xA;|   |-- denoising_unet.pth&#xA;|   |-- motion_module.pth&#xA;|   |-- pose_guider.pth&#xA;|   ‚îî‚îÄ‚îÄ reference_unet.pth&#xA;|-- dwpose&#xA;|   |-- dw-ll_ucoco_384.pth&#xA;|   ‚îî‚îÄ‚îÄ yolox_l_8x8_300e_coco.pth&#xA;|-- sd-image-variations-diffusers&#xA;|   ‚îî‚îÄ‚îÄ unet&#xA;|       |-- config.json&#xA;|       ‚îî‚îÄ‚îÄ diffusion_pytorch_model.bin&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   ‚îî‚îÄ‚îÄ pytorch_model.bin&#xA;‚îî‚îÄ‚îÄ sd-vae-ft-mse&#xA;    |-- config.json&#xA;    ‚îî‚îÄ‚îÄ diffusion_pytorch_model.bin&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;h4&gt;Preparation&lt;/h4&gt; &#xA;&lt;p&gt;Prepare your referemce images and dance videos in the folder &lt;code&gt;./assets&lt;/code&gt; and organnized as the example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./assets/&#xA;|-- images&#xA;|   ‚îî‚îÄ‚îÄ ref.png&#xA;‚îî‚îÄ‚îÄ videos&#xA;    ‚îî‚îÄ‚îÄ dance.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Pose Alignment&lt;/h4&gt; &#xA;&lt;p&gt;Get the aligned dwpose of the reference image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python pose_align.py --imgfn_refer ./assets/images/ref.png --vidfn ./assets/videos/dance.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After this, you can see the pose align results in &lt;code&gt;./assets/poses&lt;/code&gt;, where &lt;code&gt;./assets/poses/align/img_ref_video_dance.mp4&lt;/code&gt; is the aligned dwpose and the &lt;code&gt;./assets/poses/align_demo/img_ref_video_dance.mp4&lt;/code&gt; is for debug.&lt;/p&gt; &#xA;&lt;h4&gt;Inferring MusePose&lt;/h4&gt; &#xA;&lt;p&gt;Add the path of the reference image and the aligned dwpose to the test config file &lt;code&gt;./configs/test_stage_2.yaml&lt;/code&gt; as the example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;test_cases:&#xA;  &#34;./assets/images/ref.png&#34;:&#xA;    - &#34;./assets/poses/align/img_ref_video_dance.mp4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, simply run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_stage_2.py --config ./configs/test_stage_2.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;./configs/test_stage_2.yaml&lt;/code&gt; is the path to the inference configuration file.&lt;/p&gt; &#xA;&lt;p&gt;Finally, you can see the output results in &lt;code&gt;./output/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Reducing VRAM cost&lt;/h5&gt; &#xA;&lt;p&gt;If you want to reduce the VRAM cost, you could set the width and height for inference. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python test_stage_2.py --config ./configs/test_stage_2.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will generate the video at 512 x 512 first, and then resize it back to the original size of the pose video.&lt;/p&gt; &#xA;&lt;p&gt;Currently, it takes 16GB VRAM to run on 512 x 512 x 48 and takes 28GB VRAM to run on 768 x 768 x 48. However, it should be noticed that the inference resolution would affect the final results.&lt;/p&gt; &#xA;&lt;h4&gt;Face Enhancement&lt;/h4&gt; &#xA;&lt;p&gt;If you want to enhance the face region to have a better consistency of the face, you could use &lt;a href=&#34;https://github.com/facefusion/facefusion&#34;&gt;FaceFusion&lt;/a&gt;. You could use the &lt;code&gt;face-swap&lt;/code&gt; function to swap the face in the reference image to the generated video.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We thank &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;AnimateAnyone&lt;/a&gt; for their technical report, and have refer much to &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We thank open-source components like &lt;a href=&#34;https://animatediff.github.io/&#34;&gt;AnimateDiff&lt;/a&gt;, &lt;a href=&#34;https://github.com/IDEA-Research/DWPose&#34;&gt;dwpose&lt;/a&gt;, &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, etc..&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Thanks for open-sourcing!&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Detail consitency: some details of the original character are not well preserved (e.g. face region and complex clothing).&lt;/li&gt; &#xA; &lt;li&gt;Noise and flickering: we observe noise and flicking in complex background.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{musepose,&#xA;  title={MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation},&#xA;  author={Tong, Zhengyan and Li, Chao and Chen, Zhaokang and Wu, Bin and Zhou, Wenjiang},&#xA;  journal={arxiv},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Disclaimer/License&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;code&lt;/code&gt;: The code of MusePose is released under the MIT License. There is no limitation for both academic and commercial usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: The trained model are available for non-commercial research purposes only.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;other opensource model&lt;/code&gt;: Other open-source models used must comply with their license, such as &lt;code&gt;ft-mse-vae&lt;/code&gt;, &lt;code&gt;dwpose&lt;/code&gt;, etc..&lt;/li&gt; &#xA; &lt;li&gt;The testdata are collected from internet, which are available for non-commercial research purposes only.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AIGC&lt;/code&gt;: This project strives to impact the domain of AI-driven video generation positively. Users are granted the freedom to create videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>