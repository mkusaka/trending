<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-19T01:36:53Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lmnr-ai/index</title>
    <updated>2025-04-19T01:36:53Z</updated>
    <id>tag:github.com,2025-04-19:/lmnr-ai/index</id>
    <link href="https://github.com/lmnr-ai/index" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SOTA Open-Source Browser Agent for autonomously performing complex tasks on the web&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://www.ycombinator.com/companies/laminar-ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Y%20Combinator-S24-orange&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/lmnrai&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/lmnrai&#34; alt=&#34;X (formerly Twitter) Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/nNFUUDAKub&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Join_Discord-464646?&amp;amp;logo=discord&amp;amp;logoColor=5865F2&#34; alt=&#34;Static Badge&#34;&gt; &lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Index&lt;/h1&gt; &#xA;&lt;p&gt;Index is a state-of-the-art open-source browser agent that autonomously executes complex tasks on the web.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; It is powered by Claude 3.7 Sonnet with extented thinking. More models will be supported in the future.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Index is also available as a &lt;a href=&#34;https://docs.lmnr.ai/laminar-index/introduction&#34;&gt;hosted API.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; You can also try out Index via &lt;a href=&#34;https://docs.lmnr.ai/laminar-index/introduction#hosted-ui&#34;&gt;hosted UI&lt;/a&gt; or fully &lt;a href=&#34;https://x.com/skull8888888888/status/1910763169489764374&#34;&gt;self-host the UI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supports advanced &lt;a href=&#34;https://docs.lmnr.ai/laminar-index/observability&#34;&gt;browser agent observability&lt;/a&gt; powered by &lt;a href=&#34;https://lmnr.ai&#34;&gt;Laminar&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;prompt: go to ycombinator.com. summarize first 3 companies in the W25 batch and make new spreadsheet in google sheets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/2b46ee20-81b6-4188-92fb-4d97fe0b3d6a&#34;&gt;https://github.com/user-attachments/assets/2b46ee20-81b6-4188-92fb-4d97fe0b3d6a&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Index API&lt;/h2&gt; &#xA;&lt;p&gt;Index API is available as &lt;a href=&#34;https://docs.lmnr.ai/laminar-index/introduction&#34;&gt;hosted api&lt;/a&gt; on the &lt;a href=&#34;https://lmnr.ai&#34;&gt;Laminar&lt;/a&gt; platform. Index API manages remote browser sessions and agent infrastructure. Index API is the best way to run AI browser automation in production. To get started, &lt;a href=&#34;https://lmnr.ai/sign-in&#34;&gt;sign up&lt;/a&gt; and create project API key.&lt;/p&gt; &#xA;&lt;h3&gt;Install Laminar&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lmnr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use Index via API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lmnr import Laminar, AsyncLaminarClient&#xA;# you can also set LMNR_PROJECT_API_KEY environment variable&#xA;&#xA;# Initialize tracing&#xA;Laminar.initialize(project_api_key=&#34;your_api_key&#34;)&#xA;&#xA;# Initialize the client&#xA;client = AsyncLaminarClient(api_key=&#34;your_api_key&#34;)&#xA;&#xA;async def main():&#xA;&#xA;    # Run a task&#xA;    response = await client.agent.run(&#xA;        prompt=&#34;Navigate to news.ycombinator.com, find a post about AI, and summarize it&#34;&#xA;    )&#xA;&#xA;    # Print the result&#xA;    print(response.result)&#xA;    &#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When you call Index via API, you automatically get full browser agent observability on Laminar platform. Learn more about &lt;a href=&#34;https://docs.lmnr.ai/laminar-index/introduction#tracing-with-laminar&#34;&gt;Index browser observability&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Local Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Install dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install lmnr-index&#xA;&#xA;# Install playwright&#xA;playwright install chromium&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run the agent&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from index import Agent, AnthropicProvider&#xA;&#xA;async def main():&#xA;    # Initialize the LLM provider&#xA;    llm = AnthropicProvider(&#xA;            model=&#34;claude-3-7-sonnet-20250219&#34;,&#xA;            enable_thinking=True, &#xA;            thinking_token_budget=2048)&#xA;    &#xA;    # Create an agent with the LLM&#xA;    agent = Agent(llm=llm)&#xA;    &#xA;    # Run the agent with a task&#xA;    output = await agent.run(&#xA;        prompt=&#34;Navigate to news.ycombinator.com, find a post about AI, and summarize it&#34;&#xA;    )&#xA;    &#xA;    # Print the result&#xA;    print(output.result)&#xA;    &#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stream the agent&#39;s output&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from index import Agent, AnthropicProvider&#xA;&#xA;agent = Agent(llm=AnthropicProvider(model=&#34;claude-3-7-sonnet-20250219&#34;))    &#xA;&#xA;# Stream the agent&#39;s output&#xA;async for chunk in agent.run_stream(&#xA;    prompt=&#34;Navigate to news.ycombinator.com, find a post about AI, and summarize it&#34;):&#xA;    print(chunk)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Enable browser agent observability&lt;/h3&gt; &#xA;&lt;p&gt;To trace Index agent&#39;s actions and record browser session you simply need to initialize Laminar tracing before running the agent.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lmnr import Laminar&#xA;&#xA;Laminar.initialize(project_api_key=&#34;your_api_key&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you will get full observability on the agent&#39;s actions synced with the browser session in the Laminar platform.&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/lmnr-ai/index/main/static/traces.png&#34; alt=&#34;Index observability&#34; width=&#34;800&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h3&gt;Run with remote CDP url&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from index import Agent, AnthropicProvider, BrowserConfig&#xA;&#xA;async def main():&#xA;    # Configure browser to connect to an existing Chrome DevTools Protocol endpoint&#xA;    browser_config = BrowserConfig(&#xA;        cdp_url=&#34;&amp;lt;cdp_url&amp;gt;&#34;&#xA;    )&#xA;    &#xA;    # Initialize the LLM provider&#xA;    llm = AnthropicProvider(model=&#34;claude-3-7-sonnet-20250219&#34;, enable_thinking=True, thinking_token_budget=2048)&#xA;    &#xA;    # Create an agent with the LLM and browser&#xA;    agent = Agent(llm=llm, browser_config=browser_config)&#xA;    &#xA;    # Run the agent with a task&#xA;    output = await agent.run(&#xA;        prompt=&#34;Navigate to news.ycombinator.com and find the top story&#34;&#xA;    )&#xA;    &#xA;    # Print the result&#xA;    print(output.result)&#xA;    &#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Customize browser window size&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from index import Agent, AnthropicProvider, BrowserConfig&#xA;&#xA;async def main():&#xA;    # Configure browser with custom viewport size&#xA;    browser_config = BrowserConfig(&#xA;        viewport_size={&#34;width&#34;: 1200, &#34;height&#34;: 900}&#xA;    )&#xA;    &#xA;    # Initialize the LLM provider&#xA;    llm = AnthropicProvider(model=&#34;claude-3-7-sonnet-20250219&#34;)&#xA;    &#xA;    # Create an agent with the LLM and browser&#xA;    agent = Agent(llm=llm, browser_config=browser_config)&#xA;    &#xA;    # Run the agent with a task&#xA;    output = await agent.run(&#xA;        &#34;Navigate to a responsive website and capture how it looks in full HD resolution&#34;&#xA;    )&#xA;    &#xA;    # Print the result&#xA;    print(output.result)&#xA;    &#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Made with ❤️ by the &lt;a href=&#34;https://lmnr.ai&#34;&gt;Laminar team&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/Codex-CLI</title>
    <updated>2025-04-19T01:36:53Z</updated>
    <id>tag:github.com,2025-04-19:/microsoft/Codex-CLI</id>
    <link href="https://github.com/microsoft/Codex-CLI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CLI tool that uses Codex to turn natural language commands into their Bash/ZShell/PowerShell equivalents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Codex CLI - Natural Language Command Line Interface&lt;/h1&gt; &#xA;&lt;p&gt;This project uses &lt;a href=&#34;https://openai.com/blog/openai-codex/&#34;&gt;GPT-3 Codex&lt;/a&gt; to convert natural language commands into commands in PowerShell, Z shell and Bash.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Codex-CLI/main/codex_cli.gif&#34; alt=&#34;Codex Cli GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Command Line Interface (CLI) was the first major User Interface we used to interact with machines. It&#39;s incredibly powerful, you can do almost anything with a CLI, but it requires the user to express their intent extremely precisely. The user needs to &lt;em&gt;know the language of the computer&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With the advent of Large Language Models (LLMs), particularly those that have been trained on code, it&#39;s possible to interact with a CLI using Natural Language (NL). In effect, these models understand natural language &lt;em&gt;and&lt;/em&gt; code well enough that they can translate from one to another.&lt;/p&gt; &#xA;&lt;p&gt;This project aims to offer a cross-shell NL-&amp;gt;Code experience to allow users to interact with their favorite CLI using NL. The user enters a command, like &#34;what&#39;s my IP address&#34;, hits &lt;code&gt;Ctrl + G&lt;/code&gt; and gets a suggestion for a command idiomatic to the shell they&#39;re using. The project uses the GPT-3 Codex model off-the-shelf, meaning the model has not been explicitly trained for the task. Instead we rely on a discipline called prompt engineering (see &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Codex-CLI/main/#prompt-engineering-and-context-files&#34;&gt;section&lt;/a&gt; below) to coax the right commands from Codex.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: The model can still make mistakes! Don&#39;t run a command if you don&#39;t understand it. If you&#39;re not sure what a command does, hit &lt;code&gt;Ctrl + C&lt;/code&gt; to cancel it&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project took technical inspiration from the &lt;a href=&#34;https://github.com/tom-doerr/zsh_codex&#34;&gt;zsh_codex&lt;/a&gt; project, extending its functionality to span multiple shells and to customize the prompts passed to the model (see prompt engineering section below).&lt;/p&gt; &#xA;&lt;h2&gt;Statement of Purpose&lt;/h2&gt; &#xA;&lt;p&gt;This repository aims to grow the understanding of using Codex in applications by providing an example of implementation and references to support the &lt;a href=&#34;https://mybuild.microsoft.com/&#34;&gt;Microsoft Build conference in 2022&lt;/a&gt;. It is not intended to be a released product. Therefore, this repository is not for discussing OpenAI API or requesting new features.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python 3.7.1+&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[Windows]: Python is added to PATH.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;An &lt;a href=&#34;https://openai.com/api/&#34;&gt;OpenAI account&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;OpenAI API Key&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://beta.openai.com/account/org-settings&#34;&gt;OpenAI Organization Id&lt;/a&gt;. If you have multiple organizations, please update your &lt;a href=&#34;https://beta.openai.com/account/api-keys&#34;&gt;default organization&lt;/a&gt; to the one that has access to codex engines before getting the organization Id.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://beta.openai.com/docs/engines/codex-series-private-beta&#34;&gt;OpenAI Engine Id&lt;/a&gt;. It provides access to a model. For example, &lt;code&gt;code-davinci-002&lt;/code&gt; or &lt;code&gt;code-cushman-001&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Codex-CLI/main/#what-openai-engines-are-available-to-me&#34;&gt;here&lt;/a&gt; for checking available engines.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Please follow the installation instructions for PowerShell, bash or zsh from &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Codex-CLI/main/Installation.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Once configured for your shell of preference, you can use the Codex CLI by writing a comment (starting with &lt;code&gt;#&lt;/code&gt;) into your shell, and then hitting &lt;code&gt;Ctrl + G&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Codex CLI supports two primary modes: single-turn and multi-turn.&lt;/p&gt; &#xA;&lt;p&gt;By default, multi-turn mode is off. It can be toggled on and off using the &lt;code&gt;# start multi-turn&lt;/code&gt; and &lt;code&gt;# stop multi-turn&lt;/code&gt; commands.&lt;/p&gt; &#xA;&lt;p&gt;If the multi-turn mode is on, the Codex CLI will &#34;remember&#34; past interactions with the model, allowing you to refer back to previous actions and entities. If, for example, you asked the Codex CLI to change your time zone to mountain, and then said &#34;change it back to pacific&#34;, the model would have the context from the previous interaction to know that &#34;it&#34; is the user&#39;s timezone:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;# change my timezone to mountain&#xA;tzutil /s &#34;Mountain Standard Time&#34;&#xA;&#xA;# change it back to pacific&#xA;tzutil /s &#34;Pacific Standard Time&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The tool creates a &lt;code&gt;current_context.txt&lt;/code&gt; file that keeps track of past interactions, and passes them to the model on each subsequent command.&lt;/p&gt; &#xA;&lt;p&gt;When multi-turn mode is off, this tool will not keep track of interaction history. There are tradeoffs to using multi-turn mode - though it enables compelling context resolution, it also increases overhead. If, for example, the model produces the wrong script for the job, the user will want to remove that from the context, otherwise future conversation turns will be more likely to produce the wrong script again. With multi-turn mode off, the model will behave completely deterministically - the same command will always produce the same output.&lt;/p&gt; &#xA;&lt;p&gt;Any time the model seems to output consistently incorrect commands, you can use the &lt;code&gt;# stop multi-turn&lt;/code&gt; command to stop the model from remembering past interactions and load in your default context. Alternatively, the &lt;code&gt;# default context&lt;/code&gt; command does the same while preserving the multi-turn mode as on.&lt;/p&gt; &#xA;&lt;h2&gt;Commands&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;start multi-turn&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Starts a multi-turn experience&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;stop multi-turn&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stops a multi-turn experience and loads default context&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;load context &amp;lt;filename&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Loads the context file from &lt;code&gt;contexts&lt;/code&gt; folder&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;default context&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Loads default shell context&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;view context&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Opens the context file in a text editor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;save context &amp;lt;filename&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Saves the context file to &lt;code&gt;contexts&lt;/code&gt; folder, if name not specified, uses current date-time&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;show config&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Shows the current configuration of your interaction with the model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;set &amp;lt;config-key&amp;gt; &amp;lt;config-value&amp;gt;&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sets the configuration of your interaction with the model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to improve your experience by changing the token limit, engine id and temperature using the set command. For example, &lt;code&gt;# set engine cushman-codex&lt;/code&gt;, &lt;code&gt;# set temperature 0.5&lt;/code&gt;, &lt;code&gt;# set max_tokens 50&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Prompt Engineering and Context Files&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a discipline called &lt;em&gt;prompt engineering&lt;/em&gt; to coax GPT-3 Codex to generate commands from natural language. Specifically, we pass the model a series of examples of NL-&amp;gt;Commands, to give it a sense of the kind of code it should be writing, and also to nudge it towards generating commands idiomatic to the shell you&#39;re using. These examples live in the &lt;code&gt;contexts&lt;/code&gt; directory. See snippet from the PowerShell context below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;# what&#39;s the weather in New York?&#xA;(Invoke-WebRequest -uri &#34;wttr.in/NewYork&#34;).Content&#xA;&#xA;# make a git ignore with node modules and src in it&#xA;&#34;node_modules&#xA;src&#34; | Out-File .gitignore&#xA;&#xA;# open it in notepad&#xA;notepad .gitignore&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that this project models natural language commands as comments, and provide examples of the kind of PowerShell scripts we expect the model to write. These examples include single line completions, multi-line completions, and multi-turn completions (the &#34;open it in notepad&#34; example refers to the &lt;code&gt;.gitignore&lt;/code&gt; file generated on the previous turn).&lt;/p&gt; &#xA;&lt;p&gt;When a user enters a new command (say &#34;what&#39;s my IP address&#34;), we simple append that command onto the context (as a comment) and ask Codex to generate the code that should follow it. Having seen the examples above, Codex will know that it should write a short PowerShell script that satisfies the comment.&lt;/p&gt; &#xA;&lt;h2&gt;Building your own Contexts&lt;/h2&gt; &#xA;&lt;p&gt;This project comes pre-loaded with contexts for each shell, along with some bonus contexts with other capabilities. Beyond these, you can build your own contexts to coax other behaviors out of the model. For example, if you want the Codex CLI to produce Kubernetes scripts, you can create a new context with examples of commands and the &lt;code&gt;kubectl&lt;/code&gt; script the model might produce:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# make a K8s cluster IP called my-cs running on 5678:8080&#xA;kubectl create service clusterip my-cs --tcp=5678:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your context to the &lt;code&gt;contexts&lt;/code&gt; folder and run &lt;code&gt;load context &amp;lt;filename&amp;gt;&lt;/code&gt; to load it. You can also change the default context from to your context file inside &lt;code&gt;src\prompt_file.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that Codex will often produce correct scripts without any examples. Having been trained on a large corpus of code, it frequently knows how to produce specific commands. That said, building your own contexts helps coax the specific kind of script you&#39;re looking for - whether it&#39;s long or short, whether it declares variables or not, whether it refers back to previous commands, etc. You can also provide examples of your own CLI commands and scripts, to show Codex other tools it should consider using.&lt;/p&gt; &#xA;&lt;p&gt;One important thing to consider is that if you add a new context, keep the multi-turn mode on to avoid our automatic defaulting (which was added to keep faulty contexts from breaking your experience).&lt;/p&gt; &#xA;&lt;p&gt;We have added a &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Codex-CLI/main/contexts/CognitiveServiceContext.md&#34;&gt;cognitive services context&lt;/a&gt; which uses the cognitive services API to provide text to speech type responses as an example.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;DEBUG_MODE&lt;/code&gt; to use a terminal input instead of the stdin and debug the code. This is useful when adding new commands and understanding why the tool is unresponsive.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes the &lt;code&gt;openai&lt;/code&gt; package will throws errors that aren&#39;t caught by the tool, you can add a catch block at the end of &lt;code&gt;codex_query.py&lt;/code&gt; for that exception and print a custom error message.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;What OpenAI engines are available to me?&lt;/h3&gt; &#xA;&lt;p&gt;You might have access to different &lt;a href=&#34;https://beta.openai.com/docs/api-reference/engines&#34;&gt;OpenAI engines&lt;/a&gt; per OpenAI organization. To check what engines are available to you, one can query the &lt;a href=&#34;https://beta.openai.com/docs/api-reference/engines/list&#34;&gt;List engines API&lt;/a&gt; for available engines. See the following commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Shell&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl https://api.openai.com/v1/engines \&#xA;  -H &#39;Authorization: Bearer YOUR_API_KEY&#39; \&#xA;  -H &#39;OpenAI-Organization: YOUR_ORG_ID&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;PowerShell&lt;/p&gt; &lt;p&gt;PowerShell v5 (The default one comes with Windows)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;(Invoke-WebRequest -Uri https://api.openai.com/v1/engines -Headers @{&#34;Authorization&#34; = &#34;Bearer YOUR_API_KEY&#34;; &#34;OpenAI-Organization&#34; = &#34;YOUR_ORG_ID&#34;}).Content&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;PowerShell v7&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;(Invoke-WebRequest -Uri https://api.openai.com/v1/engines -Authentication Bearer -Token (ConvertTo-SecureString &#34;YOUR_API_KEY&#34; -AsPlainText -Force) -Headers @{&#34;OpenAI-Organization&#34; = &#34;YOUR_ORG_ID&#34;}).Content&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Can I run the sample on Azure?&lt;/h3&gt; &#xA;&lt;p&gt;The sample code can be currently be used with Codex on OpenAI’s API. In the coming months, the sample will be updated so you can also use it with the &lt;a href=&#34;https://aka.ms/azure-openai&#34;&gt;Azure OpenAI Service&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>