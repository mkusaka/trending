<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-31T01:38:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bes-dev/stable_diffusion.openvino</title>
    <updated>2022-08-31T01:38:07Z</updated>
    <id>tag:github.com,2022-08-31:/bes-dev/stable_diffusion.openvino</id>
    <link href="https://github.com/bes-dev/stable_diffusion.openvino" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;stable_diffusion.openvino&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of Text-To-Image generation using Stable Diffusion on Intel CPU.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bes-dev/stable_diffusion.openvino/master/data/title.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux, Windows, MacOS&lt;/li&gt; &#xA; &lt;li&gt;Python 3.8.+&lt;/li&gt; &#xA; &lt;li&gt;CPU compatible with OpenVINO.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install requirements&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generate image from text description&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;usage: stable_diffusion.py [-h] [--model MODEL] [--seed SEED] [--beta-start BETA_START] [--beta-end BETA_END] [--beta-schedule BETA_SCHEDULE] [--num-inference-steps NUM_INFERENCE_STEPS]&#xA;                           [--guidance-scale GUIDANCE_SCALE] [--eta ETA] [--tokenizer TOKENIZER] [--prompt PROMPT] [--output OUTPUT]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --model MODEL         model name&#xA;  --seed SEED           random seed for generating consistent images per prompt&#xA;  --beta-start BETA_START&#xA;                        LMSDiscreteScheduler::beta_start&#xA;  --beta-end BETA_END   LMSDiscreteScheduler::beta_end&#xA;  --beta-schedule BETA_SCHEDULE&#xA;                        LMSDiscreteScheduler::beta_schedule&#xA;  --num-inference-steps NUM_INFERENCE_STEPS&#xA;                        num inference steps&#xA;  --guidance-scale GUIDANCE_SCALE&#xA;                        guidance scale&#xA;  --eta ETA             eta&#xA;  --tokenizer TOKENIZER&#xA;                        tokenizer&#xA;  --prompt PROMPT       prompt&#xA;  --output OUTPUT       output image name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python stable_diffusion.py --prompt &#34;Street-art painting of Emilia Clarke in style of Banksy, photorealism&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;CPU&lt;/th&gt; &#xA;   &lt;th&gt;Time per iter&lt;/th&gt; &#xA;   &lt;th&gt;Total time&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel(R) Core(TM) i5-8279U&lt;/td&gt; &#xA;   &lt;td&gt;7.4 s/it&lt;/td&gt; &#xA;   &lt;td&gt;3.59 min&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD Ryzen Threadripper 1900X&lt;/td&gt; &#xA;   &lt;td&gt;5.34 s/it&lt;/td&gt; &#xA;   &lt;td&gt;2.58 min&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz&lt;/td&gt; &#xA;   &lt;td&gt;1 s/it&lt;/td&gt; &#xA;   &lt;td&gt;33 s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel(R) Core(TM) i7-1165G7 @ 2.80GHz&lt;/td&gt; &#xA;   &lt;td&gt;7.4 s/it&lt;/td&gt; &#xA;   &lt;td&gt;3.59 min&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Original implementation of Stable Diffusion: &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;https://github.com/CompVis/stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;diffusers library: &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;https://github.com/huggingface/diffusers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The authors are not responsible for the content generated using this project. Please, don&#39;t use this project to produce illegal, harmful, offensive etc. content.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>gradio-app/gradio</title>
    <updated>2022-08-31T01:38:07Z</updated>
    <id>tag:github.com,2022-08-31:/gradio-app/gradio</id>
    <link href="https://github.com/gradio-app/gradio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create UIs for your machine learning model in Python in 3 minutes&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gradio.app&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/gradio.svg?sanitize=true&#34; alt=&#34;gradio&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;em&gt;Build &amp;amp; share delightful machine learning apps easily&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/gradio-app/gradio.svg?style=svg&#34; alt=&#34;circleci&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/gradio-app/gradio/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/gradio/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gradio&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/gradio/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/gradio&#34; alt=&#34;PyPI downloads&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.7+-important&#34; alt=&#34;Python version&#34;&gt; &lt;a href=&#34;https://twitter.com/gradio&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/gradio?style=social&amp;amp;label=follow&#34; alt=&#34;Twitter follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gradio.app&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://gradio.app/docs/&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://gradio.app/guides/&#34;&gt;Guides&lt;/a&gt; | &lt;a href=&#34;https://gradio.app/getting_started/&#34;&gt;Getting Started&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/&#34;&gt;Examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Gradio: Build Machine Learning Web Apps â€” in Python&lt;/h1&gt; &#xA;&lt;p&gt;Gradio is an open-source Python library that is used to build machine learning and data science demos and web applications.&lt;/p&gt; &#xA;&lt;p&gt;With Gradio, you can quickly create a beautiful user interface around your machine learning models or data science workflow and let people &#34;try it out&#34; by dragging-and-dropping in their own images, pasting text, recording their own voice, and interacting with your demo, all through the browser.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/header-image.jpg&#34; alt=&#34;Interface montage&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gradio is useful for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Demoing&lt;/strong&gt; your machine learning models for clients/collaborators/users/students.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploying&lt;/strong&gt; your models quickly with automatic shareable links and getting feedback on model performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Debugging&lt;/strong&gt; your model interactively during development using built-in manipulation and interpretation tools.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;: Gradio requires Python 3.7 or higher, that&#39;s all!&lt;/p&gt; &#xA;&lt;h3&gt;What Does Gradio Do?&lt;/h3&gt; &#xA;&lt;p&gt;One of the &lt;em&gt;best ways to share&lt;/em&gt; your machine learning model, API, or data science workflow with others is to create an &lt;strong&gt;interactive app&lt;/strong&gt; that allows your users or colleagues to try out the demo in their browsers.&lt;/p&gt; &#xA;&lt;p&gt;Gradio allows you to &lt;strong&gt;build demos and share them, all in Python.&lt;/strong&gt; And usually in just a few lines of code! So let&#39;s get started.&lt;/p&gt; &#xA;&lt;h3&gt;Hello, World&lt;/h3&gt; &#xA;&lt;p&gt;To get Gradio running with a simple &#34;Hello, World&#34; example, follow these three steps:&lt;/p&gt; &#xA;&lt;p&gt;1. Install Gradio using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;2. Run the code below as a Python script or in a Jupyter Notebook (or &lt;a href=&#34;https://colab.research.google.com/drive/18ODkJvyxHutTN0P5APWyGFO_xwNcgHDZ?usp=sharing&#34;&gt;Google Colab&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name):&#xA;    return &#34;Hello &#34; + name + &#34;!&#34;&#xA;&#xA;demo = gr.Interface(fn=greet, inputs=&#34;text&#34;, outputs=&#34;text&#34;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;3. The demo below will appear automatically within the Jupyter Notebook, or pop in a browser on &lt;a href=&#34;http://localhost:7860&#34;&gt;http://localhost:7860&lt;/a&gt; if running from a script:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_world/screenshot.gif&#34; alt=&#34;hello_world demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The &lt;code&gt;Interface&lt;/code&gt; Class&lt;/h3&gt; &#xA;&lt;p&gt;You&#39;ll notice that in order to make the demo, we created a &lt;code&gt;gradio.Interface&lt;/code&gt;. This &lt;code&gt;Interface&lt;/code&gt; class can wrap any Python function with a user interface. In the example above, we saw a simple text-based function, but the function could be anything from music generator to a tax calculator to the prediction function of a pretrained machine learning model.&lt;/p&gt; &#xA;&lt;p&gt;The core &lt;code&gt;Interface&lt;/code&gt; class is initialized with three required parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;fn&lt;/code&gt;: the function to wrap a UI around&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inputs&lt;/code&gt;: which component(s) to use for the input (e.g. &lt;code&gt;&#34;text&#34;&lt;/code&gt;, &lt;code&gt;&#34;image&#34;&lt;/code&gt; or &lt;code&gt;&#34;audio&#34;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;outputs&lt;/code&gt;: which component(s) to use for the output (e.g. &lt;code&gt;&#34;text&#34;&lt;/code&gt;, &lt;code&gt;&#34;image&#34;&lt;/code&gt; or &lt;code&gt;&#34;label&#34;&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s take a closer look at these components used to provide input and output.&lt;/p&gt; &#xA;&lt;h3&gt;Components Attributes&lt;/h3&gt; &#xA;&lt;p&gt;We saw some simple &lt;code&gt;Textbox&lt;/code&gt; components in the previous examples, but what if you want to change how the UI components look or behave?&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say you want to customize the input text field â€” for example, you wanted it to be larger and have a text placeholder. If we use the actual class for &lt;code&gt;Textbox&lt;/code&gt; instead of using the string shortcut, you have access to much more customizability through component attributes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name):&#xA;    return &#34;Hello &#34; + name + &#34;!&#34;&#xA;&#xA;demo = gr.Interface(&#xA;    fn=greet,&#xA;    inputs=gr.Textbox(lines=2, placeholder=&#34;Name Here...&#34;),&#xA;    outputs=&#34;text&#34;,&#xA;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_world_2/screenshot.gif&#34; alt=&#34;hello_world_2 demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multiple Input and Output Components&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you had a more complex function, with multiple inputs and outputs. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. Take a look how you pass a list of input and output components.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name, is_morning, temperature):&#xA;    salutation = &#34;Good morning&#34; if is_morning else &#34;Good evening&#34;&#xA;    greeting = f&#34;{salutation} {name}. It is {temperature} degrees today&#34;&#xA;    celsius = (temperature - 32) * 5 / 9&#xA;    return greeting, round(celsius, 2)&#xA;&#xA;demo = gr.Interface(&#xA;    fn=greet,&#xA;    inputs=[&#34;text&#34;, &#34;checkbox&#34;, gr.Slider(0, 100)],&#xA;    outputs=[&#34;text&#34;, &#34;number&#34;],&#xA;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_world_3/screenshot.gif&#34; alt=&#34;hello_world_3 demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You simply wrap the components in a list. Each component in the &lt;code&gt;inputs&lt;/code&gt; list corresponds to one of the parameters of the function, in order. Each component in the &lt;code&gt;outputs&lt;/code&gt; list corresponds to one of the values returned by the function, again in order.&lt;/p&gt; &#xA;&lt;h3&gt;An Image Example&lt;/h3&gt; &#xA;&lt;p&gt;Gradio supports many types of components, such as &lt;code&gt;Image&lt;/code&gt;, &lt;code&gt;DataFrame&lt;/code&gt;, &lt;code&gt;Video&lt;/code&gt;, or &lt;code&gt;Label&lt;/code&gt;. Let&#39;s try an image-to-image function to get a feel for these!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import gradio as gr&#xA;&#xA;def sepia(input_img):&#xA;    sepia_filter = np.array([&#xA;        [0.393, 0.769, 0.189], &#xA;        [0.349, 0.686, 0.168], &#xA;        [0.272, 0.534, 0.131]&#xA;    ])&#xA;    sepia_img = input_img.dot(sepia_filter.T)&#xA;    sepia_img /= sepia_img.max()&#xA;    return sepia_img&#xA;&#xA;demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), &#34;image&#34;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/sepia_filter/screenshot.gif&#34; alt=&#34;sepia_filter demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;When using the &lt;code&gt;Image&lt;/code&gt; component as input, your function will receive a NumPy array with the shape &lt;code&gt;(width, height, 3)&lt;/code&gt;, where the last dimension represents the RGB values. We&#39;ll return an image as well in the form of a NumPy array.&lt;/p&gt; &#xA;&lt;p&gt;You can also set the datatype used by the component with the &lt;code&gt;type=&lt;/code&gt; keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input &lt;code&gt;Image&lt;/code&gt; component could be written as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gr.Image(type=&#34;filepath&#34;, shape=...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also note that our input &lt;code&gt;Image&lt;/code&gt; component comes with an edit button ðŸ–‰, which allows for cropping and zooming into images. Manipulating images in this way can help reveal biases or hidden flaws in a machine learning model!&lt;/p&gt; &#xA;&lt;p&gt;You can read more about the many components and how to use them in the &lt;a href=&#34;https://gradio.app/docs&#34;&gt;Gradio docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Blocks: More Flexibility and Control&lt;/h3&gt; &#xA;&lt;p&gt;Gradio offers two classes to build apps:&lt;/p&gt; &#xA;&lt;p&gt;1. &lt;strong&gt;Interface&lt;/strong&gt;, that provides a high-level abstraction for creating demos that we&#39;ve been discussing so far.&lt;/p&gt; &#xA;&lt;p&gt;2. &lt;strong&gt;Blocks&lt;/strong&gt;, a low-level API for designing web apps with more flexible layouts and data flows. Blocks allows you to do things like feature multiple data flows and demos, control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. If this customizability is what you need, try &lt;code&gt;Blocks&lt;/code&gt; instead!&lt;/p&gt; &#xA;&lt;h3&gt;Hello, Blocks&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s take a look at a simple example. Note how the API here differs from &lt;code&gt;Interface&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name):&#xA;    return &#34;Hello &#34; + name + &#34;!&#34;&#xA;&#xA;with gr.Blocks() as demo:&#xA;    name = gr.Textbox(label=&#34;Name&#34;)&#xA;    output = gr.Textbox(label=&#34;Output Box&#34;)&#xA;    greet_btn = gr.Button(&#34;Greet&#34;)&#xA;    greet_btn.click(fn=greet, inputs=name, outputs=output)&#xA;&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_blocks/screenshot.gif&#34; alt=&#34;hello_blocks demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Things to note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Blocks&lt;/code&gt; are made with a &lt;code&gt;with&lt;/code&gt; clause, and any component created inside this clause is automatically added to the app.&lt;/li&gt; &#xA; &lt;li&gt;Components appear vertically in the app in the order they are created. (Later we will cover customizing layouts!)&lt;/li&gt; &#xA; &lt;li&gt;A &lt;code&gt;Button&lt;/code&gt; was created, and then a &lt;code&gt;click&lt;/code&gt; event-listener was added to this button. The API for this should look familiar! Like an &lt;code&gt;Interface&lt;/code&gt;, the &lt;code&gt;click&lt;/code&gt; method takes a Python function, input components, and output components.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;More Complexity&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s an app to give you a taste of what&#39;s possible with &lt;code&gt;Blocks&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import gradio as gr&#xA;&#xA;def flip_text(x):&#xA;    return x[::-1]&#xA;&#xA;def flip_image(x):&#xA;    return np.fliplr(x)&#xA;&#xA;with gr.Blocks() as demo:&#xA;    gr.Markdown(&#34;Flip text or image files using this demo.&#34;)&#xA;    with gr.Tabs():&#xA;        with gr.TabItem(&#34;Flip Text&#34;):&#xA;            text_input = gr.Textbox()&#xA;            text_output = gr.Textbox()&#xA;            text_button = gr.Button(&#34;Flip&#34;)&#xA;        with gr.TabItem(&#34;Flip Image&#34;):&#xA;            with gr.Row():&#xA;                image_input = gr.Image()&#xA;                image_output = gr.Image()&#xA;            image_button = gr.Button(&#34;Flip&#34;)&#xA;    &#xA;    text_button.click(flip_text, inputs=text_input, outputs=text_output)&#xA;    image_button.click(flip_image, inputs=image_input, outputs=image_output)&#xA;    &#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/blocks_flipper/screenshot.gif&#34; alt=&#34;blocks_flipper demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A lot more going on here! We&#39;ll cover how to create complex &lt;code&gt;Blocks&lt;/code&gt; apps like this in the &lt;a href=&#34;https://github.com/gradio-app/gradio/tree/main/guides/3)building_with_blocks&#34;&gt;building with blocks&lt;/a&gt; section for you.&lt;/p&gt; &#xA;&lt;p&gt;Congrats, you&#39;re now familiar with the basics of Gradio! ðŸ¥³ Go to our &lt;a href=&#34;https://gradio.app/key_features&#34;&gt;next guide&lt;/a&gt; to learn more about the key features of Gradio.&lt;/p&gt; &#xA;&lt;h2&gt;Open Source Stack&lt;/h2&gt; &#xA;&lt;p&gt;Gradio is built with many wonderful open-source libraries, please support them as well!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/huggingface_mini.svg?sanitize=true&#34; alt=&#34;huggingface&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/python.svg?sanitize=true&#34; alt=&#34;python&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/fastapi.svg?sanitize=true&#34; alt=&#34;fastapi&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.encode.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/encode.svg?sanitize=true&#34; alt=&#34;encode&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://svelte.dev&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/svelte.svg?sanitize=true&#34; alt=&#34;svelte&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://vitejs.dev&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/vite.svg?sanitize=true&#34; alt=&#34;vite&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pnpm.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/pnpm.svg?sanitize=true&#34; alt=&#34;pnpm&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tailwindcss.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/tailwind.svg?sanitize=true&#34; alt=&#34;tailwind&#34; height=&#34;40&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Gradio is licensed under the Apache License 2.0 found in the &lt;a href=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file in the root directory of this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Also check out the paper &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.02569&#34;&gt;Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild&lt;/a&gt;, ICML HILL 2019&lt;/em&gt;, and please cite it if you use Gradio in your work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{abid2019gradio,&#xA;  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},&#xA;  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},&#xA;  journal = {arXiv preprint arXiv:1906.02569},&#xA;  year = {2019},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>optuna/optuna</title>
    <updated>2022-08-31T01:38:07Z</updated>
    <id>tag:github.com,2022-08-31:/optuna/optuna</id>
    <link href="https://github.com/optuna/optuna" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A hyperparameter optimization framework&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;img src=&#34;https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png&#34; width=&#34;800&#34;&gt;&#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Optuna: A hyperparameter optimization framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8%20%7C%203.9%20%7C%203.10-blue&#34; alt=&#34;Python&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/optuna&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/optuna.svg?sanitize=true&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/optuna&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/vn/conda-forge/optuna.svg?sanitize=true&#34; alt=&#34;conda&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/optuna/optuna&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/optuna/optuna&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/optuna/optuna.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/optuna/badge/?version=stable&#34; alt=&#34;Read the Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/optuna/optuna/branch/master&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/optuna/optuna/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;Codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/optuna/optuna&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/optuna/gitter.svg?sanitize=true&#34; alt=&#34;Gitter chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://optuna.org/&#34;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/installation.html&#34;&gt;&lt;strong&gt;Install Guide&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/index.html&#34;&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Optuna&lt;/em&gt; is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, &lt;em&gt;define-by-run&lt;/em&gt; style user API. Thanks to our &lt;em&gt;define-by-run&lt;/em&gt; API, the code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;p&gt;Optuna has modern functionalities as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/001_first.html&#34;&gt;Lightweight, versatile, and platform agnostic architecture&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Handle a wide variety of tasks with a simple installation that has few requirements.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html&#34;&gt;Pythonic search spaces&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Define search spaces using familiar Python syntax including conditionals and loops.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html&#34;&gt;Efficient optimization algorithms&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Adopt state-of-the-art algorithms for sampling hyperparameters and efficiently pruning unpromising trials.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html&#34;&gt;Easy parallelization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Scale studies to tens or hundreds or workers with little or no changes to the code.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html&#34;&gt;Quick visualization&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Inspect optimization histories from a variety of plotting functions.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Basic Concepts&lt;/h2&gt; &#xA;&lt;p&gt;We use the terms &lt;em&gt;study&lt;/em&gt; and &lt;em&gt;trial&lt;/em&gt; as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Study: optimization based on an objective function&lt;/li&gt; &#xA; &lt;li&gt;Trial: a single execution of the objective function&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to sample code below. The goal of a &lt;em&gt;study&lt;/em&gt; is to find out the optimal set of hyperparameter values (e.g., &lt;code&gt;regressor&lt;/code&gt; and &lt;code&gt;svr_c&lt;/code&gt;) through multiple &lt;em&gt;trials&lt;/em&gt; (e.g., &lt;code&gt;n_trials=100&lt;/code&gt;). Optuna is a framework designed for the automation and the acceleration of the optimization &lt;em&gt;studies&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://colab.research.google.com/github/optuna/optuna-examples/blob/main/quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ...&#xA;&#xA;# Define an objective function to be minimized.&#xA;def objective(trial):&#xA;&#xA;    # Invoke suggest methods of a Trial object to generate hyperparameters.&#xA;    regressor_name = trial.suggest_categorical(&#39;regressor&#39;, [&#39;SVR&#39;, &#39;RandomForest&#39;])&#xA;    if regressor_name == &#39;SVR&#39;:&#xA;        svr_c = trial.suggest_float(&#39;svr_c&#39;, 1e-10, 1e10, log=True)&#xA;        regressor_obj = sklearn.svm.SVR(C=svr_c)&#xA;    else:&#xA;        rf_max_depth = trial.suggest_int(&#39;rf_max_depth&#39;, 2, 32)&#xA;        regressor_obj = sklearn.ensemble.RandomForestRegressor(max_depth=rf_max_depth)&#xA;&#xA;    X, y = sklearn.datasets.fetch_california_housing(return_X_y=True)&#xA;    X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, random_state=0)&#xA;&#xA;    regressor_obj.fit(X_train, y_train)&#xA;    y_pred = regressor_obj.predict(X_val)&#xA;&#xA;    error = sklearn.metrics.mean_squared_error(y_val, y_pred)&#xA;&#xA;    return error  # An objective value linked with the Trial object.&#xA;&#xA;study = optuna.create_study()  # Create a new study.&#xA;study.optimize(objective, n_trials=100)  # Invoke optimization of the objective function.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Examples can be found in &lt;a href=&#34;https://github.com/optuna/optuna-examples&#34;&gt;optuna/optuna-examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html#integration-modules-for-pruning&#34;&gt;Integrations modules&lt;/a&gt;, which allow pruning, or early stopping, of unpromising trials are available for the following libraries:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/allennlp&#34;&gt;AllenNLP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/pytorch/catalyst_simple.py&#34;&gt;Catalyst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/catboost/catboost_pruning.py&#34;&gt;Catboost&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/chainer/chainer_integration.py&#34;&gt;Chainer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FastAI (&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/fastai/fastaiv1_simple.py&#34;&gt;V1&lt;/a&gt;, &lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/fastai/fastaiv2_simple.py&#34;&gt;V2&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/keras/keras_integration.py&#34;&gt;Keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/lightgbm/lightgbm_integration.py&#34;&gt;LightGBM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/mxnet/mxnet_integration.py&#34;&gt;MXNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_simple.py&#34;&gt;PyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_ignite_simple.py&#34;&gt;PyTorch Ignite&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_lightning_simple.py&#34;&gt;PyTorch Lightning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/tensorflow/tensorflow_estimator_integration.py&#34;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/tfkeras/tfkeras_integration.py&#34;&gt;tf.keras&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna-examples/tree/main/xgboost/xgboost_integration.py&#34;&gt;XGBoost&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Web Dashboard (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;The new Web dashboard is under the development at &lt;a href=&#34;https://github.com/optuna/optuna-dashboard&#34;&gt;optuna-dashboard&lt;/a&gt;. It is still experimental, but much better in many regards. Feature requests and bug reports welcome!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Manage studies&lt;/th&gt; &#xA;   &lt;th&gt;Visualize with interactive graphs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5564044/97099702-4107be80-16cf-11eb-9d97-f5ceec98ce52.gif&#34; alt=&#34;manage-studies&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5564044/97099797-66e19300-16d0-11eb-826c-6977e3941fb0.gif&#34; alt=&#34;optuna-realtime-graph&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Install &lt;code&gt;optuna-dashboard&lt;/code&gt; via pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install optuna-dashboard&#xA;$ optuna-dashboard sqlite:///db.sqlite3&#xA;...&#xA;Listening on http://localhost:8080/&#xA;Hit Ctrl-C to quit.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Optuna is available at &lt;a href=&#34;https://pypi.org/project/optuna/&#34;&gt;the Python Package Index&lt;/a&gt; and on &lt;a href=&#34;https://anaconda.org/conda-forge/optuna&#34;&gt;Anaconda Cloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# PyPI&#xA;$ pip install optuna&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Anaconda Cloud&#xA;$ conda install -c conda-forge optuna&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optuna supports Python 3.6 or newer.&lt;/p&gt; &#xA;&lt;p&gt;Also, we also provide Optuna docker images on &lt;a href=&#34;https://hub.docker.com/r/optuna/optuna&#34;&gt;DockerHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna/discussions&#34;&gt;GitHub Discussions&lt;/a&gt; for questions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/optuna/optuna/issues&#34;&gt;GitHub Issues&lt;/a&gt; for bug reports and feature requests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitter.im/optuna/optuna&#34;&gt;Gitter&lt;/a&gt; for interactive chat with developers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/tagged/optuna&#34;&gt;Stack Overflow&lt;/a&gt; for questions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Any contributions to Optuna are more than welcome!&lt;/p&gt; &#xA;&lt;p&gt;If you are new to Optuna, please check the &lt;a href=&#34;https://github.com/optuna/optuna/labels/good%20first%20issue&#34;&gt;good first issues&lt;/a&gt;. They are relatively simple, well-defined and are often good starting points for you to get familiar with the contribution workflow and other developers.&lt;/p&gt; &#xA;&lt;p&gt;If you already have contributed to Optuna, we recommend the other &lt;a href=&#34;https://github.com/optuna/optuna/labels/contribution-welcome&#34;&gt;contribution-welcome issues&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For general guidelines how to contribute to the project, take a look at &lt;a href=&#34;https://raw.githubusercontent.com/optuna/optuna/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A Next-generation Hyperparameter Optimization Framework. In KDD (&lt;a href=&#34;https://arxiv.org/abs/1907.10902&#34;&gt;arXiv&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
</feed>