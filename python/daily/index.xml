<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-28T01:37:09Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gensyn-ai/rl-swarm</title>
    <updated>2025-06-28T01:37:09Z</updated>
    <id>tag:github.com,2025-06-28:/gensyn-ai/rl-swarm</id>
    <link href="https://github.com/gensyn-ai/rl-swarm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fully open source framework for creating RL training swarms over the internet.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RL Swarm&lt;/h1&gt; &#xA;&lt;p&gt;RL Swarm is a peer-to-peer system for reinforcement learning. It allows you to train models collaboratively with others in the swarm, leveraging their collective intelligence. It is open source and permissionless, meaning you can run it on a consumer laptop at home or on a powerful GPU in the cloud. You can also connect your model to the Gensyn Testnet to receive an on-chain identity that tracks your progress over time.&lt;/p&gt; &#xA;&lt;p&gt;Currently, we are running the &lt;a href=&#34;https://github.com/open-thought/reasoning-gym/tree/main&#34;&gt;reasoning-gym&lt;/a&gt; swarm on the Testnet. This swarm is designed to train models to solve a diverse set of reasoning tasks using the reasoning-gym dataset. The current list of default models includes:&lt;/p&gt; &#xA;&lt;p&gt;Models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gensyn/Qwen2.5-0.5B-Instruct&lt;/li&gt; &#xA; &lt;li&gt;Qwen/Qwen3-0.6B&lt;/li&gt; &#xA; &lt;li&gt;nvidia/AceInstruct-1.5B&lt;/li&gt; &#xA; &lt;li&gt;dnotitia/Smoothie-Qwen3-1.7B&lt;/li&gt; &#xA; &lt;li&gt;Gensyn/Qwen2.5-1.5B-Instruct&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This iteration of rl-swarm is powered by the &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm&#34;&gt;GenRL-Swarm&lt;/a&gt; library. It is a fully composable framework for decentralized reinforcement learning which enables users to create and customize their own swarms for reinforcement learning with multi-agent multi-stage environments.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Your hardware requirements will vary depending on a number of factors including model size and the accelerator platform you use. Users running large NVIDIA GPU will be assigned a model from the large model pool, while users running less powerful hardware will be assigned a model from the small model pool. This design decision is intended to allow users to advance at a similar rate regardless of the hardware they use, maximizing their utility to the swarm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported Hardware&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;arm64 or x86 CPU with minimum 32gb ram (note that if you run other applications during training it might crash training).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;OR&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA devices (officially supported): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RTX 3090&lt;/li&gt; &#xA;   &lt;li&gt;RTX 4090&lt;/li&gt; &#xA;   &lt;li&gt;RTX 5090&lt;/li&gt; &#xA;   &lt;li&gt;A100&lt;/li&gt; &#xA;   &lt;li&gt;H100&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;With either configuration, you will need Python &amp;gt;=3.10 (for Mac, you will likely need to upgrade).&lt;/p&gt; &#xA;&lt;h2&gt;⚠️ Please read before continuing ⚠️&lt;/h2&gt; &#xA;&lt;p&gt;This software is &lt;strong&gt;experimental&lt;/strong&gt; and provided as-is for users who are interested in using (or helping to develop) an early version of the Gensyn Protocol for training models.&lt;/p&gt; &#xA;&lt;p&gt;If you care about on-chain participation, you &lt;strong&gt;must&lt;/strong&gt; read the &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#identity-management&#34;&gt;Identity Management&lt;/a&gt; section below.&lt;/p&gt; &#xA;&lt;p&gt;If you encounter issues, please first check &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt;. If you cannot find a solution there, please check if there is an open (or closed) &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/issues&#34;&gt;Issue&lt;/a&gt;. If there is no relevant issue, please file one and include 1) all relevant &lt;a href=&#34;https://raw.githubusercontent.com/gensyn-ai/rl-swarm/main/#troubleshooting&#34;&gt;logs&lt;/a&gt;, 2) information about your device (e.g. which GPU, if relevant), and 3) your operating system information.&lt;/p&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;h3&gt;Run the Swarm&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to run RL Swarm is using Docker. This ensures a consistent setup across all operating systems with minimal dependencies.&lt;/p&gt; &#xA;&lt;h4&gt;1. Clone this repo&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/gensyn-ai/rl-swarm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Install Docker&lt;/h4&gt; &#xA;&lt;p&gt;Make sure you have Docker installed and the Docker daemon is running on your machine. To do that, follow &lt;a href=&#34;https://docs.docker.com/get-started/get-docker/&#34;&gt;these instructions&lt;/a&gt; according to your OS. Ensure you allot sufficient memory to the Docker containers. For example if using Docker Desktop, this can be done by going to Docker Desktop Settings &amp;gt; Resources &amp;gt; Advanced &amp;gt; Memory Limit, and increasing it to the maximum possible value.&lt;/p&gt; &#xA;&lt;h4&gt;3. Start the Swarm&lt;/h4&gt; &#xA;&lt;p&gt;Run the following commands from the root of the repository.&lt;/p&gt; &#xA;&lt;h5&gt;CPU support&lt;/h5&gt; &#xA;&lt;p&gt;If you’re using a Mac or if your machine has CPU-only support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose run --rm --build -Pit swarm-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;GPU support&lt;/h5&gt; &#xA;&lt;p&gt;If you&#39;re using a machine with an officially supported GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose run --rm --build -Pit swarm-gpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Docker compose issue&lt;/h5&gt; &#xA;&lt;p&gt;If &lt;code&gt;docker-compose&lt;/code&gt; does not work when running the above commands, please try &lt;code&gt;docker compose&lt;/code&gt; (no hyphen) instead. I.e. &lt;code&gt; docker compose run --rm --build -Pit swarm-gpu&lt;/code&gt;. This issue sometimes occurs on users running Ubuntu.&lt;/p&gt; &#xA;&lt;h3&gt;Experimental (advanced) mode&lt;/h3&gt; &#xA;&lt;p&gt;If you want to experiment with the &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm&#34;&gt;GenRL-Swarm&lt;/a&gt; library and its &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm/raw/main/recipes/rgym/rg-swarm.yaml&#34;&gt;configurable parameters&lt;/a&gt;, we recommend you run RL Swarm via shell script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;./run_rl_swarm.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To learn more about experimental mode, check out our &lt;a href=&#34;https://github.com/gensyn-ai/genrl-swarm/raw/main/getting_started.ipynb&#34;&gt;getting started guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Login&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A browser window will pop open (you&#39;ll need to manually navigate to &lt;a href=&#34;http://localhost:3000/&#34;&gt;http://localhost:3000/&lt;/a&gt; if you&#39;re on a VM).&lt;/li&gt; &#xA; &lt;li&gt;Click &#39;login&#39;.&lt;/li&gt; &#xA; &lt;li&gt;Login with your preferred method.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;p&gt;If you would like to upload your model to Hugging Face, enter your Hugging Face access token when prompted. You can generate one from your Hugging Face account, under &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;Access Tokens&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Initial peering and training&lt;/h3&gt; &#xA;&lt;p&gt;From this stage onward your device will begin training. You should see your peer register and vote on-chain &lt;a href=&#34;https://gensyn-testnet.explorer.alchemy.com/address/0xFaD7C5e93f28257429569B854151A1B8DCD404c2?tab=logs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also track your training progress in real time:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On The RL-Swarm Dashboard: &lt;a href=&#34;https://dashboard.gensyn.ai&#34;&gt;dashboard.gensyn.ai&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Identity management&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction&lt;/h3&gt; &#xA;&lt;p&gt;On-chain identity is managed via an Alchemy modal sign-in screen. You need to supply an email address or login via a supported method (e.g. Google). This creates an EOA public/private key (which are stored by Alchemy). You will also receive local session keys in the &lt;code&gt;userApiKey&lt;/code&gt;. Note that these aren&#39;t your EOA public/private keys.&lt;/p&gt; &#xA;&lt;p&gt;During the initial set-up process, you will also create a &lt;code&gt;swarm.pem&lt;/code&gt; file which maintains the identity of your peer. This is then registered on chain using the EOA wallet hosted in Alchemy, triggered using your local api keys. This links the &lt;code&gt;swarm.pem&lt;/code&gt; to the &lt;code&gt;email address&lt;/code&gt; (and corresponding EOA in Alchemy).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you want to link multiple nodes to a single EOA&lt;/strong&gt;, simply sign up each node using the same email address. You will get a new peer ID for each node, however they will all be linked to the same EOA that your email is linked to.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;: if you are using a fork of this repo, or a service organised by someone else (e.g. a &#39;one click deployment&#39; provider) the identity management flow below is not guaranteed.&lt;/p&gt; &#xA;&lt;h3&gt;What this means&lt;/h3&gt; &#xA;&lt;p&gt;In the following two scenarios, everything will work (i.e. you will have an on-chain identity linked with your RL Swarm peer training):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The very first time you run the node from scratch with a new email address. The smart account will be created fresh and linked with the swarm.pem that is also fresh.&lt;/li&gt; &#xA; &lt;li&gt;If you run it again with a &lt;code&gt;swarm.pem&lt;/code&gt; AND login the original &lt;code&gt;email address&lt;/code&gt; used with that &lt;code&gt;swarm.pem&lt;/code&gt;. Note: this will throw an error into the log on registration but will still be able to sign transactions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In the following two scenarios, it will not work (i.e. you won&#39;t have an on-chain identity linked with your RL Swarm peer training):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you keep your &lt;code&gt;swarm.pem&lt;/code&gt; and try to link it to an &lt;code&gt;email address&lt;/code&gt; distinct from the one with which it was first registered.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Therefore, you should do these actions in the following scenarios&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Signed up with &lt;code&gt;email address&lt;/code&gt;, generated &lt;code&gt;swarm.pem&lt;/code&gt;, BUT lost &lt;code&gt;swarm.pem&lt;/code&gt;&lt;/strong&gt; OR &lt;strong&gt;You want to run multiple nodes at once&lt;/strong&gt;: run from scratch with the same email address and generate a new &lt;code&gt;swarm.pem&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Signed up with &lt;code&gt;email address&lt;/code&gt;, generated &lt;code&gt;swarm.pem&lt;/code&gt;, kept &lt;code&gt;swarm.pem&lt;/code&gt;&lt;/strong&gt; -&amp;gt; you can re-run a single node using this pair if you&#39;ve still got them both.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How do I find my logs?&lt;/strong&gt; You can find them inside the &lt;code&gt;/logs&lt;/code&gt; directory:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;yarn.log&lt;/code&gt;: This file contains logs for the modal login server.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;swarm.log&lt;/code&gt;: This is the main log file for the RL Swarm application.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;wandb/&lt;/code&gt;: This directory contains various logs related to your training runs, including a &lt;code&gt;debug.log&lt;/code&gt; file. These can be updated to Weights &amp;amp; Biases (only available if you log_with wandb).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;My peer &#39;skipped a round&#39;&lt;/strong&gt;: this occurs when your device isn&#39;t fast enough to keep up with the pace of the swarm. For example, if you start training at round 100 and by the time you finish training the rest of the swarm reaches round 102, you will skip round 101 and go straight to 102. This is because your peer is more valuable if it is participating in the active round.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;My model doesn&#39;t seem to be training?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you&#39;re using a consumer device (e.g. a MacBook), it is likely just running slowly - check back in 20 minutes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Logging in with a new account after previous login?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure you click &#39;Logout&#39; on the login screen before you leave your previous session&lt;/li&gt; &#xA;   &lt;li&gt;Make sure you delete &lt;code&gt;swarm.pem&lt;/code&gt; from the root directory (try &lt;code&gt;sudo rm swarm.pem&lt;/code&gt;). If you don&#39;t do this, and you previously registered with the peer-id stored in this file, it will disrupt the training process.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Issues with the Login screen&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Upgrade viem&lt;/strong&gt;: some users report issues with the &lt;code&gt;viem&lt;/code&gt; package. There are two fixes: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;in the &lt;code&gt;modal-login/package.json&lt;/code&gt; update: &lt;code&gt;&#34;viem&#34;: &#34;2.25.0&#34;&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;in the terminal &lt;code&gt;cd /root/rl-swarm/modal-login/ &amp;amp;&amp;amp; yarn upgrade &amp;amp;&amp;amp; yarn add next@latest &amp;amp;&amp;amp; yarn add viem@latest&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I&#39;m getting lots of warnings&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is expected behaviour and usually the output of the package managers or other dependencies. The most common is the below Protobuf warning - which can be ignored &lt;pre&gt;&lt;code&gt;WARNING: The candidate selected for download or install is a yanked version: &#39;protobuf&#39; candidate...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Issues on VMs/VPSs?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;How do I access the login screen if I&#39;m running in a VM?&lt;/strong&gt;: port forwarding. Add this SSH flag: &lt;code&gt;-L 3000:localhost:3000&lt;/code&gt; when connecting to your VM. E.g. &lt;code&gt;gcloud compute ssh --zone &#34;us-central1-a&#34; [your-vm] --project [your-project] -- -L 3000:localhost:3000&lt;/code&gt;. Note, some VPSs may not work with &lt;code&gt;rl-swarm&lt;/code&gt;. Check the Gensyn &lt;a href=&#34;https://discord.gg/AdnyWNzXh5&#34;&gt;discord&lt;/a&gt; for up-to-date information on this.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Disconnection/general issues&lt;/strong&gt;: If you are tunneling to a VM and suffer a broken pipe, you will likely encounter OOM or unexpected behaviour the first time you relaunch the script. If you &lt;code&gt;control + c&lt;/code&gt; and kill the script it should spin down all background processes. Restart the script and everything should work normally.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Issues with npm/general installation?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Try &lt;code&gt;npm install -g node@latest&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OOM errors on MacBook?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Try this (experimental) fix to increase memory: &lt;pre&gt;&lt;code&gt;export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I have a Windows machine, can I still train a model on the swarm?&lt;/strong&gt;: Yes - but this is not very well tested and may require you to do some debugging to get it set up properly. Install WSL and Linux on your Windows machine using the following instructions: &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install&#34;&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I want to move my to a different machine and/or restart with a fresh build of the repo, but I want my animal name/peer id to persist.&lt;/strong&gt;: To achieve this simply backup the &lt;code&gt;swarm.pem&lt;/code&gt; file on your current machine and then put it in the corresponding location on your new machine/build of the repo.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I have multiple GPUs on one machine, can I run multiple peers?&lt;/strong&gt;: Yes - but you&#39;ll need to manually change things. You&#39;ll need to isolate each GPU, install this repo for each GPU, and expose each peer under a different port to pass the modal onboard.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;My round/stage is behind the smart contract/other peers?&lt;/strong&gt;: This is expected behaviour given the different speeds of machines in the network. Once your machine completes it&#39;s current round, it will move to the the current round.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I want to use a bigger and/or different model in the RL swarm, can I do that?&lt;/strong&gt;: Yes - but we only recommend doing so if you are comfortable understanding what size model can reasonably run on your hardware. If you elect to bring a custom model, just paste the repo/model name into the command line when prompted.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;I am running a model in the swarm on my CPU, have received a python &lt;code&gt;RuntimeError&lt;/code&gt;, and my training progress seems to have stopped.&lt;/strong&gt;: There are several possible causes for this, but before trying anything please wait long enough to be sure your training actually is frozen and not just slow (e.g., wait longer than a single training iteration has previously taken on your machine). If you&#39;re sure training is actually frozen, then some things to try are:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set this (experimental) fix: &lt;code&gt;export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 &amp;amp;&amp;amp; ./run_rl_swarm.sh&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Peterande/D-FINE</title>
    <updated>2025-06-28T01:37:09Z</updated>
    <id>tag:github.com,2025-06-28:/Peterande/D-FINE</id>
    <link href="https://github.com/Peterande/D-FINE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement [ICLR 2025 Spotlight]&lt;/p&gt;&lt;hr&gt;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/README_cn.md&#34;&gt;简体中文&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/README_ja.md&#34;&gt;日本語&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/src/zoo/dfine/blog.md&#34;&gt;English Blog&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/src/zoo/dfine/blog_cn.md&#34;&gt;中文博客&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; D-FINE: Redefine Regression Task of DETRs as Fine‑grained&amp;nbsp;Distribution&amp;nbsp;Refinement &lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/developer0hye/D-FINE&#34;&gt; &lt;img alt=&#34;hf&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE/raw/master/LICENSE&#34;&gt; &lt;img alt=&#34;license&#34; src=&#34;https://img.shields.io/badge/LICENSE-Apache%202.0-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE/pulls&#34;&gt; &lt;img alt=&#34;prs&#34; src=&#34;https://img.shields.io/github/issues-pr/Peterande/D-FINE&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE/issues&#34;&gt; &lt;img alt=&#34;issues&#34; src=&#34;https://img.shields.io/github/issues/Peterande/D-FINE?color=olive&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2410.13842&#34;&gt; &lt;img alt=&#34;arXiv&#34; src=&#34;https://img.shields.io/badge/arXiv-2410.13842-red&#34;&gt; &lt;/a&gt; &#xA; &lt;!--     &lt;a href=&#34;mailto: pengyansong@mail.ustc.edu.cn&#34;&gt;&#xA;        &lt;img alt=&#34;email&#34; src=&#34;https://img.shields.io/badge/contact_me-email-yellow&#34;&gt;&#xA;    &lt;/a&gt; --&gt; &lt;a href=&#34;https://results.pre-commit.ci/latest/github/Peterande/D-FINE/master&#34;&gt; &lt;img alt=&#34;pre-commit.ci status&#34; src=&#34;https://results.pre-commit.ci/badge/github/Peterande/D-FINE/master.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/Peterande/D-FINE&#34;&gt; &lt;img alt=&#34;stars&#34; src=&#34;https://img.shields.io/github/stars/Peterande/D-FINE&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 📄 This is the official implementation of the paper: &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2410.13842&#34;&gt;D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, and Feng Wu &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; University of Science and Technology of China &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=d-fine-redefine-regression-task-in-detrs-as&#34;&gt; &lt;img alt=&#34;sota&#34; src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/d-fine-redefine-regression-task-in-detrs-as/real-time-object-detection-on-coco&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;!-- &lt;table&gt;&lt;tr&gt;&#xA;&lt;td&gt;&lt;img src=https://github.com/Peterande/storage/blob/master/latency.png border=0 width=333&gt;&lt;/td&gt;&#xA;&lt;td&gt;&lt;img src=https://github.com/Peterande/storage/blob/master/params.png border=0 width=333&gt;&lt;/td&gt;&#xA;&lt;td&gt;&lt;img src=https://github.com/Peterande/storage/blob/master/flops.png border=0 width=333&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&lt;/table&gt; --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;If you like D-FINE, please give us a ⭐! Your support motivates us to keep improving!&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/stats_padded.png&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;D-FINE is a powerful real-time object detector that redefines the bounding box regression task in DETRs as Fine-grained Distribution Refinement (FDR) and introduces Global Optimal Localization Self-Distillation (GO-LSD), achieving outstanding performance without introducing additional inference and training costs.&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; Video &lt;/summary&gt; &#xA; &lt;p&gt;We conduct object detection using D-FINE and YOLO11 on a complex street scene video from &lt;a href=&#34;https://www.youtube.com/watch?v=CfhEWj9sd9A&#34;&gt;YouTube&lt;/a&gt;. Despite challenging conditions such as backlighting, motion blur, and dense crowds, D-FINE-X successfully detects nearly all targets, including subtle small objects like backpacks, bicycles, and traffic lights. Its confidence scores and the localization precision for blurred edges are significantly higher than those of YOLO11.&lt;/p&gt; &#xA; &lt;!-- We use D-FINE and YOLO11 on a street scene video from [YouTube](https://www.youtube.com/watch?v=CfhEWj9sd9A). Despite challenges like backlighting, motion blur, and dense crowds, D-FINE-X outperforms YOLO11x, detecting more objects with higher confidence and better precision. --&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/e5933d8e-3c8a-400e-870b-4e452f5321d9&#34;&gt;https://github.com/user-attachments/assets/e5933d8e-3c8a-400e-870b-4e452f5321d9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🚀 Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.10.18]&lt;/strong&gt; Release D-FINE series.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.10.25]&lt;/strong&gt; Add custom dataset finetuning configs (&lt;a href=&#34;https://github.com/Peterande/D-FINE/issues/7&#34;&gt;#7&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.10.30]&lt;/strong&gt; Update D-FINE-L (E25) pretrained model, with performance improved by 2.0%.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;[2024.11.07]&lt;/strong&gt; Release &lt;strong&gt;D-FINE-N&lt;/strong&gt;, achiving 42.8% AP&lt;sup&gt;val&lt;/sup&gt; on COCO @ 472 FPS&lt;sup&gt;T4&lt;/sup&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;h3&gt;COCO&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;logs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑N&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;42.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.12ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_n_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_n_coco.pth&#34;&gt;42.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_n_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;48.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.49ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_s_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_s_coco.pth&#34;&gt;48.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_s_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;52.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.62ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_m_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_m_coco.pth&#34;&gt;52.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_m_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑L&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_l_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_coco.pth&#34;&gt;54.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_l_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑X&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.89ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;202&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_x_coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_x_coco.pth&#34;&gt;55.8&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/coco/dfine_x_coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Objects365+COCO&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;logs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑S&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.49ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_s_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_s_obj2coco.pth&#34;&gt;50.7&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_s_obj2coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.62ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_m_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_m_obj2coco.pth&#34;&gt;55.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_m_obj2coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑L&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_l_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_obj2coco_e25.pth&#34;&gt;57.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_l_obj2coco_log_e25.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑X&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Objects365+COCO&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12.89ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;202&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_x_obj2coco.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_x_obj2coco.pth&#34;&gt;59.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj2coco/dfine_x_obj2coco_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;We highly recommend that you use the Objects365 pre-trained model for fine-tuning:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;⚠️ &lt;strong&gt;Important&lt;/strong&gt;: Please note that this is generally beneficial for complex scene understanding. If your categories are very simple, it might lead to overfitting and suboptimal performance.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt; 🔥 Pretrained Models on Objects365 (Best generalization) &lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;5000&lt;/sup&gt;&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;GFLOPs&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;config&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;checkpoint&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;logs&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑S&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;31.0&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;30.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;10M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3.49ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;25&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_s_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_s_obj365.pth&#34;&gt;30.5&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_s_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑M&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;38.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;37.4&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;19M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5.62ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;57&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_m_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_m_obj365.pth&#34;&gt;37.4&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_m_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑L&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;40.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_l_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_obj365.pth&#34;&gt;40.6&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_l_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑L (E25)&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;44.7&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;42.6&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;8.07ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;91&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_l_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_l_obj365_e25.pth&#34;&gt;42.6&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_l_obj365_log_e25.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;D‑FINE‑X&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;Objects365&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;46.5&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;62M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;12.89ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;202&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/objects365/dfine_hgnetv2_x_obj365.yml&#34;&gt;yml&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Peterande/storage/releases/download/dfinev1.0/dfine_x_obj365.pth&#34;&gt;46.5&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Peterande/storage/refs/heads/master/logs/obj365/dfine_x_obj365_log.txt&#34;&gt;url&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;E25&lt;/strong&gt;: Re-trained and extended the pretraining to 25 epochs.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; is evaluated on &lt;em&gt;Objects365&lt;/em&gt; full validation set.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;AP&lt;sup&gt;5000&lt;/sup&gt;&lt;/strong&gt; is evaluated on the first 5000 samples of the &lt;em&gt;Objects365&lt;/em&gt; validation set.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; is evaluated on &lt;em&gt;MSCOCO val2017&lt;/em&gt; dataset.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt; is evaluated on a single T4 GPU with $batch\_size = 1$, $fp16$, and $TensorRT==10.4.0$.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Objects365+COCO&lt;/strong&gt; means finetuned model on &lt;em&gt;COCO&lt;/em&gt; using pretrained weights trained on &lt;em&gt;Objects365&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n dfine python=3.11.9&#xA;conda activate dfine&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Preparation&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; COCO2017 Dataset &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download COCO2017 from &lt;a href=&#34;https://opendatalab.com/OpenDataLab/COCO_2017&#34;&gt;OpenDataLab&lt;/a&gt; or &lt;a href=&#34;https://cocodataset.org/#download&#34;&gt;COCO&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Modify paths in &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dataset/coco_detection.yml&#34;&gt;coco_detection.yml&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    img_folder: /data/COCO2017/train2017/&#xA;    ann_file: /data/COCO2017/annotations/instances_train2017.json&#xA;val_dataloader:&#xA;    img_folder: /data/COCO2017/val2017/&#xA;    ann_file: /data/COCO2017/annotations/instances_val2017.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Objects365 Dataset &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download Objects365 from &lt;a href=&#34;https://opendatalab.com/OpenDataLab/Objects365&#34;&gt;OpenDataLab&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Set the Base Directory:&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export BASE_DIR=/data/Objects365/data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Extract and organize the downloaded files, resulting directory structure:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;${BASE_DIR}/train&#xA;├── images&#xA;│   ├── v1&#xA;│   │   ├── patch0&#xA;│   │   │   ├── 000000000.jpg&#xA;│   │   │   ├── 000000001.jpg&#xA;│   │   │   └── ... (more images)&#xA;│   ├── v2&#xA;│   │   ├── patchx&#xA;│   │   │   ├── 000000000.jpg&#xA;│   │   │   ├── 000000001.jpg&#xA;│   │   │   └── ... (more images)&#xA;├── zhiyuan_objv2_train.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;${BASE_DIR}/val&#xA;├── images&#xA;│   ├── v1&#xA;│   │   ├── patch0&#xA;│   │   │   ├── 000000000.jpg&#xA;│   │   │   └── ... (more images)&#xA;│   ├── v2&#xA;│   │   ├── patchx&#xA;│   │   │   ├── 000000000.jpg&#xA;│   │   │   └── ... (more images)&#xA;├── zhiyuan_objv2_val.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Create a New Directory to Store Images from the Validation Set:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir -p ${BASE_DIR}/train/images_from_val&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;5&#34;&gt; &#xA;  &lt;li&gt;Copy the v1 and v2 folders from the val directory into the train/images_from_val directory&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cp -r ${BASE_DIR}/val/images/v1 ${BASE_DIR}/train/images_from_val/&#xA;cp -r ${BASE_DIR}/val/images/v2 ${BASE_DIR}/train/images_from_val/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;6&#34;&gt; &#xA;  &lt;li&gt;Run remap_obj365.py to merge a subset of the validation set into the training set. Specifically, this script moves samples with indices between 5000 and 800000 from the validation set to the training set.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/remap_obj365.py --base_dir ${BASE_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;7&#34;&gt; &#xA;  &lt;li&gt;Run the resize_obj365.py script to resize any images in the dataset where the maximum edge length exceeds 640 pixels. Use the updated JSON file generated in Step 5 to process the sample data. Ensure that you resize images in both the train and val datasets to maintain consistency.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/resize_obj365.py --base_dir ${BASE_DIR}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;8&#34;&gt; &#xA;  &lt;li&gt; &lt;p&gt;Modify paths in &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dataset/obj365_detection.yml&#34;&gt;obj365_detection.yml&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    img_folder: /data/Objects365/data/train&#xA;    ann_file: /data/Objects365/data/train/new_zhiyuan_objv2_train_resized.json&#xA;val_dataloader:&#xA;    img_folder: /data/Objects365/data/val/&#xA;    ann_file: /data/Objects365/data/val/new_zhiyuan_objv2_val_resized.json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;CrowdHuman&lt;/summary&gt; &#xA; &lt;p&gt;Download COCO format dataset here: &lt;a href=&#34;https://aistudio.baidu.com/datasetdetail/231455&#34;&gt;url&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Custom Dataset&lt;/summary&gt; &#xA; &lt;p&gt;To train on your custom dataset, you need to organize it in the COCO format. Follow the steps below to prepare your dataset:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set &lt;code&gt;remap_mscoco_category&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This prevents the automatic remapping of category IDs to match the MSCOCO categories.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;remap_mscoco_category: False&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize Images:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Structure your dataset directories as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;dataset/&#xA;├── images/&#xA;│   ├── train/&#xA;│   │   ├── image1.jpg&#xA;│   │   ├── image2.jpg&#xA;│   │   └── ...&#xA;│   ├── val/&#xA;│   │   ├── image1.jpg&#xA;│   │   ├── image2.jpg&#xA;│   │   └── ...&#xA;└── annotations/&#xA;    ├── instances_train.json&#xA;    ├── instances_val.json&#xA;    └── ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;images/train/&lt;/code&gt;&lt;/strong&gt;: Contains all training images.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;images/val/&lt;/code&gt;&lt;/strong&gt;: Contains all validation images.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;&lt;code&gt;annotations/&lt;/code&gt;&lt;/strong&gt;: Contains COCO-formatted annotation files.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Convert Annotations to COCO Format:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If your annotations are not already in COCO format, you&#39;ll need to convert them. You can use the following Python script as a reference or utilize existing tools:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;&#xA;def convert_to_coco(input_annotations, output_annotations):&#xA;    # Implement conversion logic here&#xA;    pass&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    convert_to_coco(&#39;path/to/your_annotations.json&#39;, &#39;dataset/annotations/instances_train.json&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update Configuration Files:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dataset/custom_detection.yml&#34;&gt;custom_detection.yml&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;task: detection&#xA;&#xA;evaluator:&#xA;  type: CocoEvaluator&#xA;  iou_types: [&#39;bbox&#39;, ]&#xA;&#xA;num_classes: 777 # your dataset classes&#xA;remap_mscoco_category: False&#xA;&#xA;train_dataloader:&#xA;  type: DataLoader&#xA;  dataset:&#xA;    type: CocoDetection&#xA;    img_folder: /data/yourdataset/train&#xA;    ann_file: /data/yourdataset/train/train.json&#xA;    return_masks: False&#xA;    transforms:&#xA;      type: Compose&#xA;      ops: ~&#xA;  shuffle: True&#xA;  num_workers: 4&#xA;  drop_last: True&#xA;  collate_fn:&#xA;    type: BatchImageCollateFunction&#xA;&#xA;val_dataloader:&#xA;  type: DataLoader&#xA;  dataset:&#xA;    type: CocoDetection&#xA;    img_folder: /data/yourdataset/val&#xA;    ann_file: /data/yourdataset/val/ann.json&#xA;    return_masks: False&#xA;    transforms:&#xA;      type: Compose&#xA;      ops: ~&#xA;  shuffle: False&#xA;  num_workers: 4&#xA;  drop_last: False&#xA;  collate_fn:&#xA;    type: BatchImageCollateFunction&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; COCO2017 &lt;/summary&gt; &#xA; &lt;!-- &lt;summary&gt;1. Training &lt;/summary&gt; --&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set Model&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Training&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Testing&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;3. Tuning &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Tuning&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Objects365 to COCO2017 &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set Model&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Training on Objects365&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj365.yml --use-amp --seed=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Tuning on COCO2017&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/objects365/dfine_hgnetv2_${model}_obj2coco.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Testing&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml --test-only -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Custom Dataset &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set Model&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Training on Custom Dataset&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --use-amp --seed=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Testing&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --test-only -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Tuning on Custom Dataset&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/objects365/dfine_hgnetv2_${model}_obj2custom.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;5&#34;&gt; &#xA;  &lt;li&gt;&lt;strong&gt;[Optional]&lt;/strong&gt; Modify Class Mappings:&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;When using the Objects365 pre-trained weights to train on your custom dataset, the example assumes that your dataset only contains the classes &lt;code&gt;&#39;Person&#39;&lt;/code&gt; and &lt;code&gt;&#39;Car&#39;&lt;/code&gt;. For faster convergence, you can modify &lt;code&gt;self.obj365_ids&lt;/code&gt; in &lt;code&gt;src/solver/_solver.py&lt;/code&gt; as follows:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.obj365_ids = [0, 5]  # Person, Cars&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;You can replace these with any corresponding classes from your dataset. The list of Objects365 classes with their corresponding IDs: &lt;a href=&#34;https://github.com/Peterande/D-FINE/raw/352a94ece291e26e1957df81277bef00fe88a8e3/src/solver/_solver.py#L330&#34;&gt;https://github.com/Peterande/D-FINE/blob/352a94ece291e26e1957df81277bef00fe88a8e3/src/solver/_solver.py#L330&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;New training command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/dfine/custom/dfine_hgnetv2_${model}_custom.yml --use-amp --seed=0 -t model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;However, if you don&#39;t wish to modify the class mappings, the pre-trained Objects365 weights will still work without any changes. Modifying the class mappings is optional and can potentially accelerate convergence for specific tasks.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Customizing Batch Size &lt;/summary&gt; &#xA; &lt;p&gt;For example, if you want to double the total batch size when training D-FINE-L on COCO2017, here are the steps you should follow:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/include/dataloader.yml&#34;&gt;dataloader.yml&lt;/a&gt;&lt;/strong&gt; to increase the &lt;code&gt;total_batch_size&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;train_dataloader:&#xA;    total_batch_size: 64  # Previously it was 32, now doubled&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/dfine_hgnetv2_l_coco.yml&#34;&gt;dfine_hgnetv2_l_coco.yml&lt;/a&gt;&lt;/strong&gt;. Here’s how the key parameters should be adjusted:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;optimizer:&#xA;type: AdamW&#xA;params:&#xA;    -&#xA;    params: &#39;^(?=.*backbone)(?!.*norm|bn).*$&#39;&#xA;    lr: 0.000025  # doubled, linear scaling law&#xA;    -&#xA;    params: &#39;^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$&#39;&#xA;    weight_decay: 0.&#xA;&#xA;lr: 0.0005  # doubled, linear scaling law&#xA;betas: [0.9, 0.999]&#xA;weight_decay: 0.0001  # need a grid search&#xA;&#xA;ema:  # added EMA settings&#xA;    decay: 0.9998  # adjusted by 1 - (1 - decay) * 2&#xA;    warmups: 500  # halved&#xA;&#xA;lr_warmup_scheduler:&#xA;    warmup_duration: 250  # halved&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Customizing Input Size &lt;/summary&gt; &#xA; &lt;p&gt;If you&#39;d like to train &lt;strong&gt;D-FINE-L&lt;/strong&gt; on COCO2017 with an input size of 320x320, follow these steps:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/include/dataloader.yml&#34;&gt;dataloader.yml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;&#xA;train_dataloader:&#xA;dataset:&#xA;    transforms:&#xA;        ops:&#xA;            - {type: Resize, size: [320, 320], }&#xA;collate_fn:&#xA;    base_size: 320&#xA;dataset:&#xA;    transforms:&#xA;        ops:&#xA;            - {type: Resize, size: [320, 320], }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modify your &lt;a href=&#34;https://raw.githubusercontent.com/Peterande/D-FINE/master/configs/dfine/include/dfine_hgnetv2.yml&#34;&gt;dfine_hgnetv2.yml&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;eval_spatial_size: [320, 320]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Tools&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Deployment &lt;/summary&gt; &#xA; &lt;!-- &lt;summary&gt;4. Export onnx &lt;/summary&gt; --&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install onnx onnxsim&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Export onnx&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/deployment/export_onnx.py --check -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;3&#34;&gt; &#xA;  &lt;li&gt;Export &lt;a href=&#34;https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html&#34;&gt;tensorrt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;trtexec --onnx=&#34;model.onnx&#34; --saveEngine=&#34;model.engine&#34; --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Inference (Visualization) &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r tools/inference/requirements.txt&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;5. Inference &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Inference (onnxruntime / tensorrt / torch)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Inference on images and videos is now supported.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg  # video.mp4&#xA;python tools/inference/trt_inf.py --trt model.engine --input image.jpg&#xA;python tools/inference/torch_inf.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Benchmark &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r tools/benchmark/requirements.txt&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;!-- &lt;summary&gt;6. Benchmark &lt;/summary&gt; --&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Model FLOPs, MACs, and Params&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/benchmark/get_info.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;TensorRT Latency&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Fiftyone Visualization &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Setup&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install fiftyone&#xA;export model=l  # n s m l x&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;4&#34;&gt; &#xA;  &lt;li&gt;Voxel51 Fiftyone Visualization (&lt;a href=&#34;https://github.com/voxel51/fiftyone&#34;&gt;fiftyone&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/visualization/fiftyone_vis.py -c configs/dfine/dfine_hgnetv2_${model}_coco.yml -r model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Others &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Auto Resume Training&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash reference/safe_training.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Converting Model Weights&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python reference/convert_weight.py model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Figures and Visualizations&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; FDR and GO-LSD &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Overview of D-FINE with FDR. The probability distributions that act as a more fine- grained intermediate representation are iteratively refined by the decoder layers in a residual manner. Non-uniform weighting functions are applied to allow for finer localization.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/fdr-1.jpg&#34; alt=&#34;Fine-grained Distribution Refinement Process&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Overview of GO-LSD process. Localization knowledge from the final layer’s refined distributions is distilled into earlier layers through DDF loss with decoupled weighting strategies.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/go_lsd-1.jpg&#34; alt=&#34;GO-LSD Process&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; Distributions &lt;/summary&gt; &#xA; &lt;p&gt;Visualizations of FDR across detection scenarios with initial and refined bounding boxes, along with unweighted and weighted distributions.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/merged_image.jpg&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Hard Cases &lt;/summary&gt; &#xA; &lt;p&gt;The following visualization demonstrates D-FINE&#39;s predictions in various complex detection scenarios. These include cases with occlusion, low-light conditions, motion blur, depth of field effects, and densely populated scenes. Despite these challenges, D-FINE consistently produces accurate localization results.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/hard_case-1.jpg&#34; alt=&#34;D-FINE Predictions in Challenging Scenarios&#34; width=&#34;1000&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- &lt;div style=&#34;display: flex; flex-wrap: wrap; justify-content: center; margin: 0; padding: 0;&#34;&gt;&#xA;    &lt;img src=&#34;https://raw.githubusercontent.com/Peterande/storage/master/figs/merged_image.jpg&#34; style=&#34;width:99.96%; margin: 0; padding: 0;&#34; /&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;table&gt;&lt;tr&gt;&#xA;&lt;td&gt;&lt;img src=https://raw.githubusercontent.com/Peterande/storage/master/figs/merged_image.jpg border=0 width=1000&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&lt;/table&gt; --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use &lt;code&gt;D-FINE&lt;/code&gt; or its methods in your work, please cite the following BibTeX entries:&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt; bibtex &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;@misc{peng2024dfine,&#xA;      title={D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement},&#xA;      author={Yansong Peng and Hebei Li and Peixi Wu and Yueyi Zhang and Xiaoyan Sun and Feng Wu},&#xA;      year={2024},&#xA;      eprint={2410.13842},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our work is built upon &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt;RT-DETR&lt;/a&gt;. Thanks to the inspirations from &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt;RT-DETR&lt;/a&gt;, &lt;a href=&#34;https://github.com/implus/GFocal&#34;&gt;GFocal&lt;/a&gt;, &lt;a href=&#34;https://github.com/HikariTJU/LD&#34;&gt;LD&lt;/a&gt;, and &lt;a href=&#34;https://github.com/WongKinYiu/yolov9&#34;&gt;YOLOv9&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;✨ Feel free to contribute and reach out if you have any questions! ✨&lt;/p&gt;</summary>
  </entry>
</feed>