<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-21T01:35:21Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>liuyuan-pal/SyncDreamer</title>
    <updated>2023-09-21T01:35:21Z</updated>
    <id>tag:github.com,2023-09-21:/liuyuan-pal/SyncDreamer</id>
    <link href="https://github.com/liuyuan-pal/SyncDreamer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[Arxiv23] SyncDreamer: Generating Multiview-consistent Images from a Single-view Image&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SyncDreamer&lt;/h1&gt; &#xA;&lt;p&gt;SyncDreamer: Generating Multiview-consistent Images from a Single-view Image&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liuyuan-pal/SyncDreamer/main/assets/teaser.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://liuyuan-pal.github.io/SyncDreamer/&#34;&gt;Project page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2309.03453&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/liuyuan-pal/SyncDreamer&#34;&gt;Live Demo&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference codes and pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;News&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023-09-12: Training codes are released. We are still uploading the training data (about 1.6T) to onedrive, which would cost some time.&lt;/li&gt; &#xA; &lt;li&gt;2023-09-09: Inference codes and pretrained models are released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Preparation for inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install packages in &lt;code&gt;requirements.txt&lt;/code&gt;. We test our model on a 40G A100 GPU with 11.1 CUDA and 1.10.2 pytorch. But inference on GPUs with smaller memory (=10G) is possible.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;conda create -n syncdreamer&#xA;conda activate syncdreamer&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Download checkpoints &lt;a href=&#34;https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EjYHbCBnV-VPjBqNHdNulIABq9sYAEpSz4NPLDI72a85vw&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A docker env can be found at &lt;a href=&#34;https://hub.docker.com/repository/docker/liuyuanpal/syncdreamer-env/general&#34;&gt;https://hub.docker.com/repository/docker/liuyuanpal/syncdreamer-env/general&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you have the following models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SyncDreamer&#xA;|-- ckpt&#xA;    |-- ViT-L-14.ckpt&#xA;    |-- syncdreamer-pretrain.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;(Optional) Predict foreground mask as the alpha channel. We use &lt;a href=&#34;https://apps.microsoft.com/store/detail/paint-3d/9NBLGGH5FV99&#34;&gt;Paint3D&lt;/a&gt; to segment the foreground object interactively. We also provide a script &lt;code&gt;foreground_segment.py&lt;/code&gt; using &lt;code&gt;carvekit&lt;/code&gt; to predict foreground masks and you need to first crop the object region before feeding it to &lt;code&gt;foreground_segment.py&lt;/code&gt;. We may double check the predicted masks are correct or not.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python foreground_segment.py --input &amp;lt;image-file-to-input&amp;gt; --output &amp;lt;image-file-in-png-format-to-output&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run SyncDreamer to produce multiview-consistent images.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --ckpt ckpt/syncdreamer-pretrain.ckpt \&#xA;                   --input testset/aircraft.png \&#xA;                   --output output/aircraft \&#xA;                   --sample_num 4 \&#xA;                   --cfg_scale 2.0 \&#xA;                   --elevation 30 \&#xA;                   --crop_size 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Explanation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--ckpt&lt;/code&gt; is the checkpoint to load.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--input&lt;/code&gt; is the input image in the RGBA form. The alpha value means the foreground object mask.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--output&lt;/code&gt; is the output directory. Results would be saved to &lt;code&gt;output/aircraft/0.png&lt;/code&gt; which contains 16 images of predefined viewpoints per &lt;code&gt;png&lt;/code&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sample_num&lt;/code&gt; is the number of instances we will generate. &lt;code&gt;--sample_num 4&lt;/code&gt; means we sample 4 instances from &lt;code&gt;output/aircraft/0.png&lt;/code&gt; to &lt;code&gt;output/aircraft/3.png&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--cfg_scale&lt;/code&gt; is the &lt;em&gt;classifier-free-guidance&lt;/em&gt;. &lt;code&gt;2.0&lt;/code&gt; is OK for most cases. We may also try &lt;code&gt;1.5&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--elevation&lt;/code&gt; is the elevation angle of the input image in degree. As shown in the following figure, &lt;img src=&#34;https://raw.githubusercontent.com/liuyuan-pal/SyncDreamer/main/assets/elevation.jpg&#34; alt=&#34;elevation&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;We assume the object is locating at the origin and the input image is captured by a camera with an elevation angle. Note we don&#39;t need a very accurate elevation angle but a rough value in [-10,40] degree is OK, e.g. {0,10,20,30}.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--crop_size&lt;/code&gt; affects how we resize the object on the input image. The input image will be resize to 256*256 and the object region is resized to &lt;code&gt;crop_size&lt;/code&gt; as follows. &lt;code&gt;crop_size=-1&lt;/code&gt; means we do not resize the object but only directly resize the input image to 256*256. &lt;code&gt;crop_size=200&lt;/code&gt; works in most cases. We may also try &lt;code&gt;180&lt;/code&gt; or &lt;code&gt;150&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/liuyuan-pal/SyncDreamer/main/assets/crop_size.jpg&#34; alt=&#34;crop_size&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Suggestion&lt;/strong&gt;: We may try different &lt;code&gt;crop_size&lt;/code&gt; and &lt;code&gt;elevation&lt;/code&gt; to get the best result. SyncDreamer does not always produce good results but we may generate multiple times with different &lt;code&gt;--seed&lt;/code&gt; and select the most reasonable one.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Limited GPU memory&lt;/strong&gt;: For users with limited GPU memory, we may try &lt;code&gt;--sample_num 1&lt;/code&gt; and &lt;code&gt;--batch_view_num 4&lt;/code&gt;, which samples 1 instance and denoises 4 images on every step. This costs less than 10G GPU memory but is much slower in generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/liuyuan-pal/SyncDreamer/main/testset_parameters.sh&#34;&gt;testset_parameters.sh&lt;/a&gt; contains the command I used to generate results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Run a NeuS or a NeRF for 3D reconstruction.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# train a neus&#xA;python train_renderer.py -i output/aircraft/0.png \&#xA;                         -n aircraft-neus \&#xA;                         -b configs/neus.yaml \&#xA;                         -l output/renderer &#xA;# train a nerf&#xA;python train_renderer.py -i output/aircraft/0.png \&#xA;                         -n aircraft-nerf \&#xA;                         -b configs/nerf.yaml \&#xA;                         -l output/renderer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Explanation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-i&lt;/code&gt; contains the multiview images generated by SyncDreamer. Since SyncDreamer does not always produce good results, we may need to select a good generated image set (from &lt;code&gt;0.png&lt;/code&gt; to &lt;code&gt;3.png&lt;/code&gt;) for reconstruction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-n&lt;/code&gt; means the name. &lt;code&gt;-l&lt;/code&gt; means the log dir. Results will be saved to &lt;code&gt;&amp;lt;log_dir&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt; i.e. &lt;code&gt;output/renderer/aircraft-neus&lt;/code&gt; and &lt;code&gt;output/renderer/aircraft-nerf&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Before training, we will run &lt;code&gt;carvekit&lt;/code&gt; to find the foreground mask in &lt;code&gt;_init_dataset()&lt;/code&gt; in &lt;code&gt;renderer/renderer.py&lt;/code&gt;. The resulted masked images locate at &lt;code&gt;output/renderer/aircraft-nerf/masked-*.png&lt;/code&gt;. Sometimes, &lt;code&gt;carvekit&lt;/code&gt; may produce incorrect masks.&lt;/li&gt; &#xA; &lt;li&gt;A rendering video will be saved at &lt;code&gt;output/renderer/aircraft-neus/rendering.mp4&lt;/code&gt; or &lt;code&gt;output/renderer/aircraft-nerf/rendering.mp4&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We will only save a mesh for NeuS but not for NeRF, which is &lt;code&gt;output/renderer/aircraft-neus/mesh.ply&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Preparation for training&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate renderings for training. We provide several objaverse 3D models as examples &lt;a href=&#34;https://connecthkuhk-my.sharepoint.com/:u:/g/personal/yuanly_connect_hku_hk/EQjz-dQRY4VLvIm8JTvQzi8BL58gatT6ewLJa54iVhsOZg?e=6TF0Vs&#34;&gt;here&lt;/a&gt;. The whole objaverse dataset can be downloaded at &lt;a href=&#34;https://objaverse.allenai.org/&#34;&gt;Objaverse&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# generate renderings for fixed target views&#xA;blender --background --python blender_script.py -- \&#xA;  --object_path objaverse_examples/6f99fb8c2f1a4252b986ed5a765e1db9/6f99fb8c2f1a4252b986ed5a765e1db9.glb \&#xA;  --output_dir ./training_examples/target --camera_type fixed&#xA;  &#xA;# generate renderings for random input views&#xA;blender --background --python blender_script.py -- \&#xA;  --object_path objaverse_examples/6f99fb8c2f1a4252b986ed5a765e1db9/6f99fb8c2f1a4252b986ed5a765e1db9.glb \&#xA;  --output_dir ./training_examples/input --camera_type random&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Organize the renderings like the following. We provide rendering examples &lt;a href=&#34;https://connecthkuhk-my.sharepoint.com/:u:/g/personal/yuanly_connect_hku_hk/EZEq7wDSR85IriRhO3bkW8wBNE9UtqH3lQ86dyAFdZqCRg?e=aRYDr9&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SyncDreamer&#xA;|-- training_examples&#xA;    |-- target&#xA;        |-- &amp;lt;renderings-of-uid-0&amp;gt;&#xA;        |-- &amp;lt;renderings-of-uid-1&amp;gt;&#xA;        |-- ...&#xA;    |-- input&#xA;        |-- &amp;lt;renderings-of-uid-0&amp;gt;&#xA;        |-- &amp;lt;renderings-of-uid-1&amp;gt;&#xA;        |-- ...&#xA;    |-- uid_set.pkl # this is a .pkl file containing a list of uids. Refer to `render_batch.py` for how I generate these files.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Download the pretrained zero123-xl model &lt;a href=&#34;https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_syncdreamer.py -b configs/syncdreamer-train.yaml \&#xA;                           --finetune_from &amp;lt;path-to-your-zero123-xl-model&amp;gt; \&#xA;                           -l &amp;lt;logging-directory&amp;gt;  \&#xA;                           -c &amp;lt;checkpoint-directory&amp;gt; \&#xA;                           --gpus 0,1,2,3,4,5,6,7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note in &lt;code&gt;configs/syncdreamer-train.yaml&lt;/code&gt;, we specify the following directories which contain the training data and the validation data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;target_dir: training_examples/target&#xA;input_dir: training_examples/input&#xA;uid_set_pkl: training_examples/uid_set.pkl&#xA;validation_dir: validation_set&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During training, we will run validation to output images to &lt;code&gt;&amp;lt;log_dir&amp;gt;/&amp;lt;images&amp;gt;/val&lt;/code&gt; every 1k steps.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We have intensively borrow codes from the following repositories. Many thanks to the authors for sharing their codes.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/threestudio-project/threestudio&#34;&gt;threestudio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;stable diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;zero123&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colmap.github.io/&#34;&gt;COLMAP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Totoro97/NeuS&#34;&gt;NeuS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/guochengqian/Magic123&#34;&gt;Magic123&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lukemelas/realfusion&#34;&gt;RealFusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/One-2-3-45/One-2-3-45&#34;&gt;One-2-3-45&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful in your project, please cite the following work. :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{liu2023syncdreamer,&#xA;  title={SyncDreamer: Learning to Generate Multiview-consistent Images from a Single-view Image},&#xA;  author={Liu, Yuan and Lin, Cheng and Zeng, Zijiao and Long, Xiaoxiao and Liu, Lingjie and Komura, Taku and Wang, Wenping},&#xA;  journal={arXiv preprint arXiv:2309.03453},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>mit-han-lab/efficientvit</title>
    <updated>2023-09-21T01:35:21Z</updated>
    <id>tag:github.com,2023-09-21:/mit-han-lab/efficientvit</id>
    <link href="https://github.com/mit-han-lab/efficientvit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EfficientViT is a new family of vision models for efficient high-resolution vision.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction&lt;/h1&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you are interested in getting updates, please join our mailing list &lt;a href=&#34;https://forms.gle/Z6DNkRidJ1ouxmUk9&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/09/18] The first version of the &lt;strong&gt;EfficientViT Segment Anything Model&lt;/strong&gt; has been released.&lt;/li&gt; &#xA; &lt;li&gt;[2023/09/12] EfficientViT is highlighted by &lt;a href=&#34;https://www.mit.edu/archive/spotlight/efficient-computer-vision/&#34;&gt;MIT home page&lt;/a&gt; and &lt;a href=&#34;https://news.mit.edu/2023/ai-model-high-resolution-computer-vision-0912&#34;&gt;MIT News&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/07/18] EfficientViT is accepted by ICCV 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;EfficientViT-L0 for Segment Anything (1009 image/s on A100 GPU) &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/demo/sam_l0_box.jpg&#34; alt=&#34;demo&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/demo/sam_l0_point.jpg&#34; alt=&#34;demo&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/demo/sam_l0_all.jpg&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;EfficientViT-L1 for Semantic Segmentation (45.9ms on Nvidia Jetson AGX Orin, 82.7 mIoU on Cityscapes)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/demo/cityscapes_l1.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About EfficientViT Models&lt;/h2&gt; &#xA;&lt;p&gt;EfficientViT is a new family of vision models for efficient high-resolution dense prediction. The core building block of EfficientViT is a new lightweight multi-scale linear attention module that achieves global receptive field and multi-scale learning with only hardware-efficient operations.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n efficientvit python=3.10&#xA;conda activate efficientvit&#xA;conda install -c conda-forge mpi4py openmpi&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;ImageNet: https://www.image-net.org/&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Our code expects the ImageNet dataset directory to follow the following structure:&#xA;&#xA;imagenet&#xA;├── train&#xA;├── val&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Cityscapes: https://www.cityscapes-dataset.com/&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Our code expects the Cityscapes dataset directory to follow the following structure:&#xA;&#xA;cityscapes&#xA;├── gtFine&#xA;|   ├── train&#xA;|   ├── val&#xA;├── leftImg8bit&#xA;|   ├── train&#xA;|   ├── val&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;ADE20K: https://groups.csail.mit.edu/vision/datasets/ADE20K/&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Our code expects the ADE20K dataset directory to follow the following structure:&#xA;&#xA;ade20k&#xA;├── annotations&#xA;|   ├── training&#xA;|   ├── validation&#xA;├── images&#xA;|   ├── training&#xA;|   ├── validation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;Latency/Throughput is measured on NVIDIA Jetson Nano, NVIDIA Jetson AGX Orin, and NVIDIA A100 GPU with TensorRT, fp16. Data transfer time is included.&lt;/p&gt; &#xA;&lt;h3&gt;Segment Anything&lt;/h3&gt; &#xA;&lt;p&gt;In this version, the EfficientViT segment anything models are trained using the image embedding extracted by &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM ViT-H&lt;/a&gt; as the target. The prompt encoder and mask decoder are the same as &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM ViT-H&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Image Encoder&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;COCO-val2017 mIoU (all)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;COCO-val2017 mIoU (large)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;COCO-val2017 mIoU (medium)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;COCO-val2017 mIoU (small)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MACs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A100 Throughput&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NanoSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;744 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobileSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;297 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1009 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/jvemt62abv6tfpn33luy2/l0.pt?rlkey=ijo7i64n6kpnhnrwd3uux8vhf&amp;amp;dl=0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;815 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/7j968vmeyx1oiojubkqbm/l1.pt?rlkey=ogpprp1et1zgwucsy8731vv0h&amp;amp;dl=0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ImageNet&lt;/h3&gt; &#xA;&lt;p&gt;All EfficientViT classification models are trained on ImageNet-1K with random initialization (300 epochs + 20 warmup epochs) using supervised learning.&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/cls_results.png&#34; width=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ImageNet Top1 Acc&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ImageNet Top5 Acc&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MACs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A100 Throughput&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientNetV2-S&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.8G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2869 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientNetV2-M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;480x480&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1160 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;96.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.3G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6207 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1q5y0YbN08O4ToUBK8RfZSDKp-s1y5_44/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.9G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4998 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1FEjImtyIQhG4VsHsstLgNM09Y9qJn9Sk/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3969 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1pvYtY0ckAAMTkRq6TbwpQ0U1p_urz2fE/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;288x288&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3102 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GDr0y45YPX8iWEWNq5fEmjo0UgyZLpUs/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;320x320&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2525 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GDr0y45YPX8iWEWNq5fEmjo0UgyZLpUs/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;352x352&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2099 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1gEkrj2JScJEcUgxeBSVKpUYBbple99yI/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;384x384&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1784 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1MpjduiCTbUVS1XJri4_eqCbARJyYo74b/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;EfficientViT B series&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;ImageNet Top1 Acc&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;ImageNet Top5 Acc&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;MACs&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Jetson Nano (bs1)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Jetson Orin (bs1)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;79.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;94.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.1M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.52G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24.8ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.48ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1hKN_hvLG4nmRzbfzKY7GlqwpR5uKpOOk/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;79.9&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;94.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.1M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.68G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;28.5ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.57ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1hXcG_jB0ODMOESsSkzVye-58B4F3Cahs/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;288x288&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;80.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;95.0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.1M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.86G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;34.5ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.82ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1sE_Suz9gOOUO7o5r9eeAT4nKK8Hrbhsu/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;82.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;95.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1.6G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;50.6ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2.63ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1DiM-iqVGTrq4te8mefHl3e1c12u4qR7d/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;82.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;96.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2.1G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;58.5ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2.84ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/192OOk4ISitwlyW979M-FSJ_fYMMW9HQz/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;288x288&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;83.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;96.3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;2.6G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;69.9ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3.30ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1aodcepOyne667hvBAGpf9nDwmd5g0NpU/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;224x224&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;83.5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;96.4&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;49M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.0G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;101ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.36ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/18RZDGLiY8KsyJ7LGic4mg1JHwd-a_ky6/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;256x256&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;96.5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;49M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5.2G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;120ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.74ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1y1rnir4I0XiId-oTCcHhs7jqnrHGFi-g/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;288x288&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;84.2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;96.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;49M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;6.5G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;141ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;5.63ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1KfwbGtlyFgslNr4LIHERv6aCfkItEvRk/view?usp=share_link&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Cityscapes&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/city_results.png&#34; width=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cityscapes mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MACs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Jetson Orin Latency (bs1)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A100 Throughput (bs1)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024x2048&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;282G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.9ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;122 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/7s1ou7bdsbwwvq9bmjjuv/l1.pt?rlkey=m1ysvjkhrb0pb7uuyoir92p77&amp;amp;dl=0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024x2048&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;396G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.0ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;102 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/erehtq6j1daushirhbzc5/l2.pt?rlkey=068makdxpdh469ueps2trhte1&amp;amp;dl=0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;EfficientViT B series&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Cityscapes mIoU&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;MACs&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Jetson Nano (bs1)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Jetson Orin (bs1)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1024x2048&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;75.7&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;0.7M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.4G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;275ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.9ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Ix1Dh3xlpaf0Wzh01Xmo-hAYkoXt1EAD/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1024x2048&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;80.5&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.8M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;25G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;819ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;24.3ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1jNjLFtIUNvu5MwSupgFHLc-2kmFLiu67/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1024x2048&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;82.1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;74G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1676ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;46.5ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1bwGjzVQOg_ygML8F9JhsIj-ntn-cuWmB/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;1024x2048&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;83.0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;40M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;179G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3192ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;81.8ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/19aiy3qrKqx1n8zzy_ewYn4-Z3LM4bkn4/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;ADE20K&lt;/h3&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/ade_results.png&#34; width=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ADE20K mIoU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MACs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Jetson Orin Latency (bs1)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;A100 Throughput (bs16)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.2ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;947 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/kn4g04fgme9uctaazsgct/l1.pt?rlkey=x1052if1ae7795yafp3urib5r&amp;amp;dl=0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EfficientViT-L2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.0ms&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;758 image/s&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/565wb47z1f5re9jckr42t/l2.pt?rlkey=ojffxngf6iv0oiost6c2tskul&amp;amp;dl=0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;EfficientViT B series&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Resolution&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;ADE20K mIoU&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Params&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;MACs&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Jetson Nano (bs1)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Jetson Orin (bs1)&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B1&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.8M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;3.1G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;110ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;4.0ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13YNtEJ-mRnAhu0fIs2EnAP-3TmSneRAC/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B2&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;45.9&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;15M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;9.1G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;212ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;7.3ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1k5sWY6aJ1FCtMt4GRTZqSFlJ-u_TSHzc/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;EfficientViT-B3&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;512x512&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;49.0&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;39M&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;22G&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;411ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;12.5ms&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1ghpTf9GTTj_8mn5QJh-7cLK1_wL3pKWr/view?usp=sharing&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# segment anything&#xA;from efficientvit.sam_model_zoo import create_sam_model&#xA;&#xA;efficientvit_sam = create_sam_model(&#xA;  name=&#34;l1&#34;, weight_url=&#34;assets/checkpoints/sam/l1.pt&#34;,&#xA;)&#xA;efficientvit_sam = efficientvit_sam.cuda().eval()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor&#xA;&#xA;efficientvit_sam_predictor = EfficientViTSamPredictor(efficientvit_sam)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from efficientvit.models.efficientvit.sam import EfficientViTSamAutomaticMaskGenerator&#xA;&#xA;efficientvit_mask_generator = EfficientViTSamAutomaticMaskGenerator(efficientvit_sam)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# classification&#xA;from efficientvit.cls_model_zoo import create_cls_model&#xA;&#xA;model = create_cls_model(&#xA;  name=&#34;l2&#34;, weight_url=&#34;assets/checkpoints/cls/l2-r384.pt&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# semantic segmentation&#xA;from efficientvit.seg_model_zoo import create_seg_model&#xA;&#xA;model = create_seg_model(&#xA;  name=&#34;l2&#34;, dataset=&#34;cityscapes&#34;, weight_url=&#34;assets/checkpoints/seg/cityscapes/l2.pt&#34;&#xA;)&#xA;&#xA;model = create_seg_model(&#xA;  name=&#34;l2&#34;, dataset=&#34;ade20k&#34;, weight_url=&#34;assets/checkpoints/seg/ade20k/l2.pt&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Please run &lt;code&gt;eval_sam_coco.py&lt;/code&gt;, &lt;code&gt;eval_cls_model.py&lt;/code&gt; or &lt;code&gt;eval_seg_model.py&lt;/code&gt; to evaluate our models.&lt;/p&gt; &#xA;&lt;p&gt;Examples: &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/eval_sam_model.sh&#34;&gt;segment anything&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/eval_cls_model.sh&#34;&gt;classification&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/assets/files/eval_seg_model.sh&#34;&gt;segmentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Visualization&lt;/h2&gt; &#xA;&lt;p&gt;Please run &lt;code&gt;demo_sam_model.py&lt;/code&gt; to visualize our segment anything models.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# segment everything&#xA;python demo_sam_model.py --model l1 --mode all&#xA;&#xA;# prompt with points&#xA;python demo_sam_model.py --model l1 --mode point&#xA;&#xA;# prompt with box&#xA;python demo_sam_model.py --model l1 --mode box --box &#34;[150,70,630,400]&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please run &lt;code&gt;eval_seg_model.py&lt;/code&gt; to visualize the outputs of our semantic segmentation models.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval_seg_model.py --dataset cityscapes --crop_size 1024 --model b3 --save_path demo/cityscapes/b3/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmarking with TFLite&lt;/h2&gt; &#xA;&lt;p&gt;To generate TFLite files, please refer to &lt;code&gt;tflite_export.py&lt;/code&gt;. It requires the TinyNN package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/alibaba/TinyNeuralNetwork.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tflite_export.py --export_path model.tflite --task seg --dataset ade20k --model b3 --resolution 512 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmarking with TensorRT&lt;/h2&gt; &#xA;&lt;p&gt;To generate onnx files, please refer to &lt;code&gt;onnx_export.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/mit-han-lab/efficientvit/master/TRAINING.md&#34;&gt;TRAINING.md&lt;/a&gt; for detailed training instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Han Cai: &lt;a href=&#34;mailto:hancai@mit.edu&#34;&gt;hancai@mit.edu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ImageNet Pretrained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Segmentation Pretrained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ImageNet training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; EfficientViT L series, designed for cloud&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; EfficientViT for segment anything&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; EfficientViT for super-resolution&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Segmentation training code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If EfficientViT is useful or relevant to your research, please kindly recognize our contributions by citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{cai2022efficientvit,&#xA;  title={Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition},&#xA;  author={Cai, Han and Gan, Chuang and Han, Song},&#xA;  journal={arXiv preprint arXiv:2205.14756},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>