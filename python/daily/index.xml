<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-30T01:39:57Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>IEIT-Yuan/Yuan-2.0</title>
    <updated>2023-11-30T01:39:57Z</updated>
    <id>tag:github.com,2023-11-30:/IEIT-Yuan/Yuan-2.0</id>
    <link href="https://github.com/IEIT-Yuan/Yuan-2.0" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Yuan 2.0 Large Language Model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;源2.0&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/README-EN.md&#34;&gt;Read this in English.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;📔 更为详细的使用信息，可以参考：&lt;a href=&#34;https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf&#34;&gt;源2.0 论文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E6%BA%9020&#34;&gt;源2.0&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E7%9B%AE%E5%BD%95&#34;&gt;目录&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E4%BB%8B%E7%BB%8D&#34;&gt;介绍&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%B1%E8%AE%AD%E8%AE%A1%E5%88%92&#34;&gt;源大模型共训计划&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E5%BF%AB%E9%80%9F%E5%90%AF%E5%8A%A8&#34;&gt;快速启动&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE&#34;&gt;环境配置&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86&#34;&gt;数据预处理&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E9%A2%84%E8%AE%AD%E7%BB%83&#34;&gt;预训练&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&#34;&gt;模型微调&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E6%A8%A1%E5%9E%8B&#34;&gt;模型&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C&#34;&gt;评测结果&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/#%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8&#34;&gt;代码调用&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- markdown-toc end --&gt; &#xA;&lt;h2&gt;介绍&lt;/h2&gt; &#xA;&lt;p&gt;源2.0 是浪潮信息发布的新一代基础语言大模型。我们开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且我们提供了预训练，微调，推理服务的相关脚本，以供研发人员做进一步的开发。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;对本仓库源码的使用遵循开源许可协议 &lt;strong&gt;Apache 2.0&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;源2.0模型支持商用，不需要申请授权，请您了解并遵循&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/LICENSE-Yuan&#34;&gt;《源2.0模型许可协议》&lt;/a&gt;，勿将开源模型和代码及基于开源项目产生的衍生物用于任何可能给国家和社会带来危害的用途以及用于任何未经过安全评估和备案的服务。&lt;/p&gt; &#xA;&lt;p&gt;尽管模型在训练时我们已采取措施尽力确保数据的合规性和准确性，但模型参数量巨大且受概率随机性因素影响，我们无法保证输出内容的准确性，且模型易被输入指令所误导，本项目不承担开源模型和代码导致的数据安全、舆情风险或发生任何模型被误导、滥用、传播、不当利用而产生的风险和责任。&lt;strong&gt;您将对通过使用、复制、分发和修改模型等方式利用该开源项目所产生的风险与后果，独自承担全部责任。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;源大模型共训计划&lt;/h2&gt; &#xA;&lt;p&gt;我们希望开源的模型更符合开发者应用需求，为此我们推出源大模型共训计划，开发者提出自己的应用或场景的需求，由我们来准备训练数据并对源大模型进行增强训练，训练后的模型依然在社区开源。&lt;/p&gt; &#xA;&lt;p&gt;每月六日我们会收集前一月开发者提出的具体需求，经过评审后列入当月模型训练计划，训练完成后的模型在当月月末就会更新到开源社区。开发者只需要提出需求，由我们来进行数据准备、模型训练并开源。请开发者在issue的“源大模型共训计划”问题下提出具体需求，提出需求的具体格式无要求，只需要说清楚具体的应用场景、对大模型的能力需求以及给出输入输出的说明。&lt;/p&gt; &#xA;&lt;p&gt;以下是提出需求的一些示例（几条示例，能够反应场景的典型特性即可）：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;场景需求：能够基于业务场景生成相关内容，对场景的描述。 &amp;nbsp;输入：用户问题，输出：正确的答案。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;场景需求：我想让大模型能够阅读一个领域下的多篇论文，给出这些论文的综述，当前领域研究的热点以及未解决的问题，从而辅助学术研究。 输入为：一个领域下的多篇论文，输出为：综述研究报告，研究热点总结，未解决问题总结。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;快速启动&lt;/h2&gt; &#xA;&lt;h3&gt;环境配置&lt;/h3&gt; &#xA;&lt;p&gt;我们建议使用有我们提供的最新的docker&lt;a href=&#34;https://pan.baidu.com/s/1IKjYqlf2kAPQzGsA6EdMCA?pwd=hopd&#34;&gt;镜像文件&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;我们可以通过下面命令启动容器：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker load &amp;lt; ./yuan_v2.0.tar&#xA;docker run --gpus all -it -v /path/to/yuan_2.0:/workspace/yuan_2.0 -v /path/to/dataset:/workspace/dataset -v /path/to/checkpoints:/workspace/checkpoints yuan_v2.0:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;数据预处理&lt;/h3&gt; &#xA;&lt;p&gt;我们提供了数据预处理的脚本，参考&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/data_process.md&#34;&gt;数据预处理说明文档&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;预训练&lt;/h3&gt; &#xA;&lt;p&gt;我们提供了用于预训练的文档和 &lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/examples&#34;&gt;&lt;code&gt;example&lt;/code&gt;&lt;/a&gt;的脚本，具体使用方法可以参考&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/pretrain.md&#34;&gt;预训练说明文档&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;模型微调&lt;/h3&gt; &#xA;&lt;p&gt;请参考指令微调 &lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/instruct_tuning.md&#34;&gt;源2.0 指令微调示例&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;请注意，不同的微调脚本对应的模型并不相同，请根据需要选择对应的模型。&lt;/p&gt; &#xA;&lt;h3&gt;模型&lt;/h3&gt; &#xA;&lt;p&gt;我们提供了源2.0有监督微调的模型文件。模型文件可以通过下面的链接下载得到：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;序列长度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;源2.0-102B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1Tb9W6hEWS4bMkaE3p5s1fw?pwd=xrfo&#34;&gt;百度网盘&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;源2.0-51B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1bOypWMepdh9GFK_hHXVQbQ?pwd=1uw3&#34;&gt;百度网盘&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;源2.0-2B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://pan.baidu.com/s/1Xj8Mi2tPwuuVu7Cb0tCbtw?pwd=qxpa&#34;&gt;百度网盘&lt;/a&gt; | &lt;a href=&#34;https://www.modelscope.cn/models/YuanLLM/Yuan2.0-2B/files&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;源2.0-2B模型支持的序列长度为8192个tokens，源2.0-51B和源2.0-102B模型支持的序列长度为4096个tokens，可以根据用户设备的内存大小设置 &lt;code&gt;--max-position-embeddings&lt;/code&gt; 和 &lt;code&gt;--seq-length&lt;/code&gt; 的值。&lt;/p&gt; &#xA;&lt;h2&gt;评测结果&lt;/h2&gt; &#xA;&lt;p&gt;我们提供了&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/eval_humaneval.md&#34;&gt;HumanEval&lt;/a&gt;，&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/eval_agieval_math.md&#34;&gt;AGIEval-GK-Math&lt;/a&gt;，&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/eval_gsm8k.md&#34;&gt;GSM8K&lt;/a&gt;和&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/eval_TruthfulQA.md&#34;&gt;TruthfulQA&lt;/a&gt;的评估脚本。在4个典型任务上，我们用源2.0不同版本模型上进行了性能测试。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AGIEval-GK-Math-QA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AGIEval-GK-Math-Cloze&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th&gt;TurthfulQA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.6%&lt;/td&gt; &#xA;   &lt;td&gt;59%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.6%*&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.5%*&lt;/td&gt; &#xA;   &lt;td&gt;34%*&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.8%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.9%&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;源2.0-102B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.6%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.7%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.1%&lt;/td&gt; &#xA;   &lt;td&gt;58%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;源2.0-102B-SC&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.2%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.2%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.4%&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;* 使用与源2.0完全相同的输入数据对ChatGPT进行测试，时间2023年11月&lt;/p&gt; &#xA;&lt;h2&gt;代码调用&lt;/h2&gt; &#xA;&lt;p&gt;考虑到推理服务的效率，源2.0-51B和源2.0-102B模型在启动推理服务之前，需要将模型转换成只有张量并行的模型文件。可以参考&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/checkpoint_process.md&#34;&gt;文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;可以通过调用推理服务，向推理服务发送请求实现模型的调用，&lt;a href=&#34;https://raw.githubusercontent.com/IEIT-Yuan/Yuan-2.0/main/docs/inference_server.md&#34;&gt;源2.0 推理服务&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OthersideAI/self-operating-computer</title>
    <updated>2023-11-30T01:39:57Z</updated>
    <id>tag:github.com,2023-11-30:/OthersideAI/self-operating-computer</id>
    <link href="https://github.com/OthersideAI/self-operating-computer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Self-Operating Computer Framework&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;A framework to enable multimodal models to operate a computer.&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Using the same inputs and outputs of a human operator, the model views the screen and decides on a series of mouse and keyboard actions to reach an objective. &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/OthersideAI/self-operating-computer/raw/main/readme/self-operating-computer.png&#34; width=&#34;750&#34; style=&#34;margin: 10px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compatibility&lt;/strong&gt;: Designed for various multimodal models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integration&lt;/strong&gt;: Currently integrated with &lt;strong&gt;GPT-4v&lt;/strong&gt; as the default model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Future Plans&lt;/strong&gt;: Support for additional models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Current Challenges&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; GPT-4V&#39;s error rate in estimating XY mouse click locations is currently quite high. This framework aims to track the progress of multimodal models over time, aspiring to achieve human-level performance in computer operation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Ongoing Development&lt;/h3&gt; &#xA;&lt;p&gt;At &lt;a href=&#34;https://www.hyperwriteai.com/&#34;&gt;HyperwriteAI&lt;/a&gt;, we are developing a multimodal model with more accurate click location predictions.&lt;/p&gt; &#xA;&lt;h3&gt;Additional Thoughts&lt;/h3&gt; &#xA;&lt;p&gt;We recognize that some operating system functions may be more efficiently executed with hotkeys such as entering the Browser Address bar using &lt;code&gt;command + L&lt;/code&gt; rather than by simulating a mouse click at the correct XY location. We plan to make these improvements over time. However, it&#39;s important to note that many actions require the accurate selection of visual elements on the screen, necessitating precise XY mouse click locations. A primary focus of this project is to refine the accuracy of determining these click locations. We believe this is essential for achieving a fully self-operating computer in the current technological landscape.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OthersideAI/self-operating-computer/assets/42594239/9e8abc96-c76a-46fb-9b13-03678b3c67e0&#34;&gt;https://github.com/OthersideAI/self-operating-computer/assets/42594239/9e8abc96-c76a-46fb-9b13-03678b3c67e0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Below are instructions to set up the Self-Operating Computer Framework locally on your computer.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clone the repo&lt;/strong&gt; to a directory on your computer:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/OthersideAI/self-operating-computer.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cd into directory&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd self-operating-computer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create a Python virtual environment&lt;/strong&gt;. &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;Learn more about Python virtual environment&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Activate the virtual environment&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install the project requirements&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install Project and Command-Line Interface&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Then rename the &lt;code&gt;.example.env&lt;/code&gt; file to &lt;code&gt;.env&lt;/code&gt; so that you can save your OpenAI key in it.&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;mv .example.env .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Add your Open AI key to your new &lt;code&gt;.env&lt;/code&gt; file. If you don&#39;t have one, you can obtain an OpenAI key &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#39;your-key-here&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run it&lt;/strong&gt;!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;operate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Final Step&lt;/strong&gt;: As a last step, the Terminal app will ask for permission for &#34;Screen Recording&#34; and &#34;Accessibility&#34; in the &#34;Security &amp;amp; Privacy&#34; page of Mac&#39;s &#34;System Preferences&#34;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/OthersideAI/self-operating-computer/raw/main/readme/terminal-access-1.png&#34; width=&#34;300&#34; style=&#34;margin: 10px;&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/OthersideAI/self-operating-computer/raw/main/readme/terminal-access-2.png&#34; width=&#34;300&#34; style=&#34;margin: 10px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Contributions are Welcomed! Some Ideas:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Improvements&lt;/strong&gt;: Noticed any areas for prompt improvements? Feel free to make suggestions or submit a pull request (PR).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Enabling New Mouse Capabilities&lt;/strong&gt; (drag, hover, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Adding New Multimodal Models&lt;/strong&gt;: Integration of new multimodal models is welcomed. If you have a specific model in mind that you believe would be a valuable addition, please feel free to integrate it and submit a PR.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Framework Architecture Improvements&lt;/strong&gt;: Think you can enhance the framework architecture described in the intro? We welcome suggestions and PRs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For any input on improving this project, feel free to reach out to me on &lt;a href=&#34;https://twitter.com/josh_bickett&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Follow HyperWriteAI for More Updates&lt;/h3&gt; &#xA;&lt;p&gt;Stay updated with the latest developments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow HyperWriteAI on &lt;a href=&#34;https://twitter.com/HyperWriteAI&#34;&gt;Twitter&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Follow HyperWriteAI on &lt;a href=&#34;https://www.linkedin.com/company/othersideai/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compatibility&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project is only compatible with MacOS at this time.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/DeepSpeed-MII</title>
    <updated>2023-11-30T01:39:57Z</updated>
    <id>tag:github.com,2023-11-30:/microsoft/DeepSpeed-MII</id>
    <link href="https://github.com/microsoft/DeepSpeed-MII" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed-MII/actions/workflows/formatting.yml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/DeepSpeed-MII/actions/workflows/formatting.yml/badge.svg?sanitize=true&#34; alt=&#34;Formatting&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Microsoft/DeepSpeed/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://badgen.net/badge/license/apache2.0/blue&#34; alt=&#34;License Apache 2.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/deepspeed-mii/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/deepspeed-mii.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest) --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/docs/images/mii-white.svg#gh-light-mode-only&#34; width=&#34;400px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/docs/images/mii-dark.svg#gh-dark-mode-only&#34; width=&#34;400px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/11] &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen&#34;&gt;DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/11] &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/mii/legacy/examples/benchmark/txt2img&#34;&gt;Stable Diffusion Image Generation under 1 second w. DeepSpeed MII&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2022/10] &lt;a href=&#34;https://www.deepspeed.ai/2022/10/10/mii.html&#34;&gt;Announcing DeepSpeed Model Implementations for Inference (MII)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contents&lt;/h1&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#deepspeed-model-implementations-for-inference&#34;&gt;DeepSpeed-MII&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#key-technologies&#34;&gt;Key Technologies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#how-does-mii-work&#34;&gt;How does MII work?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#supported-models&#34;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#getting-started-with-mii&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h1&gt;DeepSpeed Model Implementations for Inference (MII)&lt;/h1&gt; &#xA;&lt;p&gt;Introducing MII, an open-source Python library designed by DeepSpeed to democratize powerful model inference with a focus on high-throughput, low latency, and cost-effectiveness.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MII v0.1 introduces several features such as blocked KV-caching, continuous batching, Dynamic SplitFuse, tensor parallelism, and high-performance CUDA kernels to support fast high throughput text-generation for LLMs such as Llama-2-70B. MII delivers up to 2.3 times higher effective throughput compared to leading systems such as vLLM. For detailed performance results please see our &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen&#34;&gt;DeepSpeed-FastGen blog&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/docs/images/fastgen-hero-light.png#gh-light-mode-only&#34; width=&#34;800px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/docs/images/fastgen-hero-dark.png#gh-dark-mode-only&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We first &lt;a href=&#34;https://www.deepspeed.ai/2022/10/10/mii.html&#34;&gt;announced MII&lt;/a&gt; in 2022, which covers all prior releases up to v0.0.9. In addition to language models, we also support accelerating &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/examples/benchmark/txt2img&#34;&gt;text2image models like Stable Diffusion&lt;/a&gt;. For more details on our previous releases please see our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/mii/legacy/&#34;&gt;legacy APIs&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Key Technologies&lt;/h1&gt; &#xA;&lt;h2&gt;MII for High-Throughput Text Generation&lt;/h2&gt; &#xA;&lt;p&gt;MII provides accelerated text-generation inference through the use of four key technologies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Blocked KV Caching&lt;/li&gt; &#xA; &lt;li&gt;Continuous Batching&lt;/li&gt; &#xA; &lt;li&gt;Dynamic SplitFuse&lt;/li&gt; &#xA; &lt;li&gt;High Performance CUDA Kernels&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a deeper dive into understanding these features please &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen&#34;&gt;refer to our blog&lt;/a&gt; which also includes a detailed performance analysis.&lt;/p&gt; &#xA;&lt;h2&gt;MII Legacy&lt;/h2&gt; &#xA;&lt;p&gt;In the past, MII introduced several &lt;a href=&#34;https://www.deepspeed.ai/2022/10/10/mii.html#inference-optimizations-with-mii&#34;&gt;key performance optimizations&lt;/a&gt; for low-latency serving scenarios:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DeepFusion for Transformers&lt;/li&gt; &#xA; &lt;li&gt;Multi-GPU Inference with Tensor-Slicing&lt;/li&gt; &#xA; &lt;li&gt;ZeRO-Inference for Resource Constrained Systems&lt;/li&gt; &#xA; &lt;li&gt;Compiler Optimizations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How does MII work?&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/docs/images/mii-arch-light.png#gh-light-mode-only&#34; width=&#34;800px&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/docs/images/mii-arch-dark.png#gh-dark-mode-only&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Figure 1: MII architecture, showing how MII automatically optimizes OSS models using DS-Inference before deploying them. DeepSpeed-FastGen optimizations in the figure have been published in &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen&#34;&gt;our blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Under-the-hood MII is powered by &lt;a href=&#34;https://github.com/microsoft/deepspeed&#34;&gt;DeepSpeed-Inference&lt;/a&gt;. Based on the model architecture, model size, batch size, and available hardware resources, MII automatically applies the appropriate set of system optimizations to minimize latency and maximize throughput.&lt;/p&gt; &#xA;&lt;h1&gt;Supported Models&lt;/h1&gt; &#xA;&lt;p&gt;MII currently supports over 13,000 models across three popular model architectures. We plan to add additional models in the near term, if there are specific model architectures you would like supported please &lt;a href=&#34;https://github.com/microsoft/DeepSpeed-MII/issues&#34;&gt;file an issue&lt;/a&gt; and let us know. All current models leverage Hugging Face in our backend to provide both the model weights and the model&#39;s corresponding tokenizer. For our current release we support the following model architectures:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model family&lt;/th&gt; &#xA;   &lt;th&gt;size range&lt;/th&gt; &#xA;   &lt;th&gt;~model count&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?other=llama&#34;&gt;llama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B - 65B&lt;/td&gt; &#xA;   &lt;td&gt;11,000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?other=llama-2&#34;&gt;llama-2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B - 70B&lt;/td&gt; &#xA;   &lt;td&gt;800&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?other=mistral&#34;&gt;mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1,100&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/models?other=opt&#34;&gt;opt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.1B - 66B&lt;/td&gt; &#xA;   &lt;td&gt;900&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;MII Legacy Model Support&lt;/h2&gt; &#xA;&lt;p&gt;MII Legacy APIs support over 50,000 different models including BERT, RoBERTa, Stable Diffusion, and other text-generation models like Bloom, GPT-J, etc. For a full list please see our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/mii/legacy/#supported-models-and-tasks&#34;&gt;legacy supported models table&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Getting Started with MII&lt;/h1&gt; &#xA;&lt;p&gt;DeepSpeed-MII allows users to create non-persistent and persistent deployments for supported models in just a few lines of code.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#non-persistent-pipeline&#34;&gt;Non-Persistent Pipeline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#persistent-deployment&#34;&gt;Persistent Deployment&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The fasest way to get started is with our &lt;a href=&#34;https://pypi.org/project/deepspeed-mii/&#34;&gt;PyPI release of DeepSpeed-MII&lt;/a&gt; which means you can get started within minutes via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install deepspeed-mii&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For ease of use and significant reduction in lengthy compile times that many projects require in this space we distribute a pre-compiled python wheel covering the majority of our custom kernels through a new library called &lt;a href=&#34;https://github.com/microsoft/DeepSpeed-Kernels&#34;&gt;DeepSpeed-Kernels&lt;/a&gt;. We have found this library to be very portable across environments with NVIDIA GPUs with compute capabilities 8.0+ (Ampere+), CUDA 11.6+, and Ubuntu 20+. In most cases you shouldn&#39;t even need to know this library exists as it is a dependency of DeepSpeed-MII and will be installed with it. However, if for whatever reason you need to compile our kernels manually please see our &lt;a href=&#34;https://github.com/microsoft/DeepSpeed-Kernels#source&#34;&gt;advanced installation docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Non-Persistent Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;A non-persistent pipeline is a great way to try DeepSpeed-MII. Non-persistent pipelines are only around for the duration of the python script you are running. The full example for running a non-persistent pipeline deployment is only 4 lines. Give it a try!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mii&#xA;pipe = mii.pipeline(&#34;mistralai/Mistral-7B-v0.1&#34;)&#xA;response = pipe([&#34;DeepSpeed is&#34;, &#34;Seattle is&#34;], max_new_tokens=128)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The returned &lt;code&gt;response&lt;/code&gt; is a list of &lt;code&gt;Response&lt;/code&gt; objects. We can access several details about the generation (e.g., &lt;code&gt;response[0].prompt_length&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;generated_text: str&lt;/code&gt; Text generated by the model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt_length: int&lt;/code&gt; Number of tokens in the original prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;generated_length: int&lt;/code&gt; Number of tokens generated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;finish_reason: str&lt;/code&gt; Reason for stopping generation. &lt;code&gt;stop&lt;/code&gt; indicates the EOS token was generated and &lt;code&gt;length&lt;/code&gt; indicates the generation reached &lt;code&gt;max_new_tokens&lt;/code&gt; or &lt;code&gt;max_length&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tensor parallelism&lt;/h3&gt; &#xA;&lt;p&gt;Taking advantage of multi-GPU systems for greater performance is easy with MII. When run with the &lt;code&gt;deepspeed&lt;/code&gt; launcher, tensor parallelism is automatically controlled by the &lt;code&gt;--num_gpus&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run on a single GPU&#xA;deepspeed --num_gpus 1 mii-example.py&#xA;&#xA;# Run on multiple GPUs&#xA;deepspeed --num_gpus 2 mii-example.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pipeline Options&lt;/h3&gt; &#xA;&lt;p&gt;While only the model name or path is required to stand up a non-persistent pipeline deployment, we offer customization options to our users:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;mii.pipeline()&lt;/code&gt; Options&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name_or_path: str&lt;/code&gt; Name or local path to a &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace&lt;/a&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_length: int&lt;/code&gt; Sets the default maximum token length for the prompt + response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;all_rank_output: bool&lt;/code&gt; When enabled, all ranks return the generated text. By default, only rank 0 will return text.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Users can also control the generation characteristics for individual prompts (i.e., when calling &lt;code&gt;pipe()&lt;/code&gt;) with the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;max_length: int&lt;/code&gt; Sets the per-prompt maximum token length for prompt + response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;min_new_tokens: int&lt;/code&gt; Sets the minimum number of tokens generated in the response. &lt;code&gt;max_length&lt;/code&gt; will take precedence over this setting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_new_tokens: int&lt;/code&gt; Sets the maximum number of tokens generated in the response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ignore_eos: bool&lt;/code&gt; (Defaults to &lt;code&gt;False&lt;/code&gt;) Setting to &lt;code&gt;True&lt;/code&gt; prevents generation from ending when the EOS token is encountered.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_p: float&lt;/code&gt; (Defaults to &lt;code&gt;0.9&lt;/code&gt;) When set below &lt;code&gt;1.0&lt;/code&gt;, filter tokens and keep only the most probable, where token probabilities sum to ≥&lt;code&gt;top_p&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_k: int&lt;/code&gt; (Defaults to &lt;code&gt;None&lt;/code&gt;) When &lt;code&gt;None&lt;/code&gt;, top-k filtering is disabled. When set, the number of highest probability tokens to keep.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;temperature: float&lt;/code&gt; (Defaults to &lt;code&gt;None&lt;/code&gt;) When &lt;code&gt;None&lt;/code&gt;, temperature is disabled. When set, modulates token probabilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;do_sample: bool&lt;/code&gt; (Defaults to &lt;code&gt;True&lt;/code&gt;) When &lt;code&gt;True&lt;/code&gt;, sample output logits. When &lt;code&gt;False&lt;/code&gt;, use greedy sampling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;return_full_text: bool&lt;/code&gt; (Defaults to &lt;code&gt;False&lt;/code&gt;) When &lt;code&gt;True&lt;/code&gt;, prepends the input prompt to the returned text&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Persistent Deployment&lt;/h2&gt; &#xA;&lt;p&gt;A persistent deployment is ideal for use with long-running and production applications. The persistent model uses a lightweight GRPC server that can be queried by multiple clients at once. The full example for running a persistent model is only 5 lines. Give it a try!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mii&#xA;client = mii.serve(&#34;mistralai/Mistral-7B-v0.1&#34;)&#xA;response = client.generate([&#34;Deepspeed is&#34;, &#34;Seattle is&#34;], max_new_tokens=128)&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The returned &lt;code&gt;response&lt;/code&gt; is a list of &lt;code&gt;Response&lt;/code&gt; objects. We can access several details about the generation (e.g., &lt;code&gt;response[0].prompt_length&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;generated_text: str&lt;/code&gt; Text generated by the model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;prompt_length: int&lt;/code&gt; Number of tokens in the original prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;generated_length: int&lt;/code&gt; Number of tokens generated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;finish_reason: str&lt;/code&gt; Reason for stopping generation. &lt;code&gt;stop&lt;/code&gt; indicates the EOS token was generated and &lt;code&gt;length&lt;/code&gt; indicates the generation reached &lt;code&gt;max_new_tokens&lt;/code&gt; or &lt;code&gt;max_length&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If we want to generate text from other processes, we can do that too:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = mii.client(&#34;mistralai/Mistral-7B-v0.1&#34;)&#xA;response = client.generate(&#34;Deepspeed is&#34;, max_new_tokens=128)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When we no longer need a persistent deployment, we can shutdown the server from any client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client.terminate_server()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Model Parallelism&lt;/h3&gt; &#xA;&lt;p&gt;Taking advantage of multi-GPU systems for better latency and throughput is also easy with the persistent deployments. Model parallelism is controlled by the &lt;code&gt;tensor_parallel&lt;/code&gt; input to &lt;code&gt;mii.serve&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = mii.serve(&#34;mistralai/Mistral-7B-v0.1&#34;, tensor_parallel=2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The resulting deployment will split the model across 2 GPUs to deliver faster inference and higher throughput than a single GPU.&lt;/p&gt; &#xA;&lt;h3&gt;Model Replicas&lt;/h3&gt; &#xA;&lt;p&gt;We can also take advantage of multi-GPU (and multi-node) systems by setting up multiple model replicas and taking advantage of the load-balancing that DeepSpeed-MII provides:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = mii.serve(&#34;mistralai/Mistral-7B-v0.1&#34;, replica_num=2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The resulting deployment will load 2 model replicas (one per GPU) and load-balance incoming requests between the 2 model instances.&lt;/p&gt; &#xA;&lt;p&gt;Model parallelism and replicas can also be combined to take advantage of systems with many more GPUs. In the example below, we run 2 model replicas, each split across 2 GPUs on a system with 4 GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = mii.serve(&#34;mistralai/Mistral-7B-v0.1&#34;, tensor_parallel=2, replica_num=2)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The choice between model parallelism and model replicas for maximum performance will depend on the nature of the hardware, model, and workload. For example, with small models users may find that model replicas provide the lowest average latency for requests. Meanwhile, large models may achieve greater overall throughput when using only model parallelism.&lt;/p&gt; &#xA;&lt;h3&gt;RESTful API&lt;/h3&gt; &#xA;&lt;p&gt;MII makes it easy to setup and run model inference via RESTful APIs by setting &lt;code&gt;enable_restful_api=True&lt;/code&gt; when creating a persistent MII deployment. The RESTful API can receive requests at &lt;code&gt;http://{HOST}:{RESTFUL_API_PORT}/mii/{DEPLOYMENT_NAME}&lt;/code&gt;. A full example is provided below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = mii.serve(&#xA;    &#34;mistralai/Mistral-7B-v0.1&#34;,&#xA;    deployment_name=&#34;mistral-deployment&#34;,&#xA;    enable_restful_api=True,&#xA;    restful_api_port=28080,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;📌 &lt;strong&gt;Note:&lt;/strong&gt; While providing a &lt;code&gt;deployment_name&lt;/code&gt; is not necessary (MII will autogenerate one for you), it is good practice to provide a &lt;code&gt;deployment_name&lt;/code&gt; so that you can ensure you are interfacing with the correct RESTful API.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;You can then send prompts to the RESTful gateway with any HTTP client, such as &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl --header &#34;Content-Type: application/json&#34; --request POST  -d &#39;{&#34;prompts&#34;: [&#34;DeepSpeed is&#34;, &#34;Seattle is&#34;], &#34;max_length&#34;: 128}&#39; http://localhost:28080/mii/mistral-deployment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or &lt;code&gt;python&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json&#xA;import requests&#xA;url = f&#34;http://localhost:28080/mii/mistral-deployment&#34;&#xA;params = {&#34;prompts&#34;: [&#34;DeepSpeed is&#34;, &#34;Seattle is&#34;], &#34;max_length&#34;: 128}&#xA;json_params = json.dumps(params)&#xA;output = requests.post(&#xA;    url, data=json_params, headers={&#34;Content-Type&#34;: &#34;application/json&#34;}&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!--&#xA;### Token Streaming&#xA;With a persistent deployment, the resulting response text can be streamed back to the client as it is generated. This functionality is useful for chatbot style applications. A simple example of streaming tokens is below:&#xA;```python&#xA;import mii&#xA;&#xA;out_tokens = []&#xA;def callback(response):&#xA;    print(f&#34;Received: {response.response}&#34;)&#xA;    out_tokens.append(response.response)&#xA;&#xA;client = mii.serve(&#34;mistralai/Mistral-7B-v0.1&#34;)&#xA;client.generate(&#34;Deepspeed is&#34;, streaming_fn=callback)&#xA;```&#xA;&#xA;To enable streaming output, we must provide `streaming_fn` with the prompt. This should be a callable function that acts as a callback and will receive the streaming tokens at they are generated. In the example above, we show a simple function that prints the current token and appends to a final output `out_tokens`.&#xA;--&gt; &#xA;&lt;h3&gt;Persistent Deployment Options&lt;/h3&gt; &#xA;&lt;p&gt;While only the model name or path is required to stand up a persistent deployment, we offer customization options to our users.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;mii.serve()&lt;/code&gt; Options&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name_or_path: str&lt;/code&gt; (Required) Name or local path to a &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace&lt;/a&gt; model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_length: int&lt;/code&gt; (Defaults to maximum sequence length in model config) Sets the default maximum token length for the prompt + response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;deployment_name: str&lt;/code&gt; (Defaults to &lt;code&gt;f&#34;{model_name_or_path}-mii-deployment&#34;&lt;/code&gt;) A unique identifying string for the persistent model. If provided, client objects should be retrieved with &lt;code&gt;client = mii.client(deployment_name)&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tensor_parallel: int&lt;/code&gt; (Defaults to &lt;code&gt;1&lt;/code&gt;) Number of GPUs to split the model across.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;replica_num: int&lt;/code&gt; (Defaults to &lt;code&gt;1&lt;/code&gt;) The number of model replicas to stand up.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;enable_restful_api: bool&lt;/code&gt; (Defaults to &lt;code&gt;False&lt;/code&gt;) When enabled, a RESTful API gateway process is launched that can be queried at &lt;code&gt;http://{host}:{restful_api_port}/mii/{deployment_name}&lt;/code&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/DeepSpeed-MII/main/#restful-api&#34;&gt;section on RESTful APIs&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;restful_api_port: int&lt;/code&gt; (Defaults to &lt;code&gt;28080&lt;/code&gt;) The port number used to interface with the RESTful API when &lt;code&gt;enable_restful_api&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;mii.client()&lt;/code&gt; Options&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;model_or_deployment_name: str&lt;/code&gt; Name of the model or &lt;code&gt;deployment_name&lt;/code&gt; passed to &lt;code&gt;mii.serve()&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Users can also control the generation characteristics for individual prompts (i.e., when calling &lt;code&gt;client.generate()&lt;/code&gt;) with the following options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;max_length: int&lt;/code&gt; Sets the per-prompt maximum token length for prompt + response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;min_new_tokens: int&lt;/code&gt; Sets the minimum number of tokens generated in the response. &lt;code&gt;max_length&lt;/code&gt; will take precedence over this setting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_new_tokens: int&lt;/code&gt; Sets the maximum number of tokens generated in the response.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ignore_eos: bool&lt;/code&gt; (Defaults to &lt;code&gt;False&lt;/code&gt;) Setting to &lt;code&gt;True&lt;/code&gt; prevents generation from ending when the EOS token is encountered.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_p: float&lt;/code&gt; (Defaults to &lt;code&gt;0.9&lt;/code&gt;) When set below &lt;code&gt;1.0&lt;/code&gt;, filter tokens and keep only the most probable, where token probabilities sum to ≥&lt;code&gt;top_p&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_k: int&lt;/code&gt; (Defaults to &lt;code&gt;None&lt;/code&gt;) When &lt;code&gt;None&lt;/code&gt;, top-k filtering is disabled. When set, the number of highest probability tokens to keep.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;temperature: float&lt;/code&gt; (Defaults to &lt;code&gt;None&lt;/code&gt;) When &lt;code&gt;None&lt;/code&gt;, temperature is disabled. When set, modulates token probabilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;do_sample: bool&lt;/code&gt; (Defaults to &lt;code&gt;True&lt;/code&gt;) When &lt;code&gt;True&lt;/code&gt;, sample output logits. When &lt;code&gt;False&lt;/code&gt;, use greedy sampling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;return_full_text: bool&lt;/code&gt; (Defaults to &lt;code&gt;False&lt;/code&gt;) When &lt;code&gt;True&lt;/code&gt;, prepends the input prompt to the returned text&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h1&gt;Trademarks&lt;/h1&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>