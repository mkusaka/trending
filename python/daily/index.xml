<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-26T01:44:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SkalskiP/top-cvpr-2023-papers</title>
    <updated>2023-06-26T01:44:54Z</updated>
    <id>tag:github.com,2023-06-26:/SkalskiP/top-cvpr-2023-papers</id>
    <link href="https://github.com/SkalskiP/top-cvpr-2023-papers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is a curated collection of the most exciting and influential CVPR 2023 papers. üî• [Paper + Code]&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;cr√®me de la cr√®me of CVPR 2023&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img width=&#34;600&#34; src=&#34;https://github.com/SkalskiP/top-cvpr-2023-papers/assets/26109316/793d71f5-6034-4342-a8b3-2a08646a6aa0&#34; alt=&#34;vancouver&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üëã hello&lt;/h2&gt; &#xA;&lt;p&gt;Computer Vision and Pattern Recognition is a massive conference. In &lt;strong&gt;2023&lt;/strong&gt; alone, &lt;strong&gt;9155&lt;/strong&gt; papers were submitted, and &lt;strong&gt;2359&lt;/strong&gt; were accepted. I created this repository to help you search for cr√®me de la cr√®me of CVPR publications. If the paper you are looking for is not on my short list, take a peek at the full &lt;a href=&#34;https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers&#34;&gt;list&lt;/a&gt; of accepted papers.&lt;/p&gt; &#xA;&lt;h2&gt;üóûÔ∏è papers&lt;/h2&gt; &#xA;&lt;!-- AUTOGENERATED_COURSES_TABLE --&gt; &#xA;&lt;!--&#xA;   WARNING: DO NOT EDIT THIS TABLE MANUALLY. IT IS AUTOMATICALLY GENERATED.&#xA;   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.&#xA;--&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;topic&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;title&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;OneFormer: One Transformer To Rule Universal Image Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SHI-Labs/OneFormer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/SHI-Labs/OneFormer?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.0622&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.0622-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;X-Decoder: Generalized Decoding for Pixel, Image and Language&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/X-Decoder&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/X-Decoder?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.1127&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.1127-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segmentation and Generative AI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Images Speak in Images: A Generalist Painter for In-Context Visual Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/baaivision/Painter&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/baaivision/Painter?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.02499&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.02499-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PACO: Parts and Attributes of Common Objects&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/paco&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/paco?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2301.01795&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2301.01795-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/ov-seg&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/ov-seg?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.0415&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2210.0415-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NeRF&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DynIBaR: Neural Dynamic Image-Based Rendering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google/dynibar&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/google/dynibar?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.11082&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.11082-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MoyGcc/vid2avatar&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MoyGcc/vid2avatar?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.11566&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2302.11566-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generative AI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D-aware Conditional Image Synthesis&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/dunbar12138/pix2pix3d&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dunbar12138/pix2pix3d?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.08509&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2302.08509-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D Human Mesh Estimation from Virtual Markers&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ShirleyMaxx/VirtualMarker&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ShirleyMaxx/VirtualMarker?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.11726&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.11726-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Transfer Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A Data-Based Perspective on Transfer Learning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MadryLab/data-transfer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MadryLab/data-transfer?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.05739&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2207.05739-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/NVlabs/ODISE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NVlabs/ODISE?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.04803&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.04803-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generative AI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google/dreambooth&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/google/dreambooth?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2208.12242-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generative AI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/timothybrooks/instruct-pix2pix?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.098&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.098-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Generative AI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;High-resolution image reconstruction with latent diffusion models from human brain activity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yu-takagi/StableDiffusionReconstruction&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yu-takagi/StableDiffusionReconstruction?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.11536&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2306.11536-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Benchmarking&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Beyond mAP: Towards better evaluation of instance segmentation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/rohitrango/beyond-map&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rohitrango/beyond-map?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.01614&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2207.01614-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NeRF&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/SamsungLabs/SPIn-NeRF&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/SamsungLabs/SPIn-NeRF?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.12254&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.12254-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/facebookresearch/omni3d&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/omni3d?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2207.1066&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2207.1066-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ECON: Explicit Clothed humans Optimized via Normal integration&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/YuliangXiu/ECON&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YuliangXiu/ECON?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.07422&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.07422-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3D&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360¬∞ Views&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/VITA-Group/NeuralLift-360&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/VITA-Group/NeuralLift-360?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.16431&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2211.16431-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- AUTOGENERATED_COURSES_TABLE --&gt; &#xA;&lt;h2&gt;ü¶∏ contribution&lt;/h2&gt; &#xA;&lt;p&gt;We would love your help in making this repository even better! If you know of an amazing paper that isn&#39;t listed here, or if you have any suggestions for improvement, feel free to open an &lt;a href=&#34;https://github.com/SkalskiP/top-cvpr-2023-papers/issues&#34;&gt;issue&lt;/a&gt; or submit a &lt;a href=&#34;https://github.com/SkalskiP/top-cvpr-2023-papers/pulls&#34;&gt;pull request&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/generative-models</title>
    <updated>2023-06-26T01:44:54Z</updated>
    <id>tag:github.com,2023-06-26:/Stability-AI/generative-models</id>
    <link href="https://github.com/Stability-AI/generative-models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative Models by Stability AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Generative Models by Stability AI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/000.jpg&#34; alt=&#34;sample1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;June 22, 2023&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We are releasing two new diffusion models for research purposes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SD-XL 0.9-base&lt;/code&gt;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The base model uses &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP-ViT/G&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/CLIP/tree/main&#34;&gt;CLIP-ViT/L&lt;/a&gt; for text encoding whereas the refiner model only uses the OpenCLIP model.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SD-XL 0.9-refiner&lt;/code&gt;: The refiner has been trained to denoise small noise levels of high quality data and as such is not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you would like to access these models for your research, please apply using one of the following links: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SDXL-0.9-Base model&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SDXL-0.9-Refiner&lt;/a&gt;. This means that you can apply for any of the two links - and if you are granted - you can access both. Please log in to your HuggingFace Account with your organization email to request access. &lt;strong&gt;We plan to do a full release soon (July).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;The codebase&lt;/h2&gt; &#xA;&lt;h3&gt;General Philosophy&lt;/h3&gt; &#xA;&lt;p&gt;Modularity is king. This repo implements a config-driven approach where we build and combine submodules by calling &lt;code&gt;instantiate_from_config()&lt;/code&gt; on objects defined in yaml configs. See &lt;code&gt;configs/&lt;/code&gt; for many examples.&lt;/p&gt; &#xA;&lt;h3&gt;Changelog from the old &lt;code&gt;ldm&lt;/code&gt; codebase&lt;/h3&gt; &#xA;&lt;p&gt;For training, we use &lt;a href=&#34;https://www.pytorchlightning.ai/index.html&#34;&gt;pytorch-lightning&lt;/a&gt;, but it should be easy to use other training wrappers around the base modules. The core diffusion model class (formerly &lt;code&gt;LatentDiffusion&lt;/code&gt;, now &lt;code&gt;DiffusionEngine&lt;/code&gt;) has been cleaned up:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial conditionings, and all combinations thereof) in a single class: &lt;code&gt;GeneralConditioner&lt;/code&gt;, see &lt;code&gt;sgm/modules/encoders/modules.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We separate guiders (such as classifier-free guidance, see &lt;code&gt;sgm/modules/diffusionmodules/guiders.py&lt;/code&gt;) from the samplers (&lt;code&gt;sgm/modules/diffusionmodules/sampling.py&lt;/code&gt;), and the samplers are independent of the model.&lt;/li&gt; &#xA; &lt;li&gt;We adopt the &lt;a href=&#34;https://arxiv.org/abs/2206.00364&#34;&gt;&#34;denoiser framework&#34;&lt;/a&gt; for both training and inference (most notable change is probably now the option to train continuous time models): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Discrete times models (denoisers) are simply a special case of continuous time models (denoisers); see &lt;code&gt;sgm/modules/diffusionmodules/denoiser.py&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The following features are now independent: weighting of the diffusion loss function (&lt;code&gt;sgm/modules/diffusionmodules/denoiser_weighting.py&lt;/code&gt;), preconditioning of the network (&lt;code&gt;sgm/modules/diffusionmodules/denoiser_scaling.py&lt;/code&gt;), and sampling of noise levels during training (&lt;code&gt;sgm/modules/diffusionmodules/sigma_sampling.py&lt;/code&gt;).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Autoencoding models have also been cleaned up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1. Clone the repo&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:Stability-AI/generative-models.git&#xA;cd generative-models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Setting up the virtualenv&lt;/h4&gt; &#xA;&lt;p&gt;This is assuming you have navigated to the &lt;code&gt;generative-models&lt;/code&gt; root after cloning it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This is tested under &lt;code&gt;python3.8&lt;/code&gt; and &lt;code&gt;python3.10&lt;/code&gt;. For other python versions, you might encounter version conflicts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyTorch 1.13&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install required packages from pypi&#xA;python3 -m venv .pt1&#xA;source .pt1/bin/activate&#xA;pip3 install wheel&#xA;pip3 install -r requirements_pt13.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyTorch 2.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install required packages from pypi&#xA;python3 -m venv .pt2&#xA;source .pt2/bin/activate&#xA;pip3 install wheel&#xA;pip3 install -r requirements_pt2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference:&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://streamlit.io/&#34;&gt;streamlit&lt;/a&gt; demo for text-to-image and image-to-image sampling in &lt;code&gt;scripts/demo/sampling.py&lt;/code&gt;. The following models are currently supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SD-XL 0.9-base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SD-XL 0.9-refiner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors&#34;&gt;SD 2.1-512&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors&#34;&gt;SD 2.1-768&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Weights for SDXL&lt;/strong&gt;: If you would like to access these models for your research, please apply using one of the following links: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SDXL-0.9-Base model&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SDXL-0.9-Refiner&lt;/a&gt;. This means that you can apply for any of the two links - and if you are granted - you can access both. Please log in to your HuggingFace Account with your organization email to request access.&lt;/p&gt; &#xA;&lt;p&gt;After obtaining the weights, place them into &lt;code&gt;checkpoints/&lt;/code&gt;. Next, start the demo using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run scripts/demo/sampling.py --server.port &amp;lt;your_port&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Invisible Watermark Detection&lt;/h3&gt; &#xA;&lt;p&gt;Images generated with our code use the &lt;a href=&#34;https://github.com/ShieldMnt/invisible-watermark/&#34;&gt;invisible-watermark&lt;/a&gt; library to embed an invisible watermark into the model output. We also provide a script to easily detect that watermark. Please note that this watermark is not the same as in previous Stable Diffusion 1.x/2.x versions.&lt;/p&gt; &#xA;&lt;p&gt;To run the script you need to either have a working installation as above or try an &lt;em&gt;experimental&lt;/em&gt; import using only a minimal amount of packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .detect&#xA;source .detect/bin/activate&#xA;&#xA;pip install &#34;numpy&amp;gt;=1.17&#34; &#34;PyWavelets&amp;gt;=1.1.1&#34; &#34;opencv-python&amp;gt;=4.1.0.25&#34;&#xA;pip install --no-deps invisible-watermark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the script you need to have a working installation as above. The script is then useable in the following ways (don&#39;t forget to activate your virtual environment beforehand, e.g. &lt;code&gt;source .pt1/bin/activate&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# test a single file&#xA;python scripts/demo/detect.py &amp;lt;your filename here&amp;gt;&#xA;# test multiple files at once&#xA;python scripts/demo/detect.py &amp;lt;filename 1&amp;gt; &amp;lt;filename 2&amp;gt; ... &amp;lt;filename n&amp;gt;&#xA;# test all files in a specific folder&#xA;python scripts/demo/detect.py &amp;lt;your folder name here&amp;gt;/*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training:&lt;/h2&gt; &#xA;&lt;p&gt;We are providing example training configs in &lt;code&gt;configs/example_training&lt;/code&gt;. To launch a training, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/&amp;lt;config1.yaml&amp;gt; configs/&amp;lt;config2.yaml&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where configs are merged from left to right (later configs overwrite the same values). This can be used to combine model, training and data configs. However, all of them can also be defined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py --base configs/example_training/toy/mnist_cond.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 1:&lt;/strong&gt; Using the non-toy-dataset configs &lt;code&gt;configs/example_training/imagenet-f8_cond.yaml&lt;/code&gt;, &lt;code&gt;configs/example_training/txt2img-clipl.yaml&lt;/code&gt; and &lt;code&gt;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&lt;/code&gt; for training will require edits depdending on the used dataset (which is expected to stored in tar-file in the &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;webdataset-format&lt;/a&gt;). To find the parts which have to be adapted, search for comments containing &lt;code&gt;USER:&lt;/code&gt; in the respective config.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 2:&lt;/strong&gt; This repository supports both &lt;code&gt;pytorch1.13&lt;/code&gt; and &lt;code&gt;pytorch2&lt;/code&gt;for training generative models. However for autoencoder training as e.g. in &lt;code&gt;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&lt;/code&gt;, only &lt;code&gt;pytorch1.13&lt;/code&gt; is supported.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 3:&lt;/strong&gt; Training latent generative models (as e.g. in &lt;code&gt;configs/example_training/imagenet-f8_cond.yaml&lt;/code&gt;) requires retrieving the checkpoint from &lt;a href=&#34;https://huggingface.co/stabilityai/sdxl-vae/tree/main&#34;&gt;Hugging Face&lt;/a&gt; and replacing the &lt;code&gt;CKPT_PATH&lt;/code&gt; placeholder in &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/configs/example_training/imagenet-f8_cond.yaml#81&#34;&gt;this line&lt;/a&gt;. The same is to be done for the provided text-to-image configs.&lt;/p&gt; &#xA;&lt;h3&gt;Building New Diffusion Models&lt;/h3&gt; &#xA;&lt;h4&gt;Conditioner&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;GeneralConditioner&lt;/code&gt; is configured through the &lt;code&gt;conditioner_config&lt;/code&gt;. Its only attribute is &lt;code&gt;emb_models&lt;/code&gt;, a list of different embedders (all inherited from &lt;code&gt;AbstractEmbModel&lt;/code&gt;) that are used to condition the generative model. All embedders should define whether or not they are trainable (&lt;code&gt;is_trainable&lt;/code&gt;, default &lt;code&gt;False&lt;/code&gt;), a classifier-free guidance dropout rate is used (&lt;code&gt;ucg_rate&lt;/code&gt;, default &lt;code&gt;0&lt;/code&gt;), and an input key (&lt;code&gt;input_key&lt;/code&gt;), for example, &lt;code&gt;txt&lt;/code&gt; for text-conditioning or &lt;code&gt;cls&lt;/code&gt; for class-conditioning. When computing conditionings, the embedder will get &lt;code&gt;batch[input_key]&lt;/code&gt; as input. We currently support two to four dimensional conditionings and conditionings of different embedders are concatenated appropriately. Note that the order of the embedders in the &lt;code&gt;conditioner_config&lt;/code&gt; is important.&lt;/p&gt; &#xA;&lt;h4&gt;Network&lt;/h4&gt; &#xA;&lt;p&gt;The neural network is set through the &lt;code&gt;network_config&lt;/code&gt;. This used to be called &lt;code&gt;unet_config&lt;/code&gt;, which is not general enough as we plan to experiment with transformer-based diffusion backbones.&lt;/p&gt; &#xA;&lt;h4&gt;Loss&lt;/h4&gt; &#xA;&lt;p&gt;The loss is configured through &lt;code&gt;loss_config&lt;/code&gt;. For standard diffusion model training, you will have to set &lt;code&gt;sigma_sampler_config&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Sampler config&lt;/h4&gt; &#xA;&lt;p&gt;As discussed above, the sampler is independent of the model. In the &lt;code&gt;sampler_config&lt;/code&gt;, we set the type of numerical solver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free guidance.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Handling&lt;/h3&gt; &#xA;&lt;p&gt;For large scale training we recommend using the datapipelines from our &lt;a href=&#34;https://github.com/Stability-AI/datapipelines&#34;&gt;datapipelines&lt;/a&gt; project. The project is contained in the requirement and automatically included when following the steps from the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/#installation&#34;&gt;Installation section&lt;/a&gt;. Small map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of data keys/values, e.g.,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;example = {&#34;jpg&#34;: x,  # this is a tensor -1...1 chw&#xA;           &#34;txt&#34;: &#34;a beautiful image&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where we expect images in -1...1, channel-first format.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SizheAn/PanoHead</title>
    <updated>2023-06-26T01:44:54Z</updated>
    <id>tag:github.com,2023-06-26:/SizheAn/PanoHead</id>
    <link href="https://github.com/SizheAn/PanoHead" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code Repository for CVPR 2023 Paper &#34;PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360 degree&#34;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360¬∞&lt;br&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.13071&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2303.13071-b31b1b&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LICENSE-CC--BY--4.0-yellow&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=Y8NXiBOEWoE&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=CVPR 2023&amp;amp;message=8 Minute Video&amp;amp;color=red&#34; height=&#34;22.5&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SizheAn/PanoHead/main/misc/teaser.png&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360¬∞&lt;/strong&gt;&lt;br&gt; Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y. Ogras, Linjie Luo &lt;br&gt;&lt;a href=&#34;https://sizhean.github.io/panohead&#34;&gt;https://sizhean.github.io/panohead&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: &lt;em&gt;Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360¬∞ with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We recommend Linux for performance and compatibility reasons.&lt;/li&gt; &#xA; &lt;li&gt;1‚Äì8 high-end NVIDIA GPUs. We have done all testing and development using V100, RTX3090, and A100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;64-bit Python 3.8 and PyTorch 1.11.0 (or later). See &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt; for PyTorch install instructions.&lt;/li&gt; &#xA; &lt;li&gt;CUDA toolkit 11.3 or later. (Why is a separate CUDA toolkit installation required? We use the custom CUDA extensions from the StyleGAN3 repo. Please see &lt;a href=&#34;https://github.com/NVlabs/stylegan3/raw/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary&#34;&gt;Troubleshooting&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Python libraries: see &lt;a href=&#34;https://raw.githubusercontent.com/SizheAn/PanoHead/main/environment.yml&#34;&gt;environment.yml&lt;/a&gt; for exact library dependencies. You can use the following commands with Miniconda3 to create and activate your Python environment: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd PanoHead&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;conda env create -f environment.yml&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;conda activate panohead&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Download the whole &lt;code&gt;models&lt;/code&gt; folder from &lt;a href=&#34;https://drive.google.com/drive/folders/1m517-F1NCTGA159dePs5R5qj02svtX1_?usp=sharing&#34;&gt;link&lt;/a&gt; and put it under the root dir.&lt;/p&gt; &#xA;&lt;p&gt;Pre-trained networks are stored as &lt;code&gt;*.pkl&lt;/code&gt; files that can be referenced using local filenames.&lt;/p&gt; &#xA;&lt;h2&gt;Generating results&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate videos using pre-trained model&#xA;&#xA;python gen_videos.py --network models/easy-khair-180-gpc0.8-trans10-025000.pkl \&#xA;--seeds 0-3 --grid 2x2 --outdir=out --cfg Head --trunc 0.7&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate images and shapes (as .mrc files) using pre-trained model&#xA;&#xA;python gen_samples.py --outdir=out --trunc=0.7 --shapes=true --seeds=0-3 \&#xA;    --network models/easy-khair-180-gpc0.8-trans10-025000.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate full head reconstruction from a single RGB image.&#xA;# Please refer to ./gen_pti_script.sh&#xA;# For this application we need to specify dataset folder instead of zip files.&#xA;&#xA;./gen_pti_script.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate full head interpolation from two seeds.&#xA;# Please refer to ./gen_interpolation.py for the implementation&#xA;&#xA;python gen_interpolation.py --network models/easy-khair-180-gpc0.8-trans10-025000.pkl\&#xA;        --trunc 0.7 --outdir interpolation_out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using networks from Python&lt;/h2&gt; &#xA;&lt;p&gt;You can use pre-trained networks in your own Python code as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.python&#34;&gt;with open(&#39;*.pkl&#39;, &#39;rb&#39;) as f:&#xA;    G = pickle.load(f)[&#39;G_ema&#39;].cuda()  # torch.nn.Module&#xA;z = torch.randn([1, G.z_dim]).cuda()    # latent codes&#xA;c = torch.cat([cam2world_pose.reshape(-1, 16), intrinsics.reshape(-1, 9)], 1) # camera parameters&#xA;img = G(z, c)[&#39;image&#39;]                           # NCHW, float32, dynamic range [-1, +1], no truncation&#xA;mask = G(z, c)[&#39;image_mask&#39;]                    # NHW, int8, [0,255]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above code requires &lt;code&gt;torch_utils&lt;/code&gt; and &lt;code&gt;dnnlib&lt;/code&gt; to be accessible via &lt;code&gt;PYTHONPATH&lt;/code&gt;. It does not need source code for the networks themselves ‚Äî their class definitions are loaded from the pickle via &lt;code&gt;torch_utils.persistence&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The pickle contains three networks. &lt;code&gt;&#39;G&#39;&lt;/code&gt; and &lt;code&gt;&#39;D&#39;&lt;/code&gt; are instantaneous snapshots taken during training, and &lt;code&gt;&#39;G_ema&#39;&lt;/code&gt; represents a moving average of the generator weights over several training steps. The networks are regular instances of &lt;code&gt;torch.nn.Module&lt;/code&gt;, with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;FFHQ-F(ullhead) consists of &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;Flickr-Faces-HQ dataset&lt;/a&gt;, &lt;a href=&#34;https://psh01087.github.io/K-Hairstyle/&#34;&gt;K-Hairstyle dataset&lt;/a&gt;, and an in-house human head dataset. For head pose estimation, we use &lt;a href=&#34;https://arxiv.org/abs/2005.10353&#34;&gt;WHENet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Due to the license issue, we are not able to release FFHQ-F dataset that we used to train the model. &lt;a href=&#34;https://raw.githubusercontent.com/SizheAn/PanoHead/main/dataset/testdata_img/&#34;&gt;test_data_img&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/SizheAn/PanoHead/main/dataset/testdata_seg/&#34;&gt;test_data_seg&lt;/a&gt; are just an example for showing the dataset struture. For the camera pose convention, please refer to &lt;a href=&#34;https://github.com/NVlabs/eg3d&#34;&gt;EG3D&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets format&lt;/h2&gt; &#xA;&lt;p&gt;For training purpose, we can use either zip files or normal folder for image dataset and segmentation dataset. For PTI, we need to use folder.&lt;/p&gt; &#xA;&lt;p&gt;To compress dataset folder to zip file, we can use &lt;a href=&#34;https://raw.githubusercontent.com/SizheAn/PanoHead/main/dataset_tool_seg.py&#34;&gt;dataset_tool_seg&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;python dataset_tool_seg.py --img_source dataset/testdata_img --seg_source  dataset/testdata_seg --img_dest dataset/testdata_img.zip --seg_dest dataset/testdata_seg.zip --resolution 512x512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Examples of training using &lt;code&gt;train.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Train with StyleGAN2 backbone from scratch with raw neural rendering resolution=64, using 8 GPUs.&#xA;# with segmentation mask, trigrid_depth@3, self-adaptive camera pose loss regularizer@10&#xA;&#xA;python train.py --outdir training-runs  --img_data dataset/testdata_img.zip --seg_data dataset/testdata_seg.zip --cfg=ffhq --batch=32 --gpus 8\\&#xA;--gamma=1 --gamma_seg=1 --gen_pose_cond=True --mirror=1 --use_torgb_raw=1 --decoder_activation=&#34;none&#34; --disc_module MaskDualDiscriminatorV2\\&#xA;--bcg_reg_prob 0.2 --triplane_depth 3 --density_noise_fade_kimg 200 --density_reg 0 --min_yaw 0 --max_yaw 180 --back_repeat 4 --trans_reg 10 --gpc_reg_prob 0.7&#xA;&#xA;&#xA;# Second stage finetuning to 128 neural rendering resolution (optional).&#xA;&#xA;python train.py --outdir results --img_data dataset/testdata_img.zip --seg_data dataset/testdata_seg.zip --cfg=ffhq --batch=32 --gpus 8\\&#xA;--resume=~/training-runs/experiment_dir/network-snapshot-025000.pkl\\&#xA;--gamma=1 --gamma_seg=1 --gen_pose_cond=True --mirror=1 --use_torgb_raw=1 --decoder_activation=&#34;none&#34; --disc_module MaskDualDiscriminatorV2\\&#xA;--bcg_reg_prob 0.2 --triplane_depth 3 --density_noise_fade_kimg 200 --density_reg 0 --min_yaw 0 --max_yaw 180 --back_repeat 4 --trans_reg 10 --gpc_reg_prob 0.7\\&#xA;--neural_rendering_resolution_final=128 --resume_kimg 1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Metrics&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;./get_metrics.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are three evaluation modes: all, front, and back as we mentioned in the paper. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/SizheAn/PanoHead/main/calc_metrics.py&#34;&gt;cal_metrics.py&lt;/a&gt; for the implementation.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repo helpful, please cite our paper using the following bib:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{An_2023_CVPR,&#xA;    author    = {An, Sizhe and Xu, Hongyi and Shi, Yichun and Song, Guoxian and Ogras, Umit Y. and Luo, Linjie},&#xA;    title     = {PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360deg},&#xA;    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;    month     = {June},&#xA;    year      = {2023},&#xA;    pages     = {20950-20959}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;This is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank Shuhong Chen for the discussion during Sizhe&#39;s internship.&lt;/p&gt; &#xA;&lt;p&gt;This repo is heavily based off the &lt;a href=&#34;https://github.com/NVlabs/eg3d&#34;&gt;NVlabs/eg3d&lt;/a&gt; repo; Huge thanks to the EG3D authors for releasing their code!&lt;/p&gt;</summary>
  </entry>
</feed>