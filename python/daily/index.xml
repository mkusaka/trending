<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-17T01:38:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pjialin/py12306</title>
    <updated>2023-09-17T01:38:46Z</updated>
    <id>tag:github.com,2023-09-17:/pjialin/py12306</id>
    <link href="https://github.com/pjialin/py12306" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🚂 12306 购票助手，支持集群，多账号，多任务购票以及 Web 页面管理&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🚂 py12306 购票助手&lt;/h1&gt; &#xA;&lt;p&gt;分布式，多账号，多任务购票&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 多日期查询余票&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 自动打码下单&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 用户状态恢复&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 电话语音通知&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 多账号、多任务、多线程支持&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 单个任务多站点查询&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 分布式运行&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Docker 支持&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 动态修改配置文件&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 邮件通知&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Web 管理页面&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 微信消息通知&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 代理池支持 (&lt;a href=&#34;https://github.com/pjialin/pyproxy-async&#34;&gt;pyproxy-async&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用&lt;/h2&gt; &#xA;&lt;p&gt;py12306 需要运行在 python 3.6 以上版本（其它版本暂未测试)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. 安装依赖&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pjialin/py12306&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. 配置程序&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp env.py.example env.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;自动打码&lt;/p&gt; &#xA;&lt;p&gt;（若快已停止服务，目前只能设置&lt;strong&gt;free&lt;/strong&gt;打码模式） free 已对接到打码共享平台，&lt;a href=&#34;https://py12306-helper.pjialin.com/&#34;&gt;https://py12306-helper.pjialin.com&lt;/a&gt;，欢迎参与分享&lt;/p&gt; &#xA;&lt;p&gt;语音通知&lt;/p&gt; &#xA;&lt;p&gt;语音验证码使用的是阿里云 API 市场上的一个服务商，需要到 &lt;a href=&#34;https://market.aliyun.com/products/56928004/cmapi026600.html&#34;&gt;https://market.aliyun.com/products/56928004/cmapi026600.html&lt;/a&gt; 购买后将 appcode 填写到配置中&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. 启动前测试&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;目前提供了一些简单的测试，包括用户账号检测，乘客信息检测，车站检测等&lt;/p&gt; &#xA;&lt;p&gt;开始测试 -t&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py -t&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;测试通知消息 (语音, 邮件) -t -n&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 默认不会进行通知测试，要对通知进行测试需要加上 -n 参数 &#xA;python main.py -t -n&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. 运行程序&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;参数列表&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;-t 测试配置信息&lt;/li&gt; &#xA; &lt;li&gt;-t -n 测试配置信息以及通知消息&lt;/li&gt; &#xA; &lt;li&gt;-c 指定自定义配置文件位置&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;分布式集群&lt;/h3&gt; &#xA;&lt;p&gt;集群依赖于 redis，目前支持情况&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;单台主节点多个子节点同时运行&lt;/li&gt; &#xA; &lt;li&gt;主节点宕机后自动切换提升子节点为主节点&lt;/li&gt; &#xA; &lt;li&gt;主节点恢复后自动恢复为真实主节点&lt;/li&gt; &#xA; &lt;li&gt;配置通过主节点同步到所有子节点&lt;/li&gt; &#xA; &lt;li&gt;主节点配置修改后无需重启子节点，支持自动更新&lt;/li&gt; &#xA; &lt;li&gt;子节点消息实时同步到主节点&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;使用&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;将配置文件的中 &lt;code&gt;CLUSTER_ENABLED&lt;/code&gt; 打开即开启分布式&lt;/p&gt; &#xA;&lt;p&gt;目前提供了一个单独的子节点配置文件 &lt;code&gt;env.slave.py.example&lt;/code&gt; 将文件修改为 &lt;code&gt;env.slave.py&lt;/code&gt;， 通过 &lt;code&gt;python main.py -c env.slave.py&lt;/code&gt; 即可快速启动&lt;/p&gt; &#xA;&lt;h2&gt;Docker 使用&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. 将配置文件下载到本地&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm pjialin/py12306 cat /config/env.py &amp;gt; env.py&#xA;# 或&#xA;curl https://raw.githubusercontent.com/pjialin/py12306/master/env.docker.py.example -o env.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. 修改好配置后运行&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm --name py12306 -p 8008:8008 -d -v $(pwd):/config -v py12306:/data pjialin/py12306&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;当前目录会多一个 12306.log 的日志文件， &lt;code&gt;tail -f 12306.log&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker-compose 中使用&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. 复制配置文件&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cp docker-compose.yml.example docker-compose.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. 从 docker-compose 运行&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;在&lt;code&gt;docker-compose.yml&lt;/code&gt;所在的目录使用命令&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Web 管理页面&lt;/h2&gt; &#xA;&lt;p&gt;目前支持用户和任务以及实时日志查看，更多功能后续会不断加入&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;使用&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;打开 Web 功能需要将配置中的 &lt;code&gt;WEB_ENABLE&lt;/code&gt; 打开，启动程序后访问当前主机地址 + 端口号 (默认 8008) 即可，如 &lt;a href=&#34;http://127.0.0.1:8008&#34;&gt;http://127.0.0.1:8008&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;更新&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;19-01-10 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;支持分布式集群&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;19-01-11 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;配置文件支持动态修改&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;19-01-12 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;新增免费打码&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;19-01-14 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;新增 Web 页面支持&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;19-01-15 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;新增 钉钉通知&lt;/li&gt; &#xA;   &lt;li&gt;新增 Telegram 通知&lt;/li&gt; &#xA;   &lt;li&gt;新增 ServerChan 和 PushBear 微信推送&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;19-01-18 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;新增 CDN 查询&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;截图&lt;/h2&gt; &#xA;&lt;h3&gt;Web 管理页面&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pjialin/py12306/raw/master/data/images/web.png&#34; alt=&#34;Web 管理页面图片&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;下单成功&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/pjialin/py12306/raw/master/data/images/order_success.png&#34; alt=&#34;下单成功图片&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;关于防封&lt;/h3&gt; &#xA;&lt;p&gt;目前查询和登录操作是分开的，查询是不依赖用户是否登录，放在 A 云 T 云容易被限制 ip，建议在其它网络环境下运行&lt;/p&gt; &#xA;&lt;p&gt;QQ 交流群 &lt;a href=&#34;https://jq.qq.com/?_wv=1027&amp;amp;k=5PgzDwV&#34;&gt;780289875&lt;/a&gt;，TG 群 &lt;a href=&#34;https://t.me/joinchat/F3sSegrF3x8KAmsd1mTu7w&#34;&gt;Py12306 交流&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Online IDE&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io#https://github.com/pjialin/py12306&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;在 Gitpod 中打开&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;感谢大佬 &lt;a href=&#34;https://github.com/testerSunshine/12306&#34;&gt;testerSunshine&lt;/a&gt;，借鉴了部分实现&lt;/li&gt; &#xA; &lt;li&gt;感谢所有提供 pr 的大佬&lt;/li&gt; &#xA; &lt;li&gt;感谢大佬 &lt;a href=&#34;https://github.com/zhaipro/easy12306&#34;&gt;zhaipro&lt;/a&gt; 的验证码本地识别模型与算法&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pjialin/py12306/raw/master/LICENSE&#34;&gt;Apache License.&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/belebele</title>
    <updated>2023-09-17T01:38:46Z</updated>
    <id>tag:github.com,2023-09-17:/facebookresearch/belebele</id>
    <link href="https://github.com/facebookresearch/belebele" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repo for the Belebele dataset, a massively multilingual reading comprehension dataset.&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; &#xA;&lt;h1&gt;The Belebele Benchmark for Massively Multilingual NLU Evaluation&lt;/h1&gt; &#xA;&lt;p&gt;Belebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the &lt;a href=&#34;https://github.com/facebookresearch/flores/tree/main/flores200&#34;&gt;FLORES-200&lt;/a&gt; dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks. While all questions directly relate to the passage, the English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. Belebele opens up new avenues for evaluating and analyzing the multilingual abilities of language models and NLP systems.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to our paper for more details, &lt;a href=&#34;https://arxiv.org/abs/2308.16884&#34;&gt;The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Composition&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;900 questions per language variant&lt;/li&gt; &#xA; &lt;li&gt;488 distinct passages, there are 1-2 associated questions for each.&lt;/li&gt; &#xA; &lt;li&gt;For each question, there is 4 multiple-choice answers, exactly 1 of which is correct.&lt;/li&gt; &#xA; &lt;li&gt;122 language/language variants (including English).&lt;/li&gt; &#xA; &lt;li&gt;900 x 122 = 109,800 total questions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;Belebele can be downloaded &lt;a href=&#34;https://dl.fbaipublicfiles.com/belebele/Belebele.zip&#34;&gt;here&lt;/a&gt; which you can download with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget --trust-server-names https://dl.fbaipublicfiles.com/belebele/Belebele.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Formatting details&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;link&lt;/code&gt; and &lt;code&gt;split&lt;/code&gt; uniquely identifies a passage.&lt;/li&gt; &#xA; &lt;li&gt;The combination of passage (&lt;code&gt;link&lt;/code&gt; and &lt;code&gt;split&lt;/code&gt;) and &lt;code&gt;question_number&lt;/code&gt; (either 1 or 2) uniquely identifies a question.&lt;/li&gt; &#xA; &lt;li&gt;The language of each row is denoted in the &lt;code&gt;dialect&lt;/code&gt; column with the FLORES-200 code (see Languages below)&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;correct_answer_num&lt;/code&gt; is one-indexed (e.g. a value of &lt;code&gt;2&lt;/code&gt; means &lt;code&gt;mc_answer2&lt;/code&gt; is correct)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Pausible Evaluation Settings&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to the parallel nature of the dataset and the simplicity of the task, there are many possible settings in which we can evaluate language models. In all evaluation settings, the metric of interest is simple accuracy (# correct / total).&lt;/p&gt; &#xA;&lt;p&gt;Evaluating models on Belebele in English can be done via finetuning, few-shot, or zero-shot. For other target languages, we propose the incomprehensive list of evaluation settings below. Settings that are compatible with evaluating non-English models (monolingual or cross-lingual) are denoted with &lt;code&gt;^&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;No finetuning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-shot with natural language instructions (English instructions)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For chat-finetuned models, we give it English instructions for the task and the sample in the target language in the same input.&lt;/li&gt; &#xA;   &lt;li&gt;For our experiments, we instruct the model to provide the letter &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, or &lt;code&gt;D&lt;/code&gt;. We perform post-processing steps and accept answers predicted as e.g. &lt;code&gt;(A)&lt;/code&gt; instead of &lt;code&gt;A&lt;/code&gt;. We sometimes additionally remove the prefix &lt;code&gt;The correct answer is&lt;/code&gt; for predictions that do not start with one of the four accepted answers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-shot with natural language instructions (translated instructions)&lt;/strong&gt;^ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Same as above, except the instructions are translated to the target language so that the instructions and samples are in the same language. The instructions can be human or machine-translated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Few-shot in-context learning (English examples)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A few samples (e.g. 5) are taken from the English training set (see below) and prompted to the model. Then, the model is evaluated with the same template but with the passages, questions, and answers in the target language.&lt;/li&gt; &#xA;   &lt;li&gt;For our experiments, we use the template: &lt;code&gt;P: &amp;lt;passage&amp;gt; \n Q: &amp;lt;question&amp;gt; \n A: &amp;lt;mc answer 1&amp;gt; \n B: &amp;lt;mc answer 2&amp;gt; \n C: &amp;lt;mc answer 3&amp;gt; \n D: &amp;lt;mc answer 4&amp;gt; \n Answer: &amp;lt;Correct answer letter&amp;gt;&lt;/code&gt;. We perform prediction by picking the answer within &lt;code&gt;[A, B, C, D]&lt;/code&gt; that has the highest probability relatively to the others.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Few-shot in-context learning (translated examples)&lt;/strong&gt;^ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Same as above, except the samples from the training set are translated to the target language so that the examples and evaluation data are in the same language. The training samples can be human or machine-translated.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;With finetuning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;English finetune &amp;amp; multilingual evaluation&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The model is finetuned to the task using the English training set, probably with a sequence classification head. Then the model is evaluated in all the target languages individually.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;English finetune &amp;amp; cross-lingual evaluation&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Same as above, except the model is evaluated in a cross-lingual setting, where for each question, the passage &amp;amp; answers could be provided in a different language. For example, passage could be in language &lt;code&gt;x&lt;/code&gt;, question in language &lt;code&gt;y&lt;/code&gt;, and answers in language &lt;code&gt;z&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Translate-train&lt;/strong&gt;^ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For each target language, the model is individually finetuned on training samples that have been machine-translated from English to that language. Each model is then evaluated in the respective target language.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Translate-train-all&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Similar to above, except here the model is trained on translated samples from all target languages at once. The single finetuned model is then evaluated on all target languages.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Translate-train-all &amp;amp; cross-lingual evaluation&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Same as above, except the single finetuned model is evaluated in a cross-lingual setting, where for each question, the passage &amp;amp; answers could be provided in a different language.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Translate-test&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The model is finetuned using the English training data and then the evaluation dataset is machine-translated to English and evaluated on the English.&lt;/li&gt; &#xA;   &lt;li&gt;This setting is primarily a reflection of the quality of the machine translation system, but is useful for comparison to multilingual models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition, there are 83 additional languages in FLORES-200 for which questions were not translated for Belebele. Since the passages exist in those target languages, machine-translating the questions &amp;amp; answers may enable decent evaluation of machine reading comprehension in those languages.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Training Set&lt;/h2&gt; &#xA;&lt;p&gt;As discussed in the paper, we also provide an assembled training set consisting of samples&lt;/p&gt; &#xA;&lt;p&gt;The Belebele dataset is intended to be used only as a test set, and not for training or validation. Therefore, for models that require additional task-specific training, we instead propose using an assembled training set consisting of samples from pre-existing multiple-choice QA datasets in English. We considered diverse datasets, and determine the most compatible to be &lt;a href=&#34;https://www.cs.cmu.edu/~glai1/data/race/&#34;&gt;RACE&lt;/a&gt;, &lt;a href=&#34;https://allenai.org/data/sciq&#34;&gt;SciQ&lt;/a&gt;, &lt;a href=&#34;https://cogcomp.seas.upenn.edu/multirc/&#34;&gt;MultiRC&lt;/a&gt;, &lt;a href=&#34;https://mattr1.github.io/mctest/&#34;&gt;MCTest&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/S19-1012/&#34;&gt;MCScript2.0&lt;/a&gt;, and &lt;a href=&#34;https://whyu.me/reclor/&#34;&gt;ReClor&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For each of the six datasets, we unpack and restructure the passages and questions from their respective formats. We then filter out less suitable samples (e.g. questions with multiple correct answers). In the end, the dataset comprises 67.5k training samples and 3.7k development samples, more than half of which are from RACE. We provide a script (&lt;code&gt;assemble_training_set.py&lt;/code&gt;) to reconstruct this dataset for anyone to perform task finetuning.&lt;/p&gt; &#xA;&lt;p&gt;Since the training set is a joint sample of other datasets, it is governed by a different license. We do not claim any of that work or datasets to be our own. See the Licenses section.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Languages in Belebele&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;FLORES-200 Code&lt;/th&gt; &#xA;   &lt;th&gt;English Name&lt;/th&gt; &#xA;   &lt;th&gt;Script&lt;/th&gt; &#xA;   &lt;th&gt;Family&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;acm_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Mesopotamian Arabic&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;afr_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afrikaans&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;als_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Tosk Albanian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Paleo-Balkanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;amh_Ethi&lt;/td&gt; &#xA;   &lt;td&gt;Amharic&lt;/td&gt; &#xA;   &lt;td&gt;Ethi&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;apc_Arab&lt;/td&gt; &#xA;   &lt;td&gt;North Levantine Arabic&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arb_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Modern Standard Arabic&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arb_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Modern Standard Arabic (Romanized)&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ars_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Najdi Arabic&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ary_arab&lt;/td&gt; &#xA;   &lt;td&gt;Moroccan Arabic&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arz_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Egyptian Arabic&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;asm_Beng&lt;/td&gt; &#xA;   &lt;td&gt;Assamese&lt;/td&gt; &#xA;   &lt;td&gt;Beng&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;azj_Latn&lt;/td&gt; &#xA;   &lt;td&gt;North Azerbaijani&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Turkic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bam_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Bambara&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Mande&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ben_Beng&lt;/td&gt; &#xA;   &lt;td&gt;Bengali&lt;/td&gt; &#xA;   &lt;td&gt;Beng&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ben_Latn^&lt;/td&gt; &#xA;   &lt;td&gt;Bengali (Romanized)&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bod_Tibt&lt;/td&gt; &#xA;   &lt;td&gt;Standard Tibetan&lt;/td&gt; &#xA;   &lt;td&gt;Tibt&lt;/td&gt; &#xA;   &lt;td&gt;Sino-Tibetan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bul_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Bulgarian&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cat_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Catalan&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Romance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ceb_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Cebuano&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ces_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Czech&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ckb_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Central Kurdish&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Iranian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dan_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Danish&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;deu_Latn&lt;/td&gt; &#xA;   &lt;td&gt;German&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ell_Grek&lt;/td&gt; &#xA;   &lt;td&gt;Greek&lt;/td&gt; &#xA;   &lt;td&gt;Grek&lt;/td&gt; &#xA;   &lt;td&gt;Hellenic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;eng_Latn&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;est_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Estonian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Uralic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;eus_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Basque&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Basque&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fin_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Finnish&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Uralic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fra_Latn&lt;/td&gt; &#xA;   &lt;td&gt;French&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Romance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fuv_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Nigerian Fulfulde&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gaz_Latn&lt;/td&gt; &#xA;   &lt;td&gt;West Central Oromo&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;grn_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Guarani&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Tupian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;guj_Gujr&lt;/td&gt; &#xA;   &lt;td&gt;Gujarati&lt;/td&gt; &#xA;   &lt;td&gt;Gujr&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hat_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Haitian Creole&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hau_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Hausa&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;heb_Hebr&lt;/td&gt; &#xA;   &lt;td&gt;Hebrew&lt;/td&gt; &#xA;   &lt;td&gt;Hebr&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hin_Deva&lt;/td&gt; &#xA;   &lt;td&gt;Hindi&lt;/td&gt; &#xA;   &lt;td&gt;Deva&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hin_Latn^&lt;/td&gt; &#xA;   &lt;td&gt;Hindi (Romanized)&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hrv_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Croatian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hun_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Hungarian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Uralic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hye_Armn&lt;/td&gt; &#xA;   &lt;td&gt;Armenian&lt;/td&gt; &#xA;   &lt;td&gt;Armn&lt;/td&gt; &#xA;   &lt;td&gt;Armenian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ibo_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Igbo&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ilo_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Ilocano&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ind_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Indonesian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;isl_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Icelandic&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ita_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Italian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Romance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;jav_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Javanese&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;jpn_Jpan&lt;/td&gt; &#xA;   &lt;td&gt;Japanese&lt;/td&gt; &#xA;   &lt;td&gt;Jpan&lt;/td&gt; &#xA;   &lt;td&gt;Japonic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kac_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Jingpho&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Sino-Tibetan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kan_Knda&lt;/td&gt; &#xA;   &lt;td&gt;Kannada&lt;/td&gt; &#xA;   &lt;td&gt;Knda&lt;/td&gt; &#xA;   &lt;td&gt;Dravidian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kat_Geor&lt;/td&gt; &#xA;   &lt;td&gt;Georgian&lt;/td&gt; &#xA;   &lt;td&gt;Geor&lt;/td&gt; &#xA;   &lt;td&gt;kartvelian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kaz_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Kazakh&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Turkic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kea_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Kabuverdianu&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Portuguese Creole&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;khk_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Halh Mongolian&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Mongolic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;khm_Khmr&lt;/td&gt; &#xA;   &lt;td&gt;Khmer&lt;/td&gt; &#xA;   &lt;td&gt;Khmr&lt;/td&gt; &#xA;   &lt;td&gt;Austroasiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kin_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Kinyarwanda&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kir_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Kyrgyz&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Turkic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;kor_Hang&lt;/td&gt; &#xA;   &lt;td&gt;Korean&lt;/td&gt; &#xA;   &lt;td&gt;Hang&lt;/td&gt; &#xA;   &lt;td&gt;Koreanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lao_Laoo&lt;/td&gt; &#xA;   &lt;td&gt;Lao&lt;/td&gt; &#xA;   &lt;td&gt;Laoo&lt;/td&gt; &#xA;   &lt;td&gt;Kra-Dai&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lin_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Lingala&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lit_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Lithuanian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lug_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Ganda&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;luo_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Luo&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Nilo-Saharan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lvs_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Standard Latvian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mal_Mlym&lt;/td&gt; &#xA;   &lt;td&gt;Malayalam&lt;/td&gt; &#xA;   &lt;td&gt;Mlym&lt;/td&gt; &#xA;   &lt;td&gt;Dravidian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mar_Deva&lt;/td&gt; &#xA;   &lt;td&gt;Marathi&lt;/td&gt; &#xA;   &lt;td&gt;Deva&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mkd_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Macedonian&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mlt_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Maltese&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mri_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Maori&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mya_Mymr&lt;/td&gt; &#xA;   &lt;td&gt;Burmese&lt;/td&gt; &#xA;   &lt;td&gt;Mymr&lt;/td&gt; &#xA;   &lt;td&gt;Sino-Tibetan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nld_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Dutch&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nob_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Norwegian Bokmål&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;npi_Deva&lt;/td&gt; &#xA;   &lt;td&gt;Nepali&lt;/td&gt; &#xA;   &lt;td&gt;Deva&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;npi_Latn^&lt;/td&gt; &#xA;   &lt;td&gt;Nepali (Romanized)&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nso_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Northern Sotho&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nya_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Nyanja&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ory_Orya&lt;/td&gt; &#xA;   &lt;td&gt;Odia&lt;/td&gt; &#xA;   &lt;td&gt;Orya&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pan_Guru&lt;/td&gt; &#xA;   &lt;td&gt;Eastern Panjabi&lt;/td&gt; &#xA;   &lt;td&gt;Guru&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pbt_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Southern Pashto&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pes_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Western Persian&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Iranian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;plt_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Plateau Malagasy&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pol_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Polish&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;por_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Portuguese&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Romance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ron_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Romanian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Romance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;rus_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Russian&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shn_Mymr&lt;/td&gt; &#xA;   &lt;td&gt;Shan&lt;/td&gt; &#xA;   &lt;td&gt;Mymr&lt;/td&gt; &#xA;   &lt;td&gt;Kra-Dai&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sin_Latn^&lt;/td&gt; &#xA;   &lt;td&gt;Sinhala (Romanized)&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sin_Sinh&lt;/td&gt; &#xA;   &lt;td&gt;Sinhala&lt;/td&gt; &#xA;   &lt;td&gt;Sinh&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;slk_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Slovak&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;slv_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Slovenian&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sna_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Shona&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;snd_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Sindhi&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;som_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Somali&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sot_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Southern Sotho&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;spa_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Spanish&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Romance&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;srp_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Serbian&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ssw_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Swati&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;sun_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Sundanese&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;swe_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Swedish&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Germanic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;swh_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Swahili&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tam_Taml&lt;/td&gt; &#xA;   &lt;td&gt;Tamil&lt;/td&gt; &#xA;   &lt;td&gt;Taml&lt;/td&gt; &#xA;   &lt;td&gt;Dravidian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tel_Telu&lt;/td&gt; &#xA;   &lt;td&gt;Telugu&lt;/td&gt; &#xA;   &lt;td&gt;Telu&lt;/td&gt; &#xA;   &lt;td&gt;Dravidian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tgk_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Tajik&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Iranian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tgl_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Tagalog&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tha_Thai&lt;/td&gt; &#xA;   &lt;td&gt;Thai&lt;/td&gt; &#xA;   &lt;td&gt;Thai&lt;/td&gt; &#xA;   &lt;td&gt;Kra-Dai&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tir_Ethi&lt;/td&gt; &#xA;   &lt;td&gt;Tigrinya&lt;/td&gt; &#xA;   &lt;td&gt;Ethi&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tsn_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Tswana&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tso_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Tsonga&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Afro-Asiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tur_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Turkish&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Turkic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ukr_Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Ukrainian&lt;/td&gt; &#xA;   &lt;td&gt;Cyrl&lt;/td&gt; &#xA;   &lt;td&gt;Balto-Slavic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;urd_Arab&lt;/td&gt; &#xA;   &lt;td&gt;Urdu&lt;/td&gt; &#xA;   &lt;td&gt;Arab&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;urd_Latn^&lt;/td&gt; &#xA;   &lt;td&gt;Urdu (Romanized)&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Indo-Aryan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;uzn_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Northern Uzbek&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Turkic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vie_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Vietnamese&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austroasiatic&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;war_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Waray&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wol_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Wolof&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;xho_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Xhosa&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;yor_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Yoruba&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;zho_Hans&lt;/td&gt; &#xA;   &lt;td&gt;Chinese (Simplified)&lt;/td&gt; &#xA;   &lt;td&gt;Hans&lt;/td&gt; &#xA;   &lt;td&gt;Sino-Tibetan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;zho_Hant&lt;/td&gt; &#xA;   &lt;td&gt;Chinese (Traditional)&lt;/td&gt; &#xA;   &lt;td&gt;Hant&lt;/td&gt; &#xA;   &lt;td&gt;Sino-Tibetan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;zsm_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Standard Malay&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Austronesian&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;zul_Latn&lt;/td&gt; &#xA;   &lt;td&gt;Zulu&lt;/td&gt; &#xA;   &lt;td&gt;Latn&lt;/td&gt; &#xA;   &lt;td&gt;Atlantic-Congo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;^ denotes a language variant not in FLORES-200&lt;/p&gt; &#xA;&lt;h2&gt;Further Stats&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;122 language variants, but 115 distinct languages (ignoring scripts)&lt;/li&gt; &#xA; &lt;li&gt;27 language families&lt;/li&gt; &#xA; &lt;li&gt;29 scripts&lt;/li&gt; &#xA; &lt;li&gt;Avg. words per passage = 79.1 (std = 26.2)&lt;/li&gt; &#xA; &lt;li&gt;Avg. sentences per passage = 4.1 (std = 1.4)&lt;/li&gt; &#xA; &lt;li&gt;Avg. words per question = 12.9(std = 4.0)&lt;/li&gt; &#xA; &lt;li&gt;Avg. words per answer = 4.2 (std = 2.9)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Belebele dataset is licensed under the license found in the LICENSE_CC-BY-SA4.0 file in the root directory of this source tree.&lt;/p&gt; &#xA;&lt;p&gt;The training set and assembly code is, however, licensed differently. The majority of the training set (data and code) is licensed under CC-BY-NC, however portions of the project are available under separate license terms: NLTK is licensed under the Apache 2.0 license; pandas and NumPy are licensed under the BSD 3-Clause License.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this data in your work, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{bandarkar2023belebele,&#xA;      title={The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants}, &#xA;      author={Lucas Bandarkar and Davis Liang and Benjamin Muller and Mikel Artetxe and Satya Narayan Shukla and Donald Husa and Naman Goyal and Abhinandan Krishnan and Luke Zettlemoyer and Madian Khabsa},&#xA;      year={2023},&#xA;      journal={arXiv preprint arXiv:2308.16884}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/evodiff</title>
    <updated>2023-09-17T01:38:46Z</updated>
    <id>tag:github.com,2023-09-17:/microsoft/evodiff</id>
    <link href="https://github.com/microsoft/evodiff" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generation of protein sequences and evolutionary alignments via discrete diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EvoDiff&lt;/h1&gt; &#xA;&lt;h3&gt;Description&lt;/h3&gt; &#xA;&lt;p&gt;In this work, we introduce a general-purpose diffusion framework, EvoDiff, that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models for controllable protein generation in sequence space. EvoDiff generates high-fidelity, diverse, and structurally-plausible proteins that cover natural sequence and functional space. Critically, EvoDiff can generate proteins inaccessible to structure-based models, such as those with disordered regions, while maintaining the ability to design scaffolds for functional structural motifs, demonstrating the universality of our sequence-based formulation. We envision that EvoDiff will expand capabilities in protein engineering beyond the structure-function paradigm toward programmable, sequence-first design.&lt;/p&gt; &#xA;&lt;p&gt;We evaluate our sequence and MSA models – EvoDiff-Seq and EvoDiff-MSA, respectively – across a range of generation tasks to demonstrate their power for controllable protein design. Below, we provide documentation for running our models.&lt;/p&gt; &#xA;&lt;p&gt;EvoDiff is described in this &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2023.09.11.556673v1&#34;&gt;preprint&lt;/a&gt;; if you use the code from this repository or the results, please cite the preprint.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/img/combined-0.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#EvoDiff&#34;&gt;Evodiff&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#table-of-contents&#34;&gt;Table of contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#installation&#34;&gt;Installation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#loading-pretrained-models&#34;&gt;Loading pretrained models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#available-models&#34;&gt;Available models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#unconditional-sequence-generation&#34;&gt;Unconditional generation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#unconditional-generation-with-evodiff-seq&#34;&gt;Unconditional sequence generation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#unconditional-generation-with-evodiff-msa&#34;&gt;Unconditional MSA generation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#conditional-sequence-generation&#34;&gt;Conditional sequence generation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#evolution-guided-protein-generation-with-evodiff-msa&#34;&gt;Evolution-guided protein generation with EvoDiff-MSA&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#generating-intrinsically-disordered-regions&#34;&gt;Generating intrinsically disordered regions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#scaffolding-functional-motifs&#34;&gt;Scaffolding functional motifs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#analysis-of-generations&#34;&gt;Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#downloading-generated-sequences&#34;&gt;Downloading generated sequences&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To download our code, we recommend creating a clean conda environment with python &lt;code&gt;v3.8.5&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name evodiff python=3.8.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In that new environment, install EvoDiff:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install evodiff&#xA;pip install git+https://github.com/microsoft/evodiff.git # bleeding edge, current repo main branch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need to install PyTorch (we tested our models on &lt;code&gt;v2.0.1&lt;/code&gt;), PyTorch Geometric, and PyTorch Scatter.&lt;/p&gt; &#xA;&lt;p&gt;We provide a notebook with installation guidance that can be found in &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/examples/evodiff.ipynb&#34;&gt;examples/evodiff.ipynb&lt;/a&gt;. It also includes examples on how to generate a smaller number of sequences and MSAs using our models. We recommend following this notebook if you would like to use our models to generate proteins.&lt;/p&gt; &#xA;&lt;p&gt;Our downstream analysis scripts make use of a variety of tools we do not include in our package installation. To run the scripts, please download the following packages in addition to EvoDiff:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhanggroup.org/TM-score/&#34;&gt;TM score&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HeliXonProtein/OmegaFold&#34;&gt;Omegafold&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dauparas/ProteinMPNN&#34;&gt;ProteinMPNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/esm/tree/main/esm/inverse_folding&#34;&gt;ESM-IF1&lt;/a&gt;; see this &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/inverse_folding/notebook.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; for setup details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hefeda/PGP&#34;&gt;PGP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/maslov-group/DR-BERT&#34;&gt;DR-BERT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We refer to the setup instructions outlined by the authors of those tools.&lt;/p&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We obtain sequences from the &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4375400/&#34;&gt;Uniref50 dataset&lt;/a&gt;, which contains approximately 42 million protein sequences. The Multiple Sequence Alignments (MSAs) are from the &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2&#34;&gt;OpenFold dataset&lt;/a&gt;, which contains 401,381 MSAs for 140,000 unique Protein Data Bank (PDB) chains and 16,000,000 UniClust30 clusters. The intrinsically disordered regions (IDR) data was obtained from the &lt;a href=&#34;https://github.com/alexxijielu/reverse_homology/&#34;&gt;Reverse Homology GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For the scaffolding structural motifs task, we use the baselines compiled in RFDiffusion. We provide pdb and fasta files used for conditionally generating sequences in the &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/examples/scaffolding-pdbs&#34;&gt;examples/scaffolding-pdbs&lt;/a&gt; folder. We also provide We provide pdb files used for conditionally generating MSAs in the &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/examples/scaffolding-msas&#34;&gt;examples/scaffolding-msas&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;To access the UniRef50 test sequences, use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;test_data = UniRefDataset(&#39;data/uniref50/&#39;, &#39;rtest&#39;, structure=False) # To access the test sequences&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The filenames for train and validation Openfold splits are saved in &lt;code&gt;data/valid_msas.csv&lt;/code&gt; and &lt;code&gt;data/train_msas.csv&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Loading pretrained models&lt;/h3&gt; &#xA;&lt;p&gt;To load a model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from evodiff.pretrained import OA_DM_38M&#xA;&#xA;model, collater, tokenizer, scheme = OA_DM_38M()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available evodiff models are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;D3PM_BLOSUM_640M()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;D3PM_BLOSUM_38M()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;D3PM_UNIFORM_640M()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;D3PM_UNIFORM_38M()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;OA_DM_640M()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;OA_DM_38M()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MSA_D3PM_BLOSUM_RANDSUB()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MSA_D3PM_BLOSUM_MAXSUB()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MSA_D3PM_UNIFORM_RANDSUB()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MSA_D3PM_UNIFORM_MAXSUB()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MSA_OA_DM_RANDSUB()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MSA_OA_DM_MAXSUB()&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It is also possible to load our LRAR baseline models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;LR_AR_640M()&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LR_AR_38M()&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: if you want to download a &lt;code&gt;BLOSUM&lt;/code&gt; model, you will first need to download &lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/data/blosum62-special-MSA.mat&#34;&gt;data/blosum62-special-MSA.mat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Available models&lt;/h2&gt; &#xA;&lt;p&gt;We investigated two types of forward processes for diffusion over discrete data modalitiesto determine which would be most effective. In order-agnostic autoregressive diffusion &lt;a href=&#34;https://arxiv.org/abs/2110.02037&#34;&gt;OADM&lt;/a&gt;, one amino acid is converted to a special mask token at each step in the forward process. After $T=L$ steps, where $L$ is the length of the sequence, the entire sequence is masked. We additionally designed discrete denoising diffusion probabilistic models &lt;a href=&#34;https://arxiv.org/abs/2107.03006&#34;&gt;D3PM&lt;/a&gt; for protein sequences. In EvoDiff-D3PM, the forward process corrupts sequences by sampling mutations according to a transition matrix, such that after $T$ steps the sequence is indistinguishable from a uniform sample over the amino acids. In the reverse process for both, a neural network model is trained to undo the previous corruption. The trained model can then generate new sequences starting from sequences of masked tokens or of uniformly-sampled amino acids for EvoDiff-OADM or EvoDiff-D3PM, respectively. We trained all EvoDiff sequence models on 42M sequences from UniRef50 using a dilated convolutional neural network architecture introduced in the &lt;a href=&#34;https://doi.org/10.1101/2022.05.19.492714&#34;&gt;CARP&lt;/a&gt; protein masked language model. We trained 38M-parameter and 640M-parameter versions for each forward corruption scheme and for left-to-right autoregressive (LRAR) decoding.&lt;/p&gt; &#xA;&lt;p&gt;To explicitly leverage evolutionary information, we designed and trained EvoDiff MSA models using the &lt;a href=&#34;https://proceedings.mlr.press/v139/rao21a.html&#34;&gt;MSA Transformer&lt;/a&gt; architecture on the &lt;a href=&#34;https://github.com/aqlaboratory/openfold&#34;&gt;OpenFold&lt;/a&gt; dataset}. To do so, we subsampled MSAs to a length of 512 residues per sequence and a depth of 64 sequences, either by randomly sampling the sequences (&#34;Random&#34;) or by greedily maximizing for sequence diversity (&#34;Max&#34;). Within each subsampling strategy, we then trained EvoDiff MSA models with the OADM and D3PM corruption schemes.&lt;/p&gt; &#xA;&lt;h2&gt;Unconditional sequence generation&lt;/h2&gt; &#xA;&lt;h3&gt;Unconditional generation with EvoDiff-Seq&lt;/h3&gt; &#xA;&lt;p&gt;EvoDiff can generate new sequences starting from sequences of masked tokens or of uniformly-sampled amino acids. All available models can be used to unconditionally generate new sequences, without needing to download the training datasets.&lt;/p&gt; &#xA;&lt;p&gt;To unconditionally generate 100 sequences from EvoDiff-Seq, run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python evodiff/generate.py --model-type oa_dm_38M --num-seqs 100 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default model type is &lt;code&gt;oa_dm_640M&lt;/code&gt;, other evodiff models available are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;oa_dm_38M&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;d3pm_blosum_38M&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;d3pm_blosum_640M&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;d3pm_uniform_38M&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;d3pm_uniform_640M&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our LRAR baseline models are also available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;lr_ar_38M&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;lr_ar_640M&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example of unconditionally generating a sequence of a specified length can be found in &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/examples/evodiff.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To evaluate the generated sequences, we implement our self-consistency Omegafold ESM-IF pipeline, as shown in &lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/self_consistency_analysis.py&#34;&gt;analysis/self_consistency_analysis.py&lt;/a&gt;. To use this evaluation script, you must have the dependencies listed under the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/evodiff/main/#installation&#34;&gt;Installation&lt;/a&gt; section installed.&lt;/p&gt; &#xA;&lt;h3&gt;Unconditional generation with EvoDiff-MSA&lt;/h3&gt; &#xA;&lt;p&gt;To explicitly leverage evolutionary information, we design and train EvoDiff-MSA models using the MSA Transformer architecture on the OpenFold dataset. To do so, we subsample MSAs to a length of 512 residues per sequence and a depth of 64 sequences, either by randomly sampling the sequences (“Random”) or by greedily maximizing for sequence diversity (“Max”).&lt;/p&gt; &#xA;&lt;p&gt;It is possible to unconditionally generate an entire MSA, using the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python evodiff/generate-msa.py --model-type msa_oa_dm_maxsub --batch-size 1 --n-sequences 64 --n-sequences 256 --subsampling MaxHamming&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default model type is &lt;code&gt;msa_oa_dm_maxsub&lt;/code&gt;, which is EvoDiff-MSA-OADM trained on Max subsampled sequences, and the other available evodiff models are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;EvoDiff-MSA OADM trained on random subsampled sequences: &lt;code&gt;msa_oa_dm_randsub&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;EvoDiff-MSA D3PM-BLOSUM trained on Max subsampled sequences:&lt;code&gt;msa_d3pm_blosum_maxsub&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;EvoDiff-MSA D3PM-BLOSUM trained on random subsampled sequences: &lt;code&gt;msa_d3pm_blosum_randsub&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;EvoDiff-MSA D3PM-Uniform trained on Max subsampled sequences: &lt;code&gt;msa_d3pm_uniform_maxsub&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;EvoDiff-MSA D3PM-Uniform trained on random subsampled sequences: &lt;code&gt;msa_d3pm_uniform_randsub&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also specify a desired number of sequences per MSA, sequence length, batch size, and more.&lt;/p&gt; &#xA;&lt;h2&gt;Conditional sequence generation&lt;/h2&gt; &#xA;&lt;p&gt;EvoDiff’s OADM diffusion framework induces a natural method for conditional sequence generation by fixing some subsequences and predicting the remainder. Because the model is trained to generate proteins with an arbitrary decoding order, this is easily accomplished by simply masking and decoding the desired portions. We apply EvoDiff’s power for controllable protein design across three scenarios: conditioning on evolutionary information encoded in MSAs, inpainting functional domains, and scaffolding structural motifs.&lt;/p&gt; &#xA;&lt;h3&gt;Evolution-guided protein generation with EvoDiff-MSA&lt;/h3&gt; &#xA;&lt;p&gt;First, we test the ability of EvoDiff-MSA (&lt;code&gt;msa_oa_dm_maxsub&lt;/code&gt;) to generate new query sequences conditioned on the remainder of an MSA, thus generating new members of a protein family without needing to train family-specific generative models.&lt;/p&gt; &#xA;&lt;p&gt;To generate a new query sequence, given an alignment, use the following with the &lt;code&gt;--start-msa&lt;/code&gt; flag. This starts conditional generation by sampling from a validation MSA. To run this script you must have the Openfold dataset and splits downloaded.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python evodiff/generate-msa.py --model-type msa_oa_dm_maxsub --batch-size 1 --n-sequences 64 --n-sequences 256 --subsampling MaxHamming --start-msa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to generate on a custom MSA, it is possible to retrofit existing code.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, the code is capable of generating an alignment given a query sequence, use the following &lt;code&gt;--start-query&lt;/code&gt; flag. This starts with the query and generates the alignment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python evodiff/generate-msa.py --model-type msa_oa_dm_maxsub --batch-size 1 --n-sequences 64 --n-sequences 256 --subsampling MaxHamming --start-query&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: you can only specify one of the above flags at a time. You cannot specify both (&lt;code&gt;--start-query&lt;/code&gt; &amp;amp; &lt;code&gt;--start-msa&lt;/code&gt;) together. Please look at &lt;code&gt;generate.py&lt;/code&gt; for more information.&lt;/p&gt; &#xA;&lt;h3&gt;Generating intrinsically disordered regions&lt;/h3&gt; &#xA;&lt;p&gt;Because EvoDiff generates directly in sequence space, we hypothesized that it could natively generate intrinsically disordered regions (IDRs). IDRs are regions within a protein sequence that lack secondary or tertiary structure, and they carry out important and diverse functional roles in the cell directly facilitated by their lack of structure. Despite their prevalence and critical roles in function and disease, IDRs do not fit neatly in the structure-function paradigm and remain outside the capabilities of structure-based protein design methods.&lt;/p&gt; &#xA;&lt;p&gt;We used inpainting with EvoDiff-Seq and EvoDiff-MSA to intentionally generate disordered regions conditioned on their surrounding structured regions, and then used DR-BERT to predict disorder scores for each residue in the generated and natural sequences. Note: to generate with our scripts here, you must have the IDR dataset downloaded. Different pre-processing steps may apply to other datasets.&lt;/p&gt; &#xA;&lt;p&gt;To run our code and generate IDRs from EvoDiff-Seq, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate/conditional_generation_msa.py --model-type msa_oa_ar_maxsub --cond-task idr --num-seqs 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or equivalently, from EvoDiff-MSA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate/conditional_generation_msa.py --model-type msa_oa_ar_maxsub --cond-task idr --query-only --max-seq-len 150 --num-seqs 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which will sample IDRs from the IDR dataset, and generate new ones.&lt;/p&gt; &#xA;&lt;h3&gt;Scaffolding functional motifs&lt;/h3&gt; &#xA;&lt;p&gt;Given that the fixed functional motif includes the residue identities for the motif, we show that a sequence-only model can be used for a motif scaffolding task. We used EvoDiff to generate scaffolds for a set of 17 motif-scaffolding problems by fixing the functional motif, supplying only the motif&#39;s amino-acid sequence as conditioning information, and then decoding the remainder of the sequence.&lt;/p&gt; &#xA;&lt;p&gt;For the scaffolding structural motifs task, we provide pdb and fasta files used for conditionally generating sequences in the &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/examples/scaffolding-pdbs&#34;&gt;examples/scaffolding-pdbs&lt;/a&gt; folder. We also provide We provide a3m files used for conditionally generating MSAs in the &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/examples/scaffolding-msas&#34;&gt;examples/scaffolding-msas&lt;/a&gt; folder. Please view the PDB codes available and select an appropriate code. In this example, we use PDB code 1prw with domains 16-35 (FSLFDKDGDGTITTKELGTV) and 52-71 (INEVDADGNGTIDFPEFLTM). An example of generating 1 MSA scaffold of a structural motif can be found in &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/examples/evodiff.ipynb&#34;&gt;this notebook&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To generate from EvoDiff-Seq:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate/conditional_generation.py --model-type oa_dm_640M --cond-task scaffold --pdb 1prw --start-idxs 15 --end-idxs 34 --start-idxs 51 --end-idxs 70 --num-seqs 100 --scaffold-min 50 --scaffold-max 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;--start-idxs&lt;/code&gt; and &lt;code&gt;--end-idxs&lt;/code&gt; indicate the start &amp;amp; end indices for the motif being scaffolded. If defining multiple motifs, you can supply the start and end index motifs as new arguments, such as in the example we provide above.&lt;/p&gt; &#xA;&lt;p&gt;Equivalent code for generating a new scaffold sequence from an EvoDiff-MSA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python generate/conditional_generation_msa.py --model-type msa_oa_dm_maxsub --cond-task scaffold --pdb 1prw --start-idxs 15 --end-idxs 34 --start-idxs 51 --end-idxs 70 --num-seqs 1 --query-only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate a custom scaffold for a given motif, one simply needs to supply the PDB ID, and the residue indices of the motif. The code will download the PDB for you. In some cases PDB files downloaded from rcsb will be incomplete, or contain additional residues. We have implemented code to circumvent PDB-reading issues, but we recommend care when generating files for this task.&lt;/p&gt; &#xA;&lt;h2&gt;Analysis of generations&lt;/h2&gt; &#xA;&lt;p&gt;To analyze the quality of the generations, we look at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;amino acid KL divergence (&lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/evodiff/plot.py&#34;&gt;aa_reconstruction_parity_plot&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;secondary structure KL divergence (&lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/calc_kl_ss.py&#34;&gt;evodiff/analysis/calc_kl_ss.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;model perplexity for sequences (&lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/sequence_perp.py&#34;&gt;evodiff/analysis/sequence_perp.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;model perplexity for MSAs (&lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/msa_perp.py&#34;&gt;evodiff/analysis/msa_perp.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fréchet inception distance (&lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/calc_fid.py&#34;&gt;evodiff/analysis/calc_fid.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Hamming distance (&lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/calc_nearestseq_hamming.py&#34;&gt;evodiff/analysis/calc_nearestseq_hamming.py&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;RMSD score (&lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/rmsd_analysis.py&#34;&gt;analysis/rmsd_analysis.py&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We also compute the self-consistency perplexity to evaluate the foldability of generated sequences. To do so, we make use of various tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhanggroup.org/TM-score/&#34;&gt;TM score&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HeliXonProtein/OmegaFold&#34;&gt;Omegafold&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dauparas/ProteinMPNN&#34;&gt;ProteinMPNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/esm/tree/main/esm/inverse_folding&#34;&gt;ESM-IF1&lt;/a&gt;; see this &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/inverse_folding/notebook.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; for setup details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hefeda/PGP&#34;&gt;PGP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/maslov-group/DR-BERT&#34;&gt;DR-BERT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We refer to the setup instructions outlined by the authors of those tools.&lt;/p&gt; &#xA;&lt;p&gt;Our analysis scripts for iterating over these tools are in the &lt;a href=&#34;https://github.com/microsoft/evodiff/tree/main/analysis/downstream_bash_scripts&#34;&gt;evodiff/analysis/downstream_bash_scripts&lt;/a&gt; folder. Once we run the scripts in this folder, we analyze the results in &lt;a href=&#34;https://github.com/microsoft/evodiff/raw/main/analysis/self_consistency_analysis.py&#34;&gt;self_consistency_analysis.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading generated sequences&lt;/h2&gt; &#xA;&lt;p&gt;We provide all generated sequences on the &lt;a href=&#34;https://zenodo.org/record/8332830&#34;&gt;EvoDiff Zenodo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To download our unconditional generated sequences from &lt;code&gt;unconditional_generations.csv&lt;/code&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -O https://zenodo.org/record/8332830/files/unconditional_generations.csv?download=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To extract all unconditionally generated sequences created using the EvoDiff-seq &lt;code&gt;oa_dm_640M&lt;/code&gt; model, run the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;df = pd.read_csv(&#39;unconditional_generations.csv&#39;, index_col = 0)&#xA;subset = df.loc[df[&#39;model&#39;] == &#39;evodiff_oa_dm_640M&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The CSV files containing generated data are organized as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Unconditional generations from sequence based models: &lt;code&gt; unconditional_generations.csv&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;sequence&lt;/code&gt;: generated sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;min hamming dist&lt;/code&gt;: minimum Hamming distance between generated sequence and all training sequences&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seq len&lt;/code&gt;: length of generated sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: model type used for generations, models: &lt;code&gt;evodiff_oa_dm_38M&lt;/code&gt;, &lt;code&gt;evodiff_oa_dm_640M&lt;/code&gt;, &lt;code&gt;evodiff_d3pm_uniform_38M&lt;/code&gt;, &lt;br&gt; &lt;code&gt;evodiff_d3pm_uniform_640M&lt;/code&gt;, &lt;code&gt;evodiff_d3pm_blosum_38M&lt;/code&gt;, &lt;code&gt;evodiff_d3pm_blosum_640M&lt;/code&gt;, &lt;code&gt;carp_38M&lt;/code&gt;, &lt;code&gt;carp_640M&lt;/code&gt;, &lt;code&gt;lr_ar_38M&lt;/code&gt; &lt;br&gt; &lt;code&gt;lr_ar_38M&lt;/code&gt;, &lt;code&gt;lr_ar_640M&lt;/code&gt;, &lt;code&gt;esm_1b&lt;/code&gt;, or &lt;code&gt;esm_2&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Sequence predictions for unconditional structure generation baselines: &lt;code&gt; esmif_predictions_unconditional_structure_generations.csv&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;sequence&lt;/code&gt;: predicted protein sequence from protein structure (using ESM-IF1 model)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seq len&lt;/code&gt;: length of generated sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: &#39;foldingdiff&#39; or &#39;rfdiffusion&#39;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Sequence generation via evolutionary alignments: &lt;code&gt; msa_evolution_conditional_generations.csv&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;sequence&lt;/code&gt;: generated query sequences&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seq len&lt;/code&gt;: length of generated sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: model type used for generations: &lt;code&gt;evodiff_msa_oa_dm_maxsub&lt;/code&gt;, &lt;code&gt;evodiff_msa_oa_dm_randsub&lt;/code&gt;, &lt;code&gt;esm_msa_1b&lt;/code&gt;, or &lt;code&gt;potts&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Generated IDRs: &lt;code&gt; idr_conditional_generations.csv&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;sequence&lt;/code&gt;: subsampled sequence that contains IDR&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seq len&lt;/code&gt;: length of generated sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;gen_idrs&lt;/code&gt;: the generated IDR sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;original_idrs&lt;/code&gt;: the original IDR sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;start_idxs&lt;/code&gt;: indices corresponding to start of IDR in sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;end_idxs&lt;/code&gt;: indices corresponding to end of IDR in sequence (inclusive)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: model type used for generations &lt;code&gt;evodiff_seq_oa_dm_640M&lt;/code&gt; or &lt;code&gt;evodiff_msa_oa_dm_maxsub&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Successfully generated scaffolds &lt;code&gt; msa_scaffold.csv&lt;/code&gt; (EvoDiff-MSA generations) or &lt;code&gt;seq_scaffold.csv&lt;/code&gt; (Evodiff-Seq generations) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;pdb&lt;/code&gt;: pdb code corresponding to scaffold task&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seqs&lt;/code&gt;: generated scaffold and motif&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;start_idxs&lt;/code&gt;: indices corresponding to start of motif&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;end_idxs&lt;/code&gt;: indices corresponding to end of motif&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seq len&lt;/code&gt;: length of generated sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;scores&lt;/code&gt;: average predicted local distance difference test (pLDDT) of sequence&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;rmsd&lt;/code&gt;: motifRMSD between predicted motif coordinates and crystal motif coordinates&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: model type used for generations&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos are subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third party trademarks or logos is subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
</feed>