<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-10T01:37:22Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>joweich/chat-miner</title>
    <updated>2022-11-10T01:37:22Z</updated>
    <id>tag:github.com,2022-11-10:/joweich/chat-miner</id>
    <link href="https://github.com/joweich/chat-miner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Lean parsers and visualizations for chat data&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;chat-miner: parsing of chat histories&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;chat-miner parses chat logs into a pandas dataframe. As of now, &lt;strong&gt;WhatsApp&lt;/strong&gt;, &lt;strong&gt;Signal&lt;/strong&gt;, &lt;strong&gt;Telegram&lt;/strong&gt;, and &lt;strong&gt;Facebook Messenger&lt;/strong&gt; export files are supported.&lt;/p&gt; &#xA;&lt;h2&gt;Export chat data&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://faq.whatsapp.com/196737011380816/?cms_id=196737011380816&amp;amp;published_only=true&#34;&gt;WhatsApp (via mobile app)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/carderne/signal-export&#34;&gt;Signal (via desktop app)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://telegram.org/blog/export-and-more&#34;&gt;Telegram (via desktop app)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.facebook.com/help/messenger-app/713635396288741?cms_id=713635396288741&amp;amp;published_only=true&#34;&gt;Facebook Messenger (via mobile app)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Following code showcases the &lt;code&gt;WhatsAppParser&lt;/code&gt; module. The usage of &lt;code&gt;SignalParser&lt;/code&gt;, &lt;code&gt;TelegramJsonParser&lt;/code&gt;, and &lt;code&gt;FacebookMessengerParser&lt;/code&gt; follows the same pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chatminer.chatparsers import WhatsAppParser&#xA;&#xA;parser = WhatsAppParser(FILEPATH)&#xA;parser.parse_file_into_df()&#xA;print(parser.df.describe())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example visualizations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import chatminer.visualizations as vis&#xA;vis.sunburst(parser.df)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/joweich/chat-miner/main/examples/sunburst.png&#34; alt=&#34;Sunburst&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import chatminer.visualizations as vis&#xA;stopwords = [&#39;media&#39;, &#39;omitted&#39;, &#39;missed&#39;, &#39;voice&#39;, &#39;call&#39;]&#xA;vis.wordcloud(parser.df, stopwords)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/joweich/chat-miner/main/examples/wordcloud.png&#34; alt=&#34;Wordcloud&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cloneofsimo/paint-with-words-sd</title>
    <updated>2022-11-10T01:37:22Z</updated>
    <id>tag:github.com,2022-11-10:/cloneofsimo/paint-with-words-sd</id>
    <link href="https://github.com/cloneofsimo/paint-with-words-sd" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Paint-with-words with Stable Diffusion : method from eDiffi that let you generate image from text-labeled segmentation map.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Paint-with-Words, Implemented with Stable diffusion&lt;/h1&gt; &#xA;&lt;h2&gt;Subtle Control of the Image Generation&lt;/h2&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/rabbit_mage.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice how without PwW the cloud is missing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/road.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice how without PwW, abandoned city is missing, and road becomes purple as well.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Shift the object : Same seed, just the segmentation map&#39;s positional difference&lt;/h2&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/aurora_1_merged.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/aurora_2_merged.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;A digital painting of a half-frozen lake near mountains under a full moon and aurora. A boat is in the middle of the lake. Highly detailed.&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Notice how nearly all of the composition remains the same, other than the position of the moon.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Recently, researchers from NVIDIA proposed &lt;a href=&#34;https://arxiv.org/abs/2211.01324&#34;&gt;eDiffi&lt;/a&gt;. In the paper, they suggested method that allows &#34;painting with word&#34;. Basically, this is like make-a-scene, but with just using adjusted cross-attention score. You can see the results and detailed method in the paper.&lt;/p&gt; &#xA;&lt;p&gt;Their paper and their method was not open-sourced. Yet, paint-with-words can be implemented with Stable Diffusion since they share common Cross Attention module. So, I implemented it with Stable Diffusion.&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/paint_with_words_figure.png&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/cloneofsimo/paint-with-words-sd.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Basic Usage&lt;/h1&gt; &#xA;&lt;p&gt;Before running, fill in the variable &lt;code&gt;HF_TOKEN&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; file with Huggingface token for Stable Diffusion, and load_dotenv().&lt;/p&gt; &#xA;&lt;p&gt;Prepare segmentation map, and map-color : tag label such as below. keys are (R, G, B) format, and values are tag label.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;    (0, 0, 0): &#34;cat,1.0&#34;,&#xA;    (255, 255, 255): &#34;dog,1.0&#34;,&#xA;    (13, 255, 0): &#34;tree,1.5&#34;,&#xA;    (90, 206, 255): &#34;sky,0.2&#34;,&#xA;    (74, 18, 1): &#34;ground,0.2&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You neeed to have them so that they are in format &#34;{label},{strength}&#34;, where strength is additional weight of the attention score you will give during generation, i.e., it will have more effect.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import dotenv&#xA;from PIL import Image&#xA;&#xA;from paint_with_words import paint_with_words&#xA;&#xA;settings = {&#xA;    &#34;color_context&#34;: {&#xA;        (0, 0, 0): &#34;cat,1.0&#34;,&#xA;        (255, 255, 255): &#34;dog,1.0&#34;,&#xA;        (13, 255, 0): &#34;tree,1.5&#34;,&#xA;        (90, 206, 255): &#34;sky,0.2&#34;,&#xA;        (74, 18, 1): &#34;ground,0.2&#34;,&#xA;    },&#xA;    &#34;color_map_img_path&#34;: &#34;contents/example_input.png&#34;,&#xA;    &#34;input_prompt&#34;: &#34;realistic photo of a dog, cat, tree, with beautiful sky, on sandy ground&#34;,&#xA;    &#34;output_img_path&#34;: &#34;contents/output_cat_dog.png&#34;,&#xA;}&#xA;&#xA;&#xA;dotenv.load_dotenv()&#xA;&#xA;color_map_image = Image.open(settings[&#34;color_map_img_path&#34;]).convert(&#34;RGB&#34;)&#xA;color_context = settings[&#34;color_context&#34;]&#xA;input_prompt = settings[&#34;input_prompt&#34;]&#xA;&#xA;img = paint_with_words(&#xA;    color_context=color_context,&#xA;    color_map_image=color_map_image,&#xA;    input_prompt=input_prompt,&#xA;    num_inference_steps=30,&#xA;    guidance_scale=7.5,&#xA;    device=&#34;cuda:0&#34;,&#xA;)&#xA;&#xA;img.save(settings[&#34;output_img_path&#34;])&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There is minimal working example in &lt;code&gt;runner.py&lt;/code&gt; that is self contained. Please have a look!&lt;/p&gt; &#xA;&lt;h1&gt;Weight Scaling&lt;/h1&gt; &#xA;&lt;p&gt;In the paper, they used $w \log (1 + \sigma) \max (Q^T K)$ to scale appropriate attention weight. However, this wasn&#39;t optimal after few tests, found by &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/4406&#34;&gt;CookiePPP&lt;/a&gt;. You can check out the effect of the functions below:&lt;/p&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_max.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) \max (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_log2_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma^2) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can define your own weight function and further tweak the configurations by defining &lt;code&gt;weight_function&lt;/code&gt; argument in &lt;code&gt;paint_with_words&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w_f = lambda w, sigma, qk: 0.4 * w * math.log(sigma**2 + 1) * qk.std()&#xA;&#xA;img = paint_with_words(&#xA;    color_context=color_context,&#xA;    color_map_image=color_map_image,&#xA;    input_prompt=input_prompt,&#xA;    num_inference_steps=20,&#xA;    guidance_scale=7.5,&#xA;    device=&#34;cuda:0&#34;,&#xA;    preloaded_utils=loaded,&#xA;    weight_function=w_f&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More on the weight function, (but higher)&lt;/h2&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_4_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_4_max.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma) \max (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- #region --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cloneofsimo/paint-with-words-sd/master/contents/compare_4_log2_std.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- #endregion --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;$w&#39; = w \log (1 + \sigma^2) std (Q^T K)$&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>d8ahazard/sd_dreambooth_extension</title>
    <updated>2022-11-10T01:37:22Z</updated>
    <id>tag:github.com,2022-11-10:/d8ahazard/sd_dreambooth_extension</id>
    <link href="https://github.com/d8ahazard/sd_dreambooth_extension" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dreambooth Extension for Stable-Diffusion-WebUI&lt;/h1&gt; &#xA;&lt;p&gt;This is a WIP port of &lt;a href=&#34;https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth&#34;&gt;Shivam Shriao&#39;s Diffusers Repo&lt;/a&gt;, which is a modified version of the default &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;Huggingface Diffusers Repo&lt;/a&gt; optimized for better performance on lower-VRAM GPUs.&lt;/p&gt; &#xA;&lt;p&gt;It also adds several other features, including training multiple concepts simultaneously, and (Coming soon) Inpainting training.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install, simply go to the &#34;Extensions&#34; tab in the SD Web UI, select the &#34;Available&#34; sub-tab, pick &#34;Load from:&#34; to load the list of extensions, and finally, click &#34;install&#34; next to the Dreambooth entry.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1633844/200368737-7fe322de-00d6-4b28-a321-5e09f072d397.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;For 8bit adam to run properly, it may be necessary to install the CU116 version of torch and torchvision, which can be accomplished below:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Edit your webui-user.bat file, add this line after &#39;set COMMANDLINE_ARGS=&#39;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;set TORCH_COMMAND=&#34;pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once installed, restart the SD-WebUI &lt;em&gt;entirely&lt;/em&gt;, not just the UI. This will ensure all the necessary requirements are installed.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Create a Model&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to the Dreambooth tab.&lt;/li&gt; &#xA; &lt;li&gt;Under the &#34;Create Model&#34; sub-tab, enter a new model name and select the source checkpoint to train from. The source checkpoint will be extracted to models\dreambooth\MODELNAME\working - the original will not be touched.&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Create&#34;. This will take a minute or two, but when done, the UI should indicate that a new model directory has been set up.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Training (Basic Settings)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;After creating a new model, select the new model name from the &#34;Model&#34; dropdown at the very top.&lt;/li&gt; &#xA; &lt;li&gt;Select the &#34;Train Model&#34; sub-tab.&lt;/li&gt; &#xA; &lt;li&gt;Fill in the paramters as described below:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;Concepts List&lt;/em&gt; - The path to a JSON file or a JSON string containing multiple concepts. See &lt;a href=&#34;https://raw.githubusercontent.com/d8ahazard/sd_dreambooth_extension/main/dreambooth/concepts_list.json&#34;&gt;here&lt;/a&gt; for an example.&lt;/p&gt; &#xA;&lt;p&gt;If a concepts list is specified, then the instance prompt, class prompt, instance data dir, and class data dir fields will be ignored.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Instance Prompt&lt;/em&gt; - A short descriptor of your subject using a UNIQUE keyword and a classifier word. If training a dog, your instance prompt could be &#34;photo of zkz dog&#34;. The key here is that &#34;zkz&#34; is not a word that might overlap with something in the real world &#34;fluff&#34;, and &#34;dog&#34; is a generic word to describe your subject. This is only necessary if using prior preservation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Class Prompt&lt;/em&gt; - A keyword indicating what type of &#34;thing&#34; your subject is. If your instance prompt is &#34;photo of zkz dog&#34;, your class prompt would be &#34;photo of a dog&#34;. Leave this blank to disable prior preservation training.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Dataset Directory&lt;/em&gt; - The path to the directory where the images described in Instance Prompt are kept. &lt;em&gt;REQUIRED&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Classification dataset directory&lt;/em&gt; - The path to the directory where the images described in Class Prompt are kept. If a class prompt is specified and this is left blank, images will be generated to /models/dreambooth/MODELNAME/classifiers/&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Total number of classification images to use&lt;/em&gt; - Leave at 0 to disable prior preservation. For best results you want ~n*10 classification images - so if you have 40 training photos, then set this to 400. This is just a guess.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Batch size&lt;/em&gt; - How many training steps to process simultaneously. You probably want to leave this at 1.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Class batch size&lt;/em&gt; - How many classification images to generate simultaneously. Set this to whatever you can safely process at once using Txt2Image, or just leave it alone.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Learning rate&lt;/em&gt; - You probably don&#39;t want to touch this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Resolution&lt;/em&gt; - The resolution to train images at. You probably want to keep this number at 512 or lower unless your GPU is insane. Lowering this (and the resolution of training images) may help with lower-VRAM GPUs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Save a checkpoint every N steps&lt;/em&gt; - How frequently to save a checkpoint from the trained data. I should probably change the default of this to 1000.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Generate a preview image every N steps&lt;/em&gt; - How frequently will an image be generated as an example of training progress.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Preview image prompt&lt;/em&gt; - The prompt to use to generate preview image. Leave blank to use the instance prompt.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Preview image negative prompt&lt;/em&gt; - Like above, but negative. Leave blank to do nothing. :P&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Number of samples to generate&lt;/em&gt; - Self explainatory?&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Sample guidance scale&lt;/em&gt; - Like CFG Scale in Txt2Image/Img2Img, used for generating preview.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Sample steps&lt;/em&gt; - Same as sample guidance scale, but the number of steps to run to generate preview. According to (this guide)[https://github.com/nitrosocke/dreambooth-training-guide], you should train for appx 100 steps per sample image. So, if you have 40 instance/sample images, you would train for 4k steps. This is, of course, a rough approximation, and other values will have an effect on final output fidelity.&lt;/p&gt; &#xA;&lt;h3&gt;Advanced Settings&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Use CPU Only&lt;/em&gt; - As indicated, this is more of a last resort if you can&#39;t get it to train with any other settings. Also, as indicated, it will be abysmally slow. Also, you &lt;em&gt;cannot&lt;/em&gt; use 8Bit-Adam with CPU Training, or you&#39;ll have a bad time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Don&#39;t Cache Latents&lt;/em&gt; - Enabling will save a bit of VRAM at the cost of a bit of speed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Train Text Encoder&lt;/em&gt; - Not required, but recommended. Enabling this will probably cost a bit more VRAM, but also purportedly increase output image fidelity.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Use 8Bit Adam&lt;/em&gt; - Enable this to save VRAM. Should now work on both windows and Linux without needing WSL.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Center Crop&lt;/em&gt; - Crop images if they aren&#39;t the right dimensions? I don&#39;t use this, and I recommend you just crop your images &#34;right&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Gradient Checkpointing&lt;/em&gt; - Enable this to save VRAM at the cost of a bit of speed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Scale Learning Rate&lt;/em&gt; - I don&#39;t use this, not sure what impact it has on performance or output quality.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Mixed Precision&lt;/em&gt; - Set to &#39;fp16&#39; to save VRAM at the cost of speed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Everything after &#39;Mixed Precision&#39;&lt;/em&gt; - Adjust at your own risk. Performance/quality benefits from changing these remain to be tested.&lt;/p&gt; &#xA;&lt;p&gt;The next two were added after I wrote the above bit, so just ignore me being a big liar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Pad Tokens&lt;/em&gt; - Pads the text tokens to a longer length for some reason.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Apply Horizontal Flip&lt;/em&gt; - &#34;Apply horizontal flip augmentation&#34;. Flips images horizontally at random, which can potentially offer better editability?&lt;/p&gt; &#xA;&lt;h3&gt;Continuing Training&lt;/h3&gt; &#xA;&lt;p&gt;Once a model has been trained for any number of steps, a config file is saved which contains all of the parameters from the UI.&lt;/p&gt; &#xA;&lt;p&gt;If you wish to continue training a model, you can simply select the model name from the dropdown and then click the blue button next to the model name dropdown to load previous parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1633844/200369076-8debef69-4b95-4341-83ac-cbbb02ee02f6.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>