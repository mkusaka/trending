<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-21T01:37:10Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-mmlab/Amphion</title>
    <updated>2023-12-21T01:37:10Z</updated>
    <id>tag:github.com,2023-12-21:/open-mmlab/Amphion</id>
    <link href="https://github.com/open-mmlab/Amphion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2312.09911&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-&lt;COLOR&gt;.svg&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/amphion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://openxlab.org.cn/usercenter/Amphion&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tts/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-TTS-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-SVC-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-TTA-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Vocoder-purple&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/metrics/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Evaluation-yellow&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LICENSE-MIT-red&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation.&lt;/strong&gt; Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development. Amphion offers a unique feature: &lt;strong&gt;visualizations&lt;/strong&gt; of classic models or architectures. We believe that these visualizations are beneficial for junior researchers and engineers who wish to gain a better understanding of the model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The North-Star objective of Amphion is to offer a platform for studying the conversion of any inputs into audio.&lt;/strong&gt; Amphion is designed to support individual generation tasks, including but not limited to,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTS&lt;/strong&gt;: Text to Speech (‚õ≥&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SVS&lt;/strong&gt;: Singing Voice Synthesis (üë®‚Äçüíª&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VC&lt;/strong&gt;: Voice Conversion (üë®‚Äçüíª&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SVC&lt;/strong&gt;: Singing Voice Conversion (‚õ≥&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTA&lt;/strong&gt;: Text to Audio (‚õ≥&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTM&lt;/strong&gt;: Text to Music (üë®‚Äçüíª&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;more‚Ä¶&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to the specific generation tasks, Amphion also includes several &lt;strong&gt;vocoders&lt;/strong&gt; and &lt;strong&gt;evaluation metrics&lt;/strong&gt;. A vocoder is an important module for producing high-quality audio signals, while evaluation metrics are critical for ensuring consistent metrics in generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;Here is the Amphion v0.1 demo, whose voice, audio effects, and singing voice are generated by our models. Just enjoy it!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/Amphion/assets/24860155/7fcdcea5-3d95-4b31-bd93-4b4da734ef9b&#34;&gt;amphion-v0.1-en&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ&amp;nbsp;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/18&lt;/strong&gt;: Amphion v0.1 release. &lt;a href=&#34;https://arxiv.org/abs/2312.09911&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/amphion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=1aw0HhcggvQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YouTube-Demo-red&#34; alt=&#34;youtube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/Amphion/pull/39&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/11/28&lt;/strong&gt;: Amphion alpha release. &lt;a href=&#34;https://github.com/open-mmlab/Amphion/pull/2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚≠ê&amp;nbsp;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;TTS: Text to Speech&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion achieves state-of-the-art performance when compared with existing open-source repositories on text-to-speech (TTS) systems. It supports the following models or architectures: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34;&gt;FastSpeech2&lt;/a&gt;: A non-autoregressive TTS architecture that utilizes feed-forward Transformer blocks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;VITS&lt;/a&gt;: An end-to-end TTS architecture that utilizes conditional variational autoencoder with adversarial learning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;Vall-E&lt;/a&gt;: A zero-shot TTS architecture that uses a neural codec language model with discrete codes.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.09116&#34;&gt;NaturalSpeech2&lt;/a&gt;: An architecture for TTS that utilizes a latent diffusion model to generate natural-sounding voices.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SVC: Singing Voice Conversion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ampion supports multiple content-based features from various pretrained models, including &lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;WeNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt;, and &lt;a href=&#34;https://github.com/auspicious3000/contentvec&#34;&gt;ContentVec&lt;/a&gt;. Their specific roles in SVC has been investigated in our NeurIPS 2023 workshop paper. &lt;a href=&#34;https://arxiv.org/abs/2310.11160&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/MultipleContentsSVC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Amphion implements several state-of-the-art model architectures, including diffusion-, transformer-, VAE- and flow-based models. The diffusion-based architecture uses &lt;a href=&#34;https://openreview.net/pdf?id=a-xFK8Ymz5J&#34;&gt;Bidirectional dilated CNN&lt;/a&gt; as a backend and supports several sampling algorithms such as &lt;a href=&#34;https://arxiv.org/pdf/2006.11239.pdf&#34;&gt;DDPM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2010.02502.pdf&#34;&gt;DDIM&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/pdf/2202.09778.pdf&#34;&gt;PNDM&lt;/a&gt;. Additionally, it supports single-step inference based on the &lt;a href=&#34;https://openreview.net/pdf?id=FmqFfMTNnv&#34;&gt;Consistency Model&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TTA: Text to Audio&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion supports the TTA with a latent diffusion model. It is designed like &lt;a href=&#34;https://arxiv.org/abs/2301.12503&#34;&gt;AudioLDM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2301.12661&#34;&gt;Make-an-Audio&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2304.00830&#34;&gt;AUDIT&lt;/a&gt;. It is also the official implementation of the text-to-audio generation part of our NeurIPS 2023 paper. &lt;a href=&#34;https://arxiv.org/abs/2304.00830&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/RECIPE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vocoder&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion supports various widely-used neural vocoders, including: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GAN-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1910.06711&#34;&gt;MelGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;HiFi-GAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts&#34;&gt;NSF-HiFiGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.04658&#34;&gt;BigVGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2305.07952&#34;&gt;APNet&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Flow-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1811.00002&#34;&gt;WaveGlow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Diffusion-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/2009.09761&#34;&gt;Diffwave&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Auto-regressive based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34;&gt;WaveNet&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1802.08435v1&#34;&gt;WaveRNN&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Amphion provides the official implementation of &lt;a href=&#34;https://arxiv.org/abs/2311.14957&#34;&gt;Multi-Scale Constant-Q Transform Discriminator&lt;/a&gt; (our ICASSP 2024 paper). It can be used to enhance any architecture GAN-based vocoders during training, and keep the inference stage (such as memory or speed) unchanged. &lt;a href=&#34;https://arxiv.org/abs/2311.14957&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/gan/tfr_enhanced_hifigan&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Amphion provides a comprehensive objective evaluation of the generated audio. The evaluation metrics contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;F0 Modeling&lt;/strong&gt;: F0 Pearson Coefficients, F0 Periodicity Root Mean Square Error, F0 Root Mean Square Error, Voiced/Unvoiced F1 Score, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Energy Modeling&lt;/strong&gt;: Energy Root Mean Square Error, Energy Pearson Coefficients, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intelligibility&lt;/strong&gt;: Character/Word Error Rate, which can be calculated based on &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spectrogram Distortion&lt;/strong&gt;: Frechet Audio Distance (FAD), Mel Cepstral Distortion (MCD), Multi-Resolution STFT Distance (MSTFT), Perceptual Evaluation of Speech Quality (PESQ), Short Time Objective Intelligibility (STOI), etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speaker Similarity&lt;/strong&gt;: Cosine similarity, which can be calculated based on &lt;a href=&#34;https://github.com/Jungjee/RawNet&#34;&gt;RawNet3&lt;/a&gt;, &lt;a href=&#34;https://github.com/wenet-e2e/wespeaker&#34;&gt;WeSpeaker&lt;/a&gt;, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;Amphion unifies the data preprocess of the open-source datasets including &lt;a href=&#34;https://audiocaps.github.io/&#34;&gt;AudioCaps&lt;/a&gt;, &lt;a href=&#34;https://www.openslr.org/60/&#34;&gt;LibriTTS&lt;/a&gt;, &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34;&gt;LJSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/M4Singer/M4Singer&#34;&gt;M4Singer&lt;/a&gt;, &lt;a href=&#34;https://wenet.org.cn/opencpop/&#34;&gt;Opencpop&lt;/a&gt;, &lt;a href=&#34;https://github.com/Multi-Singer/Multi-Singer.github.io&#34;&gt;OpenSinger&lt;/a&gt;, &lt;a href=&#34;http://vc-challenge.org/&#34;&gt;SVCC&lt;/a&gt;, &lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/3443&#34;&gt;VCTK&lt;/a&gt;, and more. The supported dataset list can be seen &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/datasets/README.md&#34;&gt;here&lt;/a&gt; (updating).&lt;/p&gt; &#xA;&lt;h2&gt;üìÄ Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/open-mmlab/Amphion.git&#xA;cd Amphion&#xA;&#xA;# Install Python Environment&#xA;conda create --name amphion python=3.9.15&#xA;conda activate amphion&#xA;&#xA;# Install Python Packages Dependencies&#xA;sh env.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üêç Usage in Python&lt;/h2&gt; &#xA;&lt;p&gt;We detail the instructions of different tasks in the following recipes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tts/README.md&#34;&gt;Text to Speech (TTS)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/README.md&#34;&gt;Singing Voice Conversion (SVC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/README.md&#34;&gt;Text to Audio (TTA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/README.md&#34;&gt;Vocoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/metrics/README.md&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôè&amp;nbsp;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ming024/FastSpeech2&#34;&gt;ming024&#39;s FastSpeech2&lt;/a&gt; and &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;jaywalnut310&#39;s VITS&lt;/a&gt; for model architecture code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;lifeiteng&#39;s VALL-E&lt;/a&gt; for training pipeline and model architecture design.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;WeNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt;, &lt;a href=&#34;https://github.com/auspicious3000/contentvec&#34;&gt;ContentVec&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Jungjee/RawNet&#34;&gt;RawNet3&lt;/a&gt; for pretrained models and inference code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;HiFi-GAN&lt;/a&gt; for GAN-based Vocoder&#39;s architecture design and training strategy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;Encodec&lt;/a&gt; for well-organized GAN Discriminator&#39;s architecture and basic blocks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion&lt;/a&gt; for model architecture design.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TensorSpeech/TensorFlowTTS&#34;&gt;TensorFlowTTS&lt;/a&gt; for preparing the MFA tools.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;¬©Ô∏è&amp;nbsp;License&lt;/h2&gt; &#xA;&lt;p&gt;Amphion is under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;. It is free for both research and commercial use cases.&lt;/p&gt; &#xA;&lt;h2&gt;üìö Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhang2023amphion,&#xA;      title={Amphion: An Open-Source Audio, Music and Speech Generation Toolkit}, &#xA;      author={Xueyao Zhang and Liumeng Xue and Yuancheng Wang and Yicheng Gu and Xi Chen and Zihao Fang and Haopeng Chen and Lexiao Zou and Chaoren Wang and Jun Han and Kai Chen and Haizhou Li and Zhizheng Wu},&#xA;      journal={arXiv},&#xA;      year={2023},&#xA;      volume={abs/2312.09911}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>hutaiHang/Faster-Diffusion</title>
    <updated>2023-12-21T01:37:10Z</updated>
    <id>tag:github.com,2023-12-21:/hutaiHang/Faster-Diffusion</id>
    <link href="https://github.com/hutaiHang/Faster-Diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄ Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/hutaiHang/Faster-Diffusion/main/doc/demo.jpg&#34; alt=&#34;demo&#34; style=&#34;zoom:150%;&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;em&gt; Our approach can easily be combined with various diffusion model-based tasks üß† (such as text-to-image, personalized generation, video generation, etc.) and various sampling strategies (like DDIM-50 steps, Dpm-solver-20 steps) to achieve training-free acceleration. &lt;/em&gt; &#xA;&lt;/div&gt; &#xA;&lt;be&gt; &#xA; &lt;be&gt; &#xA;  &lt;h2&gt;üìã TODO List&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code that combines our method with &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt; ;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code that combines our method with &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-XL-v1.0&#34;&gt;DeepFloyd-IF&lt;/a&gt;;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release code that combines our method with &lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;Text2Video-zero&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/damo/text-to-video-synthesis/summary&#34;&gt;VideoDiffusion&lt;/a&gt;;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release code that combines our method with &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release code that combines our method with &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion&#34;&gt;DreamBooth&lt;/a&gt; and &lt;a href=&#34;https://github.com/adobe-research/custom-diffusion&#34;&gt;Custom Diffusion&lt;/a&gt;;&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;üìò Introduction&lt;/h2&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;p&gt;Senmao Li*, Taihang Hu*, Fahad Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, Jian Yang&lt;/p&gt; &#xA;   &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.09608&#34;&gt;arXiv Paper&lt;/a&gt;;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;p&gt;*&lt;strong&gt;Denotes equal contribution.&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;p&gt;We propose FasterDiffusion, a training-free diffusion model acceleration scheme that can be widely integrated with various generative tasks and sampling strategies. Quantitative evaluation metrics such as FID, Clipscore, and user studies all indicate that our approach is on par with the original model in terms of genenrated-image quality. Specifically, we have observed the similarity of internal features in the Unet Encoder at adjacent time steps in the diffusion model. Consequently, it is possible to reuse Encoder features from previous time steps at specific time steps to reduce computational load. We propose a feature propagation scheme for accelerated generation, and this feature propagation enables independent computation at certain time steps, allowing us to further leverage GPU acceleration through a parallel strategy. Additionally, we introduced a prior noise injection method to improve the texture details of generated images.&lt;/p&gt; &#xA;  &lt;p&gt;Our method is not only suitable for standard text-to-image(&lt;strong&gt;~1.8x acceleration for &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt; and ~1.3x acceleration for &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-I-XL-v1.0&#34;&gt;DeepFloyd-IF&lt;/a&gt;&lt;/strong&gt; ) tasks but can also be applied to diverse tasks such as text-to-video(&lt;strong&gt;~1.5x acceleration on &lt;a href=&#34;https://modelscope.cn/models/damo/text-to-video-synthesis/summary&#34;&gt;VideoDiffusion&lt;/a&gt;)&lt;/strong&gt;, personalized generation(&lt;strong&gt;~1.8x acceleration for &lt;a href=&#34;https://github.com/XavierXiao/Dreambooth-Stable-Diffusion&#34;&gt;DreamBooth&lt;/a&gt; and &lt;a href=&#34;https://github.com/adobe-research/custom-diffusion&#34;&gt;Custom Diffusion&lt;/a&gt;&lt;/strong&gt;), and reference-guided generation(&lt;strong&gt;~2.1x acceleration for &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;&lt;/strong&gt;), among others.&lt;/p&gt; &#xA;  &lt;img src=&#34;.\doc\method.png&#34; alt=&#34;method&#34;&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;em&gt;Method Overview. For more details, please see our paper. &lt;/em&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;h2&gt;üîß Quick Start&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Create environmentÔºö&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n fastersd python=3.9&#xA;conda activate fastersd&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Execute&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# if using `stable diffusion`&#xA;python sd_demo.py&#xA;&#xA;# if using `deepfloyd if`&#xA;python if_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;sd_demo.py output&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Origin Pipeline: 2.369 seconds&#xA;Faster Diffusion: 1.407 seconds&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All of our experiments are conducted using an A40 GPU (48GB of VRAM).&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Usage&lt;/p&gt; &lt;p&gt;Our method can easily integrate with the &lt;a href=&#34;https://huggingface.co/docs/diffusers/index&#34;&gt;diffusers&lt;/a&gt; library. Below is an example of integration with &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;stable-diffusion v1.5&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;For Stable Diffusion&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffusers import StableDiffusionPipeline&#xA;import torch&#xA;from utils_sd import register_normal_pipeline, register_faster_forward, register_parallel_pipeline, seed_everything  # 1.import package&#xA;&#xA;seed_everything(2023)&#xA;model_id = &#34;runwayml/stable-diffusion-v1-5&#34;&#xA;pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)&#xA;pipe = pipe.to(&#34;cuda&#34;)&#xA;&#xA;#------------------------------&#xA;# 2. enable parallel. If memory is limited, replace it with  `register_normal_pipeline(pipe)`&#xA;register_parallel_pipeline(pipe) &#xA;# 3. encoder propagation&#xA;register_faster_forward(pipe.unet) &#xA;#------------------------------&#xA;prompt = &#34;a cat wearing sunglasses&#34;&#xA;image = pipe.call(prompt).images[0]  &#xA;  &#xA;image.save(&#34;cat.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &#xA;  &lt;h2&gt;‚ú® Qualitative results&lt;/h2&gt; &#xA;  &lt;h3&gt;Text to Image&lt;/h3&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt; ~1.8x acceleration for stable diffusion, 50 DDIM steps &lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;img src=&#34;.\doc\sd-ddim50.png&#34; alt=&#34;sd-ddim50&#34;&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt; ~1.8x acceleration for stable diffusion, 20 Dpm-solver++ steps &lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;img src=&#34;.\doc\sd-dpm++20.png&#34; alt=&#34;sd-dpm++20&#34;&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt; ~1.3x acceleration for DeepFloyd-IF &lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;img src=&#34;.\doc\if-demo.png&#34; alt=&#34;if-demo&#34;&gt; &#xA;  &lt;h3&gt;Text to Video&lt;/h3&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt; ~1.4x acceleration for Text2Video-Zero &lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;img src=&#34;.\doc\t2v-zero.png&#34; alt=&#34;t2v-zero&#34;&gt; &#xA;  &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hutaiHang/Faster-Diffusion/main/doc/videofusion-origin-demo1.gif&#34; alt=&#34;origin&#34; style=&#34;width: 95%;&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hutaiHang/Faster-Diffusion/main/doc/videofusion-ours-demo1.gif&#34; alt=&#34;ours&#34; style=&#34;width: 95%;&#34;&gt; &lt;/p&gt;&#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt;~1.5x acceleration for VideoFusion, origin video(left) and ours(right)&lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;p&gt;&lt;/p&gt; &#xA;  &lt;h3&gt;ControlNet&lt;/h3&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt; ~2.1x acceleration for ControlNet &lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;img src=&#34;.\doc\controlnet-demo.png&#34; alt=&#34;controlnet-demo&#34; style=&#34;zoom:50%;&#34;&gt; &#xA;  &lt;h3&gt;Personalized Generation&lt;/h3&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt; ~1.8x acceleration for DreamBooth and Custom Diffusion &lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;img src=&#34;.\doc\personalized-demo.png&#34; alt=&#34;personalized-demo&#34; style=&#34;zoom:50%;&#34;&gt; &#xA;  &lt;h3&gt;Other tasks based on Diffusion Model&lt;/h3&gt; &#xA;  &lt;img src=&#34;.\doc\other-task.png&#34; alt=&#34;other-task&#34; style=&#34;zoom: 43%;&#34;&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;b&gt; Integrate our method with other tasks, such as Image Editing(&lt;a href=&#34;https://github.com/google/prompt-to-prompt&#34;&gt;P2P&lt;/a&gt;) and &lt;a href=&#34;https://github.com/ziqihuangg/ReVersion&#34;&gt;Reversion&lt;/a&gt; &lt;/b&gt; &#xA;  &lt;/div&gt; &#xA;  &lt;h2&gt;üìà Quantitative results&lt;/h2&gt; &#xA;  &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hutaiHang/Faster-Diffusion/main/doc/rst1.png&#34; alt=&#34;origin&#34; style=&#34;width: 45%;margin-right: 20px;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hutaiHang/Faster-Diffusion/main/doc/rst2.png&#34; alt=&#34;ours&#34; style=&#34;width: 45%;&#34;&gt; &lt;/p&gt; &#xA;  &lt;h2&gt;Citation&lt;/h2&gt; &#xA;  &lt;pre&gt;&lt;code&gt;@misc{li2023faster,&#xA;      title={Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models}, &#xA;      author={Senmao Li and Taihang Hu and Fahad Shahbaz Khan and Linxuan Li and Shiqi Yang and Yaxing Wang and Ming-Ming Cheng and Jian Yang},&#xA;      year={2023},&#xA;      eprint={2312.09608},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/be&gt;&#xA;&lt;/be&gt;</summary>
  </entry>
  <entry>
    <title>deepseek-ai/DreamCraft3D</title>
    <updated>2023-12-21T01:37:10Z</updated>
    <id>tag:github.com,2023-12-21:/deepseek-ai/DreamCraft3D</id>
    <link href="https://github.com/deepseek-ai/DreamCraft3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DreamCraft3D&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.16818&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://mrtornado24.github.io/DreamCraft3D/&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=0FazXENkQms&#34;&gt;&lt;strong&gt;Youtube video&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Official implementation of DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mrtornado24.github.io/&#34;&gt;Jingxiang Sun&lt;/a&gt;, &lt;a href=&#34;https://bo-zhang.me/&#34;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&#34;https://dsaurus.github.io/saurus/&#34;&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href=&#34;https://lizhenwangt.github.io/&#34;&gt;Lizhen Wang&lt;/a&gt;, &lt;a href=&#34;https://github.com/StevenLiuWen&#34;&gt;Wen Liu&lt;/a&gt;, &lt;a href=&#34;https://zdaxie.github.io/&#34;&gt;Zhenda Xie&lt;/a&gt;, &lt;a href=&#34;https://liuyebin.com/&#34;&gt;Yebin Liu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: &lt;em&gt;We present DreamCraft3D, a hierarchical 3D content generation method that produces high-fidelity and coherent 3D objects. We tackle the problem by leveraging a 2D reference image to guide the stages of geometry sculpting and texture boosting. A central focus of this work is to address the consistency issue that existing works encounter. To sculpt geometries that render coherently, we perform score distillation sampling via a view-dependent diffusion model. This 3D prior, alongside several training strategies, prioritizes the geometry consistency but compromises the texture fidelity. We further propose &lt;strong&gt;Bootstrapped Score Distillation&lt;/strong&gt; to specifically boost the texture. We train a personalized diffusion model, Dreambooth, on the augmented renderings of the scene, imbuing it with 3D knowledge of the scene being optimized. The score distillation from this 3D-aware diffusion prior provides view-consistent guidance for the scene. Notably, through an alternating optimization of the diffusion prior and 3D scene representation, we achieve mutually reinforcing improvements: the optimized 3D scene aids in training the scene-specific diffusion model, which offers increasingly view-consistent guidance for 3D optimization. The optimization is thus bootstrapped and leads to substantial texture boosting. With tailored 3D priors throughout the hierarchical generation, DreamCraft3D generates coherent 3D objects with photorealistic renderings, advancing the state-of-the-art in 3D content generation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DreamCraft3D/main/assets/repo_static_v2.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Method Overview&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DreamCraft3D/main/assets/diagram-1.png&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- https://github.com/MrTornado24/DreamCraft3D/assets/45503891/8e70610c-d812-4544-86bf-7f8764e41067&#xA;&#xA;&#xA;&#xA;https://github.com/MrTornado24/DreamCraft3D/assets/45503891/b1e8ae54-1afd-4e0f-88f7-9bd5b70fd44d&#xA;&#xA;&#xA;&#xA;https://github.com/MrTornado24/DreamCraft3D/assets/45503891/ead40f9b-d7ee-4ee8-8d98-dbd0b8fbab97 --&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install threestudio&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;This part is the same as original threestudio. Skip it if you already have installed the environment.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DreamCraft3D/main/docs/installation.md&#34;&gt;installation.md&lt;/a&gt; for additional information, including installation via Docker.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You must have an NVIDIA graphics card with at least 20GB VRAM and have &lt;a href=&#34;https://developer.nvidia.com/cuda-downloads&#34;&gt;CUDA&lt;/a&gt; installed.&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(Optional, Recommended) Create a virtual environment:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 -m virtualenv venv&#xA;. venv/bin/activate&#xA;&#xA;# Newer pip versions, e.g. pip-23.x, can be much faster than old versions, e.g. pip-20.x.&#xA;# For instance, it caches the wheels of git packages to avoid unnecessarily rebuilding them later.&#xA;python3 -m pip install --upgrade pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;code&gt;PyTorch &amp;gt;= 1.12&lt;/code&gt;. We have tested on &lt;code&gt;torch1.12.1+cu113&lt;/code&gt; and &lt;code&gt;torch2.0.0+cu118&lt;/code&gt;, but other versions should also work fine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# torch1.12.1+cu113&#xA;pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113&#xA;# or torch2.0.0+cu118&#xA;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Optional, Recommended) Install ninja to speed up the compilation of CUDA extensions:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ninja&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download pre-trained models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero123&lt;/strong&gt;. We use the newest &lt;code&gt;stable-zero123.ckpt&lt;/code&gt; by default. You can download it &lt;a href=&#34;https://huggingface.co/stabilityai/stable-zero123&#34;&gt;here&lt;/a&gt; into &lt;code&gt;load/zero123/&lt;/code&gt;. In the paper we use &lt;code&gt;zero123-xl.ckpt&lt;/code&gt; and you can download it by&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd load/zero123&#xA;bash download.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Omnidata&lt;/strong&gt;. We use &lt;a href=&#34;https://github.com/EPFL-VILAB/omnidata/tree/main/omnidata_tools/torch&#34;&gt;Omnidata&lt;/a&gt; for depth and normal predition in &lt;code&gt;preprocess_image.py&lt;/code&gt; (copyed from &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;stable-dreamfusion&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd load/omnidata&#xA;gdown &#39;1Jrh-bRnJEjyMCS7f-WsaFlccfPjJPPHI&amp;amp;confirm=t&#39; # omnidata_dpt_depth_v2.ckpt&#xA;gdown &#39;1wNxVO4vVbDEMEpnAi_jwQObf2MFodcBR&amp;amp;confirm=t&#39; # omnidata_dpt_normal_v2.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;Preprocess the input image to move background and obtain its depth and normal image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python preprocess_image.py /path/to/image.png --recenter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our model is trained in multiple stages. You can run it by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;prompt=&#34;a brightly colored mushroom growing on a log&#34;&#xA;image_path=&#34;load/images/mushroom_log_rgba.png&#34;&#xA;&#xA;# --------- Stage 1 (NeRF &amp;amp; NeuS) --------- # &#xA;python launch.py --config configs/dreamcraft3d-coarse-nerf.yaml --train system.prompt_processor.prompt=&#34;$prompt&#34; data.image_path=&#34;$image_path&#34;&#xA;&#xA;ckpt=outputs/dreamcraft3d-coarse-nerf/$prompt@LAST/ckpts/last.ckpt&#xA;python launch.py --config configs/dreamcraft3d-coarse-neus.yaml --train system.prompt_processor.prompt=&#34;$prompt&#34; data.image_path=&#34;$image_path&#34; system.weights=&#34;$ckpt&#34;&#xA;&#xA;# --------- Stage 2 (Geometry Refinement) --------- # &#xA;ckpt=outputs/dreamcraft3d-coarse-neus/$prompt@LAST/ckpts/last.ckpt&#xA;python launch.py --config configs/dreamcraft3d-geometry.yaml --train system.prompt_processor.prompt=&#34;$prompt&#34; data.image_path=&#34;$image_path&#34; system.geometry_convert_from=&#34;$ckpt&#34;&#xA;&#xA;&#xA;# --------- Stage 3 (Texture Refinement) --------- # &#xA;ckpt=outputs/dreamcraft3d-geometry/$prompt@LAST/ckpts/last.ckpt&#xA;python launch.py --config configs/dreamcraft3d-texture.yaml --train system.prompt_processor.prompt=&#34;$prompt&#34; data.image_path=&#34;$image_path&#34; system.geometry_convert_from=&#34;$ckpt&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;[Optional] If the &#34;Janus problem&#34; arises in Stage 1, consider training a custom Text2Image model.&lt;/summary&gt; &#xA; &lt;p&gt;First, generate multi-view images from a single reference image by Zero123++.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python threestudio/scripts/img_to_mv.py --image_path &#39;load/mushroom.png&#39; --save_path &#39;.cache/temp&#39; --prompt &#39;a photo of mushroom&#39; --superres&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Train a personalized DeepFloyd model by DreamBooth Lora. Please check if the generated mv images above are reasonable.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export MODEL_NAME=&#34;DeepFloyd/IF-I-XL-v1.0&#34;&#xA;export INSTANCE_DIR=&#34;.cache/temp&#34;&#xA;export OUTPUT_DIR=&#34;.cache/if_dreambooth_mushroom&#34;&#xA;&#xA;accelerate launch threestudio/scripts/train_dreambooth_lora.py \&#xA;  --pretrained_model_name_or_path=$MODEL_NAME  \&#xA;  --instance_data_dir=$INSTANCE_DIR \&#xA;  --output_dir=$OUTPUT_DIR \&#xA;  --instance_prompt=&#34;a sks mushroom&#34; \&#xA;  --resolution=64 \&#xA;  --train_batch_size=4 \&#xA;  --gradient_accumulation_steps=1 \&#xA;  --learning_rate=5e-6 \&#xA;  --scale_lr \&#xA;  --max_train_steps=1200 \&#xA;  --checkpointing_steps=600 \&#xA;  --pre_compute_text_embeddings \&#xA;  --tokenizer_max_length=77 \&#xA;  --text_encoder_use_attention_mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The personalized DeepFloyd model lora is save at &lt;code&gt;.cache/if_dreambooth_mushroom&lt;/code&gt;. Now you can replace the guidance the training scripts by&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# --------- Stage 1 (NeRF &amp;amp; NeuS) --------- # &#xA;python launch.py --config configs/dreamcraft3d-coarse-nerf.yaml --train system.prompt_processor.prompt=&#34;$prompt&#34; data.image_path=&#34;$image_path&#34; system.guidance.lora_weights_path=&#34;.cache/if_dreambooth_mushroom&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;. We run the default configs on 40G A100 GPUs. For reducing memory usage, you can reduce the rendering resolution of NeuS by &lt;code&gt;data.height=128 data.width=128 data.random_camera.height=128 data.random_camera.width=128&lt;/code&gt;. You can also reduce resolution for other stages in the same way.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release the reorganized code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Realse the test image data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Clean the original dreambooth training code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Provide some running results and checkpoints.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This code is built on the amazing open-source projects &lt;a href=&#34;https://github.com/threestudio-project/threestudio&#34;&gt;threestudio-project&lt;/a&gt; and &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;stable-dreamfusion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Related links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;DreamFusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://research.nvidia.com/labs/dir/magic3d/&#34;&gt;Magic3D&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Make-it-3D&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://guochengqian.github.io/project/magic123/&#34;&gt;Magic123&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;ProlificDreamer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;DreamBooth&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{sun2023dreamcraft3d,&#xA;  title={Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior},&#xA;  author={Sun, Jingxiang and Zhang, Bo and Shao, Ruizhi and Wang, Lizhen and Liu, Wen and Xie, Zhenda and Liu, Yebin},&#xA;  journal={arXiv preprint arXiv:2310.16818},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>