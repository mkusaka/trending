<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-17T01:34:15Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>homebrewltd/ichigo</title>
    <updated>2024-10-17T01:34:15Z</updated>
    <id>tag:github.com,2024-10-17:/homebrewltd/ichigo</id>
    <link href="https://github.com/homebrewltd/ichigo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Llama3.1 learns to Listen&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;&lt;span&gt;üçì&lt;/span&gt; Ichigo: Local real-time voice AI (Formerly llama3-s).&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://homebrew.ltd/blog/llama3-just-got-ears&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Blog-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://demo.homebrew.ltd/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Demo-violet&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/homebrewltd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/homebrewltd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/homebrewltd/ichigo/main/images/ichigov0.2.jpeg&#34; width=&#34;400&#34;&gt; &#xA; &lt;p&gt;&lt;small&gt;Homebrewed early-fusion speech model&lt;/small&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; Update: September 30, 2024&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;We have rebranded from llama3-s to &lt;span&gt;üçì&lt;/span&gt; Ichigo.&lt;/li&gt; &#xA;  &lt;li&gt;Our custom-built early-fusion speech model now has a name and a voice.&lt;/li&gt; &#xA;  &lt;li&gt;It has improved multiturn capabilities and can now refuse to process inaudible queries.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING]&lt;br&gt; &lt;span&gt;üçì&lt;/span&gt; Ichigo is an open research experiment&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Join us in the &lt;code&gt;#research&lt;/code&gt; channel in &lt;a href=&#34;https://discord.com/invite/FTk2MvZwJH&#34;&gt;Homebrew&#39;s Discord&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;We livestream training runs in &lt;code&gt;#research-livestream&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;üçì&lt;/span&gt; Ichigo is an open, ongoing research experiment to extend a text-based LLM to have native &#34;listening&#34; ability. Think of it as an open data, open weight, on device Siri.&lt;/p&gt; &#xA;&lt;p&gt;It uses an &lt;a href=&#34;https://medium.com/@raj.pulapakura/multimodal-models-and-fusion-a-complete-guide-225ca91f6861#:~:text=3.3.,-Early%20Fusion&amp;amp;text=Early%20fusion%20refers%20to%20combining,fused%20representation%20through%20the%20model.&#34;&gt;early fusion&lt;/a&gt; technique inspired by &lt;a href=&#34;https://arxiv.org/abs/2405.09818&#34;&gt;Meta&#39;s Chameleon paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We &lt;del&gt;build&lt;/del&gt; train in public:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://homebrew.ltd/blog/llama-learns-to-talk&#34;&gt;Ichigo v0.3 Checkpoint Writeup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://homebrew.ltd/blog/llama3-just-got-ears&#34;&gt;Ichigo v0.2 Checkpoint Writeup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://homebrew.ltd/blog/can-llama-3-listen&#34;&gt;Ichigo v0.1 Checkpoint Writeup&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Progress&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;4 Oct: &lt;a href=&#34;https://huggingface.co/collections/homebrewltd/ichigo-66ffc7484ef31ec5596ef6d0&#34;&gt;Ichigo v0.3&lt;/a&gt; models are now available. Utilizing cleaner and improved data, our model has achieved an enhanced MMLU score of 63.79 and demonstrates stronger speech instruction-following capabilities, even in multi-turn interactions. Additionally, by incorporating noise-synthetic data, we have successfully trained the model to refuse processing non-speech audio inputs from users, further improving its functionality and user experience.&lt;/li&gt; &#xA; &lt;li&gt;23 Aug: We‚Äôre excited to share &lt;a href=&#34;https://huggingface.co/homebrewltd/llama3.1-s-instruct-v0.2&#34;&gt;Ichigo-llama3.1-s-instruct-v0.2&lt;/a&gt;, our latest multimodal checkpoint with improved speech understanding by enhancing the model&#39;s audio instruction-following capabilities through training on interleaving synthetic data.&lt;/li&gt; &#xA; &lt;li&gt;17 Aug: We pre-trained our LLaMA 3.1 model on continuous speech data, tokenized using WhisperSpeechVQ. The final loss converged to approximately 1.9, resulting in our checkpoint: &lt;a href=&#34;https://huggingface.co/homebrewltd/llama3.1-s-base-v0.2&#34;&gt;Ichigo-llama3.1-s-base-v0.2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;1 Aug: Identified typo in original training recipe, causing significant degradation (MMLU: 0.6 -&amp;gt; 0.2), proposed fixes.&lt;/li&gt; &#xA; &lt;li&gt;30 July: Presented llama3-s progress at: &lt;a href=&#34;https://lu.ma/ws8t6wom?tk=wZvFmm&#34;&gt;AI Training: From PyTorch to GPU Clusters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;19 July: &lt;a href=&#34;https://huggingface.co/homebrewltd/llama3-s-2024-07-19&#34;&gt;llama3-s-2024-07-19&lt;/a&gt; understands synthetic voice with limited results&lt;/li&gt; &#xA; &lt;li&gt;1 July: &lt;a href=&#34;https://huggingface.co/homebrewltd/llama3-s-2024-07-08&#34;&gt;llama3-s-2024-07-08&lt;/a&gt; showed converging loss (1.7) with limited data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Join Us&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span&gt;üçì&lt;/span&gt; Ichigo is an open research project. We&#39;re looking for collaborators, and will likely move towards crowdsourcing speech datasets in the future.&lt;/p&gt; &#xA;&lt;h3&gt;Quickstart with Google Colab&lt;/h3&gt; &#xA;&lt;p&gt;Checkout this notebook to try our latest model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/18IiwN0AzBZaox5o0iidXqWD1xKq11XbZ?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Synthetic Generation&lt;/h3&gt; &#xA;&lt;p&gt;For detailed information on synthetic generation, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/homebrewltd/ichigo/main/synthetic_data/README.md&#34;&gt;Synthetic Generation Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Organize the input/output directory&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First Clone the Repo from github:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/homebrewltd/llama3-s.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;The folder structure is as follows:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Ichigo&#xA;‚îú‚îÄ‚îÄ HF_Trainer                               # HF training code (deprecated)&#xA;‚îú‚îÄ‚îÄ synthetic_data                           # Synthetic data generation pipeline&#xA;    ‚îú‚îÄ‚îÄ configs                              # Audio pipeline configs&#xA;        ‚îú‚îÄ‚îÄ audio_to_audio                   # Parler audio (.wav) to semantic tokens&#xA;        ‚îú‚îÄ‚îÄ synthetic_generation_config      # TTS semantic tokens&#xA;‚îú‚îÄ‚îÄ scripts                                  # Setup scripts for Runpod&#xA;‚îú‚îÄ‚îÄ torchtune                                # Submodule: our fork of fsdp with checkpointing&#xA;‚îú‚îÄ‚îÄ model_zoo                                # Model checkpoints&#xA;‚îÇ   ‚îú‚îÄ‚îÄ LLM&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Meta-Llama-3-8B-Instruct&#xA;‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Meta-Llama-3-70B-Instruct&#xA;‚îú‚îÄ‚îÄ demo                                     # Selfhost this demo (vllm)&#xA;‚îú‚îÄ‚îÄ inference                                # Google Colab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training with HF Trainer&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv hf_trainer&#xA;chmod +x scripts/install.sh&#xA;./scripts/install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Restart shell now&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;chmod +x scripts/setup.sh&#xA;./scripts/setup.sh&#xA;source myenv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Logging Huggingface&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;huggingface-cli login --token=&amp;lt;token&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;export CUTLASS_PATH=&#34;cutlass&#34;&#xA;export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7&#xA;accelerate launch --config_file ./accelerate_config.yaml train.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training with Torchtune&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv torchtune&#xA;pip install torch torchvision tensorboard&#xA;cd ./torchtune&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also download the model using tune:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tune download homebrewltd/llama3.1-s-whispervq-init --hf-token &amp;lt;token&amp;gt;  --output-dir ../model_zoo/llama3.1-s-whispervq-init --ignore-patterns &#34;original/consolidated*&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Setup the Dataset from HF path by change the path and change the name of the model in the following YAML file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;nano torchtune/recipes/configs/jan-llama3-s/8B_full.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Training Multi GPU (1-8GPUs Supported)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;tune run --nproc_per_node 4 full_finetune_fsdp2 --config recipes/configs/jan-llama3-1-s/8B_full.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;h3&gt;WebUI&lt;/h3&gt; &#xA;&lt;p&gt;For instructions on how to self-host the Ichigo web UI demo using Docker, please visit: &lt;a href=&#34;https://github.com/homebrewltd/ichigo-demo/tree/docker&#34;&gt;Ichigo demo&lt;/a&gt;. To try our demo on a single RTX 4090 GPU, you can go directly to: &lt;a href=&#34;https://ichigo.homebrew.ltd&#34;&gt;https://ichigo.homebrew.ltd&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;We offer code for users to create a web UI demo. Please follow the instructions below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv demo&#xA;source demo/bin/activate&#xA;# First install all required packages&#xA;pip install --no-cache-dir -r ./demo/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the command below to launch a Gradio demo locally. You can add the variables &lt;code&gt;use-4bit&lt;/code&gt; and &lt;code&gt;use-8bit&lt;/code&gt; for quantized usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m demo.app --host 0.0.0.0 --port 7860 --max-seq-len 1024 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also host a demo using vLLM for faster inference but its not support streaming output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m demo.app_vllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alternatively, you can easily try our demo on &lt;a href=&#34;https://huggingface.co/spaces/jan-hq/Llama3.1-s-v0.2&#34;&gt;HuggingFace&lt;/a&gt; ü§ó&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chameleonteam2024chameleonmixedmodalearlyfusionfoundation,&#xA;      title={Chameleon: Mixed-Modal Early-Fusion Foundation Models}, &#xA;      author={Chameleon Team},&#xA;      year={2024},&#xA;      eprint={2405.09818},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;      journal={arXiv preprint}&#xA;}&#xA;&#xA;@misc{zhang2024adamminiusefewerlearning,&#xA;      title={Adam-mini: Use Fewer Learning Rates To Gain More}, &#xA;      author={Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun},&#xA;      year={2024},&#xA;      eprint={2406.16793},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG},&#xA;      journal={arXiv preprint}&#xA;}&#xA;&#xA;@misc{defossez2022highfi,&#xA;      title={High Fidelity Neural Audio Compression},&#xA;      author={D√©fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},&#xA;      year={2022},&#xA;      eprint={2210.13438},&#xA;      archivePrefix={arXiv},&#xA;      journal={arXiv preprint}&#xA;}&#xA;&#xA;@misc{WhisperSpeech,&#xA;      title={WhisperSpeech: An Open Source Text-to-Speech System Built by Inverting Whisper}, &#xA;      author={Collabora and LAION},&#xA;      year={2024},&#xA;      url={https://github.com/collabora/WhisperSpeech},&#xA;      note={GitHub repository}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtune&#34;&gt;Torchtune&lt;/a&gt;: The codebase we built upon&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;Accelerate&lt;/a&gt;: Library for easy use of distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/collabora/WhisperSpeech&#34;&gt;WhisperSpeech&lt;/a&gt;: Text-to-speech model for synthetic audio generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;Encodec&lt;/a&gt;: High-fidelity neural audio codec for efficient audio compression&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6&#34;&gt;Llama3&lt;/a&gt;: the Family of Models that we based on that has the amazing language capabilities !!!&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>