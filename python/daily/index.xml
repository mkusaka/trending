<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-07T01:34:23Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>THUDM/CogView4</title>
    <updated>2025-03-07T01:34:23Z</updated>
    <id>tag:github.com,2025-03-07:/THUDM/CogView4</id>
    <link href="https://github.com/THUDM/CogView4" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CogView4, CogView3-Plus and CogView3(ECCV 2024)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CogView4 &amp;amp; CogView3 &amp;amp; CogView-3Plus&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/README_zh.md&#34;&gt;ÈòÖËØª‰∏≠ÊñáÁâà&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/README_ja.md&#34;&gt;Êó•Êú¨Ë™û„ÅßË™≠„ÇÄ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/resources/logo.svg?sanitize=true&#34; width=&#34;50%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/spaces/THUDM-HF-SPACE/CogView4&#34; target=&#34;_blank&#34;&gt; ü§ó HuggingFace Space&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/ZhipuAI/CogView4&#34; target=&#34;_blank&#34;&gt; ü§ñModelScope Space&lt;/a&gt; &lt;a href=&#34;https://zhipuaishengchan.datasink.sensorsdata.cn/t/4z&#34; target=&#34;_blank&#34;&gt; üõ†Ô∏èZhipuAI MaaS(Faster)&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/resources/WECHAT.md&#34; target=&#34;_blank&#34;&gt; üëã WeChat Community&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.05121&#34; target=&#34;_blank&#34;&gt;üìö CogView3 Paper&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/resources/showcase.png&#34; alt=&#34;showcase.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Project Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî•üî• &lt;code&gt;2025/03/04&lt;/code&gt;: We&#39;ve adapted and open-sourced the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; version of &lt;strong&gt;CogView-4&lt;/strong&gt; model, which has 6B parameters, supports native Chinese input, and Chinese text-to-image generation. You can try it &lt;a href=&#34;https://huggingface.co/spaces/THUDM-HF-SPACE/CogView4&#34;&gt;online&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/10/13&lt;/code&gt;: We&#39;ve adapted and open-sourced the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; version of &lt;strong&gt;CogView-3Plus-3B&lt;/strong&gt; model. You can try it &lt;a href=&#34;https://huggingface.co/spaces/THUDM-HF-SPACE/CogView3-Plus-3B-Space&#34;&gt;online&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/9/29&lt;/code&gt;: We&#39;ve open-sourced &lt;strong&gt;CogView3&lt;/strong&gt; and &lt;strong&gt;CogView-3Plus-3B&lt;/strong&gt;. &lt;strong&gt;CogView3&lt;/strong&gt; is a text-to-image system based on cascading diffusion, using a relay diffusion framework. &lt;strong&gt;CogView-3Plus&lt;/strong&gt; is a series of newly developed text-to-image models based on Diffusion Transformer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Diffusers workflow adaptation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cog series fine-tuning kits (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ControlNet models and training code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Introduction&lt;/h2&gt; &#xA;&lt;h3&gt;Model Comparison&lt;/h3&gt; &#xA;&lt;table style=&#34;border-collapse: collapse; width: 100%;&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th style=&#34;text-align: center;&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th style=&#34;text-align: center;&#34;&gt;CogView4&lt;/th&gt; &#xA;   &lt;th style=&#34;text-align: center;&#34;&gt;CogView3-Plus-3B&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;Resolution&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34; style=&#34;text-align: center;&#34;&gt; 512 &amp;lt;= H, W &amp;lt;= 2048 &lt;br&gt; H * W &amp;lt;= 2^{21} &lt;br&gt; H, W \mod 32 = 0 &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;Inference Precision&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34; style=&#34;text-align: center;&#34;&gt;Only supports BF16, FP32&lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;Encoder&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;https://huggingface.co/THUDM/glm-4-9b-hf&#34; target=&#34;_blank&#34;&gt;GLM-4-9B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;https://huggingface.co/google/t5-v1_1-xxl&#34; target=&#34;_blank&#34;&gt;T5-XXL&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;Prompt Language&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;Chinese, English&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;English&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;Prompt Length Limit&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;1024 Tokens&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;224 Tokens&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;Download Links&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;https://huggingface.co/THUDM/CogView4-6B&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://modelscope.cn/models/ZhipuAI/CogView4-6B&#34;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://wisemodel.cn/models/ZhipuAI/CogView4-6B&#34;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;https://huggingface.co/THUDM/CogView3-Plus-3B&#34;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://modelscope.cn/models/ZhipuAI/CogView3-Plus-3B&#34;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://wisemodel.cn/models/ZhipuAI/CogView3-Plus-3B&#34;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Memory Usage&lt;/h3&gt; &#xA;&lt;p&gt;DIT models are tested with &lt;code&gt;BF16&lt;/code&gt; precision and &lt;code&gt;batchsize=4&lt;/code&gt;, with results shown in the table below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;enable_model_cpu_offload OFF&lt;/th&gt; &#xA;   &lt;th&gt;enable_model_cpu_offload ON&lt;/th&gt; &#xA;   &lt;th&gt;enable_model_cpu_offload ON &lt;br&gt; Text Encoder 4bit&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;512 * 512&lt;/td&gt; &#xA;   &lt;td&gt;33GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;13G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1280 * 720&lt;/td&gt; &#xA;   &lt;td&gt;35GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;13G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1024 * 1024&lt;/td&gt; &#xA;   &lt;td&gt;35GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;13G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1920 * 1280&lt;/td&gt; &#xA;   &lt;td&gt;39GB&lt;/td&gt; &#xA;   &lt;td&gt;20GB&lt;/td&gt; &#xA;   &lt;td&gt;14G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2048 * 2048&lt;/td&gt; &#xA;   &lt;td&gt;43GB&lt;/td&gt; &#xA;   &lt;td&gt;21GB&lt;/td&gt; &#xA;   &lt;td&gt;14G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Additionally, we recommend that your device has at least &lt;code&gt;32GB&lt;/code&gt; of RAM to prevent the process from being killed.&lt;/p&gt; &#xA;&lt;h3&gt;Model Metrics&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve tested on multiple benchmarks and achieved the following scores:&lt;/p&gt; &#xA;&lt;h4&gt;DPG-Bench&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Overall&lt;/th&gt; &#xA;   &lt;th&gt;Global&lt;/th&gt; &#xA;   &lt;th&gt;Entity&lt;/th&gt; &#xA;   &lt;th&gt;Attribute&lt;/th&gt; &#xA;   &lt;th&gt;Relation&lt;/th&gt; &#xA;   &lt;th&gt;Other&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SDXL&lt;/td&gt; &#xA;   &lt;td&gt;74.65&lt;/td&gt; &#xA;   &lt;td&gt;83.27&lt;/td&gt; &#xA;   &lt;td&gt;82.43&lt;/td&gt; &#xA;   &lt;td&gt;80.91&lt;/td&gt; &#xA;   &lt;td&gt;86.76&lt;/td&gt; &#xA;   &lt;td&gt;80.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-alpha&lt;/td&gt; &#xA;   &lt;td&gt;71.11&lt;/td&gt; &#xA;   &lt;td&gt;74.97&lt;/td&gt; &#xA;   &lt;td&gt;79.32&lt;/td&gt; &#xA;   &lt;td&gt;78.60&lt;/td&gt; &#xA;   &lt;td&gt;82.57&lt;/td&gt; &#xA;   &lt;td&gt;76.96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SD3-Medium&lt;/td&gt; &#xA;   &lt;td&gt;84.08&lt;/td&gt; &#xA;   &lt;td&gt;87.90&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;91.01&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;88.83&lt;/td&gt; &#xA;   &lt;td&gt;80.70&lt;/td&gt; &#xA;   &lt;td&gt;88.68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DALL-E 3&lt;/td&gt; &#xA;   &lt;td&gt;83.50&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;90.97&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;89.61&lt;/td&gt; &#xA;   &lt;td&gt;88.39&lt;/td&gt; &#xA;   &lt;td&gt;90.58&lt;/td&gt; &#xA;   &lt;td&gt;89.83&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flux.1-dev&lt;/td&gt; &#xA;   &lt;td&gt;83.79&lt;/td&gt; &#xA;   &lt;td&gt;85.80&lt;/td&gt; &#xA;   &lt;td&gt;86.79&lt;/td&gt; &#xA;   &lt;td&gt;89.98&lt;/td&gt; &#xA;   &lt;td&gt;90.04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;89.90&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-7B&lt;/td&gt; &#xA;   &lt;td&gt;84.19&lt;/td&gt; &#xA;   &lt;td&gt;86.90&lt;/td&gt; &#xA;   &lt;td&gt;88.90&lt;/td&gt; &#xA;   &lt;td&gt;89.40&lt;/td&gt; &#xA;   &lt;td&gt;89.32&lt;/td&gt; &#xA;   &lt;td&gt;89.48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CogView4-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;85.13&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83.85&lt;/td&gt; &#xA;   &lt;td&gt;90.35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;91.17&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;91.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;87.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;GenEval&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Overall&lt;/th&gt; &#xA;   &lt;th&gt;Single Obj.&lt;/th&gt; &#xA;   &lt;th&gt;Two Obj.&lt;/th&gt; &#xA;   &lt;th&gt;Counting&lt;/th&gt; &#xA;   &lt;th&gt;Colors&lt;/th&gt; &#xA;   &lt;th&gt;Position&lt;/th&gt; &#xA;   &lt;th&gt;Color attribution&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SDXL&lt;/td&gt; &#xA;   &lt;td&gt;0.55&lt;/td&gt; &#xA;   &lt;td&gt;0.98&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;   &lt;td&gt;0.39&lt;/td&gt; &#xA;   &lt;td&gt;0.85&lt;/td&gt; &#xA;   &lt;td&gt;0.15&lt;/td&gt; &#xA;   &lt;td&gt;0.23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-alpha&lt;/td&gt; &#xA;   &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;td&gt;0.98&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;0.44&lt;/td&gt; &#xA;   &lt;td&gt;0.80&lt;/td&gt; &#xA;   &lt;td&gt;0.08&lt;/td&gt; &#xA;   &lt;td&gt;0.07&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SD3-Medium&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.99&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.94&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.72&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;   &lt;td&gt;0.33&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DALL-E 3&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;   &lt;td&gt;0.96&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;td&gt;0.47&lt;/td&gt; &#xA;   &lt;td&gt;0.83&lt;/td&gt; &#xA;   &lt;td&gt;0.43&lt;/td&gt; &#xA;   &lt;td&gt;0.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flux.1-dev&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.98&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.73&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.77&lt;/td&gt; &#xA;   &lt;td&gt;0.22&lt;/td&gt; &#xA;   &lt;td&gt;0.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.80&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.99&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;   &lt;td&gt;0.59&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.90&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.79&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.66&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CogView4-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.99&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.86&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;   &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;td&gt;0.58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;T2I-CompBench&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Color&lt;/th&gt; &#xA;   &lt;th&gt;Shape&lt;/th&gt; &#xA;   &lt;th&gt;Texture&lt;/th&gt; &#xA;   &lt;th&gt;2D-Spatial&lt;/th&gt; &#xA;   &lt;th&gt;3D-Spatial&lt;/th&gt; &#xA;   &lt;th&gt;Numeracy&lt;/th&gt; &#xA;   &lt;th&gt;Non-spatial Clip&lt;/th&gt; &#xA;   &lt;th&gt;Complex 3-in-1&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SDXL&lt;/td&gt; &#xA;   &lt;td&gt;0.5879&lt;/td&gt; &#xA;   &lt;td&gt;0.4687&lt;/td&gt; &#xA;   &lt;td&gt;0.5299&lt;/td&gt; &#xA;   &lt;td&gt;0.2133&lt;/td&gt; &#xA;   &lt;td&gt;0.3566&lt;/td&gt; &#xA;   &lt;td&gt;0.4988&lt;/td&gt; &#xA;   &lt;td&gt;0.3119&lt;/td&gt; &#xA;   &lt;td&gt;0.3237&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-alpha&lt;/td&gt; &#xA;   &lt;td&gt;0.6690&lt;/td&gt; &#xA;   &lt;td&gt;0.4927&lt;/td&gt; &#xA;   &lt;td&gt;0.6477&lt;/td&gt; &#xA;   &lt;td&gt;0.2064&lt;/td&gt; &#xA;   &lt;td&gt;0.3901&lt;/td&gt; &#xA;   &lt;td&gt;0.5058&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.3197&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.3433&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SD3-Medium&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.8132&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.5885&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.7334&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.3200&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.4084&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.6174&lt;/td&gt; &#xA;   &lt;td&gt;0.3140&lt;/td&gt; &#xA;   &lt;td&gt;0.3771&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DALL-E 3&lt;/td&gt; &#xA;   &lt;td&gt;0.7785&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.6205&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.7036&lt;/td&gt; &#xA;   &lt;td&gt;0.2865&lt;/td&gt; &#xA;   &lt;td&gt;0.3744&lt;/td&gt; &#xA;   &lt;td&gt;0.5880&lt;/td&gt; &#xA;   &lt;td&gt;0.3003&lt;/td&gt; &#xA;   &lt;td&gt;0.3773&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flux.1-dev&lt;/td&gt; &#xA;   &lt;td&gt;0.7572&lt;/td&gt; &#xA;   &lt;td&gt;0.5066&lt;/td&gt; &#xA;   &lt;td&gt;0.6300&lt;/td&gt; &#xA;   &lt;td&gt;0.2700&lt;/td&gt; &#xA;   &lt;td&gt;0.3992&lt;/td&gt; &#xA;   &lt;td&gt;0.6165&lt;/td&gt; &#xA;   &lt;td&gt;0.3065&lt;/td&gt; &#xA;   &lt;td&gt;0.3628&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-7B&lt;/td&gt; &#xA;   &lt;td&gt;0.5145&lt;/td&gt; &#xA;   &lt;td&gt;0.3323&lt;/td&gt; &#xA;   &lt;td&gt;0.4069&lt;/td&gt; &#xA;   &lt;td&gt;0.1566&lt;/td&gt; &#xA;   &lt;td&gt;0.2753&lt;/td&gt; &#xA;   &lt;td&gt;0.4406&lt;/td&gt; &#xA;   &lt;td&gt;0.3137&lt;/td&gt; &#xA;   &lt;td&gt;0.3806&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CogView4-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.7786&lt;/td&gt; &#xA;   &lt;td&gt;0.5880&lt;/td&gt; &#xA;   &lt;td&gt;0.6983&lt;/td&gt; &#xA;   &lt;td&gt;0.3075&lt;/td&gt; &#xA;   &lt;td&gt;0.3708&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.6626&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.3056&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.3869&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Chinese Text Accuracy Evaluation&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Recall&lt;/th&gt; &#xA;   &lt;th&gt;F1 Score&lt;/th&gt; &#xA;   &lt;th&gt;Pick@4&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kolors&lt;/td&gt; &#xA;   &lt;td&gt;0.6094&lt;/td&gt; &#xA;   &lt;td&gt;0.1886&lt;/td&gt; &#xA;   &lt;td&gt;0.2880&lt;/td&gt; &#xA;   &lt;td&gt;0.1633&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CogView4-6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.6969&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.5532&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.6168&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.3265&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Inference Model&lt;/h2&gt; &#xA;&lt;h3&gt;Prompt Optimization&lt;/h3&gt; &#xA;&lt;p&gt;Although CogView4 series models are trained with lengthy synthetic image descriptions, we strongly recommend using a large language model to rewrite prompts before text-to-image generation, which will greatly improve generation quality.&lt;/p&gt; &#xA;&lt;p&gt;We provide an &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/inference/prompt_optimize.py&#34;&gt;example script&lt;/a&gt;. We recommend running this script to refine your prompts. Note that &lt;code&gt;CogView4&lt;/code&gt; and &lt;code&gt;CogView3&lt;/code&gt; models use different few-shot examples for prompt optimization. They need to be distinguished.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd inference&#xA;python prompt_optimize.py --api_key &#34;Zhipu AI API Key&#34; --prompt {your prompt} --base_url &#34;https://open.bigmodel.cn/api/paas/v4&#34; --model &#34;glm-4-plus&#34; --cogview_version &#34;cogview4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference Model&lt;/h3&gt; &#xA;&lt;p&gt;Run the model with &lt;code&gt;BF16&lt;/code&gt; precision:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from diffusers import CogView4Pipeline&#xA;import torch&#xA;&#xA;pipe = CogView4Pipeline.from_pretrained(&#34;THUDM/CogView4-6B&#34;, torch_dtype=torch.bfloat16).to(&#34;cuda&#34;)&#xA;&#xA;# Open it for reduce GPU memory usage&#xA;pipe.enable_model_cpu_offload()&#xA;pipe.vae.enable_slicing()&#xA;pipe.vae.enable_tiling()&#xA;&#xA;prompt = &#34;A vibrant cherry red sports car sits proudly under the gleaming sun, its polished exterior smooth and flawless, casting a mirror-like reflection. The car features a low, aerodynamic body, angular headlights that gaze forward like predatory eyes, and a set of black, high-gloss racing rims that contrast starkly with the red. A subtle hint of chrome embellishes the grille and exhaust, while the tinted windows suggest a luxurious and private interior. The scene conveys a sense of speed and elegance, the car appearing as if it&#39;s about to burst into a sprint along a coastal road, with the ocean&#39;s azure waves crashing in the background.&#34;&#xA;image = pipe(&#xA;    prompt=prompt,&#xA;    guidance_scale=3.5,&#xA;    num_images_per_prompt=1,&#xA;    num_inference_steps=50,&#xA;    width=1024,&#xA;    height=1024,&#xA;).images[0]&#xA;&#xA;image.save(&#34;cogview4.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more inference code, please check:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For using &lt;code&gt;BNB int4&lt;/code&gt; to load &lt;code&gt;text encoder&lt;/code&gt; and complete inference code annotations, check &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/inference/cli_demo_cogview4.py&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For using &lt;code&gt;TorchAO int8 or int4&lt;/code&gt; to load &lt;code&gt;text encoder &amp;amp; transformer&lt;/code&gt; and complete inference code annotations, check &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/inference/cli_demo_cogview4_int8.py&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For setting up a &lt;code&gt;gradio&lt;/code&gt; GUI DEMO, check &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/inference/gradio_web_demo.py&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/THUDM/CogView4&#xA;cd CogView4&#xA;git clone https://huggingface.co/THUDM/CogView4-6B&#xA;pip install -r inference/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;12G VRAM&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MODE=1 python inference/gradio_web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;24G VRAM 32G RAM&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MODE=2 python inference/gradio_web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;24G VRAM 64G RAM&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MODE=3 python inference/gradio_web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;48G VRAM 64G RAM&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MODE=4 python inference/gradio_web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository and the CogView3 models are licensed under &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We welcome and appreciate your code contributions. You can view the contribution guidelines &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogView4/main/resources/contribute.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Plachtaa/seed-vc</title>
    <updated>2025-03-07T01:34:23Z</updated>
    <id>tag:github.com,2025-03-07:/Plachtaa/seed-vc</id>
    <link href="https://github.com/Plachtaa/seed-vc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;zero-shot voice conversion &amp; singing voice conversion, with real-time support&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Seed-VC&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Plachta/Seed-VC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2411.09943&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2411.09943-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/seed-vc/main/README-ZH.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/seed-vc/main/README-JA.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/86325c5e-f7f6-4a04-8695-97275a5d046c&#34;&gt;real-time-demo.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Currently released model supports &lt;em&gt;zero-shot voice conversion&lt;/em&gt; üîä , &lt;em&gt;zero-shot real-time voice conversion&lt;/em&gt; üó£Ô∏è and &lt;em&gt;zero-shot singing voice conversion&lt;/em&gt; üé∂. Without any training, it is able to clone a voice given a reference speech of 1~30 seconds.&lt;/p&gt; &#xA;&lt;p&gt;We support further fine-tuning on custom data to increase performance on specific speaker/speakers, with extremely low data requirement &lt;strong&gt;(minimum 1 utterance per speaker)&lt;/strong&gt; and extremely fast training speed &lt;strong&gt;(minimum 100 steps, 2 min on T4)&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real-time voice conversion&lt;/strong&gt; is support, with algorithm delay of ~300ms and device side delay of ~100ms, suitable for online meetings, gaming and live streaming.&lt;/p&gt; &#xA;&lt;p&gt;To find a list of demos and comparisons with previous voice conversion models, please visit our &lt;a href=&#34;https://plachtaa.github.io/seed-vc/&#34;&gt;demo page&lt;/a&gt;üåê and &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/seed-vc/main/EVAL.md&#34;&gt;Evaluaiton&lt;/a&gt;üìä.&lt;/p&gt; &#xA;&lt;p&gt;We are keeping on improving the model quality and adding more features.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluationüìä&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/seed-vc/main/EVAL.md&#34;&gt;EVAL.md&lt;/a&gt; for objective evaluation results and comparisons with other baselines.&lt;/p&gt; &#xA;&lt;h2&gt;Installationüì•&lt;/h2&gt; &#xA;&lt;p&gt;Suggested python 3.10 on Windows, Mac M Series (Apple Silicon) or Linux. Windows and Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Mac M Series:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements-mac.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usageüõ†Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;We have released 3 models for different purposes:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;   &lt;th&gt;Sampling Rate&lt;/th&gt; &#xA;   &lt;th&gt;Content Encoder&lt;/th&gt; &#xA;   &lt;th&gt;Vocoder&lt;/th&gt; &#xA;   &lt;th&gt;Hidden Dim&lt;/th&gt; &#xA;   &lt;th&gt;N Layers&lt;/th&gt; &#xA;   &lt;th&gt;Params&lt;/th&gt; &#xA;   &lt;th&gt;Remarks&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v1.0&lt;/td&gt; &#xA;   &lt;td&gt;seed-uvit-tat-xlsr-tiny (&lt;a href=&#34;https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_uvit_tat_xlsr_ema.pth&#34;&gt;ü§ó&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/seed-vc/main/configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml&#34;&gt;üìÑ&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;Voice Conversion (VC)&lt;/td&gt; &#xA;   &lt;td&gt;22050&lt;/td&gt; &#xA;   &lt;td&gt;XLSR-large&lt;/td&gt; &#xA;   &lt;td&gt;HIFT&lt;/td&gt; &#xA;   &lt;td&gt;384&lt;/td&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;25M&lt;/td&gt; &#xA;   &lt;td&gt;suitable for real-time voice conversion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v1.0&lt;/td&gt; &#xA;   &lt;td&gt;seed-uvit-whisper-small-wavenet (&lt;a href=&#34;https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_seed_v2_uvit_whisper_small_wavenet_bigvgan_pruned.pth&#34;&gt;ü§ó&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/seed-vc/main/configs/presets/config_dit_mel_seed_uvit_whisper_small_wavenet.yml&#34;&gt;üìÑ&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;Voice Conversion (VC)&lt;/td&gt; &#xA;   &lt;td&gt;22050&lt;/td&gt; &#xA;   &lt;td&gt;Whisper-small&lt;/td&gt; &#xA;   &lt;td&gt;BigVGAN&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;98M&lt;/td&gt; &#xA;   &lt;td&gt;suitable for offline voice conversion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;v1.0&lt;/td&gt; &#xA;   &lt;td&gt;seed-uvit-whisper-base (&lt;a href=&#34;https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_seed_v2_uvit_whisper_base_f0_44k_bigvgan_pruned_ft_ema.pth&#34;&gt;ü§ó&lt;/a&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/seed-vc/main/configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml&#34;&gt;üìÑ&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;Singing Voice Conversion (SVC)&lt;/td&gt; &#xA;   &lt;td&gt;44100&lt;/td&gt; &#xA;   &lt;td&gt;Whisper-small&lt;/td&gt; &#xA;   &lt;td&gt;BigVGAN&lt;/td&gt; &#xA;   &lt;td&gt;768&lt;/td&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;200M&lt;/td&gt; &#xA;   &lt;td&gt;strong zero-shot performance, singing voice conversion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Checkpoints of the latest model release will be downloaded automatically when first run inference.&lt;br&gt; If you are unable to access huggingface for network reason, try using mirror by adding &lt;code&gt;HF_ENDPOINT=https://hf-mirror.com&lt;/code&gt; before every command.&lt;/p&gt; &#xA;&lt;p&gt;Command line inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --source &amp;lt;source-wav&amp;gt;&#xA;--target &amp;lt;referene-wav&amp;gt;&#xA;--output &amp;lt;output-dir&amp;gt;&#xA;--diffusion-steps 25 # recommended 30~50 for singingvoice conversion&#xA;--length-adjust 1.0&#xA;--inference-cfg-rate 0.7&#xA;--f0-condition False # set to True for singing voice conversion&#xA;--auto-f0-adjust False # set to True to auto adjust source pitch to target pitch level, normally not used in singing voice conversion&#xA;--semi-tone-shift 0 # pitch shift in semitones for singing voice conversion&#xA;--checkpoint &amp;lt;path-to-checkpoint&amp;gt;&#xA;--config &amp;lt;path-to-config&amp;gt;&#xA; --fp16 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;source&lt;/code&gt; is the path to the speech file to convert to reference voice&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;target&lt;/code&gt; is the path to the speech file as voice reference&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output&lt;/code&gt; is the path to the output directory&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;diffusion-steps&lt;/code&gt; is the number of diffusion steps to use, default is 25, use 30-50 for best quality, use 4-10 for fastest inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;length-adjust&lt;/code&gt; is the length adjustment factor, default is 1.0, set &amp;lt;1.0 for speed-up speech, &amp;gt;1.0 for slow-down speech&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inference-cfg-rate&lt;/code&gt; has subtle difference in the output, default is 0.7&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;f0-condition&lt;/code&gt; is the flag to condition the pitch of the output to the pitch of the source audio, default is False, set to True for singing voice conversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;auto-f0-adjust&lt;/code&gt; is the flag to auto adjust source pitch to target pitch level, default is False, normally not used in singing voice conversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;semi-tone-shift&lt;/code&gt; is the pitch shift in semitones for singing voice conversion, default is 0&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;checkpoint&lt;/code&gt; is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface.(&lt;code&gt;seed-uvit-whisper-small-wavenet&lt;/code&gt; if &lt;code&gt;f0-condition&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; else &lt;code&gt;seed-uvit-whisper-base&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config&lt;/code&gt; is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;fp16&lt;/code&gt; is the flag to use float16 inference, default is True&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Voice Conversion Web UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app_vc.py --checkpoint &amp;lt;path-to-checkpoint&amp;gt; --config &amp;lt;path-to-config&amp;gt; --fp16 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;checkpoint&lt;/code&gt; is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (&lt;code&gt;seed-uvit-whisper-small-wavenet&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config&lt;/code&gt; is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then open the browser and go to &lt;code&gt;http://localhost:7860/&lt;/code&gt; to use the web interface.&lt;/p&gt; &#xA;&lt;p&gt;Singing Voice Conversion Web UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app_svc.py --checkpoint &amp;lt;path-to-checkpoint&amp;gt; --config &amp;lt;path-to-config&amp;gt; --fp16 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;checkpoint&lt;/code&gt; is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (&lt;code&gt;seed-uvit-whisper-base&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config&lt;/code&gt; is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Integrated Web UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will only load pretrained models for zero-shot inference. To use custom checkpoints, please run &lt;code&gt;app_vc.py&lt;/code&gt; or &lt;code&gt;app_svc.py&lt;/code&gt; as above.&lt;/p&gt; &#xA;&lt;p&gt;Real-time voice conversion GUI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python real-time-gui.py --checkpoint &amp;lt;path-to-checkpoint&amp;gt; --config &amp;lt;path-to-config&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;checkpoint&lt;/code&gt; is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (&lt;code&gt;seed-uvit-tat-xlsr-tiny&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config&lt;/code&gt; is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] It is strongly recommended to use a GPU for real-time voice conversion. Some performance testing has been done on a NVIDIA RTX 3060 Laptop GPU, results and recommended parameter settings are listed below:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Configuration&lt;/th&gt; &#xA;   &lt;th&gt;Diffusion Steps&lt;/th&gt; &#xA;   &lt;th&gt;Inference CFG Rate&lt;/th&gt; &#xA;   &lt;th&gt;Max Prompt Length&lt;/th&gt; &#xA;   &lt;th&gt;Block Time (s)&lt;/th&gt; &#xA;   &lt;th&gt;Crossfade Length (s)&lt;/th&gt; &#xA;   &lt;th&gt;Extra context (left) (s)&lt;/th&gt; &#xA;   &lt;th&gt;Extra context (right) (s)&lt;/th&gt; &#xA;   &lt;th&gt;Latency (ms)&lt;/th&gt; &#xA;   &lt;th&gt;Inference Time per Chunk (ms)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;seed-uvit-xlsr-tiny&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;0.7&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;0.18s&lt;/td&gt; &#xA;   &lt;td&gt;0.04s&lt;/td&gt; &#xA;   &lt;td&gt;2.5s&lt;/td&gt; &#xA;   &lt;td&gt;0.02s&lt;/td&gt; &#xA;   &lt;td&gt;430ms&lt;/td&gt; &#xA;   &lt;td&gt;150ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can adjust the parameters in the GUI according to your own device performance, the voice conversion stream should work well as long as Inference Time is less than Block Time.&lt;br&gt; Note that inference speed may drop if you are running other GPU intensive tasks (e.g. gaming, watching videos)&lt;/p&gt; &#xA;&lt;p&gt;Explanations for real-time voice conversion GUI parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Diffusion Steps&lt;/code&gt; is the number of diffusion steps to use, in real-time case usually set to 4~10 for fastest inference;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Inference CFG Rate&lt;/code&gt; has subtle difference in the output, default is 0.7, set to 0.0 gains about 1.5x speed-up;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Max Prompt Length&lt;/code&gt; is the maximum length of the prompt audio, setting to a low value can speed up inference, but may reduce similarity to prompt speech;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Block Time&lt;/code&gt; is the time length of each audio chunk for inference, the higher the value, the higher the latency, note this value must be greater than the inference time per block, set according to your hardware condition;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Crossfade Length&lt;/code&gt; is the time length of crossfade between audio chunks, normally not needed to change;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Extra context (left)&lt;/code&gt; is the time length of extra history context for inference, the higher the value, the higher the inference time, but can increase stability;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Extra context (right)&lt;/code&gt; is the time length of extra future context for inference, the higher the value, the higher the inference time and latency, but can increase stability;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The algorithm delay is appoximately calculated as &lt;code&gt;Block Time * 2 + Extra context (right)&lt;/code&gt;, device side delay is usually of ~100ms. The overall delay is the sum of the two.&lt;/p&gt; &#xA;&lt;p&gt;You may wish to use &lt;a href=&#34;https://vb-audio.com/Cable/&#34;&gt;VB-CABLE&lt;/a&gt; to route audio from GUI output stream to a virtual microphone.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(GUI and audio chunking logic are modified from &lt;a href=&#34;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI&#34;&gt;RVC&lt;/a&gt;, thanks for their brilliant implementation!)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TrainingüèãÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Fine-tuning on custom data allow the model to clone someone&#39;s voice more accurately. It will largely improve speaker similarity on particular speakers, but may slightly increase WER.&lt;br&gt; A Colab Tutorial is here for you to follow: &lt;a href=&#34;https://colab.research.google.com/drive/1R1BJTqMsTXZzYAVx3j1BiemFXog9pbQG?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare your own dataset. It has to satisfy the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;File structure does not matter&lt;/li&gt; &#xA;   &lt;li&gt;Each audio file should range from 1 to 30 seconds, otherwise will be ignored&lt;/li&gt; &#xA;   &lt;li&gt;All audio files should be in on of the following formats: &lt;code&gt;.wav&lt;/code&gt; &lt;code&gt;.flac&lt;/code&gt; &lt;code&gt;.mp3&lt;/code&gt; &lt;code&gt;.m4a&lt;/code&gt; &lt;code&gt;.opus&lt;/code&gt; &lt;code&gt;.ogg&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Speaker label is not required, but make sure that each speaker has at least 1 utterance&lt;/li&gt; &#xA;   &lt;li&gt;Of course, the more data you have, the better the model will perform&lt;/li&gt; &#xA;   &lt;li&gt;Training data should be as clean as possible, BGM or noise is not desired&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Choose a model configuration file from &lt;code&gt;configs/presets/&lt;/code&gt; for fine-tuning, or create your own to train from scratch. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For fine-tuning, it should be one of the following: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;./configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml&lt;/code&gt; for real-time voice conversion&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;./configs/presets/config_dit_mel_seed_uvit_whisper_small_wavenet.yml&lt;/code&gt; for offline voice conversion&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml&lt;/code&gt; for singing voice conversion&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the following command to start training:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train.py &#xA;--config &amp;lt;path-to-config&amp;gt; &#xA;--dataset-dir &amp;lt;path-to-data&amp;gt;&#xA;--run-name &amp;lt;run-name&amp;gt;&#xA;--batch-size 2&#xA;--max-steps 1000&#xA;--max-epochs 1000&#xA;--save-every 500&#xA;--num-workers 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config&lt;/code&gt; is the path to the model config, choose one of the above for fine-tuning or create your own for training from scratch&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;dataset-dir&lt;/code&gt; is the path to the dataset directory, which should be a folder containing all the audio files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;run-name&lt;/code&gt; is the name of the run, which will be used to save the model checkpoints and logs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;batch-size&lt;/code&gt; is the batch size for training, choose depends on your GPU memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max-steps&lt;/code&gt; is the maximum number of steps to train, choose depends on your dataset size and training time&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max-epochs&lt;/code&gt; is the maximum number of epochs to train, choose depends on your dataset size and training time&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;save-every&lt;/code&gt; is the number of steps to save the model checkpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;num-workers&lt;/code&gt; is the number of workers for data loading, set to 0 for Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;If training accidentially stops, you can resume training by running the same command again, the training will continue from the last checkpoint. (Make sure &lt;code&gt;run-name&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; arguments are the same so that latest checkpoint can be found)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After training, you can use the trained model for inference by specifying the path to the checkpoint and config file.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;They should be under &lt;code&gt;./runs/&amp;lt;run-name&amp;gt;/&lt;/code&gt;, with the checkpoint named &lt;code&gt;ft_model.pth&lt;/code&gt; and config file with the same name as the training config file.&lt;/li&gt; &#xA;   &lt;li&gt;You still have to specify a reference audio file of the speaker you&#39;d like to use during inference, similar to zero-shot usage.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODOüìù&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release pretrained models: &lt;a href=&#34;https://huggingface.co/Plachta/Seed-VC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SeedVC-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Huggingface space demo: &lt;a href=&#34;https://huggingface.co/spaces/Plachta/Seed-VC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-blue&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; HTML demo page: &lt;a href=&#34;https://plachtaa.github.io/seed-vc/&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Streaming inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Reduce streaming inference latency&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Demo video for real-time voice conversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Singing voice conversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Noise resiliency for source audio&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Potential architecture improvements &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; U-ViT style skip connections&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Changed input to OpenAI Whisper&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Time as Token&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Code for training on custom data&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Few-shot/One-shot speaker fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Changed to BigVGAN from NVIDIA for singing voice decoding&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Whisper version model for singing voice conversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Objective evaluation and comparison with RVC/SoVITS for singing voice conversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Improve audio quality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; NSF vocoder for better singing voice conversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fix real-time voice conversion artifact while not talking (done by adding a VAD model)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Colab Notebook for fine-tuning example&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Replace whisper with more advanced linguistic content extractor&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More to be added&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Apple Silicon support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;On Mac - running &lt;code&gt;real-time-gui.py&lt;/code&gt; might raise an error &lt;code&gt;ModuleNotFoundError: No module named &#39;_tkinter&#39;&lt;/code&gt;, in this case a new Python version &lt;strong&gt;with Tkinter support&lt;/strong&gt; should be installed. Refer to &lt;a href=&#34;https://stackoverflow.com/questions/76105218/why-does-tkinter-or-turtle-seem-to-be-missing-or-broken-shouldnt-it-be-part&#34;&gt;This Guide on stack overflow&lt;/a&gt; for explanation of the problem and a detailed fix.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;CHANGELOGSüóíÔ∏è&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2025-03-03: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Added Mac M Series (Apple Silicon) support&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-11-26: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated v1.0 tiny version pretrained model, optimized for real-time voice conversion&lt;/li&gt; &#xA;   &lt;li&gt;Support one-shot/few-shot single/multi speaker fine-tuning&lt;/li&gt; &#xA;   &lt;li&gt;Support using custom checkpoint for webUI &amp;amp; real-time GUI&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-11-19: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;arXiv paper released&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-10-28: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated fine-tuned 44k singing voice conversion model with better audio quality&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-10-27: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Added real-time voice conversion GUI&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-10-25: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Added exhaustive evaluation results and comparisons with RVCv2 for singing voice conversion&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-10-24: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated 44kHz singing voice conversion model, with OpenAI Whisper as speech content input&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-10-07: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated v0.3 pretrained model, changed speech content encoder to OpenAI Whisper&lt;/li&gt; &#xA;   &lt;li&gt;Added objective evaluation results for v0.3 pretrained model&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-09-22: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated singing voice conversion model to use BigVGAN from NVIDIA, providing large improvement to high-pitched singing voices&lt;/li&gt; &#xA;   &lt;li&gt;Support chunking and streaming output for long audio files in Web UI&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-09-18: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated f0 conditioned model for singing voice conversion&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;2024-09-14: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Updated v0.2 pretrained model, with smaller size and less diffusion steps to achieve same quality, and additional ability to control prosody preservation&lt;/li&gt; &#xA;   &lt;li&gt;Added command line inference script&lt;/li&gt; &#xA;   &lt;li&gt;Added installation and usage instructions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>