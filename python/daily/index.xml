<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-17T01:37:29Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ossamamehmood/Hacktoberfest2023</title>
    <updated>2023-10-17T01:37:29Z</updated>
    <id>tag:github.com,2023-10-17:/ossamamehmood/Hacktoberfest2023</id>
    <link href="https://github.com/ossamamehmood/Hacktoberfest2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;About Make your Pull Request on Hacktoberfest 2023. Don&#39;t forget to spread love and if you like give us a ‚≠êÔ∏è&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hacktoberfest 2023 &lt;code&gt;OPEN YOUR&lt;/code&gt; Pull Request üéâ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ossamamehmood/hacktoberfest/raw/main/.github/logo.png&#34; alt=&#34;hacktoberfest2023&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;This Project Is Perfect For Your Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;üó£ &lt;strong&gt;Hacktoberfest 2023 encourages participation in the open-source community, which grows bigger every year.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;üì¢ &lt;strong&gt;Register &lt;a href=&#34;https://hacktoberfest2023.digitalocean.com&#34;&gt;here&lt;/a&gt; for hacktoberfest2023 and make four pull requests (PRs) between October 1st-31st to grab free DIGITAL-SWAGS üî•.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;üéâ We welcome contributions from everyone, regardless of your experience level. Here are some ways you can contribute:&lt;/p&gt; &#xA;&lt;p&gt;Do read the &lt;a href=&#34;https://raw.githubusercontent.com/ossamamehmood/Hacktoberfest2023/main/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üêû Reporting issues&lt;/li&gt; &#xA; &lt;li&gt;üí° Suggesting new features&lt;/li&gt; &#xA; &lt;li&gt;üìö Improving documentation&lt;/li&gt; &#xA; &lt;li&gt;üêõ Fixing bugs&lt;/li&gt; &#xA; &lt;li&gt;üíª Adding new code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Code of Conduct&lt;/h3&gt; &#xA;&lt;p&gt;Please read our &lt;a href=&#34;https://raw.githubusercontent.com/ossamamehmood/Hacktoberfest2023/main/codeofconduct.md&#34;&gt;Code of Conduct&lt;/a&gt; before participating in this project.&lt;/p&gt; &#xA;&lt;h3&gt;FAQs&lt;/h3&gt; &#xA;&lt;p&gt;If you have any questions, please check our &lt;a href=&#34;https://raw.githubusercontent.com/ossamamehmood/Hacktoberfest2023/main/Faqs.md&#34;&gt;FAQs&lt;/a&gt; for answers.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/hacktoberfest2023--blueviolet&#34; alt=&#34;Hacktober Badge&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/static/v1?label=%F0%9F%8C%9F&amp;amp;message=If%20Useful&amp;amp;style=style=flat&amp;amp;color=BC4E99&#34; alt=&#34;Star Badge&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributions-welcome-violet.svg?style=flat&amp;amp;logo=git&#34; alt=&#34;Contributions&#34;&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/ossamamehmood/hacktoberfest2023&#34; alt=&#34;Pull Requests Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/graphs/contributors&#34;&gt;&lt;img alt=&#34;GitHub contributors&#34; src=&#34;https://img.shields.io/github/contributors/ossamamehmood/hacktoberfest2023?color=2b9348&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/ossamamehmood/hacktoberfest2023?color=2b9348&#34; alt=&#34;License Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Our &lt;code&gt;Maintainers&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/ossamamehmood?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Ossama Mehmood&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/swarajmhatre&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/swarajmhatre?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Swaraj Mhatre&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/abdulrehmanghub&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/abdulrehmanghub?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Abdul Rehman&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/mohitsaini07&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/mohitsaini07?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Mohit Saini&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/Chifez&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/Chifez?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Em_Dev&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/dharmraj617&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/dharmraj617?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Dharmraj Patil&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/itsaakif&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/itsaakif?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Aakif Mudel&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/shaman-004&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/shaman-004?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Shaman M&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/kharshita590&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/kharshita590?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;kharshita590&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/axitchandora&#34;&gt; &lt;kbd&gt; &lt;img src=&#34;https://avatars3.githubusercontent.com/axitchandora?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt; &lt;/kbd&gt; &lt;br&gt; &lt;sub&gt;&lt;b&gt;Akshit Kumar Chandora&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributors of &lt;code&gt;Hacktoberfest 2023&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood/Hacktoberfest2023/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=ossamamehmood/Hacktoberfest2023&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>cpacker/MemGPT</title>
    <updated>2023-10-17T01:37:29Z</updated>
    <id>tag:github.com,2023-10-17:/cpacker/MemGPT</id>
    <link href="https://github.com/cpacker/MemGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Teaching LLMs memory management for unbounded context üìöü¶ô&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/#user-content-memgpt&#34;&gt;&lt;img src=&#34;https://memgpt.ai/assets/img/memgpt_logo_circle.png&#34; alt=&#34;MemGPT logo&#34; width=&#34;75&#34; align=&#34;right&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a href=&#34;https://memgpt.ai&#34;&gt;MemGPT&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;Try out our MemGPT chatbot on &lt;a href=&#34;https://discord.gg/9GEQrxmVyE&#34;&gt;Discord&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/9GEQrxmVyE&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1161736243340640419?label=Discord&amp;amp;logo=discord&amp;amp;logoColor=5865F2&amp;amp;style=flat-square&amp;amp;color=5865F2&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.08560&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2310.08560-B31B1B?logo=arxiv&amp;amp;style=flat-square&#34; alt=&#34;arXiv 2310.08560&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h2&gt;Create perpetual chatbots ü§ñ with self-editing memory!&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;br&gt; &#xA;  &lt;img src=&#34;https://memgpt.ai/assets/img/demo.gif&#34; alt=&#34;MemGPT demo video&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h2&gt;Chat with your data üóÉÔ∏è - try talking to the &lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/memgpt/personas/examples/docqa&#34;&gt;LlamaIndex API docs&lt;/a&gt;!&lt;/h2&gt;&lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://memgpt.ai/assets/img/docqa_demo.gif&#34; alt=&#34;MemGPT demo video for llamaindex api docs search&#34; width=&#34;800&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;h3&gt;ChatGPT (GPT-4) when asked the same question:&lt;/h3&gt;&lt;/summary&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;img src=&#34;https://memgpt.ai/assets/img/llama_index_gpt4.png&#34; alt=&#34;GPT-4 when asked about llamaindex api docs&#34; width=&#34;800&#34;&gt; &#xA;  &lt;/div&gt; (Question from https://github.com/run-llama/llama_index/issues/7756) &#xA; &lt;/details&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick setup&lt;/h2&gt; &#xA;&lt;p&gt;Join &lt;a href=&#34;https://discord.gg/9GEQrxmVyE&#34;&gt;Discord&lt;/a&gt; and message the MemGPT bot (in the &lt;code&gt;#memgpt&lt;/code&gt; channel). Then run the following commands (messaged to &#34;MemGPT Bot&#34;):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/profile&lt;/code&gt; (to create your profile)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/key&lt;/code&gt; (to enter your OpenAI key)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/create&lt;/code&gt; (to create a MemGPT chatbot)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure your privacy settings on this server are open so that MemGPT Bot can DM you: &lt;br&gt; MemGPT ‚Üí Privacy Settings ‚Üí Direct Messages set to ON&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://memgpt.ai/assets/img/discord/dm_settings.png&#34; alt=&#34;set DMs settings on MemGPT server to be open in MemGPT so that MemGPT Bot can message you&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;You can see the full list of available commands when you enter &lt;code&gt;/&lt;/code&gt; into the message box.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://memgpt.ai/assets/img/discord/slash_commands.png&#34; alt=&#34;MemGPT Bot slash commands&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;What is MemGPT?&lt;/h2&gt; &#xA;&lt;p&gt;Memory-GPT (or MemGPT in short) is a system that intelligently manages different memory tiers in LLMs in order to effectively provide extended context within the LLM&#39;s limited context window. For example, MemGPT knows when to push critical information to a vector database and when to retrieve it later in the chat, enabling perpetual conversations. Learn more about MemGPT in our &lt;a href=&#34;https://arxiv.org/abs/2310.08560&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running MemGPT Locally&lt;/h2&gt; &#xA;&lt;p&gt;Install dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your OpenAI API key to your environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export OPENAI_API_KEY=YOUR_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run MemGPT for as a conversation agent in CLI mode, simply run &lt;code&gt;main.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create a new starter user or starter persona (that MemGPT gets initialized with), create a new &lt;code&gt;.txt&lt;/code&gt; file in &lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/memgpt/humans/examples&#34;&gt;/memgpt/humans/examples&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/cpacker/MemGPT/main/memgpt/personas/examples&#34;&gt;/memgpt/personas/examples&lt;/a&gt;, then use the &lt;code&gt;--persona&lt;/code&gt; or &lt;code&gt;--human&lt;/code&gt; flag when running &lt;code&gt;main.py&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# assuming you created a new file /memgpt/humans/examples/me.txt&#xA;python main.py --human me.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;code&gt;main.py&lt;/code&gt; flags&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;--persona&#xA;  load a specific persona file&#xA;--human&#xA;  load a specific human file&#xA;--first&#xA;  allows you to send the first message in the chat (by default, MemGPT will send the first message)&#xA;--debug&#xA;  enables debugging output&#xA;--archival_storage_faiss_path=&amp;lt;ARCHIVAL_STORAGE_FAISS_PATH&amp;gt;&#xA;  load in document database (backed by FAISS index)&#xA;--archival_storage_files=&#34;&amp;lt;ARCHIVAL_STORAGE_FILES_GLOB&amp;gt;&#34;&#xA;  pre-load files into archival memory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive CLI commands&lt;/h3&gt; &#xA;&lt;p&gt;While using MemGPT via the CLI you can run various commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;/exit&#xA;  exit the CLI&#xA;/save&#xA;  save a checkpoint of the current agent/conversation state&#xA;/load&#xA;  load a saved checkpoint&#xA;/dump&#xA;  view the current message log (see the contents of main context)&#xA;/memory&#xA;  print the current contents of agent memory&#xA;/pop&#xA;  undo the last message in the conversation&#xA;/heartbeat&#xA;  send a heartbeat system message to the agent&#xA;/memorywarning&#xA;  send a memory warning system message to the agent&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Support&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;By default MemGPT will use &lt;code&gt;gpt-4&lt;/code&gt;, so your API key will require &lt;code&gt;gpt-4&lt;/code&gt; API access.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have any further questions, or have anything to share, we are excited to hear your feedback!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For issues and feature requests, please &lt;a href=&#34;https://github.com/cpacker/MemGPT/issues&#34;&gt;open a GitHub issue&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;Datasets used in our &lt;a href=&#34;https://arxiv.org/abs/2310.08560&#34;&gt;paper&lt;/a&gt; can be downloaded at &lt;a href=&#34;https://huggingface.co/MemGPT&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>showlab/Show-1</title>
    <updated>2023-10-17T01:37:29Z</updated>
    <id>tag:github.com,2023-10-17:/showlab/Show-1</id>
    <link href="https://github.com/showlab/Show-1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; width=&#34;100%&#34;&gt; &#xA; &lt;h1&gt;üé¨Show-1&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://junhaozhang98.github.io/&#34; target=&#34;_blank&#34;&gt;David Junhao Zhang&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://zhangjiewu.github.io/&#34; target=&#34;_blank&#34;&gt;Jay Zhangjie Wu&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://jia-wei-liu.github.io/&#34; target=&#34;_blank&#34;&gt;Jia-Wei Liu&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt; &#xA;  &lt;br&gt; &#xA;  &lt;a href=&#34;https://ruizhaocv.github.io/&#34; target=&#34;_blank&#34;&gt;Rui Zhao&lt;sup&gt;&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://siacorplab.nus.edu.sg/people/ran-lingmin/&#34; target=&#34;_blank&#34;&gt;Lingmin Ran&lt;sup&gt;&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://ycgu.site/&#34; target=&#34;_blank&#34;&gt;Yuchao Gu&lt;sup&gt;&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://scholar.google.com/citations?user=No9OsocAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34;&gt;Difei Gao&lt;sup&gt;&lt;/sup&gt;&lt;/a&gt;‚ÄÉ &#xA;  &lt;a href=&#34;https://sites.google.com/view/showlab/home?authuser=0&#34; target=&#34;_blank&#34;&gt;Mike Zheng Shou&lt;sup&gt;‚úâ&lt;/sup&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;a href=&#34;https://sites.google.com/view/showlab/home?authuser=0&#34; target=&#34;_blank&#34;&gt;Show Lab, National University of Singapore&lt;/a&gt; &#xA;   &lt;br&gt; &#xA;   &lt;sup&gt;*&lt;/sup&gt; Equal Contribution‚ÄÉ &#xA;   &lt;sup&gt;‚úâ&lt;/sup&gt; Corresponding Author &#xA;  &lt;/div&gt; &#xA;  &lt;hr&gt; &#xA;  &lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/showlab/Show-1?style=social&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fshowlab%2FShow-1&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;h3&gt;&lt;a href=&#34;https://showlab.github.io/Show-1&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2309.15818&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2309.15818&#34;&gt;PDF&lt;/a&gt;&lt;/h3&gt; &#xA;  &lt;h2&gt;News&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[10/12/2023] Code and weights released!&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;h2&gt;Setup&lt;/h2&gt; &#xA;  &lt;h3&gt;Requirements&lt;/h3&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;Pytorch 2.0+ is highly recommended for more efficiency and speed on GPUs.&lt;/p&gt; &#xA;  &lt;h3&gt;Weights&lt;/h3&gt; &#xA;  &lt;p&gt;All weights are available in show lab &lt;a href=&#34;https://huggingface.co/showlab&#34;&gt;huggingface&lt;/a&gt;! Please check &lt;a href=&#34;https://huggingface.co/showlab/show-1-base&#34;&gt;key frames generation&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/showlab/show-1-interpolation&#34;&gt;interpolation&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/showlab/show-1-sr1&#34;&gt;superresolution stage 1&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/showlab/show-1-sr2&#34;&gt;superresolution stage 2&lt;/a&gt; modules. We also use &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-II-L-v1.0&#34;&gt;deep-floyd-if superresolution stage 1&lt;/a&gt; model for the first frame superresolution. To download deep-floyd-if models, you need follow their &lt;a href=&#34;https://huggingface.co/DeepFloyd/IF-II-L-v1.0&#34;&gt;official instructions.&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;h2&gt;Inference&lt;/h2&gt; &#xA;  &lt;p&gt;To run diffusion models for text-to-video generation, run this command:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;The output videos from different modules will be stored in &#34;outputs&#34; folder with the gif format. The code will automatically download module weights from huggingface. Otherwise, you can download weights manually with git lfs then change the &#34;pretrained_model_path&#34; to your local path. Take key frames generation module for example:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git lfs install&#xA;git clone https://huggingface.co/showlab/show-1-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;h2&gt;Demo Video&lt;/h2&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/showlab/Show-1/assets/55792387/32242135-25a5-4757-b494-91bf314581e8&#34;&gt;https://github.com/showlab/Show-1/assets/55792387/32242135-25a5-4757-b494-91bf314581e8&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;h2&gt;Citation&lt;/h2&gt; &#xA;  &lt;p&gt;If you make use of our work, please cite our paper.&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{zhang2023show1,&#xA;      title={Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation}, &#xA;      author={David Junhao Zhang and Jay Zhangjie Wu and Jia-Wei Liu and Rui Zhao and Lingmin Ran and Yuchao Gu and Difei Gao and Mike Zheng Shou},&#xA;      year={2023},&#xA;      eprint={2309.15818},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;h2&gt;Shoutouts&lt;/h2&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This code heavily builds on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;, &lt;a href=&#34;https://github.com/deep-floyd/IF&#34;&gt;deep-floyd-if&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis&#34;&gt;modelscope&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/cerspense/zeroscope_v2_576w&#34;&gt;zeroscope&lt;/a&gt;. Thanks for open-sourcing!&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/div&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
</feed>