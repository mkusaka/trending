<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-10T01:39:22Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>W01fh4cker/CVE-2024-27198-RCE</title>
    <updated>2024-03-10T01:39:22Z</updated>
    <id>tag:github.com,2024-03-10:/W01fh4cker/CVE-2024-27198-RCE</id>
    <link href="https://github.com/W01fh4cker/CVE-2024-27198-RCE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CVE-2024-27198 &amp; CVE-2024-27199 Authentication Bypass --&gt; RCE in JetBrains TeamCity Pre-2023.11.4&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cyberspace Mapping Dork&lt;/h1&gt; &#xA;&lt;h2&gt;Fofa&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;app=&#34;JET_BRAINS-TeamCity&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ZoomEye&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;app:&#34;JetBrains TeamCity&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hunter.how&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;product.name=&#34;TeamCity&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Shodan&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;http.component:&#34;teamcity&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;How to use&lt;/h1&gt; &#xA;&lt;p&gt;I&#39;m using &lt;code&gt;Python3.9&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install requests urllib3 faker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;[!] Problem&lt;/h1&gt; &#xA;&lt;p&gt;There are currently some problems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If yakit proxy is not used, uploading of malicious plug-ins will fail;&lt;/li&gt; &#xA; &lt;li&gt;After the webshell is uploaded, if you want to make a post request, an error will be reported, as shown below:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240308230546869.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;But this problem can also be solved. Just get the X-TC-CSRF-Token corresponding to the session id through a 403 error. For example, I uploaded the webshell ofbehinder3.0 and connected:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240308230808193.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;There are still some details that need to be improved, such as the removal of malicious plugins, etc.If you have relevant coding skills, you can make pull requests.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Vulnerability recurrence environment construction&lt;/h1&gt; &#xA;&lt;p&gt;Use docker to pull the vulnerable image and start it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo docker pull jetbrains/teamcity-server:2023.11.3&#xA;sudo docker run -it -d --name teamcity -u root -p 8111:8111 jetbrains/teamcity-server:2023.11.3&#xA;# sudo ufw disable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then make basic settings:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240307232348600.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240307232404056.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Reference&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.rapid7.com/blog/post/2024/03/04/etr-cve-2024-27198-and-cve-2024-27199-jetbrains-teamcity-multiple-authentication-bypass-vulnerabilities-fixed/&#34;&gt;https://www.rapid7.com/blog/post/2024/03/04/etr-cve-2024-27198-and-cve-2024-27199-jetbrains-teamcity-multiple-authentication-bypass-vulnerabilities-fixed/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Chocapikk/CVE-2024-27198&#34;&gt;https://github.com/Chocapikk/CVE-2024-27198&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/maxtext</title>
    <updated>2024-03-10T01:39:22Z</updated>
    <id>tag:github.com,2024-03-10:/google/maxtext</id>
    <link href="https://github.com/google/maxtext" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple, performant and scalable Jax LLM!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/google/maxtext/actions/workflows/UnitTests.yml&#34;&gt;&lt;img src=&#34;https://github.com/google/maxtext/actions/workflows/UnitTests.yml/badge.svg?sanitize=true&#34; alt=&#34;Unit Tests&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;MaxText is a &lt;strong&gt;high performance&lt;/strong&gt;, &lt;strong&gt;arbitrarily scalable&lt;/strong&gt;, &lt;strong&gt;open-source&lt;/strong&gt;, &lt;strong&gt;simple&lt;/strong&gt;, &lt;strong&gt;easily forkable&lt;/strong&gt;, &lt;strong&gt;well-tested&lt;/strong&gt;, &lt;strong&gt;batteries included&lt;/strong&gt; LLM written in pure Python/Jax and targeting Google Cloud TPUs. MaxText typically achieves 55% to 60% model-flop utilization and scales from single host to very large clusters while staying simple and &#34;optimization-free&#34; thanks to the power of Jax and the XLA compiler.&lt;/p&gt; &#xA;&lt;p&gt;MaxText aims to be a launching off point for ambitious LLM projects both in research and production. We encourage users to start by experimenting with MaxText out of the box and then fork and modify MaxText to meet their needs.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/#runtime-performance-results&#34;&gt;Runtime Performance Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/#comparison-to-alternatives&#34;&gt;Comparison To Alternatives&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/#features-and-diagnostics&#34;&gt;Features and Diagnostics&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;We recommend starting with a single host first and then moving to multihost.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started: Download Dataset and Configure&lt;/h2&gt; &#xA;&lt;p&gt;You need to run these steps once per project prior to any local development or cluster experiments.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create two gcs buckets in your project, one for to downloading and retrieving the dataset and the other for storing the logs.&lt;/li&gt; &#xA; &lt;li&gt;Download the dataset in your gcs bucket&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_dataset.sh {GCS_PROJECT} {GCS_BUCKET_NAME}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set config values for &lt;code&gt;base_output_directory&lt;/code&gt; and &lt;code&gt;dataset_path&lt;/code&gt; in &lt;code&gt;configs/base.yml&lt;/code&gt;. &lt;code&gt;tokenizer_path&lt;/code&gt; is full path for loading the tokenizer. MaxText assumes these GCS buckets are created in the same project and that it has permissions to read and write from them. We also recommend reviewing the configurable options in &lt;code&gt;configs/base.yml&lt;/code&gt;, for instance you may change the &lt;code&gt;steps&lt;/code&gt; or &lt;code&gt;logging_period&lt;/code&gt; by either modifying &lt;code&gt;configs/base.yml&lt;/code&gt; or by passing in &lt;code&gt;steps&lt;/code&gt; and &lt;code&gt;logging_period&lt;/code&gt; as additional args to the &lt;code&gt;train.py&lt;/code&gt; call.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To run maxtext the TPUVMs must have permission to read the gcs bucket. These permissions are granted by service account roles, such as the &lt;code&gt;STORAGE ADMIN&lt;/code&gt; role.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started: Local Development for single host&lt;/h2&gt; &#xA;&lt;p&gt;Local development is a convenient way to run MaxText on a single host. It doesn&#39;t scale to multiple hosts.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.google.com/tpu/docs/users-guide-tpu-vm#creating_a_cloud_tpu_vm_with_gcloud&#34;&gt;Create and SSH to the single-host TPU of your choice.&lt;/a&gt; We recommend a &lt;code&gt;v4-8&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Clone MaxText onto that TPUVM.&lt;/li&gt; &#xA; &lt;li&gt;Within the root directory of that &lt;code&gt;git&lt;/code&gt; repo, install dependencies by running:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;After installation completes, run training with the command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 MaxText/train.py MaxText/configs/base.yml run_name=$YOUR_JOB_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;If you want to decode, you can decode as follows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 MaxText/decode.py MaxText/configs/base.yml run_name=$YOUR_JOB_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Be aware, these decodings will be random. To get high quality decodings you need pass in a checkpoint, typically via the &lt;code&gt;load_parameters_path&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h4&gt;Running on NVIDIA GPUs&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use &lt;code&gt;bash docker_build_dependency_image.sh DEVICE=gpu&lt;/code&gt; can be used to build a container with the required dependencies.&lt;/li&gt; &#xA; &lt;li&gt;After installation is completed, run training with the command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 MaxText/train.py MaxText/configs/base.yml run_name=$YOUR_JOB_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;If you want to decode, you can decode as follows.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 MaxText/decode.py MaxText/configs/base.yml run_name=$YOUR_JOB_NAME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you see the following error when running inside a container, set a larger &lt;code&gt;--shm-size&lt;/code&gt; (e.g. &lt;code&gt;--shm-size=1g&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Failed to execute XLA Runtime executable: run time error: custom call &#39;xla.gpu.all_reduce&#39; failed: external/xla/xla/service/gpu/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&amp;amp;comm, nranks, id, rank) failed: unhandled cuda error (run with NCCL_DEBUG=INFO for details); current tracing scope: all-reduce-start.2; current profiling annotation: XlaModule:#hlo_module=jit__unnamed_wrapped_function_,program_id=7#.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Starting: Multihost development&lt;/h2&gt; &#xA;&lt;p&gt;There are three patterns for running MaxText with more than one host.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;[GKE, recommended] &lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/getting_started/Run_MaxText_via_xpk.md&#34;&gt;Running Maxtext with xpk&lt;/a&gt; - Quick Experimentation and Production support&lt;/li&gt; &#xA; &lt;li&gt;[GCE] &lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/getting_started/Run_MaxText_via_multihost_job.md&#34;&gt;Running Maxtext with Multihost Jobs&lt;/a&gt; - Long Running Production Jobs with Queued Resources&lt;/li&gt; &#xA; &lt;li&gt;[GCE] &lt;a href=&#34;https://raw.githubusercontent.com/google/maxtext/main/getting_started/Run_MaxText_via_multihost_runner.md&#34;&gt;Running Maxtext with Multihost Runner&lt;/a&gt; - Fast experiments via multiple ssh connections.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Starting: Preflight Optimizations&lt;/h2&gt; &#xA;&lt;p&gt;Once you&#39;ve gotten workloads running, there are important optimizations you might want to put on your cluster. Please check the doc &lt;a href=&#34;https://github.com/google/maxtext/raw/main/PREFLIGHT.md&#34;&gt;PREFLIGHT.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Runtime Performance Results&lt;/h1&gt; &#xA;&lt;h2&gt;TPU v4&lt;/h2&gt; &#xA;&lt;p&gt;For a 22B model. See full run configs in &lt;code&gt;MaxText/configs/v4/&lt;/code&gt; as &lt;code&gt;22b.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hardware&lt;/th&gt; &#xA;   &lt;th&gt;TFLOP/sec/chip&lt;/th&gt; &#xA;   &lt;th&gt;MFU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1x v4-128&lt;/td&gt; &#xA;   &lt;td&gt;156&lt;/td&gt; &#xA;   &lt;td&gt;56.7%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2x v4-128&lt;/td&gt; &#xA;   &lt;td&gt;152&lt;/td&gt; &#xA;   &lt;td&gt;55.2%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4x v4-128&lt;/td&gt; &#xA;   &lt;td&gt;149&lt;/td&gt; &#xA;   &lt;td&gt;54.3%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x v4-128&lt;/td&gt; &#xA;   &lt;td&gt;146&lt;/td&gt; &#xA;   &lt;td&gt;53.2%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For a 52B model. See full run configs in &lt;code&gt;MaxText/configs/v4/&lt;/code&gt; as &lt;code&gt;52b.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hardware&lt;/th&gt; &#xA;   &lt;th&gt;TFLOP/sec/chip&lt;/th&gt; &#xA;   &lt;th&gt;MFU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1x v4-384&lt;/td&gt; &#xA;   &lt;td&gt;154&lt;/td&gt; &#xA;   &lt;td&gt;56.0%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2x v4-384&lt;/td&gt; &#xA;   &lt;td&gt;162&lt;/td&gt; &#xA;   &lt;td&gt;58.9%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TPU v5e&lt;/h2&gt; &#xA;&lt;p&gt;For 16B, 32B, 64B, and 128B models. See full run configs in &lt;code&gt;MaxText/configs/v5e/&lt;/code&gt; as &lt;code&gt;16b.sh&lt;/code&gt;, &lt;code&gt;32b.sh&lt;/code&gt;, &lt;code&gt;64b.sh&lt;/code&gt;, &lt;code&gt;128b.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hardware&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;16B TFLOP/sec/chip&lt;/th&gt; &#xA;   &lt;th&gt;16B MFU&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;32B TFLOP/sec/chip&lt;/th&gt; &#xA;   &lt;th&gt;32B MFU&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;64B TFLOP/sec/chip&lt;/th&gt; &#xA;   &lt;th&gt;64B MFU&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;128B TFLOP/sec/chip&lt;/th&gt; &#xA;   &lt;th&gt;128B MFU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1x v5e-256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;120&lt;/td&gt; &#xA;   &lt;td&gt;61.10%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;132&lt;/td&gt; &#xA;   &lt;td&gt;66.86%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;118&lt;/td&gt; &#xA;   &lt;td&gt;59.90%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;110&lt;/td&gt; &#xA;   &lt;td&gt;56.06%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2x v5e-256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;117&lt;/td&gt; &#xA;   &lt;td&gt;59.37%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;64.81%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;112&lt;/td&gt; &#xA;   &lt;td&gt;56.66%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;110&lt;/td&gt; &#xA;   &lt;td&gt;55.82%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4x v5e-256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;117&lt;/td&gt; &#xA;   &lt;td&gt;59.14%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;126&lt;/td&gt; &#xA;   &lt;td&gt;64.10%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;110&lt;/td&gt; &#xA;   &lt;td&gt;55.85%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;108&lt;/td&gt; &#xA;   &lt;td&gt;54.93%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8x v5e-256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;115&lt;/td&gt; &#xA;   &lt;td&gt;58.27%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;125&lt;/td&gt; &#xA;   &lt;td&gt;63.67%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;108&lt;/td&gt; &#xA;   &lt;td&gt;54.96%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;104&lt;/td&gt; &#xA;   &lt;td&gt;52.93%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16x v5e-256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;111&lt;/td&gt; &#xA;   &lt;td&gt;56.56%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;123&lt;/td&gt; &#xA;   &lt;td&gt;62.26%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;105&lt;/td&gt; &#xA;   &lt;td&gt;53.29%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;50.86%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;32x v5e-256&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;108&lt;/td&gt; &#xA;   &lt;td&gt;54.65%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;119&lt;/td&gt; &#xA;   &lt;td&gt;60.40%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;99&lt;/td&gt; &#xA;   &lt;td&gt;50.18%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;91&lt;/td&gt; &#xA;   &lt;td&gt;46.25%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More details on reproducing these results can be found in &lt;code&gt;MaxText/configs/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Comparison to Alternatives&lt;/h1&gt; &#xA;&lt;p&gt;MaxText is heavily inspired by &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;MinGPT&lt;/a&gt;/&lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;NanoGPT&lt;/a&gt;, elegant standalone GPT implementations written in PyTorch and targeting Nvidia GPUs. MaxText is more complex but has an MFU more than three times the &lt;a href=&#34;https://twitter.com/karpathy/status/1613250489097027584?cxt=HHwWgIDUhbixteMsAAAA&#34;&gt;17%&lt;/a&gt; reported most recently with that codebase, is massively scalable and implements a key-value cache for efficient auto-regressive decoding.&lt;/p&gt; &#xA;&lt;p&gt;MaxText is more similar to &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Nvidia/Megatron-LM&lt;/a&gt;, a very well tuned LLM implementation targeting Nvidia GPUs. The two implementations achieve comparable MFUs. The difference in the codebases highlights the different programming strategies. MaxText is pure Python, relying heavily on the XLA compiler to achieve high performance. By contrast, Megatron-LM is a mix of Python and CUDA, relying on well-optimized CUDA kernels to achieve high performance.&lt;/p&gt; &#xA;&lt;p&gt;MaxText is also comparable to &lt;a href=&#34;https://github.com/google/paxml&#34;&gt;Pax&lt;/a&gt;. Like Pax, MaxText provides high-performance and scalable implementations of LLMs in Jax. Pax focuses on enabling powerful configuration parameters, enabling developers to change the model by editing config parameters. By contrast, MaxText is a simple, concrete implementation of an LLM that encourages users to extend by forking and directly editing the source code. The right choice depends on your project&#39;s priorities.&lt;/p&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;Whether you are forking MaxText for your own needs or intending to contribute back to the community, we wanted to offer simple testing recipes.&lt;/p&gt; &#xA;&lt;p&gt;To run unit tests and lint, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash unit_test_and_lint.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The full suite of end-to-end tests is in &lt;code&gt;end_to_end/&lt;/code&gt;. We run them with a nightly cadence.&lt;/p&gt; &#xA;&lt;h1&gt;Features and Diagnostics&lt;/h1&gt; &#xA;&lt;h2&gt;Collect Stack Traces&lt;/h2&gt; &#xA;&lt;p&gt;When running a Single Program, Multiple Data (SPMD) job on TPU VMs, the overall process can hang if there is any error or any VM hangs/crashes for some reason. In this scenario, capturing stack traces will help to identify and troubleshoot the issues for the jobs running on TPU VMs.&lt;/p&gt; &#xA;&lt;p&gt;The following configurations will help to debug a fault or when a program is stuck or hung somewhere by collecting stack traces. Change the parameter values accordingly in &lt;code&gt;MaxText/configs/base.yml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set &lt;code&gt;collect_stack_trace: True&lt;/code&gt; to enable collection of stack traces on faults or when the program is hung. This setting will periodically dump the traces for the program to help in debugging. To disable this, set &lt;code&gt;collect_stack_trace: False&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;stack_trace_to_cloud: False&lt;/code&gt; to display stack traces on console. &lt;code&gt;stack_trace_to_cloud: True&lt;/code&gt; will create a temporary file in &lt;code&gt;/tmp/debugging&lt;/code&gt; in the TPUs to store the stack traces. There is an agent running on TPU VMs that will periodically upload the traces from the temporary directory to cloud logging in the gcp project. You can view the traces in Logs Explorer on Cloud Logging using the following query:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;logName=&#34;projects/&amp;lt;project_name&amp;gt;/logs/tpu.googleapis.com%2Fruntime_monitor&#34;&#xA;jsonPayload.verb=&#34;stacktraceanalyzer&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;code&gt;stack_trace_interval_seconds&lt;/code&gt; signifies the duration in seconds between each stack trace collection event. Setting &lt;code&gt;stack_trace_interval_seconds: 600&lt;/code&gt; will collect the stack traces every 600 seconds (10 minutes).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Here is the related PyPI package: &lt;a href=&#34;https://pypi.org/project/cloud-tpu-diagnostics&#34;&gt;https://pypi.org/project/cloud-tpu-diagnostics&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Ahead of Time Compilation (AOT)&lt;/h2&gt; &#xA;&lt;p&gt;To compile your training run ahead of time, we provide a tool &lt;code&gt;train_compile.py&lt;/code&gt;. This tool allows you to compile the main &lt;code&gt;train_step&lt;/code&gt; in &lt;code&gt;train.py&lt;/code&gt; for target hardware (e.g. a large number of v5e devices) without using the target hardware, and instead you may use only a CPU or a single VM from a different family. This compilation helps with two main goals:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;It will flag any out of memory (OOM) information, such as when the &lt;code&gt;per_device_batch_size&lt;/code&gt; is set too high, with an identical OOM stack trace as if it was compiled on the target hardware.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The ahead of time compilation can be saved and then loaded for fast startup and restart times on the target hardware.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The tool &lt;code&gt;train_compile.py&lt;/code&gt; is tightly linked to &lt;code&gt;train.py&lt;/code&gt; and uses the same configuration file &lt;code&gt;configs/base.yml&lt;/code&gt;. Although you don&#39;t need to run on a TPU, you do need to install &lt;code&gt;jax[tpu]&lt;/code&gt; in addition to other dependencies, so we recommend running &lt;code&gt;setup.sh&lt;/code&gt; to install these if you have not already done so.&lt;/p&gt; &#xA;&lt;h3&gt;Example AOT 1: Compile ahead of time basics&lt;/h3&gt; &#xA;&lt;p&gt;After installing the dependencies listed above, you are ready to compile ahead of time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Run the below on a single machine, e.g. a CPU&#xA;python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 compile_topology_num_slices=2 \&#xA;global_parameter_scale=16 per_device_batch_size=4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will compile a 16B parameter MaxText model on 2 v5e pods.&lt;/p&gt; &#xA;&lt;h3&gt;Example AOT 2: Save compiled function, then load and run it&lt;/h3&gt; &#xA;&lt;p&gt;Here is an example that saves then loads the compiled &lt;code&gt;train_step&lt;/code&gt;, starting with the save:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1: Run AOT and save compiled function&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Run the below on a single machine, e.g. a CPU&#xA;export LIBTPU_INIT_ARGS=&#34;--xla_enable_async_all_gather=true&#34;&#xA;python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 \&#xA;compile_topology_num_slices=2 \&#xA;compiled_trainstep_file=my_compiled_train.pickle global_parameter_scale=16 \&#xA;per_device_batch_size=4 steps=10000 learning_rate=1e-3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2: Run train.py and load the compiled function&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To load the compiled train_step, you just need to pass &lt;code&gt;compiled_trainstep_file=my_compiled_train.pickle&lt;/code&gt; into &lt;code&gt;train.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Run the below on each host of the target hardware, e.g. each host on 2 slices of v5e-256&#xA;export LIBTPU_INIT_ARGS=&#34;--xla_enable_async_all_gather=true&#34;&#xA;python3 MaxText/train.py MaxText/configs/base.yml run_name=example_load_compile \&#xA;compiled_trainstep_file=my_compiled_train.pickle \&#xA;global_parameter_scale=16  per_device_batch_size=4 steps=10000 learning_rate=1e-3 \&#xA;base_output_directory=gs://my-output-bucket dataset_path=gs://my-dataset-bucket&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the save step of example 2 above we included exporting the compiler flag &lt;code&gt;LIBTPU_INIT_ARGS&lt;/code&gt; and &lt;code&gt;learning_rate&lt;/code&gt; because those affect the compiled object &lt;code&gt;my_compiled_train.pickle.&lt;/code&gt; The sizes of the model (e.g. &lt;code&gt;global_parameter_scale&lt;/code&gt;, &lt;code&gt;max_sequence_length&lt;/code&gt; and &lt;code&gt;per_device_batch&lt;/code&gt;) are fixed when you initially compile via &lt;code&gt;compile_train.py&lt;/code&gt;, you will see a size error if you try to run the saved compiled object with different sizes than you compiled with. However a subtle note is that the &lt;strong&gt;learning rate schedule&lt;/strong&gt; is also fixed when you run &lt;code&gt;compile_train&lt;/code&gt; - which is determined by both &lt;code&gt;steps&lt;/code&gt; and &lt;code&gt;learning_rate&lt;/code&gt;. The optimizer parameters such as &lt;code&gt;adam_b1&lt;/code&gt; are passed only as shaped objects to the compiler - thus their real values are determined when you run &lt;code&gt;train.py&lt;/code&gt;, not during the compilation. If you do pass in different shapes (e.g. &lt;code&gt;per_device_batch&lt;/code&gt;), you will get a clear error message reporting that the compiled signature has different expected shapes than what was input. If you attempt to run on different hardware than the compilation targets requested via &lt;code&gt;compile_topology&lt;/code&gt;, you will get an error saying there is a failure to map the devices from the compiled to your real devices. Using different XLA flags or a LIBTPU than what was compiled will probably run silently with the environment you compiled in without error. However there is no guaranteed behavior in this case; you should run in the same environment you compiled in.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Open Models&lt;/h2&gt; &#xA;&lt;p&gt;MaxText supports training and inference of various open models. Follow user guides under &lt;a href=&#34;https://github.com/google/maxtext/tree/main/getting_started&#34;&gt;getting started&lt;/a&gt; section to know more.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;Gemma&lt;/a&gt;: a family of open-weights Large Language Model (LLM) by &lt;a href=&#34;https://deepmind.google/&#34;&gt;Google DeepMind&lt;/a&gt;, based on Gemini research and technology. &lt;br&gt; You can run decode and finetuning using instructions mentioned &lt;a href=&#34;https://github.com/google/maxtext/raw/main/getting_started/Run_Gemma.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>JoeanAmier/XHS-Downloader</title>
    <updated>2024-03-10T01:39:22Z</updated>
    <id>tag:github.com,2024-03-10:/JoeanAmier/XHS-Downloader</id>
    <link href="https://github.com/JoeanAmier/XHS-Downloader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;小红书链接提取/作品采集工具：提取账号发布、收藏、点赞作品链接；提取搜索结果作品、用户链接；采集小红书作品信息；提取小红书作品下载地址；下载小红书无水印作品文件！&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/XHS-Downloader.png&#34; alt=&#34;&#34; height=&#34;256&#34; width=&#34;256&#34;&gt;&#xA; &lt;br&gt; &#xA; &lt;h1&gt;XHS-Downloader&lt;/h1&gt; &#xA; &lt;p&gt;简体中文 | &lt;a href=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/README_EN.md&#34;&gt;English&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img alt=&#34;GitHub&#34; src=&#34;https://img.shields.io/github/license/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=ff7a45&#34;&gt; &#xA; &lt;img alt=&#34;GitHub forks&#34; src=&#34;https://img.shields.io/github/forks/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=9254de&#34;&gt; &#xA; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=ff7875&#34;&gt; &#xA; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/UserScript-ffec3d?style=for-the-badge&amp;amp;logo=tampermonkey&amp;amp;logoColor=%2300485B&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;img alt=&#34;GitHub code size in bytes&#34; src=&#34;https://img.shields.io/github/languages/code-size/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=73d13d&#34;&gt; &#xA; &lt;img alt=&#34;GitHub release (with filter)&#34; src=&#34;https://img.shields.io/github/v/release/JoeanAmier/XHS-Downloader?style=for-the-badge&amp;amp;color=40a9ff&#34;&gt; &#xA; &lt;img alt=&#34;GitHub all releases&#34; src=&#34;https://img.shields.io/github/downloads/JoeanAmier/XHS-Downloader/total?style=for-the-badge&amp;amp;color=f759ab&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;🔥 &lt;b&gt;小红书链接提取/作品采集工具&lt;/b&gt;：提取账号发布、收藏、点赞作品链接；提取搜索结果作品、用户链接；采集小红书作品信息；提取小红书作品下载地址；下载小红书无水印作品文件！&lt;/p&gt; &#xA;&lt;p&gt;❤️ 作者仅在 GitHub 发布 XHS-Downloader，未与任何个人或网站合作发布，项目没有任何收费计划，谨防上当受骗！&lt;/p&gt; &#xA;&lt;h1&gt;📑 项目功能&lt;/h1&gt; &#xA;&lt;ul&gt;&#xA; &lt;b&gt;程序功能&lt;/b&gt; &#xA; &lt;li&gt;✅ 采集小红书作品信息&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取小红书作品下载地址&lt;/li&gt; &#xA; &lt;li&gt;✅ 下载小红书无水印作品文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 自动跳过已下载的作品文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 作品文件完整性处理机制&lt;/li&gt; &#xA; &lt;li&gt;✅ 自定义图文作品文件下载格式&lt;/li&gt; &#xA; &lt;li&gt;✅ 持久化储存作品信息至文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 作品文件储存至单独文件夹&lt;/li&gt; &#xA; &lt;li&gt;✅ 后台监听剪贴板下载作品&lt;/li&gt; &#xA; &lt;li&gt;✅ 记录已下载作品 ID&lt;/li&gt; &#xA; &lt;li&gt;☑️ 支持 API 调用功能&lt;/li&gt; &#xA; &lt;li&gt;☑️ 支持命令行参数下载作品文件&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt;&#xA; &lt;b&gt;脚本功能&lt;/b&gt; &#xA; &lt;li&gt;✅ 下载小红书无水印作品文件&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取发现页面作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取账号发布作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取账号收藏作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取账号点赞作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取搜索结果作品链接&lt;/li&gt; &#xA; &lt;li&gt;✅ 提取搜索结果用户链接&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;📸 程序截图&lt;/h1&gt; &#xA;&lt;p&gt;&lt;b&gt;🎥 点击图片观看演示视频&lt;/b&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/程序运行截图CN1.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/程序运行截图CN2.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;hr&gt; &#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1PJ4m1Y7Jt/&#34;&gt;&lt;img src=&#34;static/screenshot/程序运行截图CN3.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;h1&gt;🔗 支持链接&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;https://www.xiaohongshu.com/explore/作品ID&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;https://www.xiaohongshu.com/discovery/item/作品ID&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;https://xhslink.com/分享码&lt;/code&gt;&lt;/li&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;b&gt;支持单次输入多个作品链接，链接之间使用空格分隔。&lt;/b&gt;&lt;/p&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;🪟 关于终端&lt;/h1&gt; &#xA;&lt;p&gt;⭐ 推荐使用 &lt;a href=&#34;https://learn.microsoft.com/zh-cn/windows/terminal/install&#34;&gt;Windows 终端&lt;/a&gt; （Windows 11 默认终端）运行程序以便获得最佳显示效果！&lt;/p&gt; &#xA;&lt;h1&gt;🥣 使用方法&lt;/h1&gt; &#xA;&lt;p&gt;如果仅需下载无水印作品文件，建议选择 &lt;b&gt;程序运行&lt;/b&gt;；如果有其他需求，建议选择 &lt;b&gt;源码运行&lt;/b&gt;！&lt;/p&gt; &#xA;&lt;h2&gt;🖱 程序运行&lt;/h2&gt; &#xA;&lt;p&gt;Windows 10 及以上用户可前往 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/releases/latest&#34;&gt;Releases&lt;/a&gt; 下载程序压缩包，解压后打开程序文件夹，双击运行 &lt;code&gt;main.exe&lt;/code&gt; 即可使用。&lt;/p&gt; &#xA;&lt;p&gt;若通过此方式使用程序，文件默认下载路径为：&lt;code&gt;.\_internal\Download&lt;/code&gt;；配置文件路径为：&lt;code&gt;.\_internal\settings.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;⌨️ 源码运行&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;安装版本号不低于 &lt;code&gt;3.12&lt;/code&gt; 的 Python 解释器&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt&lt;/code&gt; 命令安装程序所需模块&lt;/li&gt; &#xA; &lt;li&gt;下载本项目最新的源码或 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/releases/latest&#34;&gt;Releases&lt;/a&gt; 发布的源码至本地&lt;/li&gt; &#xA; &lt;li&gt;运行 &lt;code&gt;main.py&lt;/code&gt; 即可使用&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;🕹 用户脚本&lt;/h1&gt; &#xA;&lt;img src=&#34;static/screenshot/用户脚本截图1.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;img src=&#34;static/screenshot/用户脚本截图2.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;p&gt;如果您的浏览器安装了 &lt;a href=&#34;https://www.tampermonkey.net/&#34;&gt;Tampermonkey&lt;/a&gt; 浏览器扩展程序，可以添加 &lt;a href=&#34;https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/XHS-Downloader.js&#34;&gt;用户脚本&lt;/a&gt;，无需下载安装即可体验项目功能！&lt;/p&gt; &#xA;&lt;p&gt;提示：使用 XHS-Downloader 用户脚本批量提取作品链接，搭配 XHS-Downloader 程序可以实现批量下载无水印作品文件！&lt;/p&gt; &#xA;&lt;h1&gt;💻 二次开发&lt;/h1&gt; &#xA;&lt;p&gt;如果有其他需求，可以根据 &lt;code&gt;main.py&lt;/code&gt; 的注释提示进行代码调用或修改！&lt;/p&gt; &#xA;&lt;pre&gt;&#xA;async def example():&#xA;    &#34;&#34;&#34;通过代码设置参数，适合二次开发&#34;&#34;&#34;&#xA;    # 示例链接&#xA;    error_link = &#34;https://github.com/JoeanAmier/XHS_Downloader&#34;&#xA;    demo_link = &#34;https://www.xiaohongshu.com/explore/xxxxxxxxxx&#34;&#xA;    multiple_links = f&#34;{demo_link} {demo_link} {demo_link}&#34;&#xA;    # 实例对象&#xA;    work_path = &#34;D:\\&#34;  # 作品数据/文件保存根路径，默认值：项目根路径&#xA;    folder_name = &#34;Download&#34;  # 作品文件储存文件夹名称（自动创建），默认值：Download&#xA;    user_agent = &#34;&#34;  # 请求头 User-Agent，可选参数&#xA;    cookie = &#34;&#34;  # 小红书网页版 Cookie，无需登录，必需参数&#xA;    proxy = None  # 网络代理&#xA;    timeout = 5  # 请求数据超时限制，单位：秒，默认值：10&#xA;    chunk = 1024 * 1024 * 10  # 下载文件时，每次从服务器获取的数据块大小，单位：字节&#xA;    max_retry = 2  # 请求数据失败时，重试的最大次数，单位：秒，默认值：5&#xA;    record_data = False  # 是否记录作品数据至文件&#xA;    image_format = &#34;WEBP&#34;  # 图文作品文件下载格式，支持：PNG、WEBP&#xA;    folder_mode = False  # 是否将每个作品的文件储存至单独的文件夹&#xA;    async with XHS() as xhs:&#xA;        pass  # 使用默认参数&#xA;    async with XHS(work_path=work_path,&#xA;                   folder_name=folder_name,&#xA;                   user_agent=user_agent,&#xA;                   cookie=cookie,&#xA;                   proxy=proxy,&#xA;                   timeout=timeout,&#xA;                   chunk=chunk,&#xA;                   max_retry=max_retry,&#xA;                   record_data=record_data,&#xA;                   image_format=image_format,&#xA;                   folder_mode=folder_mode,&#xA;                   ) as xhs:  # 使用自定义参数&#xA;        download = True  # 是否下载作品文件，默认值：False&#xA;        efficient = True  # 高效模式，禁用请求延时&#xA;        # 返回作品详细信息，包括下载地址&#xA;        print(await xhs.extract(error_link, download, efficient))  # 获取数据失败时返回空字典&#xA;        print(await xhs.extract(demo_link, download, efficient))&#xA;        print(await xhs.extract(multiple_links, download, efficient))  # 支持传入多个作品链接&#xA;&lt;/pre&gt; &#xA;&lt;h1&gt;⚙️ 配置文件&lt;/h1&gt; &#xA;&lt;p&gt;项目根目录下的 &lt;code&gt;settings.json&lt;/code&gt; 文件，首次运行自动生成，可以自定义部分运行参数。&lt;/p&gt; &#xA;&lt;p&gt;建议自行设置 &lt;code&gt;cookie&lt;/code&gt; 参数，若不设置该参数，程序功能可能无法正常使用！&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;参数&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;类型&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;含义&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;默认值&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;work_path&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;作品数据 / 文件保存根路径&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;项目根路径&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;folder_name&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;作品文件储存文件夹名称&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Download&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;user_agent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;请求头 User-Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;默认 UA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;cookie&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;小红书网页版 Cookie，&lt;b&gt;无需登录&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;无&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;proxy&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;设置程序代理&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;null&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;timeout&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;请求数据超时限制，单位：秒&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;chunk&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;下载文件时，每次从服务器获取的数据块大小，单位：字节&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1048576(1 MB)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;max_retry&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;int&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;请求数据失败时，重试的最大次数，单位：秒&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;record_data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;是否记录作品数据至 &lt;code&gt;TXT&lt;/code&gt; 文件&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;false&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;image_format&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;图文作品文件下载格式，支持：&lt;code&gt;PNG&lt;/code&gt;、&lt;code&gt;WEBP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PNG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;folder_mode&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;bool&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;是否将每个作品的文件储存至单独的文件夹；文件夹名称与文件名称保持一致&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;false&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;language&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;str&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;设置程序语言，目前支持：&lt;code&gt;zh-CN&lt;/code&gt;、&lt;code&gt;en-GB&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zh-CN&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;🌐 Cookie&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;打开浏览器（可选无痕模式启动），访问 &lt;code&gt;https://www.xiaohongshu.com/explore&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;按下 &lt;code&gt;F12&lt;/code&gt; 打开开发人员工具&lt;/li&gt; &#xA; &lt;li&gt;选择 &lt;code&gt;网络&lt;/code&gt; 选项卡&lt;/li&gt; &#xA; &lt;li&gt;选择 &lt;code&gt;Fetch/XHR&lt;/code&gt; 筛选器&lt;/li&gt; &#xA; &lt;li&gt;点击小红书页面任意作品&lt;/li&gt; &#xA; &lt;li&gt;在 &lt;code&gt;网络&lt;/code&gt; 选项卡挑选包含 Cookie 的数据包&lt;/li&gt; &#xA; &lt;li&gt;检查 Cookie 是否包含 &lt;code&gt;web_session&lt;/code&gt; 字段&lt;/li&gt; &#xA; &lt;li&gt;全选复制包含 &lt;code&gt;web_session&lt;/code&gt; 字段的 Cookie&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;static/screenshot/获取Cookie示意图.png&#34; alt=&#34;&#34;&gt; &#xA;&lt;h1&gt;🗳 下载记录&lt;/h1&gt; &#xA;&lt;p&gt;XHS-Downloader 会将下载过的作品 ID 储存至数据库，当重复下载相同的作品时，XHS-Downloader 会自动跳过该作品的文件下载（即使作品文件不存在），如果想要重新下载作品文件，请先删除数据库中对应的作品 ID，再使用 XHS-Downloader 下载作品文件！&lt;/p&gt; &#xA;&lt;h1&gt;♥️ 支持项目&lt;/h1&gt; &#xA;&lt;p&gt;如果 &lt;b&gt;XHS-Downloader&lt;/b&gt; 对您有帮助，请考虑为它点个 &lt;b&gt;Star&lt;/b&gt; ⭐，感谢您的支持！&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;微信(WeChat)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;支付宝(Alipay)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;./static/微信赞助二维码.png&#34; alt=&#34;微信赞助二维码&#34; height=&#34;200&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;./static/支付宝赞助二维码.png&#34; alt=&#34;支付宝赞助二维码&#34; height=&#34;200&#34; width=&#34;200&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;如果您愿意，可以考虑提供资助为 &lt;b&gt;XHS-Downloader&lt;/b&gt; 提供额外的支持！&lt;/p&gt; &#xA;&lt;h1&gt;✉️ 联系作者&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;QQ: 2437596031（联系请说明来意）&lt;/li&gt; &#xA; &lt;li&gt;QQ Group: &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/static/QQ%E7%BE%A4%E8%81%8A%E4%BA%8C%E7%BB%B4%E7%A0%81.png&#34;&gt;点击扫码加入群聊&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email: yonglelolu@gmail.com&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt; &lt;b&gt;如果您在使用 XHS-Downloader 的时候遇到问题，请先阅读&lt;a href=&#34;https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/raw/main/README-zh_CN.md&#34;&gt;《提问的智慧》&lt;/a&gt;，然后加入 QQ 群聊寻求帮助！&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt; &lt;b&gt;如果您通过 Email 联系我，我可能无法及时查看并回复信息，我会尽力在七天内回复您的邮件；如果有紧急事项或需要更快的回复，请通过其他方式与我联系，谢谢理解！&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;b&gt;如果您对抖音 / TikTok 感兴趣，可以了解一下我的另一个开源项目 &lt;a href=&#34;https://github.com/JoeanAmier/TikTokDownloader&#34;&gt;TikTokDownloader&lt;/a&gt;&lt;/b&gt;&lt;/p&gt; &#xA;&lt;h1&gt;⚠️ 免责声明&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用者对本项目的使用由使用者自行决定，并自行承担风险。作者对使用者使用本项目所产生的任何损失、责任、或风险概不负责。&lt;/li&gt; &#xA; &lt;li&gt;本项目的作者提供的代码和功能是基于现有知识和技术的开发成果。作者尽力确保代码的正确性和安全性，但不保证代码完全没有错误或缺陷。&lt;/li&gt; &#xA; &lt;li&gt;使用者在使用本项目时必须严格遵守 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt; 的要求，并在适当的地方注明使用了 &lt;a href=&#34;https://github.com/JoeanAmier/XHS-Downloader/raw/master/LICENSE&#34;&gt;GNU General Public License v3.0&lt;/a&gt; 的代码。 &lt;/li&gt; &#xA; &lt;li&gt;使用者在任何情况下均不得将本项目的作者、贡献者或其他相关方与使用者的使用行为联系起来，或要求其对使用者使用本项目所产生的任何损失或损害负责。&lt;/li&gt; &#xA; &lt;li&gt;使用者在使用本项目的代码和功能时，必须自行研究相关法律法规，并确保其使用行为合法合规。任何因违反法律法规而导致的法律责任和风险，均由使用者自行承担。&lt;/li&gt; &#xA; &lt;li&gt;本项目的作者不会提供 XHS-Downloader 项目的付费版本，也不会提供与 XHS-Downloader 项目相关的任何商业服务。&lt;/li&gt; &#xA; &lt;li&gt;基于本项目进行的任何二次开发、修改或编译的程序与原创作者无关，原创作者不承担与二次开发行为或其结果相关的任何责任，使用者应自行对因二次开发可能带来的各种情况负全部责任。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;b&gt;在使用本项目的代码和功能之前，请您认真考虑并接受以上免责声明。如果您对上述声明有任何疑问或不同意，请不要使用本项目的代码和功能。如果您使用了本项目的代码和功能，则视为您已完全理解并接受上述免责声明，并自愿承担使用本项目的一切风险和后果。&lt;/b&gt; &#xA;&lt;h1&gt;💡 代码参考&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://textual.textualize.io/&#34;&gt;https://textual.textualize.io/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.aiohttp.org/en/stable/&#34;&gt;https://docs.aiohttp.org/en/stable/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>