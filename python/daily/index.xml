<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-21T01:33:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rspeer/wordfreq</title>
    <updated>2024-09-21T01:33:56Z</updated>
    <id>tag:github.com,2024-09-21:/rspeer/wordfreq</id>
    <link href="https://github.com/rspeer/wordfreq" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Access a database of word frequencies, in various natural languages.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;wordfreq is a Python library for looking up the frequencies of words in many languages, based on many sources of data.&lt;/p&gt; &#xA;&lt;p&gt;The word frequencies are a snapshot of language usage through about 2021. I may continue to make packaging updates, but the data is unlikely to be updated again. The world where I had a reasonable way to collect reliable word frequencies is not the world we live in now. See &lt;a href=&#34;https://raw.githubusercontent.com/rspeer/wordfreq/master/SUNSET.md&#34;&gt;SUNSET.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;Author: Robyn Speer&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;wordfreq requires Python 3 and depends on a few other Python modules (msgpack, langcodes, and regex). You can install it and its dependencies in the usual way, either by getting it from pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install wordfreq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or by getting the repository and installing it for development, using &lt;a href=&#34;https://python-poetry.org/&#34;&gt;poetry&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/rspeer/wordfreq/master/#additional-cjk-installation&#34;&gt;Additional CJK installation&lt;/a&gt; for extra steps that are necessary to get Chinese, Japanese, and Korean word frequencies.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;wordfreq provides access to estimates of the frequency with which a word is used, in over 40 languages (see &lt;em&gt;Supported languages&lt;/em&gt; below). It uses many different data sources, not just one corpus.&lt;/p&gt; &#xA;&lt;p&gt;It provides both &#39;small&#39; and &#39;large&#39; wordlists:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &#39;small&#39; lists take up very little memory and cover words that appear at least once per million words.&lt;/li&gt; &#xA; &lt;li&gt;The &#39;large&#39; lists cover words that appear at least once per 100 million words.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The default list is &#39;best&#39;, which uses &#39;large&#39; if it&#39;s available for the language, and &#39;small&#39; otherwise.&lt;/p&gt; &#xA;&lt;p&gt;The most straightforward function for looking up frequencies is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;word_frequency(word, lang, wordlist=&#39;best&#39;, minimum=0.0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This function looks up a word&#39;s frequency in the given language, returning its frequency as a decimal between 0 and 1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from wordfreq import word_frequency&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#39;cafe&#39;, &#39;en&#39;)&#xA;1.23e-05&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#39;café&#39;, &#39;en&#39;)&#xA;5.62e-06&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#39;cafe&#39;, &#39;fr&#39;)&#xA;1.51e-06&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#39;café&#39;, &#39;fr&#39;)&#xA;5.75e-05&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;zipf_frequency&lt;/code&gt; is a variation on &lt;code&gt;word_frequency&lt;/code&gt; that aims to return the word frequency on a human-friendly logarithmic scale. The Zipf scale was proposed by Marc Brysbaert, who created the SUBTLEX lists. The Zipf frequency of a word is the base-10 logarithm of the number of times it appears per billion words. A word with Zipf value 6 appears once per thousand words, for example, and a word with Zipf value 3 appears once per million words.&lt;/p&gt; &#xA;&lt;p&gt;Reasonable Zipf values are between 0 and 8, but because of the cutoffs described above, the minimum Zipf value appearing in these lists is 1.0 for the &#39;large&#39; wordlists and 3.0 for &#39;small&#39;. We use 0 as the default Zipf value for words that do not appear in the given wordlist, although it should mean one occurrence per billion words.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from wordfreq import zipf_frequency&#xA;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;the&#39;, &#39;en&#39;)&#xA;7.73&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;word&#39;, &#39;en&#39;)&#xA;5.26&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;frequency&#39;, &#39;en&#39;)&#xA;4.36&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;zipf&#39;, &#39;en&#39;)&#xA;1.49&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;zipf&#39;, &#39;en&#39;, wordlist=&#39;small&#39;)&#xA;0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The parameters to &lt;code&gt;word_frequency&lt;/code&gt; and &lt;code&gt;zipf_frequency&lt;/code&gt; are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;word&lt;/code&gt;: a Unicode string containing the word to look up. Ideally the word is a single token according to our tokenizer, but if not, there is still hope -- see &lt;em&gt;Tokenization&lt;/em&gt; below.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;lang&lt;/code&gt;: the BCP 47 or ISO 639 code of the language to use, such as &#39;en&#39;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;wordlist&lt;/code&gt;: which set of word frequencies to use. Current options are &#39;small&#39;, &#39;large&#39;, and &#39;best&#39;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;minimum&lt;/code&gt;: If the word is not in the list or has a frequency lower than &lt;code&gt;minimum&lt;/code&gt;, return &lt;code&gt;minimum&lt;/code&gt; instead. You may want to set this to the minimum value contained in the wordlist, to avoid a discontinuity where the wordlist ends.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Frequency bins&lt;/h2&gt; &#xA;&lt;p&gt;wordfreq&#39;s wordlists are designed to load quickly and take up little space in the repository. We accomplish this by avoiding meaningless precision and packing the words into frequency bins.&lt;/p&gt; &#xA;&lt;p&gt;In wordfreq, all words that have the same Zipf frequency rounded to the nearest hundredth have the same frequency. We don&#39;t store any more precision than that. So instead of having to store that the frequency of a word is .000011748975549395302, where most of those digits are meaningless, we just store the frequency bins and the words they contain.&lt;/p&gt; &#xA;&lt;p&gt;Because the Zipf scale is a logarithmic scale, this preserves the same relative precision no matter how far down you are in the word list. The frequency of any word is precise to within 1%.&lt;/p&gt; &#xA;&lt;p&gt;(This is not a claim about &lt;em&gt;accuracy&lt;/em&gt;, but about &lt;em&gt;precision&lt;/em&gt;. We believe that the way we use multiple data sources and discard outliers makes wordfreq a more accurate measurement of the way these words are really used in written language, but it&#39;s unclear how one would measure this accuracy.)&lt;/p&gt; &#xA;&lt;h2&gt;The figure-skating metric&lt;/h2&gt; &#xA;&lt;p&gt;We combine word frequencies from different sources in a way that&#39;s designed to minimize the impact of outliers. The method reminds me of the scoring system in Olympic figure skating:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Find the frequency of each word according to each data source.&lt;/li&gt; &#xA; &lt;li&gt;For each word, drop the sources that give it the highest and lowest frequency.&lt;/li&gt; &#xA; &lt;li&gt;Average the remaining frequencies.&lt;/li&gt; &#xA; &lt;li&gt;Rescale the resulting frequency list to add up to 1.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Numbers&lt;/h2&gt; &#xA;&lt;p&gt;These wordlists would be enormous if they stored a separate frequency for every number, such as if we separately stored the frequencies of 484977 and 484978 and 98.371 and every other 6-character sequence that could be considered a number.&lt;/p&gt; &#xA;&lt;p&gt;Instead, we have a frequency-bin entry for every number of the same &#34;shape&#34;, such as &lt;code&gt;##&lt;/code&gt; or &lt;code&gt;####&lt;/code&gt; or &lt;code&gt;#.#####&lt;/code&gt;, with &lt;code&gt;#&lt;/code&gt; standing in for digits. (For compatibility with earlier versions of wordfreq, our stand-in character is actually &lt;code&gt;0&lt;/code&gt;.) This is the same form of aggregation that the word2vec vocabulary does.&lt;/p&gt; &#xA;&lt;p&gt;Single-digit numbers are unaffected by this process; &#34;0&#34; through &#34;9&#34; have their own entries in each language&#39;s wordlist.&lt;/p&gt; &#xA;&lt;p&gt;When asked for the frequency of a token containing multiple digits, we multiply the frequency of that aggregated entry by a distribution estimating the frequency of those digits. The distribution only looks at two things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The value of the first digit&lt;/li&gt; &#xA; &lt;li&gt;Whether it is a 4-digit sequence that&#39;s likely to represent a year&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The first digits are assigned probabilities by Benford&#39;s law, and years are assigned probabilities from a distribution that peaks at the &#34;present&#34;. I explored this in a Twitter thread at &lt;a href=&#34;https://twitter.com/r_speer/status/1493715982887571456&#34;&gt;https://twitter.com/r_speer/status/1493715982887571456&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The part of this distribution representing the &#34;present&#34; is not strictly a peak and doesn&#39;t move forward with time as the present does. Instead, it&#39;s a 20-year-long plateau from 2019 to 2039. (2019 is the last time Google Books Ngrams was updated, and 2039 is a time by which I will probably have figured out a new distribution.)&lt;/p&gt; &#xA;&lt;p&gt;Some examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;2022&#34;, &#34;en&#34;)&#xA;5.15e-05&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;1922&#34;, &#34;en&#34;)&#xA;8.19e-06&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;1022&#34;, &#34;en&#34;)&#xA;1.28e-07&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Aside from years, the distribution does not care about the meaning of the numbers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;90210&#34;, &#34;en&#34;)&#xA;3.34e-10&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;92222&#34;, &#34;en&#34;)&#xA;3.34e-10&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;802.11n&#34;, &#34;en&#34;)&#xA;9.04e-13&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;899.19n&#34;, &#34;en&#34;)&#xA;9.04e-13&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The digit rule applies to other systems of digits, and only cares about the numeric value of the digits:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;٥٤&#34;, &#34;ar&#34;)&#xA;6.64e-05&#xA;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;54&#34;, &#34;ar&#34;)&#xA;6.64e-05&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It doesn&#39;t know which language uses which writing system for digits:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; word_frequency(&#34;٥٤&#34;, &#34;en&#34;)&#xA;5.4e-05&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sources and supported languages&lt;/h2&gt; &#xA;&lt;p&gt;This data comes from a Luminoso project called &lt;a href=&#34;https://github.com/LuminosoInsight/exquisite-corpus&#34;&gt;Exquisite Corpus&lt;/a&gt;, whose goal is to download good, varied, multilingual corpus data, process it appropriately, and combine it into unified resources such as wordfreq.&lt;/p&gt; &#xA;&lt;p&gt;Exquisite Corpus compiles 8 different domains of text, some of which themselves come from multiple sources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wikipedia&lt;/strong&gt;, representing encyclopedic text&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Subtitles&lt;/strong&gt;, from OPUS OpenSubtitles 2018 and SUBTLEX&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;News&lt;/strong&gt;, from NewsCrawl 2014 and GlobalVoices&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Books&lt;/strong&gt;, from Google Books Ngrams 2012&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Web&lt;/strong&gt; text, from OSCAR&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Twitter&lt;/strong&gt;, representing short-form social media&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reddit&lt;/strong&gt;, representing potentially longer Internet comments&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Miscellaneous&lt;/strong&gt; word frequencies: in Chinese, we import a free wordlist that comes with the Jieba word segmenter, whose provenance we don&#39;t really know&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The following languages are supported, with reasonable tokenization and at least 3 different sources of word frequencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Language    Code    #  Large?   WP    Subs  News  Books Web   Twit. Redd. Misc.&#xA;──────────────────────────────┼────────────────────────────────────────────────&#xA;Arabic      ar      5  Yes    │ Yes   Yes   Yes   -     Yes   Yes   -     -&#xA;Bangla      bn      5  Yes    │ Yes   Yes   Yes   -     Yes   Yes   -     -&#xA;Bosnian     bs [1]  3  -      │ Yes   Yes   -     -     -     Yes   -     -&#xA;Bulgarian   bg      4  -      │ Yes   Yes   -     -     Yes   Yes   -     -&#xA;Catalan     ca      5  Yes    │ Yes   Yes   Yes   -     Yes   Yes   -     -&#xA;Chinese     zh [3]  7  Yes    │ Yes   Yes   Yes   Yes   Yes   Yes   -     Jieba&#xA;Croatian    hr [1]  3         │ Yes   Yes   -     -     -     Yes   -     -&#xA;Czech       cs      5  Yes    │ Yes   Yes   Yes   -     Yes   Yes   -     -&#xA;Danish      da      4  -      │ Yes   Yes   -     -     Yes   Yes   -     -&#xA;Dutch       nl      5  Yes    │ Yes   Yes   Yes   -     Yes   Yes   -     -&#xA;English     en      7  Yes    │ Yes   Yes   Yes   Yes   Yes   Yes   Yes   -&#xA;Finnish     fi      6  Yes    │ Yes   Yes   Yes   -     Yes   Yes   Yes   -&#xA;French      fr      7  Yes    │ Yes   Yes   Yes   Yes   Yes   Yes   Yes   -&#xA;German      de      7  Yes    │ Yes   Yes   Yes   Yes   Yes   Yes   Yes   -&#xA;Greek       el      4  -      │ Yes   Yes   -     -     Yes   Yes   -     -&#xA;Hebrew      he      5  Yes    │ Yes   Yes   -     Yes   Yes   Yes   -     -&#xA;Hindi       hi      4  Yes    │ Yes   -     -     -     Yes   Yes   Yes   -&#xA;Hungarian   hu      4  -      │ Yes   Yes   -     -     Yes   Yes   -     -&#xA;Icelandic   is      3  -      │ Yes   Yes   -     -     Yes   -     -     -&#xA;Indonesian  id      3  -      │ Yes   Yes   -     -     -     Yes   -     -&#xA;Italian     it      7  Yes    │ Yes   Yes   Yes   Yes   Yes   Yes   Yes   -&#xA;Japanese    ja      5  Yes    │ Yes   Yes   -     -     Yes   Yes   Yes   -&#xA;Korean      ko      4  -      │ Yes   Yes   -     -     -     Yes   Yes   -&#xA;Latvian     lv      4  -      │ Yes   Yes   -     -     Yes   Yes   -     -&#xA;Lithuanian  lt      3  -      │ Yes   Yes   -     -     Yes   -     -     -&#xA;Macedonian  mk      5  Yes    │ Yes   Yes   Yes   -     Yes   Yes   -     -&#xA;Malay       ms      3  -      │ Yes   Yes   -     -     -     Yes   -     -&#xA;Norwegian   nb [2]  5  Yes    │ Yes   Yes   -     -     Yes   Yes   Yes   -&#xA;Persian     fa      4  -      │ Yes   Yes   -     -     Yes   Yes   -     -&#xA;Polish      pl      6  Yes    │ Yes   Yes   Yes   -     Yes   Yes   Yes   -&#xA;Portuguese  pt      5  Yes    │ Yes   Yes   Yes   -     Yes   Yes   -     -&#xA;Romanian    ro      3  -      │ Yes   Yes   -     -     Yes   -     -     -&#xA;Russian     ru      5  Yes    │ Yes   Yes   Yes   Yes   -     Yes   -     -&#xA;Slovak      sk      3  -      │ Yes   Yes   -     -     Yes   -     -     -&#xA;Slovenian   sl      3  -      │ Yes   Yes   -     -     Yes   -     -     -&#xA;Serbian     sr [1]  3  -      │ Yes   Yes   -     -     -     Yes   -     -&#xA;Spanish     es      7  Yes    │ Yes   Yes   Yes   Yes   Yes   Yes   Yes   -&#xA;Swedish     sv      5  Yes    │ Yes   Yes   -     -     Yes   Yes   Yes   -&#xA;Tagalog     fil     3  -      │ Yes   Yes   -     -     Yes   -     -     -&#xA;Tamil       ta      3  -      │ Yes   -     -     -     Yes   Yes   -     -&#xA;Turkish     tr      4  -      │ Yes   Yes   -     -     Yes   Yes   -     -&#xA;Ukrainian   uk      5  Yes    │ Yes   Yes   -     -     Yes   Yes   Yes   -&#xA;Urdu        ur      3  -      │ Yes   -     -     -     Yes   Yes   -     -&#xA;Vietnamese  vi      3  -      │ Yes   Yes   -     -     Yes   -     -     -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;[1] Bosnian, Croatian, and Serbian use the same underlying word list, because they share most of their vocabulary and grammar, they were once considered the same language, and language detection cannot distinguish them. This word list can also be accessed with the language code &lt;code&gt;sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[2] The Norwegian text we have is specifically written in Norwegian Bokmål, so we give it the language code &#39;nb&#39; instead of the vaguer code &#39;no&#39;. We would use &#39;nn&#39; for Nynorsk, but there isn&#39;t enough data to include it in wordfreq.&lt;/p&gt; &#xA;&lt;p&gt;[3] This data represents text written in both Simplified and Traditional Chinese, with primarily Mandarin Chinese vocabulary. See &#34;Multi-script languages&#34; below.&lt;/p&gt; &#xA;&lt;p&gt;Some languages provide &#39;large&#39; wordlists, including words with a Zipf frequency between 1.0 and 3.0. These are available in 14 languages that are covered by enough data sources.&lt;/p&gt; &#xA;&lt;h2&gt;Other functions&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;tokenize(text, lang)&lt;/code&gt; splits text in the given language into words, in the same way that the words in wordfreq&#39;s data were counted in the first place. See &lt;em&gt;Tokenization&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;top_n_list(lang, n, wordlist=&#39;best&#39;)&lt;/code&gt; returns the most common &lt;em&gt;n&lt;/em&gt; words in the list, in descending frequency order.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from wordfreq import top_n_list&#xA;&amp;gt;&amp;gt;&amp;gt; top_n_list(&#39;en&#39;, 10)&#xA;[&#39;the&#39;, &#39;to&#39;, &#39;and&#39;, &#39;of&#39;, &#39;a&#39;, &#39;in&#39;, &#39;i&#39;, &#39;is&#39;, &#39;for&#39;, &#39;that&#39;]&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; top_n_list(&#39;es&#39;, 10)&#xA;[&#39;de&#39;, &#39;la&#39;, &#39;que&#39;, &#39;el&#39;, &#39;en&#39;, &#39;y&#39;, &#39;a&#39;, &#39;los&#39;, &#39;no&#39;, &#39;un&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;iter_wordlist(lang, wordlist=&#39;best&#39;)&lt;/code&gt; iterates through all the words in a wordlist, in descending frequency order.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;get_frequency_dict(lang, wordlist=&#39;best&#39;)&lt;/code&gt; returns all the frequencies in a wordlist as a dictionary, for cases where you&#39;ll want to look up a lot of words and don&#39;t need the wrapper that &lt;code&gt;word_frequency&lt;/code&gt; provides.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;available_languages(wordlist=&#39;best&#39;)&lt;/code&gt; returns a dictionary whose keys are language codes, and whose values are the data file that will be loaded to provide the requested wordlist in each language.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;get_language_info(lang)&lt;/code&gt; returns a dictionary of information about how we preprocess text in this language, such as what script we expect it to be written in, which characters we normalize together, and how we tokenize it. See its docstring for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;random_words(lang=&#39;en&#39;, wordlist=&#39;best&#39;, nwords=5, bits_per_word=12)&lt;/code&gt; returns a selection of random words, separated by spaces. &lt;code&gt;bits_per_word=n&lt;/code&gt; will select each random word from 2^n words.&lt;/p&gt; &#xA;&lt;p&gt;If you happen to want an easy way to get &lt;a href=&#34;https://xkcd.com/936/&#34;&gt;a memorable, xkcd-style password&lt;/a&gt; with 60 bits of entropy, this function will almost do the job. In this case, you should actually run the similar function &lt;code&gt;random_ascii_words&lt;/code&gt;, limiting the selection to words that can be typed in ASCII. But maybe you should just use &lt;a href=&#34;https://github.com/beala/xkcd-password&#34;&gt;xkpa&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tokenization&lt;/h2&gt; &#xA;&lt;p&gt;wordfreq uses the Python package &lt;code&gt;regex&lt;/code&gt;, which is a more advanced implementation of regular expressions than the standard library, to separate text into tokens that can be counted consistently. &lt;code&gt;regex&lt;/code&gt; produces tokens that follow the recommendations in &lt;a href=&#34;http://unicode.org/reports/tr29/&#34;&gt;Unicode Annex #29, Text Segmentation&lt;/a&gt;, including the optional rule that splits words between apostrophes and vowels.&lt;/p&gt; &#xA;&lt;p&gt;There are exceptions where we change the tokenization to work better with certain languages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;In Arabic and Hebrew, it additionally normalizes ligatures and removes combining marks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In Japanese and Korean, instead of using the regex library, it uses the external library &lt;code&gt;mecab-python3&lt;/code&gt;. This is an optional dependency of wordfreq, and compiling it requires the &lt;code&gt;libmecab-dev&lt;/code&gt; system package to be installed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In Chinese, it uses the external Python library &lt;code&gt;jieba&lt;/code&gt;, another optional dependency.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;While the @ sign is usually considered a symbol and not part of a word, wordfreq will allow a word to end with &#34;@&#34; or &#34;@s&#34;. This is one way of writing gender-neutral words in Spanish and Portuguese.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When wordfreq&#39;s frequency lists are built in the first place, the words are tokenized according to this function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from wordfreq import tokenize&#xA;&amp;gt;&amp;gt;&amp;gt; tokenize(&#39;l@s niñ@s&#39;, &#39;es&#39;)&#xA;[&#39;l@s&#39;, &#39;niñ@s&#39;]&#xA;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;l@s&#39;, &#39;es&#39;)&#xA;3.03&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Because tokenization in the real world is far from consistent, wordfreq will also try to deal gracefully when you query it with texts that actually break into multiple tokens:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;New York&#39;, &#39;en&#39;)&#xA;5.32&#xA;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;北京地铁&#39;, &#39;zh&#39;)  # &#34;Beijing Subway&#34;&#xA;3.29&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The word frequencies are combined with the half-harmonic-mean function in order to provide an estimate of what their combined frequency would be. In Chinese, where the word breaks must be inferred from the frequency of the resulting words, there is also a penalty to the word frequency for each word break that must be inferred.&lt;/p&gt; &#xA;&lt;p&gt;This method of combining word frequencies implicitly assumes that you&#39;re asking about words that frequently appear together. It&#39;s not multiplying the frequencies, because that would assume they are statistically unrelated. So if you give it an uncommon combination of tokens, it will hugely over-estimate their frequency:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; zipf_frequency(&#39;owl-flavored&#39;, &#39;en&#39;)&#xA;3.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Multi-script languages&lt;/h2&gt; &#xA;&lt;p&gt;Two of the languages we support, Serbian and Chinese, are written in multiple scripts. To avoid spurious differences in word frequencies, we automatically transliterate the characters in these languages when looking up their words.&lt;/p&gt; &#xA;&lt;p&gt;Serbian text written in Cyrillic letters is automatically converted to Latin letters, using standard Serbian transliteration, when the requested language is &lt;code&gt;sr&lt;/code&gt; or &lt;code&gt;sh&lt;/code&gt;. If you request the word list as &lt;code&gt;hr&lt;/code&gt; (Croatian) or &lt;code&gt;bs&lt;/code&gt; (Bosnian), no transliteration will occur.&lt;/p&gt; &#xA;&lt;p&gt;Chinese text is converted internally to a representation we call &#34;Oversimplified Chinese&#34;, where all Traditional Chinese characters are replaced with their Simplified Chinese equivalent, &lt;em&gt;even if&lt;/em&gt; they would not be written that way in context. This representation lets us use a straightforward mapping that matches both Traditional and Simplified words, unifying their frequencies when appropriate, and does not appear to create clashes between unrelated words.&lt;/p&gt; &#xA;&lt;p&gt;Enumerating the Chinese wordlist will produce some unfamiliar words, because people don&#39;t actually write in Oversimplified Chinese, and because in practice Traditional and Simplified Chinese also have different word usage.&lt;/p&gt; &#xA;&lt;h2&gt;Similar, overlapping, and varying languages&lt;/h2&gt; &#xA;&lt;p&gt;As much as we would like to give each language its own distinct code and its own distinct word list with distinct source data, there aren&#39;t actually sharp boundaries between languages.&lt;/p&gt; &#xA;&lt;p&gt;Sometimes, it&#39;s convenient to pretend that the boundaries between languages coincide with national borders, following the maxim that &#34;a language is a dialect with an army and a navy&#34; (Max Weinreich). This gets complicated when the linguistic situation and the political situation diverge. Moreover, some of our data sources rely on language detection, which of course has no idea which country the writer of the text belongs to.&lt;/p&gt; &#xA;&lt;p&gt;So we&#39;ve had to make some arbitrary decisions about how to represent the fuzzier language boundaries, such as those within Chinese, Malay, and Croatian/Bosnian/Serbian.&lt;/p&gt; &#xA;&lt;p&gt;Smoothing over our arbitrary decisions is the fact that we use the &lt;code&gt;langcodes&lt;/code&gt; module to find the best match for a language code. If you ask for word frequencies in &lt;code&gt;cmn-Hans&lt;/code&gt; (the fully specific language code for Mandarin in Simplified Chinese), you will get the &lt;code&gt;zh&lt;/code&gt; wordlist, for example.&lt;/p&gt; &#xA;&lt;h2&gt;Additional CJK installation&lt;/h2&gt; &#xA;&lt;p&gt;Chinese, Japanese, and Korean have additional external dependencies so that they can be tokenized correctly. They can all be installed at once by requesting the &#39;cjk&#39; feature:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install wordfreq[cjk]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can put &lt;code&gt;wordfreq[cjk]&lt;/code&gt; in a list of dependencies, such as the &lt;code&gt;[tool.poetry.dependencies]&lt;/code&gt; list of your own project.&lt;/p&gt; &#xA;&lt;p&gt;Tokenizing Chinese depends on the &lt;code&gt;jieba&lt;/code&gt; package, tokenizing Japanese depends on &lt;code&gt;mecab-python3&lt;/code&gt; and &lt;code&gt;ipadic&lt;/code&gt;, and tokenizing Korean depends on &lt;code&gt;mecab-python3&lt;/code&gt; and &lt;code&gt;mecab-ko-dic&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As of version 2.4.2, you no longer have to install dictionaries separately.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;wordfreq&lt;/code&gt; is freely redistributable under the Apache license (see &lt;code&gt;LICENSE.txt&lt;/code&gt;), and it includes data files that may be redistributed under a Creative Commons Attribution-ShareAlike 4.0 license (&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;https://creativecommons.org/licenses/by-sa/4.0/&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;wordfreq&lt;/code&gt; contains data extracted from Google Books Ngrams (&lt;a href=&#34;http://books.google.com/ngrams&#34;&gt;http://books.google.com/ngrams&lt;/a&gt;) and Google Books Syntactic Ngrams (&lt;a href=&#34;http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html&#34;&gt;http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html&lt;/a&gt;). The terms of use of this data are:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Ngram Viewer graphs and data may be freely used for any purpose, although&#xA;acknowledgement of Google Books Ngram Viewer as the source, and inclusion&#xA;of a link to http://books.google.com/ngrams, would be appreciated.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;wordfreq&lt;/code&gt; also contains data derived from the following Creative Commons-licensed sources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The Leeds Internet Corpus, from the University of Leeds Centre for Translation Studies (&lt;a href=&#34;http://corpus.leeds.ac.uk/list.html&#34;&gt;http://corpus.leeds.ac.uk/list.html&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Wikipedia, the free encyclopedia (&lt;a href=&#34;http://www.wikipedia.org&#34;&gt;http://www.wikipedia.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ParaCrawl, a multilingual Web crawl (&lt;a href=&#34;https://paracrawl.eu&#34;&gt;https://paracrawl.eu&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It contains data from OPUS OpenSubtitles 2018 (&lt;a href=&#34;http://opus.nlpl.eu/OpenSubtitles.php&#34;&gt;http://opus.nlpl.eu/OpenSubtitles.php&lt;/a&gt;), whose data originates from the OpenSubtitles project (&lt;a href=&#34;http://www.opensubtitles.org/&#34;&gt;http://www.opensubtitles.org/&lt;/a&gt;) and may be used with attribution to OpenSubtitles.&lt;/p&gt; &#xA;&lt;p&gt;It contains data from various SUBTLEX word lists: SUBTLEX-US, SUBTLEX-UK, SUBTLEX-CH, SUBTLEX-DE, and SUBTLEX-NL, created by Marc Brysbaert et al. (see citations below) and available at &lt;a href=&#34;http://crr.ugent.be/programs-data/subtitle-frequencies&#34;&gt;http://crr.ugent.be/programs-data/subtitle-frequencies&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I (Robyn Speer) have obtained permission by e-mail from Marc Brysbaert to distribute these wordlists in wordfreq, to be used for any purpose, not just for academic use, under these conditions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wordfreq and code derived from it must credit the SUBTLEX authors.&lt;/li&gt; &#xA; &lt;li&gt;It must remain clear that SUBTLEX is freely available data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These terms are similar to the Creative Commons Attribution-ShareAlike license.&lt;/p&gt; &#xA;&lt;p&gt;Some additional data was collected by a custom application that watches the streaming Twitter API, in accordance with Twitter&#39;s Developer Agreement &amp;amp; Policy. This software gives statistics about words that are commonly used on Twitter; it does not display or republish any Twitter content.&lt;/p&gt; &#xA;&lt;h2&gt;Can I convert wordfreq to a more convenient form for my purposes, like a CSV file?&lt;/h2&gt; &#xA;&lt;p&gt;No. The CSV format does not have any space for attribution or license information, and therefore does not follow the CC-By-SA license. Even if you tried to include the proper attribution in a header or in another file, someone would likely just strip it out.&lt;/p&gt; &#xA;&lt;p&gt;wordfreq isn&#39;t particularly separable from its code, anyway. It depends on its normalization and word segmentation process, which is implemented in Python code, to give appropriate results.&lt;/p&gt; &#xA;&lt;p&gt;A reasonable way to transform wordfreq would be to port the library to another programming language, with all credits included and packaged in the usual way for that language.&lt;/p&gt; &#xA;&lt;h2&gt;Citing wordfreq&lt;/h2&gt; &#xA;&lt;p&gt;If you use wordfreq in your research, please cite it! We publish the code through Zenodo so that it can be reliably cited using a DOI. The current citation is:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Robyn Speer. (2022). rspeer/wordfreq: v3.0 (v3.0.2). Zenodo. &lt;a href=&#34;https://doi.org/10.5281/zenodo.7199437&#34;&gt;https://doi.org/10.5281/zenodo.7199437&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The same citation in BibTex format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{robyn_speer_2022_7199437,&#xA;  author       = {Robyn Speer},&#xA;  title        = {rspeer/wordfreq: v3.0},&#xA;  month        = sep,&#xA;  year         = 2022,&#xA;  publisher    = {Zenodo},&#xA;  version      = {v3.0.2},&#xA;  doi          = {10.5281/zenodo.7199437},&#xA;  url          = {https://doi.org/10.5281/zenodo.7199437}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citations to work that wordfreq is built on&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., &amp;amp; Turchi, M. (2015). Findings of the 2015 Workshop on Statistical Machine Translation. &lt;a href=&#34;http://www.statmt.org/wmt15/results.html&#34;&gt;http://www.statmt.org/wmt15/results.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Brysbaert, M. &amp;amp; New, B. (2009). Moving beyond Kucera and Francis: A Critical Evaluation of Current Word Frequency Norms and the Introduction of a New and Improved Word Frequency Measure for American English. Behavior Research Methods, 41 (4), 977-990. &lt;a href=&#34;http://sites.google.com/site/borisnew/pub/BrysbaertNew2009.pdf&#34;&gt;http://sites.google.com/site/borisnew/pub/BrysbaertNew2009.pdf&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Brysbaert, M., Buchmeier, M., Conrad, M., Jacobs, A.M., Bölte, J., &amp;amp; Böhl, A. (2011). The word frequency effect: A review of recent developments and implications for the choice of frequency estimates in German. Experimental Psychology, 58, 412-424.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Cai, Q., &amp;amp; Brysbaert, M. (2010). SUBTLEX-CH: Chinese word and character frequencies based on film subtitles. PLoS One, 5(6), e10729. &lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010729&#34;&gt;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010729&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Davis, M. (2012). Unicode text segmentation. Unicode Standard Annex, 29. &lt;a href=&#34;http://unicode.org/reports/tr29/&#34;&gt;http://unicode.org/reports/tr29/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Halácsy, P., Kornai, A., Németh, L., Rung, A., Szakadát, I., &amp;amp; Trón, V. (2004). Creating open language resources for Hungarian. In Proceedings of the 4th international conference on Language Resources and Evaluation (LREC2004). &lt;a href=&#34;http://mokk.bme.hu/resources/webcorpus/&#34;&gt;http://mokk.bme.hu/resources/webcorpus/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Keuleers, E., Brysbaert, M. &amp;amp; New, B. (2010). SUBTLEX-NL: A new frequency measure for Dutch words based on film subtitles. Behavior Research Methods, 42(3), 643-650. &lt;a href=&#34;http://crr.ugent.be/papers/SUBTLEX-NL_BRM.pdf&#34;&gt;http://crr.ugent.be/papers/SUBTLEX-NL_BRM.pdf&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Kudo, T. (2005). Mecab: Yet another part-of-speech and morphological analyzer. &lt;a href=&#34;http://mecab.sourceforge.net/&#34;&gt;http://mecab.sourceforge.net/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Lin, Y., Michel, J.-B., Aiden, E. L., Orwant, J., Brockman, W., and Petrov, S. (2012). Syntactic annotations for the Google Books Ngram Corpus. Proceedings of the ACL 2012 system demonstrations, 169-174. &lt;a href=&#34;http://aclweb.org/anthology/P12-3029&#34;&gt;http://aclweb.org/anthology/P12-3029&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Lison, P. and Tiedemann, J. (2016). OpenSubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles. In Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016). &lt;a href=&#34;http://stp.lingfil.uu.se/~joerg/paper/opensubs2016.pdf&#34;&gt;http://stp.lingfil.uu.se/~joerg/paper/opensubs2016.pdf&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ortiz Suárez, P. J., Sagot, B., and Romary, L. (2019). Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. In Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. &lt;a href=&#34;https://oscar-corpus.com/publication/2019/clmc7/asynchronous/&#34;&gt;https://oscar-corpus.com/publication/2019/clmc7/asynchronous/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ParaCrawl (2018). Provision of Web-Scale Parallel Corpora for Official European Languages. &lt;a href=&#34;https://paracrawl.eu/&#34;&gt;https://paracrawl.eu/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;van Heuven, W. J., Mandera, P., Keuleers, E., &amp;amp; Brysbaert, M. (2014). SUBTLEX-UK: A new and improved word frequency database for British English. The Quarterly Journal of Experimental Psychology, 67(6), 1176-1190. &lt;a href=&#34;http://www.tandfonline.com/doi/pdf/10.1080/17470218.2013.850521&#34;&gt;http://www.tandfonline.com/doi/pdf/10.1080/17470218.2013.850521&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>