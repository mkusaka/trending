<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-05T01:37:23Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Pennyw0rth/NetExec</title>
    <updated>2023-10-05T01:37:23Z</updated>
    <id>tag:github.com,2023-10-05:/Pennyw0rth/NetExec</id>
    <link href="https://github.com/Pennyw0rth/NetExec" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Network Execution Tool&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-blue.svg?sanitize=true&#34; alt=&#34;Supported Python versions&#34;&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=al3x_n3ff&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/al3xn3ff?label=al3x_n3ff&amp;amp;style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=_zblurx&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/_zblurx?label=_zblurx&amp;amp;style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=MJHallenbeck&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/MJHallenbeck?label=MJHallenbeck&amp;amp;style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸš© This is the open source repository of NetExec maintained by a community of passionate people&lt;/p&gt; &#xA;&lt;h1&gt;NetExec - The Network Execution Tool&lt;/h1&gt; &#xA;&lt;p&gt;This project was initially created in 2015 by @byt3bl33d3r, known as CrackMapExec. In 2019 @mpgn_x64 started maintaining the project for the next 4 years, adding a lot of great tools and features. In September 2023 he retired from maintaining the project.&lt;/p&gt; &#xA;&lt;p&gt;Along with many other contributers, we (NeffIsBack, Marshall-Hallenbeck, and zblurx) developed new features, bugfixes, and helped maintain the original project CrackMapExec. During this time, with both a private and public repository, community contributions were not easily merged into the project. The 6-8 month discrepancy between the code bases caused many development issues and heavily reduced community-driven development. With the end of mpgn&#39;s maintainer role, we (the remaining most active contributors) decided to maintain the project together as a fully free and open source project under the new name &lt;strong&gt;NetExec&lt;/strong&gt; ðŸš€ Going forward, our intent is to maintain a community-driven and maintained project with regular updates for everyone to use.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- placeholder for nxc logo--&gt; &lt;/p&gt; &#xA;&lt;p&gt;You are on the &lt;strong&gt;latest up-to-date&lt;/strong&gt; repository of the project NetExec (nxc) ! ðŸŽ‰&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ðŸš§ If you want to report a problem, open un &lt;a href=&#34;https://github.com/Pennyw0rth/NetExec/issues&#34;&gt;Issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ðŸ”€ If you want to contribute, open a &lt;a href=&#34;https://github.com/Pennyw0rth/NetExec/pulls&#34;&gt;Pull Request&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ðŸ’¬ If you want to discuss, open a &lt;a href=&#34;https://github.com/Pennyw0rth/NetExec/discussions&#34;&gt;Discussion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Official Discord Channel&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t have a Github account, you can ask your questions on Discord!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/pjwUTQzg8R&#34;&gt;&lt;img src=&#34;https://discordapp.com/api/guilds/1148685154601160794/widget.png?style=banner3&#34; alt=&#34;NetExec&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Documentation, Tutorials, Examples&lt;/h1&gt; &#xA;&lt;p&gt;See the project&#39;s &lt;a href=&#34;https://netexec.wiki/&#34;&gt;wiki&lt;/a&gt; (in development) for documentation and usage examples&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;Please see the installation instructions on the &lt;a href=&#34;https://netexec.wiki/getting-started/installation&#34;&gt;wiki&lt;/a&gt; (in development)&lt;/p&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;Development guidelines and recommendations in development&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgments&lt;/h1&gt; &#xA;&lt;p&gt;All the hard work and development over the years from everyone in the CrackMapExec project&lt;/p&gt; &#xA;&lt;h1&gt;Code Contributors&lt;/h1&gt; &#xA;&lt;p&gt;Awesome code contributors of NetExec:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mpgn&#34;&gt;&lt;img src=&#34;https://github.com/mpgn.png?size=50&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Marshall-Hallenbeck&#34;&gt;&lt;img src=&#34;https://github.com/Marshall-Hallenbeck.png?size=50&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zblurx&#34;&gt;&lt;img src=&#34;https://github.com/zblurx.png?size=50&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NeffIsBack&#34;&gt;&lt;img src=&#34;https://github.com/NeffIsBack.png?size=50&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Hackndo&#34;&gt;&lt;img src=&#34;https://github.com/Hackndo.png?size=50&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>danielgross/localpilot</title>
    <updated>2023-10-05T01:37:23Z</updated>
    <id>tag:github.com,2023-10-05:/danielgross/localpilot</id>
    <link href="https://github.com/danielgross/localpilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;localpilot&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Use GitHub Copilot locally on your Macbook with one-click!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/danielgross/localpilot/assets/279531/521d0613-7423-4839-a5e8-42098cd65a5e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/danielgross/localpilot/assets/279531/3259981b-39f7-4bfa-8a45-84bde6d4ba4c&#34;&gt;https://github.com/danielgross/localpilot/assets/279531/3259981b-39f7-4bfa-8a45-84bde6d4ba4c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This video is not sped up or slowed down.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, open VS Code Settings and add the following to your settings.json file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&#34;github.copilot.advanced&#34;: {&#xA;    &#34;debug.testOverrideProxyUrl&#34;: &#34;http://localhost:5001&#34;,&#xA;    &#34;debug.overrideProxyUrl&#34;: &#34;http://localhost:5001&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create a virtualenv to run this Python process, install the requirements, and download the models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;virtualenv venv&#xA;source venv/bin/activate&#xA;pip install -r requirements.txt&#xA;# First setup run. This will download several models to your ~/models folder.&#xA;python app.py --setup &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run it!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Enjoy your on-device Copilot!&lt;/p&gt; &#xA;&lt;h2&gt;Caveat FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is the code as good as GitHub Copilot?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For simple line completions yes. For simple function completions, mostly. For complex functions... maybe.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is it as fast as GitHub Copilot?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On my Macbook Pro with an Apple M2 Max, the 7b models are roughly as fast. The 34b models are not. Please consider this repo a demonstration of a very inefficient implementation. I&#39;m sure we can make it faster; please do submit a pull request if you&#39;d like to help. For example, I think we need debouncer because sometimes llama.cpp/GGML isn&#39;t fast at interrupting itself when a newer request comes in.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can this be packaged as a simple Mac app?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes!, I&#39;m sure it can be, I just haven&#39;t had the time. Please do submit a pull request if you&#39;re into that sort of thing!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Should there be a meta-model that routes to a 1b for autocomplete, 7b for more complex autocomplete, and a 34b for program completion?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hmm, that seems like an interesting idea.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OK, but in summary, is it good?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Only if your network is bad. I don&#39;t think it&#39;s competitive if you have fast Internet. But it sure is awesome on airplanes and while tethering!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>MIC-DKFZ/nnUNet</title>
    <updated>2023-10-05T01:37:23Z</updated>
    <id>tag:github.com,2023-10-05:/MIC-DKFZ/nnUNet</id>
    <link href="https://github.com/MIC-DKFZ/nnUNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the new nnU-Net!&lt;/h1&gt; &#xA;&lt;p&gt;Click &lt;a href=&#34;https://github.com/MIC-DKFZ/nnUNet/tree/nnunetv1&#34;&gt;here&lt;/a&gt; if you were looking for the old one instead.&lt;/p&gt; &#xA;&lt;p&gt;Coming from V1? Check out the &lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/tldr_migration_guide_from_v1.md&#34;&gt;TLDR Migration Guide&lt;/a&gt;. Reading the rest of the documentation is still strongly recommended ;-)&lt;/p&gt; &#xA;&lt;h1&gt;What is nnU-Net?&lt;/h1&gt; &#xA;&lt;p&gt;Image datasets are enormously diverse: image dimensionality (2D, 3D), modalities/input channels (RGB image, CT, MRI, microscopy, ...), image sizes, voxel sizes, class ratio, target structure properties and more change substantially between datasets. Traditionally, given a new problem, a tailored solution needs to be manually designed and optimized - a process that is prone to errors, not scalable and where success is overwhelmingly determined by the skill of the experimenter. Even for experts, this process is anything but simple: there are not only many design choices and data properties that need to be considered, but they are also tightly interconnected, rendering reliable manual pipeline optimization all but impossible!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/assets/nnU-Net_overview.png&#34; alt=&#34;nnU-Net overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;nnU-Net is a semantic segmentation method that automatically adapts to a given dataset. It will analyze the provided training cases and automatically configure a matching U-Net-based segmentation pipeline. No expertise required on your end! You can simply train the models and use them for your application&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Upon release, nnU-Net was evaluated on 23 datasets belonging to competitions from the biomedical domain. Despite competing with handcrafted solutions for each respective dataset, nnU-Net&#39;s fully automated pipeline scored several first places on open leaderboards! Since then nnU-Net has stood the test of time: it continues to be used as a baseline and method development framework (&lt;a href=&#34;https://arxiv.org/abs/2101.00232&#34;&gt;9 out of 10 challenge winners at MICCAI 2020&lt;/a&gt; and 5 out of 7 in MICCAI 2021 built their methods on top of nnU-Net, &lt;a href=&#34;https://amos22.grand-challenge.org/final-ranking/&#34;&gt;we won AMOS2022 with nnU-Net&lt;/a&gt;)!&lt;/p&gt; &#xA;&lt;p&gt;Please cite the &lt;a href=&#34;https://www.google.com/url?q=https://www.nature.com/articles/s41592-020-01008-z&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1677235958581755&amp;amp;usg=AOvVaw3dWL0SrITLhCJUBiNIHCQO&#34;&gt;following paper&lt;/a&gt; when using nnU-Net:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., &amp;amp; Maier-Hein, K. H. (2021). nnU-Net: a self-configuring &#xA;method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What can nnU-Net do for you?&lt;/h2&gt; &#xA;&lt;p&gt;If you are a &lt;strong&gt;domain scientist&lt;/strong&gt; (biologist, radiologist, ...) looking to analyze your own images, nnU-Net provides an out-of-the-box solution that is all but guaranteed to provide excellent results on your individual dataset. Simply convert your dataset into the nnU-Net format and enjoy the power of AI - no expertise required!&lt;/p&gt; &#xA;&lt;p&gt;If you are an &lt;strong&gt;AI researcher&lt;/strong&gt; developing segmentation methods, nnU-Net:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;offers a fantastic out-of-the-box applicable baseline algorithm to compete against&lt;/li&gt; &#xA; &lt;li&gt;can act as a method development framework to test your contribution on a large number of datasets without having to tune individual pipelines (for example evaluating a new loss function)&lt;/li&gt; &#xA; &lt;li&gt;provides a strong starting point for further dataset-specific optimizations. This is particularly used when competing in segmentation challenges&lt;/li&gt; &#xA; &lt;li&gt;provides a new perspective on the design of segmentation methods: maybe you can find better connections between dataset properties and best-fitting segmentation pipelines?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is the scope of nnU-Net?&lt;/h2&gt; &#xA;&lt;p&gt;nnU-Net is built for semantic segmentation. It can handle 2D and 3D images with arbitrary input modalities/channels. It can understand voxel spacings, anisotropies and is robust even when classes are highly imbalanced.&lt;/p&gt; &#xA;&lt;p&gt;nnU-Net relies on supervised learning, which means that you need to provide training cases for your application. The number of required training cases varies heavily depending on the complexity of the segmentation problem. No one-fits-all number can be provided here! nnU-Net does not require more training cases than other solutions - maybe even less due to our extensive use of data augmentation.&lt;/p&gt; &#xA;&lt;p&gt;nnU-Net expects to be able to process entire images at once during preprocessing and postprocessing, so it cannot handle enormous images. As a reference: we tested images from 40x40x40 pixels all the way up to 1500x1500x1500 in 3D and 40x40 up to ~30000x30000 in 2D! If your RAM allows it, larger is always possible.&lt;/p&gt; &#xA;&lt;h2&gt;How does nnU-Net work?&lt;/h2&gt; &#xA;&lt;p&gt;Given a new dataset, nnU-Net will systematically analyze the provided training cases and create a &#39;dataset fingerprint&#39;. nnU-Net then creates several U-Net configurations for each dataset:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2d&lt;/code&gt;: a 2D U-Net (for 2D and 3D datasets)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3d_fullres&lt;/code&gt;: a 3D U-Net that operates on a high image resolution (for 3D datasets only)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;3d_lowres&lt;/code&gt; â†’ &lt;code&gt;3d_cascade_fullres&lt;/code&gt;: a 3D U-Net cascade where first a 3D U-Net operates on low resolution images and then a second high-resolution 3D U-Net refined the predictions of the former (for 3D datasets with large image sizes only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that not all U-Net configurations are created for all datasets. In datasets with small image sizes, the U-Net cascade (and with it the 3d_lowres configuration) is omitted because the patch size of the full resolution U-Net already covers a large part of the input images.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;nnU-Net configures its segmentation pipelines based on a three-step recipe:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fixed parameters&lt;/strong&gt; are not adapted. During development of nnU-Net we identified a robust configuration (that is, certain architecture and training properties) that can simply be used all the time. This includes, for example, nnU-Net&#39;s loss function, (most of the) data augmentation strategy and learning rate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rule-based parameters&lt;/strong&gt; use the dataset fingerprint to adapt certain segmentation pipeline properties by following hard-coded heuristic rules. For example, the network topology (pooling behavior and depth of the network architecture) are adapted to the patch size; the patch size, network topology and batch size are optimized jointly given some GPU memory constraint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Empirical parameters&lt;/strong&gt; are essentially trial-and-error. For example the selection of the best U-net configuration for the given dataset (2D, 3D full resolution, 3D low resolution, 3D cascade) and the optimization of the postprocessing strategy.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to get started?&lt;/h2&gt; &#xA;&lt;p&gt;Read these:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/installation_instructions.md&#34;&gt;Installation instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/dataset_format.md&#34;&gt;Dataset conversion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/how_to_use_nnunet.md&#34;&gt;Usage instructions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additional information:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/region_based_training.md&#34;&gt;Region-based training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/manual_data_splits.md&#34;&gt;Manual data splits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/pretraining_and_finetuning.md&#34;&gt;Pretraining and finetuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/explanation_normalization.md&#34;&gt;Intensity Normalization in nnU-Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/explanation_plans_files.md&#34;&gt;Manually editing nnU-Net configurations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/extending_nnunet.md&#34;&gt;Extending nnU-Net&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/changelog.md&#34;&gt;What is different in V2?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Competitions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/competitions/AutoPETII.md&#34;&gt;AutoPET II&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Where does nnU-Net perform well and where does it not perform?&lt;/h2&gt; &#xA;&lt;p&gt;nnU-Net excels in segmentation problems that need to be solved by training from scratch, for example: research applications that feature non-standard image modalities and input channels, challenge datasets from the biomedical domain, majority of 3D segmentation problems, etc . We have yet to find a dataset for which nnU-Net&#39;s working principle fails!&lt;/p&gt; &#xA;&lt;p&gt;Note: On standard segmentation problems, such as 2D RGB images in ADE20k and Cityscapes, fine-tuning a foundation model (that was pretrained on a large corpus of similar images, e.g. Imagenet 22k, JFT-300M) will provide better performance than nnU-Net! That is simply because these models allow much better initialization. Foundation models are not supported by nnU-Net as they 1) are not useful for segmentation problems that deviate from the standard setting (see above mentioned datasets), 2) would typically only support 2D architectures and 3) conflict with our core design principle of carefully adapting the network topology for each dataset (if the topology is changed one can no longer transfer pretrained weights!)&lt;/p&gt; &#xA;&lt;h2&gt;What happened to the old nnU-Net?&lt;/h2&gt; &#xA;&lt;p&gt;The core of the old nnU-Net was hacked together in a short time period while participating in the Medical Segmentation Decathlon challenge in 2018. Consequently, code structure and quality were not the best. Many features were added later on and didn&#39;t quite fit into the nnU-Net design principles. Overall quite messy, really. And annoying to work with.&lt;/p&gt; &#xA;&lt;p&gt;nnU-Net V2 is a complete overhaul. The &#34;delete everything and start again&#34; kind. So everything is better (in the author&#39;s opinion haha). While the segmentation performance &lt;a href=&#34;https://docs.google.com/spreadsheets/d/13gqjIKEMPFPyMMMwA1EML57IyoBjfC3-QCTn4zRN_Mg/edit?usp=sharing&#34;&gt;remains the same&lt;/a&gt;, a lot of cool stuff has been added. It is now also much easier to use it as a development framework and to manually fine-tune its configuration to new datasets. A big driver for the reimplementation was also the emergence of &lt;a href=&#34;http://helmholtz-imaging.de&#34;&gt;Helmholtz Imaging&lt;/a&gt;, prompting us to extend nnU-Net to more image formats and domains. Take a look &lt;a href=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/changelog.md&#34;&gt;here&lt;/a&gt; for some highlights.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/assets/HI_Logo.png&#34; height=&#34;100px&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/MIC-DKFZ/nnUNet/master/documentation/assets/dkfz_logo.png&#34; height=&#34;100px&#34;&gt; &#xA;&lt;p&gt;nnU-Net is developed and maintained by the Applied Computer Vision Lab (ACVL) of &lt;a href=&#34;http://helmholtz-imaging.de&#34;&gt;Helmholtz Imaging&lt;/a&gt; and the &lt;a href=&#34;https://www.dkfz.de/en/mic/index.php&#34;&gt;Division of Medical Image Computing&lt;/a&gt; at the &lt;a href=&#34;https://www.dkfz.de/en/index.html&#34;&gt;German Cancer Research Center (DKFZ)&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>