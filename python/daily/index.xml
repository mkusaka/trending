<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-18T01:35:08Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>prs-eth/Marigold</title>
    <updated>2025-05-18T01:35:08Z</updated>
    <id>tag:github.com,2025-05-18:/prs-eth/Marigold</id>
    <link href="https://github.com/prs-eth/Marigold" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2024 - Oral, Best Paper Award Candidate] Marigold: Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Marigold Computer Vision&lt;/h1&gt; &#xA;&lt;p&gt;This project implements Marigold, a Computer Vision method for estimating image characteristics. Initially proposed for extracting high-resolution depth maps in our CVPR 2024 paper &lt;strong&gt;&#34;Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation&#34;&lt;/strong&gt;, we extended the method to other modalities as described in our follow-up paper &lt;strong&gt;&#34;Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis&#34;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://marigoldcomputervision.github.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-website.svg?sanitize=true&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2505.09358&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-pdf.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Depth-Demo-yellow&#34; alt=&#34;Depth Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold-normals&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Normals-Demo-yellow&#34; alt=&#34;Normals Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold-iid&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Image%20Intrinsics-Demo-yellow&#34; alt=&#34;Intrinsics Demo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-depth-v1-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Depth-Model-green&#34; alt=&#34;Depth Model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-normals-v1-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Normals-Model-green&#34; alt=&#34;Normals Model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Image%20Intrinsics%20Appearance-Model-green&#34; alt=&#34;Intrinsics Appearance Model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Image%20Intrinsics%20Lighting-Model-green&#34; alt=&#34;Intrinsics Lighting Model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/docs/diffusers/using-diffusers/marigold_usage&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-hfdiffusers.svg?sanitize=true&#34; alt=&#34;Diffusers Tutorial&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Team: &lt;a href=&#34;http://www.kebingxin.com/&#34;&gt;Bingxin Ke&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kevin-qu-b3417621b/&#34;&gt;Kevin Qu&lt;/a&gt;, &lt;a href=&#34;https://tianfwang.github.io/&#34;&gt;Tianfu Wang&lt;/a&gt; &lt;a href=&#34;https://nandometzger.github.io/&#34;&gt;Nando Metzger&lt;/a&gt;, &lt;a href=&#34;https://shengyuh.github.io/&#34;&gt;Shengyu Huang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/bobboli0202/&#34;&gt;Bo Li&lt;/a&gt;, &lt;a href=&#34;https://www.obukhov.ai/&#34;&gt;Anton Obukhov&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=FZuNgqIAAAAJ&#34;&gt;Konrad Schindler&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normal prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model&#39;s architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/teaser_marigold_all.jpg&#34; alt=&#34;teaser_all&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://marigoldmonodepth.github.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-website.svg?sanitize=true&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.02145&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-pdf.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-yellow&#34; alt=&#34;Hugging Face Space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-depth-v1-1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face%20-Model-green&#34; alt=&#34;Hugging Face Model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-colab.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In &lt;strong&gt;CVPR 2024 (Oral, Best Paper Award Candidate)&lt;/strong&gt;&lt;br&gt; Team: &lt;a href=&#34;http://www.kebingxin.com/&#34;&gt;Bingxin Ke&lt;/a&gt;, &lt;a href=&#34;https://www.obukhov.ai/&#34;&gt;Anton Obukhov&lt;/a&gt;, &lt;a href=&#34;https://shengyuh.github.io/&#34;&gt;Shengyu Huang&lt;/a&gt;, &lt;a href=&#34;https://nandometzger.github.io/&#34;&gt;Nando Metzger&lt;/a&gt;, &lt;a href=&#34;https://rcdaudt.github.io/&#34;&gt;Rodrigo Caye Daudt&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=FZuNgqIAAAAJ&#34;&gt;Konrad Schindler&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We present Marigold, a diffusion model, and an associated fine-tuning protocol for monocular depth estimation. Its core principle is to leverage the rich visual knowledge stored in modern generative image models. Our model, derived from Stable Diffusion and fine-tuned with synthetic data, can zero-shot transfer to unseen data, offering state-of-the-art monocular depth estimation results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/teaser_marigold_depth.png&#34; alt=&#34;teaser_depth&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📢 News&lt;/h2&gt; &#xA;&lt;p&gt;2025-05-15: Released code and a &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1&#34;&gt;checkpoint&lt;/a&gt; of Marigold Intrinsic Image Decomposition predicting Albedo, diffuse Shading, and non-diffuse Residual (Marigold-IID-Lighting v1.1).&lt;br&gt; 2025-05-15: Released code and a &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1&#34;&gt;checkpoint&lt;/a&gt; of Marigold Intrinsic Image Decomposition predicting Albedo, Roughness, and Metallicity (Marigold-IID-Appearance v1.1).&lt;br&gt; 2025-05-15: Released code and a &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-normals-v1-1&#34;&gt;checkpoint&lt;/a&gt; of Marigold Surface Normals Estimation (v1.1).&lt;br&gt; 2025-05-15: Released an updated &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-depth-v1-1&#34;&gt;checkpoint&lt;/a&gt; of Marigold Depth (v1.1), trained with updated noise scheduler settings (zero-SNR and trailing timestamps), and augmentations.&lt;br&gt; 2024-05-28: Training code is released.&lt;br&gt; 2024-05-27: Marigold pipelines are merged into the &lt;code&gt;diffusers&lt;/code&gt; core starting v0.28.0 &lt;a href=&#34;https://github.com/huggingface/diffusers/releases/tag/v0.28.0&#34;&gt;release&lt;/a&gt;!&lt;br&gt; 2024-03-23: Added a Latent Consistency Model (LCM) &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-depth-lcm-v1-0&#34;&gt;checkpoint&lt;/a&gt;.&lt;br&gt; 2024-03-04: The paper is accepted at CVPR 2024.&lt;br&gt; 2023-12-22: Contributed to Diffusers &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/community#marigold-depth-estimation&#34;&gt;community pipeline&lt;/a&gt;.&lt;br&gt; 2023-12-19: Updated &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/LICENSE.txt&#34;&gt;license&lt;/a&gt; to Apache License, Version 2.0.&lt;br&gt; 2023-12-08: Added the first interactive &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold&#34;&gt;Hugging Face Space Demo&lt;/a&gt; of depth estimation.&lt;br&gt; 2023-12-05: Added a &lt;a href=&#34;https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing&#34;&gt;Google Colab&lt;/a&gt;&lt;br&gt; 2023-12-04: Added an &lt;a href=&#34;https://arxiv.org/abs/2312.02145&#34;&gt;arXiv paper&lt;/a&gt; and inference code (this repository).&lt;/p&gt; &#xA;&lt;h2&gt;🚀 Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We offer several ways to interact with Marigold&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;A family of free online interactive demos: &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%2520Depth-Demo-yellow&#34; height=&#34;16&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold-normals&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%2520Normals-Demo-yellow&#34; height=&#34;16&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/prs-eth/marigold-iid&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%2520Image%2520Intrinsics-Demo-yellow&#34; height=&#34;16&#34;&gt;&lt;/a&gt; (kudos to the HF team for the GPU grants)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Marigold pipelines are part of &lt;a href=&#34;https://huggingface.co/docs/diffusers/using-diffusers/marigold_usage&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-hfdiffusers.svg?sanitize=true&#34; height=&#34;16&#34;&gt;&lt;/a&gt; - a one-stop shop for diffusion 🧨!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the demo locally (requires a GPU and an &lt;code&gt;nvidia-docker2&lt;/code&gt;, see &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html&#34;&gt;Installation Guide&lt;/a&gt;): &lt;code&gt;docker run -it -p 7860:7860 --platform=linux/amd64 --gpus all registry.hf.space/prs-eth-marigold:latest python app.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extended demo on a Google Colab: &lt;a href=&#34;https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-colab.svg?sanitize=true&#34; height=&#34;16&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you just want to see the examples, visit our gallery: &lt;a href=&#34;https://marigoldcomputervision.github.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/doc/badges/badge-website.svg?sanitize=true&#34; height=&#34;16&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Finally, local development instructions with this codebase are given below.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🛠️ Setup&lt;/h2&gt; &#xA;&lt;p&gt;The inference code was tested on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 22.04 LTS, Python 3.10.12, CUDA 11.7, GeForce RTX 3090 (pip)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🪧 A Note for Windows users&lt;/h3&gt; &#xA;&lt;p&gt;We recommend running the code in WSL2:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install WSL following &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install#install-wsl-command&#34;&gt;installation guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install CUDA support for WSL following &lt;a href=&#34;https://docs.nvidia.com/cuda/wsl-user-guide/index.html#cuda-support-for-wsl-2&#34;&gt;installation guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Find your drives in &lt;code&gt;/mnt/&amp;lt;drive letter&amp;gt;/&lt;/code&gt;; check &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/faq#how-do-i-access-my-c--drive-&#34;&gt;WSL FAQ&lt;/a&gt; for more details. Navigate to the working directory of choice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📦 Repository&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository (requires git):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/prs-eth/Marigold.git&#xA;cd Marigold&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;💻 Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv/marigold&#xA;source venv/marigold/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Keep the environment activated before running the inference script. Activate the environment again after restarting the terminal session.&lt;/p&gt; &#xA;&lt;h2&gt;🏃 Testing on your images&lt;/h2&gt; &#xA;&lt;h3&gt;📷 Prepare images&lt;/h3&gt; &#xA;&lt;p&gt;Use selected images from our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash script/download_sample_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or place your images in a directory, for example, under &lt;code&gt;input/in-the-wild_example&lt;/code&gt;, and run the following inference command.&lt;/p&gt; &#xA;&lt;h3&gt;🚀 Run inference (for practical usage)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth&#xA;python script/depth/run.py \&#xA;    --checkpoint prs-eth/marigold-depth-v1-1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Normals&#xA;python script/normals/run.py \&#xA;    --checkpoint prs-eth/marigold-normals-v1-1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# IID (appearance model)&#xA;python script/iid/run.py \&#xA;    --checkpoint prs-eth/marigold-iid-appearance-v1-1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example \&#xA;    --fp16&#xA;&#xA;# IID (lighting model)&#xA;python script/iid/run.py \&#xA;    --checkpoint prs-eth/marigold-iid-lighting-v1-1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example \&#xA;    --fp16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;⚙️ Inference settings&lt;/h3&gt; &#xA;&lt;p&gt;The default settings are optimized for the best results. However, the behavior of the code can be customized:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--half_precision&lt;/code&gt; or &lt;code&gt;--fp16&lt;/code&gt;: Run with half-precision (16-bit float) to have faster speed and reduced VRAM usage, but might lead to suboptimal results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--ensemble_size&lt;/code&gt;: Number of inference passes in the ensemble. Larger values tend to give better results in evaluations at the cost of slower inference; for most cases 1 is enough. Default: 1.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--denoise_steps&lt;/code&gt;: Number of denoising diffusion steps. Default settings are defined in the model checkpoints and are sufficient for most cases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;By default, the inference script resizes input images to the &lt;em&gt;processing resolution&lt;/em&gt;, and then resizes the prediction back to the original resolution. This gives the best quality, as Stable Diffusion, from which Marigold is derived, performs best at 768x768 resolution.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--processing_res&lt;/code&gt;: the processing resolution; set as 0 to process the input resolution directly. When unassigned (&lt;code&gt;None&lt;/code&gt;), will read default setting from model config. Default: &lt;code&gt;None&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--output_processing_res&lt;/code&gt;: produce output at the processing resolution instead of upsampling it to the input resolution. Default: False.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;--resample_method&lt;/code&gt;: the resampling method used to resize images and depth predictions. This can be one of &lt;code&gt;bilinear&lt;/code&gt;, &lt;code&gt;bicubic&lt;/code&gt;, or &lt;code&gt;nearest&lt;/code&gt;. Default: &lt;code&gt;bilinear&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--seed&lt;/code&gt;: Random seed can be set to ensure additional reproducibility. Default: None (unseeded). Note: forcing &lt;code&gt;--batch_size 1&lt;/code&gt; helps to increase reproducibility. To ensure full reproducibility, &lt;a href=&#34;https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms&#34;&gt;deterministic mode&lt;/a&gt; needs to be used.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--batch_size&lt;/code&gt;: Batch size of repeated inference. Default: 0 (best value determined automatically).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--color_map&lt;/code&gt;: &lt;a href=&#34;https://matplotlib.org/stable/users/explain/colors/colormaps.html&#34;&gt;Colormap&lt;/a&gt; used to colorize the depth prediction. Default: Spectral. Set to &lt;code&gt;None&lt;/code&gt; to skip colored depth map generation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--apple_silicon&lt;/code&gt;: Use Apple Silicon MPS acceleration.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🎮 Run inference (for academic comparisons)&lt;/h3&gt; &#xA;&lt;p&gt;These settings correspond to our paper. For academic comparison, please run with the settings below (if you only want to do fast inference on your own images, you can set &lt;code&gt;--ensemble_size 1&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth&#xA;python script/depth/run.py \&#xA;    --checkpoint prs-eth/marigold-depth-v1-1 \&#xA;    --denoise_steps 1 \&#xA;    --ensemble_size 10 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Normals&#xA;python script/normals/run.py \&#xA;    --checkpoint prs-eth/marigold-normals-v1-1 \&#xA;    --denoise_steps 4 \&#xA;    --ensemble_size 10 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# IID (appearance model)&#xA;python script/iid/run.py \&#xA;    --checkpoint prs-eth/marigold-iid-appearance-v1-1 \&#xA;    --denoise_steps 4 \&#xA;    --ensemble_size 1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&#xA;# IID (lighting model)&#xA;python script/iid/run.py \&#xA;    --checkpoint prs-eth/marigold-iid-lighting-v1-1 \&#xA;    --denoise_steps 4 \&#xA;    --ensemble_size 1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth (the original CVPR version)&#xA;python script/depth/run.py \&#xA;    --checkpoint prs-eth/marigold-depth-v1-0 \&#xA;    --denoise_steps 50 \&#xA;    --ensemble_size 10 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find all results in the &lt;code&gt;output&lt;/code&gt; directory. Enjoy!&lt;/p&gt; &#xA;&lt;h3&gt;⬇ Checkpoint cache&lt;/h3&gt; &#xA;&lt;p&gt;By default, the checkpoint (&lt;a href=&#34;https://huggingface.co/prs-eth/marigold-depth-v1-1&#34;&gt;depth&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-normals-v1-1&#34;&gt;normals&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1&#34;&gt;iid&lt;/a&gt;) is stored in the Hugging Face cache. The &lt;code&gt;HF_HOME&lt;/code&gt; environment variable defines its location and can be overridden, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export HF_HOME=$(pwd)/cache&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use the following script to download the checkpoint weights locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash script/download_weights.sh marigold-depth-v1-1           # depth checkpoint&#xA;bash script/download_weights.sh marigold-normals-v1-1         # normals checkpoint&#xA;bash script/download_weights.sh marigold-iid-appearance-v1-1  # iid appearance checkpoint&#xA;bash script/download_weights.sh marigold-iid-lighting-v1-1    # iid lighting checkpoint&#xA;# bash script/download_weights.sh marigold-depth-v1-0         # CVPR depth checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At inference, specify the checkpoint path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth&#xA;python script/depth/run.py \&#xA;    --checkpoint checkpoint/marigold-depth-v1-1 \&#xA;    --denoise_steps 4 \&#xA;    --ensemble_size 1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Normals&#xA;python script/normals/run.py \&#xA;    --checkpoint checkpoint/marigold-normals-v1-1 \&#xA;    --denoise_steps 4 \&#xA;    --ensemble_size 1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# IID (appearance model)&#xA;python script/iid/run.py \&#xA;    --checkpoint checkpoint/marigold-iid-appearance-v1-1 \&#xA;    --denoise_steps 4 \&#xA;    --ensemble_size 1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&#xA;# IID (lighting model)&#xA;python script/iid/run.py \&#xA;    --checkpoint checkpoint/marigold-iid-lighting-v1-1 \&#xA;    --denoise_steps 4 \&#xA;    --ensemble_size 1 \&#xA;    --input_rgb_dir input/in-the-wild_example \&#xA;    --output_dir output/in-the-wild_example&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🦿 Evaluation on test datasets &lt;a name=&#34;evaluation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Install additional dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements+.txt -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set data directory variable (also needed in evaluation scripts) and download the evaluation datasets (&lt;a href=&#34;https://share.phys.ethz.ch/~pf/bingkedata/marigold/evaluation_dataset&#34;&gt;depth&lt;/a&gt;, &lt;a href=&#34;https://share.phys.ethz.ch/~pf/bingkedata/marigold/marigold_normals/evaluation_dataset&#34;&gt;normals&lt;/a&gt;) into the corresponding subfolders:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export BASE_DATA_DIR=&amp;lt;YOUR_DATA_DIR&amp;gt;  # Set target data directory&#xA;&#xA;# Depth&#xA;wget -r -np -nH --cut-dirs=4 -R &#34;index.html*&#34; -P ${BASE_DATA_DIR} https://share.phys.ethz.ch/~pf/bingkedata/marigold/evaluation_dataset/&#xA;&#xA;# Normals&#xA;wget -r -np -nH --cut-dirs=4 -R &#34;index.html*&#34; -P ${BASE_DATA_DIR} https://share.phys.ethz.ch/~pf/bingkedata/marigold/marigold_normals/evaluation_dataset.zip&#xA;unzip ${BASE_DATA_DIR}/evaluation_dataset.zip -d ${BASE_DATA_DIR}/&#xA;rm -f ${BASE_DATA_DIR}/evaluation_dataset.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For download instructions of the intrinsic image decomposition test data, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/iid/dataset_preprocess/interiorverse_appearance/README.md&#34;&gt;iid-appearance instructions&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/iid/dataset_preprocess/hypersim_lighting/README.md&#34;&gt;iid-lighting instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run inference and evaluation scripts, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth&#xA;bash script/depth/eval/11_infer_nyu.sh  # Run inference&#xA;bash script/depth/eval/12_eval_nyu.sh   # Evaluate predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Normals&#xA;bash script/normals/eval/11_infer_scannet.sh  # Run inference&#xA;bash script/normals/eval/12_eval_scannet.sh   # Evaluate predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# IID&#xA;bash script/iid/eval/11_infer_appearance_interiorverse.sh  # Run inference&#xA;bash script/iid/eval/12_eval_appearance_interiorverse.sh   # Evaluate predictions&#xA;&#xA;bash script/iid/eval/21_infer_lighting_hypersim.sh  # Run inference&#xA;bash script/iid/eval/22_eval_lighting_hypersim.sh   # Evaluate predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth (the original CVPR version)&#xA;bash script/depth/eval_old/11_infer_nyu.sh  # Run inference&#xA;bash script/depth/eval_old/12_eval_nyu.sh   # Evaluate predictions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: although the seed has been set, the results might still be slightly different on different hardware.&lt;/p&gt; &#xA;&lt;h2&gt;🏋️ Training&lt;/h2&gt; &#xA;&lt;p&gt;Based on the previously created environment, install extended requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements++.txt -r requirements+.txt -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set environment parameters for the data directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export BASE_DATA_DIR=YOUR_DATA_DIR        # directory of training data&#xA;export BASE_CKPT_DIR=YOUR_CHECKPOINT_DIR  # directory of pretrained checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download Stable Diffusion v2 &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2&#34;&gt;checkpoint&lt;/a&gt; into &lt;code&gt;${BASE_CKPT_DIR}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prepare for training data&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Depth&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prepare for &lt;a href=&#34;https://github.com/apple/ml-hypersim&#34;&gt;Hypersim&lt;/a&gt; and &lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-2/&#34;&gt;Virtual KITTI 2&lt;/a&gt; datasets and save into &lt;code&gt;${BASE_DATA_DIR}&lt;/code&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/depth/dataset_preprocess/hypersim/README.md&#34;&gt;this README&lt;/a&gt; for Hypersim preprocessing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Normals&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prepare for &lt;a href=&#34;https://github.com/apple/ml-hypersim&#34;&gt;Hypersim&lt;/a&gt;, &lt;a href=&#34;https://interiorverse.github.io/&#34;&gt;Interiorverse&lt;/a&gt; and &lt;a href=&#34;http://sintel.is.tue.mpg.de/&#34;&gt;Sintel&lt;/a&gt; datasets and save into &lt;code&gt;${BASE_DATA_DIR}&lt;/code&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/normals/dataset_preprocess/hypersim/README.md&#34;&gt;this README&lt;/a&gt; for Hypersim preprocessing, &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/normals/dataset_preprocess/interiorverse/README.md&#34;&gt;this README&lt;/a&gt; for Interiorverse and &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/normals/dataset_preprocess/sintel/README.md&#34;&gt;this README&lt;/a&gt; for Sintel.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intrinsic Image Decomposition&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Appearance model&lt;/em&gt;: Prepare for &lt;a href=&#34;https://interiorverse.github.io/&#34;&gt;Interiorverse&lt;/a&gt; dataset and save into &lt;code&gt;${BASE_DATA_DIR}&lt;/code&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/iid/dataset_preprocess/interiorverse_appearance/README.md&#34;&gt;this README&lt;/a&gt; for Interiorverse preprocessing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Lighting model&lt;/em&gt;: Prepare for &lt;a href=&#34;https://github.com/apple/ml-hypersim&#34;&gt;Hypersim&lt;/a&gt; dataset and save into &lt;code&gt;${BASE_DATA_DIR}&lt;/code&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/script/iid/dataset_preprocess/hypersim_lighting/README.md&#34;&gt;this README&lt;/a&gt; for Hypersim preprocessing.&lt;/p&gt; &#xA;&lt;h3&gt;Run training script&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth&#xA;python script/depth/train.py --config config/train_marigold_depth.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Normals&#xA;python script/normals/train.py --config config/train_marigold_normals.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# IID (appearance model)&#xA;python script/iid/train.py --config config/train_marigold_iid_appearance.yaml&#xA;&#xA;# IID (lighting model)&#xA;python script/iid/train.py --config config/train_marigold_iid_lighting.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Resume from a checkpoint, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Depth&#xA;python script/depth/train.py --resume_run output/marigold_base/checkpoint/latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Normals&#xA;python script/normals/train.py --resume_run output/train_marigold_normals/checkpoint/latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# IID (appearance model)&#xA;python script/iid/train.py --resume_run output/train_marigold_iid_appearance/checkpoint/latest&#xA;&#xA;# IID (lighting model)&#xA;python script/iid/train.py --resume_run output/train_marigold_iid_lighting/checkpoint/latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Compose checkpoint:&lt;/h3&gt; &#xA;&lt;p&gt;Only the U-Net and scheduler config are updated during training. They are saved in the training directory. To use the inference pipeline with your training result:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;replace &lt;code&gt;unet&lt;/code&gt; folder in Marigold checkpoints with that in the &lt;code&gt;checkpoint&lt;/code&gt; output folder.&lt;/li&gt; &#xA; &lt;li&gt;replace the &lt;code&gt;scheduler/scheduler_config.json&lt;/code&gt; file in Marigold checkpoints with &lt;code&gt;checkpoint/scheduler_config.json&lt;/code&gt; generated during training. Then refer to &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/#evaluation&#34;&gt;this section&lt;/a&gt; for evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Although random seeds have been set, the training result might be slightly different on different hardwares. It&#39;s recommended to train without interruption.&lt;/p&gt; &#xA;&lt;h2&gt;✏️ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/CONTRIBUTING.md&#34;&gt;this&lt;/a&gt; instruction.&lt;/p&gt; &#xA;&lt;h2&gt;🤔 Troubleshooting&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Problem&lt;/th&gt; &#xA;   &lt;th&gt;Solution&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(Windows) Invalid DOS bash script on WSL&lt;/td&gt; &#xA;   &lt;td&gt;Run &lt;code&gt;dos2unix &amp;lt;script_name&amp;gt;&lt;/code&gt; to convert script format&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;(Windows) error on WSL: &lt;code&gt;Could not load library libcudnn_cnn_infer.so.8. Error: libcuda.so: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Run &lt;code&gt;export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Training takes a long time to start&lt;/td&gt; &#xA;   &lt;td&gt;Use folders for data instead of tar files (modification in config files is required).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🎓 Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our papers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@InProceedings{ke2023repurposing,&#xA;  title={Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation},&#xA;  author={Bingxin Ke and Anton Obukhov and Shengyu Huang and Nando Metzger and Rodrigo Caye Daudt and Konrad Schindler},&#xA;  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{ke2025marigold,&#xA;  title={Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis},&#xA;  author={Bingxin Ke and Kevin Qu and Tianfu Wang and Nando Metzger and Shengyu Huang and Bo Li and Anton Obukhov and Konrad Schindler},&#xA;  year={2025},&#xA;  eprint={2505.09358},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🎫 License&lt;/h2&gt; &#xA;&lt;p&gt;This code of this work is licensed under the Apache License, Version 2.0 (as defined in the &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;The models are licensed under RAIL++-M License (as defined in the &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/LICENSE-MODEL.txt&#34;&gt;LICENSE-MODEL&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;By downloading and using the code and model you agree to the terms in &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/LICENSE.txt&#34;&gt;LICENSE&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/prs-eth/Marigold/main/LICENSE-MODEL.txt&#34;&gt;LICENSE-MODEL&lt;/a&gt; respectively.&lt;/p&gt;</summary>
  </entry>
</feed>