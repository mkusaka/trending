<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-02-13T01:45:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/peft</title>
    <updated>2023-02-13T01:45:33Z</updated>
    <id>tag:github.com,2023-02-13:/huggingface/peft</id>
    <link href="https://github.com/huggingface/peft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ðŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;p&gt;ðŸ¤— PEFT&lt;/p&gt;&lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;p&gt;Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model&#39;s parameters. Fine-tuning large-scale PLMs is often prohibitively costly. In this regard, PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.&lt;/p&gt; &#xA;&lt;p&gt;Seamlessly integrated with ðŸ¤— Accelerate for large scale models leveraging DeepSpeed and Big Model Inference.&lt;/p&gt; &#xA;&lt;p&gt;Supported methods:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;LoRA: &lt;a href=&#34;https://arxiv.org/pdf/2106.09685.pdf&#34;&gt;LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prefix Tuning: &lt;a href=&#34;https://arxiv.org/pdf/2110.07602.pdf&#34;&gt;P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;P-Tuning: &lt;a href=&#34;https://arxiv.org/pdf/2103.10385.pdf&#34;&gt;GPT Understands, Too&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prompt Tuning: &lt;a href=&#34;https://arxiv.org/pdf/2104.08691.pdf&#34;&gt;The Power of Scale for Parameter-Efficient Prompt Tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForSeq2SeqLM&#xA;from peft import get_peft_config, get_peft_model, LoraConfig, TaskType&#xA;model_name_or_path = &#34;bigscience/mt0-large&#34;&#xA;tokenizer_name_or_path = &#34;bigscience/mt0-large&#34;&#xA;&#xA;peft_config = LoraConfig(&#xA;    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1&#xA;)&#xA;&#xA;model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)&#xA;model = get_peft_model(model, peft_config)&#xA;model.print_trainable_parameters()&#xA;# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Use Cases&lt;/h2&gt; &#xA;&lt;h3&gt;Get comparable performance to full finetuning by adapting LLMs to downstream tasks using consumer hardware&lt;/h3&gt; &#xA;&lt;p&gt;GPU memory required for adapting LLMs on the few-shot dataset &lt;code&gt;ought/raft/twitter_complaints&lt;/code&gt;. Here, settings considered are full finetuning, PEFT-LoRA using plain PyTorch and PEFT-LoRA using DeepSpeed with CPU Offloading.&lt;/p&gt; &#xA;&lt;p&gt;Hardware: Single A100 80GB GPU with CPU RAM above 64GB&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Full Finetuning&lt;/th&gt; &#xA;   &lt;th&gt;PEFT-LoRA PyTorch&lt;/th&gt; &#xA;   &lt;th&gt;PEFT-LoRA DeepSpeed with CPU Offloading&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bigscience/T0_3B (3B params)&lt;/td&gt; &#xA;   &lt;td&gt;47.14GB GPU / 2.96GB CPU&lt;/td&gt; &#xA;   &lt;td&gt;14.4GB GPU / 2.96GB CPU&lt;/td&gt; &#xA;   &lt;td&gt;9.8GB GPU / 17.8GB CPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bigscience/mt0-xxl (12B params)&lt;/td&gt; &#xA;   &lt;td&gt;OOM GPU&lt;/td&gt; &#xA;   &lt;td&gt;56GB GPU / 3GB CPU&lt;/td&gt; &#xA;   &lt;td&gt;22GB GPU / 52GB CPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bigscience/bloomz-7b1 (7B params)&lt;/td&gt; &#xA;   &lt;td&gt;OOM GPU&lt;/td&gt; &#xA;   &lt;td&gt;32GB GPU / 3.8GB CPU&lt;/td&gt; &#xA;   &lt;td&gt;18.1GB GPU / 35GB CPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Performance of PEFT-LoRA tuned &lt;code&gt;bigscience/T0_3B&lt;/code&gt; on &lt;code&gt;ought/raft/twitter_complaints&lt;/code&gt; leaderboard. A point to note is that we didn&#39;t try to sequeeze performance by playing around with input instruction templates, LoRA hyperparams and other training related hyperparams. Also, we didn&#39;t use the larger 13B mt0-xxl model. So, we are already seeing comparable performance to SoTA with parameter efficient tuning. Also, the final checkpoint size is just &lt;code&gt;19MB&lt;/code&gt; in comparison to &lt;code&gt;11GB&lt;/code&gt; size of the backbone &lt;code&gt;bigscience/T0_3B&lt;/code&gt; model.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Submission Name&lt;/th&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Human baseline (crowdsourced)&lt;/td&gt; &#xA;   &lt;td&gt;0.897&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flan-T5&lt;/td&gt; &#xA;   &lt;td&gt;0.892&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lora-t0-3b&lt;/td&gt; &#xA;   &lt;td&gt;0.863&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Therefore, we can see that performance comparable to SoTA is achievable by PEFT methods with consumer hardware such as 16GB and 24GB GPUs.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Parameter Efficient Tuning of Diffusion Models&lt;/h3&gt; &#xA;&lt;p&gt;GPU memory required by different settings during training is given below. The final checkpoint size is `8.8 MB``.&lt;/p&gt; &#xA;&lt;p&gt;Hardware: Single A100 80GB GPU with CPU RAM above 64G&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Full Finetuning&lt;/th&gt; &#xA;   &lt;th&gt;PEFT-LoRA&lt;/th&gt; &#xA;   &lt;th&gt;PEFT-LoRA with Gradient Checkpoitning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CompVis/stable-diffusion-v1-4&lt;/td&gt; &#xA;   &lt;td&gt;27.5GB GPU / 3.97GB CPU&lt;/td&gt; &#xA;   &lt;td&gt;15.5GB GPU / 3.84GB CPU&lt;/td&gt; &#xA;   &lt;td&gt;8.12GB GPU / 3.77GB CPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt; An example of using LoRA for parameter efficient dreambooth training is given in &lt;code&gt;~examples/lora_dreambooth/train_dreambooth.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MODEL_NAME= &#34;CompVis/stable-diffusion-v1-4&#34; #&#34;stabilityai/stable-diffusion-2-1&#34;&#xA;export INSTANCE_DIR=&#34;path-to-instance-images&#34;&#xA;export CLASS_DIR=&#34;path-to-class-images&#34;&#xA;export OUTPUT_DIR=&#34;path-to-save-model&#34;&#xA;&#xA;accelerate launch train_dreambooth.py \&#xA;  --pretrained_model_name_or_path=$MODEL_NAME  \&#xA;  --instance_data_dir=$INSTANCE_DIR \&#xA;  --class_data_dir=$CLASS_DIR \&#xA;  --output_dir=$OUTPUT_DIR \&#xA;  --train_text_encoder \&#xA;  --with_prior_preservation --prior_loss_weight=1.0 \&#xA;  --instance_prompt=&#34;a photo of sks dog&#34; \&#xA;  --class_prompt=&#34;a photo of dog&#34; \&#xA;  --resolution=512 \&#xA;  --train_batch_size=1 \&#xA;  --lr_scheduler=&#34;constant&#34; \&#xA;  --lr_warmup_steps=0 \&#xA;  --num_class_images=200 \&#xA;  --use_lora \&#xA;  --lora_r 16 \&#xA;  --lora_alpha 27 \&#xA;  --lora_text_encoder_r 16 \&#xA;  --lora_text_encoder_alpha 17 \&#xA;  --learning_rate=1e-4 \&#xA;  --gradient_accumulation_steps=1 \&#xA;  --gradient_checkpointing \&#xA;  --max_train_steps=800&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Try out the ðŸ¤— Gradio Space which should run seamlessly on a T4 instance: &lt;a href=&#34;https://huggingface.co/spaces/smangrul/peft-lora-sd-dreambooth&#34;&gt;smangrul/peft-lora-sd-dreambooth&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_lora_dreambooth_gradio_space.png&#34; alt=&#34;peft lora dreambooth gradio space&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Parameter Efficient Tuning of LLMs for RLHF components such as Ranker and Policy [ToDo]&lt;/h3&gt; &#xA;&lt;h3&gt;INT8 training of large models in Colab using PEFT LoRA and bits_and_bytes&lt;/h3&gt; &#xA;&lt;p&gt;Here is now a demo on how to fine tune OPT-6.7b (14GB in fp16) in a Google colab: &lt;a href=&#34;https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here is now a demo on how to fine tune wishper-large (1.5B params) (14GB in fp16) in a Google colab: [ToDo]&lt;/p&gt; &#xA;&lt;h3&gt;Save compute and storage even for medium and small models&lt;/h3&gt; &#xA;&lt;p&gt;Save storage by avoiding full finetuning of models on each of the downstream tasks/datasets, With PEFT methods, users only need to store tiny checkpoints in the order of &lt;code&gt;MBs&lt;/code&gt; all the while retaining performance comparable to full finetuning.&lt;/p&gt; &#xA;&lt;p&gt;An example of using LoRA for the task of adaping &lt;code&gt;LayoutLMForTokenClassification&lt;/code&gt; on &lt;code&gt;FUNSD&lt;/code&gt; dataset is given in &lt;code&gt;~examples/token_classification/PEFT_LoRA_LayoutLMForTokenClassification_on_FUNSD.py&lt;/code&gt;. We can observe that with only &lt;code&gt;0.62 %&lt;/code&gt; of parameters being trainable, we achieve performance (F1 0.777) comparable to full finetuning (F1 0.786) (without any hyerparam tuning runs for extracting more performance), and the checkpoint of this is only &lt;code&gt;2.8MB&lt;/code&gt;. Now, if there are &lt;code&gt;N&lt;/code&gt; such datasets, just have these PEFT models one for each dataset and save a lot of storage without having to worry about the problem of catastrophic forgetting or overfitting of backbone/base model.&lt;/p&gt; &#xA;&lt;p&gt;Another example is fine-tuning &lt;code&gt;roberta-large&lt;/code&gt; on &lt;code&gt;MRPC&lt;/code&gt; GLUE dataset suing differenct PEFT methods. The notebooks are given in &lt;code&gt;~examples/sequence_classification&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;PEFT + ðŸ¤— Accelerate&lt;/h2&gt; &#xA;&lt;p&gt;PEFT models work with ðŸ¤— Accelerate out of the box. Use ðŸ¤— Accelerate for Distributed training on various hardware such as GPUs, Apple Silicon devices etc during training. Use ðŸ¤— Accelerate for inferencing on consumer hardware with small resources.&lt;/p&gt; &#xA;&lt;h3&gt;Example of PEFT model training using ðŸ¤— Accelerate&#39;s DeepSpeed integration&lt;/h3&gt; &#xA;&lt;p&gt;Currently DeepSpeed requires PR &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/pull/2653&#34;&gt;ZeRO3 handling frozen weights&lt;/a&gt; to fix &lt;a href=&#34;https://github.com/microsoft/DeepSpeed/issues/2615&#34;&gt;[REQUEST] efficiently deal with frozen weights during training&lt;/a&gt; issue. An example is provided in &lt;code&gt;~examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py&lt;/code&gt;. a. First, run &lt;code&gt;accelerate&lt;/code&gt; config --config_file ds_zero3_cpu.yaml` and answer the questionnaire. Below are the contents of the config file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;compute_environment: LOCAL_MACHINE&#xA;deepspeed_config:&#xA;  gradient_accumulation_steps: 1&#xA;  gradient_clipping: 1.0&#xA;  offload_optimizer_device: cpu&#xA;  offload_param_device: cpu&#xA;  zero3_init_flag: true&#xA;  zero3_save_16bit_model: true&#xA;  zero_stage: 3&#xA;distributed_type: DEEPSPEED&#xA;downcast_bf16: &#39;no&#39;&#xA;dynamo_backend: &#39;NO&#39;&#xA;fsdp_config: {}&#xA;machine_rank: 0&#xA;main_training_function: main&#xA;megatron_lm_config: {}&#xA;mixed_precision: &#39;no&#39;&#xA;num_machines: 1&#xA;num_processes: 1&#xA;rdzv_backend: static&#xA;same_network: true&#xA;use_cpu: false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;b. run the below command to launch the example script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch --config_file ds_zero3_cpu.yaml examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;c. output logs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GPU Memory before entering the train : 1916&#xA;GPU Memory consumed at the end of the train (end-begin): 66&#xA;GPU Peak Memory consumed during the train (max-begin): 7488&#xA;GPU Total Peak Memory consumed during the train (max): 9404&#xA;CPU Memory before entering the train : 19411&#xA;CPU Memory consumed at the end of the train (end-begin): 0&#xA;CPU Peak Memory consumed during the train (max-begin): 0&#xA;CPU Total Peak Memory consumed during the train (max): 19411&#xA;epoch=4: train_ppl=tensor(1.0705, device=&#39;cuda:0&#39;) train_epoch_loss=tensor(0.0681, device=&#39;cuda:0&#39;)&#xA;100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27&amp;lt;00:00,  3.92s/it]&#xA;GPU Memory before entering the eval : 1982&#xA;GPU Memory consumed at the end of the eval (end-begin): -66&#xA;GPU Peak Memory consumed during the eval (max-begin): 672&#xA;GPU Total Peak Memory consumed during the eval (max): 2654&#xA;CPU Memory before entering the eval : 19411&#xA;CPU Memory consumed at the end of the eval (end-begin): 0&#xA;CPU Peak Memory consumed during the eval (max-begin): 0&#xA;CPU Total Peak Memory consumed during the eval (max): 19411&#xA;accuracy=100.0&#xA;eval_preds[:10]=[&#39;no complaint&#39;, &#39;no complaint&#39;, &#39;complaint&#39;, &#39;complaint&#39;, &#39;no complaint&#39;, &#39;no complaint&#39;, &#39;no complaint&#39;, &#39;complaint&#39;, &#39;complaint&#39;, &#39;no complaint&#39;]&#xA;dataset[&#39;train&#39;][label_column][:10]=[&#39;no complaint&#39;, &#39;no complaint&#39;, &#39;complaint&#39;, &#39;complaint&#39;, &#39;no complaint&#39;, &#39;no complaint&#39;, &#39;no complaint&#39;, &#39;complaint&#39;, &#39;complaint&#39;, &#39;no complaint&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example of PEFT model inference using ðŸ¤— Accelerate&#39;s Big Model Inferencing capabilities&lt;/h3&gt; &#xA;&lt;p&gt;An example is provided in &lt;code&gt;~examples/causal_language_modeling/peft_lora_clm_accelerate_big_model_inference.ipynb&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Models support matrix&lt;/h2&gt; &#xA;&lt;h3&gt;Causal Language Modeling&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;Prefix Tuning&lt;/th&gt; &#xA;   &lt;th&gt;P-Tuning&lt;/th&gt; &#xA;   &lt;th&gt;Prompt Tuning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bloom&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPT&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-Neo&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-J&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Conditional Generation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;Prefix Tuning&lt;/th&gt; &#xA;   &lt;th&gt;P-Tuning&lt;/th&gt; &#xA;   &lt;th&gt;Prompt Tuning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BART&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Sequence Classification&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;Prefix Tuning&lt;/th&gt; &#xA;   &lt;th&gt;P-Tuning&lt;/th&gt; &#xA;   &lt;th&gt;Prompt Tuning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bloom&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPT&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-Neo&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-J&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deberta&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deberta-v2&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Token Classification&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;Prefix Tuning&lt;/th&gt; &#xA;   &lt;th&gt;P-Tuning&lt;/th&gt; &#xA;   &lt;th&gt;Prompt Tuning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BERT&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RoBERTa&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-2&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bloom&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPT&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-Neo&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-J&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deberta&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deberta-v2&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Text-to-Image Generation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;Prefix Tuning&lt;/th&gt; &#xA;   &lt;th&gt;P-Tuning&lt;/th&gt; &#xA;   &lt;th&gt;Prompt Tuning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Stable Diffusion&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Image Classification&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;Prefix Tuning&lt;/th&gt; &#xA;   &lt;th&gt;P-Tuning&lt;/th&gt; &#xA;   &lt;th&gt;Prompt Tuning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Swin&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note that we have tested LoRA for &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/vit&#34;&gt;ViT&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/swin&#34;&gt;Swin&lt;/a&gt; for fine-tuning on image classification. However, it should be possible to use LoRA for any compatible model &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=image-classification&amp;amp;sort=downloads&amp;amp;search=vit&#34;&gt;provided&lt;/a&gt; by ðŸ¤— Transformers. Check out the respective examples to learn more. If you run into problems, please open an issue.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;The same principle applies to our &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=image-segmentation&amp;amp;sort=downloads&#34;&gt;segmentation models&lt;/a&gt; as well.&lt;/p&gt; &#xA;&lt;h3&gt;Semantic Segmentation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;LoRA&lt;/th&gt; &#xA;   &lt;th&gt;Prefix Tuning&lt;/th&gt; &#xA;   &lt;th&gt;P-Tuning&lt;/th&gt; &#xA;   &lt;th&gt;Prompt Tuning&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SegFormer&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Caveats:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Below is an example of using PyTorch FSDP for training. However, it doesn&#39;t lead to any GPU memory savings. Please refer issue &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/91165&#34;&gt;[FSDP] FSDP with CPU offload consumes 1.65X more GPU memory when training models with most of the params frozen&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from peft.utils.other import fsdp_auto_wrap_policy&#xA;&#xA;...&#xA;&#xA;if os.environ.get(&#34;ACCELERATE_USE_FSDP&#34;, None) is not None:&#xA;    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)&#xA;&#xA;model = accelerator.prepare(model)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example of parameter efficient tuning with &lt;code&gt;mt0-xxl&lt;/code&gt; base model using ðŸ¤— Accelerate is provided in &lt;code&gt;~examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py&lt;/code&gt;. a. First, run &lt;code&gt;accelerate config --config_file fsdp_config.yaml&lt;/code&gt; and answer the questionnaire. Below are the contents of the config file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;command_file: null&#xA;commands: null&#xA;compute_environment: LOCAL_MACHINE&#xA;deepspeed_config: {}&#xA;distributed_type: FSDP&#xA;downcast_bf16: &#39;no&#39;&#xA;dynamo_backend: &#39;NO&#39;&#xA;fsdp_config:&#xA;  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP&#xA;  fsdp_backward_prefetch_policy: BACKWARD_PRE&#xA;  fsdp_offload_params: true&#xA;  fsdp_sharding_strategy: 1&#xA;  fsdp_state_dict_type: FULL_STATE_DICT&#xA;  fsdp_transformer_layer_cls_to_wrap: T5Block&#xA;gpu_ids: null&#xA;machine_rank: 0&#xA;main_process_ip: null&#xA;main_process_port: null&#xA;main_training_function: main&#xA;megatron_lm_config: {}&#xA;mixed_precision: &#39;no&#39;&#xA;num_machines: 1&#xA;num_processes: 2&#xA;rdzv_backend: static&#xA;same_network: true&#xA;tpu_name: null&#xA;tpu_zone: null&#xA;use_cpu: false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;b. run the below command to launch the example script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch --config_file fsdp_config.yaml examples/peft_lora_seq2seq_accelerate_fsdp.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;When using &lt;code&gt;P_TUNING&lt;/code&gt; or &lt;code&gt;PROMPT_TUNING&lt;/code&gt; with &lt;code&gt;SEQ_2_SEQ&lt;/code&gt; task, remember to remove the &lt;code&gt;num_virtual_token&lt;/code&gt; virtual prompt predictions from the left side of the model outputs during evaluations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For encoder-decoder models, &lt;code&gt;P_TUNING&lt;/code&gt; or &lt;code&gt;PROMPT_TUNING&lt;/code&gt; doesn&#39;t support &lt;code&gt;generate&lt;/code&gt; functionality of transformers because &lt;code&gt;generate&lt;/code&gt; strictly requires &lt;code&gt;decoder_input_ids&lt;/code&gt; but &lt;code&gt;P_TUNING&lt;/code&gt;/&lt;code&gt;PROMPT_TUNING&lt;/code&gt; appends soft prompt embeddings to &lt;code&gt;input_embeds&lt;/code&gt; to create new &lt;code&gt;input_embeds&lt;/code&gt; to be given to the model. Therefore, &lt;code&gt;generate&lt;/code&gt; doesn&#39;t support this yet.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Backlog:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explore and possibly integrate &lt;code&gt;(IA)^3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add tests&lt;/li&gt; &#xA; &lt;li&gt;Add more use cases and examples&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citing ðŸ¤— PEFT&lt;/h2&gt; &#xA;&lt;p&gt;If you use ðŸ¤— PEFT in your publication, please cite it by using the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{peft,&#xA;  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},&#xA;  author =       {Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul},&#xA;  howpublished = {\url{https://github.com/huggingface/peft}},&#xA;  year =         {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>calapsss/face_detection_tutorial</title>
    <updated>2023-02-13T01:45:33Z</updated>
    <id>tag:github.com,2023-02-13:/calapsss/face_detection_tutorial</id>
    <link href="https://github.com/calapsss/face_detection_tutorial" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple facial recognition system using OpenCV and Raspberry Pi 4. Haar cascade classifier is used to detect faces in input image. Steps to set up system and required libraries are outlined in README. Ideal starting point for learning about facial recognition and open-source implementation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Simple Facial Recognition with OpenCV and Raspberry Pi 4&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains a simple facial recognition system using OpenCV and Raspberry Pi 4. The system uses the Haar cascade classifier for facial detection and the Raspberry Pi&#39;s Picamera for video capture.&lt;/p&gt; &#xA;&lt;p&gt;The full tutorial is in: &lt;a href=&#34;http://bit.ly/3ldDkj0&#34;&gt;http://bit.ly/3ldDkj0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Raspberry Pi 4 Picamera OpenCV&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Check that your Picamera is working properly by running the following command: &lt;code&gt;libcamera-hello&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Update and upgrade your Raspberry Pi by running the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the required libraries by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev&#xA;sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev&#xA;sudo apt-get install libxvidcore-dev libx264-dev&#xA;sudo apt-get install qt4-dev-tools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install OpenCV by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install opencv-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/calapsss/face_detection_tutorial.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the code by using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python face_detection_tutorial.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Conclusion&lt;/h2&gt; &#xA;&lt;p&gt;With this project, you can now build your own facial recognition system and get a hands-on experience in computer vision and machine learning. If you encounter any issues or have any questions, feel free to reach out to us through the GitHub repository. Our team will be happy to assist you in resolving any issues and answering any questions you may have.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pix2pixzero/pix2pix-zero</title>
    <updated>2023-02-13T01:45:33Z</updated>
    <id>tag:github.com,2023-02-13:/pix2pixzero/pix2pix-zero</id>
    <link href="https://github.com/pix2pixzero/pix2pix-zero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pix2pix-zero&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://pix2pixzero.github.io/&#34;&gt;&lt;strong&gt;[website]&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This is author&#39;s reimplementation of &#34;Zero-shot Image-to-Image Translation&#34; using the diffusers library. &lt;br&gt; The results in the paper are based on the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;CompVis&lt;/a&gt; library, which will be released later.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[New!]&lt;/strong&gt; Code for editing real and synthetic images released!&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div class=&#34;gif&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/main.gif&#34; align=&#34;center&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;We propose pix2pix-zero, a diffusion-based image-to-image approach that allows users to specify the edit direction on-the-fly (e.g., cat to dog). Our method can directly use pre-trained &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, for editing real and synthetic images while preserving the input image&#39;s structure. Our method is training-free and prompt-free, as it requires neither manual text prompting for each input image nor costly fine-tuning for each task.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: no finetuning required, no text input needed, input structure preserved.&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;All our results are based on &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;stable-diffusion-v1-4&lt;/a&gt; model. Please the website for more results.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/results_teaser.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;The top row for each of the results below show editing of real images, and the bottom row shows synthetic image editing.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/grid_dog2cat.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/grid_zebra2horse.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/grid_cat2dog.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/grid_horse2zebra.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/grid_tree2fall.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Real Image Editing&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/results_real.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Synthetic Image Editing&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/results_syn.jpg&#34; align=&#34;center&#34; width=&#34;800px&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Method Details&lt;/h2&gt; &#xA;&lt;p&gt;Given an input image, we first generate text captions using &lt;a href=&#34;https://github.com/salesforce/LAVIS&#34;&gt;BLIP&lt;/a&gt; and apply regularized DDIM inversion to obtain our inverted noise map. Then, we obtain reference cross-attention maps that correspoind to the structure of the input image by denoising, guided with the CLIP embeddings of our generated text (c). Next, we denoise with edited text embeddings, while enforcing a loss to match current cross-attention maps with the reference cross-attention maps.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/method.jpeg&#34; align=&#34;center&#34; width=&#34;900&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Environment Setup&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide a &lt;a href=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/environment.yml&#34;&gt;conda env file&lt;/a&gt; that contains all the required dependencies &lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Following this, you can activate the conda environment with the command below. &lt;pre&gt;&lt;code&gt;conda activate pix2pix-zero&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real Image Translation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First, run the inversion command below to obtain the input noise that reconstructs the image. The command below will save the inversion in the results folder as &lt;code&gt;output/test_cat/inversion/cat_1.pt&lt;/code&gt; and the BLIP-generated prompt as &lt;code&gt;output/test_cat/prompt/cat_1.txt&lt;/code&gt; &lt;pre&gt;&lt;code&gt;python src/inversion.py  \&#xA;        --input_image &#34;assets/test_images/cats/cat_1.png&#34; \&#xA;        --results_folder &#34;output/test_cat&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Next, we can perform image editing with the editing direction as shown below. The command below will save the edited image as &lt;code&gt;output/test_cat/edit/cat_1.png&lt;/code&gt; &lt;pre&gt;&lt;code&gt;python src/edit_real.py \&#xA;    --inversion &#34;output/test_cat/inversion/cat_1.pt&#34; \&#xA;    --prompt &#34;output/test_cat/prompt/cat_1.txt&#34; \&#xA;    --task_name &#34;cat2dog&#34; \&#xA;    --results_folder &#34;output/test_cat/&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing Synthetic Images&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Similarly, we can edit the synthetic images generated by Stable Diffusion with the following command. &lt;pre&gt;&lt;code&gt;python src/edit_synthetic.py \&#xA;    --results_folder &#34;output/synth_editing&#34; \&#xA;    --prompt_str &#34;a high resolution painting of a cat in the style of van gough&#34; \&#xA;    --task &#34;cat2dog&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Tips and Debugging&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Controlling the Image Structure:&lt;/strong&gt;&lt;br&gt; The &lt;code&gt;--xa_guidance&lt;/code&gt; flag controls the amount of cross-attention guidance to be applied when performing the edit. If the output edited image does not retain the structure from the input, increasing the value will typically address the issue. We recommend changing the value in increments of 0.05.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Improving Image Quality:&lt;/strong&gt;&lt;br&gt; If the output image quality is low or has some artifacts, using more steps for both the inversion and editing would be helpful. This can be controlled with the &lt;code&gt;--num_ddim_steps&lt;/code&gt; flag.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reducing the VRAM Requirements:&lt;/strong&gt;&lt;br&gt; We can reduce the VRAM requirements using lower precision and setting the flag &lt;code&gt;--use_float_16&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Finding Custom Edit Directions&lt;/strong&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We provide some pre-computed directions in the assets &lt;a href=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/embeddings_sd_1.4&#34;&gt;folder&lt;/a&gt;. To generate new edit directions, users can first generate two files containing a large number of sentences (~1000) and then run the command as shown below. &lt;pre&gt;&lt;code&gt;  python src/make_edit_direction.py \&#xA;    --file_source_sentences sentences/apple.txt \&#xA;    --file_target_sentences sentences/orange.txt \&#xA;    --output_folder assets/embeddings_sd_1.4&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;After running the above command, you can set the flag &lt;code&gt;--task apple2orange&lt;/code&gt; for the new edit.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comparison&lt;/h2&gt; &#xA;&lt;p&gt;Comparisons with different baselines, including, SDEdit + word swap, DDIM + word swap, and prompt-to-propmt. Our method successfully applies the edit, while preserving the structure of the input image.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pix2pixzero/pix2pix-zero/main/assets/comparison.jpg&#34; align=&#34;center&#34; width=&#34;900&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>