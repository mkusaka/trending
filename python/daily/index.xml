<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-30T01:32:20Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>521xueweihan/HelloGitHub</title>
    <updated>2022-06-30T01:32:20Z</updated>
    <id>tag:github.com,2022-06-30:/521xueweihan/HelloGitHub</id>
    <link href="https://github.com/521xueweihan/HelloGitHub" rel="alternate"></link>
    <summary type="html">&lt;p&gt;分享 GitHub 上有趣、入门级的开源项目。Share interesting, entry-level open source projects on GitHub.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/readme.gif&#34;&gt; &lt;br&gt;中文 | &lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/README_en.md&#34;&gt;English&lt;/a&gt; &lt;br&gt;分享 GitHub 上有趣、入门级的开源项目。&lt;br&gt;兴趣是最好的老师，这里能够帮你找到编程的兴趣！ &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/weixin.png&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square&#34; alt=&#34;WeiXin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/521xueweihan/HelloGitHub/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/521xueweihan/HelloGitHub/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://weibo.com/hellogithub&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square&#34; alt=&#34;Sina Weibo&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;简介&lt;/h2&gt; &#xA;&lt;p&gt;HelloGitHub 分享 GitHub 上有趣、入门级的开源项目。&lt;strong&gt;每月 28 号&lt;/strong&gt;以月刊的形式&lt;a href=&#34;https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA5MzYyNzQ0MQ==&amp;amp;action=getalbum&amp;amp;album_id=1331197538447310849#wechat_redirect&#34;&gt;更新发布&lt;/a&gt;，内容包括：&lt;strong&gt;有趣、入门级的开源项目&lt;/strong&gt;、&lt;strong&gt;开源书籍&lt;/strong&gt;、&lt;strong&gt;实战项目&lt;/strong&gt;、&lt;strong&gt;企业级项目&lt;/strong&gt;等，让你用很短时间感受到开源的魅力，爱上开源！&lt;/p&gt; &#xA;&lt;h2&gt;内容&lt;/h2&gt; &#xA;&lt;p&gt;获得更好的阅读体验 &lt;a href=&#34;https://hellogithub.com&#34;&gt;官网&lt;/a&gt; 或 &lt;a href=&#34;https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/weixin.png&#34;&gt;HelloGitHub 公众号&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;span&gt;📇&lt;/span&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;span&gt;🎃&lt;/span&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;span&gt;🍺&lt;/span&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;span&gt;🍥&lt;/span&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;img alt=&#34;octocat&#34; src=&#34;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&#34;&gt;)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub75.md&#34;&gt;第 75 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub74.md&#34;&gt;第 74 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub73.md&#34;&gt;第 73 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub72.md&#34;&gt;第 72 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub71.md&#34;&gt;第 71 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub70.md&#34;&gt;第 70 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub69.md&#34;&gt;第 69 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub68.md&#34;&gt;第 68 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub67.md&#34;&gt;第 67 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub66.md&#34;&gt;第 66 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub65.md&#34;&gt;第 65 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub64.md&#34;&gt;第 64 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub63.md&#34;&gt;第 63 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub62.md&#34;&gt;第 62 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub61.md&#34;&gt;第 61 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub60.md&#34;&gt;第 60 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub59.md&#34;&gt;第 59 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub58.md&#34;&gt;第 58 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub57.md&#34;&gt;第 57 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub56.md&#34;&gt;第 56 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub55.md&#34;&gt;第 55 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub54.md&#34;&gt;第 54 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub53.md&#34;&gt;第 53 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub52.md&#34;&gt;第 52 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub51.md&#34;&gt;第 51 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub50.md&#34;&gt;第 50 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub49.md&#34;&gt;第 49 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub48.md&#34;&gt;第 48 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub47.md&#34;&gt;第 47 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub46.md&#34;&gt;第 46 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub45.md&#34;&gt;第 45 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub44.md&#34;&gt;第 44 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub43.md&#34;&gt;第 43 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub42.md&#34;&gt;第 42 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub41.md&#34;&gt;第 41 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub40.md&#34;&gt;第 40 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub39.md&#34;&gt;第 39 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub38.md&#34;&gt;第 38 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub37.md&#34;&gt;第 37 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub36.md&#34;&gt;第 36 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub35.md&#34;&gt;第 35 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub34.md&#34;&gt;第 34 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub33.md&#34;&gt;第 33 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub32.md&#34;&gt;第 32 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub31.md&#34;&gt;第 31 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub05.md&#34;&gt;第 05 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub04.md&#34;&gt;第 04 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub03.md&#34;&gt;第 03 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub02.md&#34;&gt;第 02 期&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub01.md&#34;&gt;第 01 期&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;欢迎&lt;a href=&#34;https://github.com/521xueweihan/HelloGitHub/issues/new&#34;&gt;推荐或自荐&lt;/a&gt;项目成为 &lt;strong&gt;HelloGitHub&lt;/strong&gt; 的&lt;a href=&#34;https://github.com/521xueweihan/HelloGitHub/raw/master/content/contributors.md&#34;&gt;贡献者&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;赞助&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;width: 80px;&#34;&gt; &lt;a href=&#34;https://www.ucloud.cn/site/active/kuaijie.html?invitation_code=C1xF2ECA89A2592&#34;&gt; &lt;img src=&#34;https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/ucloud.png&#34; width=&#34;60px&#34;&gt;&lt;br&gt; &lt;sub&gt;云主机&lt;/sub&gt;&lt;br&gt; &lt;sub&gt;仅 4 元/月&lt;/sub&gt; &lt;/a&gt; &lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;width: 80px;&#34;&gt; &lt;a href=&#34;https://www.upyun.com/&#34;&gt; &lt;img src=&#34;https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/upyun.png&#34; width=&#34;60px&#34;&gt;&lt;br&gt; &lt;sub&gt;CDN&lt;/sub&gt;&lt;br&gt; &lt;sub&gt;开启全网加速&lt;/sub&gt; &lt;/a&gt; &lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34; style=&#34;width: 80px;&#34;&gt; &lt;a href=&#34;https://doc.rentsoft.cn/&#34;&gt; &lt;img src=&#34;https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/im.png&#34; width=&#34;60px&#34;&gt;&lt;br&gt; &lt;sub&gt;OpenIM&lt;/sub&gt;&lt;br&gt; &lt;sub&gt;开源IM力争No.1&lt;/sub&gt; &lt;/a&gt; &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;声明&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&#34;&gt;&lt;img alt=&#34;知识共享许可协议&#34; style=&#34;border-width: 0&#34; src=&#34;https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;本作品采用 &lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&#34;&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 进行许可。&lt;a href=&#34;mailto:595666367@qq.com&#34;&gt;联系我&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huawei-noah/Efficient-AI-Backbones</title>
    <updated>2022-06-30T01:32:20Z</updated>
    <id>tag:github.com,2022-06-30:/huawei-noah/Efficient-AI-Backbones</id>
    <link href="https://github.com/huawei-noah/Efficient-AI-Backbones" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Efficient AI Backbones including GhostNet, TNT and MLP, developed by Huawei Noah&#39;s Ark Lab.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Efficient AI Backbones&lt;/h1&gt; &#xA;&lt;p&gt;including GhostNet, TNT (Transformer in Transformer), AugViT, WaveMLP and ViG developed by Huawei Noah&#39;s Ark Lab.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#ghostnet-code&#34;&gt;GhostNet Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#tinynet-code&#34;&gt;TinyNet Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#tnt-code&#34;&gt;TNT Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#tnt-code&#34;&gt;PyramidTNT Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#legonet-code&#34;&gt;LegoNet Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#versatile-filters-code&#34;&gt;Versatile Filters Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#augvit-code&#34;&gt;AugViT Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#wavemlp-code&#34;&gt;WaveMLP Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#vig-code&#34;&gt;ViG Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huawei-noah/Efficient-AI-Backbones/master/#other-versions-of-ghostNet&#34;&gt;Other versions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;2022/06/17 The code of &lt;a href=&#34;https://arxiv.org/abs/2206.00272&#34;&gt;Vision GNN (ViG)&lt;/a&gt; is released at &lt;a href=&#34;https://github.com/huawei-noah/CV-Backbones/tree/master/vig_pytorch&#34;&gt;./vig_pytorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;2022/02/06 Transformer in Transformer is selected as the &lt;strong&gt;&lt;a href=&#34;https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/&#34;&gt;Most Influential NeurIPS 2021 Papers&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;2022/01/06 The extended version of &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/ghostnet_pytorch&#34;&gt;GhostNet&lt;/a&gt; is accepted by IJCV.&lt;/p&gt; &#xA;&lt;p&gt;2021/09/28 The paper of TNT (Transformer in Transformer) is accepted by NeurIPS 2021.&lt;/p&gt; &#xA;&lt;p&gt;2021/09/18 The extended version of &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/versatile_filters&#34;&gt;Versatile Filters&lt;/a&gt; is accepted by T-PAMI.&lt;/p&gt; &#xA;&lt;p&gt;2021/08/30 GhostNet paper is selected as the &lt;strong&gt;&lt;a href=&#34;https://www.paperdigest.org/2021/08/most-influential-cvpr-papers-2021-08/&#34;&gt;Most Influential CVPR 2020 Papers&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;2020/10/31 GhostNet+TinyNet achieves better performance. See details in our NeurIPS 2020 paper: &lt;a href=&#34;https://arxiv.org/abs/2010.14819&#34;&gt;arXiv&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;GhostNet Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides GhostNet &lt;strong&gt;pretrained models&lt;/strong&gt; and &lt;strong&gt;inference code&lt;/strong&gt; for TensorFlow and PyTorch:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensorflow: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/ghostnet_tensorflow&#34;&gt;./ghostnet_tensorflow&lt;/a&gt; with pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/ghostnet_pytorch&#34;&gt;./ghostnet_pytorch&lt;/a&gt; with pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;We also opensource code on &lt;a href=&#34;https://www.mindspore.cn/resources/hub&#34;&gt;MindSpore Hub&lt;/a&gt; and &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv&#34;&gt;MindSpore Model Zoo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For &lt;strong&gt;training&lt;/strong&gt;, please refer to &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv/tinynet&#34;&gt;tinynet&lt;/a&gt; or &lt;a href=&#34;https://rwightman.github.io/pytorch-image-models/training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5&#34;&gt;timm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;TinyNet Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides TinyNet &lt;strong&gt;pretrained models&lt;/strong&gt; and &lt;strong&gt;inference code&lt;/strong&gt; for PyTorch:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/tinynet_pytorch&#34;&gt;./tinynet_pytorch&lt;/a&gt; with pretrained model.&lt;/li&gt; &#xA; &lt;li&gt;We also opensource training code on &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv&#34;&gt;MindSpore Model Zoo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TNT Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides &lt;strong&gt;training code&lt;/strong&gt; and &lt;strong&gt;pretrained models&lt;/strong&gt; of TNT (Transformer in Transformer) for PyTorch:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/tnt_pytorch&#34;&gt;./tnt_pytorch&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We also opensource code on &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv/TNT&#34;&gt;MindSpore Model Zoo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The code of PyramidTNT is also released:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/tnt_pytorch&#34;&gt;./tnt_pytorch&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LegoNet Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides the implementation of paper &lt;a href=&#34;http://proceedings.mlr.press/v97/yang19c/yang19c.pdf&#34;&gt;LegoNet: Efficient Convolutional Neural Networks with Lego Filters (ICML 2019)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/legonet_pytorch&#34;&gt;./legonet_pytorch&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Versatile Filters Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides the implementation of paper &lt;a href=&#34;https://papers.nips.cc/paper/7433-learning-versatile-filters-for-efficient-convolutional-neural-networks&#34;&gt;Learning Versatile Filters for Efficient Convolutional Neural Networks (NeurIPS 2018)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/versatile_filters&#34;&gt;./versatile_filters&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AugViT Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides the implementation of paper &lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf&#34;&gt;Augmented Shortcuts for Vision Transformers (NeurIPS 2021)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-backbones/tree/master/augvit_pytorch&#34;&gt;./augvit_pytorch&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We also release the code on &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv/augvit&#34;&gt;MindSpore Model Zoo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;WaveMLP Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides the implementation of paper &lt;a href=&#34;https://arxiv.org/pdf/2111.12294.pdf&#34;&gt;An Image Patch is a Wave: Quantum Inspired Vision MLP (CVPR 2022)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch&#34;&gt;./wavemlp_pytorch&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We also release the code on &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp&#34;&gt;MindSpore Model Zoo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ViG Code&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides the implementation of paper &lt;a href=&#34;https://arxiv.org/abs/2206.00272&#34;&gt;Vision GNN: An Image is Worth Graph of Nodes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch: &lt;a href=&#34;https://github.com/huawei-noah/CV-Backbones/tree/master/vig_pytorch&#34;&gt;./vig_pytorch&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We also release the code on &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv/ViG&#34;&gt;MindSpore Model Zoo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{ghostnet,&#xA;  title={GhostNet: More Features from Cheap Operations},&#xA;  author={Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},&#xA;  booktitle={CVPR},&#xA;  year={2020}&#xA;}&#xA;@inproceedings{tinynet,&#xA;  title={Model Rubik’s Cube: Twisting Resolution, Depth and Width for TinyNets},&#xA;  author={Han, Kai and Wang, Yunhe and Zhang, Qiulin and Zhang, Wei and Xu, Chunjing and Zhang, Tong},&#xA;  booktitle={NeurIPS},&#xA;  year={2020}&#xA;}&#xA;@inproceedings{tnt,&#xA;  title={Transformer in transformer},&#xA;  author={Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},&#xA;  booktitle={NeurIPS},&#xA;  year={2021}&#xA;}&#xA;@inproceedings{legonet,&#xA;  title={LegoNet: Efficient Convolutional Neural Networks with Lego Filters},&#xA;  author={Yang, Zhaohui and Wang, Yunhe and Liu, Chuanjian and Chen, Hanting and Xu, Chunjing and Shi, Boxin and Xu, Chao and Xu, Chang},&#xA;  booktitle={ICML},&#xA;  year={2019}&#xA;}&#xA;@inproceedings{wang2018learning,&#xA;  title={Learning versatile filters for efficient convolutional neural networks},&#xA;  author={Wang, Yunhe and Xu, Chang and Chunjing, XU and Xu, Chao and Tao, Dacheng},&#xA;  booktitle={NeurIPS},&#xA;  year={2018}&#xA;}&#xA;@inproceedings{tang2021augmented,&#xA;  title={Augmented shortcuts for vision transformers},&#xA;  author={Tang, Yehui and Han, Kai and Xu, Chang and Xiao, An and Deng, Yiping and Xu, Chao and Wang, Yunhe},&#xA;  booktitle={NeurIPS},&#xA;  year={2021}&#xA;}&#xA;@inproceedings{tang2022image,&#xA;  title={An Image Patch is a Wave: Phase-Aware Vision MLP},&#xA;  author={Tang, Yehui and Han, Kai and Guo, Jianyuan and Xu, Chang and Li, Yanxi and Xu, Chao and Wang, Yunhe},&#xA;  booktitle={CVPR},&#xA;  year={2022}&#xA;}&#xA;@misc{vig,&#xA;  title={Vision GNN: An Image is Worth Graph of Nodes}, &#xA;  author={Kai Han and Yunhe Wang and Jianyuan Guo and Yehui Tang and Enhua Wu},&#xA;  year={2022},&#xA;  eprint={2206.00272},&#xA;  archivePrefix={arXiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other versions of GhostNet&lt;/h2&gt; &#xA;&lt;p&gt;This repo provides the TensorFlow/PyTorch code of GhostNet. Other versions and applications can be found in the following:&lt;/p&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;timm: &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models/raw/master/timm/models/ghostnet.py&#34;&gt;code with pretrained model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Darknet: &lt;a href=&#34;https://github.com/AlexeyAB/darknet/files/3997987/ghostnet.cfg.txt&#34;&gt;cfg file&lt;/a&gt;, and &lt;a href=&#34;https://github.com/AlexeyAB/darknet/issues/4418&#34;&gt;description&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gluon/Keras/Chainer: &lt;a href=&#34;https://github.com/osmr/imgclsmob&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Paddle: &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleClas/raw/master/ppcls/modeling/architectures/ghostnet.py&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Bolt inference framework: &lt;a href=&#34;https://github.com/huawei-noah/bolt/raw/master/docs/BENCHMARK.md&#34;&gt;benckmark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Human pose estimation: &lt;a href=&#34;https://github.com/tensorboy/centerpose/raw/master/lib/models/backbones/ghost_net.py&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;YOLO with GhostNet backbone: &lt;a href=&#34;https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Face recognition: &lt;a href=&#34;https://github.com/cavalleria/cavaface.pytorch/raw/master/backbone/ghostnet.py&#34;&gt;cavaface&lt;/a&gt;, &lt;a href=&#34;https://github.com/JDAI-CV/FaceX-Zoo&#34;&gt;FaceX-Zoo&lt;/a&gt;, &lt;a href=&#34;https://github.com/Tencent/TFace&#34;&gt;TFace&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>WwZzz/easyFL</title>
    <updated>2022-06-30T01:32:20Z</updated>
    <id>tag:github.com,2022-06-30:/WwZzz/easyFL</id>
    <link href="https://github.com/WwZzz/easyFL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An experimental platform to quickly realize and compare with popular centralized federated learning algorithms. A realization of federated learning algorithm on fairness (FedFV, Federated Learning with Fair Averaging, https://fanxlxmu.github.io/publication/ijcai2021/) was accepted by IJCAI-21 (https://www.ijcai.org/proceedings/2021/223).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;easyFL: A Lightning Framework for Federated Learning&lt;/h1&gt; &#xA;&lt;p&gt;This repository is PyTorch implementation for the IJCAI-21 paper &lt;a href=&#34;https://fanxlxmu.github.io/publication/ijcai2021/&#34;&gt;Federated Learning with Fair Averaging&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Our easyFL is a strong and reusable experimental platform for research on federated learning (FL) algorithm, which has provided a few easy-to-use modules to hold out for those who want to do various federated learning experiments. In short, it is easy for FL-researchers to&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;quickly realize and compare popular centralized federated learning algorithms (&lt;a href=&#34;https://github.com/WwZzz/easyFL/raw/main/algorithm/README.md#refer-anchor-1&#34;&gt;Look Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;transform traditional machine learning tasks into federated tasks by following our paradigm of constructing data pipeline (&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/benchmark/README.md&#34;&gt;Look Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;make interesting observations during training time to get a deeper insight of federated learning in a code-incremental manner without destorying the original realization (&lt;a href=&#34;https://github.com/WwZzz/easyFL/raw/main/algorithm/README.md#refer-anchor-2&#34;&gt;Look Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;efficiently manage and analyze the experiment records with &lt;code&gt;utils/result_analysis.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#Requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#QuickStart&#34;&gt;QuickStart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#Architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#Remark&#34;&gt;Remark&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#Citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#Contacts&#34;&gt;Contacts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#FedRME&#34;&gt;FedRME&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#References&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The project is implemented using Python3 with dependencies below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;numpy&amp;gt;=1.17.2&#xA;pytorch&amp;gt;=1.3.1&#xA;torchvision&amp;gt;=0.4.2&#xA;cvxopt&amp;gt;=1.2.0&#xA;scipy&amp;gt;=1.3.1&#xA;matplotlib&amp;gt;=3.1.1&#xA;prettytable&amp;gt;=2.1.0&#xA;ujson&amp;gt;=4.0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;QuickStart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, run the command below to get the splited dataset MNIST:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# generate the splited dataset&#xA;python generate_fedtask.py --benchmark mnist_classification --dist 0 --skew 0 --num_clients 100&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Second&lt;/strong&gt;, run the command below to quickly get a result of the basic algorithm FedAvg on MNIST with a simple CNN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python main.py --task mnist_classification_cnum100_dist0_skew0_seed0 --model cnn --algorithm fedavg --num_rounds 20 --num_epochs 5 --learning_rate 0.215 --proportion 0.1 --batch_size 10 --eval_interval 1&#xA;# if using gpu, add the id of the gpu device as &#39;--gpu id&#39; to the end of the command like this&#xA;# python main.py --task mnist_classification_cnum100_dist0_skew0_seed0 --model cnn --algorithm fedavg --num_rounds 20 --num_epochs 5 --learning_rate 0.215 --proportion 0.1 --batch_size 10 --eval_interval 1 --gpu 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The result will be stored in &lt;code&gt; ./fedtask/mnist_classification_cnum100_dist0_skew0_seed0/record/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Third&lt;/strong&gt;, run the command below to get a visualization of the result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# change to the ./utils folder&#xA;cd ../utils&#xA;# visualize the results&#xA;python result_analysis.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/example_mnist_trainloss.png&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/example_mnist_testloss.png&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/example_mnist_testacc.png&#34; width=&#34;230&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;5&#34;&gt;The rounds necessary for FedAVG to achieve 99% test accuracy on MNIST using CNN with E=5 (reported in &lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-1&#34;&gt;[McMahan. et al. 2017]&lt;/a&gt; / ours)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Proportion&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;iid&lt;/td&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;non-iid&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;B=FULL&lt;/td&gt; &#xA;   &lt;td&gt;B=10&lt;/td&gt; &#xA;   &lt;td&gt;B=FULL&lt;/td&gt; &#xA;   &lt;td&gt;B=10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.0&lt;/td&gt; &#xA;   &lt;td&gt;387 / 325&lt;/td&gt; &#xA;   &lt;td&gt;50 / 91&lt;/td&gt; &#xA;   &lt;td&gt;1181 / 1021&lt;/td&gt; &#xA;   &lt;td&gt;956 / 771&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.1&lt;/td&gt; &#xA;   &lt;td&gt;339 / 203&lt;/td&gt; &#xA;   &lt;td&gt;18 / 18 &lt;/td&gt; &#xA;   &lt;td&gt;1100 / 453&lt;/td&gt; &#xA;   &lt;td&gt;206 / 107&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;   &lt;td&gt;337 / 207&lt;/td&gt; &#xA;   &lt;td&gt;18 / 19 &lt;/td&gt; &#xA;   &lt;td&gt;978 / 525&lt;/td&gt; &#xA;   &lt;td&gt;200 / 95&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.5&lt;/td&gt; &#xA;   &lt;td&gt;164 / 214&lt;/td&gt; &#xA;   &lt;td&gt;18 / 18 &lt;/td&gt; &#xA;   &lt;td&gt;1067 / 606&lt;/td&gt; &#xA;   &lt;td&gt;261 / 105&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;246 / 267&lt;/td&gt; &#xA;   &lt;td&gt;16 / 18&lt;/td&gt; &#xA;   &lt;td&gt;-- / 737&lt;/td&gt; &#xA;   &lt;td&gt;97 / 90&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;7&#34;&gt; Accelarating FL Process by Increasing Parallelism For FedAVG on MNIST using CNN (20/100 clients per round)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Num_threads&lt;/td&gt; &#xA;   &lt;td&gt;1 &lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mean of time cost per round(s/r)&lt;/td&gt; &#xA;   &lt;td&gt;19.5434 &lt;/td&gt; &#xA;   &lt;td&gt;13.5733&lt;/td&gt; &#xA;   &lt;td&gt;9.9935&lt;/td&gt; &#xA;   &lt;td&gt;9.3092&lt;/td&gt; &#xA;   &lt;td&gt;9.2885&lt;/td&gt; &#xA;   &lt;td&gt;&lt;b&gt;8.3867&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Reproduced FL Algorithms&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Publication&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FedAvg&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-1&#34;&gt;[McMahan et al., 2017]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AISTATS&#39; 2017&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FedProx&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-2&#34;&gt;[Li et al., 2020]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MLSys&#39; 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FedFV&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-3&#34;&gt;[Wang et al., 2021]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IJCAI&#39; 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;qFFL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-4&#34;&gt;[Li et al., 2019]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ICLR&#39; 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AFL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-5&#34;&gt;[Mohri et al., 2019]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ICML&#39; 2019&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FedMGDA+&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-6&#34;&gt;[Hu et al., 2020]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pre-print&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FedFA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-7&#34;&gt;[Huang et al., 2020]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pre-print&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SCAFFOLD&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-11&#34;&gt;[Karimireddy et al., 2020]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ICML&#39; 2020&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FedDyn&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-12&#34;&gt;[Acar et al., 2021]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ICLR&#39; 2021&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;...&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;For those who want to realize their own federaed algorithms or reproduce others, please see &lt;code&gt;algorithms/readme.md&lt;/code&gt;, where we take two simple examples to show how to use easyFL for the popurse.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Partition Visualizing&lt;/h3&gt; &#xA;&lt;p&gt;We also provide the visualization of dataset partitioned by labels. Here we take the partition of CIFAR100/MNIST/CIFAR10 as the examples. Across all the examples, each row in the figure corresponds to the local data of one client, and different colors represent different labels. The x axis is the number of samples in the local dataset.&lt;/p&gt; &#xA;&lt;h4&gt;Di ~ D where dist=0&lt;/h4&gt; &#xA;&lt;p&gt;Each local dataset is I.I.D. drawn from the global distribution. Here we allocate the data of CIFAR100 to 100 clients. The iid can also be gengerated by setting (dist=2, skew=0) or (dist=1, skew=0). We list the results of the three IID partition manners below (i.e. dist=0,1,2 from left to right).&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar100_classification_cnum100_dist0_skew0_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar100_classification_cnum100_dist1_skew0.0_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar100_classification_cnum100_dist2_skew0.0_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;|{Di(Y)}|=K where dist=1&lt;/h4&gt; &#xA;&lt;p&gt;Each local dataset is allocated K labels of data. The visualization of the partition is on MNIST. There are 10 clients in each picture.&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/mnist_classification_cnum10_dist1_skew0.39_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/mnist_classification_cnum10_dist1_skew0.69_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/mnist_classification_cnum10_dist1_skew0.79_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Di ~ Dirichlet(αP) where dist=2&lt;/h4&gt; &#xA;&lt;p&gt;Here the partitioned dataset obeys the dirichlet(alpha * p) distirbution. The dataset is allocated to 100 clients and each client has a similar amount data size (i.e. balance). The hyperparameters &lt;code&gt;skewness&lt;/code&gt; controls the non-i.i.d. degree of the federated dataset, which increases from the left (skewness=0.0 =&amp;gt; alpha=inf) to the right (skewness=1.0 =&amp;gt; alpha=0).&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar10_classification_cnum100_dist2_skew0.0_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar10_classification_cnum100_dist2_skew0.2_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar10_classification_cnum100_dist2_skew0.4_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar10_classification_cnum100_dist2_skew0.6_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar10_classification_cnum100_dist2_skew0.8_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/cifar10_classification_cnum100_dist2_skew1.0_seed0.jpg&#34; width=&#34;230&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;To generate these fedtasks, run the command below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# I.I.D.&#xA;python generated_fedtask.py --dist 0 --skew 0 --num_client 100 --benchmark cifar100_classification&#xA;# skew=0.39,0.69,0.79&#xA;python generated_fedtask.py --dist 1 --skew 0.39 --num_client 10 --benchmark mnist_classification&#xA;# varying skew from 0.0 to 1.0&#xA;python generated_fedtask.py --dist 2 --skew 0.0 --num_client 100 --benchmark cifar10_classification&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Options&lt;/h3&gt; &#xA;&lt;p&gt;Basic options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;task&lt;/code&gt; is to choose the task of splited dataset. Options: name of fedtask (e.g. &lt;code&gt;mnist_client100_dist0_beta0_noise0&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;algorithm&lt;/code&gt; is to choose the FL algorithm. Options: &lt;code&gt;fedfv&lt;/code&gt;, &lt;code&gt;fedavg&lt;/code&gt;, &lt;code&gt;fedprox&lt;/code&gt;, …&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;model&lt;/code&gt; should be the corresponding model of the dataset. Options: &lt;code&gt;mlp&lt;/code&gt;, &lt;code&gt;cnn&lt;/code&gt;, &lt;code&gt;resnet18.&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Server-side options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sample&lt;/code&gt; decides the way to sample clients in each round. Options: &lt;code&gt;uniform&lt;/code&gt; means uniformly, &lt;code&gt;md&lt;/code&gt; means choosing with probability.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;aggregate&lt;/code&gt; decides the way to aggregate clients&#39; model. Options: &lt;code&gt;uniform&lt;/code&gt;, &lt;code&gt;weighted_scale&lt;/code&gt;, &lt;code&gt;weighted_com&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;num_rounds&lt;/code&gt; is the number of communication rounds.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;proportion&lt;/code&gt; is the proportion of clients to be selected in each round.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;lr_scheduler&lt;/code&gt; is the global learning rate scheduler.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;learning_rate_decay&lt;/code&gt; is the decay rate of the global learning rate.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Client-side options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;num_epochs&lt;/code&gt; is the number of local training epochs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;num_steps&lt;/code&gt; is the number of local updating steps and the default value is -1. If this term is set to larger than 0, then &lt;code&gt;num_epochs&lt;/code&gt; is invalid.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;learning_rate &lt;/code&gt; is the step size when locally training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;batch_size &lt;/code&gt; is the size of one batch data during local training. &lt;code&gt;batch_size = full_batch&lt;/code&gt; if &lt;code&gt;batch_size==-1&lt;/code&gt; and &lt;code&gt;batch_size=|Di|*batch_size&lt;/code&gt; if &lt;code&gt;1&amp;gt;batch_size&amp;gt;0&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;optimizer&lt;/code&gt; is to choose the optimizer. Options: &lt;code&gt;SGD&lt;/code&gt;, &lt;code&gt;Adam&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;momentum&lt;/code&gt; is the ratio of the momentum item when the optimizer SGD taking each step.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;seed &lt;/code&gt; is the initial random seed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;gpu &lt;/code&gt; is the id of the GPU device, &lt;code&gt;-1&lt;/code&gt; for CPU.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;eval_interval &lt;/code&gt; controls the interval between every two evaluations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;net_drop&lt;/code&gt; controls the dropout of clients after being selected in each communication round according to distribution Beta(net_drop,1). The larger this term is, the more possible for clients to drop.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;net_active&lt;/code&gt; controls the active rate of clients before being selected in each communication round according to distribution Beta(net_active,1). The larger this term is, the more possible for clients to be active.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;num_threads&lt;/code&gt; is the number of threads in the clients computing session that aims to accelarate the training process.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additional hyper-parameters for particular federated algorithms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mu&lt;/code&gt; is the parameter for FedProx.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;alpha&lt;/code&gt; is the parameter for FedFV.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tau&lt;/code&gt; is the parameter for FedFV.&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each additional parameter can be defined in &lt;code&gt;./utils/fflow.read_option&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;We seperate the FL system into four parts: &lt;code&gt;benchmark&lt;/code&gt;, &lt;code&gt;fedtask&lt;/code&gt;, &lt;code&gt;method&lt;/code&gt; and &lt;code&gt;utils&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;├─ benchmark&#xA;│  ├─ mnist_classification&#x9;&#x9;&#x9;//classification on mnist dataset&#xA;│  │  ├─ model                   //the corresponding model&#xA;│  |  └─ core.py                 //the core supporting for the dataset, and each contains three necessary classes(e.g. TaskGen, TaskReader, TaskCalculator)&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#xA;│  ├─ ...&#xA;│  ├─ RAW_DATA                   // storing the downloaded raw dataset&#xA;│  └─ toolkits.py&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//the basic tools for generating federated dataset&#xA;├─ fedtask&#xA;│  ├─ mnist_client100_dist0_beta0_noise0//IID(beta=0) MNIST for 100 clients with not predefined noise&#xA;│  │  ├─ record&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//the directionary of the running result&#xA;│  |  └─ data.json&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//the splitted federated dataset (fedtask)&#xA;|  └─ ...&#xA;├─ method&#xA;│  ├─ fedavg.py&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//FL algorithm implementation inherit fedbase.py&#xA;│  ├─ fedbase.py&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//FL algorithm superclass(i.e.,fedavg)&#xA;│  ├─ fedfv.py&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//our FL algorithm&#xA;│  ├─ fedprox.py&#xA;|  └─ ...&#xA;├─ utils&#xA;│  ├─ fflow.py&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//option to read, initialize,...&#xA;│  ├─ fmodule.py&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//model-level operators&#xA;│  ├─ network_simulator.py&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;//simulating the network heterogeneity&#xA;│  └─ result_analysis.py&#x9;&#x9;&#x9;&#x9;        //to generate the visualization of record&#xA;├─ generate_fedtask.py&#x9;&#x9;&#x9;&#x9;&#x9;        //generate fedtask&#xA;├─ requirements.txt&#xA;└─ main.py                       //run this file to start easyFL system&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/easyfl_benchmark.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; This module is to generate `fedtask` by partitioning the particular distribution data through `generate_fedtask.py`. To generate different `fedtask`, there are three parameters: `dist`, `num_clients `, `beta`. `dist` denotes the distribution type (e.g. `0` denotes iid and balanced distribution, `1` denotes niid-label-quantity and balanced distribution). `num_clients` is the number of clients participate in FL system, and `beta` controls the degree of non-iid for different `dist`. Each dataset can correspond to differrent models (mlp, cnn, resnet18, …). We refer to &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-1&#34;&gt;[McMahan et al., 2017]&lt;/a&gt;, &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-2&#34;&gt;[Li et al., 2020]&lt;/a&gt;, &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-8&#34;&gt;[Li et al., 2021]&lt;/a&gt;, &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-4&#34;&gt;[Li et al., 2019]&lt;/a&gt;, &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-9&#34;&gt;[Caldas et al., 2018]&lt;/a&gt;, &#xA;&lt;a href=&#34;https://raw.githubusercontent.com/WwZzz/easyFL/main/#refer-anchor-10&#34;&gt;[He et al., 2020]&lt;/a&gt; when realizing this module. Further details are described in `benchmark/README.md`. &#xA;&lt;h3&gt;Fedtask&lt;/h3&gt; &#xA;&lt;p&gt;We define each task as a combination of the federated dataset of a particular distribution and the experimental results on it. The raw dataset is processed into .json file, following LEAF (&lt;a href=&#34;https://github.com/TalwalkarLab/leaf&#34;&gt;https://github.com/TalwalkarLab/leaf&lt;/a&gt;). The architecture of the data.json file is described as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#34;&#34;&#34;&#xA;# store the raw data&#xA;{&#xA;    &#39;store&#39;: &#39;XY&#39;&#xA;    &#39;client_names&#39;: [&#39;user0&#39;, ..., &#39;user99&#39;]&#xA;    &#39;user0&#39;: {&#xA;       &#39;dtrain&#39;: {&#39;x&#39;: [...], &#39;y&#39;: [...]},&#xA;       &#39;dvalid&#39;: {&#39;x&#39;: [...], &#39;y&#39;: [...]},&#xA;     },...,&#xA;    &#39;user99&#39;: {&#xA;       &#39;dtrain&#39;: {&#39;x&#39;: [...], &#39;y&#39;: [...]},&#xA;       &#39;dvalid&#39;: {&#39;x&#39;: [...], &#39;y&#39;: [...]},&#xA;     },&#xA;    &#39;dtest&#39;: {&#39;x&#39;:[...], &#39;y&#39;:[...]}&#xA;}&#xA;# store the index of data in the original dataset&#xA;{&#xA;    &#39;store&#39;: &#39;IDX&#39;&#xA;    &#39;datasrc&#39;:{&#xA;        &#39;class_path&#39;: &#39;torchvision.datasets&#39;,&#xA;        &#39;class_name&#39;: dataset_class_name,&#xA;        &#39;train_args&#39;: {&#xA;             &#39;root&#39;: &#34;str(raw_data_path)&#34;,&#xA;             ...&#xA;        },&#xA;        &#39;test_args&#39;: {&#xA;             &#39;root&#39;: &#34;str(raw_data_path)&#34;,&#xA;             ...&#xA;         }&#xA;    }&#xA;    &#39;client_names&#39;: [&#39;user0&#39;, ..., &#39;user99&#39;]&#xA;    &#39;user0&#39;: {&#xA;       &#39;dtrain&#39;: [...],&#xA;       &#39;dvalid&#39;: [...],&#xA;     },...,&#xA;    &#39;dtest&#39;: [...]&#xA;}&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the file &lt;code&gt;./generate_fedtask.py&lt;/code&gt; to get the splited dataset (.json file).&lt;/p&gt; &#xA;&lt;p&gt;Since the task-specified models are usually orthogonal to the FL algorithms, we don&#39;t consider it an important part in this system. And the model and the basic loss function are defined in &lt;code&gt;./task/dataset_name/model_name.py&lt;/code&gt;. Further details are described in &lt;code&gt;fedtask/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Algorithm&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/WwZzz/myfigs/raw/master/fig0.png&#34; alt=&#34;image&#34;&gt; This module is the specific federated learning algorithm implementation. Each method contains two classes: the &lt;code&gt;Server&lt;/code&gt; and the &lt;code&gt;Client&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Server&lt;/h4&gt; &#xA;&lt;p&gt;The whole FL system starts with the &lt;code&gt;main.py&lt;/code&gt;, which runs &lt;code&gt;server.run()&lt;/code&gt; after initialization. Then the server repeat the method &lt;code&gt;iterate()&lt;/code&gt; for &lt;code&gt;num_rounds&lt;/code&gt; times, which simulates the communication process in FL. In the &lt;code&gt;iterate()&lt;/code&gt;, the &lt;code&gt;BaseServer&lt;/code&gt; start with sampling clients by &lt;code&gt;select()&lt;/code&gt;, and then exchanges model parameters with them by &lt;code&gt;communicate()&lt;/code&gt;, and finally aggregate the different models into a new one with &lt;code&gt;aggregate()&lt;/code&gt;. Therefore, anyone who wants to customize its own method that specifies some operations on the server-side should rewrite the method &lt;code&gt;iterate()&lt;/code&gt; and particular methods mentioned above.&lt;/p&gt; &#xA;&lt;h4&gt;Client&lt;/h4&gt; &#xA;&lt;p&gt;The clients reponse to the server after the server &lt;code&gt;communicate_with()&lt;/code&gt; them, who first &lt;code&gt;unpack()&lt;/code&gt; the received package and then train the model with their local dataset by &lt;code&gt;train()&lt;/code&gt;. After training the model, the clients &lt;code&gt;pack()&lt;/code&gt; send package (e.g. parameters, loss, gradient,... ) to the server through &lt;code&gt;reply()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Further details of this module are described in &lt;code&gt;algorithm/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Utils&lt;/h3&gt; &#xA;&lt;p&gt;Utils is composed of commonly used operations: model-level operation (we convert model layers and parameters to dictionary type and apply it in the whole FL system), the flow controlling of the framework in and the supporting visualization templates to the result. To visualize the results, please run &lt;code&gt;./utils/result_analysis.py&lt;/code&gt;. Further details are described in &lt;code&gt;utils/README.md&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Remark&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since we&#39;ve made great changes on the latest version, to fully reproduce the reported results in our paper &lt;a href=&#34;https://fanxlxmu.github.io/publication/ijcai2021/&#34;&gt;Federated Learning with Fair Averaging&lt;/a&gt;, please use another branch &lt;code&gt;easyFL v1.0&lt;/code&gt; of this project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;FedRME&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A realization of federated learning algorithm on Road Markings Extraction from Mobile LiDAR Point Clouds (FedRME, &lt;a href=&#34;https://fanxlxmu.github.io/publication/paper/CSCWD22-FedRME.pdf&#34;&gt;https://fanxlxmu.github.io/publication/paper/CSCWD22-FedRME.pdf&lt;/a&gt;) was accepted by 2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (IEEE CSCWD 2022). The source code for FedRME will be release as soon as possible.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper in your publications if this code helps your research.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wang2021federated,&#xA;  title={Federated Learning with Fair Averaging},&#xA;  author={Wang, Zheng and Fan, Xiaoliang and Qi, Jianzhong and Wen, Chenglu and Wang, Cheng and Yu, Rongshan},&#xA;  journal={arXiv preprint arXiv:2104.14937},&#xA;  year={2021}&#xA;}te&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contacts&lt;/h2&gt; &#xA;&lt;p&gt;Zheng Wang, &lt;a href=&#34;mailto:zwang@stu.xmu.edu.cn&#34;&gt;zwang@stu.xmu.edu.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Xiaoliang Fan, &lt;a href=&#34;mailto:fanxiaoliang@xmu.edu.cn&#34;&gt;fanxiaoliang@xmu.edu.cn&lt;/a&gt;, &lt;a href=&#34;https://fanxlxmu.github.io&#34;&gt;https://fanxlxmu.github.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;div id=&#34;refer-anchor-1&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[McMahan. et al., 2017] &lt;a href=&#34;https://arxiv.org/abs/1602.05629&#34;&gt;Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-2&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Li et al., 2020] &lt;a href=&#34;https://arxiv.org/abs/1812.06127&#34;&gt;Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv e-prints, page arXiv:1812.06127, 2020.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-3&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Wang et al., 2021] &lt;a href=&#34;https://arxiv.org/abs/2104.14937&#34;&gt;Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang and Rongshan Yu. Federated Learning with Fair Averaging. arXiv e-prints, page arXiv:2104.14937, 2021.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-4&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Li et al., 2019] &lt;a href=&#34;https://arxiv.org/abs/1905.10497&#34;&gt; Tian Li, Maziar Sanjabi, and Virginia Smith. Fair resource allocation in federated learning. CoRR, abs/1905.10497, 2019.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-5&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Mohri et al., 2019] &lt;a href=&#34;https://arxiv.org/abs/1902.00146&#34;&gt;Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. CoRR, abs/1902.00146, 2019.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-6&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Hu et al., 2020] &lt;a href=&#34;https://arxiv.org/abs/2006.11489&#34;&gt;Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu. Fedmgda+: Federated learning meets multi-objective optimization. arXiv e-prints, page arXiv:2006.11489, 2020.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-7&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Huang et al., 2020] &lt;a href=&#34;https://arxiv.org/abs/2012.10069&#34;&gt;Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, and Junbo Zhang. Fairness and accuracy in federated learning. arXiv e-prints, page arXiv:2012.10069, 2020.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-8&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Li et al., 2021]&lt;a href=&#34;https://arxiv.org/abs/2102.02079&#34;&gt;Li, Qinbin and Diao, Yiqun and Chen, Quan and He, Bingsheng. Federated Learning on Non-IID Data Silos: An Experimental Study. arXiv preprint arXiv:2102.02079, 2021.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-9&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Caldas et al., 2018] &lt;a href=&#34;https://arxiv.org/abs/1812.01097&#34;&gt;Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný, H. Brendan McMahan, Virginia Smith, Ameet Talwalkar. LEAF: A Benchmark for Federated Settings. arXiv preprint arXiv:1812.01097, 2018.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-10&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[He et al., 2020] &lt;a href=&#34;https://arxiv.org/abs/2007.13518&#34;&gt;He, Chaoyang and Li, Songze and So, Jinhyun and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and Shen, Li and Zhao, Peilin and Kang, Yan and Liu, Yang and Raskar, Ramesh and Yang, Qiang and Annavaram, Murali and Avestimehr, Salman. FedML: A Research Library and Benchmark for Federated Machine Learning. arXiv preprint arXiv:2007.13518, 2020.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-11&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Karimireddy et al., 2020] &lt;a href=&#34;https://arxiv.org/abs/1910.06378v3&#34;&gt;Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, Ananda Theertha Suresh, SCAFFOLD: Stochastic Controlled Averaging for Federated Learning, Proceedings of the 37th International Conference on Machine Learning, PMLR 119:5132-5143, 2020.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div id=&#34;refer-anchor-12&#34;&gt;&lt;/div&gt; &#xA;&lt;p&gt;[Acar et al., 2021] &lt;a href=&#34;https://openreview.net/forum?id=B7v4QMR6Z9w&#34;&gt;Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, Venkatesh Saligrama. Federated Learning Based on Dynamic Regularization. International Conference on Learning Representations (ICLR), 2021&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>