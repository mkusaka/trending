<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-18T01:42:20Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>baichuan-inc/baichuan-7B</title>
    <updated>2023-06-18T01:42:20Z</updated>
    <id>tag:github.com,2023-06-18:/baichuan-inc/baichuan-7B</id>
    <link href="https://github.com/baichuan-inc/baichuan-7B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A large-scale 7B pretraining language model developed by BaiChuan-Inc.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; baichuan-7B &lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34; style=&#34;display: flex; flex-direction: row; justify-content: center; align-items: center&#34;&gt; 🤗 &lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34; target=&#34;_blank&#34; style=&#34;margin-right: 15px; margin-left: 10px&#34;&gt;Hugging Face&lt;/a&gt; • 🤖 &lt;a href=&#34;https://modelscope.cn/organization/baichuan-inc&#34; target=&#34;_blank&#34; style=&#34;margin-left: 10px&#34;&gt;ModelScope&lt;/a&gt; • &lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B/raw/main/media/wechat.jpeg?raw=true&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; style=&#34;display: inline-block; margin-left: 10px&#34;&gt; &lt;span style=&#34;color: blue;&#34;&gt;Wechat&lt;/span&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/modelscope.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;b&gt;中文&lt;/b&gt; | &lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B/raw/main/README_EN.md&#34;&gt;English&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;介绍&lt;/h1&gt; &#xA;&lt;p&gt;baichuan-7B 是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，在大约1.2万亿 tokens 上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威 benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。&lt;/p&gt; &#xA;&lt;h2&gt;数据&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;原始数据包括开源的中英文数据和自行抓取的中文互联网数据，以及部分高质量知识性数据。&lt;/li&gt; &#xA; &lt;li&gt;参考相关数据工作，频率和质量是数据处理环节重点考虑的两个维度。 我们基于启发式规则和质量模型打分，对原始数据集进行篇章和句子粒度的过滤。在全量数据上，利用局部敏感哈希方法，对篇章和句子粒度做滤重。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;整体流程如下所示：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/baichuan-inc/baichuan-7B/main/media/data_process.png&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;经过不断的调整和多轮测试，最终确认了一个在下游任务上表现最好的中英文配比。&lt;/li&gt; &#xA; &lt;li&gt;我们使用了一个基于自动学习的数据权重策略，对不同类别的数据进行配比。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;分词&lt;/h2&gt; &#xA;&lt;p&gt;我们参考学术界方案使用 SentencePiece 中的 byte pair encoding (BPE)作为分词算法，并且进行了以下的优化：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;目前大部分开源模型主要基于英文优化，因此对中文语料存在效率较低的问题。我们使用2000万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。&lt;/li&gt; &#xA; &lt;li&gt;对于数学领域，我们参考了 LLaMA 和 Galactica 中的方案，对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助。&lt;/li&gt; &#xA; &lt;li&gt;对于罕见字词（如特殊符号等），支持 UTF-8-characters 的 byte 编码，因此做到未知字词的全覆盖。&lt;/li&gt; &#xA; &lt;li&gt;我们分析了不同分词器对语料的压缩率，如下表，可见我们的分词器明显优于 LLaMA, Falcon 等开源模型，并且对比其他中文分词器在压缩率相当的情况下，训练和推理效率更高。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;baichuan-7B&lt;/th&gt; &#xA;   &lt;th&gt;LLaMA&lt;/th&gt; &#xA;   &lt;th&gt;Falcon&lt;/th&gt; &#xA;   &lt;th&gt;mpt-7B&lt;/th&gt; &#xA;   &lt;th&gt;ChatGLM&lt;/th&gt; &#xA;   &lt;th&gt;moss-moon-003&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Compress Rate&lt;/td&gt; &#xA;   &lt;td&gt;0.737&lt;/td&gt; &#xA;   &lt;td&gt;1.312&lt;/td&gt; &#xA;   &lt;td&gt;1.049&lt;/td&gt; &#xA;   &lt;td&gt;1.206&lt;/td&gt; &#xA;   &lt;td&gt;0.631&lt;/td&gt; &#xA;   &lt;td&gt;0.659&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vocab Size&lt;/td&gt; &#xA;   &lt;td&gt;64000&lt;/td&gt; &#xA;   &lt;td&gt;32000&lt;/td&gt; &#xA;   &lt;td&gt;65024&lt;/td&gt; &#xA;   &lt;td&gt;50254&lt;/td&gt; &#xA;   &lt;td&gt;130344&lt;/td&gt; &#xA;   &lt;td&gt;106029&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;模型结构&lt;/h2&gt; &#xA;&lt;p&gt;整体模型基于标准的 Transformer 结构，我们采用了和 LLaMA 一样的模型设计&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;位置编码：&lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;rotary-embedding&lt;/a&gt; 是现阶段被大多模型采用的位置编码方案，具有更好的外延效果。虽然训练过程中最大长度为4096，但是实际测试中模型可以很好的扩展到 5000 tokens 上，如下图： &lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/baichuan-inc/baichuan-7B/main/media/long-context-ppl.png&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;激活层：SwiGLU, Feedforward 变化为(8/3)倍的隐含层大小，即11008&lt;/li&gt; &#xA; &lt;li&gt;Layer-Normalization: 基于 &lt;a href=&#34;https://arxiv.org/abs/1910.07467&#34;&gt;RMSNorm&lt;/a&gt; 的 Pre-Normalization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;训练稳定性和吞吐&lt;/h2&gt; &#xA;&lt;p&gt;我们在原本的LLaMA框架上进行诸多修改以提升训练时的吞吐，具体包括：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;算子优化技术：采用更高效算子，如 Flash-attention，NVIDIA apex 的 RMSNorm 等。&lt;/li&gt; &#xA; &lt;li&gt;算子切分技术：将部分计算算子进行切分，减小内存峰值。&lt;/li&gt; &#xA; &lt;li&gt;混合精度技术：降低在不损失模型精度的情况下加速计算过程。&lt;/li&gt; &#xA; &lt;li&gt;训练容灾技术：训练平台和训练框架联合优化，IaaS + PaaS 实现分钟级的故障定位和任务恢复。&lt;/li&gt; &#xA; &lt;li&gt;通信优化技术，具体包括： &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;采用拓扑感知的集合通信算法，避免网络拥塞问题，提高通信效率。&lt;/li&gt; &#xA;   &lt;li&gt;根据卡数自适应设置 bucket size，提高带宽利用率。&lt;/li&gt; &#xA;   &lt;li&gt;根据模型和集群环境，调优通信原语的触发时机，从而将计算和通信重叠。&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;基于上述的几个优化技术，我们在千卡A800机器上达到了7B模型182Tflops的吞吐，GPU峰值算力利用率高达58.3% 。&lt;/p&gt; &#xA;&lt;p&gt;最终的loss如下图：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/baichuan-inc/baichuan-7B/main/media/7b.loss.png&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h1&gt;公开benchmark榜单&lt;/h1&gt; &#xA;&lt;h2&gt;中文评测&lt;/h2&gt; &#xA;&lt;h3&gt;C-Eval&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cevalbenchmark.com/index.html&#34;&gt;C-Eval 数据集&lt;/a&gt;是一个全面的中文基础模型评测数据集，涵盖了52个学科和四个难度的级别。我们使用该数据集的dev集作为 few-shot 的来源，在 test 集上进行了 5-shot 测试。&lt;/p&gt; &#xA;&lt;p&gt;先修改 &lt;code&gt;evaluate_zh.py&lt;/code&gt; 中的 OPENMODEL_PATH 和 CEVAL_DATA_PATH 两个值，分别是模型（文件夹）存放的路径和 C-Eval 数据集的路径。再执行下面的脚本。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;shot=5  # few-shot&#xA;gpu=0  # 显卡id&#xA;split=test  # 评估测试集&#xA;model_id=baichuan-7b   # 待评估的模型&#xA;task=ceval  # 任务名称：ceval&#xA;echo gpu_idx-${gpu}-${model_id}_${task}_${split}_${shot}-shot&#xA;nohup python  evaluate_zh.py --gpu_idx ${gpu} --model_id ${model_id} --task ${task} --shot ${shot} --split ${split} --show_detail  &amp;gt; ${model_id}_${task}_${split}_${shot}-shot_record.txt 2&amp;gt;&amp;amp;1 &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;结果&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model 5-shot&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;   &lt;th&gt;Avg(Hard)&lt;/th&gt; &#xA;   &lt;th&gt;STEM&lt;/th&gt; &#xA;   &lt;th&gt;Social Sciences&lt;/th&gt; &#xA;   &lt;th&gt;Humanities&lt;/th&gt; &#xA;   &lt;th&gt;Others&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td&gt;68.7&lt;/td&gt; &#xA;   &lt;td&gt;54.9&lt;/td&gt; &#xA;   &lt;td&gt;67.1&lt;/td&gt; &#xA;   &lt;td&gt;77.6&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;67.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT&lt;/td&gt; &#xA;   &lt;td&gt;54.4&lt;/td&gt; &#xA;   &lt;td&gt;41.4&lt;/td&gt; &#xA;   &lt;td&gt;52.9&lt;/td&gt; &#xA;   &lt;td&gt;61.8&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;53.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Claude-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;54.2&lt;/td&gt; &#xA;   &lt;td&gt;39.0&lt;/td&gt; &#xA;   &lt;td&gt;51.9&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;52.1&lt;/td&gt; &#xA;   &lt;td&gt;53.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Claude-instant-v1.0&lt;/td&gt; &#xA;   &lt;td&gt;45.9&lt;/td&gt; &#xA;   &lt;td&gt;35.5&lt;/td&gt; &#xA;   &lt;td&gt;43.1&lt;/td&gt; &#xA;   &lt;td&gt;53.8&lt;/td&gt; &#xA;   &lt;td&gt;44.2&lt;/td&gt; &#xA;   &lt;td&gt;45.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon-003-base (16B)&lt;/td&gt; &#xA;   &lt;td&gt;27.4&lt;/td&gt; &#xA;   &lt;td&gt;24.5&lt;/td&gt; &#xA;   &lt;td&gt;27.0&lt;/td&gt; &#xA;   &lt;td&gt;29.1&lt;/td&gt; &#xA;   &lt;td&gt;27.2&lt;/td&gt; &#xA;   &lt;td&gt;26.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya-LLaMA-13B-pretrain&lt;/td&gt; &#xA;   &lt;td&gt;30.2&lt;/td&gt; &#xA;   &lt;td&gt;22.7&lt;/td&gt; &#xA;   &lt;td&gt;27.7&lt;/td&gt; &#xA;   &lt;td&gt;34.4&lt;/td&gt; &#xA;   &lt;td&gt;32.0&lt;/td&gt; &#xA;   &lt;td&gt;28.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B-hf&lt;/td&gt; &#xA;   &lt;td&gt;27.1&lt;/td&gt; &#xA;   &lt;td&gt;25.9&lt;/td&gt; &#xA;   &lt;td&gt;27.1&lt;/td&gt; &#xA;   &lt;td&gt;26.8&lt;/td&gt; &#xA;   &lt;td&gt;27.9&lt;/td&gt; &#xA;   &lt;td&gt;26.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM-6B&lt;/td&gt; &#xA;   &lt;td&gt;34.5&lt;/td&gt; &#xA;   &lt;td&gt;23.1&lt;/td&gt; &#xA;   &lt;td&gt;30.4&lt;/td&gt; &#xA;   &lt;td&gt;39.6&lt;/td&gt; &#xA;   &lt;td&gt;37.4&lt;/td&gt; &#xA;   &lt;td&gt;34.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon-7B&lt;/td&gt; &#xA;   &lt;td&gt;25.8&lt;/td&gt; &#xA;   &lt;td&gt;24.3&lt;/td&gt; &#xA;   &lt;td&gt;25.8&lt;/td&gt; &#xA;   &lt;td&gt;26.0&lt;/td&gt; &#xA;   &lt;td&gt;25.8&lt;/td&gt; &#xA;   &lt;td&gt;25.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-LLaMA-v2-pretrain (7B)&lt;/td&gt; &#xA;   &lt;td&gt;24.0&lt;/td&gt; &#xA;   &lt;td&gt;22.5&lt;/td&gt; &#xA;   &lt;td&gt;23.1&lt;/td&gt; &#xA;   &lt;td&gt;25.3&lt;/td&gt; &#xA;   &lt;td&gt;25.2&lt;/td&gt; &#xA;   &lt;td&gt;23.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TigerBot-7B-base&lt;/td&gt; &#xA;   &lt;td&gt;25.7&lt;/td&gt; &#xA;   &lt;td&gt;27.0&lt;/td&gt; &#xA;   &lt;td&gt;27.3&lt;/td&gt; &#xA;   &lt;td&gt;24.7&lt;/td&gt; &#xA;   &lt;td&gt;23.4&lt;/td&gt; &#xA;   &lt;td&gt;26.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aquila-7B&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25.5&lt;/td&gt; &#xA;   &lt;td&gt;25.2&lt;/td&gt; &#xA;   &lt;td&gt;25.6&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;   &lt;td&gt;25.2&lt;/td&gt; &#xA;   &lt;td&gt;26.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOM-7B&lt;/td&gt; &#xA;   &lt;td&gt;22.8&lt;/td&gt; &#xA;   &lt;td&gt;20.2&lt;/td&gt; &#xA;   &lt;td&gt;21.8&lt;/td&gt; &#xA;   &lt;td&gt;23.3&lt;/td&gt; &#xA;   &lt;td&gt;23.9&lt;/td&gt; &#xA;   &lt;td&gt;23.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOMZ-7B&lt;/td&gt; &#xA;   &lt;td&gt;35.7&lt;/td&gt; &#xA;   &lt;td&gt;25.8&lt;/td&gt; &#xA;   &lt;td&gt;31.3&lt;/td&gt; &#xA;   &lt;td&gt;43.5&lt;/td&gt; &#xA;   &lt;td&gt;36.6&lt;/td&gt; &#xA;   &lt;td&gt;35.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;baichuan-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;42.8&lt;/td&gt; &#xA;   &lt;td&gt;31.5&lt;/td&gt; &#xA;   &lt;td&gt;38.2&lt;/td&gt; &#xA;   &lt;td&gt;52.0&lt;/td&gt; &#xA;   &lt;td&gt;46.2&lt;/td&gt; &#xA;   &lt;td&gt;39.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Gaokao&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ExpressAI/AI-Gaokao&#34;&gt;Gaokao&lt;/a&gt; 是一个以中国高考题作为评测大语言模型能力的数据集，用以评估模型的语言能力和逻辑推理能力。 我们只保留了其中的单项选择题，随机划分后对所有模型进行统一 5-shot 测试。&lt;/p&gt; &#xA;&lt;h3&gt;结果&lt;/h3&gt; &#xA;&lt;p&gt;以下是测试的结果。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-LLaMA-v2-pretrain&lt;/td&gt; &#xA;   &lt;td&gt;21.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya-LLaMA-13B-pretrain&lt;/td&gt; &#xA;   &lt;td&gt;23.17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon-7B&lt;/td&gt; &#xA;   &lt;td&gt;23.98&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TigerBot-7B-base&lt;/td&gt; &#xA;   &lt;td&gt;25.94&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td&gt;27.81&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM-6B&lt;/td&gt; &#xA;   &lt;td&gt;21.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOM-7B&lt;/td&gt; &#xA;   &lt;td&gt;26.96&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOMZ-7B&lt;/td&gt; &#xA;   &lt;td&gt;28.72&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aquila-7B&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;24.39&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;baichuan-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;36.24&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;AGIEval&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/AGIEval&#34;&gt;AGIEval&lt;/a&gt; 旨在评估模型的认知和解决问题相关的任务中的一般能力。 我们只保留了其中的四选一单项选择题，随机划分后对所有模型进行了统一5-shot测试。&lt;/p&gt; &#xA;&lt;h3&gt;结果&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-LLaMA-v2-pretrain&lt;/td&gt; &#xA;   &lt;td&gt;23.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya-LLaMA-13B-pretrain&lt;/td&gt; &#xA;   &lt;td&gt;27.64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon-7B&lt;/td&gt; &#xA;   &lt;td&gt;27.18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TigerBot-7B-base&lt;/td&gt; &#xA;   &lt;td&gt;25.19&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td&gt;28.17&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM-6B&lt;/td&gt; &#xA;   &lt;td&gt;23.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOM-7B&lt;/td&gt; &#xA;   &lt;td&gt;26.55&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOMZ-7B&lt;/td&gt; &#xA;   &lt;td&gt;30.27&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aquila-7B&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25.58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;baichuan-7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;34.44&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt;其中 Aquila 模型来源于智源官方网站(&lt;a href=&#34;https://model.baai.ac.cn/model-detail/100098&#34;&gt;https://model.baai.ac.cn/model-detail/100098&lt;/a&gt;) 仅做参考&lt;/p&gt; &#xA;&lt;h2&gt;英文榜单&lt;/h2&gt; &#xA;&lt;p&gt;除了中文之外，我们也测试了模型在英文上的效果，&lt;a href=&#34;https://arxiv.org/abs/2009.03300&#34;&gt;MMLU&lt;/a&gt; 是包含57个多选任务的英文评测数据集，涵盖了初等数学、美国历史、计算机科学、法律等，难度覆盖高中水平到专家水平，是目前主流的LLM评测数据集。&lt;/p&gt; &#xA;&lt;p&gt;我们采用了&lt;a href=&#34;https://github.com/hendrycks/test&#34;&gt;开源&lt;/a&gt; 的评测方案，最终 5-shot 结果如下所示：&lt;/p&gt; &#xA;&lt;h3&gt;结果&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Humanities&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Social Sciences&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STEM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Other&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;34.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon-7B&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mpt-7B&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM-6B&lt;sup&gt;0&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;35.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOM-7B&lt;sup&gt;0&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;25.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLOOMZ-7B&lt;sup&gt;0&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;31.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon-003-base (16B)&lt;sup&gt;0&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;24.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon-003-sft (16B)&lt;sup&gt;0&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;baichuan-7B&lt;sup&gt;0&lt;/sup&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;38.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;48.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;35.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;48.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;42.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;上标说明：&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;0:重新复现&#xA;1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#xA;2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;复现方法&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/hendrycks/test&#xA;cd test&#xA;wget https://people.eecs.berkeley.edu/~hendrycks/data.tar&#xA;tar xf data&#xA;mkdir results&#xA;cp evaluate_mmlu.py .&#xA;python evaluation/evaluate_mmlu.py -m /path/to/baichuan-7b&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;其中在 MMLU 上57个任务的具体细指标如下图：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/baichuan-inc/baichuan-7B/main/media/MMLU-57-tasks.png&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;其中各个学科的指标如下图：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;media/MMLU 21 Subjects.png&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h1&gt;推理方法&lt;/h1&gt; &#xA;&lt;p&gt;推理代码已经在&lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B&#34;&gt;官方 Huggingface 库&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;baichuan-inc/baichuan-7B&#34;, trust_remote_code=True)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;baichuan-inc/baichuan-7B&#34;, device_map=&#34;auto&#34;, trust_remote_code=True)&#xA;inputs = tokenizer(&#39;登鹳雀楼-&amp;gt;王之涣\n夜雨寄北-&amp;gt;&#39;, return_tensors=&#39;pt&#39;)&#xA;inputs = inputs.to(&#39;cuda:0&#39;)&#xA;pred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)&#xA;print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;训练方法&lt;/h1&gt; &#xA;&lt;h2&gt;安装依赖&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;准备数据&lt;/h2&gt; &#xA;&lt;p&gt;用户将训练语料按总rank数的倍数均匀切分成多个 UTF-8 文本文件，放置在语料目录（默认为 &lt;code&gt;data_dir&lt;/code&gt; ）下。各个rank进程将会读取语料目录下的不同文件，全部加载到内存后，开始后续训练过程。以上是简化的示范流程，建议用户在正式训练任务中，根据需求调整数据生产逻辑。&lt;/p&gt; &#xA;&lt;h2&gt;下载 tokenizer 模型&lt;/h2&gt; &#xA;&lt;p&gt;下载 tokenizer 模型文件 &lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B/blob/main/tokenizer.model&#34;&gt;tokenizer.model&lt;/a&gt; ，放置在项目目录下。&lt;/p&gt; &#xA;&lt;h2&gt;配置 DeepSpeed&lt;/h2&gt; &#xA;&lt;p&gt;本示范代码采用 DeepSpeed 框架进行训练。用户需根据集群情况，修改 &lt;code&gt;config/hostfile&lt;/code&gt; ，如果是多机多卡，需要修改 ssh 中各个节点的 IP 配置。具体可以参见DeepSpeed&lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;官方说明&lt;/a&gt; 。&lt;/p&gt; &#xA;&lt;h2&gt;执行训练&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;协议&lt;/h1&gt; &#xA;&lt;p&gt;对本仓库源码的使用遵循开源许可协议 &lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B/raw/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;baichuan-7B支持商用。如果将baichuan-7B 模型或其衍生品用作商业用途，请您按照如下方式联系许可方，以进行登记并向许可方申请书面授权：联系邮箱：&lt;a href=&#34;mailto:opensource@baichuan-inc.com&#34;&gt;opensource@baichuan-inc.com&lt;/a&gt;， 具体许可协议可见&lt;a href=&#34;https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;《baichuan-7B 模型许可协议》&lt;/a&gt;。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>open-mmlab/mmagic</title>
    <updated>2023-06-18T01:42:20Z</updated>
    <id>tag:github.com,2023-06-18:/open-mmlab/mmagic</id>
    <link href="https://github.com/open-mmlab/mmagic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox. Unlock the magic 🪄: Generative-AI (AIGC), easy-to-use APIs, awsome model zoo, diffusion models, for text-to-image generation, image/video restoration/enhancement, etc.&lt;/p&gt;&lt;hr&gt;&lt;div id=&#34;top&#34; align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/_static/image/mmagic-logo.png&#34; width=&#34;500px&#34;&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;font size=&#34;10&#34;&gt;&lt;b&gt;M&lt;/b&gt;ultimodal &lt;b&gt;A&lt;/b&gt;dvanced, &lt;b&gt;G&lt;/b&gt;enerative, and &lt;b&gt;I&lt;/b&gt;ntelligent &lt;b&gt;C&lt;/b&gt;reation (MMagic [em&#39;mædʒɪk])&lt;/font&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;HOT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA;  &lt;b&gt;&lt;font size=&#34;5&#34;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt; &#xA;  &lt;sup&gt; &lt;a href=&#34;https://platform.openmmlab.com&#34;&gt; &lt;i&gt;&lt;font size=&#34;4&#34;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt; &lt;/a&gt; &lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt;&#xA;  &amp;nbsp;&#xA; &lt;/div&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/mmagic/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/mmagic.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-latest-blue&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/actions&#34;&gt;&lt;img src=&#34;https://github.com/open-mmlab/mmagic/workflows/build/badge.svg?sanitize=true&#34; alt=&#34;badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/open-mmlab/mmagic&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/open-mmlab/mmagic/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/open/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;&lt;img src=&#34;https://isitmaintained.com/badge/resolution/open-mmlab/mmagic.svg?sanitize=true&#34; alt=&#34;issue resolution&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/&#34;&gt;📘Documentation&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/get_started/install.html&#34;&gt;🛠️Installation&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/model_zoo/overview.html&#34;&gt;📊Model Zoo&lt;/a&gt; | &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/changelog.html&#34;&gt;🆕Update News&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmagic/projects&#34;&gt;🚀Ongoing Projects&lt;/a&gt; | &lt;a href=&#34;https://github.com/open-mmlab/mmagic/issues&#34;&gt;🤔Reporting Issues&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/README_zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://openmmlab.medium.com/&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218352562-cdded397-b0f3-4ca1-b8dd-a60df8dca75b.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/raweFPmdzG&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://twitter.com/OpenMMLab&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/openmmlab&#34; style=&#34;text-decoration:none;&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png&#34; width=&#34;3%&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🚀 What&#39;s New &lt;a&gt;&lt;img width=&#34;35&#34; height=&#34;20&#34; src=&#34;https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-4fe0-bbba-39ffb77730be.png&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;New release &lt;a href=&#34;https://github.com/open-mmlab/mmagic/releases/tag/v1.0.1&#34;&gt;&lt;strong&gt;MMagic v1.0.1&lt;/strong&gt;&lt;/a&gt; [26/05/2023]:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support tomesd for StableDiffusion speed-up.&lt;/li&gt; &#xA; &lt;li&gt;Support all inpainting/matting/image restoration models inferencer.&lt;/li&gt; &#xA; &lt;li&gt;Support animated drawings.&lt;/li&gt; &#xA; &lt;li&gt;Support Style-Based Global Appearance Flow for Virtual Try-On.&lt;/li&gt; &#xA; &lt;li&gt;Fix inferencer in pip-install.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are excited to announce the release of MMagic v1.0.0 that inherits from &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After iterative updates with OpenMMLab 2.0 framework and merged with MMGeneration, MMEditing has become a powerful tool that supports low-level algorithms based on both GAN and CNN. Today, MMEditing embraces Generative AI and transforms into a more advanced and comprehensive AIGC toolkit: &lt;strong&gt;MMagic&lt;/strong&gt; (&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;A&lt;/strong&gt;dvanced, &lt;strong&gt;G&lt;/strong&gt;enerative, and &lt;strong&gt;I&lt;/strong&gt;ntelligent &lt;strong&gt;C&lt;/strong&gt;reation). MMagic will provide more agile and flexible experimental support for researchers and AIGC enthusiasts, and help you on your AIGC exploration journey.&lt;/p&gt; &#xA;&lt;p&gt;We highlight the following new features.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. New Models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We support 11 new models in 4 new tasks.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text2Image / Diffusion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ControlNet&lt;/li&gt; &#xA;   &lt;li&gt;DreamBooth&lt;/li&gt; &#xA;   &lt;li&gt;Stable Diffusion&lt;/li&gt; &#xA;   &lt;li&gt;Disco Diffusion&lt;/li&gt; &#xA;   &lt;li&gt;GLIDE&lt;/li&gt; &#xA;   &lt;li&gt;Guided Diffusion&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;3D-aware Generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;EG3D&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Image Restoration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;NAFNet&lt;/li&gt; &#xA;   &lt;li&gt;Restormer&lt;/li&gt; &#xA;   &lt;li&gt;SwinIR&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Image Colorization &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;InstColorization&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Magic Diffusion Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For the Diffusion Model, we provide the following &#34;magic&#34; :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support image generation based on Stable Diffusion and Disco Diffusion.&lt;/li&gt; &#xA; &lt;li&gt;Support Finetune methods such as Dreambooth and DreamBooth LoRA.&lt;/li&gt; &#xA; &lt;li&gt;Support controllability in text-to-image generation using ControlNet.&lt;/li&gt; &#xA; &lt;li&gt;Support acceleration and optimization strategies based on xFormers to improve training and inference efficiency.&lt;/li&gt; &#xA; &lt;li&gt;Support video generation based on MultiFrame Render.&lt;/li&gt; &#xA; &lt;li&gt;Support calling basic models and sampling strategies through DiffuserWrapper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Upgraded Framework&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;By using MMEngine and MMCV of OpenMMLab 2.0 framework, MMagic has upgraded in the following new features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Refactor DataSample to support the combination and splitting of batch dimensions.&lt;/li&gt; &#xA; &lt;li&gt;Refactor DataPreprocessor and unify the data format for various tasks during training and inference.&lt;/li&gt; &#xA; &lt;li&gt;Refactor MultiValLoop and MultiTestLoop, supporting the evaluation of both generation-type metrics (e.g. FID) and reconstruction-type metrics (e.g. SSIM), and supporting the evaluation of multiple datasets at once.&lt;/li&gt; &#xA; &lt;li&gt;Support visualization on local files or using tensorboard and wandb.&lt;/li&gt; &#xA; &lt;li&gt;Support for 33+ algorithms accelerated by Pytorch 2.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;MMagic&lt;/strong&gt; has supported all the tasks, models, metrics, and losses in &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt; and unifies interfaces of all components based on &lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt; 😍.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/changelog.md&#34;&gt;changelog.md&lt;/a&gt; for details and release history.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/migration/overview.md&#34;&gt;migration documents&lt;/a&gt; to migrate from &lt;a href=&#34;https://github.com/open-mmlab/mmagic/tree/0.x&#34;&gt;old version&lt;/a&gt; MMEditing 0.x to new version MMagic 1.x .&lt;/p&gt; &#xA;&lt;h2&gt;📄 Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-introduction&#34;&gt;📖 Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-contributing&#34;&gt;🙌 Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-installation&#34;&gt;🛠️ Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-model-zoo&#34;&gt;📊 Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-acknowledgement&#34;&gt;🤝 Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-citation&#34;&gt;🖊️ Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#-license&#34;&gt;🎫 License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#%EF%B8%8F-%EF%B8%8Fopenmmlab-family&#34;&gt;🏗️ ️OpenMMLab Family&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📖 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;MMagic (&lt;strong&gt;M&lt;/strong&gt;ultimodal &lt;strong&gt;A&lt;/strong&gt;dvanced, &lt;strong&gt;G&lt;/strong&gt;enerative, and &lt;strong&gt;I&lt;/strong&gt;ntelligent &lt;strong&gt;C&lt;/strong&gt;reation) is an advanced and comprehensive AIGC toolkit that inherits from &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmgeneration&#34;&gt;MMGeneration&lt;/a&gt;. It is an open-source image and video editing&amp;amp;generating toolbox based on PyTorch. It is a part of the &lt;a href=&#34;https://openmmlab.com/&#34;&gt;OpenMMLab&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;Currently, MMagic support multiple image and video generation/editing tasks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/49083766/233564593-7d3d48ed-e843-4432-b610-35e3d257765c.mp4&#34;&gt;https://user-images.githubusercontent.com/49083766/233564593-7d3d48ed-e843-4432-b610-35e3d257765c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The best practice on our main branch works with &lt;strong&gt;Python 3.8+&lt;/strong&gt; and &lt;strong&gt;PyTorch 1.9+&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;✨ Major features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State of the Art Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMagic provides state-of-the-art generative models to process, edit and synthesize images and videos.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Powerful and Popular Applications&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MMagic supports popular and contemporary image restoration, text-to-image, 3D-aware generation, inpainting, matting, super-resolution and generation applications. Specifically, MMagic supports fine-tuning for stable diffusion and many exciting diffusion&#39;s application such as ControlNet Animation with SAM. MMagic also supports GAN interpolation, GAN projection, GAN manipulations and many other popular GAN’s applications. It’s time to begin your AIGC exploration journey!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;By using MMEngine and MMCV of OpenMMLab 2.0 framework, MMagic decompose the editing framework into different modules and one can easily construct a customized editor framework by combining different modules. We can define the training process just like playing with Legos and provide rich components and strategies. In MMagic, you can complete controls on the training process with different levels of APIs. With the support of &lt;a href=&#34;https://github.com/open-mmlab/mmengine/raw/main/mmengine/model/wrappers/seperate_distributed.py&#34;&gt;MMSeparateDistributedDataParallel&lt;/a&gt;, distributed training for dynamic architectures can be easily implemented.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🙌 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;More and more community contributors are joining us to make our repo better. Some recent projects are contributed by the community including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/glide/configs/README.md&#34;&gt;GLIDE&lt;/a&gt; is contributed by @Taited.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/restormer/README.md&#34;&gt;Restormer&lt;/a&gt; is contributed by @AlexZou14.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/swinir/README.md&#34;&gt;SwinIR&lt;/a&gt; is contributed by @Zdafeng.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/README.md&#34;&gt;Projects&lt;/a&gt; is opened to make it easier for everyone to add projects to MMagic.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all contributions to improve MMagic. Please refer to &lt;a href=&#34;https://github.com/open-mmlab/mmcv/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; in MMCV and &lt;a href=&#34;https://github.com/open-mmlab/mmengine/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; in MMEngine for more details about the contributing guideline.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🛠️ Installation&lt;/h2&gt; &#xA;&lt;p&gt;MMagic depends on &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt; and &lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;. Below are quick steps for installation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Install PyTorch following &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;official instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; Install MMCV, MMEngine and MMagic with &lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip3 install openmim&#xA;mim install &#39;mmcv&amp;gt;=2.0.0&#39;&#xA;mim install &#39;mmengine&#39;&#xA;mim install &#39;mmagic&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step 3.&lt;/strong&gt; Verify MMagic has been successfully installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd ~&#xA;python -c &#34;import mmagic; print(mmagic.__version__)&#34;&#xA;# Example output: 1.0.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;After installing MMagic successfully, now you are able to play with MMagic! To generate an image from text, you only need several lines of codes by MMagic!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mmagic.apis import MMagicInferencer&#xA;sd_inferencer = MMagicInferencer(model_name=&#39;stable_diffusion&#39;)&#xA;text_prompts = &#39;A panda is having dinner at KFC&#39;&#xA;result_out_dir = &#39;output/sd_res.png&#39;&#xA;sd_inferencer.infer(text=text_prompts, result_out_dir=result_out_dir)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/get_started/quick_run.md&#34;&gt;quick run&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/user_guides/inference.md&#34;&gt;inference&lt;/a&gt; for the basic usage of MMagic.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install MMagic from source&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also experiment on the latest developed version rather than the stable release by installing MMagic from source with the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/open-mmlab/mmagic.git&#xA;cd mmagic&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/docs/en/get_started/install.md&#34;&gt;installation&lt;/a&gt; for more detailed instruction.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📊 Model Zoo&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;b&gt;Supported algorithms&lt;/b&gt; &#xA;&lt;/div&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Conditional GANs&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Unconditional GANs&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Restoration&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Super-Resolution&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/sngan_proj/README.md&#34;&gt;SNGAN/Projection GAN (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/sagan/README.md&#34;&gt;SAGAN (ICML&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/biggan/README.md&#34;&gt;BIGGAN/BIGGAN-DEEP (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dcgan/README.md&#34;&gt;DCGAN (ICLR&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/wgan-gp/README.md&#34;&gt;WGAN-GP (NeurIPS&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/lsgan/README.md&#34;&gt;LSGAN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/ggan/README.md&#34;&gt;GGAN (ArXiv&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/pggan/README.md&#34;&gt;PGGAN (ICLR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/singan/README.md&#34;&gt;SinGAN (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv1/README.md&#34;&gt;StyleGANV1 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv2/README.md&#34;&gt;StyleGANV2 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/styleganv3/README.md&#34;&gt;StyleGANV3 (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/swinir/README.md&#34;&gt;SwinIR (ICCVW&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/nafnet/README.md&#34;&gt;NAFNet (ECCV&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/restormer/README.md&#34;&gt;Restormer (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/srcnn/README.md&#34;&gt;SRCNN (TPAMI&#39;2015)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/srgan_resnet/README.md&#34;&gt;SRResNet&amp;amp;SRGAN (CVPR&#39;2016)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/edsr/README.md&#34;&gt;EDSR (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/esrgan/README.md&#34;&gt;ESRGAN (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/rdn/README.md&#34;&gt;RDN (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dic/README.md&#34;&gt;DIC (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/ttsr/README.md&#34;&gt;TTSR (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/glean/README.md&#34;&gt;GLEAN (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/liif/README.md&#34;&gt;LIIF (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/real_esrgan/README.md&#34;&gt;Real-ESRGAN (ICCVW&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Video Super-Resolution&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Video Interpolation&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Colorization&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Image Translation&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/edvr/README.md&#34;&gt;EDVR (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tof/README.md&#34;&gt;TOF (IJCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tdan/README.md&#34;&gt;TDAN (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/basicvsr/README.md&#34;&gt;BasicVSR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/iconvsr/README.md&#34;&gt;IconVSR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/basicvsr_pp/README.md&#34;&gt;BasicVSR++ (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/real_basicvsr/README.md&#34;&gt;RealBasicVSR (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/tof/README.md&#34;&gt;TOFlow (IJCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/cain/README.md&#34;&gt;CAIN (AAAI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/flavr/README.md&#34;&gt;FLAVR (CVPR&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/inst_colorization/README.md&#34;&gt;InstColorization (CVPR&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/pix2pix/README.md&#34;&gt;Pix2Pix (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/cyclegan/README.md&#34;&gt;CycleGAN (ICCV&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr align=&#34;center&#34; valign=&#34;bottom&#34;&gt; &#xA;   &lt;td&gt; &lt;b&gt;Inpainting&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Matting&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;Text-to-Image&lt;/b&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;b&gt;3D-aware Generation&lt;/b&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr valign=&#34;top&#34;&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/global_local/README.md&#34;&gt;Global&amp;amp;Local (ToG&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/deepfillv1/README.md&#34;&gt;DeepFillv1 (CVPR&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/partial_conv/README.md&#34;&gt;PConv (ECCV&#39;2018)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/deepfillv2/README.md&#34;&gt;DeepFillv2 (CVPR&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/aot_gan/README.md&#34;&gt;AOT-GAN (TVCG&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dim/README.md&#34;&gt;DIM (CVPR&#39;2017)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/indexnet/README.md&#34;&gt;IndexNet (ICCV&#39;2019)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/gca/README.md&#34;&gt;GCA (AAAI&#39;2020)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/projects/glide/configs/README.md&#34;&gt;GLIDE (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/guided_diffusion/README.md&#34;&gt;Guided Diffusion (NeurIPS&#39;2021)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/disco_diffusion/README.md&#34;&gt;Disco-Diffusion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/stable_diffusion/README.md&#34;&gt;Stable-Diffusion (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/dreambooth/README.md&#34;&gt;DreamBooth (2022)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/controlnet/README.md&#34;&gt;ControlNet (2023)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/controlnet_animation/README.md&#34;&gt;ControlNet Animation (2023)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/configs/eg3d/README.md&#34;&gt;EG3D (CVPR&#39;2022)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;   &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://mmagic.readthedocs.io/en/latest/model_zoo/overview.html&#34;&gt;model_zoo&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🤝 Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;MMagic is an open source project that is contributed by researchers and engineers from various colleges and companies. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new methods.&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks. Thank you all!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/open-mmlab/mmagic/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=open-mmlab/mmagic&#34;&gt; &lt;/a&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🖊️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If MMagic is helpful to your research, please cite it as below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mmagic2023,&#xA;    title = {{MMagic}: {OpenMMLab} Multimodal Advanced, Generative, and Intelligent Creation Toolbox},&#xA;    author = {{MMagic Contributors}},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmagic}},&#xA;    year = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{mmediting2022,&#xA;    title = {{MMEditing}: {OpenMMLab} Image and Video Editing Toolbox},&#xA;    author = {{MMEditing Contributors}},&#xA;    howpublished = {\url{https://github.com/open-mmlab/mmediting}},&#xA;    year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎫 License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;. Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/LICENSE&#34;&gt;LICENSES&lt;/a&gt; for the careful check, if you are using our code for commercial matters.&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🏗️ ️OpenMMLab Family&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmengine&#34;&gt;MMEngine&lt;/a&gt;: OpenMMLab foundational library for training deep learning models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmcv&#34;&gt;MMCV&lt;/a&gt;: OpenMMLab foundational library for computer vision.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mim&#34;&gt;MIM&lt;/a&gt;: MIM installs OpenMMLab packages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpretrain&#34;&gt;MMPreTrain&lt;/a&gt;: OpenMMLab Pre-training Toolbox and Benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34;&gt;MMDetection&lt;/a&gt;: OpenMMLab detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection3d&#34;&gt;MMDetection3D&lt;/a&gt;: OpenMMLab&#39;s next-generation platform for general 3D object detection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrotate&#34;&gt;MMRotate&lt;/a&gt;: OpenMMLab rotated object detection toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;: OpenMMLab semantic segmentation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34;&gt;MMOCR&lt;/a&gt;: OpenMMLab text detection, recognition, and understanding toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmpose&#34;&gt;MMPose&lt;/a&gt;: OpenMMLab pose estimation toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmhuman3d&#34;&gt;MMHuman3D&lt;/a&gt;: OpenMMLab 3D human parametric model toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmselfsup&#34;&gt;MMSelfSup&lt;/a&gt;: OpenMMLab self-supervised learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmrazor&#34;&gt;MMRazor&lt;/a&gt;: OpenMMLab model compression toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmfewshot&#34;&gt;MMFewShot&lt;/a&gt;: OpenMMLab fewshot learning toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;: OpenMMLab&#39;s next-generation action understanding toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmtracking&#34;&gt;MMTracking&lt;/a&gt;: OpenMMLab video perception toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmflow&#34;&gt;MMFlow&lt;/a&gt;: OpenMMLab optical flow toolbox and benchmark.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmagic&#34;&gt;MMagic&lt;/a&gt;: OpenMMLab Multimodal Advanced, Generative, and Intelligent Creation Toolbox.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdeploy&#34;&gt;MMDeploy&lt;/a&gt;: OpenMMLab model deployment framework.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;right&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/mmagic/main/#top&#34;&gt;🔝Back to top&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SqueezeAILab/SqueezeLLM</title>
    <updated>2023-06-18T01:42:20Z</updated>
    <id>tag:github.com,2023-06-18:/SqueezeAILab/SqueezeLLM</id>
    <link href="https://github.com/SqueezeAILab/SqueezeLLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SqueezeLLM: Dense-and-Sparse Quantization&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SqueezeLLM: Dense-and-Sparse Quantization [&lt;a href=&#34;https://arxiv.org/abs/2306.07629&#34;&gt;Paper&lt;/a&gt;]&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SqueezeAILab/SqueezeLLM/main/figs/thumbnail.png&#34; alt=&#34;Thumbnail&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;SqueezeLLM is a post-training quantization framework that incorporates a new method called Dense-and-Sparse Quantization to enable efficient LLM serving.&lt;/p&gt; &#xA;&lt;p&gt;TLDR: Deploying LLMs is difficult due to their large memory size. This can be addressed with reduced precision quantization. But a naive method hurts performance. We address this with a new Dense-and-Sparse Quantization method. Dense-and-Sparse splits weight matrices into two components: A dense component that can be heavily quantized without affecting model performance, as well as a sparse part that preserves sensitive and outlier parts of the weight matrices With this approach, we are able to serve larger models with smaller memory footprint, the same latency, and &lt;strong&gt;yet higher accuracy and quality&lt;/strong&gt;. For instance, the Squeeze variant of the Vicuna models can be served within 6 GB of memory and reach 2% higher MMLU than the baseline model in FP16 with an even 2x larger memory footprint. For more details please check out our &lt;a href=&#34;https://arxiv.org/abs/2306.07629&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updates:&lt;/strong&gt; Vicuna-7B and 13B, and LLaMA-30B are all supported with both 3-bit and 4-bit.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a conda environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create --name sqllm python=3.9 -y&#xA;conda activate sqllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone and install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/SqueezeAILab/SqueezeLLM&#xA;cd SqueezeLLM&#xA;pip install -e .&#xA;cd squeezellm&#xA;python setup_cuda.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;Currently, we support &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA&lt;/a&gt; 7B, 13B, and 30B, as well as the instruction-tuned &lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;Vicuna&lt;/a&gt; 7B and 13B. For each model, we support 3-bit and 4-bit quantized models, with sparse levels of 0% (dense-only), 0.05%, and 0.45%. See our &lt;a href=&#34;https://arxiv.org/abs/2306.07629&#34;&gt;Paper&lt;/a&gt; for more detailed information on these configurations. Below are the links to download the models.&lt;/p&gt; &#xA;&lt;h3&gt;LLaMA&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Bitwidth&lt;/th&gt; &#xA;   &lt;th&gt;Dense-only (0%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-llama-7b-w3-s0/blob/main/sq-llama-7b-w3-s0.pt&#34;&gt;sq-llama-7b-w3-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-llama-7b-w4-s0/blob/main/sq-llama-7b-w4-s0.pt&#34;&gt;sq-llama-7b-w4-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-llama-13b-w3-s0/blob/main/sq-llama-13b-w3-s0.pt&#34;&gt;sq-llama-13b-w3-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-llama-13b-w4-s0/blob/main/sq-llama-13b-w4-s0.pt&#34;&gt;sq-llama-13b-w4-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-30B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-llama-30b-w3-s0/blob/main/sq-llama-30b-w3-s0.pt&#34;&gt;sq-llama-30b-w3-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-30B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-llama-30b-w4-s0/blob/main/sq-llama-30b-w4-s0.pt&#34;&gt;sq-llama-30b-w4-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Vicuna&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Bitwidth&lt;/th&gt; &#xA;   &lt;th&gt;Dense-only (0%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-7B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-vicuna-7b-w3-s0/blob/main/sq-vicuna-7b-w3-s0.pt&#34;&gt;sq-vicuna-7b-w3-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-7B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-vicuna-7b-w4-s0/blob/main/sq-vicuna-7b-w4-s0.pt&#34;&gt;sq-vicuna-7b-w4-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-vicuna-13b-w3-s0/blob/main/sq-vicuna-13b-w3-s0.pt&#34;&gt;sq-vicuna-13b-w3-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/squeeze-ai-lab/sq-vicuna-13b-w4-s0/blob/main/sq-vicuna-13b-w4-s0.pt&#34;&gt;sq-vicuna-13b-w4-s0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Sparsity levels with 0.05% and 0.45% are coming soon!&lt;/p&gt; &#xA;&lt;p&gt;The LLaMA model &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/LICENSE&#34;&gt;license&lt;/a&gt; is currently only available for research purposes. We direct everyone to carefully review the license before using the quantized models. Similar to other works on LLaMA, we only release the quantized portions of the model in &lt;a href=&#34;https://huggingface.co/squeeze-ai-lab&#34;&gt;Huggingface Model Hub&lt;/a&gt;. To successfully run our code, you need to first obtain the original, pre-trained LLaMA model in the Huggingface-compatible format locally and provide the path in the commands below. We have scripts that will substitute the necessary components, but you will need the original model for those scripts to run.&lt;/p&gt; &#xA;&lt;h3&gt;Benchmarking&lt;/h3&gt; &#xA;&lt;p&gt;The following code will run and benchmark the 3-bit quantized LLaMA-7B model on the C4 dataset. The &lt;code&gt;--torch_profile&lt;/code&gt; argument can be passed when running benchmarking to replicate the runtime results from the paper. Download the quantized model (e.g. &lt;code&gt;sq-llama-7b-w3-s0.pt&lt;/code&gt;) locally from the link above. You can follow the same procedure for other quantized models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python llama.py &amp;lt;path-to-llama-7b-hf&amp;gt; c4 --wbits 3 --load sq-llama-7b-w3-s0.pt --benchmark 128 --check&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Perplexity Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;The following code will evaluate perplexity using the 3-bit quantized LLaMA-7B model on the C4 dataset, following the same evaluation methodology of &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;GPTQ&lt;/a&gt; and &lt;a href=&#34;https://github.com/qwopqwop200/GPTQ-for-LLaMa/&#34;&gt;GPTQ-For-LLaMA&lt;/a&gt;. Download the quantized model (e.g. &lt;code&gt;sq-llama-7b-w3-s0.pt&lt;/code&gt;) locally from the link above. You can follow the same procedure for other quantized models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python llama.py &amp;lt;path-to-llama-7b-hf&amp;gt; c4 --wbits 3 --load sq-llama-7b-w3-s0.pt --eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The code was tested on A5000 and A6000 GPUs with Cuda 11.3 and CUDNN 8.2.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This code reuses components from several libraries including &lt;a href=&#34;https://github.com/IST-DASLab/gptq&#34;&gt;GPTQ&lt;/a&gt; as well as &lt;a href=&#34;https://github.com/qwopqwop200/GPTQ-for-LLaMa/&#34;&gt;GPTQ-For-LLaMA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;SqueezeLLM has been developed as part of the following paper. We appreciate it if you would please cite the following paper if you found the library useful for your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{kim2023squeezellm,&#xA;  title={SqueezeLLM: Dense-and-Sparse Quantization},&#xA;  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael and Keutzer, Kurt},&#xA;  journal={arXiv},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>