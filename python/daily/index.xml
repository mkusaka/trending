<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-25T01:33:46Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DepthAnything/Depth-Anything-V2</title>
    <updated>2025-01-25T01:33:46Z</updated>
    <id>tag:github.com,2025-01-25:/DepthAnything/Depth-Anything-V2</id>
    <link href="https://github.com/DepthAnything/Depth-Anything-V2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[NeurIPS 2024] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Depth Anything V2&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://liheyoung.github.io/&#34;&gt;&lt;strong&gt;Lihe Yang&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; · &lt;a href=&#34;https://bingykang.github.io/&#34;&gt;&lt;strong&gt;Bingyi Kang&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;2†&lt;/sup&gt; · &lt;a href=&#34;http://speedinghzl.github.io/&#34;&gt;&lt;strong&gt;Zilong Huang&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;br&gt; &lt;a href=&#34;http://zhaozhen.me/&#34;&gt;&lt;strong&gt;Zhen Zhao&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://xiaogang00.github.io/&#34;&gt;&lt;strong&gt;Xiaogang Xu&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://sites.google.com/site/jshfeng/&#34;&gt;&lt;strong&gt;Jiashi Feng&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt; · &lt;a href=&#34;https://hszhao.github.io/&#34;&gt;&lt;strong&gt;Hengshuang Zhao&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;HKU   &lt;sup&gt;2&lt;/sup&gt;TikTok &lt;br&gt; †project lead *corresponding author&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.09414&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Depth%20Anything%20V2-red&#34; alt=&#34;Paper PDF&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://depth-anything-v2.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-Depth%20Anything%20V2-green&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/depth-anything/Depth-Anything-V2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/depth-anything/DA-2K&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Benchmark-DA--2K-yellow&#34; alt=&#34;Benchmark&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This work presents Depth Anything V2. It significantly outperforms &lt;a href=&#34;https://github.com/LiheYoung/Depth-Anything&#34;&gt;V1&lt;/a&gt; in fine-grained details and robustness. Compared with SD-based models, it enjoys faster inference speed, fewer parameters, and higher depth accuracy.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/assets/teaser.png&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2025-01-22:&lt;/strong&gt; &lt;a href=&#34;https://videodepthanything.github.io&#34;&gt;Video Depth Anything&lt;/a&gt; has been released. It generates consistent depth maps for super-long videos (e.g., over 5 minutes).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-12-22:&lt;/strong&gt; &lt;a href=&#34;https://promptda.github.io/&#34;&gt;Prompt Depth Anything&lt;/a&gt; has been released. It supports 4K resolution metric depth estimation when low-res LiDAR is used to prompt the DA models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-07-06:&lt;/strong&gt; Depth Anything V2 is supported in &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;Transformers&lt;/a&gt;. See the &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2&#34;&gt;instructions&lt;/a&gt; for convenient usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-06-25:&lt;/strong&gt; Depth Anything is integrated into &lt;a href=&#34;https://developer.apple.com/machine-learning/models/&#34;&gt;Apple Core ML Models&lt;/a&gt;. See the instructions (&lt;a href=&#34;https://huggingface.co/apple/coreml-depth-anything-small&#34;&gt;V1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/apple/coreml-depth-anything-v2-small&#34;&gt;V2&lt;/a&gt;) for usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-06-22:&lt;/strong&gt; We release &lt;a href=&#34;https://github.com/DepthAnything/Depth-Anything-V2/tree/main/metric_depth#pre-trained-models&#34;&gt;smaller metric depth models&lt;/a&gt; based on Depth-Anything-V2-Small and Base.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-06-20:&lt;/strong&gt; Our repository and project page are flagged by GitHub and removed from the public for 6 days. Sorry for the inconvenience.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-06-14:&lt;/strong&gt; Paper, project page, code, models, demo, and benchmark are all released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pre-trained Models&lt;/h2&gt; &#xA;&lt;p&gt;We provide &lt;strong&gt;four models&lt;/strong&gt; of varying scales for robust relative depth estimation:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Depth-Anything-V2-Small&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;24.8M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth?download=true&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Depth-Anything-V2-Base&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;97.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth?download=true&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Depth-Anything-V2-Large&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;335.3M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Depth-Anything-V2-Giant&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1.3B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Coming soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Prepraration&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/DepthAnything/Depth-Anything-V2&#xA;cd Depth-Anything-V2&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the checkpoints listed &lt;a href=&#34;https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/#pre-trained-models&#34;&gt;here&lt;/a&gt; and put them under the &lt;code&gt;checkpoints&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Use our models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import torch&#xA;&#xA;from depth_anything_v2.dpt import DepthAnythingV2&#xA;&#xA;DEVICE = &#39;cuda&#39; if torch.cuda.is_available() else &#39;mps&#39; if torch.backends.mps.is_available() else &#39;cpu&#39;&#xA;&#xA;model_configs = {&#xA;    &#39;vits&#39;: {&#39;encoder&#39;: &#39;vits&#39;, &#39;features&#39;: 64, &#39;out_channels&#39;: [48, 96, 192, 384]},&#xA;    &#39;vitb&#39;: {&#39;encoder&#39;: &#39;vitb&#39;, &#39;features&#39;: 128, &#39;out_channels&#39;: [96, 192, 384, 768]},&#xA;    &#39;vitl&#39;: {&#39;encoder&#39;: &#39;vitl&#39;, &#39;features&#39;: 256, &#39;out_channels&#39;: [256, 512, 1024, 1024]},&#xA;    &#39;vitg&#39;: {&#39;encoder&#39;: &#39;vitg&#39;, &#39;features&#39;: 384, &#39;out_channels&#39;: [1536, 1536, 1536, 1536]}&#xA;}&#xA;&#xA;encoder = &#39;vitl&#39; # or &#39;vits&#39;, &#39;vitb&#39;, &#39;vitg&#39;&#xA;&#xA;model = DepthAnythingV2(**model_configs[encoder])&#xA;model.load_state_dict(torch.load(f&#39;checkpoints/depth_anything_v2_{encoder}.pth&#39;, map_location=&#39;cpu&#39;))&#xA;model = model.to(DEVICE).eval()&#xA;&#xA;raw_img = cv2.imread(&#39;your/image/path&#39;)&#xA;depth = model.infer_image(raw_img) # HxW raw depth map in numpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you do not want to clone this repository, you can also load our models through &lt;a href=&#34;https://github.com/huggingface/transformers/&#34;&gt;Transformers&lt;/a&gt;. Below is a simple code snippet. Please refer to the &lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2&#34;&gt;official page&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note 1: Make sure you can connect to Hugging Face and have installed the latest Transformers.&lt;/li&gt; &#xA; &lt;li&gt;Note 2: Due to the &lt;a href=&#34;https://github.com/huggingface/transformers/pull/31522#issuecomment-2184123463&#34;&gt;upsampling difference&lt;/a&gt; between OpenCV (we used) and Pillow (HF used), predictions may differ slightly. So you are more recommended to use our models through the way introduced above.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;from PIL import Image&#xA;&#xA;pipe = pipeline(task=&#34;depth-estimation&#34;, model=&#34;depth-anything/Depth-Anything-V2-Small-hf&#34;)&#xA;image = Image.open(&#39;your/image/path&#39;)&#xA;depth = pipe(image)[&#34;depth&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running script on &lt;em&gt;images&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py \&#xA;  --encoder &amp;lt;vits | vitb | vitl | vitg&amp;gt; \&#xA;  --img-path &amp;lt;path&amp;gt; --outdir &amp;lt;outdir&amp;gt; \&#xA;  [--input-size &amp;lt;size&amp;gt;] [--pred-only] [--grayscale]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--img-path&lt;/code&gt;: You can either 1) point it to an image directory storing all interested images, 2) point it to a single image, or 3) point it to a text file storing all image paths.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--input-size&lt;/code&gt; (optional): By default, we use input size &lt;code&gt;518&lt;/code&gt; for model inference. &lt;em&gt;&lt;strong&gt;You can increase the size for even more fine-grained results.&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pred-only&lt;/code&gt; (optional): Only save the predicted depth map, without raw image.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--grayscale&lt;/code&gt; (optional): Save the grayscale depth map, without applying color palette.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --encoder vitl --img-path assets/examples --outdir depth_vis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running script on &lt;em&gt;videos&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run_video.py \&#xA;  --encoder &amp;lt;vits | vitb | vitl | vitg&amp;gt; \&#xA;  --video-path assets/examples_video --outdir video_depth_vis \&#xA;  [--input-size &amp;lt;size&amp;gt;] [--pred-only] [--grayscale]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Our larger model has better temporal consistency on videos.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Gradio demo&lt;/h3&gt; &#xA;&lt;p&gt;To use our gradio demo locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also try our &lt;a href=&#34;https://huggingface.co/spaces/Depth-Anything/Depth-Anything-V2&#34;&gt;online demo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note: Compared to V1, we have made a minor modification to the DINOv2-DPT architecture (originating from this &lt;a href=&#34;https://github.com/LiheYoung/Depth-Anything/issues/81&#34;&gt;issue&lt;/a&gt;).&lt;/strong&gt;&lt;/em&gt; In V1, we &lt;em&gt;unintentionally&lt;/em&gt; used features from the last four layers of DINOv2 for decoding. In V2, we use &lt;a href=&#34;https://github.com/DepthAnything/Depth-Anything-V2/raw/2cbc36a8ce2cec41d38ee51153f112e87c8e42d8/depth_anything_v2/dpt.py#L164-L169&#34;&gt;intermediate features&lt;/a&gt; instead. Although this modification did not improve details or accuracy, we decided to follow this common practice.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuned to Metric Depth Estimation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/metric_depth&#34;&gt;metric depth estimation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;DA-2K Evaluation Benchmark&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/DepthAnything/Depth-Anything-V2/main/DA-2K.md&#34;&gt;DA-2K benchmark&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Support&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We sincerely appreciate all the community support for our Depth Anything series. Thank you a lot!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple Core ML: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://developer.apple.com/machine-learning/models&#34;&gt;https://developer.apple.com/machine-learning/models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/apple/coreml-depth-anything-v2-small&#34;&gt;https://huggingface.co/apple/coreml-depth-anything-v2-small&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/apple/coreml-depth-anything-small&#34;&gt;https://huggingface.co/apple/coreml-depth-anything-small&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Transformers: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2&#34;&gt;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything&#34;&gt;https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TensorRT: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/spacewalk01/depth-anything-tensorrt&#34;&gt;https://github.com/spacewalk01/depth-anything-tensorrt&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/zhujiajian98/Depth-Anythingv2-TensorRT-python&#34;&gt;https://github.com/zhujiajian98/Depth-Anythingv2-TensorRT-python&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ONNX: &lt;a href=&#34;https://github.com/fabio-sim/Depth-Anything-ONNX&#34;&gt;https://github.com/fabio-sim/Depth-Anything-ONNX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ComfyUI: &lt;a href=&#34;https://github.com/kijai/ComfyUI-DepthAnythingV2&#34;&gt;https://github.com/kijai/ComfyUI-DepthAnythingV2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Transformers.js (real-time depth in web): &lt;a href=&#34;https://huggingface.co/spaces/Xenova/webgpu-realtime-depth-estimation&#34;&gt;https://huggingface.co/spaces/Xenova/webgpu-realtime-depth-estimation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Android: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/shubham0204/Depth-Anything-Android&#34;&gt;https://github.com/shubham0204/Depth-Anything-Android&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/FeiGeChuanShu/ncnn-android-depth_anything&#34;&gt;https://github.com/FeiGeChuanShu/ncnn-android-depth_anything&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We are sincerely grateful to the awesome Hugging Face team (&lt;a href=&#34;https://huggingface.co/pcuenq&#34;&gt;@Pedro Cuenca&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/nielsr&#34;&gt;@Niels Rogge&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/merve&#34;&gt;@Merve Noyan&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/amyeroberts&#34;&gt;@Amy Roberts&lt;/a&gt;, et al.) for their huge efforts in supporting our models in Transformers and Apple Core ML.&lt;/p&gt; &#xA;&lt;p&gt;We also thank the &lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;DINOv2&lt;/a&gt; team for contributing such impressive models to our community.&lt;/p&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;Depth-Anything-V2-Small model is under the Apache-2.0 license. Depth-Anything-V2-Base/Large/Giant models are under the CC-BY-NC-4.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{depth_anything_v2,&#xA;  title={Depth Anything V2},&#xA;  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},&#xA;  journal={arXiv:2406.09414},&#xA;  year={2024}&#xA;}&#xA;&#xA;@inproceedings{depth_anything_v1,&#xA;  title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, &#xA;  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},&#xA;  booktitle={CVPR},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>