<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-26T01:39:39Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>potamides/DeTikZify</title>
    <updated>2025-04-26T01:39:39Z</updated>
    <id>tag:github.com,2025-04-26:/potamides/DeTikZify</id>
    <link href="https://github.com/potamides/DeTikZify" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeTi&lt;em&gt;k&lt;/em&gt;Zify&lt;br&gt;&lt;sub&gt;&lt;sup&gt;Synthesizing Graphics Programs for Scientific Figures and Sketches with Ti&lt;em&gt;k&lt;/em&gt;Z&lt;/sup&gt;&lt;/sub&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openreview.net/forum?id=bcVLFQCOjc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/View%20on%20OpenReview-8C1B13?labelColor=gray&amp;amp;logo=data:image/png;base64,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&#34; alt=&#34;OpenReview&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.15306&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/View%20on%20arXiv-B31B1B?logo=arxiv&amp;amp;labelColor=gray&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/nllg/detikzify-664460c521aa7c2880095a8b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/View%20on%20Hugging%20Face-blue?labelColor=gray&amp;amp;logo=data:image/png;base64,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&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1hPWqucbPGTavNlYvOBvSNBAwdcPZKe8F&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce &lt;a href=&#34;https://github.com/potamides/DeTikZify&#34;&gt;DeTi&lt;em&gt;k&lt;/em&gt;Zify&lt;/a&gt;, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving &lt;a href=&#34;https://github.com/pgf-tikz/pgf&#34;&gt;Ti&lt;em&gt;k&lt;/em&gt;Z&lt;/a&gt; graphics programs based on sketches and existing figures. We also introduce an MCTS-based inference algorithm that enables DeTi&lt;em&gt;k&lt;/em&gt;Zify to iteratively refine its outputs without the need for additional training.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/potamides/DeTikZify/assets/53401822/203d2853-0b5c-4a2b-9d09-3ccb65880cd3&#34;&gt;https://github.com/potamides/DeTikZify/assets/53401822/203d2853-0b5c-4a2b-9d09-3ccb65880cd3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2025-03-17&lt;/strong&gt;: We release &lt;a href=&#34;https://huggingface.co/nllg/tikzero-adapter&#34;&gt;Ti&lt;em&gt;k&lt;/em&gt;Zero&lt;/a&gt; adapters which plug directly into &lt;a href=&#34;https://huggingface.co/nllg/detikzify-v2-8b&#34;&gt;DeTi&lt;em&gt;k&lt;/em&gt;Zify&lt;sub&gt;v2&lt;/sub&gt; (8b)&lt;/a&gt; and enable zero-shot text-conditioning, and &lt;a href=&#34;https://huggingface.co/nllg/tikzero-plus-10b&#34;&gt;Ti&lt;em&gt;k&lt;/em&gt;Zero+&lt;/a&gt; with additional end-to-end fine-tuning. For more information see our &lt;a href=&#34;https://arxiv.org/abs/2503.11509&#34;&gt;paper&lt;/a&gt; and usage examples &lt;a href=&#34;https://raw.githubusercontent.com/potamides/DeTikZify/main/#usage&#34;&gt;below&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-12-05&lt;/strong&gt;: We release &lt;a href=&#34;https://huggingface.co/nllg/detikzify-v2-8b&#34;&gt;DeTi&lt;em&gt;k&lt;/em&gt;Zify&lt;sub&gt;v2&lt;/sub&gt; (8b)&lt;/a&gt;, our latest model which surpasses all previous versions in our evaluation and make it the new default model in our &lt;a href=&#34;https://huggingface.co/spaces/nllg/DeTikZify&#34;&gt;Hugging Face Space&lt;/a&gt;. Check out the &lt;a href=&#34;https://huggingface.co/nllg/detikzify-v2-8b-preview#model-card-for-detikzifyv2-8b&#34;&gt;model card&lt;/a&gt; for more information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-09-24&lt;/strong&gt;: DeTi&lt;em&gt;k&lt;/em&gt;Zify was accepted at &lt;a href=&#34;https://neurips.cc/Conferences/2024&#34;&gt;NeurIPS 2024&lt;/a&gt; as a &lt;a href=&#34;https://neurips.cc/virtual/2024/poster/94474&#34;&gt;spotlight paper&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you encounter difficulties with installation and inference on your own hardware, consider visiting our &lt;a href=&#34;https://huggingface.co/spaces/nllg/DeTikZify&#34;&gt;Hugging Face Space&lt;/a&gt; (please note that restarting the space can take up to 30 minutes). Should you experience long queues, you have the option to &lt;a href=&#34;https://huggingface.co/spaces/nllg/DeTikZify?duplicate=true&#34;&gt;duplicate&lt;/a&gt; it with a paid private GPU runtime for a more seamless experience. Additionally, you can try our demo on &lt;a href=&#34;https://colab.research.google.com/drive/1hPWqucbPGTavNlYvOBvSNBAwdcPZKe8F&#34;&gt;Google Colab&lt;/a&gt;. However, setting up the environment there might take some time, and the free tier only supports inference for the 1b models.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The Python package of DeTi&lt;em&gt;k&lt;/em&gt;Zify can be easily installed using &lt;a href=&#34;https://pip.pypa.io/en/stable&#34;&gt;pip&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install &#39;detikzify[legacy] @ git+https://github.com/potamides/DeTikZify&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;[legacy]&lt;/code&gt; extra is only required if you plan to use the DeTi&lt;em&gt;k&lt;/em&gt;Zify&lt;sub&gt;v1&lt;/sub&gt; models. If you only plan to use DeTi&lt;em&gt;k&lt;/em&gt;Zify&lt;sub&gt;v2&lt;/sub&gt; you can remove it. If your goal is to run the included &lt;a href=&#34;https://raw.githubusercontent.com/potamides/DeTikZify/main/examples&#34;&gt;examples&lt;/a&gt;, it is easier to clone the repository and install it in editable mode like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/potamides/DeTikZify&#xA;pip install -e DeTikZify[examples]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition, DeTi&lt;em&gt;k&lt;/em&gt;Zify requires a full &lt;a href=&#34;https://www.tug.org/texlive&#34;&gt;TeX Live 2023&lt;/a&gt; installation, &lt;a href=&#34;https://www.ghostscript.com&#34;&gt;ghostscript&lt;/a&gt;, and &lt;a href=&#34;https://poppler.freedesktop.org&#34;&gt;poppler&lt;/a&gt; which you have to install through your package manager or via other means.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] For interactive use and general &lt;a href=&#34;https://raw.githubusercontent.com/potamides/DeTikZify/main/detikzify/webui#usage-tips&#34;&gt;usage tips&lt;/a&gt;, we recommend checking out our &lt;a href=&#34;https://raw.githubusercontent.com/potamides/DeTikZify/main/detikzify/webui&#34;&gt;web UI&lt;/a&gt;, which can be started directly from the command line (use &lt;code&gt;--help&lt;/code&gt; for a list of all options):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python -m detikzify.webui --light&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If all required dependencies are installed, the full range of DeTi&lt;em&gt;k&lt;/em&gt;Zify features such as compiling, rendering, and saving Ti&lt;em&gt;k&lt;/em&gt;Z graphics, and MCTS-based inference can be accessed through its programming interface:&lt;/p&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary&gt;DeTi&lt;i&gt;k&lt;/i&gt;Zify Example&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from operator import itemgetter&#xA;&#xA;from detikzify.model import load&#xA;from detikzify.infer import DetikzifyPipeline&#xA;&#xA;image = &#34;https://w.wiki/A7Cc&#34;&#xA;pipeline = DetikzifyPipeline(*load(&#xA;    model_name_or_path=&#34;nllg/detikzify-v2-8b&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;    torch_dtype=&#34;bfloat16&#34;,&#xA;))&#xA;&#xA;# generate a single TikZ program&#xA;fig = pipeline.sample(image=image)&#xA;&#xA;# if it compiles, rasterize it and show it&#xA;if fig.is_rasterizable:&#xA;    fig.rasterize().show()&#xA;&#xA;# run MCTS for 10 minutes and generate multiple TikZ programs&#xA;figs = set()&#xA;for score, fig in pipeline.simulate(image=image, timeout=600):&#xA;    figs.add((score, fig))&#xA;&#xA;# save the best TikZ program&#xA;best = sorted(figs, key=itemgetter(0))[-1][1]&#xA;best.save(&#34;fig.tex&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Through &lt;a href=&#34;https://huggingface.co/nllg/tikzero-adapter&#34;&gt;Ti&lt;em&gt;k&lt;/em&gt;Zero&lt;/a&gt; adapters and &lt;a href=&#34;https://huggingface.co/nllg/tikzero-plus-10b&#34;&gt;Ti&lt;em&gt;k&lt;/em&gt;Zero+&lt;/a&gt; it is also possible to synthesize graphics programs conditioned on text (cf. our &lt;a href=&#34;https://arxiv.org/abs/2503.11509&#34;&gt;paper&lt;/a&gt; for details). Note that this currently only supported through the programming interface:&lt;/p&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary&gt;Ti&lt;i&gt;k&lt;/i&gt;Zero+ Example&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from detikzify.model import load&#xA;from detikzify.infer import DetikzifyPipeline&#xA;&#xA;caption = &#34;A multi-layer perceptron with two hidden layers.&#34;&#xA;pipeline = DetikzifyPipeline(*load(&#xA;    model_name_or_path=&#34;nllg/tikzero-plus-10b&#34;,&#xA;    device_map=&#34;auto&#34;,&#xA;    torch_dtype=&#34;bfloat16&#34;,&#xA;))&#xA;&#xA;# generate a single TikZ program&#xA;fig = pipeline.sample(text=caption)&#xA;&#xA;# if it compiles, rasterize it and show it&#xA;if fig.is_rasterizable:&#xA;    fig.rasterize().show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Ti&lt;i&gt;k&lt;/i&gt;Zero Example&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from detikzify.model import load, load_adapter&#xA;from detikzify.infer import DetikzifyPipeline&#xA;&#xA;caption = &#34;A multi-layer perceptron with two hidden layers.&#34;&#xA;pipeline = DetikzifyPipeline(&#xA;    *load_adapter(&#xA;        *load(&#xA;            model_name_or_path=&#34;nllg/detikzify-v2-8b&#34;,&#xA;            device_map=&#34;auto&#34;,&#xA;            torch_dtype=&#34;bfloat16&#34;,&#xA;        ),&#xA;        adapter_name_or_path=&#34;nllg/tikzero-adapter&#34;,&#xA;    )&#xA;)&#xA;&#xA;# generate a single TikZ program&#xA;fig = pipeline.sample(text=caption)&#xA;&#xA;# if it compiles, rasterize it and show it&#xA;if fig.is_rasterizable:&#xA;    fig.rasterize().show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;More involved examples, for example for evaluation and training, can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/potamides/DeTikZify/main/examples&#34;&gt;examples&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Model Weights &amp;amp; Datasets&lt;/h2&gt; &#xA;&lt;p&gt;We upload all our DeTi&lt;em&gt;k&lt;/em&gt;Zify models and datasets to the &lt;a href=&#34;https://huggingface.co/collections/nllg/detikzify-664460c521aa7c2880095a8b&#34;&gt;Hugging Face Hub&lt;/a&gt; (Ti&lt;em&gt;k&lt;/em&gt;Zero models are available &lt;a href=&#34;https://huggingface.co/collections/nllg/tikzero-67d1952fab69f5bd172de1fe&#34;&gt;here&lt;/a&gt;). However, please note that for the public release of the &lt;a href=&#34;https://huggingface.co/datasets/nllg/datikz-v2&#34;&gt;DaTi&lt;em&gt;k&lt;/em&gt;Z&lt;sub&gt;v2&lt;/sub&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/datasets/nllg/datikz-v3&#34;&gt;DaTi&lt;em&gt;k&lt;/em&gt;Z&lt;sub&gt;v3&lt;/sub&gt;&lt;/a&gt; datasets, we had to remove a considerable portion of Ti&lt;em&gt;k&lt;/em&gt;Z drawings originating from &lt;a href=&#34;https://arxiv.org&#34;&gt;arXiv&lt;/a&gt;, as the &lt;a href=&#34;https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html&#34;&gt;arXiv non-exclusive license&lt;/a&gt; does not permit redistribution. We do, however, release our &lt;a href=&#34;https://github.com/potamides/DaTikZ&#34;&gt;dataset creation scripts&lt;/a&gt; and encourage anyone to recreate the full version of DaTi&lt;em&gt;k&lt;/em&gt;Z themselves.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If DeTi&lt;em&gt;k&lt;/em&gt;Zify and Ti&lt;em&gt;k&lt;/em&gt;Zero have been beneficial for your research or applications, we kindly request you to acknowledge this by citing them as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{belouadi2024detikzify,&#xA;    title={{DeTikZify}: Synthesizing Graphics Programs for Scientific Figures and Sketches with {TikZ}},&#xA;    author={Jonas Belouadi and Simone Paolo Ponzetto and Steffen Eger},&#xA;    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},&#xA;    year={2024},&#xA;    url={https://openreview.net/forum?id=bcVLFQCOjc}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{belouadi2025tikzero,&#xA;    title={{TikZero}: Zero-Shot Text-Guided Graphics Program Synthesis},&#xA;    author={Jonas Belouadi and Eddy Ilg and Margret Keuper and Hideki Tanaka and Masao Utiyama and Raj Dabre and Steffen Eger and Simone Paolo Ponzetto},&#xA;    year={2025},&#xA;    eprint={2503.11509},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CL},&#xA;    url={https://arxiv.org/abs/2503.11509},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The implementation of the DeTi&lt;em&gt;k&lt;/em&gt;Zify model architecture is based on &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt; and &lt;a href=&#34;https://github.com/potamides/AutomaTikZ&#34;&gt;AutomaTikZ&lt;/a&gt; (v1), and &lt;a href=&#34;https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3&#34;&gt;Idefics 3&lt;/a&gt; (v2). Our MCTS implementation is based on &lt;a href=&#34;https://github.com/namin/llm-verified-with-monte-carlo-tree-search&#34;&gt;VerMCTS&lt;/a&gt;. The Ti&lt;em&gt;k&lt;/em&gt;Zero architecture draws inspiration from &lt;a href=&#34;https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/&#34;&gt;Flamingo&lt;/a&gt; and &lt;a href=&#34;https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices&#34;&gt;LLaMA 3.2-Vision&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>