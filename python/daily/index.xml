<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-05T01:34:40Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chrishayuk/mcp-cli</title>
    <updated>2025-07-05T01:34:40Z</updated>
    <id>tag:github.com,2025-07-05:/chrishayuk/mcp-cli</id>
    <link href="https://github.com/chrishayuk/mcp-cli" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP CLI - Model Context Protocol Command Line Interface&lt;/h1&gt; &#xA;&lt;p&gt;A powerful, feature-rich command-line interface for interacting with Model Context Protocol servers. This client enables seamless communication with LLMs through integration with the &lt;a href=&#34;https://github.com/chrishayuk/chuk-tool-processor&#34;&gt;CHUK Tool Processor&lt;/a&gt; and &lt;a href=&#34;https://github.com/chrishayuk/chuk-llm&#34;&gt;CHUK-LLM&lt;/a&gt;, providing tool usage, conversation management, and multiple operational modes.&lt;/p&gt; &#xA;&lt;h2&gt;üîÑ Architecture Overview&lt;/h2&gt; &#xA;&lt;p&gt;The MCP CLI is built on a modular architecture with clean separation of concerns:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/chrishayuk/chuk-tool-processor&#34;&gt;CHUK Tool Processor&lt;/a&gt;&lt;/strong&gt;: Async-native tool execution and MCP server communication&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/chrishayuk/chuk-llm&#34;&gt;CHUK-LLM&lt;/a&gt;&lt;/strong&gt;: Unified LLM provider configuration and client management&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP CLI&lt;/strong&gt;: Rich user interface and command orchestration (this project)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåü Features&lt;/h2&gt; &#xA;&lt;h3&gt;Multiple Operational Modes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat Mode&lt;/strong&gt;: Conversational interface with streaming responses and automated tool usage&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interactive Mode&lt;/strong&gt;: Command-driven shell interface for direct server operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Command Mode&lt;/strong&gt;: Unix-friendly mode for scriptable automation and pipelines&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Direct Commands&lt;/strong&gt;: Run individual commands without entering interactive mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Chat Interface&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streaming Responses&lt;/strong&gt;: Real-time response generation with live UI updates&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent Tool Execution&lt;/strong&gt;: Execute multiple tools simultaneously while preserving conversation order&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Smart Interruption&lt;/strong&gt;: Interrupt streaming responses or tool execution with Ctrl+C&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Performance Metrics&lt;/strong&gt;: Response timing, words/second, and execution statistics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich Formatting&lt;/strong&gt;: Markdown rendering, syntax highlighting, and progress indicators&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Comprehensive Provider Support&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: GPT models (&lt;code&gt;gpt-4o&lt;/code&gt;, &lt;code&gt;gpt-4o-mini&lt;/code&gt;, &lt;code&gt;gpt-4-turbo&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: Claude models (&lt;code&gt;claude-3-opus&lt;/code&gt;, &lt;code&gt;claude-3-sonnet&lt;/code&gt;, &lt;code&gt;claude-3-haiku&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: Local models (&lt;code&gt;llama3.2&lt;/code&gt;, &lt;code&gt;qwen2.5-coder&lt;/code&gt;, &lt;code&gt;deepseek-coder&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Custom Providers&lt;/strong&gt;: Extensible architecture for additional providers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dynamic Switching&lt;/strong&gt;: Change providers and models mid-conversation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Robust Tool System&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic Discovery&lt;/strong&gt;: Server-provided tools are automatically detected and catalogued&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Provider Adaptation&lt;/strong&gt;: Tool names are automatically sanitized for provider compatibility&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent Execution&lt;/strong&gt;: Multiple tools can run simultaneously with proper coordination&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich Progress Display&lt;/strong&gt;: Real-time progress indicators and execution timing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tool History&lt;/strong&gt;: Complete audit trail of all tool executions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streaming Tool Calls&lt;/strong&gt;: Support for tools that return streaming data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Configuration Management&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Environment Integration&lt;/strong&gt;: API keys and settings via environment variables&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;File-based Config&lt;/strong&gt;: YAML and JSON configuration files&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User Preferences&lt;/strong&gt;: Persistent settings for active providers and models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Validation &amp;amp; Diagnostics&lt;/strong&gt;: Built-in provider health checks and configuration validation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Enhanced User Experience&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Platform Support&lt;/strong&gt;: Windows, macOS, and Linux with platform-specific optimizations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Rich Console Output&lt;/strong&gt;: Colorful, formatted output with automatic fallbacks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Command Completion&lt;/strong&gt;: Context-aware tab completion for all interfaces&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive Help&lt;/strong&gt;: Detailed help system with examples and usage patterns&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Graceful Error Handling&lt;/strong&gt;: User-friendly error messages with troubleshooting hints&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìã Prerequisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python 3.11 or higher&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Keys&lt;/strong&gt; (as needed): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenAI: &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable&lt;/li&gt; &#xA;   &lt;li&gt;Anthropic: &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; environment variable&lt;/li&gt; &#xA;   &lt;li&gt;Custom providers: Provider-specific configuration&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local Services&lt;/strong&gt; (as needed): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ollama: Local installation for Ollama models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP Servers&lt;/strong&gt;: Server configuration file (default: &lt;code&gt;server_config.json&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install from Source&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/chrishayuk/mcp-cli&#xA;cd mcp-cli  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install the package&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e &#34;.[cli,dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Verify installation&lt;/strong&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcp-cli --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using UV (Recommended)&lt;/h3&gt; &#xA;&lt;p&gt;UV provides faster dependency resolution and better environment management:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install UV if not already installed&#xA;pip install uv&#xA;&#xA;# Install dependencies&#xA;uv sync --reinstall&#xA;&#xA;# Run with UV&#xA;uv run mcp-cli --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üß∞ Global Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;Command-line Arguments&lt;/h3&gt; &#xA;&lt;p&gt;Global options available for all modes and commands:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--server&lt;/code&gt;: Specify server(s) to connect to (comma-separated)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--config-file&lt;/code&gt;: Path to server configuration file (default: &lt;code&gt;server_config.json&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--provider&lt;/code&gt;: LLM provider (&lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;anthropic&lt;/code&gt;, &lt;code&gt;ollama&lt;/code&gt;, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: Specific model to use (provider-dependent)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--disable-filesystem&lt;/code&gt;: Disable filesystem access (default: enabled)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--api-base&lt;/code&gt;: Override API endpoint URL&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--api-key&lt;/code&gt;: Override API key&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--verbose&lt;/code&gt;: Enable detailed logging&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--quiet&lt;/code&gt;: Suppress non-essential output&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Environment Variables&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LLM_PROVIDER=openai              # Default provider&#xA;export LLM_MODEL=gpt-4o-mini           # Default model&#xA;export OPENAI_API_KEY=sk-...           # OpenAI API key&#xA;export ANTHROPIC_API_KEY=sk-ant-...    # Anthropic API key&#xA;export MCP_TOOL_TIMEOUT=120            # Tool execution timeout (seconds)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üåê Available Modes&lt;/h2&gt; &#xA;&lt;h3&gt;1. Chat Mode (Default)&lt;/h3&gt; &#xA;&lt;p&gt;Provides a natural language interface with streaming responses and automatic tool usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Default mode (no subcommand needed)&#xA;mcp-cli --server sqlite&#xA;&#xA;# Explicit chat mode&#xA;mcp-cli chat --server sqlite&#xA;&#xA;# With specific provider and model&#xA;mcp-cli chat --server sqlite --provider anthropic --model claude-3-sonnet&#xA;&#xA;# With custom configuration&#xA;mcp-cli chat --server sqlite --provider openai --api-key sk-... --model gpt-4o&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Interactive Mode&lt;/h3&gt; &#xA;&lt;p&gt;Command-driven shell interface for direct server operations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcp-cli interactive --server sqlite&#xA;&#xA;# With provider selection&#xA;mcp-cli interactive --server sqlite --provider ollama --model llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Command Mode&lt;/h3&gt; &#xA;&lt;p&gt;Unix-friendly interface for automation and scripting:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Process text with LLM&#xA;mcp-cli cmd --server sqlite --prompt &#34;Analyze this data&#34; --input data.txt&#xA;&#xA;# Execute tools directly&#xA;mcp-cli cmd --server sqlite --tool list_tables --output tables.json&#xA;&#xA;# Pipeline-friendly processing&#xA;echo &#34;SELECT * FROM users LIMIT 5&#34; | mcp-cli cmd --server sqlite --tool read_query --input -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Direct Commands&lt;/h3&gt; &#xA;&lt;p&gt;Execute individual commands without entering interactive mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# List available tools&#xA;mcp-cli tools --server sqlite&#xA;&#xA;# Show provider configuration&#xA;mcp-cli provider list&#xA;&#xA;# Ping servers&#xA;mcp-cli ping --server sqlite&#xA;&#xA;# List resources&#xA;mcp-cli resources --server sqlite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ñ Using Chat Mode&lt;/h2&gt; &#xA;&lt;p&gt;Chat mode provides the most advanced interface with streaming responses and intelligent tool usage.&lt;/p&gt; &#xA;&lt;h3&gt;Starting Chat Mode&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Simple startup&#xA;mcp-cli --server sqlite&#xA;&#xA;# Multiple servers&#xA;mcp-cli --server sqlite,filesystem&#xA;&#xA;# Specific provider configuration&#xA;mcp-cli --server sqlite --provider anthropic --model claude-3-opus&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chat Commands (Slash Commands)&lt;/h3&gt; &#xA;&lt;h4&gt;Provider &amp;amp; Model Management&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/provider                           # Show current configuration&#xA;/provider list                      # List all providers&#xA;/provider config                    # Show detailed configuration&#xA;/provider diagnostic               # Test provider connectivity&#xA;/provider set openai api_key sk-... # Configure provider settings&#xA;/provider anthropic                # Switch to Anthropic&#xA;/provider openai gpt-4o            # Switch provider and model&#xA;&#xA;/model                             # Show current model&#xA;/model gpt-4o                      # Switch to specific model&#xA;/models                            # List available models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Tool Management&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/tools                             # List available tools&#xA;/tools --all                       # Show detailed tool information&#xA;/tools --raw                       # Show raw JSON definitions&#xA;/tools call                        # Interactive tool execution&#xA;&#xA;/toolhistory                       # Show tool execution history&#xA;/th -n 5                          # Last 5 tool calls&#xA;/th 3                             # Details for call #3&#xA;/th --json                        # Full history as JSON&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Conversation Management&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/conversation                      # Show conversation history&#xA;/ch -n 10                         # Last 10 messages&#xA;/ch 5                             # Details for message #5&#xA;/ch --json                        # Full history as JSON&#xA;&#xA;/save conversation.json            # Save conversation to file&#xA;/compact                          # Summarize conversation&#xA;/clear                            # Clear conversation history&#xA;/cls                              # Clear screen only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Session Control&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/verbose                          # Toggle verbose/compact display&#xA;/interrupt                        # Stop running operations&#xA;/servers                          # List connected servers&#xA;/help                            # Show all commands&#xA;/help tools                       # Help for specific command&#xA;/exit                            # Exit chat mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chat Features&lt;/h3&gt; &#xA;&lt;h4&gt;Streaming Responses&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Real-time text generation with live updates&lt;/li&gt; &#xA; &lt;li&gt;Performance metrics (words/second, response time)&lt;/li&gt; &#xA; &lt;li&gt;Graceful interruption with Ctrl+C&lt;/li&gt; &#xA; &lt;li&gt;Progressive markdown rendering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Tool Execution&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatic tool discovery and usage&lt;/li&gt; &#xA; &lt;li&gt;Concurrent execution with progress indicators&lt;/li&gt; &#xA; &lt;li&gt;Verbose and compact display modes&lt;/li&gt; &#xA; &lt;li&gt;Complete execution history and timing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Provider Integration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Seamless switching between providers&lt;/li&gt; &#xA; &lt;li&gt;Model-specific optimizations&lt;/li&gt; &#xA; &lt;li&gt;API key and endpoint management&lt;/li&gt; &#xA; &lt;li&gt;Health monitoring and diagnostics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üñ•Ô∏è Using Interactive Mode&lt;/h2&gt; &#xA;&lt;p&gt;Interactive mode provides a command shell for direct server interaction.&lt;/p&gt; &#xA;&lt;h3&gt;Starting Interactive Mode&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcp-cli interactive --server sqlite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive Commands&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;help                              # Show available commands&#xA;exit                              # Exit interactive mode&#xA;clear                             # Clear terminal&#xA;&#xA;# Provider management&#xA;provider                          # Show current provider&#xA;provider list                     # List providers&#xA;provider anthropic                # Switch provider&#xA;&#xA;# Tool operations&#xA;tools                             # List tools&#xA;tools --all                       # Detailed tool info&#xA;tools call                        # Interactive tool execution&#xA;&#xA;# Server operations&#xA;servers                           # List servers&#xA;ping                              # Ping all servers&#xA;resources                         # List resources&#xA;prompts                           # List prompts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìÑ Using Command Mode&lt;/h2&gt; &#xA;&lt;p&gt;Command mode provides Unix-friendly automation capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Command Mode Options&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--input FILE                      # Input file (- for stdin)&#xA;--output FILE                     # Output file (- for stdout)&#xA;--prompt TEXT                     # Prompt template&#xA;--tool TOOL                       # Execute specific tool&#xA;--tool-args JSON                  # Tool arguments as JSON&#xA;--system-prompt TEXT              # Custom system prompt&#xA;--raw                             # Raw output without formatting&#xA;--single-turn                     # Disable multi-turn conversation&#xA;--max-turns N                     # Maximum conversation turns&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Text processing&#xA;echo &#34;Analyze this data&#34; | mcp-cli cmd --server sqlite --input - --output analysis.txt&#xA;&#xA;# Tool execution&#xA;mcp-cli cmd --server sqlite --tool list_tables --raw&#xA;&#xA;# Complex queries&#xA;mcp-cli cmd --server sqlite --tool read_query --tool-args &#39;{&#34;query&#34;: &#34;SELECT COUNT(*) FROM users&#34;}&#39;&#xA;&#xA;# Batch processing with GNU Parallel&#xA;ls *.txt | parallel mcp-cli cmd --server sqlite --input {} --output {}.summary --prompt &#34;Summarize: {{input}}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîß Provider Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;Automatic Configuration&lt;/h3&gt; &#xA;&lt;p&gt;The CLI automatically manages provider configurations using the CHUK-LLM library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Configure a provider&#xA;mcp-cli provider set openai api_key sk-your-key-here&#xA;mcp-cli provider set anthropic api_base https://api.anthropic.com&#xA;&#xA;# Test configuration&#xA;mcp-cli provider diagnostic openai&#xA;&#xA;# List available models&#xA;mcp-cli provider list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Manual Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Providers are configured in &lt;code&gt;~/.chuk_llm/providers.yaml&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;openai:&#xA;  api_base: https://api.openai.com/v1&#xA;  default_model: gpt-4o-mini&#xA;&#xA;anthropic:&#xA;  api_base: https://api.anthropic.com&#xA;  default_model: claude-3-sonnet&#xA;&#xA;ollama:&#xA;  api_base: http://localhost:11434&#xA;  default_model: llama3.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;API keys are stored securely in &lt;code&gt;~/.chuk_llm/.env&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=sk-your-key-here&#xA;ANTHROPIC_API_KEY=sk-ant-your-key-here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìÇ Server Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;server_config.json&lt;/code&gt; file with your MCP server configurations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;sqlite&#34;: {&#xA;      &#34;command&#34;: &#34;python&#34;,&#xA;      &#34;args&#34;: [&#34;-m&#34;, &#34;mcp_server.sqlite_server&#34;],&#xA;      &#34;env&#34;: {&#xA;        &#34;DATABASE_PATH&#34;: &#34;database.db&#34;&#xA;      }&#xA;    },&#xA;    &#34;filesystem&#34;: {&#xA;      &#34;command&#34;: &#34;npx&#34;,&#xA;      &#34;args&#34;: [&#34;-y&#34;, &#34;@modelcontextprotocol/server-filesystem&#34;, &#34;/path/to/allowed/files&#34;],&#xA;      &#34;env&#34;: {}&#xA;    },&#xA;    &#34;brave-search&#34;: {&#xA;      &#34;command&#34;: &#34;npx&#34;,&#xA;      &#34;args&#34;: [&#34;-y&#34;, &#34;@modelcontextprotocol/server-brave-search&#34;],&#xA;      &#34;env&#34;: {&#xA;        &#34;BRAVE_API_KEY&#34;: &#34;your-brave-api-key&#34;&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìà Advanced Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Multi-Provider Workflow&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Start with OpenAI&#xA;mcp-cli chat --server sqlite --provider openai --model gpt-4o&#xA;&#xA;# In chat, switch to Anthropic for reasoning tasks&#xA;&amp;gt; /provider anthropic claude-3-opus&#xA;&#xA;# Switch to Ollama for local processing&#xA;&amp;gt; /provider ollama llama3.2&#xA;&#xA;# Compare responses across providers&#xA;&amp;gt; /provider openai&#xA;&amp;gt; What&#39;s the capital of France?&#xA;&amp;gt; /provider anthropic  &#xA;&amp;gt; What&#39;s the capital of France?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Complex Tool Workflows&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Database analysis workflow&#xA;&amp;gt; List all tables in the database&#xA;[Tool: list_tables] ‚Üí products, customers, orders&#xA;&#xA;&amp;gt; Show me the schema for the products table&#xA;[Tool: describe_table] ‚Üí id, name, price, category, stock&#xA;&#xA;&amp;gt; Find the top 10 most expensive products&#xA;[Tool: read_query] ‚Üí SELECT name, price FROM products ORDER BY price DESC LIMIT 10&#xA;&#xA;&amp;gt; Export this data to a CSV file&#xA;[Tool: write_file] ‚Üí Saved to expensive_products.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Automation and Scripting&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Batch data processing&#xA;for file in data/*.csv; do&#xA;  mcp-cli cmd --server sqlite \&#xA;    --tool analyze_data \&#xA;    --tool-args &#34;{\&#34;file_path\&#34;: \&#34;$file\&#34;}&#34; \&#xA;    --output &#34;results/$(basename &#34;$file&#34; .csv)_analysis.json&#34;&#xA;done&#xA;&#xA;# Pipeline processing&#xA;cat input.txt | \&#xA;  mcp-cli cmd --server sqlite --prompt &#34;Extract key entities&#34; --input - | \&#xA;  mcp-cli cmd --server sqlite --prompt &#34;Categorize these entities&#34; --input - &amp;gt; output.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Performance Monitoring&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Enable verbose mode for detailed timing&#xA;&amp;gt; /verbose&#xA;&#xA;# Monitor tool execution times&#xA;&amp;gt; /toolhistory&#xA;Tool Call History (15 calls)&#xA;#  | Tool        | Arguments                    | Time&#xA;1  | list_tables | {}                          | 0.12s&#xA;2  | read_query  | {&#34;query&#34;: &#34;SELECT...&#34;}      | 0.45s&#xA;...&#xA;&#xA;# Check provider performance&#xA;&amp;gt; /provider diagnostic&#xA;Provider Diagnostics&#xA;Provider   | Status      | Response Time | Features&#xA;openai     | ‚úÖ Ready    | 234ms        | üì°üîßüëÅÔ∏è&#xA;anthropic  | ‚úÖ Ready    | 187ms        | üì°üîß&#xA;ollama     | ‚úÖ Ready    | 56ms         | üì°üîß&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîç Troubleshooting&lt;/h2&gt; &#xA;&lt;h3&gt;Common Issues&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&#34;Missing argument &#39;KWARGS&#39;&#34; error&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Use equals sign format&#xA;mcp-cli chat --server=sqlite --provider=openai&#xA;&#xA;# Or add double dash&#xA;mcp-cli chat -- --server sqlite --provider openai&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Provider not found&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcp-cli provider diagnostic&#xA;mcp-cli provider set &amp;lt;provider&amp;gt; api_key &amp;lt;your-key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool execution timeout&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MCP_TOOL_TIMEOUT=300  # 5 minutes&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connection issues&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcp-cli ping --server &amp;lt;server-name&amp;gt;&#xA;mcp-cli servers&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Debug Mode&lt;/h3&gt; &#xA;&lt;p&gt;Enable verbose logging for troubleshooting:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcp-cli --verbose chat --server sqlite&#xA;mcp-cli --log-level DEBUG interactive --server sqlite&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üîí Security Considerations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Keys&lt;/strong&gt;: Stored securely in environment variables or protected files&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;File Access&lt;/strong&gt;: Filesystem access can be disabled with &lt;code&gt;--disable-filesystem&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tool Validation&lt;/strong&gt;: All tool calls are validated before execution&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Timeout Protection&lt;/strong&gt;: Configurable timeouts prevent hanging operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Server Isolation&lt;/strong&gt;: Each server runs in its own process&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Performance Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Concurrent Tool Execution&lt;/strong&gt;: Multiple tools can run simultaneously&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streaming Responses&lt;/strong&gt;: Real-time response generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connection Pooling&lt;/strong&gt;: Efficient reuse of client connections&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt;: Tool metadata and provider configurations are cached&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Async Architecture&lt;/strong&gt;: Non-blocking operations throughout&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì¶ Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;Core dependencies are organized into feature groups:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;cli&lt;/strong&gt;: Rich terminal UI, command completion, provider integrations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;dev&lt;/strong&gt;: Development tools, testing utilities, linting&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;chuk-tool-processor&lt;/strong&gt;: Core tool execution and MCP communication&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;chuk-llm&lt;/strong&gt;: Unified LLM provider management&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Install with specific features:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;mcp-cli[cli]&#34;        # Basic CLI features&#xA;pip install &#34;mcp-cli[cli,dev]&#34;    # CLI with development tools&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Please see our &lt;a href=&#34;https://raw.githubusercontent.com/chrishayuk/mcp-cli/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;Development Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/chrishayuk/mcp-cli&#xA;cd mcp-cli&#xA;pip install -e &#34;.[cli,dev]&#34;&#xA;pre-commit install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Tests&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pytest&#xA;pytest --cov=mcp_cli --cov-report=html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/chrishayuk/mcp-cli/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/chrishayuk/chuk-tool-processor&#34;&gt;CHUK Tool Processor&lt;/a&gt;&lt;/strong&gt; - Async-native tool execution&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/chrishayuk/chuk-llm&#34;&gt;CHUK-LLM&lt;/a&gt;&lt;/strong&gt; - Unified LLM provider management&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Textualize/rich&#34;&gt;Rich&lt;/a&gt;&lt;/strong&gt; - Beautiful terminal formatting&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://typer.tiangolo.com/&#34;&gt;Typer&lt;/a&gt;&lt;/strong&gt; - CLI framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/prompt-toolkit/python-prompt-toolkit&#34;&gt;Prompt Toolkit&lt;/a&gt;&lt;/strong&gt; - Interactive input&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîó Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://modelcontextprotocol.io/&#34;&gt;Model Context Protocol&lt;/a&gt;&lt;/strong&gt; - Core protocol specification&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/modelcontextprotocol/servers&#34;&gt;MCP Servers&lt;/a&gt;&lt;/strong&gt; - Official MCP server implementations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/chrishayuk/chuk-tool-processor&#34;&gt;CHUK Tool Processor&lt;/a&gt;&lt;/strong&gt; - Tool execution engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/chrishayuk/chuk-llm&#34;&gt;CHUK-LLM&lt;/a&gt;&lt;/strong&gt; - LLM provider abstraction&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>megadose/toutatis</title>
    <updated>2025-07-05T01:34:40Z</updated>
    <id>tag:github.com,2025-07-05:/megadose/toutatis</id>
    <link href="https://github.com/megadose/toutatis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Toutatis&lt;/h1&gt; &#xA;&lt;p&gt;üëã Hi there! For any professional inquiries or collaborations, please reach out to me at: &lt;a href=&#34;mailto:megadose@protonmail.com&#34;&gt;megadose@protonmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üìß Preferably, use your professional email for correspondence. Let&#39;s keep it short and sweet, and all in English!&lt;/p&gt; &#xA;&lt;p&gt;Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more &lt;br&gt; For BTC Donations : 1FHDM49QfZX6pJmhjLE5tB2K6CaTLMZpXZ&lt;/p&gt; &#xA;&lt;h2&gt;üí° Prerequisite&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-370/&#34;&gt;Python 3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; &#xA;&lt;h3&gt;With PyPI&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install toutatis&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;With Github&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/megadose/toutatis.git&#xA;cd toutatis/&#xA;python3 setup.py install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìö Usage:&lt;/h2&gt; &#xA;&lt;h3&gt;Find information from a username&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;toutatis -u username -s instagramsessionid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Find information from an Instagram ID&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;toutatis -i instagramID -s instagramsessionid&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìà Example&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;Informations about     : xxxusernamexxx&#xA;Full Name              : xxxusernamesxx | userID : 123456789&#xA;Verified               : False | Is buisness Account : False&#xA;Is private Account     : False&#xA;Follower               : xxx | Following : xxx&#xA;Number of posts        : x&#xA;Number of tag in posts : x&#xA;External url           : http://example.com&#xA;IGTV posts             : x&#xA;Biography              : example biography&#xA;Public Email           : public@example.com&#xA;Public Phone           : +00 0 00 00 00 00&#xA;Obfuscated email       : me********s@examplemail.com&#xA;Obfuscated phone       : +00 0xx xxx xx 00&#xA;------------------------&#xA;Profile Picture        : https://scontent-X-X.cdninstagram.com/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìö To retrieve the sessionID&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://files.catbox.moe/1rfi6j.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Thank you to :&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/eyupergin&#34;&gt;EyupErgin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yazeed44&#34;&gt;yazeed44&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>