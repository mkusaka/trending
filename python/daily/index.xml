<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-27T01:29:17Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>apple/corenet</title>
    <updated>2024-04-27T01:29:17Z</updated>
    <id>tag:github.com,2024-04-27:/apple/corenet</id>
    <link href="https://github.com/apple/corenet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CoreNet: A library for training deep neural networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CoreNet: A library for training deep neural networks&lt;/h1&gt; &#xA;&lt;p&gt;CoreNet is a deep neural network toolkit that allows researchers and engineers to train standard and novel small and large-scale models for variety of tasks, including foundation models (e.g., CLIP and LLM), object classification, object detection, and semantic segmentation.&lt;/p&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#whats-new&#34;&gt;What&#39;s new?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#research-efforts-at-apple-using-corenet&#34;&gt;Research efforts at Apple using CoreNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#directory-structure&#34;&gt;Directory Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#maintainers&#34;&gt;Maintainers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#contributing-to-corenet&#34;&gt;Contributing to CoreNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#relationship-with-cvnets&#34;&gt;Relationship with CVNets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s new?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;&lt;strong&gt;April 2024&lt;/strong&gt;&lt;/em&gt;: Version 0.1.0 of the CoreNet library includes &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenELM&lt;/li&gt; &#xA;   &lt;li&gt;CatLIP&lt;/li&gt; &#xA;   &lt;li&gt;MLX examples&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Research efforts at Apple using CoreNet&lt;/h2&gt; &#xA;&lt;p&gt;Below is the list of publications from Apple that uses CoreNet. Also, training and evaluation recipes, as well as links to pre-trained models, can be found inside the &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/&#34;&gt;projects&lt;/a&gt; folder. Please refer to it for further details.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.14619&#34;&gt;OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.15653&#34;&gt;CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.08983&#34;&gt;Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.14108&#34;&gt;CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.14189&#34;&gt;FastVit: A Fast Hybrid Vision Transformer using Structural Reparameterization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.00238&#34;&gt;Bytes Are All You Need: Transformers Operating Directly on File Bytes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.04040&#34;&gt;MobileOne: An Improved One millisecond Mobile Backbone&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.10553&#34;&gt;RangeAugment: Efficient Online Augmentation with Range Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02680&#34;&gt;Separable Self-attention for Mobile Vision Transformers (MobileViTv2)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.02002&#34;&gt;CVNets: High performance library for Computer Vision, ACM MM&#39;22&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.02178&#34;&gt;MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer, ICLR&#39;22&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You will need Git LFS (instructions below) to run tests and Jupyter notebooks (&lt;a href=&#34;https://jupyter.org/install&#34;&gt;instructions&lt;/a&gt;) in this repository, and to contribute to it so we recommend that you install and activate it first.&lt;/p&gt; &#xA;&lt;p&gt;On Linux we recommend to use Python 3.10+ and PyTorch (version &amp;gt;= v2.1.0), on macOS system Python 3.9+ should be sufficient.&lt;/p&gt; &#xA;&lt;p&gt;Note that the optional dependencies listed below are required if you&#39;d like to make contributions and/or run tests.&lt;/p&gt; &#xA;&lt;p&gt;For Linux (substitute &lt;code&gt;apt&lt;/code&gt; for your package manager):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install git-lfs&#xA;&#xA;git clone git@github.com:apple/corenet.git&#xA;cd corenet&#xA;git lfs install&#xA;git lfs pull&#xA;# The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.&#xA;python3 -m venv venv &amp;amp;&amp;amp; source venv/bin/activate&#xA;python3 -m pip install --editable .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install optional dependencies for audio and video processing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install libsox-dev ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For macOS, assuming you use Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install git-lfs&#xA;&#xA;git clone git@github.com:apple/corenet.git&#xA;cd corenet&#xA;cd \$(pwd -P)  # See the note below.&#xA;git lfs install&#xA;git lfs pull&#xA;# The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.&#xA;python3 -m venv venv &amp;amp;&amp;amp; source venv/bin/activate&#xA;python3 -m pip install --editable .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install optional dependencies for audio and video processing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install sox ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that on macOS the file system is case insensitive, and case sensitivity can cause issues with Git. You should access the repository on disk as if the path were case sensitive, i.e. with the same capitalization as you see when you list the directories &lt;code&gt;ls&lt;/code&gt;. You can switch to such a path with the &lt;code&gt;cd $(pwd -P)&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;h2&gt;Directory Structure&lt;/h2&gt; &#xA;&lt;p&gt;This section provides quick access and a brief description for important CoreNet directories.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt; Description &lt;/th&gt; &#xA;   &lt;th&gt; Quick Access &lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;!-- Row boilerplate (copy-paste the following commented snippet for adding a new row to the table.)&#xA;&lt;tr&gt; &lt;td&gt; &lt;h3&gt; title &lt;/h3&gt; &#xA;description&#xA;&lt;/td&gt; &lt;td&gt; &lt;pre&gt;&#xA;folders&#xA;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt;&#xA;--&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h3&gt; Getting Started &lt;/h3&gt; Working with the examples is an easy way to get started with CoreNet. &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&#xA;└── tutorials&#xA; &amp;nbsp;  ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/tutorials/train_a_new_model_on_a_new_dataset_from_scratch.ipynb&#34;&gt;train_a_new_model_on_a_new_dataset_from_scratch.ipynb&lt;/a&gt;&#xA; &amp;nbsp;  ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/tutorials/guide_slurm_and_multi_node_training.md&#34;&gt;guide_slurm_and_multi_node_training.md&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/tutorials/clip.ipynb&#34;&gt;clip.ipynb&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/tutorials/semantic_segmentation.ipynb&#34;&gt;semantic_segmentation.ipynb&lt;/a&gt;&#xA;    └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/tutorials/object_detection.ipynb&#34;&gt;object_detection.ipynb&lt;/a&gt;&#xA;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h3&gt; Training Recipes &lt;/h3&gt; CoreNet provides reproducible training recipes, in addition to the pretrained model weights and checkpoints for the publications that are listed in &lt;code&gt;projects/&lt;/code&gt; directory. &lt;p&gt;Publication project directories generally contain the following contents:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;README.md&lt;/code&gt; provides documentation, links to the pretrained weights, and citations.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;&amp;lt;task_name&amp;gt;/&amp;lt;model_name&amp;gt;.yaml&lt;/code&gt; provides configuration for reproducing the trainings and evaluations.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&#xA;└── projects&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/byteformer&#34;&gt;byteformer&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/catlip&#34;&gt;catlip&lt;/a&gt; (*)&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/clip&#34;&gt;clip&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/fastvit&#34;&gt;fastvit&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/mobilenet_v1&#34;&gt;mobilenet_v1&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/mobilenet_v2&#34;&gt;mobilenet_v2&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/mobilenet_v3&#34;&gt;mobilenet_v3&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/mobileone&#34;&gt;mobileone&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/mobilevit&#34;&gt;mobilevit&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/mobilevit_v2&#34;&gt;mobilevit_v2&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/openelm&#34;&gt;openelm&lt;/a&gt; (*)&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/range_augment&#34;&gt;range_augment&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/resnet&#34;&gt;resnet&lt;/a&gt;&#xA;    └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/projects/vit&#34;&gt;vit&lt;/a&gt;&#xA;&lt;br&gt;&#xA;(*) Newly released.&#xA;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h3&gt; MLX Examples &lt;/h3&gt; MLX examples demonstrate how to run CoreNet models efficiently on Apple Silicon. Please find further information in the &lt;code&gt;README.md&lt;/code&gt; file within the corresponding example directory. &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&#xA;└──mlx_example&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/mlx_examples/clip&#34;&gt;clip&lt;/a&gt;&#xA;    └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/mlx_examples/open_elm&#34;&gt;open_elm&lt;/a&gt;&#xA;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h3&gt; Model Implementations &lt;/h3&gt; Models are organized by tasks (e.g. &#34;classification&#34;). You can find all model implementations for each task in the corresponding task folder. &lt;p&gt;Each model class is decorated by a &lt;code&gt;@MODEL_REGISTRY.register(name=&#34;&amp;lt;model_name&amp;gt;&#34;, type=&#34;&amp;lt;task_name&amp;gt;&#34;)&lt;/code&gt; decorator. To use a model class in CoreNet training or evaluation, assign &lt;code&gt;moels.&amp;lt;task_name&amp;gt;.name = &amp;lt;model_name&amp;gt;&lt;/code&gt; in the YAML configuration.&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&#xA;└── corenet&#xA;    └── modeling&#xA;        └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/models&#34;&gt;models&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/models/audio_classification&#34;&gt;audio_classification&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/models/classification&#34;&gt;classification&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/models/detection&#34;&gt;detection&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/models/language_modeling&#34;&gt;language_modeling&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/models/multi_modal_img_text&#34;&gt;multi_modal_img_text&lt;/a&gt;&#xA;            └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/models/segmentation&#34;&gt;segmentation&lt;/a&gt;&#xA;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h3&gt; Datasets &lt;/h3&gt; Similarly to the models, datasets are also categorized by tasks. &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&#xA;└── corenet&#xA;    └── data&#xA;        └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/datasets&#34;&gt;datasets&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/datasets/audio_classification&#34;&gt;audio_classification&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/datasets/classification&#34;&gt;classification&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/datasets/detection&#34;&gt;detection&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/datasets/language_modeling&#34;&gt;language_modeling&lt;/a&gt;&#xA;            ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/datasets/multi_modal_img_text&#34;&gt;multi_modal_img_text&lt;/a&gt;&#xA;            └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/datasets/segmentation&#34;&gt;segmentation&lt;/a&gt;&#xA;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h3&gt; Other key directories &lt;/h3&gt; In this section, we have highlighted the rest of the key directories that implement classes corresponding to the names that are referenced in the YAML configurations. &lt;/td&gt; &#xA;   &lt;td&gt; &lt;pre&gt;&#xA;└── corenet&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/loss_fn&#34;&gt;loss_fn&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/metrics&#34;&gt;metrics&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/optims&#34;&gt;optims&lt;/a&gt;&#xA;    │&amp;nbsp;&amp;nbsp; └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/optims/scheduler&#34;&gt;scheduler&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/train_eval_pipelines&#34;&gt;train_eval_pipelines&lt;/a&gt;&#xA;    ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data&#34;&gt;data&lt;/a&gt;&#xA;    │&amp;nbsp;&amp;nbsp; ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/collate_fns&#34;&gt;collate_fns&lt;/a&gt;&#xA;    │&amp;nbsp;&amp;nbsp; ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/sampler&#34;&gt;sampler&lt;/a&gt;&#xA;    │&amp;nbsp;&amp;nbsp; ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/text_tokenizer&#34;&gt;text_tokenizer&lt;/a&gt;&#xA;    │&amp;nbsp;&amp;nbsp; ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/transforms&#34;&gt;transforms&lt;/a&gt;&#xA;    │&amp;nbsp;&amp;nbsp; └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/data/video_reader&#34;&gt;video_reader&lt;/a&gt;&#xA;    └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling&#34;&gt;modeling&lt;/a&gt;&#xA;        ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/layers&#34;&gt;layers&lt;/a&gt;&#xA;        ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/modules&#34;&gt;modules&lt;/a&gt;&#xA;        ├── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/neural_augmentor&#34;&gt;neural_augmentor&lt;/a&gt;&#xA;        └── &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/corenet/modeling/text_encoders&#34;&gt;text_encoders&lt;/a&gt;&#xA;&lt;/pre&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Maintainers&lt;/h2&gt; &#xA;&lt;p&gt;This code is developed by &lt;a href=&#34;https://sacmehta.github.io&#34; target=&#34;_blank&#34;&gt;Sachin&lt;/a&gt;, and is now maintained by Sachin, &lt;a href=&#34;https://mchorton.com&#34; target=&#34;_blank&#34;&gt;Maxwell Horton&lt;/a&gt;, &lt;a href=&#34;https://www.mohammad.pro&#34; target=&#34;_blank&#34;&gt;Mohammad Sekhavat&lt;/a&gt;, and Yanzi Jin.&lt;/p&gt; &#xA;&lt;h3&gt;Previous Maintainers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://farzadab.github.io&#34; target=&#34;_blank&#34;&gt;Farzad&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing to CoreNet&lt;/h2&gt; &#xA;&lt;p&gt;We welcome PRs from the community! You can find information about contributing to CoreNet in our &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; document.&lt;/p&gt; &#xA;&lt;p&gt;Please remember to follow our &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;For license details, see &lt;a href=&#34;https://raw.githubusercontent.com/apple/corenet/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Relationship with CVNets&lt;/h2&gt; &#xA;&lt;p&gt;CoreNet evolved from CVNets, to encompass a broader range of applications beyond computer vision. Its expansion facilitated the training of foundational models, including LLMs.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful, please cite the following paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{mehta2022cvnets, &#xA;     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, &#xA;     title = {CVNets: High Performance Library for Computer Vision}, &#xA;     year = {2022}, &#xA;     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, &#xA;     series = {MM &#39;22} &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>CyanVoxel/TagStudio</title>
    <updated>2024-04-27T01:29:17Z</updated>
    <id>tag:github.com,2024-04-27:/CyanVoxel/TagStudio</id>
    <link href="https://github.com/CyanVoxel/TagStudio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A file and photo management application and system.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TagStudio (Preview/Alpha): A User-Focused Document Management System&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;60%&#34; src=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/github_header.png&#34;&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] This is still a &lt;strong&gt;&lt;em&gt;very&lt;/em&gt;&lt;/strong&gt; rough personal project of mine in its infancy. I’m open-sourcing it now in order to accept contributors sooner and to better facilitate the direction of the project from an earlier stage. There &lt;strong&gt;&lt;em&gt;are&lt;/em&gt;&lt;/strong&gt; bugs, and there will &lt;strong&gt;&lt;em&gt;very likely&lt;/em&gt;&lt;/strong&gt; be breaking changes!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;TagStudio is a photo &amp;amp; file organization application with an underlying system that focuses on giving freedom and flexibility to the user. No proprietary programs or formats, no sea of sidecar files, and no complete upheaval of your filesystem structure.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;80%&#34; src=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/screenshot.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#goals&#34;&gt;Goals&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#priorities&#34;&gt;Priorities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#current-features&#34;&gt;Current Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#faq&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Goals&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To achieve a portable, privacy-oriented, open, extensible, and feature-rich system of organizing and rediscovering files.&lt;/li&gt; &#xA; &lt;li&gt;To provide powerful methods for organization, notably the concept of tag composition, or “taggable tags”.&lt;/li&gt; &#xA; &lt;li&gt;To create an implementation of such a system that is resilient against a user’s actions outside the program (modifying, moving, or renaming files) while also not burdening the user with mandatory sidecar files or otherwise requiring them to change their existing file structures and workflows.&lt;/li&gt; &#xA; &lt;li&gt;To support a wide range of users spanning across different platforms, multi-user setups, and those with large (several terabyte) libraries.&lt;/li&gt; &#xA; &lt;li&gt;To make the darn thing look like nice, too. It’s 2024, not 1994.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Priorities&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;The concept.&lt;/strong&gt; Even if TagStudio as a project or application fails, I’d hope that the idea lives on in a superior project. The &lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#goals&#34;&gt;goals&lt;/a&gt; outlined above don’t reference TagStudio once - &lt;em&gt;TagStudio&lt;/em&gt; is what references the &lt;em&gt;goals.&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The system.&lt;/strong&gt; Frontends and implementations can vary, as they should. The core underlying metadata management system is what should be interoperable between different frontends, programs, and operating systems. A standard implementation for this should settle as development continues. This opens up the doors for improved and varied clients, integration with third-party applications, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The application.&lt;/strong&gt; If nothing else, TagStudio the application serves as the first (and so far only) implementation for this system of metadata management. This has the responsibility of doing the idea justice and showing just what’s possible when it comes to user file management.&lt;/li&gt; &#xA; &lt;li&gt;(The name.) I think it’s fine for an app or client, but it doesn’t really make sense for a system or standard. I suppose this will evolve with time.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Current Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create libraries/vaults centered around a system directory. Libraries contain a series of entries: the representations of your files combined with metadata fields. Each entry represents a file in your library’s directory, and is linked to its location.&lt;/li&gt; &#xA; &lt;li&gt;Add metadata to your library entries, including: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Name, Author, Artist (Single-Line Text Fields)&lt;/li&gt; &#xA;   &lt;li&gt;Description, Notes (Multiline Text Fields)&lt;/li&gt; &#xA;   &lt;li&gt;Tags, Meta Tags, Content Tags (Tag Boxes)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create rich tags composed of a name, a list of aliases, and a list of “subtags” - being tags in which these tags inherit values from.&lt;/li&gt; &#xA; &lt;li&gt;Search for entries based on tags, metadata, or filename (using &lt;code&gt;filename: &amp;lt;query&amp;gt;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Special search conditions for entries that are: &lt;code&gt;untagged&lt;/code&gt;/&lt;code&gt;no tags&lt;/code&gt; and &lt;code&gt;empty&lt;/code&gt;/&lt;code&gt;no fields&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] For more information on the project itself, please see the &lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#faq&#34;&gt;FAQ&lt;/a&gt; section and other docs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] TagStudio is only currently verified to work on Windows. I&#39;ve run into issues with the Qt code running on Linux, but I don&#39;t know how severe these issues are. There&#39;s also likely bugs regarding filenames and portability of the databases across different OSes.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.12&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Creating the Virtual Environment&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Skip this step if launching from the .sh script on Linux.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In the root repository directory, create a python virtual environment:&lt;br&gt; &lt;code&gt;python3 -m venv .venv&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Activate your environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows w/Powershell: &lt;code&gt;.venv\Scripts\Activate.ps1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows w/Command Prompt: &lt;code&gt;.venv\Scripts\activate.bat&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Linux/macOS: &lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the required packages:&lt;br&gt; &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;Learn more about setting up a virtual environment &lt;a href=&#34;https://docs.python.org/3/tutorial/venv.html&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Launching&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Depending on your system, Python may be called &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;py&lt;/code&gt;, &lt;code&gt;python3&lt;/code&gt;, or &lt;code&gt;py3&lt;/code&gt;. These instructions use the alias &lt;code&gt;python3&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Optional Arguments&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;--open &amp;lt;path&amp;gt;&lt;/code&gt; / &lt;code&gt;-o &amp;lt;path&amp;gt;&lt;/code&gt; Path to a TagStudio Library folder to open on start.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;p&gt;To launch TagStudio, launch the &lt;code&gt;start_win.bat&lt;/code&gt; file. You can modify this .bat file or create a shortcut and add one or more additional arguments if desired.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, with the virtual environment loaded, run the python file at &lt;code&gt;tagstudio\tag_studio.py&lt;/code&gt; from your terminal. If you&#39;re in the project&#39;s root directory, simply run &lt;code&gt;python3 tagstudio/tag_studio.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] TagStudio on Linux &amp;amp; macOS likely won&#39;t function correctly at this time. If you&#39;re trying to run this in order to help test, debug, and improve compatibility, then charge on ahead!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;macOS&lt;/h4&gt; &#xA;&lt;p&gt;With the virtual environment loaded, run the python file at &#34;tagstudio/tag_studio.py&#34; from your terminal. If you&#39;re in the project&#39;s root directory, simply run &lt;code&gt;python3 tagstudio/tag_studio.py&lt;/code&gt;. When launching the program in the future, remember to activate the virtual environment each time before launching &lt;em&gt;(an easier method is currently being worked on).&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Linux&lt;/h4&gt; &#xA;&lt;p&gt;Run the &#34;TagStudio.sh&#34; script, and the program should launch! (Make sure that the script is marked as executable). Note that launching from the script from outside of a terminal will not launch a terminal window with any debug or crash information. If you wish to see this information, just launch the shell script directly from your terminal with &lt;code&gt;sh TagStudio.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;NixOS&lt;/h5&gt; &#xA;&lt;p&gt;Use the provided &lt;code&gt;flake.nix&lt;/code&gt; file to create and enter a working environment by running &lt;code&gt;nix develop&lt;/code&gt;. Then, run the above &lt;code&gt;TagStudio.sh&lt;/code&gt; script.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Creating/Opening a Library&lt;/h3&gt; &#xA;&lt;p&gt;With TagStudio opened, start by creating a new library or opening an existing one using File -&amp;gt; Open/Create Library from the menu bar. TagStudio will automatically create a new library from the chosen directory if one does not already exist. Upon creating a new library, TagStudio will automatically scan your folders for files and add those to your library (no files are moved during this process!).&lt;/p&gt; &#xA;&lt;h3&gt;Refreshing the Library&lt;/h3&gt; &#xA;&lt;p&gt;In order to scan for new files or file changes, you’ll need to manually go to File -&amp;gt; Refresh Directories.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] In the future, library refreshing will also be automatically done in the background, or additionally on app startup.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Adding Metadata to Entries&lt;/h3&gt; &#xA;&lt;p&gt;To add a metadata field to a file entry, start by clicking the “Add Field” button under the file preview in the right-hand preview panel. From the dropdown menu, select the type of metadata field you’d like to add to the entry.&lt;/p&gt; &#xA;&lt;h3&gt;Editing Metadata Fields&lt;/h3&gt; &#xA;&lt;h4&gt;Text Line / Text Box&lt;/h4&gt; &#xA;&lt;p&gt;Hover over the field and click the pencil icon. From there, add or edit text in the dialog box popup.&lt;/p&gt; &#xA;&lt;h4&gt;Tag Box&lt;/h4&gt; &#xA;&lt;p&gt;Click the “+” button at the end of the Tags list, and search for tags to add inside the new dialog popup. Click the “+” button next to whichever tags you want to add. Alternatively, after you search for a tag, press the Enter/Return key to add the add the first item in the list. Press Enter/Return once more to close the dialog box&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Keyboard control and navigation is currently &lt;em&gt;very&lt;/em&gt; buggy, but will be improved in future versions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Creating Tags&lt;/h3&gt; &#xA;&lt;p&gt;To create a new tag, click on Edit -&amp;gt; New Tag from the menu bar. From there, enter a tag name, shorthand name, any tag aliases separated by newlines, any subtags, and an optional color.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The tag &lt;strong&gt;shorthand&lt;/strong&gt; is a type of alias that displays in situations when screen space is more valuable (ex. as a subtag for other tags).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Aliases&lt;/strong&gt; are alternate names for a tag. These let you search for terms other than the exact tag name in order to find the tag again.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Subtags&lt;/strong&gt; are tags in which this tag is a child tag of. In other words, tags under this section are parents of this tag. For example, if you had a tag for a character from a show, you would make the show a subtag of this character. This would display as “Character (Show)” in most areas of the app. The first tag in this list is used as the tag shown in parentheses for specification.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;strong&gt;color&lt;/strong&gt; dropdown lets you select an optional color for this tag to display as.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Editing Tags&lt;/h3&gt; &#xA;&lt;p&gt;To edit a tag, right-click the tag in the tag field of the preview pane and select “Edit Tag”&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] There is currently no method to view all tags that you’ve created in your library. This is a top priority for future releases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Relinking Renamed/Moved Files&lt;/h3&gt; &#xA;&lt;p&gt;Inevitably, some of the files inside your library will be renamed, moved, or deleted. If a file has been renamed or moved, TagStudio will display the thumbnail as a red tag with a cross through it &lt;em&gt;(this icon is also used for items with broken thumbnails).&lt;/em&gt; To relink moved files or delete these entries, go to Tools -&amp;gt; Manage Unlinked Entries. Click the “Refresh” button to scan your library for unlinked entries. Once complete, you can attempt to “Search &amp;amp; Relink” any unlinked entries to their respective files, or “Delete Unlinked Entries” in the event the original files have been deleted and you no longer wish to keep their metadata entries inside your library.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] There is currently no method to relink entries to files that have been renamed - only moved or deleted. This is a top priority for future releases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] If multiple matches for a moved file are found (matches are currently defined as files with a matching filename as the original), TagStudio will currently ignore the match groups. Adding a GUI for manual selection, as well as smarter automated relinking, are top priorities for future versions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Saving the Library&lt;/h3&gt; &#xA;&lt;p&gt;Libraries are saved upon exiting the program. To manually save, select File -&amp;gt; Save Library from the menu bar. To save a backup of your library, select File -&amp;gt; Save Library Backup from the menu bar.&lt;/p&gt; &#xA;&lt;h3&gt;Half-Implemented Features&lt;/h3&gt; &#xA;&lt;h4&gt;Fix Duplicate Files&lt;/h4&gt; &#xA;&lt;p&gt;Load in a .dupeguru file generated by &lt;a href=&#34;https://github.com/arsenetar/dupeguru/&#34;&gt;dupeGuru&lt;/a&gt; and mirror metadata across entries marked as duplicates. After mirroring, return to dupeGuru to manage deletion of the duplicate files. After deletion, use the “Fix Unlinked Entries” feature in TagStudio to delete the duplicate set of entries for the now-deleted files&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] While this feature is functional, it’s a pretty roundabout process and can be streamlined in the future.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Image Collage&lt;/h4&gt; &#xA;&lt;p&gt;Create an image collage of your photos and videos.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] Collage sizes and options are hardcoded.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Macros&lt;/h4&gt; &#xA;&lt;p&gt;Apply tags and other metadata automatically depending on certain criteria. Set specific macros to run when the files are added to the library. Part of this includes applying tags automatically based on parent folders.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] Macro options are hardcoded, and there’s currently no way for the user to interface with this (still incomplete) system at all.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Gallery-dl Sidecar Importing&lt;/h4&gt; &#xA;&lt;p&gt;Import JSON sidecar data generated by &lt;a href=&#34;https://github.com/mikf/gallery-dl&#34;&gt;gallery-dl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] This feature is not supported or documented in any official capacity whatsoever. It will likely be rolled-in to a larger and more generalized sidecar importing feature in the future.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;What State Is the Project Currently In?&lt;/h3&gt; &#xA;&lt;p&gt;As of writing (Alpha v9.1.0) the project is in a “useable” state, however it lacks proper testing and quality of life features. Currently the program has only been verified to work properly on Windows, and is unlikely to properly run on Linux or macOS in its current state, however this functionality is a priority going forward with testers.&lt;/p&gt; &#xA;&lt;h3&gt;What Features Are You Planning on Adding?&lt;/h3&gt; &#xA;&lt;p&gt;Of the several features I have planned for the project, these are broken up into “priority” features and “future” features. Priority features were originally intended for the first public release, however are currently absent from the Alpha v9.x.x builds.&lt;/p&gt; &#xA;&lt;h4&gt;Priority Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improved search &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sortable Search&lt;/li&gt; &#xA;   &lt;li&gt;Boolean Search&lt;/li&gt; &#xA;   &lt;li&gt;Coexisting Text + Tag Search&lt;/li&gt; &#xA;   &lt;li&gt;Searchable File Metadata&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Tag management view&lt;/li&gt; &#xA; &lt;li&gt;Applying metadata via multi-selection&lt;/li&gt; &#xA; &lt;li&gt;Easier ways to apply tags in bulk &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tag Search Panel&lt;/li&gt; &#xA;   &lt;li&gt;Recent Tags Panel&lt;/li&gt; &#xA;   &lt;li&gt;Top Tags Panel&lt;/li&gt; &#xA;   &lt;li&gt;Pinned Tags Panel&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Apply tags based on system folders&lt;/li&gt; &#xA; &lt;li&gt;Better (stable, performant) library grid view&lt;/li&gt; &#xA; &lt;li&gt;Improved entry relinking&lt;/li&gt; &#xA; &lt;li&gt;Cached thumbnails&lt;/li&gt; &#xA; &lt;li&gt;Collations&lt;/li&gt; &#xA; &lt;li&gt;Resizable thumbnail grid&lt;/li&gt; &#xA; &lt;li&gt;User-defined metadata fields&lt;/li&gt; &#xA; &lt;li&gt;Multiple directory support&lt;/li&gt; &#xA; &lt;li&gt;SQLite (or similar) save files&lt;/li&gt; &#xA; &lt;li&gt;Reading of EXIF and XMP fields&lt;/li&gt; &#xA; &lt;li&gt;Improved UI/UX&lt;/li&gt; &#xA; &lt;li&gt;Better internal API for accessing Entries, Tags, Fields, etc. from the library.&lt;/li&gt; &#xA; &lt;li&gt;Proper testing workflow&lt;/li&gt; &#xA; &lt;li&gt;Continued code cleanup and modularization&lt;/li&gt; &#xA; &lt;li&gt;Reassessment of save file structure in order to prioritize portability (leading to exportable tags, presets, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Future Features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for multiple simultaneous users/clients&lt;/li&gt; &#xA; &lt;li&gt;Draggable files outside the program&lt;/li&gt; &#xA; &lt;li&gt;Ability to ignore specific files&lt;/li&gt; &#xA; &lt;li&gt;A finished “macro system” for automatic tagging based on predetermined criteria.&lt;/li&gt; &#xA; &lt;li&gt;Different library views&lt;/li&gt; &#xA; &lt;li&gt;Date and time fields&lt;/li&gt; &#xA; &lt;li&gt;Entry linking/referencing&lt;/li&gt; &#xA; &lt;li&gt;Audio waveform previews&lt;/li&gt; &#xA; &lt;li&gt;3D object previews&lt;/li&gt; &#xA; &lt;li&gt;Additional previews for miscellaneous file types&lt;/li&gt; &#xA; &lt;li&gt;Exportable/sharable tags and settings&lt;/li&gt; &#xA; &lt;li&gt;Optional global tags and settings, spanning across libraries&lt;/li&gt; &#xA; &lt;li&gt;Importing &amp;amp; exporting libraries to/from other programs&lt;/li&gt; &#xA; &lt;li&gt;Port to a more performant language and modern frontend (Rust?, Tauri?, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Plugin system&lt;/li&gt; &#xA; &lt;li&gt;Local OCR search&lt;/li&gt; &#xA; &lt;li&gt;Support for local machine learning-based tag suggestions for images&lt;/li&gt; &#xA; &lt;li&gt;Mobile version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Features I Likely Won’t Add/Pull&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Native Cloud Integration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;There are plenty of services already (native or third-party) that allow you to mount your cloud drives as virtual drives on your system. Pointing TagStudio to one of these mounts should function similarly to what native integration would look like.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Native ChatGPT/Non-Local LLM Integration &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This could mean different things depending on what you&#39;re intending. Whether it&#39;s trying to use an LLM to replace the native search, or to trying to use a model for image recognition, I&#39;m not interested in hooking people&#39;s TagStudio libraries into non-local LLMs such as ChatGPT and/or turn the program into a &#34;chatbot&#34; interface (see: &lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#goals&#34;&gt;Goals/Privacy&lt;/a&gt;). I wouldn&#39;t, however, mind using &lt;strong&gt;locally&lt;/strong&gt; hosted models to provide the &lt;em&gt;optional&lt;/em&gt; ability for additional searching and tagging methods (especially when it comes to facial recognition).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why Is the Version Already v9?&lt;/h3&gt; &#xA;&lt;p&gt;I’ve been developing this project over several years in private, and have gone through several major iterations and rewrites in that time. This “major version” is just a number at the end of the day, and if I wanted to I couldn’t released this as “Version 0” or “Version 1.0”, but I’ve decided to stick to my original version numbers to avoid needing to go in and change existing documentation and code comments. Version 10 is intended to include all of the “Priority Features” I’ve outlined in the &lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#what-features-are-you-planning-on-adding&#34;&gt;previous&lt;/a&gt; section. I’ve also labeled this version as an Alpha, and will likely reset the numbers when a feature-complete beta is reached.&lt;/p&gt; &#xA;&lt;h3&gt;Wait, Is There a CLI Version?&lt;/h3&gt; &#xA;&lt;p&gt;As of right now, no. However, I &lt;em&gt;did&lt;/em&gt; have a CLI version in the recent past before dedicating my efforts to the Qt GUI version. I’ve left in the currently-inoperable CLI code just in case anyone was curious about it. Also yes, it’s just a bunch of glorified print statements (&lt;em&gt;the outlook for some form of curses on Windows didn’t look great at the time, and I just needed a driver for the newly refactored code...).&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Can I Contribute?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Yes!!&lt;/strong&gt; I recommend taking a look at the &lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#priority-features&#34;&gt;Priority Features&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#future-features&#34;&gt;Future Features&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/#features-i-likely-wont-addpull&#34;&gt;Features I Won&#39;t Pull&lt;/a&gt; lists, as well as the project issues to see what’s currently being worked on. Please do not submit pull requests with new feature additions without opening up an issue with a feature request first.&lt;/p&gt; &#xA;&lt;p&gt;As of writing I don’t have a concrete style guide, just try to stay within or close enough to the PEP 8 style guide and/or match the style of the existing code.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Doriandarko/maestro</title>
    <updated>2024-04-27T01:29:17Z</updated>
    <id>tag:github.com,2024-04-27:/Doriandarko/maestro</id>
    <link href="https://github.com/Doriandarko/maestro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A framework for Claude Opus to intelligently orchestrate subagents.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Maestro - A Framework for Claude Opus, GPT and local LLMs to Orchestrate Subagents&lt;/h1&gt; &#xA;&lt;p&gt;This Python script demonstrates an AI-assisted task breakdown and execution workflow using the Anthropic API. It utilizes two AI models, Opus and Haiku, to break down an objective into sub-tasks, execute each sub-task, and refine the results into a cohesive final output.&lt;/p&gt; &#xA;&lt;h2&gt;New&lt;/h2&gt; &#xA;&lt;p&gt;Mestro now runs locally thanks to the Ollama platform. Experience the power of Llama 3 locally!&lt;/p&gt; &#xA;&lt;p&gt;Before running the script&lt;/p&gt; &#xA;&lt;p&gt;Install Ollama client from here &lt;a href=&#34;https://ollama.com/download&#34;&gt;https://ollama.com/download&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ollama.pull(&#39;llama3:70b&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will depend on the model you want to use it, you only need to do it once or if you want to update the model when a new version it&#39;s out. In the script I am using both versions but you can customize the model you want to use&lt;/p&gt; &#xA;&lt;p&gt;ollama.pull(&#39;llama3:70b&#39;) ollama.pull(&#39;llama3:8b&#39;)&lt;/p&gt; &#xA;&lt;p&gt;Then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-ollama.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Higly requested features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GROQ SUPPORT Experience the power of maestro thanks to Groq super fast api responses.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install groq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-groq.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SEARCH 🔍&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Now, when it&#39;s creating a task for its subagent, Claude Opus will perform a search and get the best answer to help the subagent solve that task even better.&lt;/p&gt; &#xA;&lt;p&gt;Make sure you replace your Tavil API for search to work&lt;/p&gt; &#xA;&lt;p&gt;Get one here &lt;a href=&#34;https://tavily.com/&#34;&gt;https://tavily.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT4 SUPPORT&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Add support for GPT-4 as an orchestrator in maestro-gpt.py Simply&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro-gpt.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After you complete your installs.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Breaks down an objective into manageable sub-tasks using the Opus model&lt;/li&gt; &#xA; &lt;li&gt;Executes each sub-task using the Haiku model&lt;/li&gt; &#xA; &lt;li&gt;Provides the Haiku model with memory of previous sub-tasks for context&lt;/li&gt; &#xA; &lt;li&gt;Refines the sub-task results into a final output using the Opus model&lt;/li&gt; &#xA; &lt;li&gt;Generates a detailed exchange log capturing the entire task breakdown and execution process&lt;/li&gt; &#xA; &lt;li&gt;Saves the exchange log to a Markdown file for easy reference&lt;/li&gt; &#xA; &lt;li&gt;Utilizes an improved prompt for the Opus model to better assess task completion&lt;/li&gt; &#xA; &lt;li&gt;Creates code files and folders when working on code projects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To run this script, you need to have the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python installed&lt;/li&gt; &#xA; &lt;li&gt;Anthropic API key&lt;/li&gt; &#xA; &lt;li&gt;Required Python packages: &lt;code&gt;anthropic&lt;/code&gt; and &lt;code&gt;rich&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository or download the script file.&lt;/li&gt; &#xA; &lt;li&gt;Install the required Python packages by running the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Replace the placeholder API key in the script with your actual Anthropic API key:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = Anthropic(api_key=&#34;YOUR_API_KEY_HERE&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If using search, replace your Tavil API&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tavily = TavilyClient(api_key=&#34;YOUR API KEY HERE&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a terminal or command prompt and navigate to the directory containing the script.&lt;/li&gt; &#xA; &lt;li&gt;Run the script using the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python maestro.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Enter your objective when prompted:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Please enter your objective: Your objective here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will start the task breakdown and execution process. It will display the progress and results in the console using formatted panels.&lt;/p&gt; &#xA;&lt;p&gt;Once the process is complete, the script will display the refined final output and save the full exchange log to a Markdown file with a filename based on the objective.&lt;/p&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;p&gt;The script consists of the following main functions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;opus_orchestrator(objective, previous_results=None)&lt;/code&gt;: Calls the Opus model to break down the objective into sub-tasks or provide the final output. It uses an improved prompt to assess task completion and includes the phrase &#34;The task is complete:&#34; when the objective is fully achieved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;haiku_sub_agent(prompt, previous_haiku_tasks=None)&lt;/code&gt;: Calls the Haiku model to execute a sub-task prompt, providing it with the memory of previous sub-tasks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;opus_refine(objective, sub_task_results)&lt;/code&gt;: Calls the Opus model to review and refine the sub-task results into a cohesive final output.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The script follows an iterative process, repeatedly calling the opus_orchestrator function to break down the objective into sub-tasks until the final output is provided. Each sub-task is then executed by the haiku_sub_agent function, and the results are stored in the task_exchanges and haiku_tasks lists.&lt;/p&gt; &#xA;&lt;p&gt;The loop terminates when the Opus model includes the phrase &#34;The task is complete:&#34; in its response, indicating that the objective has been fully achieved.&lt;/p&gt; &#xA;&lt;p&gt;Finally, the opus_refine function is called to review and refine the sub-task results into a final output. The entire exchange log, including the objective, task breakdown, and refined final output, is saved to a Markdown file.&lt;/p&gt; &#xA;&lt;h2&gt;Customization&lt;/h2&gt; &#xA;&lt;p&gt;You can customize the script according to your needs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adjust the max_tokens parameter in the client.messages.create() function calls to control the maximum number of tokens generated by the AI models.&lt;/li&gt; &#xA; &lt;li&gt;Change the models to what you prefer, like replacing Haiku with Sonnet or Opus.&lt;/li&gt; &#xA; &lt;li&gt;Modify the console output formatting by updating the rich library&#39;s Panel and Console configurations.&lt;/li&gt; &#xA; &lt;li&gt;Customize the exchange log formatting and file extension by modifying the relevant code sections.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This script is released under the MIT License.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anthropic for providing the AI models and API.&lt;/li&gt; &#xA; &lt;li&gt;Rich for the beautiful console formatting.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>