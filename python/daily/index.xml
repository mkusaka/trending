<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-04T01:40:03Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Sanster/IOPaint</title>
    <updated>2024-02-04T01:40:03Z</updated>
    <id>tag:github.com,2024-02-04:/Sanster/IOPaint</id>
    <link href="https://github.com/Sanster/IOPaint" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;IOPaint&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;A free and open-source inpainting &amp;amp; outpainting tool powered by SOTA AI model.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Sanster/IOPaint&#34;&gt; &lt;img alt=&#34;total download&#34; src=&#34;https://pepy.tech/badge/iopaint&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/iopaint&#34;&gt; &lt;img alt=&#34;version&#34; src=&#34;https://img.shields.io/pypi/v/iopaint&#34;&gt; &lt;/a&gt; &lt;a href=&#34;&#34;&gt; &lt;img alt=&#34;python version&#34; src=&#34;https://img.shields.io/pypi/pyversions/iopaint&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Erase&lt;/th&gt; &#xA;   &lt;th&gt;Replace Object&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;video src=&#34;https://github.com/Sanster/IOPaint/assets/3998421/264bc27c-0abd-4d8b-bb1e-0078ab264c4a&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;video src=&#34;https://github.com/Sanster/IOPaint/assets/3998421/1de5c288-e0e1-4f32-926d-796df0655846&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Draw Text&lt;/th&gt; &#xA;   &lt;th&gt;Out-painting&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#xA;    &lt;video src=&#34;https://github.com/Sanster/IOPaint/assets/3998421/ffd4eda4-f7d4-4693-93d8-d2cd5aa7c6d6&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&#xA;    &lt;video src=&#34;https://github.com/Sanster/IOPaint/assets/3998421/c4af8aef-8c29-49e0-96eb-0aae2f768da2&#34;&gt;&lt;/video&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Start webui&lt;/h3&gt; &#xA;&lt;p&gt;IOPaint provides a convenient webui for using the latest AI models to edit your images. You can install and start IOPaint easily by running following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# In order to use GPU, install cuda version of pytorch first.&#xA;# pip3 install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu118&#xA;# AMD GPU users, please utilize the following command, only works on linux, as pytorch is not yet supported on Windows with ROCm.&#xA;# pip3 install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/rocm5.6&#xA;&#xA;pip3 install iopaint&#xA;iopaint start --model=lama --device=cpu --port=8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it, you can start using IOPaint by visiting &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; in your web browser.&lt;/p&gt; &#xA;&lt;h3&gt;Batch processing&lt;/h3&gt; &#xA;&lt;p&gt;You can also use IOPaint in the command line to batch process images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;iopaint run --model=lama --device=cpu \&#xA;--input=/path/to/image_folder \&#xA;--mask=/path/to/mask_folder \&#xA;--output=output_dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;--input&lt;/code&gt; is the folder containing input images, &lt;code&gt;--mask&lt;/code&gt; is the folder containing corresponding mask images. When &lt;code&gt;--mask&lt;/code&gt; is a path to a mask file, all images will be processed using this mask.&lt;/p&gt; &#xA;&lt;p&gt;You can see more information about the available models and plugins supported by IOPaint below.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Completely free and open-source, fully self-hosted, support CPU &amp;amp; GPU &amp;amp; Apple Silicon&lt;/li&gt; &#xA; &lt;li&gt;Supports various AI models: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.iopaint.com/models#erase-models&#34;&gt;Erase models&lt;/a&gt;: These models can be used to remove unwanted object, defect, watermarks, people from image. I have also developed a macOS native app called &lt;a href=&#34;https://opticlean.io/&#34;&gt;OptiClean&lt;/a&gt; that provides this feature.&lt;/li&gt; &#xA;   &lt;li&gt;Stable Diffusion models: You can use any Stable Diffusion Inpainting(or normal) models from &lt;a href=&#34;https://huggingface.co/models?other=stable-diffusion&#34;&gt;Huggingface&lt;/a&gt; in IOPaint. Some popular used models include: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-inpainting&#34;&gt;runwayml/stable-diffusion-inpainting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1&#34;&gt;diffusers/stable-diffusion-xl-1.0-inpainting-0.1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/andregn/Realistic_Vision_V3.0-inpainting&#34;&gt;andregn/Realistic_Vision_V3.0-inpainting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lykon/dreamshaper-8-inpainting&#34;&gt;Lykon/dreamshaper-8-inpainting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Sanster/anything-4.0-inpainting&#34;&gt;Sanster/anything-4.0-inpainting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Sanster/PowerPaint-V1-stable-diffusion-inpainting&#34;&gt;Sanster/PowerPaint-V1-stable-diffusion-inpainting&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Other Diffusion models: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Sanster/AnyText&#34;&gt;Sanster/AnyText&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/timbrooks/instruct-pix2pix&#34;&gt;timbrooks/instruct-pix2pix&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Fantasy-Studio/Paint-by-Example&#34;&gt;Fantasy-Studio/Paint-by-Example&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder-inpaint&#34;&gt;kandinsky-community/kandinsky-2-2-decoder-inpaint&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Plugins &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://iopaint.com/plugins/interactive_seg&#34;&gt;Segment Anything&lt;/a&gt;: Accurate and fast interactive object segmentation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://iopaint.com/plugins/rembg&#34;&gt;RemoveBG&lt;/a&gt;: Remove image background or generate masks for foreground objects&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://iopaint.com/plugins/anime_seg&#34;&gt;Anime Segmentation&lt;/a&gt;: Similar to RemoveBG, the model is specifically trained for anime images.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://iopaint.com/plugins/RealESRGAN&#34;&gt;RealESRGAN&lt;/a&gt;: Super Resolution&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://iopaint.com/plugins/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt;: Face Restoration&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://iopaint.com/plugins/RestoreFormer&#34;&gt;RestoreFormer&lt;/a&gt;: Face Restoration&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://iopaint.com/file_manager&#34;&gt;FileManager&lt;/a&gt;: Browse your pictures conveniently and save them directly to the output directory.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>allenai/OLMo</title>
    <updated>2024-02-04T01:40:03Z</updated>
    <id>tag:github.com,2024-02-04:/allenai/OLMo</id>
    <link href="https://github.com/allenai/OLMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modeling, training, eval, and inference code for OLMo&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;img src=&#34;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&#34; width=&#34;300&#34;/&gt; --&gt; &#xA; &lt;img src=&#34;https://allenai.org/olmo/olmo-7b-animation.gif&#34; alt=&#34;OLMo Logo&#34; width=&#34;800&#34; style=&#34;margin-left:&#39;auto&#39; margin-right:&#39;auto&#39; display:&#39;block&#39;&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt;OLMo: Open Language Model&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub License&#34; src=&#34;https://img.shields.io/github/license/allenai/OLMo&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/allenai/OLMo.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2402.00838.pdf&#34;&gt; &lt;img alt=&#34;Paper URL&#34; src=&#34;https://img.shields.io/badge/arxiv-2402.00838-blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;OLMo is a repository for training and using AI2&#39;s state-of-the-art open language models. It is built by scientists, for scientists.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First install &lt;a href=&#34;https://pytorch.org&#34;&gt;PyTorch&lt;/a&gt; according to the instructions specific to your operating system.&lt;/p&gt; &#xA;&lt;p&gt;To install from source (recommended for training/fine-tuning) run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/allenai/OLMo.git&#xA;cd OLMo&#xA;pip install -e .[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise you can install the model code by itself directly from PyPI with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ai2-olmo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models overview&lt;/h2&gt; &#xA;&lt;p&gt;The core models in the OLMo family released so far are (all trained on the &lt;a href=&#34;https://huggingface.co/datasets/allenai/dolma&#34;&gt;Dolma dataset&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Training Tokens&lt;/th&gt; &#xA;   &lt;th&gt;Context Length&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-1B&#34;&gt;OLMo 1B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3 Trillion&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-7B&#34;&gt;OLMo 7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.5 Trillion&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-7B-Twin-2T&#34;&gt;OLMo 7B Twin 2T&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2 Trillion&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;To fine-tune an OLMo model using our trainer you&#39;ll first need to prepare your dataset by tokenizing it and saving the tokens IDs to a flat numpy memory-mapped array. See &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/scripts/prepare_tulu_data.py&#34;&gt;&lt;code&gt;scripts/prepare_tulu_data.py&lt;/code&gt;&lt;/a&gt; for an example with the Tulu V2 dataset, which can be easily modified for other datasets.&lt;/p&gt; &#xA;&lt;p&gt;Next, prepare your training config. There are many examples in the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/configs&#34;&gt;&lt;code&gt;configs/&lt;/code&gt;&lt;/a&gt; directory that you can use as a starting point. The most important thing is to make sure the model parameters (the &lt;code&gt;model&lt;/code&gt; field in the config) match up with the checkpoint you&#39;re starting from. To be safe you can always start from the config that comes with the model checkpoint. At a minimum you&#39;ll need to make the following changes to the config or provide the corresponding overrides from the command line:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update &lt;code&gt;load_path&lt;/code&gt; to point to the checkpoint you want to start from.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;reset_trainer_state&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;data.paths&lt;/code&gt; to point to the &lt;code&gt;token_ids.npy&lt;/code&gt; file you generated.&lt;/li&gt; &#xA; &lt;li&gt;Optionally update &lt;code&gt;data.label_mask_paths&lt;/code&gt; to point to the &lt;code&gt;label_mask.npy&lt;/code&gt; file you generated, unless you don&#39;t need special masking for the loss.&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;evaluators&lt;/code&gt; to add/remove in-loop evaluations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you&#39;re satisfied with your training config, you can launch the training job via &lt;code&gt;torchrun&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node=8 scripts/train.py {path_to_train_config} \&#xA;    --data.paths=[{path_to_data}/input_ids.npy] \&#xA;    --data.label_mask_paths=[{path_to_data}/label_mask.npy] \&#xA;    --load_path={path_to_checkpoint} \&#xA;    --reset_trainer_state&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: passing CLI overrides like &lt;code&gt;--reset_trainer_state&lt;/code&gt; is only necessary if you didn&#39;t update those fields in your config.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;You can utilize our HuggingFace integration to run inference on the olmo checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hf_olmo import * # registers the Auto* classes&#xA;&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;olmo = AutoModelForCausalLM.from_pretrained(&#34;allenai/OLMo-7B&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;allenai/OLMo-7B&#34;)&#xA;&#xA;message = [&#34;Language modeling is &#34;]&#xA;inputs = tokenizer(message, return_tensors=&#39;pt&#39;, return_token_type_ids=False)&#xA;response = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)&#xA;print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, with the huggingface pipeline abstraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;olmo_pipe = pipeline(&#34;text-generation&#34;, model=&#34;allenai/OLMo-7B&#34;)&#xA;print(olmo_pipe(&#34;Language modeling is&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference on finetuned checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;If you finetune the model using the code above, you can use the conversion script to convert a native OLMo checkpoint to a HuggingFace-compatible checkpoint&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python hf_olmo/convert_olmo_to_hf.py --checkpoint-dir /path/to/checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;olmo = AutoModelForCausalLM.from_pretrained(&#34;allenai/OLMo-7B&#34;, torch_dtype=torch.float16, load_in_8bit=True)  # requires bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The quantized model is more sensitive to typing / cuda, so it is recommended to pass the inputs as inputs.input_ids.to(&#39;cuda&#39;) to avoid potential issues.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Additional tools for evaluating OLMo models are available at the &lt;a href=&#34;https://github.com/allenai/ai2-olmo-eval&#34;&gt;OLMo Eval&lt;/a&gt; repo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>g1879/DrissionPage</title>
    <updated>2024-02-04T01:40:03Z</updated>
    <id>tag:github.com,2024-02-04:/g1879/DrissionPage</id>
    <link href="https://github.com/g1879/DrissionPage" rel="alternate"></link>
    <summary type="html">&lt;p&gt;基于python的网页自动化工具。既能控制浏览器，也能收发数据包。可兼顾浏览器自动化的便利性和requests的高效率。功能强大，内置无数人性化设计和便捷功能。语法简洁而优雅，代码量少。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;✨️ 概述&lt;/h1&gt; &#xA;&lt;p&gt;DrissionPage 是一个基于 python 的网页自动化工具。&lt;/p&gt; &#xA;&lt;p&gt;它既能控制浏览器，也能收发数据包，还能把两者合而为一。&lt;/p&gt; &#xA;&lt;p&gt;可兼顾浏览器自动化的便利性和 requests 的高效率。&lt;/p&gt; &#xA;&lt;p&gt;它功能强大，内置无数人性化设计和便捷功能。&lt;/p&gt; &#xA;&lt;p&gt;它的语法简洁而优雅，代码量少，对新手友好。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitee.com/g1879/DrissionPage/stargazers&#34;&gt;&lt;img src=&#34;https://gitee.com/g1879/DrissionPage/badge/star.svg?theme=dark&#34; alt=&#34;star&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitee.com/g1879/DrissionPage/members&#34;&gt;&lt;img src=&#34;https://gitee.com/g1879/DrissionPage/badge/fork.svg?theme=dark&#34; alt=&#34;fork&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;项目地址：&lt;a href=&#34;https://gitee.com/g1879/DrissionPage&#34;&gt;gitee&lt;/a&gt; | &lt;a href=&#34;https://github.com/g1879/DrissionPage&#34;&gt;github&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;您的星星是对我最大的支持💖&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;支持系统：Windows、Linux、Mac&lt;/p&gt; &#xA;&lt;p&gt;python 版本：3.6 及以上&lt;/p&gt; &#xA;&lt;p&gt;支持浏览器：Chromium 内核浏览器(如 Chrome 和 Edge)，electron 应用&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;🛠 如何使用&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;📖 使用文档：&lt;/strong&gt; &lt;a href=&#34;https://g1879.gitee.io/drissionpagedocs&#34;&gt;点击查看&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;交流 QQ 群：&lt;/strong&gt; 636361957&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;📕 背景&lt;/h1&gt; &#xA;&lt;p&gt;用 requests 做数据采集面对要登录的网站时，要分析数据包、JS 源码，构造复杂的请求，往往还要应付验证码、JS 混淆、签名参数等反爬手段，门槛较高，开发效率不高。 使用浏览器，可以很大程度上绕过这些坑，但浏览器运行效率不高。&lt;/p&gt; &#xA;&lt;p&gt;因此，这个库设计初衷，是将它们合而为一，同时实现“写得快”和“跑得快”。能够在不同需要时切换相应模式，并提供一种人性化的使用方法，提高开发和运行效率。&lt;br&gt; 除了合并两者，本库还以网页为单位封装了常用功能，提供非常简便的操作和语句，使用户可减少考虑细节，专注功能实现。 以简单的方式实现强大的功能，使代码更优雅。&lt;/p&gt; &#xA;&lt;p&gt;以前的版本是对 selenium 进行重新封装实现的。从 3.0 开始，作者另起炉灶，对底层进行了重新开发，摆脱对 selenium 的依赖，增强了功能，提升了运行效率。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;💡 理念&lt;/h1&gt; &#xA;&lt;p&gt;简洁而强大！&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;☀️ 特性和亮点&lt;/h1&gt; &#xA;&lt;p&gt;作者经过长期实践，踩过无数坑，总结出的经验全写到这个库里了。&lt;/p&gt; &#xA;&lt;h2&gt;🎇 强大的自研内核&lt;/h2&gt; &#xA;&lt;p&gt;本库采用全自研的内核，内置了 N 多实用功能，对常用功能作了整合和优化，对比 selenium，有以下优点：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;无 webdriver 特征&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;无需为不同版本的浏览器下载不同的驱动&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;运行速度更快&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;可以跨&lt;code&gt;&amp;lt;iframe&amp;gt;&lt;/code&gt;查找元素，无需切入切出&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;把&lt;code&gt;&amp;lt;iframe&amp;gt;&lt;/code&gt;看作普通元素，获取后可直接在其中查找元素，逻辑更清晰&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;可以同时操作浏览器中的多个标签页，即使标签页为非激活状态，无需切换&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;可以直接读取浏览器缓存来保存图片，无需用 GUI 点击另存&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;可以对整个网页截图，包括视口外的部分（90以上版本浏览器支持）&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;可处理非&lt;code&gt;open&lt;/code&gt;状态的 shadow-root&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎇 亮点功能&lt;/h2&gt; &#xA;&lt;p&gt;除了以上优点，本库还内置了无数人性化设计。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;极简的语法规则。集成大量常用功能，代码更优雅&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;定位元素更加容易，功能更强大稳定&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;无处不在的等待和自动重试功能。使不稳定的网络变得易于控制，程序更稳定，编写更省心&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;提供强大的下载工具。操作浏览器时也能享受快捷可靠的下载功能&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;允许反复使用已经打开的浏览器。无须每次运行从头启动浏览器，调试超方便&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;使用 ini 文件保存常用配置，自动调用，提供便捷的设置，远离繁杂的配置项&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;内置 lxml 作为解析引擎，解析速度成几个数量级提升&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;使用 POM 模式封装，可直接用于测试，便于扩展&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;高度集成的便利功能，从每个细节中体现&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;还有很多细节，这里不一一列举，欢迎实际使用中体验：）&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;🔖 版本历史&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://g1879.gitee.io/drissionpagedocs/history/introduction/&#34;&gt;点击查看版本历史&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;🖐🏻 免责声明&lt;/h1&gt; &#xA;&lt;p&gt;请勿将 DrissionPage 应用到任何可能会违反法律规定和道德约束的工作中,请友善使用 DrissionPage，遵守蜘蛛协议，不要将 DrissionPage 用于任何非法用途。如您选择使用 DrissionPage 即代表您遵守此协议，作者不承担任何由于您违反此协议带来任何的法律风险和损失，一切后果由您承担。&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;☕ 请我喝咖啡&lt;/h1&gt; &#xA;&lt;p&gt;如果本项目对您有所帮助，不妨请作者我喝杯咖啡 ：）&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://gitee.com/g1879/DrissionPageDocs/raw/master/docs/imgs/code.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>