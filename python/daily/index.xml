<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-24T01:42:34Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deforum-art/sd-webui-modelscope-text2video</title>
    <updated>2023-03-24T01:42:34Z</updated>
    <id>tag:github.com,2023-03-24:/deforum-art/sd-webui-modelscope-text2video</id>
    <link href="https://github.com/deforum-art/sd-webui-modelscope-text2video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Auto1111 extension consisting of implementation of ModelScope text2video using only Auto1111 webui dependencies&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ModelScope text2video Extension for AUTOMATIC1111&#39;s StableDiffusion WebUI&lt;/h1&gt; &#xA;&lt;p&gt;Auto1111 extension consisting of implementation of ModelScope text2video using only Auto1111 webui dependencies and downloadable models (so no logins required anywhere)&lt;/p&gt; &#xA;&lt;p&gt;8gbs vram should be enough to run on GPU with low vram vae on at 256x256 (and we are already getting reports of people launching 192x192 videos &lt;a href=&#34;https://github.com/deforum-art/sd-webui-modelscope-text2video/discussions/27&#34;&gt;with 4gbs of vram&lt;/a&gt;). 24 frames length 256x256 video definitely fits into 12gbs of NVIDIA GeForce RTX 2080 Ti. We will appreciate &lt;em&gt;any&lt;/em&gt; help with this extension, &lt;em&gt;especially&lt;/em&gt; pull-requests.&lt;/p&gt; &#xA;&lt;p&gt;Test examples:&lt;/p&gt; &#xA;&lt;p&gt;Prompt: &lt;code&gt;flowers turning into lava&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/14872007/226214023-2d3892d8-64d4-4312-baab-575aafedae09.mp4&#34;&gt;https://user-images.githubusercontent.com/14872007/226214023-2d3892d8-64d4-4312-baab-575aafedae09.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: &lt;code&gt;cinematic explosion by greg rutkowski&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/14872007/226345611-a1f0601f-db32-41bd-b983-80d363eca4d5.mp4&#34;&gt;https://user-images.githubusercontent.com/14872007/226345611-a1f0601f-db32-41bd-b983-80d363eca4d5.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: &lt;code&gt;really attractive anime girl skating, by makoto shinkai, cinematic lighting&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/14872007/226468406-ce43fa0c-35f2-4625-a892-9fb3411d96bb.mp4&#34;&gt;https://user-images.githubusercontent.com/14872007/226468406-ce43fa0c-35f2-4625-a892-9fb3411d96bb.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Where to get the weights&lt;/h2&gt; &#xA;&lt;p&gt;Download the following files from the &lt;a href=&#34;https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/tree/main&#34;&gt;original HuggingFace repository&lt;/a&gt;. Alternatively, &lt;a href=&#34;https://huggingface.co/kabachuha/modelscope-damo-text2video-pruned-weights/tree/main&#34;&gt;download half-precision fp16 pruned weights (they are smaller and use less vram on loading)&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VQGAN_autoencoder.pth&lt;/li&gt; &#xA; &lt;li&gt;configuration.json&lt;/li&gt; &#xA; &lt;li&gt;open_clip_pytorch_model.bin&lt;/li&gt; &#xA; &lt;li&gt;text2video_pytorch_model.pth&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And put them in &lt;code&gt;stable-diffusion-webui/models/ModelScope/t2v&lt;/code&gt;. Create those 2 folders if they are missing.&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/14872007/226345377-bad6dda5-f921-4233-b832-843e78854cbb.png&#34; alt=&#34;Screenshot 2023-03-20 at 15-52-21 Stable Diffusion&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/14872007/226345398-d37133a8-3e5f-43f3-ae13-37dc609cd14c.png&#34; alt=&#34;Screenshot 2023-03-20 at 15-52-15 Stable Diffusion&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dev resources&lt;/h2&gt; &#xA;&lt;p&gt;HuggingFace space:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis&#34;&gt;https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The model PyTorch implementation from ModelScope:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope/tree/master/modelscope/models/multi_modal/video_synthesis&#34;&gt;https://github.com/modelscope/modelscope/tree/master/modelscope/models/multi_modal/video_synthesis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Google Colab from the devs:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1uW1ZqswkQ9Z9bp5Nbo5z59cAn7I0hE6R?usp=sharing&#34;&gt;https://colab.research.google.com/drive/1uW1ZqswkQ9Z9bp5Nbo5z59cAn7I0hE6R?usp=sharing&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deep-diver/Alpaca-LoRA-Serve</title>
    <updated>2023-03-24T01:42:34Z</updated>
    <id>tag:github.com,2023-03-24:/deep-diver/Alpaca-LoRA-Serve</id>
    <link href="https://github.com/deep-diver/Alpaca-LoRA-Serve" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Alpaca-LoRA as Chatbot service&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶ô üöÄ Alpaca-LoRA as a Chatbot Service&lt;/h1&gt; &#xA;&lt;p&gt;üöß This project is still under development process. While serving the project, I noticed there are some bugs emitted by the model itself such as too many line breaks which causes OOM eventually. You can propose PR, but I will merge any improvement at any time as soon as I spot any problems.&lt;/p&gt; &#xA;&lt;p&gt;üîó &lt;strong&gt;Demo link&lt;/strong&gt;: &lt;a href=&#34;https://notebooksf.jarvislabs.ai/43j3x9FSS8Tg0sqvMlDgKPo9vsoSTTKRsX4RIdC3tNd6qeQ6ktlA0tyWRAR3fe_l&#34;&gt;Batch Mode&lt;/a&gt; and &lt;a href=&#34;https://notebookse.jarvislabs.ai/BuOu_VbEuUHb09VEVHhfnFq4-PMhBRVCcfHBRCOrq7c4O9GI4dIGoidvNf76UsRL/&#34;&gt;Streaming Mode&lt;/a&gt; (both are running on a single A6000 instance)&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;easiest way&lt;/strong&gt; to run this project is to use Colab. Just open up the &lt;a href=&#34;https://github.com/deep-diver/Alpaca-LoRA-Serve/raw/main/notebooks/alpaca_lora_in_colab.ipynb&#34;&gt;alpaca_lora_in_colab&lt;/a&gt; notebook in Colab (there is a button &lt;code&gt;open in colab&lt;/code&gt;), and run every cell sequentially. With the standard GPU instance(&lt;em&gt;&lt;strong&gt;T4&lt;/strong&gt;&lt;/em&gt;), you can run 7B and 13B models. With the premium GPU instance(&lt;em&gt;&lt;strong&gt;A100 40GB&lt;/strong&gt;&lt;/em&gt;), you can even run 30B model! Screenshotüëáüèº Just note that the connection could be somewhat unstable, so I recommend you to use Colab for development purpose.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/hZ3771L/Screen-Shot-2023-03-22-at-9-36-15-PM.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository demonstrates Alpaca-LoRA as a Chatbot service with &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-LoRA&lt;/a&gt; and &lt;a href=&#34;https://gradio.app/&#34;&gt;Gradio&lt;/a&gt;. It comes with the following features:&lt;/p&gt; &#xA;&lt;h3&gt;Mode&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Batch Generation Mode&lt;/strong&gt;: batch generation mode aggregates requests up to &lt;code&gt;batch_size&lt;/code&gt;, and pass the prompts in the requests to the model. It waits the current requests are fully handled. For instance, with &lt;code&gt;batch_size=4&lt;/code&gt;, if a user sends a request, that is under processing. While it is under processing, if other users are connected, up to 4 requests from the users are aggregated and processed as soon as the current one is done.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Streaming Mode&lt;/strong&gt;: streaming mode handles multiple requests in a interleaving way with threads. For instance, if there are two users (A and B) are connected, A&#39;s request is handled, and then B&#39;s request is handled, and then A&#39;s request is handled again.... This is because of the nature of streaming mode which generates and &lt;code&gt;yield&lt;/code&gt; tokens in one by one manner.&lt;/p&gt; &#xA;&lt;h3&gt;Context management&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Alpaca-LoRA as a Chatbot Service manages context in two ways. First of all, it remembers(stores) every history of the conversations by default as in the following code snippet. &lt;code&gt;context_string&lt;/code&gt; is set as &lt;em&gt;&lt;strong&gt;&#34;Below is a history of instructions that describe tasks, paired with an input that provides further context. Write a response that appropriately completes the request by remembering the conversation history.&#34;&lt;/strong&gt;&lt;/em&gt; by default, but it could be set manually via the &lt;code&gt;Context&lt;/code&gt; field on top of the screen. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;additionally, there is a &lt;code&gt;Summarize&lt;/code&gt; button in the middle (you need to expand the component labeled as &lt;em&gt;&lt;strong&gt;&#34;Helper Buttons&#34;&lt;/strong&gt;&lt;/em&gt;). If you click this button, it automatically input &lt;em&gt;&lt;strong&gt;&#34;summarize our conversations so far in three sentences.&#34;&lt;/strong&gt;&lt;/em&gt; as a prompt, and the resulting generated text will be inserted into the &lt;code&gt;Context&lt;/code&gt; field. THen all the conversation history up to this point will be ignored. That means the conversation fresh restarts with the below code snippet except &lt;code&gt;context_string&lt;/code&gt; will be filled up with the model generated text.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;em&gt;NOTE: the only last 2,000 characters are kept, and this number can be configured in &lt;code&gt;constants.py&lt;/code&gt;&lt;/em&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;f&#34;&#34;&#34;{context_string}&#xA;&#xA;### Input: {input} # Surrounding information to AI&#xA;&#xA;### Instruction: {prompt1} # First instruction/prompt given by user&#xA;&#xA;### Response {response1} # First response on the first prompt by AI&#xA;&#xA;### Instruction: {prompt2} # Second instruction/prompt given by user&#xA;&#xA;### Response: {response2} # Second response on the first prompt by AI&#xA;....&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;misc.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There is a &lt;code&gt;continue&lt;/code&gt; button in the middle of screen. What it does is to simply send &lt;em&gt;&lt;strong&gt;&#34;continue.&#34;&lt;/strong&gt;&lt;/em&gt; prompt to the model. This is useful if you get incomplete previous response from the model. With the &lt;em&gt;&lt;strong&gt;&#34;continue.&#34;&lt;/strong&gt;&lt;/em&gt;, the model tries to complete the response. Also, since this is a continuation of the response, the &lt;em&gt;&lt;strong&gt;&#34;continue.&#34;&lt;/strong&gt;&lt;/em&gt; prompt will be hidden to make chatting history more natural.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Currently supported LoRA checkpoints&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/tloen/alpaca-lora-7b&#34;&gt;tloen/alpaca-lora-7b&lt;/a&gt;: the original 7B Alpaca-LoRA checkpoint by tloen&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-13b&#34;&gt;chansung/alpaca-lora-13b&lt;/a&gt;: the 13B Alpaca-LoRA checkpoint by myself(chansung) with the same script to tune the original 7B model&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/koalpaca-lora-13b&#34;&gt;chansung/koalpaca-lora-13b&lt;/a&gt;: the 13B Alpaca-LoRA checkpoint by myself(chansung) with the Korean dataset created by &lt;a href=&#34;https://github.com/Beomi/KoAlpaca&#34;&gt;KoAlpaca project&lt;/a&gt; by Beomi. It works for English(user) to Korean(AI) conversations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/chansung/alpaca-lora-30b&#34;&gt;chansung/alpaca-lora-30b&lt;/a&gt;: the 30B Alpaca-LoRA checkpoint by myself(chansung) with the same script to tune the original 7B model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Prerequisites&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note that the code only works &lt;code&gt;Python &amp;gt;= 3.9&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ conda create -n alpaca-serve python=3.9&#xA;$ conda activate alpaca-serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ cd Alpaca-LoRA-Serve&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run Gradio application&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ BASE_URL=decapoda-research/llama-7b-hf&#xA;$ FINETUNED_CKPT_URL=tloen/alpaca-lora-7b&#xA;&#xA;$ python app.py --base_url $BASE_URL --ft_ckpt_url $FINETUNED_CKPT_URL --port 6006&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the following flags are supported&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;usage: app.py [-h] [--base_url BASE_URL] [--ft_ckpt_url FT_CKPT_URL] [--port PORT] [--batch_size BATCH_SIZE]&#xA;              [--api_open API_OPEN] [--share SHARE] [--gen_config_path GEN_CONFIG_PATH]&#xA;&#xA;Gradio Application for Alpaca-LoRA as a chatbot service&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --base_url BASE_URL   Hugging Face Hub URL&#xA;  --ft_ckpt_url FT_CKPT_URL&#xA;                        Hugging Face Hub URL&#xA;  --port PORT           port number where the app is served&#xA;  --batch_size BATCH_SIZE&#xA;                        how many requests to handle at the same time&#xA;                        default is set to 1 which enables streaming mode&#xA;  --api_open API_OPEN   do you want to open as API&#xA;  --share SHARE         do you want to share temporarily (useful in Colab env)&#xA;  --gen_config_path GEN_CONFIG_PATH&#xA;                        which config to use for GenerationConfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Design figure&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.ibb.co/w069GYg/Screenshot-2023-03-20-at-1-25-29-PM.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;I am thankful to &lt;a href=&#34;https://jarvislabs.ai/&#34;&gt;Jarvislabs.ai&lt;/a&gt; who generously provided free GPU resources to experiment with Alpaca-LoRA deployment and share it to communities to try out.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bigscience-workshop/petals</title>
    <updated>2023-03-24T01:42:34Z</updated>
    <id>tag:github.com,2023-03-24:/bigscience-workshop/petals</id>
    <link href="https://github.com/bigscience-workshop/petals" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üå∏ Run 100B+ language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.imgur.com/7eR7Pan.png&#34; width=&#34;400&#34;&gt;&lt;br&gt; Run 100B+ language models at home, BitTorrent-style.&lt;br&gt; Fine-tuning and inference &lt;a href=&#34;https://github.com/bigscience-workshop/petals#benchmarks&#34;&gt;up to 10x faster&lt;/a&gt; than offloading&lt;br&gt;&lt;br&gt; &lt;a href=&#34;https://pypi.org/project/petals/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/petals.svg?color=green&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;Generate text using distributed 176B-parameter &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/bigscience/bloomz&#34;&gt;BLOOMZ&lt;/a&gt; and fine-tune them for your own tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from petals import DistributedBloomForCausalLM&#xA;&#xA;model = DistributedBloomForCausalLM.from_pretrained(&#34;bigscience/bloom-petals&#34;, tuning_mode=&#34;ptune&#34;, pre_seq_len=16)&#xA;# Embeddings &amp;amp; prompts are on your device, BLOOM blocks are distributed across the Internet&#xA;&#xA;inputs = tokenizer(&#34;A cat sat&#34;, return_tensors=&#34;pt&#34;)[&#34;input_ids&#34;]&#xA;outputs = model.generate(inputs, max_new_tokens=5)&#xA;print(tokenizer.decode(outputs[0]))  # A cat sat on a mat...&#xA;&#xA;# Fine-tuning (updates only prompts or adapters hosted locally)&#xA;optimizer = torch.optim.AdamW(model.parameters())&#xA;for input_ids, labels in data_loader:&#xA;    outputs = model.forward(input_ids)&#xA;    loss = cross_entropy(outputs.logits, labels)&#xA;    optimizer.zero_grad()&#xA;    loss.backward()&#xA;    optimizer.step()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üöÄ &amp;nbsp;&lt;b&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing&#34;&gt;Try now in Colab&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;üîè Your data will be processed by other people in the public swarm. Learn more about privacy &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety&#34;&gt;here&lt;/a&gt;. For sensitive data, you can set up a &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm&#34;&gt;private swarm&lt;/a&gt; among people you trust.&lt;/p&gt; &#xA;&lt;h3&gt;Connect your GPU and increase Petals capacity&lt;/h3&gt; &#xA;&lt;p&gt;Run this in an &lt;a href=&#34;https://www.anaconda.com&#34;&gt;Anaconda&lt;/a&gt; env (requires Linux and Python 3.7+):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;pip install -U petals&#xA;python -m petals.cli.run_server bigscience/bloom-petals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use our &lt;a href=&#34;https://www.docker.com&#34;&gt;Docker&lt;/a&gt; image (works on Linux, macOS, and Windows with &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&#34;&gt;WSL2&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo docker run -p 31330:31330 --ipc host --gpus all --volume petals-cache:/cache --rm \&#xA;    learningathome/petals:main python -m petals.cli.run_server bigscience/bloom-petals --port 31330&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üìö See &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-server&#34;&gt;FAQ&lt;/a&gt; to learn how to configure the server to use multiple GPUs, address common issues, etc.&lt;/p&gt; &#xA;&lt;p&gt;You can also host &lt;a href=&#34;https://huggingface.co/bigscience/bloomz&#34;&gt;BLOOMZ&lt;/a&gt;, a version of BLOOM fine-tuned to follow human instructions in the zero-shot regime ‚Äî just replace &lt;code&gt;bloom-petals&lt;/code&gt; with &lt;code&gt;bloomz-petals&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üîí Hosting a server does not allow others to run custom code on your computer. Learn more about security &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üí¨ If you have any issues or feedback, let us know on &lt;a href=&#34;https://discord.gg/D9MwApKgWa&#34;&gt;our Discord server&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h3&gt;Check out tutorials, examples, and more&lt;/h3&gt; &#xA;&lt;p&gt;Basic tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Getting started: &lt;a href=&#34;https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prompt-tune BLOOM to create a personified chatbot: &lt;a href=&#34;https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prompt-tune BLOOM for text semantic classification: &lt;a href=&#34;https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Useful tools and advanced guides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://chat.petals.ml&#34;&gt;Chatbot web app&lt;/a&gt; (connects to Petals via an HTTP endpoint): &lt;a href=&#34;https://github.com/borzunov/chat.petals.ml&#34;&gt;source code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://health.petals.ml&#34;&gt;Monitor&lt;/a&gt; for the public swarm: &lt;a href=&#34;https://github.com/borzunov/health.petals.ml&#34;&gt;source code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Launch your own swarm: &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm&#34;&gt;guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run a custom foundation model: &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals&#34;&gt;guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learning more:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Frequently asked questions: &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;In-depth system description: &lt;a href=&#34;https://arxiv.org/abs/2209.01188&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìã If you build an app running BLOOM with Petals, make sure it follows the BLOOM&#39;s &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;terms of use&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Petals runs large language models like &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM-176B&lt;/a&gt; &lt;strong&gt;collaboratively&lt;/strong&gt; ‚Äî you load a small part of the model, then team up with people serving the other parts to run inference or fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;Single-batch inference runs at ‚âà 1 sec per step (token) ‚Äî &lt;a href=&#34;https://github.com/bigscience-workshop/petals#benchmarks&#34;&gt;up to 10x faster&lt;/a&gt; than offloading, enough for &lt;a href=&#34;http://chat.petals.ml&#34;&gt;chatbots&lt;/a&gt; and other interactive apps. Parallel inference reaches hundreds of tokens/sec.&lt;/li&gt; &#xA; &lt;li&gt;Beyond classic language model APIs ‚Äî you can employ any fine-tuning and sampling methods, execute custom paths through the model, or see its hidden states. You get the comforts of an API with the flexibility of PyTorch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.imgur.com/RTYF3yW.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üìö &amp;nbsp;&lt;b&gt;&lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions&#34;&gt;See FAQ&lt;/a&gt;&lt;/b&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; üìú &amp;nbsp;&lt;b&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.01188.pdf&#34;&gt;Read paper&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how to install Petals with &lt;a href=&#34;https://www.anaconda.com/products/distribution&#34;&gt;Anaconda&lt;/a&gt; on Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;pip install -U petals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t use Anaconda, you can install PyTorch in &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;any other way&lt;/a&gt;. If you want to run models with 8-bit weights, please install PyTorch with CUDA 11.x or newer for compatility with &lt;a href=&#34;https://github.com/timDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the instructions for macOS and Windows, the full requirements, and troubleshooting advice in our &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-client&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Network&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Single-batch inference&lt;br&gt;(steps/s)&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Parallel forward&lt;br&gt;(tokens/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Bandwidth&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Round-trip&lt;br&gt;latency&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Sequence length&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Batch size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;6&#34;&gt;Offloading, max. possible speed on 1x A100 &lt;sup&gt;1&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;256 Gbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.18&lt;/td&gt; &#xA;   &lt;td&gt;0.18&lt;/td&gt; &#xA;   &lt;td&gt;2.7&lt;/td&gt; &#xA;   &lt;td&gt;170.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;128 Gbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.09&lt;/td&gt; &#xA;   &lt;td&gt;0.09&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;152.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;6&#34;&gt;Petals on 14 heterogeneous servers across Europe and North America &lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;Real world&lt;/td&gt; &#xA;   &lt;td&gt;0.83&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;   &lt;td&gt;32.6&lt;/td&gt; &#xA;   &lt;td&gt;179.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;6&#34;&gt;Petals on 3 servers, with one A100 each &lt;sup&gt;3&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;1 Gbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt; 5 ms&lt;/td&gt; &#xA;   &lt;td&gt;1.71&lt;/td&gt; &#xA;   &lt;td&gt;1.54&lt;/td&gt; &#xA;   &lt;td&gt;70.0&lt;/td&gt; &#xA;   &lt;td&gt;253.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;100 Mbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt; 5 ms&lt;/td&gt; &#xA;   &lt;td&gt;1.66&lt;/td&gt; &#xA;   &lt;td&gt;1.49&lt;/td&gt; &#xA;   &lt;td&gt;56.4&lt;/td&gt; &#xA;   &lt;td&gt;182.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;100 Mbit/s&lt;/td&gt; &#xA;   &lt;td&gt;100 ms&lt;/td&gt; &#xA;   &lt;td&gt;1.23&lt;/td&gt; &#xA;   &lt;td&gt;1.11&lt;/td&gt; &#xA;   &lt;td&gt;19.7&lt;/td&gt; &#xA;   &lt;td&gt;112.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;strong&gt;An upper bound for offloading performance.&lt;/strong&gt; We base our offloading numbers on the best possible hardware setup for offloading: CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes per GPU and PCIe switches for pairs of GPUs. We assume zero latency for the upper bound estimation. In 8-bit, the model uses 1 GB of memory per billion parameters. PCIe 4.0 with 16 lanes has a throughput of 256 Gbit/s, so offloading 176B parameters takes 5.5 seconds. The throughput is twice as slow (128 Gbit/s) if we have two GPUs behind the same PCIe switch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;strong&gt;A real-world distributed setting&lt;/strong&gt; with 14 servers holding 2√ó RTX 3060, 4√ó 2080Ti, 2√ó 3090, 2√ó A4000, and 4√ó A5000 GPUs. These are personal servers and servers from university labs, spread across Europe and North America and connected to the Internet at speeds of 100‚Äì1000 Mbit/s. 4 servers operate from under firewalls.&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;strong&gt;An optimistic setup&lt;/strong&gt; that requires least communication. The client nodes have 8 CPU cores and no GPU.&lt;/p&gt; &#xA;&lt;p&gt;We provide more evaluations and discuss these results in more detail in &lt;strong&gt;Section 3.3&lt;/strong&gt; of our &lt;a href=&#34;https://arxiv.org/pdf/2209.01188.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#contributing&#34;&gt;FAQ&lt;/a&gt; on contributing.&lt;/p&gt; &#xA;&lt;h2&gt;üìú Citation&lt;/h2&gt; &#xA;&lt;p&gt;Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. &lt;a href=&#34;https://arxiv.org/abs/2209.01188&#34;&gt;Petals: Collaborative Inference and Fine-tuning of Large Models.&lt;/a&gt; &lt;em&gt;arXiv preprint arXiv:2209.01188,&lt;/em&gt; 2022.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{borzunov2022petals,&#xA;  title = {Petals: Collaborative Inference and Fine-tuning of Large Models},&#xA;  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Ryabinin, Max and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},&#xA;  journal = {arXiv preprint arXiv:2209.01188},&#xA;  year = {2022},&#xA;  url = {https://arxiv.org/abs/2209.01188}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; This project is a part of the &lt;a href=&#34;https://bigscience.huggingface.co/&#34;&gt;BigScience&lt;/a&gt; research workshop. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://petals.ml/bigscience.png&#34; width=&#34;150&#34;&gt; &lt;/p&gt;</summary>
  </entry>
</feed>