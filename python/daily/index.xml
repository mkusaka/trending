<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-28T01:33:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>donydchen/mvsplat</title>
    <updated>2024-03-28T01:33:44Z</updated>
    <id>tag:github.com,2024-03-28:/donydchen/mvsplat</id>
    <link href="https://github.com/donydchen/mvsplat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🌊[arXiv&#39;24] MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MVSplat&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation of &lt;strong&gt;MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Authors: &lt;a href=&#34;https://donydchen.github.io/&#34;&gt;Yuedong Chen&lt;/a&gt;, &lt;a href=&#34;https://haofeixu.github.io/&#34;&gt;Haofei Xu&lt;/a&gt;, &lt;a href=&#34;https://chuanxiaz.com/&#34;&gt;Chuanxia Zheng&lt;/a&gt;, &lt;a href=&#34;https://bohanzhuang.github.io/&#34;&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href=&#34;https://people.inf.ethz.ch/marc.pollefeys/&#34;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&#34;https://www.cvlibs.net/&#34;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&#34;https://personal.ntu.edu.sg/astjcham/&#34;&gt;Tat-Jen Cham&lt;/a&gt; and &lt;a href=&#34;https://jianfei-cai.github.io/&#34;&gt;Jianfei Cai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://donydchen.github.io/mvsplat/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2403.14627&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/14_E_5R6ojOWnLSrSVLVEMHnTiKsfddjU&#34;&gt;Pretrained Models&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/donydchen/mvsplat/assets/5866866/c5dc5de1-819e-462f-85a2-815e239d8ff2&#34;&gt;https://github.com/donydchen/mvsplat/assets/5866866/c5dc5de1-819e-462f-85a2-815e239d8ff2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To get started, create a conda virtual environment using Python 3.10+ and install the requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n mvsplat python=3.10&#xA;conda activate mvsplat&#xA;pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acquiring Datasets&lt;/h2&gt; &#xA;&lt;p&gt;Our MVSplat uses the same training datasets as pixelSplat. Below we quote pixelSplat&#39;s &lt;a href=&#34;https://github.com/dcharatan/pixelsplat?tab=readme-ov-file#acquiring-datasets&#34;&gt;detailed instructions&lt;/a&gt; on getting datasets.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;pixelSplat was trained using versions of the RealEstate10k and ACID datasets that were split into ~100 MB chunks for use on server cluster file systems. Small subsets of the Real Estate 10k and ACID datasets in this format can be found &lt;a href=&#34;https://drive.google.com/drive/folders/1joiezNCyQK2BvWMnfwHJpm2V77c7iYGe?usp=sharing&#34;&gt;here&lt;/a&gt;. To use them, simply unzip them into a newly created &lt;code&gt;datasets&lt;/code&gt; folder in the project root directory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you would like to convert downloaded versions of the Real Estate 10k and ACID datasets to our format, you can use the &lt;a href=&#34;https://github.com/dcharatan/real_estate_10k_tools&#34;&gt;scripts here&lt;/a&gt;. Reach out to us (pixelSplat) if you want the full versions of our processed datasets, which are about 500 GB and 160 GB for Real Estate 10k and ACID respectively.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Running the Code&lt;/h2&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;To render novel views and compute evaluation metrics from a pretrained model,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;get the &lt;a href=&#34;https://drive.google.com/drive/folders/14_E_5R6ojOWnLSrSVLVEMHnTiKsfddjU&#34;&gt;pretrained models&lt;/a&gt;, and save them to &lt;code&gt;/checkpoints&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;run the following:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# re10k&#xA;python -m src.main +experiment=re10k \&#xA;checkpointing.load=checkpoints/re10k.ckpt \&#xA;mode=test \&#xA;dataset/view_sampler=evaluation \&#xA;test.compute_scores=true&#xA;&#xA;# acid&#xA;python -m src.main +experiment=acid \&#xA;checkpointing.load=checkpoints/acid.ckpt \&#xA;mode=test \&#xA;dataset/view_sampler=evaluation \&#xA;dataset.view_sampler.index_path=assets/evaluation_index_acid.json \&#xA;test.compute_scores=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the rendered novel views will be stored under &lt;code&gt;outputs/test&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To render videos from a pretrained model, run the following&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# re10k&#xA;python -m src.main +experiment=re10k \&#xA;checkpointing.load=checkpoints/re10k.ckpt \&#xA;mode=test \&#xA;dataset/view_sampler=evaluation \&#xA;dataset.view_sampler.index_path=assets/evaluation_index_re10k_video.json \&#xA;test.save_video=true \&#xA;test.save_image=false \&#xA;test.compute_scores=false&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;Run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download the backbone pretrained weight from unimath and save to &#39;checkpoints/&#39;&#xA;wget &#39;https://s3.eu-central-1.amazonaws.com/avg-projects/unimatch/pretrained/gmdepth-scale1-resumeflowthings-scannet-5d9d7964.pth&#39; -P checkpoints&#xA;# train mvsplat&#xA;python -m src.main +experiment=re10k data_loader.train.batch_size=14&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our models are trained with a single A100 (80GB) GPU. They can also be trained on multiple GPUs with smaller RAM by setting a smaller &lt;code&gt;data_loader.train.batch_size&lt;/code&gt; per GPU.&lt;/p&gt; &#xA;&lt;h3&gt;Ablations&lt;/h3&gt; &#xA;&lt;p&gt;We also provide a collection of our &lt;a href=&#34;https://drive.google.com/drive/folders/14_E_5R6ojOWnLSrSVLVEMHnTiKsfddjU&#34;&gt;ablation models&lt;/a&gt; (under folder &#39;ablations&#39;). To evaluate them, &lt;em&gt;e.g.&lt;/em&gt;, the &#39;base&#39; model, run the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Table 3: base&#xA;python -m src.main +experiment=re10k \&#xA;checkpointing.load=checkpoints/ablations/re10k_worefine.ckpt \&#xA;mode=test \&#xA;dataset/view_sampler=evaluation \&#xA;test.compute_scores=true \&#xA;wandb.name=abl/re10k_base \&#xA;model.encoder.wo_depth_refine=true &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More running commands can be found at &lt;a href=&#34;https://raw.githubusercontent.com/donydchen/mvsplat/main/more_commands.sh&#34;&gt;more_commands.sh&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{chen2024mvsplat,&#xA;    title   = {MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images},&#xA;    author  = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},&#xA;    journal = {arXiv preprint arXiv:2403.14627},&#xA;    year    = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The project is largely based on &lt;a href=&#34;https://github.com/dcharatan/pixelsplat&#34;&gt;pixelSplat&lt;/a&gt; and has incorporated numerous code snippets from &lt;a href=&#34;https://github.com/autonomousvision/unimatch&#34;&gt;UniMatch&lt;/a&gt;. Many thanks to these two projects for their excellent contributions!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yl4579/StyleTTS2</title>
    <updated>2024-03-28T01:33:44Z</updated>
    <id>tag:github.com,2024-03-28:/yl4579/StyleTTS2</id>
    <link href="https://github.com/yl4579/StyleTTS2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models&lt;/h1&gt; &#xA;&lt;h3&gt;Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS synthesis on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2306.07691&#34;&gt;https://arxiv.org/abs/2306.07691&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Audio samples: &lt;a href=&#34;https://styletts2.github.io/&#34;&gt;https://styletts2.github.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Online demo: &lt;a href=&#34;https://huggingface.co/spaces/styletts2/styletts2&#34;&gt;Hugging Face&lt;/a&gt; (thank &lt;a href=&#34;https://github.com/fakerybakery&#34;&gt;@fakerybakery&lt;/a&gt; for the wonderful online demo)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/ha8sxdG2K4&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1197679063150637117?logo=discord&amp;amp;logoColor=white&amp;amp;label=Join%20our%20Community&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training and inference demo code for single-speaker models (LJSpeech)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Test training code for multi-speaker models (VCTK and LibriTTS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Finish demo code for multispeaker model and upload pre-trained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add a finetuning script for new speakers with base pre-trained multispeaker models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fix DDP (accelerator) for &lt;code&gt;train_second.py&lt;/code&gt; &lt;strong&gt;(I have tried everything I could to fix this but had no success, so if you are willing to help, please see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/7&#34;&gt;#7&lt;/a&gt;)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/yl4579/StyleTTS2.git&#xA;cd StyleTTS2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install python requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -U&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also install phonemizer and espeak if you want to run the demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install phonemizer&#xA;sudo apt-get install espeak-ng&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download and extract the &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34;&gt;LJSpeech dataset&lt;/a&gt;, unzip to the data folder and upsample the data to 24 kHz. The text aligner and pitch extractor are pre-trained on 24 kHz data, but you can easily change the preprocessing and re-train them using your own preprocessing. For LibriTTS, you will need to combine train-clean-360 with train-clean-100 and rename the folder train-clean-460 (see &lt;a href=&#34;https://github.com/yl4579/StyleTTS/raw/main/Data/val_list_libritts.txt&#34;&gt;val_list_libritts.txt&lt;/a&gt; as an example).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;First stage training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch train_first.py --config_path ./Configs/config.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Second stage training &lt;strong&gt;(DDP version not working, so the current version uses DP, again see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/7&#34;&gt;#7&lt;/a&gt; if you want to help)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_second.py --config_path ./Configs/config.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can run both consecutively and it will train both the first and second stages. The model will be saved in the format &#34;epoch_1st_%05d.pth&#34; and &#34;epoch_2nd_%05d.pth&#34;. Checkpoints and Tensorboard logs will be saved at &lt;code&gt;log_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The data list format needs to be &lt;code&gt;filename.wav|transcription|speaker&lt;/code&gt;, see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Data/val_list.txt&#34;&gt;val_list.txt&lt;/a&gt; as an example. The speaker labels are needed for multi-speaker models because we need to sample reference audio for style diffusion model training.&lt;/p&gt; &#xA;&lt;h3&gt;Important Configurations&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Configs/config.yml&#34;&gt;config.yml&lt;/a&gt;, there are a few important configurations to take care of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OOD_data&lt;/code&gt;: The path for out-of-distribution texts for SLM adversarial training. The format should be &lt;code&gt;text|anything&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;min_length&lt;/code&gt;: Minimum length of OOD texts for training. This is to make sure the synthesized speech has a minimum length.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_len&lt;/code&gt;: Maximum length of audio for training. The unit is frame. Since the default hop size is 300, one frame is approximately &lt;code&gt;300 / 24000&lt;/code&gt; (0.0125) second. Lowering this if you encounter the out-of-memory issue.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;multispeaker&lt;/code&gt;: Set to true if you want to train a multispeaker model. This is needed because the architecture of the denoiser is different for single and multispeaker models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;batch_percentage&lt;/code&gt;: This is to make sure during SLM adversarial training there are no out-of-memory (OOM) issues. If you encounter OOM problem, please set a lower number for this.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pre-trained modules&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils&#34;&gt;Utils&lt;/a&gt; folder, there are three pre-trained models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils/ASR&#34;&gt;ASR&lt;/a&gt; folder&lt;/strong&gt;: It contains the pre-trained text aligner, which was pre-trained on English (LibriTTS), Japanese (JVS), and Chinese (AiShell) corpus. It works well for most other languages without fine-tuning, but you can always train your own text aligner with the code here: &lt;a href=&#34;https://github.com/yl4579/AuxiliaryASR&#34;&gt;yl4579/AuxiliaryASR&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils/JDC&#34;&gt;JDC&lt;/a&gt; folder&lt;/strong&gt;: It contains the pre-trained pitch extractor, which was pre-trained on English (LibriTTS) corpus only. However, it works well for other languages too because F0 is independent of language. If you want to train on singing corpus, it is recommended to train a new pitch extractor with the code here: &lt;a href=&#34;https://github.com/yl4579/PitchExtractor&#34;&gt;yl4579/PitchExtractor&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils/PLBERT&#34;&gt;PLBERT&lt;/a&gt; folder&lt;/strong&gt;: It contains the pre-trained &lt;a href=&#34;https://arxiv.org/abs/2301.08810&#34;&gt;PL-BERT&lt;/a&gt; model, which was pre-trained on English (Wikipedia) corpus only. It probably does not work very well on other languages, so you will need to train a different PL-BERT for different languages using the repo here: &lt;a href=&#34;https://github.com/yl4579/PL-BERT&#34;&gt;yl4579/PL-BERT&lt;/a&gt;. You can also use the &lt;a href=&#34;https://huggingface.co/papercup-ai/multilingual-pl-bert&#34;&gt;multilingual PL-BERT&lt;/a&gt; which supports 14 languages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Common Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Loss becomes NaN&lt;/strong&gt;: If it is the first stage, please make sure you do not use mixed precision, as it can cause loss becoming NaN for some particular datasets when the batch size is not set properly (need to be more than 16 to work well). For the second stage, please also experiment with different batch sizes, with higher batch sizes being more likely to cause NaN loss values. We recommend the batch size to be 16. You can refer to issues &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/10&#34;&gt;#10&lt;/a&gt; and &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/11&#34;&gt;#11&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Out of memory&lt;/strong&gt;: Please either use lower &lt;code&gt;batch_size&lt;/code&gt; or &lt;code&gt;max_len&lt;/code&gt;. You may refer to issue &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/10&#34;&gt;#10&lt;/a&gt; for more information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Non-English dataset&lt;/strong&gt;: You can train on any language you want, but you will need to use a pre-trained PL-BERT model for that language. We have a pre-trained &lt;a href=&#34;https://huggingface.co/papercup-ai/multilingual-pl-bert&#34;&gt;multilingual PL-BERT&lt;/a&gt; that supports 14 languages. You may refer to &lt;a href=&#34;https://github.com/yl4579/StyleTTS/issues/10&#34;&gt;yl4579/StyleTTS#10&lt;/a&gt; and &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/70&#34;&gt;#70&lt;/a&gt; for some examples to train on Chinese datasets.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;The script is modified from &lt;code&gt;train_second.py&lt;/code&gt; which uses DP, as DDP does not work for &lt;code&gt;train_second.py&lt;/code&gt;. Please see the bold section above if you are willing to help with this problem.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_finetune.py --config_path ./Configs/config_ft.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please make sure you have the LibriTTS checkpoint downloaded and unzipped under the folder. The default configuration &lt;code&gt;config_ft.yml&lt;/code&gt; finetunes on LJSpeech with 1 hour of speech data (around 1k samples) for 50 epochs. This took about 4 hours to finish on four NVidia A100. The quality is slightly worse (similar to NaturalSpeech on LJSpeech) than LJSpeech model trained from scratch with 24 hours of speech data, which took around 2.5 days to finish on four A100. The samples can be found at &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/discussions/65#discussioncomment-7668393&#34;&gt;#65 (comment)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you are using a &lt;strong&gt;single GPU&lt;/strong&gt; (because the script doesn&#39;t work with DDP) and want to save training speed and VRAM, you can do (thank &lt;a href=&#34;https://github.com/korakoe&#34;&gt;@korakoe&lt;/a&gt; for making the script at &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/pull/100&#34;&gt;#100&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --mixed_precision=fp16 --num_processes=1 train_finetune_accelerate.py --config_path ./Configs/config_ft.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Finetune_Demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Common Issues&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Kreevoz&#34;&gt;@Kreevoz&lt;/a&gt; has made detailed notes on common issues in finetuning, with suggestions in maximizing audio quality: &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/discussions/81&#34;&gt;#81&lt;/a&gt;. Some of these also apply to training from scratch. &lt;a href=&#34;https://github.com/IIEleven11&#34;&gt;@IIEleven11&lt;/a&gt; has also made a guideline for fine-tuning: &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/discussions/128&#34;&gt;#128&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Out of memory after &lt;code&gt;joint_epoch&lt;/code&gt;&lt;/strong&gt;: This is likely because your GPU RAM is not big enough for SLM adversarial training run. You may skip that but the quality could be worse. Setting &lt;code&gt;joint_epoch&lt;/code&gt; a larger number than &lt;code&gt;epochs&lt;/code&gt; could skip the SLM advesariral training.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Demo/Inference_LJSpeech.ipynb&#34;&gt;Inference_LJSpeech.ipynb&lt;/a&gt; (single-speaker) and &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Demo/Inference_LibriTTS.ipynb&#34;&gt;Inference_LibriTTS.ipynb&lt;/a&gt; (multi-speaker) for details. For LibriTTS, you will also need to download &lt;a href=&#34;https://huggingface.co/yl4579/StyleTTS2-LibriTTS/resolve/main/reference_audio.zip&#34;&gt;reference_audio.zip&lt;/a&gt; and unzip it under the &lt;code&gt;demo&lt;/code&gt; before running the demo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The pretrained StyleTTS 2 on LJSpeech corpus in 24 kHz can be downloaded at &lt;a href=&#34;https://huggingface.co/yl4579/StyleTTS2-LJSpeech/tree/main&#34;&gt;https://huggingface.co/yl4579/StyleTTS2-LJSpeech/tree/main&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LJSpeech.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The pretrained StyleTTS 2 model on LibriTTS can be downloaded at &lt;a href=&#34;https://huggingface.co/yl4579/StyleTTS2-LibriTTS/tree/main&#34;&gt;https://huggingface.co/yl4579/StyleTTS2-LibriTTS/tree/main&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LibriTTS.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can import StyleTTS 2 and run it in your own code. However, the inference depends on a GPL-licensed package, so it is not included directly in this repository. A &lt;a href=&#34;https://github.com/NeuralVox/StyleTTS2&#34;&gt;GPL-licensed fork&lt;/a&gt; has an importable script, as well as an experimental streaming API, etc. A &lt;a href=&#34;https://pypi.org/project/styletts2/&#34;&gt;fully MIT-licensed package&lt;/a&gt; that uses gruut (albeit lower quality due to mismatch between phonemizer and gruut) is also available.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Before using these pre-trained models, you agree to inform the listeners that the speech samples are synthesized by the pre-trained models, unless you have the permission to use the voice you synthesize. That is, you agree to only use voices whose speakers grant the permission to have their voice cloned, either directly or by license before making synthesized voices public, or you have to publicly announce that these voices are synthesized if you do not have the permission to use these voices.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Common Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-pitched background noise&lt;/strong&gt;: This is caused by numerical float differences in older GPUs. For more details, please refer to issue &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/13&#34;&gt;#13&lt;/a&gt;. Basically, you will need to use more modern GPUs or do inference on CPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-trained model license&lt;/strong&gt;: You only need to abide by the above rules if you use &lt;strong&gt;the pre-trained models&lt;/strong&gt; and the voices are &lt;strong&gt;NOT&lt;/strong&gt; in the training set, i.e., your reference speakers are not from any open access dataset. For more details of rules to use the pre-trained models, please see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/37&#34;&gt;#37&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/archinetai/audio-diffusion-pytorch&#34;&gt;archinetai/audio-diffusion-pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;jik876/hifi-gan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rishikksh20/iSTFTNet-pytorch&#34;&gt;rishikksh20/iSTFTNet-pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/01-nsf&#34;&gt;nii-yamagishilab/project-NN-Pytorch-scripts/project/01-nsf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Code: MIT License&lt;/p&gt; &#xA;&lt;p&gt;Pre-Trained Models: Before using these pre-trained models, you agree to inform the listeners that the speech samples are synthesized by the pre-trained models, unless you have the permission to use the voice you synthesize. That is, you agree to only use voices whose speakers grant the permission to have their voice cloned, either directly or by license before making synthesized voices public, or you have to publicly announce that these voices are synthesized if you do not have the permission to use these voices.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenInterpreter/open-interpreter</title>
    <updated>2024-03-28T01:33:44Z</updated>
    <id>tag:github.com,2024-03-28:/OpenInterpreter/open-interpreter</id>
    <link href="https://github.com/OpenInterpreter/open-interpreter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A natural language interface for computers&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;● Open Interpreter&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/Hvz9Axh84z&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1146610656779440188?logo=discord&amp;amp;style=flat&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenInterpreter/open-interpreter/main/docs/README_JA.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ドキュメント-日本語-white.svg&#34; alt=&#34;JA doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenInterpreter/open-interpreter/main/docs/README_ZH.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/文档-中文版-white.svg&#34; alt=&#34;ZH doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenInterpreter/open-interpreter/main/docs/README_IN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hindi-white.svg?sanitize=true&#34; alt=&#34;IN doc&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=license&amp;amp;message=AGPL&amp;amp;color=white&amp;amp;style=flat&#34; alt=&#34;License&#34;&gt; &lt;br&gt; &lt;br&gt; &lt;strong&gt;We launched a new computer (the 01) with Open Interpreter at the center. &lt;a href=&#34;https://github.com/OpenInterpreter/01&#34;&gt;Star the repo →&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;br&gt;&lt;a href=&#34;https://0ggfznkwh4j.typeform.com/to/G21i9lJ2&#34;&gt;Get early access to the desktop app&lt;/a&gt;‎ ‎ |‎ ‎ &lt;a href=&#34;https://docs.openinterpreter.com/&#34;&gt;Documentation&lt;/a&gt;&lt;br&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/KillianLucas/open-interpreter/assets/63927363/08f0d493-956b-4d49-982e-67d4b20c4b56&#34; alt=&#34;poster&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;!--&lt;p align=&#34;center&#34;&gt;&#xA;&lt;strong&gt;The New Computer Update&lt;/strong&gt; introduces &lt;strong&gt;&lt;code&gt;--os&lt;/code&gt;&lt;/strong&gt; and a new &lt;strong&gt;Computer API&lt;/strong&gt;. &lt;a href=&#34;https://changes.openinterpreter.com/log/the-new-computer-update&#34;&gt;Read On →&lt;/a&gt;&#xA;&lt;/p&gt;--&gt; &#xA;&lt;br&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install open-interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Not working? Read our &lt;a href=&#34;https://docs.openinterpreter.com/getting-started/setup&#34;&gt;setup guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open Interpreter&lt;/strong&gt; lets LLMs run code (Python, Javascript, Shell, and more) locally. You can chat with Open Interpreter through a ChatGPT-like interface in your terminal by running &lt;code&gt;$ interpreter&lt;/code&gt; after installing.&lt;/p&gt; &#xA;&lt;p&gt;This provides a natural-language interface to your computer&#39;s general-purpose capabilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create and edit photos, videos, PDFs, etc.&lt;/li&gt; &#xA; &lt;li&gt;Control a Chrome browser to perform research&lt;/li&gt; &#xA; &lt;li&gt;Plot, clean, and analyze large datasets&lt;/li&gt; &#xA; &lt;li&gt;...etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;⚠️ Note: You&#39;ll be asked to approve code before it&#39;s run.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KillianLucas/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60&#34;&gt;https://github.com/KillianLucas/open-interpreter/assets/63927363/37152071-680d-4423-9af3-64836a6f7b60&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;An interactive demo is also available on Google Colab:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Along with an example voice interface, inspired by &lt;em&gt;Her&lt;/em&gt;:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1NojYGHDgxH6Y1G1oxThEBBb2AtyODBIK&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install open-interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Terminal&lt;/h3&gt; &#xA;&lt;p&gt;After installation, simply run &lt;code&gt;interpreter&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from interpreter import interpreter&#xA;&#xA;interpreter.chat(&#34;Plot AAPL and META&#39;s normalized stock prices&#34;) # Executes a single command&#xA;interpreter.chat() # Starts an interactive chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Comparison to ChatGPT&#39;s Code Interpreter&lt;/h2&gt; &#xA;&lt;p&gt;OpenAI&#39;s release of &lt;a href=&#34;https://openai.com/blog/chatgpt-plugins#code-interpreter&#34;&gt;Code Interpreter&lt;/a&gt; with GPT-4 presents a fantastic opportunity to accomplish real-world tasks with ChatGPT.&lt;/p&gt; &#xA;&lt;p&gt;However, OpenAI&#39;s service is hosted, closed-source, and heavily restricted:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No internet access.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wfhbrian.com/mastering-chatgpts-code-interpreter-list-of-python-packages/&#34;&gt;Limited set of pre-installed packages&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;100 MB maximum upload, 120.0 second runtime limit.&lt;/li&gt; &#xA; &lt;li&gt;State is cleared (along with any generated files or links) when the environment dies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Open Interpreter overcomes these limitations by running in your local environment. It has full access to the internet, isn&#39;t restricted by time or file size, and can utilize any package or library.&lt;/p&gt; &#xA;&lt;p&gt;This combines the power of GPT-4&#39;s Code Interpreter with the flexibility of your local development environment.&lt;/p&gt; &#xA;&lt;h2&gt;Commands&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; The Generator Update (0.1.5) introduced streaming:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;message = &#34;What operating system are we on?&#34;&#xA;&#xA;for chunk in interpreter.chat(message, display=False, stream=True):&#xA;  print(chunk)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive Chat&lt;/h3&gt; &#xA;&lt;p&gt;To start an interactive chat in your terminal, either run &lt;code&gt;interpreter&lt;/code&gt; from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or &lt;code&gt;interpreter.chat()&lt;/code&gt; from a .py file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.chat()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;You can also stream each chunk:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;message = &#34;What operating system are we on?&#34;&#xA;&#xA;for chunk in interpreter.chat(message, display=False, stream=True):&#xA;  print(chunk)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Programmatic Chat&lt;/h3&gt; &#xA;&lt;p&gt;For more precise control, you can pass messages directly to &lt;code&gt;.chat(message)&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.chat(&#34;Add subtitles to all videos in /videos.&#34;)&#xA;&#xA;# ... Streams output to your terminal, completes task ...&#xA;&#xA;interpreter.chat(&#34;These look great but can you make the subtitles bigger?&#34;)&#xA;&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Start a New Chat&lt;/h3&gt; &#xA;&lt;p&gt;In Python, Open Interpreter remembers conversation history. If you want to start fresh, you can reset it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.messages = []&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Save and Restore Chats&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;interpreter.chat()&lt;/code&gt; returns a List of messages, which can be used to resume a conversation with &lt;code&gt;interpreter.messages = messages&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messages = interpreter.chat(&#34;My name is Killian.&#34;) # Save messages to &#39;messages&#39;&#xA;interpreter.messages = [] # Reset interpreter (&#34;Killian&#34; will be forgotten)&#xA;&#xA;interpreter.messages = messages # Resume chat from &#39;messages&#39; (&#34;Killian&#34; will be remembered)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Customize System Message&lt;/h3&gt; &#xA;&lt;p&gt;You can inspect and configure Open Interpreter&#39;s system message to extend its functionality, modify permissions, or give it more context.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.system_message += &#34;&#34;&#34;&#xA;Run shell commands with -y so the user doesn&#39;t have to confirm them.&#xA;&#34;&#34;&#34;&#xA;print(interpreter.system_message)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Change your Language Model&lt;/h3&gt; &#xA;&lt;p&gt;Open Interpreter uses &lt;a href=&#34;https://docs.litellm.ai/docs/providers/&#34;&gt;LiteLLM&lt;/a&gt; to connect to hosted language models.&lt;/p&gt; &#xA;&lt;p&gt;You can change the model by setting the model parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter --model gpt-3.5-turbo&#xA;interpreter --model claude-2&#xA;interpreter --model command-nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Python, set the model on the object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;interpreter.llm.model = &#34;gpt-3.5-turbo&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.litellm.ai/docs/providers/&#34;&gt;Find the appropriate &#34;model&#34; string for your language model here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running Open Interpreter locally&lt;/h3&gt; &#xA;&lt;h4&gt;Terminal&lt;/h4&gt; &#xA;&lt;p&gt;Open Interpreter can use OpenAI-compatible server to run models locally. (LM Studio, jan.ai, ollama etc)&lt;/p&gt; &#xA;&lt;p&gt;Simply run &lt;code&gt;interpreter&lt;/code&gt; with the api_base URL of your inference server (for LM studio it is &lt;code&gt;http://localhost:1234/v1&lt;/code&gt; by default):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter --api_base &#34;http://localhost:1234/v1&#34; --api_key &#34;fake_key&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively you can use Llamafile without installing any third party software just by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a more detailed guide check out &lt;a href=&#34;https://www.youtube.com/watch?v=CEs51hGWuGU?si=cN7f6QhfT4edfG5H&#34;&gt;this video by Mike Bird&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to run LM Studio in the background.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;https://lmstudio.ai/&lt;/a&gt; then start it.&lt;/li&gt; &#xA; &lt;li&gt;Select a model then click &lt;strong&gt;↓ Download&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;strong&gt;↔️&lt;/strong&gt; button on the left (below 💬).&lt;/li&gt; &#xA; &lt;li&gt;Select your model at the top, then click &lt;strong&gt;Start Server&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Once the server is running, you can begin your conversation with Open Interpreter.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Local mode sets your &lt;code&gt;context_window&lt;/code&gt; to 3000, and your &lt;code&gt;max_tokens&lt;/code&gt; to 1000. If your model has different requirements, set these parameters manually (see below).&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Python&lt;/h4&gt; &#xA;&lt;p&gt;Our Python package gives you more control over each setting. To replicate and connect to LM Studio, use these settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from interpreter import interpreter&#xA;&#xA;interpreter.offline = True # Disables online features like Open Procedures&#xA;interpreter.llm.model = &#34;openai/x&#34; # Tells OI to send messages in OpenAI&#39;s format&#xA;interpreter.llm.api_key = &#34;fake_key&#34; # LiteLLM, which we use to talk to LM Studio, requires this&#xA;interpreter.llm.api_base = &#34;http://localhost:1234/v1&#34; # Point this at any OpenAI compatible server&#xA;&#xA;interpreter.chat()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Context Window, Max Tokens&lt;/h4&gt; &#xA;&lt;p&gt;You can modify the &lt;code&gt;max_tokens&lt;/code&gt; and &lt;code&gt;context_window&lt;/code&gt; (in tokens) of locally running models.&lt;/p&gt; &#xA;&lt;p&gt;For local mode, smaller context windows will use less RAM, so we recommend trying a much shorter window (~1000) if it&#39;s failing / if it&#39;s slow. Make sure &lt;code&gt;max_tokens&lt;/code&gt; is less than &lt;code&gt;context_window&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;interpreter --local --max_tokens 1000 --context_window 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Verbose mode&lt;/h3&gt; &#xA;&lt;p&gt;To help you inspect Open Interpreter we have a &lt;code&gt;--verbose&lt;/code&gt; mode for debugging.&lt;/p&gt; &#xA;&lt;p&gt;You can activate verbose mode by using its flag (&lt;code&gt;interpreter --verbose&lt;/code&gt;), or mid-chat:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ interpreter&#xA;...&#xA;&amp;gt; %verbose true &amp;lt;- Turns on verbose mode&#xA;&#xA;&amp;gt; %verbose false &amp;lt;- Turns off verbose mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Interactive Mode Commands&lt;/h3&gt; &#xA;&lt;p&gt;In the interactive mode, you can use the below commands to enhance your experience. Here&#39;s a list of available commands:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Available Commands:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;%verbose [true/false]&lt;/code&gt;: Toggle verbose mode. Without arguments or with &lt;code&gt;true&lt;/code&gt; it enters verbose mode. With &lt;code&gt;false&lt;/code&gt; it exits verbose mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;%reset&lt;/code&gt;: Resets the current session&#39;s conversation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;%undo&lt;/code&gt;: Removes the previous user message and the AI&#39;s response from the message history.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;%tokens [prompt]&lt;/code&gt;: (&lt;em&gt;Experimental&lt;/em&gt;) Calculate the tokens that will be sent with the next prompt as context and estimate their cost. Optionally calculate the tokens and estimated cost of a &lt;code&gt;prompt&lt;/code&gt; if one is provided. Relies on &lt;a href=&#34;https://docs.litellm.ai/docs/completion/token_usage#2-cost_per_token&#34;&gt;LiteLLM&#39;s &lt;code&gt;cost_per_token()&lt;/code&gt; method&lt;/a&gt; for estimated costs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;%help&lt;/code&gt;: Show the help message.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Configuration / Profiles&lt;/h3&gt; &#xA;&lt;p&gt;Open Interpreter allows you to set default behaviors using &lt;code&gt;yaml&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;p&gt;This provides a flexible way to configure the interpreter without changing command-line arguments every time.&lt;/p&gt; &#xA;&lt;p&gt;Run the following command to open the profiles directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;interpreter --profiles&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can add &lt;code&gt;yaml&lt;/code&gt; files there. The default profile is named &lt;code&gt;default.yaml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Multiple Profiles&lt;/h4&gt; &#xA;&lt;p&gt;Open Interpreter supports multiple &lt;code&gt;yaml&lt;/code&gt; files, allowing you to easily switch between configurations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;interpreter --profile my_profile.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Sample FastAPI Server&lt;/h2&gt; &#xA;&lt;p&gt;The generator update enables Open Interpreter to be controlled via HTTP REST endpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# server.py&#xA;&#xA;from fastapi import FastAPI&#xA;from fastapi.responses import StreamingResponse&#xA;from interpreter import interpreter&#xA;&#xA;app = FastAPI()&#xA;&#xA;@app.get(&#34;/chat&#34;)&#xA;def chat_endpoint(message: str):&#xA;    def event_stream():&#xA;        for result in interpreter.chat(message, stream=True):&#xA;            yield f&#34;data: {result}\n\n&#34;&#xA;&#xA;    return StreamingResponse(event_stream(), media_type=&#34;text/event-stream&#34;)&#xA;&#xA;@app.get(&#34;/history&#34;)&#xA;def history_endpoint():&#xA;    return interpreter.messages&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install fastapi uvicorn&#xA;uvicorn server:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also start a server identical to the one above by simply running &lt;code&gt;interpreter.server()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Android&lt;/h2&gt; &#xA;&lt;p&gt;The step-by-step guide for installing Open Interpreter on your Android device can be found in the &lt;a href=&#34;https://github.com/MikeBirdTech/open-interpreter-termux&#34;&gt;open-interpreter-termux repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Safety Notice&lt;/h2&gt; &#xA;&lt;p&gt;Since generated code is executed in your local environment, it can interact with your files and system settings, potentially leading to unexpected outcomes like data loss or security risks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;⚠️ Open Interpreter will ask for user confirmation before executing code.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;interpreter -y&lt;/code&gt; or set &lt;code&gt;interpreter.auto_run = True&lt;/code&gt; to bypass this confirmation, in which case:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Be cautious when requesting commands that modify files or system settings.&lt;/li&gt; &#xA; &lt;li&gt;Watch Open Interpreter like a self-driving car, and be prepared to end the process by closing your terminal.&lt;/li&gt; &#xA; &lt;li&gt;Consider running Open Interpreter in a restricted environment like Google Colab or Replit. These environments are more isolated, reducing the risks of executing arbitrary code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There is &lt;strong&gt;experimental&lt;/strong&gt; support for a &lt;a href=&#34;https://raw.githubusercontent.com/OpenInterpreter/open-interpreter/main/docs/SAFE_MODE.md&#34;&gt;safe mode&lt;/a&gt; to help mitigate some risks.&lt;/p&gt; &#xA;&lt;h2&gt;How Does it Work?&lt;/h2&gt; &#xA;&lt;p&gt;Open Interpreter equips a &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/function-calling&#34;&gt;function-calling language model&lt;/a&gt; with an &lt;code&gt;exec()&lt;/code&gt; function, which accepts a &lt;code&gt;language&lt;/code&gt; (like &#34;Python&#34; or &#34;JavaScript&#34;) and &lt;code&gt;code&lt;/code&gt; to run.&lt;/p&gt; &#xA;&lt;p&gt;We then stream the model&#39;s messages, code, and your system&#39;s outputs to the terminal as Markdown.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Thank you for your interest in contributing! We welcome involvement from the community.&lt;/p&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/OpenInterpreter/open-interpreter/main/docs/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for more details on how to get involved.&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://github.com/KillianLucas/open-interpreter/raw/main/docs/ROADMAP.md&#34;&gt;our roadmap&lt;/a&gt; to preview the future of Open Interpreter.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This software is not affiliated with OpenAI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/KillianLucas/open-interpreter/assets/63927363/1b19a5db-b486-41fd-a7a1-fe2028031686&#34; alt=&#34;thumbnail-ncu&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Having access to a junior programmer working at the speed of your fingertips ... can make new workflows effortless and efficient, as well as open the benefits of programming to new audiences.&lt;/p&gt; &#xA; &lt;p&gt;— &lt;em&gt;OpenAI&#39;s Code Interpreter Release&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt;</summary>
  </entry>
</feed>