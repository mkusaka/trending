<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-11T01:33:30Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Yuukiy/JavSP</title>
    <updated>2024-03-11T01:33:30Z</updated>
    <id>tag:github.com,2024-03-11:/Yuukiy/JavSP</id>
    <link href="https://github.com/Yuukiy/JavSP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;æ±‡æ€»å¤šç«™ç‚¹æ•°æ®çš„AVå…ƒæ•°æ®åˆ®å‰Šå™¨&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/Yuukiy/JavSP/raw/master/image/javsp_logo.png?raw=true&#34; alt=&#34;JavSP&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Jav Scraper Package&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ±‡æ€»å¤šç«™ç‚¹æ•°æ®çš„AVå…ƒæ•°æ®åˆ®å‰Šå™¨&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æå–å½±ç‰‡æ–‡ä»¶åä¸­çš„ç•ªå·ä¿¡æ¯ï¼Œè‡ªåŠ¨æŠ“å–å¹¶æ±‡æ€»å¤šä¸ªç«™ç‚¹æ•°æ®çš„ AV å…ƒæ•°æ®ï¼ŒæŒ‰ç…§æŒ‡å®šçš„è§„åˆ™åˆ†ç±»æ•´ç†å½±ç‰‡æ–‡ä»¶ï¼Œå¹¶åˆ›å»ºä¾› Embyã€Jellyfinã€Kodi ç­‰è½¯ä»¶ä½¿ç”¨çš„å…ƒæ•°æ®æ–‡ä»¶&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Docker &amp;amp; WebUI&lt;/strong&gt;: ç”±äºç²¾åŠ›æ‰€é™ï¼Œç›®å‰è¿˜æ²¡æœ‰åšDockerçš„æ”¯æŒã€‚æ­¤å¤–ï¼ŒUIç•Œé¢ä¹Ÿä¸æ˜¯&lt;a href=&#34;https://github.com/Yuukiy/JavSP/issues/148&#34;&gt;æ­¤é¡¹ç›®çš„ç›®æ ‡&lt;/a&gt;ã€‚å¦‚æœä½ éœ€è¦è¿™ä¸¤ä¸ªåŠŸèƒ½ï¼Œå¯ä»¥è¯•è¯•&lt;a href=&#34;https://github.com/tetato/JavSP-Docker&#34;&gt;@tetato/JavSP-Docker&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;i18n&lt;/strong&gt;: This project currently supports only Chinese. However, if you&#39;re willing, you can &lt;a href=&#34;https://github.com/Yuukiy/JavSP/discussions/157&#34;&gt;vote here&lt;/a&gt; for the language you&#39;d like to see added&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/Yuukiy/JavSP&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://github.com/996icu/996.ICU/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Anti%20996-blue.svg?sanitize=true&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.8-green.svg?sanitize=true&#34; alt=&#34;Python 3.8&#34;&gt; &lt;a href=&#34;https://github.com/Yuukiy/JavSP/actions/workflows/test-web-funcs.yml&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/Yuukiy/JavSP/test-web-funcs.yml?label=crawlers%20test&#34; alt=&#34;Crawlers test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Yuukiy/JavSP/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/Yuukiy/JavSP&#34; alt=&#34;Latest release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://996.icu&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/link-996.icu-red.svg?sanitize=true&#34; alt=&#34;996.icu&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;åŠŸèƒ½ç‰¹ç‚¹&lt;/h2&gt; &#xA;&lt;p&gt;ä¸‹é¢è¿™äº›æ˜¯ä¸€äº›å·²å®ç°æˆ–å¾…å®ç°çš„åŠŸèƒ½ï¼Œåœ¨é€æ¸å®ç°å’Œå®Œå–„ï¼Œå¦‚æœæƒ³åˆ°æ–°çš„åŠŸèƒ½ç‚¹ä¹Ÿä¼šåŠ è¿›æ¥ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; è‡ªåŠ¨è¯†åˆ«å½±ç‰‡ç•ªå·&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ”¯æŒå¤„ç†å½±ç‰‡åˆ†ç‰‡&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ±‡æ€»å¤šä¸ªç«™ç‚¹çš„æ•°æ®ç”ŸæˆNFOæ•°æ®æ–‡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; æ¯å¤©è‡ªåŠ¨å¯¹ç«™ç‚¹æŠ“å–å™¨è¿›è¡Œæµ‹è¯•&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; å¤šçº¿ç¨‹å¹¶è¡ŒæŠ“å–&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ä¸‹è½½é«˜æ¸…å°é¢&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; åŸºäºAIäººä½“åˆ†æè£å‰ªç´ äººç­‰éå¸¸è§„å°é¢çš„æµ·æŠ¥&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; è‡ªåŠ¨æ£€æŸ¥å’Œæ›´æ–°æ–°ç‰ˆæœ¬&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ç¿»è¯‘æ ‡é¢˜å’Œå‰§æƒ…ç®€ä»‹&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; åŒ¹é…æœ¬åœ°å­—å¹•&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ä½¿ç”¨å°ç¼©ç•¥å›¾åˆ›å»ºæ–‡ä»¶å¤¹å°é¢&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ä¿æŒä¸åŒç«™ç‚¹é—´ genre åˆ†ç±»çš„ç»Ÿä¸€&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ä¸åŒçš„è¿è¡Œæ¨¡å¼ï¼ˆæŠ“å–æ•°æ®+æ•´ç†ï¼Œä»…æŠ“å–æ•°æ®ï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å¯é€‰ï¼šæ‰€æœ‰ç«™ç‚¹å‡æŠ“å–å¤±è´¥æ—¶ç”±äººå·¥ä»‹å…¥&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å®‰è£…&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;æƒ³è¦å¿«é€Ÿä¸Šæ‰‹ï¼Ÿ&lt;/p&gt; &lt;p&gt;å‰å¾€&lt;a href=&#34;https://github.com/Yuukiy/JavSP/releases/latest&#34;&gt;è½¯ä»¶å‘å¸ƒé¡µ&lt;/a&gt;ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„è½¯ä»¶ï¼Œæ— éœ€å®‰è£…é¢å¤–å·¥å…·ï¼Œå¼€ç®±å³ç”¨&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ›´å–œæ¬¢æºä»£ç ï¼Ÿ&lt;/p&gt; &lt;p&gt;è¯·ç¡®ä¿å·²å®‰è£… Python ï¼ˆæ­¤é¡¹ç›®ä»¥ Python 3.8 å¼€å‘ï¼‰&lt;/p&gt; &lt;pre&gt;&lt;code&gt; git clone https://github.com/Yuukiy/JavSP.git&#xA; cd JavSP&#xA; pip install -r requirements.txt&#xA; python JavSP.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ä½¿ç”¨&lt;/h2&gt; &#xA;&lt;p&gt;è½¯ä»¶å¼€ç®±å³ç”¨ï¼Œé¦–æ¬¡è¿è¡Œæ—¶ä¼šåœ¨è½¯ä»¶ç›®å½•ä¸‹ç”Ÿæˆé»˜è®¤çš„é…ç½®æ–‡ä»¶ &lt;code&gt;config.ini&lt;/code&gt;ã€‚å¦‚æœæƒ³è®©è½¯ä»¶æ›´ç¬¦åˆä½ çš„ä½¿ç”¨éœ€æ±‚ï¼Œä¹Ÿè®¸ä½ éœ€è¦æ›´æ”¹é…ç½®æ–‡ä»¶:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ä»¥ä»»æ„æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€ &lt;code&gt;config.ini&lt;/code&gt;ï¼Œæ ¹æ®å„ä¸ªé…ç½®é¡¹çš„è¯´æ˜é€‰æ‹©ä½ éœ€è¦çš„é…ç½®å³å¯ã€‚&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;æ­¤å¤–è½¯ä»¶ä¹Ÿæ”¯æŒä»å‘½ä»¤è¡ŒæŒ‡å®šè¿è¡Œå‚æ•°ï¼ˆå‘½ä»¤è¡Œå‚æ•°çš„ä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ï¼‰ã€‚è¿è¡Œ &lt;code&gt;JavSP -h&lt;/code&gt; æŸ¥çœ‹æ”¯æŒçš„å‚æ•°åˆ—è¡¨&lt;/p&gt; &#xA;&lt;p&gt;æ›´è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜è¯·å‰å¾€ &lt;a href=&#34;https://github.com/Yuukiy/JavSP/wiki&#34;&gt;JavSP Wiki&lt;/a&gt; æŸ¥çœ‹&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœä½¿ç”¨çš„æ—¶å€™é‡åˆ°é—®é¢˜ä¹Ÿæ¬¢è¿ç»™æˆ‘åé¦ˆğŸ˜Š&lt;/p&gt; &#xA;&lt;h2&gt;é—®é¢˜åé¦ˆ&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœä½¿ç”¨ä¸­é‡åˆ°äº† Bugï¼Œè¯·&lt;a href=&#34;https://github.com/Yuukiy/JavSP/issues&#34;&gt;å‰å¾€ Issue åŒºåé¦ˆ&lt;/a&gt;ï¼ˆæé—®å‰è¯·å…ˆæœç´¢æ˜¯å¦å·²æœ‰ç±»ä¼¼é—®é¢˜ï¼‰&lt;/p&gt; &#xA;&lt;h2&gt;å‚ä¸è´¡çŒ®&lt;/h2&gt; &#xA;&lt;p&gt;æ­¤é¡¹ç›®ä¸éœ€è¦æèµ ã€‚å¦‚æœä½ æƒ³è¦å¸®åŠ©æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼Œæ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼å‚ä¸è¿›æ¥ï¼ˆå¹¶ä¸ä»…å±€é™äºä»£ç ï¼‰ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;å¸®åŠ©æ’°å†™å’Œæ”¹è¿›Wiki&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¸®åŠ©å®Œå–„å•å…ƒæµ‹è¯•æ•°æ®ï¼ˆä¸å¿…éè¦å†™ä»£ç ï¼Œä¾‹å¦‚å¦‚æœä½ å‘ç°æœ‰æŸç³»åˆ—çš„ç•ªå·è¯†åˆ«ä¸å‡†ç¡®ï¼Œæ€»ç»“ä¸€ä¸‹æissueä¹Ÿæ˜¯å¾ˆå¥½çš„ï¼‰&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¸®åŠ©ç¿»è¯‘ genre&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bugfix / æ–°åŠŸèƒ½ï¼Ÿæ¬¢è¿å‘ Pull Request&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;è¦ä¸è€ƒè™‘ç‚¹ä¸ª Star ?ï¼ˆæˆ‘ä¼šå¾ˆå¼€å¿ƒçš„ï¼‰&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;è®¸å¯&lt;/h2&gt; &#xA;&lt;p&gt;æ­¤é¡¹ç›®çš„æ‰€æœ‰æƒåˆ©ä¸è®¸å¯å— GPL-3.0 License ä¸ &lt;a href=&#34;https://github.com/996icu/996.ICU/raw/master/LICENSE_CN&#34;&gt;Anti 996 License&lt;/a&gt; å…±åŒé™åˆ¶ã€‚æ­¤å¤–ï¼Œå¦‚æœä½ ä½¿ç”¨æ­¤é¡¹ç›®ï¼Œè¡¨æ˜ä½ è¿˜é¢å¤–æ¥å—ä»¥ä¸‹æ¡æ¬¾ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;æœ¬è½¯ä»¶ä»…ä¾›å­¦ä¹  Python å’ŒæŠ€æœ¯äº¤æµä½¿ç”¨&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;è¯·å‹¿åœ¨å¾®åšã€å¾®ä¿¡ç­‰å¢™å†…çš„å…¬å…±ç¤¾äº¤å¹³å°ä¸Šå®£ä¼ æ­¤é¡¹ç›®&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç”¨æˆ·åœ¨ä½¿ç”¨æœ¬è½¯ä»¶æ—¶ï¼Œè¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç¦æ­¢å°†æœ¬è½¯ä»¶ç”¨äºå•†ä¸šç”¨é€”&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Yuukiy/JavSP&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Vahe1994/AQLM</title>
    <updated>2024-03-11T01:33:30Z</updated>
    <id>tag:github.com,2024-03-11:/Vahe1994/AQLM</id>
    <link href="https://github.com/Vahe1994/AQLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Pytorch repository for Extreme Compression of Large Language Models via Additive Quantization https://arxiv.org/pdf/2401.06118.pdf&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AQLM&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch implementation for &lt;a href=&#34;https://arxiv.org/pdf/2401.06118.pdf&#34;&gt;Extreme Compression of Large Language Models via Additive Quantization&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;Learn how to run the prequantized models using this Google Colab examples:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Basic AQLM &lt;br&gt; generation&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Streaming with &lt;br&gt; GPU/CPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference with CUDA &lt;br&gt; graphs (3x speedup)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Fine-tuning &lt;br&gt; with PEFT&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/colab_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;AQLM In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/streaming_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;AQLM In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/aqlm_cuda_graph.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/aqlm_2bit_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;This repository is currently designed to work with models of &lt;code&gt;LLaMA&lt;/code&gt;, &lt;code&gt;Mistral&lt;/code&gt; and &lt;code&gt;Mixtral&lt;/code&gt; families. The models reported below use &lt;strong&gt;full model fine-tuning&lt;/strong&gt; as described in appendix A, with cross-entropy objective with teacher logits.&lt;/p&gt; &#xA;&lt;p&gt;We provide a number of prequantized models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;AQLM scheme&lt;/th&gt; &#xA;   &lt;th&gt;WikiText 2 PPL&lt;/th&gt; &#xA;   &lt;th&gt;Model size, Gb&lt;/th&gt; &#xA;   &lt;th&gt;Hub link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;5.92&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b&lt;/td&gt; &#xA;   &lt;td&gt;2x8&lt;/td&gt; &#xA;   &lt;td&gt;6.69&lt;/td&gt; &#xA;   &lt;td&gt;2.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-7b-AQLM-2Bit-2x8-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7b&lt;/td&gt; &#xA;   &lt;td&gt;8x8&lt;/td&gt; &#xA;   &lt;td&gt;6.61&lt;/td&gt; &#xA;   &lt;td&gt;2.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-7b-AQLM-2Bit-8x8-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-13b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;5.22&lt;/td&gt; &#xA;   &lt;td&gt;4.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-13b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;3.83&lt;/td&gt; &#xA;   &lt;td&gt;18.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-70b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70b&lt;/td&gt; &#xA;   &lt;td&gt;2x8&lt;/td&gt; &#xA;   &lt;td&gt;4.21&lt;/td&gt; &#xA;   &lt;td&gt;18.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Llama-2-70b-AQLM-2Bit-2x8-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral-8x7b&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;3.35&lt;/td&gt; &#xA;   &lt;td&gt;12.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral-8x7b-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;12.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ISTA-DASLab/Mixtral-8x7B-Instruct-v0_1-AQLM-2Bit-1x16-hf&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference kernels&lt;/h3&gt; &#xA;&lt;p&gt;AQLM quantization setpus vary mainly on the number of codebooks used as well as the codebook sizes in bits. The most popular setups, as well as inference kernels they support are:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kernel&lt;/th&gt; &#xA;   &lt;th&gt;Number of codebooks&lt;/th&gt; &#xA;   &lt;th&gt;Codebook size, bits&lt;/th&gt; &#xA;   &lt;th&gt;Scheme Notation&lt;/th&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Speedup&lt;/th&gt; &#xA;   &lt;th&gt;Fast GPU inference&lt;/th&gt; &#xA;   &lt;th&gt;Fast CPU inference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Triton&lt;/td&gt; &#xA;   &lt;td&gt;K&lt;/td&gt; &#xA;   &lt;td&gt;N&lt;/td&gt; &#xA;   &lt;td&gt;KxN&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~0.7x&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âŒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;1x16&lt;/td&gt; &#xA;   &lt;td&gt;Best&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~1.3x&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âŒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;2x8&lt;/td&gt; &#xA;   &lt;td&gt;OK&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~3.0x&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âŒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Numba&lt;/td&gt; &#xA;   &lt;td&gt;K&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;Kx8&lt;/td&gt; &#xA;   &lt;td&gt;Good&lt;/td&gt; &#xA;   &lt;td&gt;Up to ~4.0x&lt;/td&gt; &#xA;   &lt;td&gt;âŒ&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;To run the models, one would have to install an inference library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install aqlm[gpu,cpu]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;, specifying either &lt;code&gt;gpu&lt;/code&gt;, &lt;code&gt;cpu&lt;/code&gt; or both based on one&#39;s inference setting.&lt;/p&gt; &#xA;&lt;p&gt;Then, one can use the familiar &lt;code&gt;.from_pretrained&lt;/code&gt; method provided by the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM&#xA;&#xA;quantized_model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf&#34;,&#xA;    trust_remote_code=True, torch_dtype=&#34;auto&#34;&#xA;).cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice that &lt;code&gt;torch_dtype&lt;/code&gt; should be set to either &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;&#34;auto&#34;&lt;/code&gt; on GPU and &lt;code&gt;torch.float32&lt;/code&gt; on CPU. After that, the model can be used exactly the same as one would use and unquantized model.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install packages from &lt;code&gt;requirements.txt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loading / caching datasets and tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;The script will require downloading and caching locally the relevant tokenizer and the datasets. They will be saved in default Huggingface Datasets directory unless alternative location is provided by env variables. See &lt;a href=&#34;https://huggingface.co/docs/datasets/main/en/cache#cache-directory&#34;&gt;relevant Datasets documentation section&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;When quantizing models with AQLM, we recommend that you use a subset of the original data the model was trained on.&lt;/p&gt; &#xA;&lt;p&gt;For Llama-2 models, the closest available dataset is &lt;a href=&#34;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample&#34;&gt;RedPajama&lt;/a&gt; . To load subset of RedPajama provide &#34;pajama&#34; in --dataset argument. This will process nsamples data and tokenize it using provided model tokenizer.&lt;/p&gt; &#xA;&lt;p&gt;Additionally we provide tokenized Redpajama for LLama and Solar/Mistral models for 4096 context lengths stored in &lt;a href=&#34;https://huggingface.co/datasets/Vahe1994/AQLM&#34;&gt;Hunggingface&lt;/a&gt; . To load it, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from huggingface_hub import hf_hub_download&#xA;&#xA;hf_hub_download(repo_id=&#34;Vahe1994/AQLM&#34;, filename=&#34;data/name.pth&#34;,repo_type=&#34;dataset&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use downloaded data from HF, place it in data folder(optional) and set correct path to it in &#34;--dataset&#34; argument in main.py.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; These subsets are already processed with the corresponding model tokenizer. If you want to quantize another model (e.g. mistral/mixtral), please re-tokenize the data with provided script in src/datautils.&lt;/p&gt; &#xA;&lt;h3&gt;WandB logging&lt;/h3&gt; &#xA;&lt;p&gt;One can optionally log the data to &lt;code&gt;Weights and Biases&lt;/code&gt; service (wandb). Run &lt;code&gt;pip install wandb&lt;/code&gt; for W&amp;amp;B logging. Specify &lt;code&gt;$WANDB_ENTITY&lt;/code&gt;, &lt;code&gt;$WANDB_PROJECT&lt;/code&gt;, &lt;code&gt;$WANDB_NAME&lt;/code&gt; environment variables prior to running experiments. use &lt;code&gt;--wandb&lt;/code&gt; argument to enable logging&lt;/p&gt; &#xA;&lt;h3&gt;GPU and RAM requirements&lt;/h3&gt; &#xA;&lt;p&gt;This code was developed and tested using a several A100 GPU with 80GB GPU RAM. You can use the &lt;code&gt;--offload activations&lt;/code&gt; option to reduce VRAM usage. For &lt;code&gt;Language Model Evaluation Harness&lt;/code&gt; evaluation one needs to have enough memory to load whole model + activation tensors on one or several devices.&lt;/p&gt; &#xA;&lt;h3&gt;Quantization time&lt;/h3&gt; &#xA;&lt;p&gt;AQLM quantization takes considerably longer to calibrate than simpler quantization methods such as GPTQ. This only impacts quantization time, not inference time.&lt;/p&gt; &#xA;&lt;p&gt;For instance, quantizing a 7B model with default configuration takes about 1 day on a single A100 gpu. Similarly, quantizing a 70B model on a single GPU would take 10-14 days. If you have multiple GPUs with fast interconnect, you can run AQLM multi-gpu to speed up comparison - simply set CUDA_VISIBLE_DEVICES for multiple GPUs. Quantizing 7B model on two gpus reduces quantization time to ~14.5 hours. Similarly, quantizing a 70B model on 8 x A100 GPUs takes 3 days 18 hours.&lt;/p&gt; &#xA;&lt;p&gt;If you need to speed up quantization without adding more GPUs, you may also increase --relative_mse_tolerance , --finetune_relative_mse_tolerance or set --init_max_points_per_centroid . However, that usually comes at a cost of reduced model accuracy.&lt;/p&gt; &#xA;&lt;h3&gt;Model downloading&lt;/h3&gt; &#xA;&lt;p&gt;The code requires the LLaMA model to be downloaded in Huggingface format and saved locally. The scripts below assume that &lt;code&gt;$TRANSFORMERS_CACHE&lt;/code&gt; variable points to the Huggingface Transformers cache folder. To download and cache the models, run this in the same environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;model_name = &#34;meta-llama/Llama-2-7b-hf&#34;  # or whatever else you wish to download&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=&#34;auto&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=&#34;auto&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to quantize a model with AQLM&lt;/h3&gt; &#xA;&lt;p&gt;This script compresses the model and then tests its performance in terms of perplexity using WikiText2, C4, and Penn Treebank datasets.&lt;/p&gt; &#xA;&lt;p&gt;The command to launch the script should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0   # or e.g. 0,1,2,3&#xA;export MODEL_PATH=&amp;lt;PATH_TO_MODEL_ON_HUB&amp;gt;&#xA;export DATASET_PATH=&amp;lt;INSERT DATASET NAME OR PATH TO CUSTOM DATA&amp;gt;&#xA;export SAVE_PATH=/path/to/save/quantized/model/&#xA;export WANDB_PROJECT=MY_AQ_EXPS&#xA;export WANDB_NAME=COOL_EXP_NAME&#xA;&#xA;python main.py $MODEL_PATH $DATASET_PATH --nsamples=1024 \&#xA; --num_codebooks=1 --nbits_per_codebook=16 --in_group_size=8 \&#xA; --relative_mse_tolerance=0.01 --finetune_relative_mse_tolerance=0.001 \&#xA; --finetune_batch_size=32 --local_batch_size=1 --offload_activations \&#xA; --wandb --save $SAVE_PATH&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Main CLI arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; - by default, the code will use all available GPUs. If you want to use specific GPUs (or one GPU), use this variable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - a path to either hugginface hub (e.g. meta-llama/Llama-2-7b-hf) or a local folder with transformers model and a tokenizer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DATASET_PATH&lt;/code&gt; - either a path to calibration data (see above) or a standard dataset &lt;code&gt;[c4, ptb, wikitext2]&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;for llama-2 models, you can use &lt;code&gt;DATASET_PATH=./data/red_pajama_n=1024_4096_context_length.pth&lt;/code&gt; for a slice of RedPajama (up to 1024 samples)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--nsamples&lt;/code&gt; - the number of calibration data &lt;em&gt;sequences&lt;/em&gt;. If this parameter is not set, take all calibration data avaialble.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_codebooks&lt;/code&gt; - number of codebooks per layer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--nbits_per_codebook&lt;/code&gt; - each codebook will contain 2 ** nbits_per_codebook vectors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--in_group_size&lt;/code&gt; - how many weights are quantized together (aka &#34;g&#34; in the arXiv paper)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--finetune_batch_size&lt;/code&gt; - (for fine-tuning only) the total number of sequences used for each optimization step&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--local_batch_size&lt;/code&gt; - when accumulating finetune_batch_size, process this many samples per GPU per forward pass (affects GPU RAM usage)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--relative_mse_tolerance&lt;/code&gt;- (for initial calibration) - stop training when (current_epoch_mse / previous_epoch_mse) &amp;gt; (1 - relative_mse_tolerance)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--finetune_relative_mse_tolerance&lt;/code&gt;- same, but for fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--offload_activations&lt;/code&gt; -- during calibration, move activations from GPU memory to RAM. This reduces VRAM usage while slowing calibration by ~10% (depending on your hardware).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--save&lt;/code&gt; -- path to save/load quantized model. (see also: &lt;code&gt;--load&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--wandb&lt;/code&gt; - if this parameter is set, the code will log results to wandb&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are additional hyperparameters aviailable. Run &lt;code&gt;python main.py --help&lt;/code&gt; for more details on command line arguments, including compression parameters.&lt;/p&gt; &#xA;&lt;h3&gt;Zero-shot benchmarks via LM Evaluation Harness&lt;/h3&gt; &#xA;&lt;p&gt;To perform zero-shot evaluation, we use &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;Language Model Evaluation Harness&lt;/a&gt; framework with slight modifications. This repository contains a copy of LM Evaluation Harness repo from early 2023 in &lt;code&gt;lm-eval-harness&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Before running the code make sure that you have all the requirements and dependencies of &lt;code&gt;lm-eval-harness&lt;/code&gt; installed. To install them run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r lm-evaluation-harness/requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The main script launching the evaluation procedure is &lt;code&gt;lmeval.py&lt;/code&gt; .&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_VISIBLE_DEVICES=0,1,2,3  # optional: select GPUs&#xA;export QUANTZED_MODEL=&amp;lt;PATH_TO_SAVED_QUANTIZED_MODEL_FROM_MAIN.py&amp;gt;&#xA;export MODEL_PATH=&amp;lt;INSERT_PATH_TO_ORIINAL_MODEL_ON_HUB&amp;gt;&#xA;export DATASET=&amp;lt;INSERT DATASET NAME OR PATH TO CUSTOM DATA&amp;gt;&#xA;export WANDB_PROJECT=MY_AQ_LM_EVAL&#xA;export WANDB_NAME=COOL_EVAL_NAME&#xA;&#xA;python lmeval.py \&#xA;    --model hf-causal \&#xA;    --model_args pretrained=$MODEL_PATH,dtype=float16,use_accelerate=True \&#xA;    --load $QUANTZED_MODEL \&#xA;    --tasks winogrande,piqa,hellaswag,arc_easy,arc_challenge \&#xA;    --batch_size 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Preparing models for inference&lt;/h3&gt; &#xA;&lt;p&gt;To convert a model into a &lt;em&gt;Hugging Face&lt;/em&gt; compatible format, use &lt;code&gt;convert_to_hf.py&lt;/code&gt; with corresponding arguments:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt; - the original pretrained model (corresponds to &lt;code&gt;MODEL_PATH&lt;/code&gt; of &lt;code&gt;main.py&lt;/code&gt;, e.g. &lt;code&gt;meta-llama/Llama-2-7b-hf&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--in_path&lt;/code&gt; - the folder containing an initially quantized model (corresponds to &lt;code&gt;--save&lt;/code&gt; of &lt;code&gt;main.py&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--out_path&lt;/code&gt; - the folder to save &lt;code&gt;transformers&lt;/code&gt; model to.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The conversion automatically&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute something substantial (more than a typo), please open an issue first. We use black and isort for all pull requests. Before committing your code run &lt;code&gt;black . &amp;amp;&amp;amp; isort .&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you found this work useful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{egiazarian2024extreme,&#xA;      title={Extreme Compression of Large Language Models via Additive Quantization}, &#xA;      author={Vage Egiazarian and Andrei Panferov and Denis Kuznedelev and Elias Frantar and Artem Babenko and Dan Alistarh},&#xA;      year={2024},&#xA;      eprint={2401.06118},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>hatchet-dev/hatchet</title>
    <updated>2024-03-11T01:33:30Z</updated>
    <id>tag:github.com,2024-03-11:/hatchet-dev/hatchet</id>
    <link href="https://github.com/hatchet-dev/hatchet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A distributed, fault-tolerant task queue&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://framerusercontent.com/images/KBMnpSO12CyE6UANhf4mhrg6na0.png?scale-down-to=200&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://framerusercontent.com/images/KBMnpSO12CyE6UANhf4mhrg6na0.png?scale-down-to=200&#34;&gt; &#xA;  &lt;a href=&#34;https://hatchet.run&#34;&gt; &lt;img alt=&#34;Hatchet Logo&#34; src=&#34;https://framerusercontent.com/images/KBMnpSO12CyE6UANhf4mhrg6na0.png?scale-down-to=200&#34;&gt; &lt;/a&gt; &#xA; &lt;/picture&gt; &#xA; &lt;h3&gt;A Distributed, Fault-Tolerant Task Queue&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://docs.hatchet.run&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-docs.hatchet.run-3F16E4&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-purple.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pkg.go.dev/github.com/hatchet-dev/hatchet&#34;&gt;&lt;img src=&#34;https://pkg.go.dev/badge/github.com/hatchet-dev/hatchet.svg?sanitize=true&#34; alt=&#34;Go Reference&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/@hatchet-dev/typescript-sdk&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dm/%40hatchet-dev%2Ftypescript-sdk&#34; alt=&#34;NPM Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/ZMeUafwH89&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1088927970518909068?style=social&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hatchet_dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/hatchet-dev.svg?style=social&amp;amp;label=Follow%20%40hatchet-dev&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hatchet-dev/hatchet&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hatchet-dev/hatchet?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.hatchet.run&#34;&gt;Documentation&lt;/a&gt; Â· &lt;a href=&#34;https://hatchet.run&#34;&gt;Website&lt;/a&gt; Â· &lt;a href=&#34;https://github.com/hatchet-dev/hatchet/issues&#34;&gt;Issues&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;What is Hatchet?&lt;/h3&gt; &#xA;&lt;p&gt;Hatchet replaces difficult to manage legacy queues or pub/sub systems so you can design durable workloads that recover from failure and solve for problems like &lt;strong&gt;concurrency&lt;/strong&gt;, &lt;strong&gt;fairness&lt;/strong&gt;, and &lt;strong&gt;rate limiting&lt;/strong&gt;. Instead of managing your own task queue or pub/sub system, you can use Hatchet to distribute your functions between a set of workers with minimal configuration or infrastructure:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;500&#34; height=&#34;500&#34; src=&#34;https://github.com/hatchet-dev/hatchet/assets/25448214/c3defa1e-d9d9-4419-94e5-b4ea4a748f8d&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What Makes Hatchet Great?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;âš¡ï¸ &lt;strong&gt;Ultra-low Latency and High Throughput Scheduling:&lt;/strong&gt; Hatchet is built on a low-latency queue (&lt;code&gt;25ms&lt;/code&gt; average start), perfectly balancing real-time interaction capabilities with the reliability required for mission-critical tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;â˜®ï¸ &lt;strong&gt;Concurrency, Fairness, and Rate Limiting:&lt;/strong&gt; Implement FIFO, LIFO, Round Robin, and Priority Queues with Hatchetâ€™s built-in strategies, designed to circumvent common scaling pitfalls with minimal configuration. &lt;a href=&#34;https://docs.hatchet.run&#34;&gt;Read Docs â†’&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸ”¥ğŸ§¯ &lt;strong&gt;Resilience by Design:&lt;/strong&gt; With customizable retry policies and integrated error handling, Hatchet ensures your operations recover swiftly from transient failures. You can break large jobs down into small tasks so you can finish a run without rerunning work. &lt;a href=&#34;https://docs.hatchet.run&#34;&gt;Read Docs â†’&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhanced Visibility and Control:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Observability.&lt;/strong&gt; All of your runs are fully searchable, allowing you to quickly identify issues. We track latency, error rates, or custom metrics in your run.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(Practical) Durable Execution.&lt;/strong&gt; Replay events and manually pick up execution from specific steps in your workflow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cron.&lt;/strong&gt; Set recurring schedules for functions runs to execute.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;One-Time Scheduling.&lt;/strong&gt; Schedule a function run to execute at a specific time and date in the future.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spike Protection.&lt;/strong&gt; Smooth out spikes in traffic and only execute what your system can handle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incremental Streaming.&lt;/strong&gt; Subscribe to updates as your functions progress in the background worker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example Use Cases:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fairness for Generative AI:&lt;/strong&gt; Don&#39;t let busy users overwhelm your system. Hatchet lets you distribute requests to your workers fairly with configurable policies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Batch Processing for Document Indexing:&lt;/strong&gt; Hatchet can handle large-scale batch processing of documents, images, and other data and resume mid-job on failure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Workflow Orchestration for Multi-Modal Systems:&lt;/strong&gt; Hatchet can handle orchestrating multi-modal inputs and outputs, with full DAG-style execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Correctness for Event-Based Processing:&lt;/strong&gt; Respond to external events or internal events within your system and replay events automatically.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Hatchet supports your technology stack with open-source SDKs for Python, Typescript, and Go. To get started, see the Hatchet documentation &lt;a href=&#34;https://docs.hatchet.run/home/quickstart/installation&#34;&gt;here&lt;/a&gt;, or check out our quickstart repos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-go-quickstart&#34;&gt;Go SDK Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-python-quickstart&#34;&gt;Python SDK Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-typescript-quickstart&#34;&gt;Typescript SDK Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SDK repositories&lt;/h3&gt; &#xA;&lt;p&gt;Hatchet comes with a native Go SDK. The following SDKs are also available:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hatchet-dev/hatchet-typescript&#34;&gt;Typescript SDK&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you encounter any issues with the SDKs, please submit an issue in the respective repository.&lt;/p&gt; &#xA;&lt;h4&gt;Is there a managed cloud version of Hatchet?&lt;/h4&gt; &#xA;&lt;p&gt;Yes, we are offering a have a cloud version to select companies while in beta who are helping to build and shape the product. Please &lt;a href=&#34;mailto:contact@hatchet.run&#34;&gt;reach out&lt;/a&gt; or &lt;a href=&#34;https://hatchet.run/request-access&#34;&gt;request access&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h4&gt;Is there a self-hosted version of Hatchet?&lt;/h4&gt; &#xA;&lt;p&gt;Yes, instructions for self-hosting our open source docker containers can be found in our &lt;a href=&#34;https://docs.hatchet.run/self-hosting/docker-compose&#34;&gt;documentation&lt;/a&gt;. Please &lt;a href=&#34;mailto:contact@hatchet.run&#34;&gt;reach out&lt;/a&gt; if you&#39;re interested in support.&lt;/p&gt; &#xA;&lt;h2&gt;How does this compare to alternatives (Celery, BullMQ)?&lt;/h2&gt; &#xA;&lt;p&gt;Why build another managed queue? We wanted to build something with the benefits of full transactional enqueueing - particularly for dependent, DAG-style execution - and felt strongly that Postgres solves for 99.9% of queueing use-cases better than most alternatives (Celery uses Redis or RabbitMQ as a broker, BullMQ uses Redis). Since the introduction of &lt;code&gt;SKIP LOCKED&lt;/code&gt; and the milestones of recent PG releases (like active-active replication), it&#39;s becoming more feasible to horizontally scale Postgres across multiple regions and vertically scale to 10k TPS or more. Many queues (like BullMQ) are built on Redis and data loss can occur when suffering OOM if you&#39;re not careful, and using PG helps avoid an entire class of problems.&lt;/p&gt; &#xA;&lt;p&gt;We also wanted something that was significantly easier to use and debug for application developers. A lot of times the burden of building task observability falls on the infra/platform team (for example, asking the infra team to build a Grafana view for their tasks based on exported prom metrics). We&#39;re building this type of observability directly into Hatchet.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please submit any bugs that you encounter via Github issues. However, please reach out on &lt;a href=&#34;https://discord.gg/ZMeUafwH89&#34;&gt;Discord&lt;/a&gt; before submitting a feature request - as the project is very early, we&#39;d like to build a solid foundation before adding more complex features.&lt;/p&gt; &#xA;&lt;h2&gt;I&#39;d Like to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;See the contributing docs &lt;a href=&#34;https://docs.hatchet.run/contributing&#34;&gt;here&lt;/a&gt;, and please let us know what you&#39;re interesting in working on in the #contributing channel on &lt;a href=&#34;https://discord.gg/ZMeUafwH89&#34;&gt;Discord&lt;/a&gt;. This will help us shape the direction of the project and will make collaboration much easier!&lt;/p&gt;</summary>
  </entry>
</feed>