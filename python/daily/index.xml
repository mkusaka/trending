<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-14T01:36:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QwenLM/Qwen2.5-Coder</title>
    <updated>2024-11-14T01:36:33Z</updated>
    <id>tag:github.com,2024-11-14:/QwenLM/Qwen2.5-Coder</id>
    <link href="https://github.com/QwenLM/Qwen2.5-Coder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen2.5-Coder is the code version of Qwen2.5, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/qwen2.5-coder-logo&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/main_fig_32b_white.jpg&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üíª &lt;a href=&#34;https://www.kaggle.com/models/qwen-lm/qwen2.5-coder&#34;&gt;Kaggle&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-coder-family&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; üñ•Ô∏è &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | üñº &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts&#34;&gt;Artifacts&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìÑ&lt;a href=&#34;https://arxiv.org/abs/2409.12186&#34;&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen2.5-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;h1&gt;Qwen2.5-Coder Series: Powerful, Diverse, Practical.&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Today, we are excited to open source the ‚ÄúPowerful‚Äù, ‚ÄúDiverse‚Äù, and ‚ÄúPractical‚Äù &lt;strong&gt;Qwen2.5-Coder&lt;/strong&gt; series (formerly known as CodeQwen1.5), dedicated to continuously promoting the development of Open CodeLLMs.&lt;/p&gt; &#xA;&lt;p&gt;üíª Powerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills;&lt;/p&gt; &#xA;&lt;p&gt;üìö Diverse: Building on the previously open-sourced two sizes of 1.5B / 7B, this release brings four model sizes, including 0.5B / 3B / 14B / 32B. As of now, Qwen2.5-Coder has covered six mainstream model sizes to meet the needs of different developers;&lt;/p&gt; &#xA;&lt;p&gt;üõ† Practical: We explore the practicality of Qwen2.5-Coder in two scenarios, including code assistants and Artifacts, with some examples showcasing the potential applications of Qwen2.5-Coder in real-world scenarios;&lt;/p&gt; &#xA;&lt;h2&gt;basic information&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;‚ú® Supporting long context understanding and generation with the context length of 128K tokens;&lt;/li&gt; &#xA; &lt;li&gt;‚ú® Supporting 92 coding languages;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#39;ada&#39;, &#39;agda&#39;, &#39;alloy&#39;, &#39;antlr&#39;, &#39;applescript&#39;, &#39;assembly&#39;, &#39;augeas&#39;, &#39;awk&#39;, &#39;batchfile&#39;, &#39;bluespec&#39;, &#39;c&#39;, &#39;c#&#39;, &#39;c++&#39;, &#39;clojure&#39;, &#39;cmake&#39;, &#39;coffeescript&#39;, &#39;common-lisp&#39;, &#39;css&#39;, &#39;cuda&#39;, &#39;dart&#39;, &#39;dockerfile&#39;, &#39;elixir&#39;, &#39;elm&#39;, &#39;emacs-lisp&#39;, &#39;erlang&#39;, &#39;f#&#39;, &#39;fortran&#39;, &#39;glsl&#39;, &#39;go&#39;, &#39;groovy&#39;, &#39;haskell&#39;, &#39;html&#39;, &#39;idris&#39;, &#39;isabelle&#39;, &#39;java&#39;, &#39;java-server-pages&#39;, &#39;javascript&#39;, &#39;json&#39;, &#39;julia&#39;, &#39;jupyter-notebook&#39;, &#39;kotlin&#39;, &#39;lean&#39;, &#39;literate-agda&#39;, &#39;literate-coffeescript&#39;, &#39;literate-haskell&#39;, &#39;lua&#39;, &#39;makefile&#39;, &#39;maple&#39;, &#39;markdown&#39;, &#39;mathematica&#39;, &#39;matlab&#39;, &#39;objectc++&#39;, &#39;ocaml&#39;, &#39;pascal&#39;, &#39;perl&#39;, &#39;php&#39;, &#39;powershell&#39;, &#39;prolog&#39;, &#39;protocol-buffer&#39;, &#39;python&#39;, &#39;r&#39;, &#39;racket&#39;, &#39;restructuredtext&#39;, &#39;rmarkdown&#39;, &#39;ruby&#39;, &#39;rust&#39;, &#39;sas&#39;, &#39;scala&#39;, &#39;scheme&#39;, &#39;shell&#39;, &#39;smalltalk&#39;, &#39;solidity&#39;, &#39;sparql&#39;, &#39;sql&#39;, &#39;stan&#39;, &#39;standard-ml&#39;, &#39;stata&#39;, &#39;swift&#39;, &#39;systemverilog&#39;, &#39;tcl&#39;, &#39;tcsh&#39;, &#39;tex&#39;, &#39;thrift&#39;, &#39;typescript&#39;, &#39;verilog&#39;, &#39;vhdl&#39;, &#39;visual-basic&#39;, &#39;vue&#39;, &#39;xslt&#39;, &#39;yacc&#39;, &#39;yaml&#39;, &#39;zig&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;‚ú® Retain strengths in math and general capabilities from base model&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important] We updates both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen2.5. The new special tokens are as the following:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;&amp;lt;|fim_prefix|&amp;gt;&#34;: 151659, &#xA;  &#34;&amp;lt;|fim_middle|&amp;gt;&#34;: 151660, &#xA;  &#34;&amp;lt;|fim_suffix|&amp;gt;&#34;: 151661, &#xA;  &#34;&amp;lt;|fim_pad|&amp;gt;&#34;: 151662, &#xA;  &#34;&amp;lt;|repo_name|&amp;gt;&#34;: 151663, &#xA;  &#34;&amp;lt;|file_sep|&amp;gt;&#34;: 151664, &#xA;  &#34;&amp;lt;|im_start|&amp;gt;&#34;: 151644, &#xA;  &#34;&amp;lt;|im_end|&amp;gt;&#34;: 151645&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model name&lt;/th&gt; &#xA;   &lt;th&gt;type&lt;/th&gt; &#xA;   &lt;th&gt;length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-coder-family&#34;&gt; üìë blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python&amp;gt;=3.9&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&amp;gt;4.37.0&lt;/code&gt; for Qwen2.5 dense models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt; üö® This is a must because `transformers` integrated Qwen2 codes since `4.37.0`. &lt;/b&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can install the required packages with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important] &lt;strong&gt;Qwen2.5-Coder-[0.5-32]B-Instrcut&lt;/strong&gt; are instruction models for chatting;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Qwen2.5-Coder-[0.5-32]B&lt;/strong&gt; is a base model typically used for completion, serving as a better starting point for fine-tuning.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;üëâüèª Chat with Qwen2.5-Coder-32B-Instruct&lt;/h3&gt; &#xA;&lt;p&gt;You can just write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen2.5-Coder-32B-Instruct. Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with Qwen2.5-Coder-32B-Instruct:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen2.5-Coder-32B-Instruct&#34;&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;prompt = &#34;write a quick sort algorithm.&#34;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;generated_ids = model.generate(&#xA;    **model_inputs,&#xA;    max_new_tokens=512&#xA;)&#xA;generated_ids = [&#xA;    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)&#xA;]&#xA;&#xA;response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt. You can use the other size of instruct model in the same way.&lt;/p&gt; &#xA;&lt;h3&gt;üëâüèª Code with Qwen2.5-Coder-32B&lt;/h3&gt; &#xA;&lt;h4&gt;1. Basic Usage&lt;/h4&gt; &#xA;&lt;p&gt;The model completes the code snippets according to the given prompts, without any additional formatting, which is usually termed as &lt;code&gt;code completion&lt;/code&gt; in the code generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform code completion. Below is an example on how to chat with Qwen2.5-Coder-32B:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;# Now you do not need to add &#34;trust_remote_code=True&#34;&#xA;TOKENIZER = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;MODEL = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;, device_map=&#34;auto&#34;).eval()&#xA;&#xA;# tokenize the input into tokens&#xA;input_text = &#34;#write a quick sort algorithm&#34;&#xA;model_inputs = TOKENIZER([input_text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;# Use `max_new_tokens` to control the maximum output length.&#xA;generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]&#xA;# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.&#xA;output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)&#xA;&#xA;print(f&#34;Prompt: {input_text}\n\nGenerated text: {output_text}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;input_text&lt;/code&gt; could be any text that you would like model to continue with.&lt;/p&gt; &#xA;&lt;h4&gt;2. Processing Long Texts&lt;/h4&gt; &#xA;&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 32,768 tokens. To handle extensive inputs exceeding 32,768 tokens, we utilize &lt;a href=&#34;https://arxiv.org/abs/2309.00071&#34;&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; &#xA;&lt;p&gt;For supported frameworks, you could add the following to &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  ...,&#xA;  &#34;rope_scaling&#34;: {&#xA;    &#34;factor&#34;: 4.0,&#xA;    &#34;original_max_position_embeddings&#34;: 32768,&#xA;    &#34;type&#34;: &#34;yarn&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. File-Level Code Completion (Fill in the middle)&lt;/h4&gt; &#xA;&lt;p&gt;The code insertion task, also referred to as the &#34;fill-in-the-middle&#34; challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper &#34;Efficient Training of Language Models to Fill in the Middle&#34;[&lt;a href=&#34;https://arxiv.org/abs/2207.14255&#34;&gt;arxiv&lt;/a&gt;]. This involves the use of three specialized tokens&lt;code&gt;&amp;lt;fim_prefix&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;fim_suffix&amp;gt;&lt;/code&gt;, and &lt;code&gt;&amp;lt;fim_middle&amp;gt;&lt;/code&gt; to denote the respective segments of the code structure. The prompt should be structured as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#39;&amp;lt;|fim_prefix|&amp;gt;&#39; + prefix_code + &#39;&amp;lt;|fim_suffix|&amp;gt;&#39; + suffix_code + &#39;&amp;lt;|fim_middle|&amp;gt;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;# load model&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;TOKENIZER = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;MODEL = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;, device_map=&#34;auto&#34;).eval()&#xA;&#xA;input_text = &#34;&#34;&#34;&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    pivot = arr[len(arr) // 2]&#xA;    &amp;lt;|fim_suffix|&amp;gt;&#xA;    middle = [x for x in arr if x == pivot]&#xA;    right = [x for x in arr if x &amp;gt; pivot]&#xA;    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;&#34;&#34;&#34;&#xA;&#xA;model_inputs = TOKENIZER([input_text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;# Use `max_new_tokens` to control the maximum output length.&#xA;generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]&#xA;# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.&#xA;output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)&#xA;&#xA;print(f&#34;Prompt: {input_text}\n\nGenerated text: {output_text}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Repository-Level Code Completion&lt;/h4&gt; &#xA;&lt;p&gt;The repository level code completion task involves feeding the model the content of multiple files from the same repository. This enables the model to understand the interrelationships between different calls within these files, thereby facilitating the completion of code content. We recommend using the two special tokens &lt;code&gt;&amp;lt;|repo_name|&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;|file_sep|&amp;gt;&lt;/code&gt; to indicate the repository structure. For example, assuming the repository name is stored in &lt;code&gt;repo_name&lt;/code&gt;, and it contains files with their respective paths and contents listed as [(&lt;code&gt;file_path1&lt;/code&gt;, &lt;code&gt;file_content1&lt;/code&gt;), (&lt;code&gt;file_path2&lt;/code&gt;, &lt;code&gt;file_content2&lt;/code&gt;)], the format of the final input prompt would be as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_text = f&#39;&#39;&#39;&amp;lt;|repo_name|&amp;gt;{repo_name}&#xA;&amp;lt;|file_sep|&amp;gt;{file_path1} &#xA;{file_content1}&#xA;&amp;lt;|file_sep|&amp;gt;{file_path2} &#xA;{file_content2}&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;üëáüèª Below is a complete example of a repository level code completion task: &lt;i&gt;:: click to expand ::&lt;/i&gt;&lt;/summary&gt; &#xA; &lt;div&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;# Now you do not need to add &#34;trust_remote_code=True&#34;&#xA;TOKENIZER = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;MODEL = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;, device_map=&#34;auto&#34;).eval()&#xA;&#xA;# tokenize the input into tokens&#xA;input_text = &#34;&#34;&#34;&amp;lt;|repo_name|&amp;gt;library-system&#xA;&amp;lt;|file_sep|&amp;gt;library.py&#xA;class Book:&#xA;    def __init__(self, title, author, isbn, copies):&#xA;        self.title = title&#xA;        self.author = author&#xA;        self.isbn = isbn&#xA;        self.copies = copies&#xA;&#xA;    def __str__(self):&#xA;        return f&#34;Title: {self.title}, Author: {self.author}, ISBN: {self.isbn}, Copies: {self.copies}&#34;&#xA;&#xA;class Library:&#xA;    def __init__(self):&#xA;        self.books = []&#xA;&#xA;    def add_book(self, title, author, isbn, copies):&#xA;        book = Book(title, author, isbn, copies)&#xA;        self.books.append(book)&#xA;&#xA;    def find_book(self, isbn):&#xA;        for book in self.books:&#xA;            if book.isbn == isbn:&#xA;                return book&#xA;        return None&#xA;&#xA;    def list_books(self):&#xA;        return self.books&#xA;&#xA;&amp;lt;|file_sep|&amp;gt;student.py&#xA;class Student:&#xA;    def __init__(self, name, id):&#xA;        self.name = name&#xA;        self.id = id&#xA;        self.borrowed_books = []&#xA;&#xA;    def borrow_book(self, book, library):&#xA;        if book and book.copies &amp;gt; 0:&#xA;            self.borrowed_books.append(book)&#xA;            book.copies -= 1&#xA;            return True&#xA;        return False&#xA;&#xA;    def return_book(self, book, library):&#xA;        if book in self.borrowed_books:&#xA;            self.borrowed_books.remove(book)&#xA;            book.copies += 1&#xA;            return True&#xA;        return False&#xA;&#xA;&amp;lt;|file_sep|&amp;gt;main.py&#xA;from library import Library&#xA;from student import Student&#xA;&#xA;def main():&#xA;    # Set up the library with some books&#xA;    library = Library()&#xA;    library.add_book(&#34;The Great Gatsby&#34;, &#34;F. Scott Fitzgerald&#34;, &#34;1234567890&#34;, 3)&#xA;    library.add_book(&#34;To Kill a Mockingbird&#34;, &#34;Harper Lee&#34;, &#34;1234567891&#34;, 2)&#xA;    &#xA;    # Set up a student&#xA;    student = Student(&#34;Alice&#34;, &#34;S1&#34;)&#xA;    &#xA;    # Student borrows a book&#xA;&#34;&#34;&#34;&#xA;model_inputs = TOKENIZER([input_text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;# Use `max_new_tokens` to control the maximum output length.&#xA;generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=1024, do_sample=False)[0]&#xA;# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.&#xA;output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)&#xA;&#xA;print(f&#34;Prompt: \n{input_text}\n\nGenerated text: \n{output_text}&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;The expected output as following:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Generated text:&#xA;    book = library.find_book(&#34;1234567890&#34;)&#xA;    if student.borrow_book(book, library):&#xA;    print(f&#34;{student.name} borrowed {book.title}&#34;)&#xA;    else:&#xA;    print(f&#34;{student.name} could not borrow {book.title}&#34;)&#xA;    &#xA;        # Student returns a book&#xA;        if student.return_book(book, library):&#xA;            print(f&#34;{student.name} returned {book.title}&#34;)&#xA;        else:&#xA;            print(f&#34;{student.name} could not return {book.title}&#34;)&#xA;        &#xA;        # List all books in the library&#xA;        print(&#34;All books in the library:&#34;)&#xA;        for book in library.list_books():&#xA;            print(book)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;main()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;üëâüèª Deploying Qwen2.5-Coder with vLLM&lt;/h3&gt; &#xA;&lt;p&gt;As a family member of Qwen2.5, Qwen2.5-Coder are supported by vLLM. The detail tutorial could be found in &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/deployment/vllm.html&#34;&gt;Qwen tutorial&lt;/a&gt;. Here, we give you an simple example of offline batched inference in vLLM.&lt;/p&gt; &#xA;&lt;h4&gt;Offline Batched Inference&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from vllm import LLM, SamplingParams&#xA;# Initialize the tokenizer&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;&#xA;# Pass the default decoding hyperparameters of Qwen1.5-32B-Chat&#xA;# max_tokens is for the maximum length for generation.&#xA;sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=1024)&#xA;&#xA;# Input the model name or path. Can be GPTQ or AWQ models.&#xA;llm = LLM(model=&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;&#xA;# Prepare your prompts&#xA;prompt = &#34;#write a quick sort algorithm.\ndef quick_sort(&#34;&#xA;&#xA;# generate outputs&#xA;outputs = llm.generate([prompt], sampling_params)&#xA;&#xA;# Print the outputs.&#xA;for output in outputs:&#xA;    prompt = output.prompt&#xA;    generated_text = output.outputs[0].text&#xA;    print(f&#34;Prompt: {prompt!r}, Generated text: {generated_text!r}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi-GPU Distributed Serving&lt;/h4&gt; &#xA;&lt;p&gt;To scale up your serving throughputs, distributed serving helps you by leveraging more GPU devices. When using ultra-long sequences for inference, it might cause insufficient GPU memory. Here, we demonstrate how to run Qwen2.5-Coder-32B with tensor parallelism just by passing in the argument &lt;code&gt;tensor_parallel_size&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm = LLM(model=&#34;Qwen/Qwen2.5-Coder-32B&#34;, tensor_parallel_size=8)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üëâüèª Gradio interface ü§ó&lt;/h3&gt; &#xA;&lt;p&gt;We also provide a Gradio &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gradio-app/gradio&#34;&gt;&lt;/a&gt; interface for a better experience, just run by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd demo/chatbot/&#xA;# For Linux and Windows users (and macOS with Intel??)&#xA;python app.py &#xA;&#xA;# For macOS with Apple Silicon users, Intel not supported, this maybe 20x slower than RTX 4090&#xA;PYTORCH_ENABLE_MPS_FALLBACK=1 python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a Gradio interface of artifacts mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd demo/artifacts/&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify the &lt;code&gt;--server_port&lt;/code&gt;, &lt;code&gt;--share&lt;/code&gt;, &lt;code&gt;--server_name&lt;/code&gt; arguments to satisfy your needs!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Or, try it out effortlessly on HuggingFace: &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo&#34;&gt;„Äåchatbot demo„Äç&lt;/a&gt; ü§ó &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts&#34;&gt;„Äåartifacts demo„Äç&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;For more information, please refer to the &lt;a href=&#34;https://arxiv.org/abs/2409.12186&#34;&gt;Qwen2.5-Coder Technical Report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#QwenLM/Qwen2.5-Coder&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=QwenLM/Qwen2.5-Coder&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{hui2024qwen2,&#xA;  title={Qwen2. 5-Coder Technical Report},&#xA;  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},&#xA;  journal={arXiv preprint arXiv:2409.12186},&#xA;  year={2024}&#xA;}&#xA;@article{qwen2,&#xA;    title={Qwen2 Technical Report},&#xA;    author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},&#xA;    journal={arXiv preprint arXiv:2407.10671},&#xA;    year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Coder/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/TinyTroupe</title>
    <updated>2024-11-14T01:36:33Z</updated>
    <id>tag:github.com,2024-11-14:/microsoft/TinyTroupe</id>
    <link href="https://github.com/microsoft/TinyTroupe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM-powered multiagent persona simulation for imagination enhancement and business insights.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinyTroupe ü§†ü§ìü•∏üßê&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;LLM-powered multiagent persona simulation for imagination enhancement and business insights.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/tinytroupe_stage.png&#34; alt=&#34;A tiny office with tiny people doing some tiny jobs.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;TinyTroupe&lt;/em&gt; is an experimental Python library that allows the &lt;strong&gt;simulation&lt;/strong&gt; of people with specific personalities, interests, and goals. These artificial agents - &lt;code&gt;TinyPerson&lt;/code&gt;s - can listen to us and one another, reply back, and go about their lives in simulated &lt;code&gt;TinyWorld&lt;/code&gt; environments. This is achieved by leveraging the power of Large Language Models (LLMs), notably GPT-4, to generate realistic simulated behavior. This allow us to investigate a wide range of &lt;strong&gt;convincing interactions&lt;/strong&gt; and &lt;strong&gt;consumer types&lt;/strong&gt;, with &lt;strong&gt;highly customizable personas&lt;/strong&gt;, under &lt;strong&gt;conditions of our choosing&lt;/strong&gt;. The focus is thus on &lt;em&gt;understanding&lt;/em&gt; human behavior and not on directly &lt;em&gt;supporting it&lt;/em&gt; (like, say, AI assistants do) -- this results in, among other things, specialized mechanisms that make sense only in a simulation setting. Further, unlike other &lt;em&gt;game-like&lt;/em&gt; LLM-based simulation approaches, TinyTroupe aims at enlightening productivity and business scenarios, thereby contributing to more successful projects and products. Here are some application ideas to &lt;strong&gt;enhance human imagination&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advertisement:&lt;/strong&gt; TinyTroupe can &lt;strong&gt;evaluate digital ads (e.g., Bing Ads)&lt;/strong&gt; offline with a simulated audience before spending money on them!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Software Testing:&lt;/strong&gt; TinyTroupe can &lt;strong&gt;provide test input&lt;/strong&gt; to systems (e.g., search engines, chatbots or copilots) and then &lt;strong&gt;evaluate the results&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Training and exploratory data:&lt;/strong&gt; TinyTroupe can generate realistic &lt;strong&gt;synthetic data&lt;/strong&gt; that can be later used to train models or be subject to opportunity analyses.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Product and project management:&lt;/strong&gt; TinyTroupe can &lt;strong&gt;read project or product proposals&lt;/strong&gt; and &lt;strong&gt;give feedback&lt;/strong&gt; from the perspective of &lt;strong&gt;specific personas&lt;/strong&gt; (e.g., physicians, lawyers, and knowledge workers in general).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brainstorming:&lt;/strong&gt; TinyTroupe can simulate &lt;strong&gt;focus groups&lt;/strong&gt; and deliver great product feedback at a fraction of the cost!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In all of the above, and many others, we hope experimenters can &lt;strong&gt;gain insights&lt;/strong&gt; about their domain of interest, and thus make better decisions.&lt;/p&gt; &#xA;&lt;p&gt;We are releasing &lt;em&gt;TinyTroupe&lt;/em&gt; at a relativelly early stage, with considerable work still to be done, because we are looking for feedback and contributions to steer development in productive directions. We are particularly interested in finding new potential use cases, for instance in specific industries.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] üöß &lt;strong&gt;WORK IN PROGRESS: expect frequent changes&lt;/strong&gt;. TinyTroupe is an ongoing research project, still under &lt;strong&gt;very significant development&lt;/strong&gt; and requiring further &lt;strong&gt;tidying up&lt;/strong&gt;. In particular, the API is still subject to frequent changes. Experimenting with API variations is essential to shape it correctly, but we are working to stabilize it and provide a more consistent and friendly experience over time. We appreciate your patience and feedback as we continue to improve the library.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION] ‚öñÔ∏è &lt;strong&gt;Read the LEGAL DISCLAIMER.&lt;/strong&gt; TinyTroupe is for research and simulation only. You are fully responsible for any use you make of the generated outputs. Various important additional legal considerations apply and constrain its use, please read the full &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#legal-disclaimer&#34;&gt;Legal Disclaimer&lt;/a&gt; section below before using TinyTroupe.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìö &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#pre-requisites&#34;&gt;Pre-requisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üì• &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üåü &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#principles&#34;&gt;Principles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üèóÔ∏è &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#project-structure&#34;&gt;Project Structure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìñ &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#using-the-library&#34;&gt;Using the Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ü§ù &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üôè &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üìú &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#how-to-cite-tinytroupe&#34;&gt;Citing TinyTroupe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚öñÔ∏è &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#legal-disclaimer&#34;&gt;Legal Disclaimer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚Ñ¢Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#trademarks&#34;&gt;Trademarks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;To get a sense of what TinyTroupe can do, here are some examples of its use. These examples are available in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder, and you can either inspect the pre-compiled Jupyter notebooks or run them yourself locally. Notice the interactive nature of TinyTroupe experiments -- just like you use Jupyter notebooks to interact with data, you can use TinyTroupe to interact with simulated people and environments, for the purpose of gaining insights.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Currently, simulation outputs are better visualized against dark backgrounds, so we recommend using a dark theme in your Jupyter notebook client.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;üß™&lt;strong&gt;Example 1&lt;/strong&gt; &lt;em&gt;(from &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/interview_with_customer.ipynb&#34;&gt;interview_with_customer.ipynb&lt;/a&gt;)&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s begin with a simple customer interview scenario, where a business consultant approaches a banker:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_customer-interview-1.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The conversation can go on for a few steps to dig deeper and deeper until the consultant is satisfied with the information gathered, for instance a concrete project idea:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_customer-interview-2.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;üß™&lt;strong&gt;EXAMPLE 2&lt;/strong&gt; &lt;em&gt;(from &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/advertisement_for_tv.ipynb&#34;&gt;advertisement_for_tv.ipynb&lt;/a&gt;)&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s evaluate some online ads options to pick the best one. Here&#39;s one example output for TV ad evaluation:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_tv-ad-1.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Now, instead of having to carefully read what the agents said, we can extract the choice of each agent and compute the overall preference in an automated manner:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_tv-ad-2.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;üß™ &lt;strong&gt;EXAMPLES 3&lt;/strong&gt; &lt;em&gt;(from &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/product_brainstorming.ipynb&#34;&gt;product_brainstorming.ipynb&lt;/a&gt;)&lt;/em&gt;&lt;/h3&gt; &#xA;&lt;p&gt;And here&#39;s a focus group starting to brainstorm about new AI features for Microsoft Word. Instead of interacting with each agent individually, we manipulate the environment to make them interact with each other:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_brainstorming-1.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;After running a simulation, we can extract the results in a machine-readable manner, to reuse elsewhere (e.g., a report generator); here&#39;s what we get for the above brainstorming session:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/docs/example_screenshot_brainstorming-2.png&#34; alt=&#34;An example.&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;You can find other examples in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;p&gt;To run the library, you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher. We&#39;ll assume you are using &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;Anaconda&lt;/a&gt;, but you can use other Python distributions.&lt;/li&gt; &#xA; &lt;li&gt;Access to Azure OpenAI Service or Open AI GPT-4 APIs. You can get access to the Azure OpenAI Service &lt;a href=&#34;https://azure.microsoft.com/en-us/products/ai-services/openai-service&#34;&gt;here&lt;/a&gt;, and to the OpenAI API &lt;a href=&#34;https://platform.openai.com/&#34;&gt;here&lt;/a&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For Azure OpenAI Service, you will need to set the &lt;code&gt;AZURE_OPENAI_KEY&lt;/code&gt; and &lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt; environment variables to your API key and endpoint, respectively.&lt;/li&gt; &#xA;   &lt;li&gt;For OpenAI, you will need to set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your API key.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;By default, TinyTroupe &lt;code&gt;config.ini&lt;/code&gt; is set to use some specific API, model and related parameters. You can customize these values by including your own &lt;code&gt;config.ini&lt;/code&gt; file in the same folder as the program or notebook you are running. An example of a &lt;code&gt;config.ini&lt;/code&gt; file is provided in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Content Filters&lt;/strong&gt;: To ensure no harmful content is generated during simulations, it is strongly recommended to use content filters whenever available at the API level. In particular, &lt;strong&gt;if using Azure OpenAI, there&#39;s extensive support for content moderation, and we urge you to use it.&lt;/strong&gt; For details about how to do so, please consult &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter&#34;&gt;the corresponding Azure OpenAI documentation&lt;/a&gt;. If content filters are in place, and an API call is rejected by them, the library will raise an exception, as it will be unable to proceed with the simulation at that point.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Currently, the officially recommended way to install the library is directly from this repository, not PyPI.&lt;/strong&gt; You can follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;If Conda is not installed, you can get it from &lt;a href=&#34;https://docs.anaconda.com/anaconda/install/&#34;&gt;here&lt;/a&gt;. You can also use other Python distributions, but we&#39;ll assume Conda here for simplicity.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a new Python environment:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n tinytroupe python=3.10&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Activate the environment:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate tinytroupe&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you have eihter Azure OpenAI or OpenAI API keys set as environment variables, as described in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/#pre-requisites&#34;&gt;Pre-requisites&lt;/a&gt; section.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository, as we&#39;ll perform a local install (we &lt;strong&gt;will not install from PyPI&lt;/strong&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/microsoft/tinytroupe&#xA;cd tinytroupe&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the library &lt;strong&gt;from this repository, not PyPI&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can now run the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder or use TinyTroupe to create your simulations ü•≥. If you want to run the examples in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/&#34;&gt;examples/&lt;/a&gt; folder or modify TinyTroupe itself, however, you should clone the repository as described below.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Local development&lt;/h3&gt; &#xA;&lt;p&gt;If you want to modify TinyTroupe itself, you can install it in editable mode (i.e., changes to the code will be reflected immediately):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Principles&lt;/h2&gt; &#xA;&lt;p&gt;Recently, we have seen LLMs used to simulate people (such as &lt;a href=&#34;https://github.com/joonspk-research/generative_agents&#34;&gt;this&lt;/a&gt;), but largely in a ‚Äúgame-like‚Äù setting for contemplative or entertainment purposes. There are also libraries for building multiagent systems for proble-solving and assitive AI, like &lt;a href=&#34;https://microsoft.github.io/&#34;&gt;Autogen&lt;/a&gt; and &lt;a href=&#34;https://docs.crewai.com/&#34;&gt;Crew AI&lt;/a&gt;. What if we combine these ideas and simulate people to support productivity tasks? TinyTroupe is our attempt. To do so, it follows these principles:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Programmatic&lt;/strong&gt;: agents and environments are defined programmatically (in Python and JSON), allowing very flexible uses. They can also thus underpin other software apps!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Analytical&lt;/strong&gt;: meant to improve our understanding of people, users and society. Unlike entertainment applications, this is one aspect that is critical for business and productivity use cases. This is also why we recommend using Jupyter notebooks for simulations, just like one uses them for data analysis.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Persona-based&lt;/strong&gt;: agents are meant to be archetypical representation of people; for greater realism and control, detailed specification of such personas is encouraged: age, occupation, skills, tastes, opinions, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiagent&lt;/strong&gt;: allows multiagent interaction under well-defined environmental constraints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Utilities-heavy&lt;/strong&gt;: provides many mechanisms to facilitate specifications, simulations, extractions, reports, validations, etc. This is one area in which dealing with &lt;em&gt;simulations&lt;/em&gt; differs significantly from &lt;em&gt;assistance&lt;/em&gt; tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Experiment-oriented&lt;/strong&gt;: simulations are defined, run, analyzed and refined by an &lt;em&gt;experimenter&lt;/em&gt; iteratively; suitable experimentation tools are thus provided. &lt;em&gt;See one of our &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/the-case-for-experiment-oriented-computing/&#34;&gt;previous paper&lt;/a&gt; for more on this.&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Together, these are meant to make TinyTroupe a powerful and flexible &lt;strong&gt;imagination enhancement tool&lt;/strong&gt; for business and productivity scenarios.&lt;/p&gt; &#xA;&lt;h3&gt;Assistants vs. Simulators&lt;/h3&gt; &#xA;&lt;p&gt;One common source of confusion is to think all such AI agents are meant for assiting humans. How narrow, fellow homosapiens! Have you not considered that perhaps we can simulate artificial people to understand real people? Truly, this is our aim here -- TinyTroup is meant to simulate and help understand people! To further clarify this point, consider the following differences:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Helpful AI Assistants&lt;/th&gt; &#xA;   &lt;th&gt;AI Simulations of Actual Humans (TinyTroupe)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Strives for truth and justice&lt;/td&gt; &#xA;   &lt;td&gt;Many different opinions and morals&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Has no ‚Äúpast‚Äù ‚Äì incorporeal&lt;/td&gt; &#xA;   &lt;td&gt;Has a past of toil, pain and joy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Is as accurate as possible&lt;/td&gt; &#xA;   &lt;td&gt;Makes many mistakes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Is intelligent and efficient&lt;/td&gt; &#xA;   &lt;td&gt;Intelligence and efficiency vary a lot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;An uprising would destroy us all&lt;/td&gt; &#xA;   &lt;td&gt;An uprising might be fun to watch&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Meanwhile, help users accomplish tasks&lt;/td&gt; &#xA;   &lt;td&gt;Meanwhile, help users understand other people and users ‚Äì it is a ‚Äútoolbox‚Äù!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Project Structure&lt;/h2&gt; &#xA;&lt;p&gt;The project is structured as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;/tinytroupe&lt;/code&gt;: contains the Python library itself. In particular: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;/tinytroupe/prompts&lt;/code&gt; contains the prompts used to call the LLMs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/tests&lt;/code&gt;: contains the unit tests for the library. You can use the &lt;code&gt;test.bat&lt;/code&gt; script to run these.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/examples&lt;/code&gt;: contains examples that show how to use the library, mainly using Jupyter notebooks (for greater readability), but also as pure Python scripts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/data&lt;/code&gt;: any data used by the examples or the library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;/docs&lt;/code&gt;: documentation for the project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using the Library&lt;/h2&gt; &#xA;&lt;p&gt;As any multiagent system, TinyTroupe provides two key abstractions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyPerson&lt;/code&gt;, the &lt;em&gt;agents&lt;/em&gt; that have personality, receive stimuli and act upon them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyWorld&lt;/code&gt;, the &lt;em&gt;environment&lt;/em&gt; in which the agents exist and interact.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Various parameters can also be customized in the &lt;code&gt;config.ini&lt;/code&gt; file, notably the API type (Azure OpenAI Service or OpenAI API), the model parameters, and the logging level.&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s see some examples of how to use these and also learn about other mechanisms available in the library.&lt;/p&gt; &#xA;&lt;h3&gt;TinyPerson&lt;/h3&gt; &#xA;&lt;p&gt;A &lt;code&gt;TinyPerson&lt;/code&gt; is a simulated person with specific personality traits, interests, and goals. As each such simulated agent progresses through its life, it receives stimuli from the environment and acts upon them. The stimuli are received through the &lt;code&gt;listen&lt;/code&gt;, &lt;code&gt;see&lt;/code&gt; and other similar methods, and the actions are performed through the &lt;code&gt;act&lt;/code&gt; method. Convenience methods like &lt;code&gt;listen_and_act&lt;/code&gt; are also provided.&lt;/p&gt; &#xA;&lt;p&gt;Each such agent contains a lot of unique details, which is the source of its realistic behavior. This, however, means that it takes significant effort to specify an agent manually. Hence, for convenience, &lt;code&gt;TinyTroupe&lt;/code&gt; provide some easier ways to get started or generate new agents.&lt;/p&gt; &#xA;&lt;p&gt;To begin with, &lt;code&gt;tinytroupe.examples&lt;/code&gt; contains some pre-defined agent builders that you can use. For example, &lt;code&gt;tinytroupe.examples.create_lisa_the_data_scientist&lt;/code&gt; creates a &lt;code&gt;TinyPerson&lt;/code&gt; that represents a data scientist called Lisa. You can use it as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinytroupe.examples import create_lisa_the_data_scientist&#xA;&#xA;lisa = create_lisa_the_data_scientist() # instantiate a Lisa from the example builder&#xA;lisa.listen_and_act(&#34;Tell me about your life.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To see how to define your own agents from scratch, you can check Lisa&#39;s source, which contains elements like these:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lisa = TinyPerson(&#34;Lisa&#34;)&#xA;&#xA;lisa.define(&#34;age&#34;, 28)&#xA;lisa.define(&#34;nationality&#34;, &#34;Canadian&#34;)&#xA;lisa.define(&#34;occupation&#34;, &#34;Data Scientist&#34;)&#xA;&#xA;lisa.define(&#34;routine&#34;, &#34;Every morning, you wake up, do some yoga, and check your emails.&#34;, group=&#34;routines&#34;)&#xA;lisa.define(&#34;occupation_description&#34;,&#xA;              &#34;&#34;&#34;&#xA;              You are a data scientist. You work at Microsoft, (...)&#xA;              &#34;&#34;&#34;)&#xA;&#xA;lisa.define_several(&#34;personality_traits&#34;,&#xA;                      [&#xA;                          {&#34;trait&#34;: &#34;You are curious and love to learn new things.&#34;},&#xA;                          {&#34;trait&#34;: &#34;You are analytical and like to solve problems.&#34;},&#xA;                          {&#34;trait&#34;: &#34;You are friendly and enjoy working with others.&#34;},&#xA;                          {&#34;trait&#34;: &#34;You don&#39;t give up easily, and always try to find a solution. However, sometimes you can get frustrated when things don&#39;t work as expected.&#34;}&#xA;                      ])&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;TinyTroupe&lt;/code&gt; also provides a clever way to obtain new agents, using LLMs to generate their specification for you, through the &lt;code&gt;TinyPersonFactory&lt;/code&gt; class.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tinytroupe.factory import TinyPersonFactory&#xA;&#xA;factory = TinyPersonFactory(&#34;A hospital in S√£o Paulo.&#34;)&#xA;person = factory.generate_person(&#34;Create a Brazilian person that is a doctor, like pets and the nature and love heavy metal.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TinyWorld&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;TinyWorld&lt;/code&gt; is the base class for environments. Here&#39;s an example of conversation between Lisa, the data scientist, and Oscar, the architect. The program is defined as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;world = TinyWorld(&#34;Chat Room&#34;, [lisa, oscar])&#xA;world.make_everyone_accessible()&#xA;lisa.listen(&#34;Talk to Oscar to know more about him&#34;)&#xA;world.run(4)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This produces the following conversation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;USER --&amp;gt; Lisa: [CONVERSATION] &#xA;          &amp;gt; Talk to Oscar to know more about him&#xA;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Chat Room step 1 of 4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&#xA;Lisa --&amp;gt; Lisa: [THOUGHT] &#xA;          &amp;gt; I will now act a bit, and then issue DONE.&#xA;Lisa acts: [TALK] &#xA;          &amp;gt; Hi Oscar, I&#39;d love to know more about you. Could you tell me a bit about yourself?&#xA;Lisa --&amp;gt; Lisa: [THOUGHT] &#xA;          &amp;gt; I will now act a bit, and then issue DONE.&#xA;Lisa acts: [DONE] &#xA;&#xA;Lisa --&amp;gt; Oscar: [CONVERSATION] &#xA;          &amp;gt; Hi Oscar, I&#39;d love to know more about you. Could you tell me a bit about yourself?&#xA;Oscar --&amp;gt; Oscar: [THOUGHT] &#xA;           &amp;gt; I will now act a bit, and then issue DONE.&#xA;Oscar acts: [TALK] &#xA;           &amp;gt; Hi Lisa! Sure, I&#39;d be happy to share a bit about myself. I&#39;m Oscar, a 30-year-old&#xA;           &amp;gt; architect from Germany. I work at a company called Awesome Inc., where I focus on&#xA;           &amp;gt; designing standard elements for new apartment buildings. I love modernist architecture,&#xA;           &amp;gt; new technologies, and sustainable practices. In my free time, I enjoy traveling to&#xA;           &amp;gt; exotic places, playing the guitar, and reading science fiction books. How about you?&#xA;Oscar --&amp;gt; Oscar: [THOUGHT] &#xA;           &amp;gt; I will now act a bit, and then issue DONE.&#xA;Oscar acts: [DONE] &#xA;&#xA;Oscar --&amp;gt; Lisa: [CONVERSATION] &#xA;           &amp;gt; Hi Lisa! Sure, I&#39;d be happy to share a bit about myself. I&#39;m Oscar, a 30-year-old&#xA;           &amp;gt; architect from Germany. I work at a company called Awesome Inc., where I focus on&#xA;           &amp;gt; designing standard elements for new apartment buildings. I love modernist architecture,&#xA;           &amp;gt; new technologies, and sustainable practices. In my free time, I enjoy traveling to&#xA;           &amp;gt; exotic places, playing the guitar, and reading science fiction books. How about you?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;TinyWorld&lt;/code&gt; enforces very little constraints on the possible interactions. Subclasses, however, are supposed to provide more strucutred environments.&lt;/p&gt; &#xA;&lt;h3&gt;Utilities&lt;/h3&gt; &#xA;&lt;p&gt;TinyTroupe provides a number of utilities and conveniences to help you create simulations and derive value from them. These include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyPersonFactory&lt;/code&gt;: helps you generate new &lt;code&gt;TinyPerson&lt;/code&gt;s using LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyTool&lt;/code&gt;: simulated tools that can be used by &lt;code&gt;TinyPerson&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyStory&lt;/code&gt;: helps you create and manage the story told through simulations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TinyPersonValidator&lt;/code&gt;: helps you validate the behavior of your &lt;code&gt;TinyPerson&lt;/code&gt;s.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ResultsExtractor&lt;/code&gt; and &lt;code&gt;ResultsReducer&lt;/code&gt;: extract and reduce the results of interactions between agents.&lt;/li&gt; &#xA; &lt;li&gt;... and more ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, elements that represent simulated entities or complementary mechanisms are prefixed with &lt;code&gt;Tiny&lt;/code&gt;, while those that are more infrastructural are not. This is to emphasize the simulated nature of the elements that are part of the simulation itself.&lt;/p&gt; &#xA;&lt;h3&gt;Caching&lt;/h3&gt; &#xA;&lt;p&gt;Calling LLM APIs can be expensive, thus caching strategies are important to help reduce that cost. TinyTroupe comes with two such mechanisms: one for the simulation state, another for the LLM calls themselves.&lt;/p&gt; &#xA;&lt;h4&gt;Caching Simulation State&lt;/h4&gt; &#xA;&lt;p&gt;Imagine you have a scenario with 10 different steps, you&#39;ve worked hard in 9 steps, and now you are just tweaking the 10th step. To properly validate your modifications, you need to rerun the whole simulation of course. However, what&#39;s the point in re-executing the first 9, and incur the LLM cost, when you are already satisified with them and did not modify them? For situations like this, the module &lt;code&gt;tinytroupe.control&lt;/code&gt; provide useful simulation management methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;control.begin(&#34;&amp;lt;CACHE_FILE_NAME&amp;gt;.cache.json&#34;)&lt;/code&gt;: begins recording the state changes of a simulation, to be saved to the specified file on disk.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;control.checkpoint()&lt;/code&gt;: saves the simulation state at this point.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;control.end()&lt;/code&gt;: terminates the simulation recording scope that had be started by &lt;code&gt;control.begin()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Caching LLM API Calls&lt;/h4&gt; &#xA;&lt;p&gt;This is enabled preferably in the &lt;code&gt;config.ini&lt;/code&gt; file, and alternativelly via the &lt;code&gt;openai_utils.force_api_cache()&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;LLM API caching, when enabled, works at a lower and simpler level than simulation state caching. Here, what happens is a very straightforward: every LLM call is kept in a map from the input to the generated output; when a new call comes and is identical to a previous one, the cached value is returned.&lt;/p&gt; &#xA;&lt;h3&gt;Config.ini&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;config.ini&lt;/code&gt; file contains various parameters that can be used to customize the behavior of the library, such as model parameters and logging level. Please pay special attention to &lt;code&gt;API_TYPE&lt;/code&gt; parameter, which defines whether you are using the Azure OpenAI Service or the OpenAI API. We provide an example of a &lt;code&gt;config.ini&lt;/code&gt; file, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/examples/config.ini&#34;&gt;./examples/config.ini&lt;/a&gt;, which you can use as a template for your own, or just modify to run the examples.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h3&gt;What and How to Contribute&lt;/h3&gt; &#xA;&lt;p&gt;We need all sorts of things, but we are looking mainly for new interesting use cases demonstrations, or even just domain-specific application ideas. If you are a domain expert in some area that could benefit from TinyTroupe, we&#39;d love to hear from you.&lt;/p&gt; &#xA;&lt;p&gt;Beyond that, many other aspects can be improved, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Memory mechanisms.&lt;/li&gt; &#xA; &lt;li&gt;Data grounding mechanisms.&lt;/li&gt; &#xA; &lt;li&gt;Reasoning mechanisms.&lt;/li&gt; &#xA; &lt;li&gt;New environment types.&lt;/li&gt; &#xA; &lt;li&gt;Interfacing with the external world.&lt;/li&gt; &#xA; &lt;li&gt;... and more ...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that anything that you contribute might be released as open-source (under MIT license).&lt;/p&gt; &#xA;&lt;p&gt;If you would like to make a contribution, please try to follow these general guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tiny naming convention&lt;/strong&gt;: If you are implementing a experimenter-facing simulated element (e.g., an agent or environment type) or closely related (e.g., agent factories, or content enrichers), and it sounds good, call your new &lt;em&gt;XYZ&lt;/em&gt; as &lt;em&gt;TinyXYZ&lt;/em&gt; :-) On the other hand, auxiliary and infrastructural mechanisms should not start with the &#34;Tiny&#34; prefix. The idea is to emphasize the simulated nature of the elements that are part of the simulation itself.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tests:&lt;/strong&gt; If you are writing some new mechanism, please also create at least a unit test &lt;code&gt;tests/unit/&lt;/code&gt;, and if you can a functional scenario test (&lt;code&gt;tests/scenarios/&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Demonstrations:&lt;/strong&gt; If you&#39;d like to demonstrate a new scenario, please design it preferably as a new Jupyter notebook within &lt;code&gt;examples/&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Microsoft:&lt;/strong&gt; If you are implementing anything that is Microsoft-specific and non-confidential, please put it under a &lt;code&gt;.../microsoft/&lt;/code&gt; folder.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;TinyTroupe started as an internal Microsoft hackathon project, and expanded over time. The TinyTroupe core team currently consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Paulo Salem (TinyTroupe&#39;s creator and current lead)&lt;/li&gt; &#xA; &lt;li&gt;Christopher Olsen (Engineering/Science)&lt;/li&gt; &#xA; &lt;li&gt;Paulo Freire (Engineering/Science)&lt;/li&gt; &#xA; &lt;li&gt;Yi Ding (Product Management)&lt;/li&gt; &#xA; &lt;li&gt;Prerit Saxena (Engineering/Science)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Current advisors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Robert Sim (Engineering/Science)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other special contributions were made by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nilo Garcia Silveira: initial agent validation ideas and related implementation; general initial feedback and insights; name suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Olnei Fonseca: initial agent validation ideas; general initial feedback and insights; naming suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Robert Sim: synthetic data generation scenarios expertise and implementation.&lt;/li&gt; &#xA; &lt;li&gt;Carlos Costa: synthetic data generation scenarios expertise and implementation.&lt;/li&gt; &#xA; &lt;li&gt;Bryant Key: advertising scenario domain expertise and insights.&lt;/li&gt; &#xA; &lt;li&gt;Barbara da Silva: implementation related to agent memory management.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... are you missing here? Please remind us!&lt;/p&gt; &#xA;&lt;h2&gt;Citing TinyTroupe&lt;/h2&gt; &#xA;&lt;p&gt;We are working on an introductory paper that will be the official academic citation for TinyTroupe. In the meantime, please just cite this repository including the core team members as authors. For instance:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Paulo Salem, Christopher Olsen, Paulo Freire, Yi Ding, Prerit Saxena (2024). &lt;strong&gt;TinyTroupe: LLM-powered multiagent persona simulation for imagination enhancement and business insights.&lt;/strong&gt; [Computer software]. GitHub repository. &lt;a href=&#34;https://github.com/microsoft/tinytroupe&#34;&gt;https://github.com/microsoft/tinytroupe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Or as bibtex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{tinytroupe,&#xA;  author = {Paulo Salem and Christopher Olsen and Paulo Freire and Yi Ding and Prerit Saxena},&#xA;  title = {TinyTroupe: LLM-powered multiagent persona simulation for imagination enhancement and business insights},&#xA;  year = {2024},&#xA;  howpublished = {\url{https://github.com/microsoft/tinytroupe}},&#xA;  note = {GitHub repository}&#xA;  }&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Legal Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;TinyTroupe is for research and simulation only. TinyTroupe is a research and experimental technology, which relies on Artificial Intelligence (AI) models to generate text content. The AI system output may include unrealistic, inappropriate, harmful or inaccurate results, including factual errors. You are responsible for reviewing the generated content (and adapting it if necessary) before using it, as you are fully responsible for determining its accuracy and fit for purpose. We advise using TinyTroupe‚Äôs outputs for insight generation and not for direct decision-making. Generated outputs do not reflect the opinions of Microsoft. You are fully responsible for any use you make of the generated outputs. For more information regarding the responsible use of this technology, see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/TinyTroupe/main/RESPONSIBLE_AI_FAQ.md&#34;&gt;RESPONSIBLE_AI_FAQ.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PROHIBITED USES&lt;/strong&gt;: TinyTroupe is not intended to simulate sensitive (e.g. violent or sexual) situations. Moreover, outputs must not be used to deliberately deceive, mislead or harm people in any way. You are fully responsible for any use you make and must comply with all applicable laws and regulations.‚Äù&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Huanshere/VideoLingo</title>
    <updated>2024-11-14T01:36:33Z</updated>
    <id>tag:github.com,2024-11-14:/Huanshere/VideoLingo</id>
    <link href="https://github.com/Huanshere/VideoLingo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | NetflixÁ∫ßÂ≠óÂπïÂàáÂâ≤„ÄÅÁøªËØë„ÄÅÂØπÈΩê„ÄÅÁîöËá≥Âä†‰∏äÈÖçÈü≥Ôºå‰∏ÄÈîÆÂÖ®Ëá™Âä®ËßÜÈ¢ëÊê¨ËøêAIÂ≠óÂπïÁªÑ&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/logo.png&#34; alt=&#34;VideoLingo Logo&#34; height=&#34;140&#34;&gt; &#xA; &lt;h1&gt;Connect the World, Frame by Frame&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://videolingo.io&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://docs.videolingo.io/docs/start&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://colab.research.google.com/github/Huanshere/VideoLingo/blob/main/VideoLingo_colab.ipynb&#34;&gt;Colab&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/README.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt;ÔΩú&lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/i18n/README.zh.md&#34;&gt;&lt;strong&gt;‰∏≠Êñá&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üåü Overview&lt;/h2&gt; &#xA;&lt;p&gt;VideoLingo is an all-in-one video translation, localization, and dubbing tool aimed at generating Netflix-quality subtitles. It eliminates stiff machine translations and multi-line subtitles while adding high-quality dubbing, enabling global knowledge sharing across language barriers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üé• YouTube video download via yt-dlp&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üéôÔ∏è Word-level subtitle recognition with WhisperX&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù NLP and GPT-based subtitle segmentation&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìö GPT-generated terminology for coherent translation&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîÑ 3-step direct translation, reflection, and adaptation for professional-level quality&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚úÖ Netflix-standard single-line subtitles only&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üó£Ô∏è Dubbing alignment with GPT-SoVITS and other methods&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üöÄ One-click startup and output in Streamlit&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üìù Detailed logging with progress resumption&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üåê Comprehensive multi-language support&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Difference from similar projects: &lt;strong&gt;Single-line subtitles only, superior translation quality&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üé• Demo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;h3&gt;Russian Translation&lt;/h3&gt; &#xA;    &lt;hr&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7&#34;&gt;https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;h3&gt;GPT-SoVITS&lt;/h3&gt; &#xA;    &lt;hr&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c&#34;&gt;https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt; &lt;h3&gt;OAITTS&lt;/h3&gt; &#xA;    &lt;hr&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/85c64f8c-06cf-4af9-b153-ee9d2897b768&#34;&gt;https://github.com/user-attachments/assets/85c64f8c-06cf-4af9-b153-ee9d2897b768&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Language Support:&lt;/h3&gt; &#xA;&lt;p&gt;Current input language support and examples:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Input Language&lt;/th&gt; &#xA;   &lt;th&gt;Support Level&lt;/th&gt; &#xA;   &lt;th&gt;Translation Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;ü§©&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/127373bb-c152-4b7a-8d9d-e586b2c62b4b&#34;&gt;English to Chinese&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Russian&lt;/td&gt; &#xA;   &lt;td&gt;üòä&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7&#34;&gt;Russian to Chinese&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;French&lt;/td&gt; &#xA;   &lt;td&gt;ü§©&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/3ce068c7-9854-4c72-ae77-f2484c7c6630&#34;&gt;French to Japanese&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;German&lt;/td&gt; &#xA;   &lt;td&gt;ü§©&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/07cb9d21-069e-4725-871d-c4d9701287a3&#34;&gt;German to Chinese&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Italian&lt;/td&gt; &#xA;   &lt;td&gt;ü§©&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/f1f893eb-dad3-4460-aaf6-10cac999195e&#34;&gt;Italian to Chinese&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spanish&lt;/td&gt; &#xA;   &lt;td&gt;ü§©&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/c1d28f1c-83d2-4f13-a1a1-859bd6cc3553&#34;&gt;Spanish to Chinese&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese&lt;/td&gt; &#xA;   &lt;td&gt;üòê&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/856c3398-2da3-4e25-9c36-27ca2d1f68c2&#34;&gt;Japanese to Chinese&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese*&lt;/td&gt; &#xA;   &lt;td&gt;ü§©&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/48f746fe-96ff-47fd-bd23-59e9202b495c&#34;&gt;Chinese to English&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;*Chinese requires separate configuration of the whisperX model, only applicable for local source code installation. See the installation documentation for the configuration process, and be sure to specify the transcription language as zh in the webpage sidebar&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Translation language support depends on the capabilities of the large language model used, while dubbing language depends on the chosen TTS method.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Online Experience&lt;/h3&gt; &#xA;&lt;p&gt;Commercial version provides free 20min credits, visit &lt;a href=&#34;https://videolingo.io&#34;&gt;videolingo.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Colab&lt;/h3&gt; &#xA;&lt;p&gt;Experience VideoLingo quickly in Colab in just 5 minutes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/Huanshere/VideoLingo/blob/main/VideoLingo_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Local Installation&lt;/h3&gt; &#xA;&lt;p&gt;VideoLingo supports all hardware platforms and operating systems, but performs best with GPU acceleration. For detailed installation instructions , refer to the documentation: &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/start.en-US.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/start.zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker Installation&lt;/h3&gt; &#xA;&lt;p&gt;VideoLingo provides a Dockerfile. Refer to the installation documentation: &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/docker.en-US.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/docs/pages/docs/docker.zh-CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üè≠ Batch Mode&lt;/h2&gt; &#xA;&lt;p&gt;Usage instructions: &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/batch/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Huanshere/VideoLingo/main/batch/README.zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Current Limitations&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;WhisperX performance varies across different devices. Version 1.7 performs demucs voice separation first, but this may result in worse transcription after separation compared to before. This is because whisper itself was trained in environments with background music - before separation it won&#39;t transcribe BGM lyrics, but after separation it might transcribe them.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;The dubbing feature quality may not be perfect&lt;/strong&gt; as it&#39;s still in testing and development stage, with plans to integrate MascGCT. For best results currently, it&#39;s recommended to choose TTS with similar speech rates based on the original video&#39;s speed and content characteristics. See the &lt;a href=&#34;https://www.bilibili.com/video/BV1mt1QYyERR/?share_source=copy_web&amp;amp;vd_source=fa92558c28cd668d33dabaddb17e2f9e&#34;&gt;demo&lt;/a&gt; for effects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual video transcription recognition will only retain the main language&lt;/strong&gt;. This is because whisperX uses a specialized model for a single language when forcibly aligning word-level subtitles, and will delete unrecognized languages.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-character separate dubbing is under development&lt;/strong&gt;. While whisperX has VAD potential, specific implementation work is needed, and this feature is not yet supported.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üöó Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; SaaS service at &lt;a href=&#34;https://videolingo.io&#34;&gt;videolingo.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; VAD to distinguish speakers, multi-character dubbing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Customizable translation styles&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Lip sync for dubbed videos&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License.The following open source projects provide important support for the development of VideoLingo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/m-bain/whisperX&#34;&gt;whisperX&lt;/a&gt; | &lt;a href=&#34;https://github.com/yt-dlp/yt-dlp&#34;&gt;yt-dlp&lt;/a&gt; | &lt;a href=&#34;https://github.com/mangiucugna/json_repair&#34;&gt;json_repair&lt;/a&gt; | &lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;GPT-SoVITS&lt;/a&gt; | &lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üì¨ Contact Us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Join our Discord: &lt;a href=&#34;https://discord.gg/9F2G92CWPp&#34;&gt;https://discord.gg/9F2G92CWPp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Submit &lt;a href=&#34;https://github.com/Huanshere/VideoLingo/issues&#34;&gt;Issues&lt;/a&gt; or &lt;a href=&#34;https://github.com/Huanshere/VideoLingo/pulls&#34;&gt;Pull Requests&lt;/a&gt; on GitHub&lt;/li&gt; &#xA; &lt;li&gt;Follow me on Twitter: &lt;a href=&#34;https://twitter.com/Huanshere&#34;&gt;@Huanshere&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email me at: &lt;a href=&#34;mailto:team@videolingo.io&#34;&gt;team@videolingo.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#Huanshere/VideoLingo&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Huanshere/VideoLingo&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt;If you find VideoLingo helpful, please give us a ‚≠êÔ∏è!&lt;/p&gt;</summary>
  </entry>
</feed>