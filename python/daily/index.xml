<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-11T01:34:52Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>snap-stanford/Biomni</title>
    <updated>2025-07-11T01:34:52Z</updated>
    <id>tag:github.com,2025-07-11:/snap-stanford/Biomni</id>
    <link href="https://github.com/snap-stanford/Biomni" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Biomni: a general-purpose biomedical AI agent&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/snap-stanford/Biomni/main/figs/biomni_logo.png&#34; alt=&#34;Biomni Logo&#34; width=&#34;600px&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://join.slack.com/t/biomnigroup/shared_invite/zt-38dat07mc-mmDIYzyCrNtV4atULTHRiw&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Join-Slack-4A154B?style=for-the-badge&amp;amp;logo=slack&#34; alt=&#34;Join Slack&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://biomni.stanford.edu&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Try-Web%20UI-blue?style=for-the-badge&#34; alt=&#34;Web UI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://x.com/ProjectBiomni&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Follow-on%20X-black?style=for-the-badge&amp;amp;logo=x&#34; alt=&#34;Follow on X&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/company/project-biomni&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Follow-LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&#34; alt=&#34;Follow on LinkedIn&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2025.05.30.656746v1&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Read-Paper-green?style=for-the-badge&#34; alt=&#34;Paper&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Biomni: A General-Purpose Biomedical AI Agent&lt;/h1&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Biomni is a general-purpose biomedical AI agent designed to autonomously execute a wide range of research tasks across diverse biomedical subfields. By integrating cutting-edge large language model (LLM) reasoning with retrieval-augmented planning and code-based execution, Biomni helps scientists dramatically enhance research productivity and generate testable hypotheses.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Our software environment is massive and we provide a single setup.sh script to setup. Follow this &lt;a href=&#34;https://raw.githubusercontent.com/snap-stanford/Biomni/main/biomni_env/README.md&#34;&gt;file&lt;/a&gt; to setup the env first.&lt;/p&gt; &#xA;&lt;p&gt;Then activate the environment E1:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate biomni_e1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;then install the latest biomni package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install biomni --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install from the github source version.&lt;/p&gt; &#xA;&lt;p&gt;Lastly, configure your API keys in bash profile &lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export ANTHROPIC_API_KEY=&#34;YOUR_API_KEY&#34;&#xA;export OPENAI_API_KEY=&#34;YOUR_API_KEY&#34; # optional if you just use Claude&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;p&gt;Once inside the environment, you can start using Biomni:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from biomni.agent import A1&#xA;&#xA;# Initialize the agent with data path, Data lake will be automatically downloaded on first run (~11GB)&#xA;agent = A1(path=&#39;./data&#39;, llm=&#39;claude-sonnet-4-20250514&#39;)&#xA;&#xA;# Execute biomedical tasks using natural language&#xA;agent.go(&#34;Plan a CRISPR screen to identify genes that regulate T cell exhaustion, generate 32 genes that maximize the perturbation effect.&#34;)&#xA;agent.go(&#34;Perform scRNA-seq annotation at [PATH] and generate meaningful hypothesis&#34;)&#xA;agent.go(&#34;Predict ADMET properties for this compound: CC(C)CC1=CC=C(C=C1)C(C)C(=O)O&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§ù Contributing to Biomni&lt;/h2&gt; &#xA;&lt;p&gt;Biomni is an open-science initiative that thrives on community contributions. We welcome:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîß New Tools&lt;/strong&gt;: Specialized analysis functions and algorithms&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìä Datasets&lt;/strong&gt;: Curated biomedical data and knowledge bases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üíª Software&lt;/strong&gt;: Integration of existing biomedical software packages&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìã Benchmarks&lt;/strong&gt;: Evaluation datasets and performance metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üìö Misc&lt;/strong&gt;: Tutorials, examples, and use cases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;üîß Update existing tools&lt;/strong&gt;: many current tools are not optimized - fix and replacements are welcome!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out this &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snap-stanford/Biomni/main/CONTRIBUTION.md&#34;&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; on how to contribute to the Biomni ecosystem.&lt;/p&gt; &#xA;&lt;p&gt;If you have particular tool/database/software in mind that you want to add, you can also submit to &lt;a href=&#34;https://forms.gle/nu2n1unzAYodTLVj6&#34;&gt;this form&lt;/a&gt; and the biomni team will implement them.&lt;/p&gt; &#xA;&lt;h2&gt;üî¨ Call for Contributors: Help Build Biomni-E2&lt;/h2&gt; &#xA;&lt;p&gt;Biomni-E1 only scratches the surface of what‚Äôs possible in the biomedical action space.&lt;/p&gt; &#xA;&lt;p&gt;Now, we‚Äôre building &lt;strong&gt;Biomni-E2&lt;/strong&gt; ‚Äî a next-generation environment developed &lt;strong&gt;with and for the community&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We believe that by collaboratively defining and curating a shared library of standard biomedical actions, we can accelerate science for everyone.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Join us in shaping the future of biomedical AI agent.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Contributors with significant impact&lt;/strong&gt; (e.g., 10+ significant &amp;amp; integrated tool contributions or equivalent) will be &lt;strong&gt;invited as co-authors&lt;/strong&gt; on our upcoming paper in a top-tier journal or conference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;All contributors&lt;/strong&gt; will be acknowledged in our publications.&lt;/li&gt; &#xA; &lt;li&gt;More contributor perks...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let‚Äôs build it together.&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials and Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/snap-stanford/Biomni/main/tutorials/biomni_101.ipynb&#34;&gt;Biomni 101&lt;/a&gt;&lt;/strong&gt; - Basic concepts and first steps&lt;/p&gt; &#xA;&lt;p&gt;More to come!&lt;/p&gt; &#xA;&lt;h2&gt;üåê Web Interface&lt;/h2&gt; &#xA;&lt;p&gt;Experience Biomni through our no-code web interface at &lt;strong&gt;&lt;a href=&#34;https://biomni.stanford.edu&#34;&gt;biomni.stanford.edu&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/E0BRvl23hLs&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/E0BRvl23hLs/maxresdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release schedule&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 8 Real-world research task benchmark/leaderboard release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; A tutorial on how to contribute to Biomni&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; A tutorial on baseline agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Biomni A1+E1 release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Note&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This release was frozen as of April 15 2025, so it differs from the current web platform.&lt;/li&gt; &#xA; &lt;li&gt;Biomni itself is Apache 2.0-licensed, but certain integrated tools, databases, or software may carry more restrictive commercial licenses. Review each component carefully before any commercial use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite Us&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{huang2025biomni,&#xA;  title={Biomni: A General-Purpose Biomedical AI Agent},&#xA;  author={Huang, Kexin and Zhang, Serena and Wang, Hanchen and Qu, Yuanhao and Lu, Yingzhou and Roohani, Yusuf and Li, Ryan and Qiu, Lin and Zhang, Junze and Di, Yin and others},&#xA;  journal={bioRxiv},&#xA;  pages={2025--05},&#xA;  year={2025},&#xA;  publisher={Cold Spring Harbor Laboratory}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ali-vilab/VACE</title>
    <updated>2025-07-11T01:34:52Z</updated>
    <id>tag:github.com,2025-07-11:/ali-vilab/VACE</id>
    <link href="https://github.com/ali-vilab/VACE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementations for paper: VACE: All-in-One Video Creation and Editing&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;VACE: All-in-One Video Creation and Editing&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;Zeyinzi Jiang&lt;sup&gt;*&lt;/sup&gt;&lt;/strong&gt; ¬∑ &lt;strong&gt;Zhen Han&lt;sup&gt;*&lt;/sup&gt;&lt;/strong&gt; ¬∑ &lt;strong&gt;Chaojie Mao&lt;sup&gt;*‚Ä†&lt;/sup&gt;&lt;/strong&gt; ¬∑ &lt;strong&gt;Jingfeng Zhang&lt;/strong&gt; ¬∑ &lt;strong&gt;Yulin Pan&lt;/strong&gt; ¬∑ &lt;strong&gt;Yu Liu&lt;/strong&gt; &lt;br&gt; &lt;b&gt;Tongyi Lab - &lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;&lt;img src=&#34;https://ali-vilab.github.io/VACE-Page/assets/logos/wan_logo.png&#34; alt=&#34;wan_logo&#34; style=&#34;margin-bottom: -4px; height: 20px;&#34;&gt;&lt;/a&gt; &lt;/b&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2503.07598&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/VACE-arXiv-red&#34; alt=&#34;Paper PDF&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ali-vilab.github.io/VACE-Page/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/VACE-Project_Page-green&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/collections/ali-vilab/vace-67eca186ff3e3564726aff38&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/VACE-HuggingFace_Model-yellow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/collections/VACE-8fa5fcfd386e43&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/VACE-ModelScope_Model-purple&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;VACE&lt;/strong&gt; is an all-in-one model designed for video creation and editing. It encompasses various tasks, including reference-to-video generation (&lt;strong&gt;R2V&lt;/strong&gt;), video-to-video editing (&lt;strong&gt;V2V&lt;/strong&gt;), and masked video-to-video editing (&lt;strong&gt;MV2V&lt;/strong&gt;), allowing users to compose these tasks freely. This functionality enables users to explore diverse possibilities and streamlines their workflows effectively, offering a range of capabilities, such as Move-Anything, Swap-Anything, Reference-Anything, Expand-Anything, Animate-Anything, and more.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/assets/materials/teaser.jpg&#34;&gt; &#xA;&lt;h2&gt;üéâ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; May 14, 2025: üî•Wan2.1-VACE-1.3B and Wan2.1-VACE-14B models are now available at &lt;a href=&#34;https://huggingface.co/Wan-AI/Wan2.1-VACE-14B&#34;&gt;HuggingFace&lt;/a&gt; and &lt;a href=&#34;https://www.modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B&#34;&gt;ModelScope&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mar 31, 2025: üî•VACE-Wan2.1-1.3B-Preview and VACE-LTX-Video-0.9 models are now available at &lt;a href=&#34;https://huggingface.co/collections/ali-vilab/vace-67eca186ff3e3564726aff38&#34;&gt;HuggingFace&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/collections/VACE-8fa5fcfd386e43&#34;&gt;ModelScope&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mar 31, 2025: üî•Release code of model inference, preprocessing, and gradio demos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mar 11, 2025: We propose &lt;a href=&#34;https://ali-vilab.github.io/VACE-Page/&#34;&gt;VACE&lt;/a&gt;, an all-in-one model for video creation and editing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü™Ñ Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Models&lt;/th&gt; &#xA;   &lt;th&gt;Download Link&lt;/th&gt; &#xA;   &lt;th&gt;Video Size&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VACE-Wan2.1-1.3B-Preview&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview&#34;&gt;Huggingface&lt;/a&gt; ü§ó &lt;a href=&#34;https://modelscope.cn/models/iic/VACE-Wan2.1-1.3B-Preview&#34;&gt;ModelScope&lt;/a&gt; ü§ñ&lt;/td&gt; &#xA;   &lt;td&gt;~ 81 x 480 x 832&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/blob/main/LICENSE.txt&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VACE-LTX-Video-0.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ali-vilab/VACE-LTX-Video-0.9&#34;&gt;Huggingface&lt;/a&gt; ü§ó &lt;a href=&#34;https://modelscope.cn/models/iic/VACE-LTX-Video-0.9&#34;&gt;ModelScope&lt;/a&gt; ü§ñ&lt;/td&gt; &#xA;   &lt;td&gt;~ 97 x 512 x 768&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.license.txt&#34;&gt;RAIL-M&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-VACE-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B&#34;&gt;Huggingface&lt;/a&gt; ü§ó &lt;a href=&#34;https://www.modelscope.cn/models/Wan-AI/Wan2.1-VACE-1.3B&#34;&gt;ModelScope&lt;/a&gt; ü§ñ&lt;/td&gt; &#xA;   &lt;td&gt;~ 81 x 480 x 832&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/blob/main/LICENSE.txt&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wan2.1-VACE-14B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Wan-AI/Wan2.1-VACE-14B&#34;&gt;Huggingface&lt;/a&gt; ü§ó &lt;a href=&#34;https://www.modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B&#34;&gt;ModelScope&lt;/a&gt; ü§ñ&lt;/td&gt; &#xA;   &lt;td&gt;~ 81 x 720 x 1280&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/blob/main/LICENSE.txt&#34;&gt;Apache-2.0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The input supports any resolution, but to achieve optimal results, the video size should fall within a specific range.&lt;/li&gt; &#xA; &lt;li&gt;All models inherit the license of the original model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Installation&lt;/h2&gt; &#xA;&lt;p&gt;The codebase was tested with Python 3.10.13, CUDA version 12.4, and PyTorch &amp;gt;= 2.5.1.&lt;/p&gt; &#xA;&lt;h3&gt;Setup for Model Inference&lt;/h3&gt; &#xA;&lt;p&gt;You can setup for VACE model inference by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ali-vilab/VACE.git &amp;amp;&amp;amp; cd VACE&#xA;pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124  # If PyTorch is not installed.&#xA;pip install -r requirements.txt&#xA;pip install wan@git+https://github.com/Wan-Video/Wan2.1  # If you want to use Wan2.1-based VACE.&#xA;pip install ltx-video@git+https://github.com/Lightricks/LTX-Video@ltx-video-0.9.1 sentencepiece --no-deps # If you want to use LTX-Video-0.9-based VACE. It may conflict with Wan.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please download your preferred base model to &lt;code&gt;&amp;lt;repo-root&amp;gt;/models/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Setup for Preprocess Tools&lt;/h3&gt; &#xA;&lt;p&gt;If you need preprocessing tools, please install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements/annotator.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please download &lt;a href=&#34;https://huggingface.co/ali-vilab/VACE-Annotators&#34;&gt;VACE-Annotators&lt;/a&gt; to &lt;code&gt;&amp;lt;repo-root&amp;gt;/models/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local Directories Setup&lt;/h3&gt; &#xA;&lt;p&gt;It is recommended to download &lt;a href=&#34;https://huggingface.co/datasets/ali-vilab/VACE-Benchmark&#34;&gt;VACE-Benchmark&lt;/a&gt; to &lt;code&gt;&amp;lt;repo-root&amp;gt;/benchmarks/&lt;/code&gt; as examples in &lt;code&gt;run_vace_xxx.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We recommend to organize local directories as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;VACE&#xA;‚îú‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ benchmarks&#xA;‚îÇ   ‚îî‚îÄ‚îÄ VACE-Benchmark&#xA;‚îÇ       ‚îî‚îÄ‚îÄ assets&#xA;‚îÇ           ‚îî‚îÄ‚îÄ examples&#xA;‚îÇ               ‚îú‚îÄ‚îÄ animate_anything&#xA;‚îÇ               ‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îÇ               ‚îî‚îÄ‚îÄ ...&#xA;‚îú‚îÄ‚îÄ models&#xA;‚îÇ   ‚îú‚îÄ‚îÄ VACE-Annotators&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îú‚îÄ‚îÄ VACE-LTX-Video-0.9&#xA;‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...&#xA;‚îÇ   ‚îî‚îÄ‚îÄ VACE-Wan2.1-1.3B-Preview&#xA;‚îÇ       ‚îî‚îÄ‚îÄ ...&#xA;‚îî‚îÄ‚îÄ ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA;&lt;p&gt;In VACE, users can input &lt;strong&gt;text prompt&lt;/strong&gt; and optional &lt;strong&gt;video&lt;/strong&gt;, &lt;strong&gt;mask&lt;/strong&gt;, and &lt;strong&gt;image&lt;/strong&gt; for video generation or editing. Detailed instructions for using VACE can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/UserGuide.md&#34;&gt;User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Inference CIL&lt;/h3&gt; &#xA;&lt;h4&gt;1) End-to-End Running&lt;/h4&gt; &#xA;&lt;p&gt;To simply run VACE without diving into any implementation details, we suggest an end-to-end pipeline. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# run V2V depth&#xA;python vace/vace_pipeline.py --base wan --task depth --video assets/videos/test.mp4 --prompt &#39;xxx&#39;&#xA;&#xA;# run MV2V inpainting by providing bbox&#xA;python vace/vace_pipeline.py --base wan --task inpainting --mode bbox --bbox 50,50,550,700 --video assets/videos/test.mp4 --prompt &#39;xxx&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script will run video preprocessing and model inference sequentially, and you need to specify all the required args of preprocessing (&lt;code&gt;--task&lt;/code&gt;, &lt;code&gt;--mode&lt;/code&gt;, &lt;code&gt;--bbox&lt;/code&gt;, &lt;code&gt;--video&lt;/code&gt;, etc.) and inference (&lt;code&gt;--prompt&lt;/code&gt;, etc.). The output video together with intermediate video, mask and images will be saved into &lt;code&gt;./results/&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí°&lt;strong&gt;Note&lt;/strong&gt;: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/run_vace_pipeline.sh&#34;&gt;run_vace_pipeline.sh&lt;/a&gt; for usage examples of different task pipelines.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2) Preprocessing&lt;/h4&gt; &#xA;&lt;p&gt;To have more flexible control over the input, before VACE model inference, user inputs need to be preprocessed into &lt;code&gt;src_video&lt;/code&gt;, &lt;code&gt;src_mask&lt;/code&gt;, and &lt;code&gt;src_ref_images&lt;/code&gt; first. We assign each &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/vace/configs/__init__.py&#34;&gt;preprocessor&lt;/a&gt; a task name, so simply call &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/vace/vace_preproccess.py&#34;&gt;&lt;code&gt;vace_preprocess.py&lt;/code&gt;&lt;/a&gt; and specify the task name and task params. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-angular2html&#34;&gt;# process video depth&#xA;python vace/vace_preproccess.py --task depth --video assets/videos/test.mp4&#xA;&#xA;# process video inpainting by providing bbox&#xA;python vace/vace_preproccess.py --task inpainting --mode bbox --bbox 50,50,550,700 --video assets/videos/test.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The outputs will be saved to &lt;code&gt;./processed/&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí°&lt;strong&gt;Note&lt;/strong&gt;: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/run_vace_pipeline.sh&#34;&gt;run_vace_pipeline.sh&lt;/a&gt; preprocessing methods for different tasks. Moreover, refer to &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/vace/configs/&#34;&gt;vace/configs/&lt;/a&gt; for all the pre-defined tasks and required params. You can also customize preprocessors by implementing at &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/vace/annotators/__init__.py&#34;&gt;&lt;code&gt;annotators&lt;/code&gt;&lt;/a&gt; and register them at &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/vace/configs&#34;&gt;&lt;code&gt;configs&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;3) Model inference&lt;/h4&gt; &#xA;&lt;p&gt;Using the input data obtained from &lt;strong&gt;Preprocessing&lt;/strong&gt;, the model inference process can be performed as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Wan2.1 single GPU inference (1.3B-480P)&#xA;python vace/vace_wan_inference.py --ckpt_dir &amp;lt;path-to-model&amp;gt; --src_video &amp;lt;path-to-src-video&amp;gt; --src_mask &amp;lt;path-to-src-mask&amp;gt; --src_ref_images &amp;lt;paths-to-src-ref-images&amp;gt; --prompt &#34;xxx&#34;&#xA;&#xA;# For Wan2.1 Multi GPU Acceleration inference (1.3B-480P)&#xA;pip install &#34;xfuser&amp;gt;=0.4.1&#34;&#xA;torchrun --nproc_per_node=8 vace/vace_wan_inference.py --dit_fsdp --t5_fsdp --ulysses_size 1 --ring_size 8 --ckpt_dir &amp;lt;path-to-model&amp;gt; --src_video &amp;lt;path-to-src-video&amp;gt; --src_mask &amp;lt;path-to-src-mask&amp;gt; --src_ref_images &amp;lt;paths-to-src-ref-images&amp;gt; --prompt &#34;xxx&#34;&#xA;&#xA;# For Wan2.1 Multi GPU Acceleration inference (14B-720P)&#xA;torchrun --nproc_per_node=8 vace/vace_wan_inference.py --dit_fsdp --t5_fsdp --ulysses_size 8 --ring_size 1 --size 720p --model_name &#39;vace-14B&#39; --ckpt_dir &amp;lt;path-to-model&amp;gt; --src_video &amp;lt;path-to-src-video&amp;gt; --src_mask &amp;lt;path-to-src-mask&amp;gt; --src_ref_images &amp;lt;paths-to-src-ref-images&amp;gt; --prompt &#34;xxx&#34;&#xA;&#xA;# For LTX inference, run&#xA;python vace/vace_ltx_inference.py --ckpt_path &amp;lt;path-to-model&amp;gt; --text_encoder_path &amp;lt;path-to-model&amp;gt; --src_video &amp;lt;path-to-src-video&amp;gt; --src_mask &amp;lt;path-to-src-mask&amp;gt; --src_ref_images &amp;lt;paths-to-src-ref-images&amp;gt; --prompt &#34;xxx&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output video together with intermediate video, mask and images will be saved into &lt;code&gt;./results/&lt;/code&gt; by default.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üí°&lt;strong&gt;Note&lt;/strong&gt;: (1) Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/vace/vace_wan_inference.py&#34;&gt;vace/vace_wan_inference.py&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/ali-vilab/VACE/main/vace/vace_ltx_inference.py&#34;&gt;vace/vace_ltx_inference.py&lt;/a&gt; for the inference args. (2) For LTX-Video and English language Wan2.1 users, you need prompt extension to unlock the full model performance. Please follow the &lt;a href=&#34;https://github.com/Wan-Video/Wan2.1?tab=readme-ov-file#2-using-prompt-extension&#34;&gt;instruction of Wan2.1&lt;/a&gt; and set &lt;code&gt;--use_prompt_extend&lt;/code&gt; while running inference. (3) When performing prompt extension in editing tasks, it&#39;s important to pay attention to the results of expanding plain text. Since the visual information being input is unknown, this may lead to the extended output not matching the video being edited, which can affect the final outcome.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Inference Gradio&lt;/h3&gt; &#xA;&lt;p&gt;For preprocessors, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python vace/gradios/vace_preprocess_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For model inference, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Wan2.1 gradio inference&#xA;python vace/gradios/vace_wan_demo.py&#xA;&#xA;# For LTX gradio inference&#xA;python vace/gradios/vace_ltx_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We are grateful for the following awesome projects, including &lt;a href=&#34;https://github.com/modelscope/scepter&#34;&gt;Scepter&lt;/a&gt;, &lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Lightricks/LTX-Video&#34;&gt;LTX-Video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{vace,&#xA;    title = {VACE: All-in-One Video Creation and Editing},&#xA;    author = {Jiang, Zeyinzi and Han, Zhen and Mao, Chaojie and Zhang, Jingfeng and Pan, Yulin and Liu, Yu},&#xA;    journal = {arXiv preprint arXiv:2503.07598},&#xA;    year = {2025}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/smollm</title>
    <updated>2025-07-11T01:34:52Z</updated>
    <id>tag:github.com,2025-07-11:/huggingface/smollm</id>
    <link href="https://github.com/huggingface/smollm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Everything about the SmolLM and SmolVLM family of models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Smol Models ü§è&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to Smol Models, a family of efficient and lightweight AI models from Hugging Face. Our mission is to create fully open powerful yet compact models, for text and vision, that can run effectively on-device while maintaining strong performance.&lt;/p&gt; &#xA;&lt;h2&gt;[NEW] SmolLM3 (Language Model)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/2bf61ea2-8d2e-426b-ba40-0242d34325d2&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our 3B model outperforms Llama 3.2 3B and Qwen2.5 3B while staying competitive with larger 4B alternatives (Qwen3 &amp;amp; Gemma3). Beyond the performance numbers, we&#39;re sharing exactly how we built it using public datasets and training frameworks.&lt;/p&gt; &#xA;&lt;p&gt;Ressources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hf.co/HuggingFaceTB/SmolLM3-3B-Base&#34;&gt;SmolLM3-Base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hf.co/HuggingFaceTB/SmolLM3-3B&#34;&gt;SmolLM3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hf.co/blog/smollm3&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Summary:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;3B model&lt;/strong&gt; trained on 11T tokens, SoTA at the 3B scale and competitive with 4B models&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fully open model&lt;/strong&gt;, open weights + full training details including public data mixture and training configs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instruct model&lt;/strong&gt; with &lt;strong&gt;dual mode reasoning,&lt;/strong&gt; supporting think/no_think modes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multilingual support&lt;/strong&gt; for 6 languages: English, French, Spanish, German, Italian, and Portuguese&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long context&lt;/strong&gt; up to 128k with NoPE and using YaRN&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/f1b76d3b-af2b-4218-91b3-4ce815bdf0a8&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üëÅÔ∏è SmolVLM (Vision Language Model)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct&#34;&gt;SmolVLM&lt;/a&gt; is our compact multimodal model that can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Process both images and text and perform tasks like visual QA, image description, and visual storytelling&lt;/li&gt; &#xA; &lt;li&gt;Handle multiple images in a single conversation&lt;/li&gt; &#xA; &lt;li&gt;Run efficiently on-device&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Repository Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;smollm/&#xA;‚îú‚îÄ‚îÄ text/               # SmolLM3/2/1 related code and resources&#xA;‚îú‚îÄ‚îÄ vision/            # SmolVLM related code and resources&#xA;‚îî‚îÄ‚îÄ tools/             # Shared utilities and inference tools&#xA;    ‚îú‚îÄ‚îÄ smol_tools/    # Lightweight AI-powered tools&#xA;    ‚îú‚îÄ‚îÄ smollm_local_inference/&#xA;    ‚îî‚îÄ‚îÄ smolvlm_local_inference/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;SmolLM3&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;HuggingFaceTB/SmolLM3-3B&#34;&#xA;device = &#34;cuda&#34;  # for GPU usage or &#34;cpu&#34; for CPU usage&#xA;&#xA;# load the tokenizer and the model&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;).to(device)&#xA;&#xA;# prepare the model input&#xA;prompt = &#34;Give me a brief explanation of gravity in simple terms.&#34;&#xA;messages_think = [&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;&#xA;text = tokenizer.apply_chat_template(&#xA;    messages_think,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True,&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;# Generate the output&#xA;generated_ids = model.generate(**model_inputs, max_new_tokens=32768)&#xA;&#xA;# Get and decode the output&#xA;output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]&#xA;print(tokenizer.decode(output_ids, skip_special_tokens=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SmolVLM&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoProcessor, AutoModelForVision2Seq&#xA;&#xA;processor = AutoProcessor.from_pretrained(&#34;HuggingFaceTB/SmolVLM-Instruct&#34;)&#xA;model = AutoModelForVision2Seq.from_pretrained(&#34;HuggingFaceTB/SmolVLM-Instruct&#34;)&#xA;&#xA;messages = [&#xA;    {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: [&#xA;            {&#34;type&#34;: &#34;image&#34;},&#xA;            {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What&#39;s in this image?&#34;}&#xA;        ]&#xA;    }&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Ecosystem&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/RvHjdlRT5gGQt5mJuhXH9.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smollm/main/text/README.md&#34;&gt;SmolLM3 Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smollm/main/vision/README.md&#34;&gt;SmolVLM Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/smollm/main/tools/README.md&#34;&gt;Local Inference Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&#34;&gt;SmolLM3 Models Collection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9&#34;&gt;SmolLM2 Models Collection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct&#34;&gt;SmolVLM Model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/collections/HuggingFaceTB/smollm3-pretraining-datasets-685a7353fdc01aecde51b1d9&#34;&gt;SmolLM3 Pretraining dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceTB/smoltalk&#34;&gt;SmolTalk&lt;/a&gt; - Our instruction-tuning dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceTB/finemath&#34;&gt;FineMath&lt;/a&gt; - Mathematics pretraining dataset&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu&#34;&gt;FineWeb-Edu&lt;/a&gt; - Educational content pretraining dataset&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>