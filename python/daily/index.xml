<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-16T01:41:13Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tatsu-lab/stanford_alpaca</title>
    <updated>2023-03-16T01:41:13Z</updated>
    <id>tag:github.com,2023-03-16:/tatsu-lab/stanford_alpaca</id>
    <link href="https://github.com/tatsu-lab/stanford_alpaca" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code and documentation to train Stanford&#39;s Alpaca models, and generate the data.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a href=&#34;https://crfm.stanford.edu/alpaca/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/assets/logo.png&#34; alt=&#34;Stanford-Alpaca&#34; style=&#34;width: 50%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Stanford Alpaca: An Instruction-following LLaMA Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://crfm.stanford.edu/alpaca/&#34;&gt;&lt;strong&gt;web demo&lt;/strong&gt;&lt;/a&gt; to interact with our Alpaca model&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/#data-release&#34;&gt;52K data&lt;/a&gt; used for fine-tuning the model&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/#data-generation-process&#34;&gt;generating the data&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The current Alpaca model is fine-tuned from a 7B LLaMA model [1] on 52K instruction-following data generated by the techniques in the Self-Instruct [2] paper, with some modifications that we discuss in the next section. In a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the &lt;code&gt;text-davinci-003&lt;/code&gt; model on the Self-Instruct instruction-following evaluation suite [2].&lt;/p&gt; &#xA;&lt;p&gt;Alpaca is still under development, and there are many limitations that have to be addressed. Importantly, we have not yet fine-tuned the Alpaca model to be safe and harmless. We thus encourage users to be cautious when interacting with Alpaca, and to report any concerning behavior to help improve the safety and ethical considerations of the model.&lt;/p&gt; &#xA;&lt;p&gt;Our initial release contains the data generation procedure, dataset, and training recipe. We intend to release the model weights if we are given permission to do so by the creators of LLaMA. For now, we have chosen to host a live demo to help readers better understand the capabilities and limits of Alpaca, as well as a way to help us better evaluate Alpaca&#39;s performance on a broader audience.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please read our release &lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34;&gt;blog post&lt;/a&gt; for more details about the model, our discussion of the potential harm and limitations of Alpaca models, and our thought process for releasing a reproducible model.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[1]: LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;https://arxiv.org/abs/2302.13971v1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;https://arxiv.org/abs/2212.10560&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Data Release&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json&#34;&gt;&lt;code&gt;alpaca_data.json&lt;/code&gt;&lt;/a&gt; contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 52K instructions is unique.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;input&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, optional context or input for the task. For example, when the instruction is &#34;Summarize the following article&#34;, the input is the article. Around 40% of the examples have an input.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;text-davinci-003&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We used the following prompts for fine-tuning the Alpaca model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with a non-empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Input:&#xA;{input}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with an empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During inference (eg for the web demo), we use the user instruction with an empty input field (second option).&lt;/p&gt; &#xA;&lt;h2&gt;Data Generation Process&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt; Running the code &lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set environment variables &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your OpenAI API key.&lt;/li&gt; &#xA;  &lt;li&gt;Install the dependencies with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;python -m generate_instruction generate_instruction_following_data&lt;/code&gt; to generate the data.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;We built on the data generation pipeline from &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt; and made the following modifications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We used &lt;code&gt;text-davinci-003&lt;/code&gt; to generate the instruction data instead of &lt;code&gt;davinci&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We wrote a new prompt (&lt;code&gt;prompt.txt&lt;/code&gt;) that explicitly gave the requirement of instruction generation to &lt;code&gt;text-davinci-003&lt;/code&gt;. Note: there is a slight error in the prompt we used, and future users should incorporate the edit in &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/pull/24&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca/pull/24&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;We adopted much more aggressive batch decoding, i.e., generating 20 instructions at once, which significantly reduced the cost of data generation.&lt;/li&gt; &#xA; &lt;li&gt;We simplified the data generation pipeline by discarding the difference between classification and non-classification instructions.&lt;/li&gt; &#xA; &lt;li&gt;We only generated a single instance for each instruction, instead of 2 to 3 instances as in [1].&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This produced an instruction-following dataset with 52K examples obtained at a much lower cost (less than $500). In a preliminary study, we also find our 52K generated data to be much more diverse than the data released by &lt;a href=&#34;https://github.com/yizhongw/self-instruct/raw/main/data/seed_tasks.jsonl&#34;&gt;self-instruct&lt;/a&gt;. We plot the below figure (in the style of Figure 2 in the &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;self-instruct paper&lt;/a&gt; to demonstrate the diversity of our data. The inner circle of the plot represents the root verb of the instructions, and the outer circle represents the direct objects.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/assets/parse_analysis.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/assets/parse_analysis.png&#34; width=&#34;750&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;We fine-tune our models using standard Hugging Face training code with the following hyperparameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Batch size&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning rate&lt;/td&gt; &#xA;   &lt;td&gt;2e-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Epochs&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Max length&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Weight decay&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Given Hugging Face hasn&#39;t officially supported the LLaMA models, we fine-tuned LLaMA with Hugging Face&#39;s transformers library by installing it from a particular fork (i.e. this &lt;a href=&#34;https://github.com/huggingface/transformers/pull/21955&#34;&gt;PR&lt;/a&gt; to be merged). The hash of the specific commit we installed was &lt;code&gt;68d640f7c368bcaaaecfc678f11908ebbd3d6176&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce our fine-tuning runs for LLaMA, first install the requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the particular fork of Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Below is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP &lt;code&gt;full_shard&lt;/code&gt; mode. We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using &lt;strong&gt;Python 3.10&lt;/strong&gt;. Replace &lt;code&gt;&amp;lt;your_random_port&amp;gt;&lt;/code&gt; with a port of your own, &lt;code&gt;&amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt;&lt;/code&gt; with the path to your converted checkpoint and tokenizer (following instructions in the PR), and &lt;code&gt;&amp;lt;your_output_dir&amp;gt;&lt;/code&gt; with where you want to store your outputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt; \&#xA;    --data_path ./alpaca_data.json \&#xA;    --bf16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2000 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LLaMADecoderLayer&#39; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same script also works for OPT fine-tuning. Here&#39;s an example for fine-tuning OPT-6.7B&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &#34;facebook/opt-6.7b&#34; \&#xA;    --data_path ./alpaca_data.json \&#xA;    --bf16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2000 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;OPTDecoderLayer&#39; \&#xA;    --tf32 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the given training script is meant to be simple and easy to use, and is not particularly optimized. To run on more gpus, you may prefer to turn down &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to keep a global batch size of 128. Global batch size has not been tested for optimality.&lt;/p&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;p&gt;All grad students below contributed equally and the order is determined by random draw.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rohantaori.com/&#34;&gt;Rohan Taori&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ishaan.io/&#34;&gt;Ishaan Gulrajani&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tiiiger.github.io/&#34;&gt;Tianyi Zhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yanndubs.github.io/&#34;&gt;Yann Dubois&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lxuechen.com/&#34;&gt;Xuechen Li&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All advised by &lt;a href=&#34;https://thashim.github.io/&#34;&gt;Tatsunori B. Hashimoto&lt;/a&gt;. Yann is also advised by &lt;a href=&#34;https://cs.stanford.edu/~pliang/&#34;&gt;Percy Liang&lt;/a&gt; and Xuechen is also advised by &lt;a href=&#34;https://guestrin.su.domains/&#34;&gt;Carlos Guestrin&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;Please cite the repo if you use the data or code in this repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{alpaca,&#xA;  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },&#xA;  title = {Stanford Alpaca: An Instruction-following LLaMA model},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Naturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2].&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgements&lt;/h3&gt; &#xA;&lt;p&gt;We thank Yizhong Wang for his help in explaining the data generation pipeline in Self-Instruct and providing the code for the parse analysis plot. We thank Yifan Mai for helpful support, and members of the Stanford NLP Group as well as the Center for Research on Foundation Models (CRFM) for their helpful feedback.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lifeiteng/vall-e</title>
    <updated>2023-03-16T01:41:13Z</updated>
    <id>tag:github.com,2023-03-16:/lifeiteng/vall-e</id>
    <link href="https://github.com/lifeiteng/vall-e" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch implementation of VALL-E(Zero-Shot Text-To-Speech), Reproduced Demo https://lifeiteng.github.io/valle/index.html&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Language : 🇺🇸 | &lt;a href=&#34;https://raw.githubusercontent.com/lifeiteng/vall-e/main/README.zh-CN.md&#34;&gt;🇨🇳&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An unofficial PyTorch implementation of VALL-E(&lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;We can train the VALL-E model on one GPU.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lifeiteng/vall-e/main/docs/images/Overview.jpg&#34; alt=&#34;model&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference: In-Context Learning via Prompting&lt;/h2&gt; &#xA;&lt;p&gt;see &lt;a href=&#34;https://github.com/lifeiteng/vall-e/raw/main/egs/libritts/README.md#inference&#34;&gt;LibriTTS/Inference&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lifeiteng/vall-e/main/docs/images/vallf.png&#34; width=&#34;500&#34; height=&#34;400&#34;&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://valle-demo.github.io/&#34;&gt;official demo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Broader impacts&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Since VALL-E could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To avoid abuse, Well-trained models and services will not be provided.&lt;/p&gt; &#xA;&lt;h2&gt;Progress&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/feiteng&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://cdn.buymeacoffee.com/buttons/v2/default-blue.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: 40px !important;width: 145px !important;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Text and Audio Tokenizer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset module and loaders&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; VALL-F: &lt;code&gt;seq-to-seq + PrefixLanguageModel&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AR Decoder&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; NonAR Decoder&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; VALL-E: &lt;code&gt;PrefixLanguageModel&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AR Decoder&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; NonAR Decoder&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; update README.zh-CN&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference: In-Context Learning via Prompting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To get up and running quickly just follow the steps below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# PyTorch&#xA;pip install torch==1.13.1 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116&#xA;pip install torchmetrics==0.11.1&#xA;# fbank&#xA;pip install librosa==0.8.1&#xA;&#xA;# phonemizer&#xA;apt-get install espeak-ng&#xA;## OSX: brew install espeak&#xA;pip install phonemizer&#xA;&#xA;# lhotse&#xA;# https://github.com/lhotse-speech/lhotse/pull/956&#xA;# https://github.com/lhotse-speech/lhotse/pull/960&#xA;pip uninstall lhotse&#xA;pip uninstall lhotse&#xA;pip install git+https://github.com/lhotse-speech/lhotse&#xA;&#xA;# k2 icefall&#xA;# pip install k2&#xA;git clone https://github.com/k2-fsa/k2.git&#xA;cd k2&#xA;export PATH=/usr/local/cuda/bin:${PATH}&#xA;export K2_MAKE_ARGS=&#34;-j12&#34;&#xA;export K2_CMAKE_ARGS=&#34;-DK2_WITH_CUDA=OFF&#34;&#xA;python setup.py install&#xA;cd -&#xA;&#xA;git clone https://github.com/k2-fsa/icefall&#xA;cd icefall&#xA;pip install -r requirements.txt&#xA;export PYTHONPATH=`pwd`/../icefall:$PYTHONPATH&#xA;echo &#34;export PYTHONPATH=`pwd`/../icefall:\$PYTHONPATH&#34; &amp;gt;&amp;gt; ~/.zshrc&#xA;echo &#34;export PYTHONPATH=`pwd`/../icefall:\$PYTHONPATH&#34; &amp;gt;&amp;gt; ~/.bashrc&#xA;cd -&#xA;source ~/.zshrc&#xA;&#xA;# valle&#xA;git clone https://github.com/lifeiteng/valle.git&#xA;cd valle&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lifeiteng/vall-e/main/egs/libritts/README.md&#34;&gt;egs/libritts/README.md&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;SummaryWriter segmentation fault (core dumped)&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LINE &lt;code&gt;tb_writer = SummaryWriter(log_dir=f&#34;{params.exp_dir}/tensorboard&#34;)&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;FIX &lt;a href=&#34;https://github.com/tensorflow/tensorboard/pull/6135/files&#34;&gt;https://github.com/tensorflow/tensorboard/pull/6135/files&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;file=`python  -c &#39;import site; print(f&#34;{site.getsitepackages()[0]}/tensorboard/summary/writer/event_file_writer.py&#34;)&#39;`&#xA;sed -i &#39;s/import tf/import tensorflow_stub as tf/g&#39; $file&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Parallelize bin/tokenizer.py on multi-GPUs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.buymeacoffee.com/feiteng&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://cdn.buymeacoffee.com/buttons/v2/default-blue.png&#34; alt=&#34;Buy Me A Coffee&#34; style=&#34;height: 40px !important;width: 145px !important;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;To cite this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{valle,&#xA;  author={Feiteng Li},&#xA;  title={VALL-E: A neural codec language model},&#xA;  year={2023},&#xA;  url={http://github.com/lifeiteng/valle}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{VALL-E,&#xA;  title     = {Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers},&#xA;  author    = {Chengyi Wang, Sanyuan Chen, Yu Wu,&#xA;               Ziqiang Zhang, Long Zhou, Shujie Liu,&#xA;               Zhuo Chen, Yanqing Liu, Huaming Wang,&#xA;               Jinyu Li, Lei He, Sheng Zhao, Furu Wei},&#xA;  year      = {2023},&#xA;  eprint    = {2301.02111},&#xA;  archivePrefix = {arXiv},&#xA;  volume    = {abs/2301.02111},&#xA;  url       = {http://arxiv.org/abs/2301.02111},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>MasterBin-IIAU/UNINEXT</title>
    <updated>2023-03-16T01:41:13Z</updated>
    <id>tag:github.com,2023-03-16:/MasterBin-IIAU/UNINEXT</id>
    <link href="https://github.com/MasterBin-IIAU/UNINEXT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR&#39;23] Universal Instance Perception as Object Discovery and Retrieval&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Universal Instance Perception as Object Discovery and Retrieval&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/Framework.png&#34; alt=&#34;UNINEXT&#34;&gt; This is the official implementation of the paper &lt;a href=&#34;https://arxiv.org/abs/2303.06674&#34;&gt;Universal Instance Perception as Object Discovery and Retrieval&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/visual-object-tracking-on-lasot-ext?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot-ext&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/visual-object-tracking-on-lasot?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/visual-object-tracking-on-trackingnet?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-trackingnet&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/visual-tracking-on-tnl2k?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-tracking-on-tnl2k&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multiple-object-tracking-on-bdd100k-val?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/multiple-object-tracking-on-bdd100k-val&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-and-segmentation-on-3?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/multi-object-tracking-and-segmentation-on-3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-youtube-vis-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-ovis-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-refer-1?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refer-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-davis?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-davis&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-comprehension-on?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco-3?p=universal-instance-perception-as-object&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco-3&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Highlight&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;UNINEXT is accepted by &lt;strong&gt;CVPR2023&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;UNINEXT reformulates diverse instance perception tasks into &lt;strong&gt;a unified object discovery and retrieval paradigm&lt;/strong&gt; and can flexibly perceive different types of objects by simply changing the input prompts.&lt;/li&gt; &#xA; &lt;li&gt;UNINEXT achieves &lt;strong&gt;superior performance on 20 challenging benchmarks using a single model with the same model parameters&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/task-radar.png&#34; alt=&#34;TASK-RADAR&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Object-centric understanding is one of the most essential and challenging problems in computer vision. In this work, we mainly discuss 10 sub-tasks, distributed on the vertices of the cube shown in the above figure. Since all these tasks aim to perceive instances of certain properties, UNINEXT reorganizes them into three types according to the different input prompts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Category Names &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Object Detection&lt;/li&gt; &#xA;   &lt;li&gt;Instance Segmentation&lt;/li&gt; &#xA;   &lt;li&gt;Multiple Object Tracking (MOT)&lt;/li&gt; &#xA;   &lt;li&gt;Multi-Object Tracking and Segmentation (MOTS)&lt;/li&gt; &#xA;   &lt;li&gt;Video Instance Segmentation (VIS)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Language Expressions &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Referring Expression Comprehension (REC)&lt;/li&gt; &#xA;   &lt;li&gt;Referring Expression Segmentation (RES)&lt;/li&gt; &#xA;   &lt;li&gt;Referring Video Object Segmentation (R-VOS)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Target Annotations &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Single Object Tracking (SOT)&lt;/li&gt; &#xA;   &lt;li&gt;Video Object Segmentation (VOS)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Then we propose a unified prompt-guided object discovery and retrieval formulation to solve all the above tasks. Extensive experiments demonstrate that UNINEXT achieves superior performance on 20 challenging benchmarks.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/40926230/224527028-f31e8de0-b8aa-4cfb-a83b-63a70ff5bd52.mp4&#34;&gt;https://user-images.githubusercontent.com/40926230/224527028-f31e8de0-b8aa-4cfb-a83b-63a70ff5bd52.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;UNINEXT can flexibly perceive various types of objects by simply changing the input prompts, such as category names, language expressions, and target annotations. We also provide a simple &lt;a href=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/demo.sh&#34;&gt;demo script&lt;/a&gt;, which supports 4 image-level tasks (object detection, instance segmentation, REC, RES).&lt;/p&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Retrieval by Category Names&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/res-od.png&#34; alt=&#34;OD-IS&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/res-vis-mots.png&#34; alt=&#34;MOT-MOTS-VIS&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Retrieval by Language Expressions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/res-rec-res-rvos.png&#34; alt=&#34;REC-RES-RVOS&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Retrieval by Target Annotations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/res-sot-vos.png&#34; alt=&#34;SOT-VOS&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Installation: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Data preparation: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/DATA.md&#34;&gt;DATA.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Training: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/TRAIN.md&#34;&gt;TRAIN.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Testing: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/TEST.md&#34;&gt;TEST.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;Model zoo: Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/MasterBin-IIAU/UNINEXT/master/assets/MODEL_ZOO.md&#34;&gt;MODEL_ZOO.md&lt;/a&gt; for more details.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Citing UNINEXT&lt;/h2&gt; &#xA;&lt;p&gt;If you find UNINEXT useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{UNINEXT,&#xA;  title={Universal Instance Perception as Object Discovery and Retrieval},&#xA;  author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},&#xA;  booktitle={CVPR},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/MasterBin-IIAU/Unicorn&#34;&gt;Unicorn&lt;/a&gt; for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS).&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/wjf5203/VNext&#34;&gt;VNext&lt;/a&gt; for providing experience of Video Instance Segmentation (VIS).&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/wjn922/ReferFormer&#34;&gt;ReferFormer&lt;/a&gt; for providing experience of REC, RES, and R-VOS.&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/microsoft/GLIP&#34;&gt;GLIP&lt;/a&gt; for the idea of unifying object detection and phrase grounding.&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/facebookresearch/Detic&#34;&gt;Detic&lt;/a&gt; for the implementation of multi-dataset training.&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/IDEA-Research/detrex&#34;&gt;detrex&lt;/a&gt; for the implementation of denoising mechnism.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>