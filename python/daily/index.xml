<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-17T01:34:37Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AmberSahdev/Open-Interface</title>
    <updated>2025-01-17T01:34:37Z</updated>
    <id>tag:github.com,2025-01-17:/AmberSahdev/Open-Interface</id>
    <link href="https://github.com/AmberSahdev/Open-Interface" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Control Any Computer Using LLMs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Interface&lt;/h1&gt; &#xA;&lt;picture&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/icon.png&#34; align=&#34;right&#34; alt=&#34;Open Interface Logo&#34; width=&#34;120&#34; height=&#34;120&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h3&gt;Control Your Computer Using LLMs&lt;/h3&gt; &#xA;&lt;p&gt;Open Interface&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Self-drives your computer by sending your requests to an LLM backend (GPT-4o, etc) to figure out the required steps.&lt;/li&gt; &#xA; &lt;li&gt;Automatically executes these steps by simulating keyboard and mouse input.&lt;/li&gt; &#xA; &lt;li&gt;Course-corrects by sending the LLM backend updated screenshots of the progress as needed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h4&gt;Full Autopilot for All Computers Using LLMs&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white&#34; alt=&#34;macOS&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&#34; alt=&#34;Linux&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#install&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white&#34; alt=&#34;Windows&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;(https://github.com/AmberSahdev/Open-Interface/releases/latest)&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/AmberSahdev/Open-Interface/total.svg?sanitize=true&#34; alt=&#34;Github All Releases&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/languages/code-size/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub code size in bytes&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub&#34;&gt; &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/AmberSahdev/Open-Interface&#34; alt=&#34;GitHub Latest Release)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Demo&lt;/ins&gt; üíª&lt;/h3&gt; &#xA;&lt;p&gt;&#34;Solve Today&#39;s Wordle&#34;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/wordle_demo_2x.gif&#34; alt=&#34;Solve Today&#39;s Wordle&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/raw/main/MEDIA.md#demos&#34;&gt;More Demos&lt;/a&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &#34;Make me a meal plan in Google Docs&#34; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/meal_plan_demo_2x.gif&#34; style=&#34;margin: 5px; border-radius: 10px;&#34;&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &#34;Write a Web App&#34; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/code_web_app_demo_2x.gif&#34; style=&#34;margin: 5px; border-radius: 10px;&#34;&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Install&lt;/ins&gt; üíΩ&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Apple_Computer_Logo_rainbow.svg/640px-Apple_Computer_Logo_rainbow.svg.png&#34; alt=&#34;MacOS Logo&#34; width=&#34;13&#34; height=&#34;15&#34;&gt; &lt;b&gt;MacOS&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Download the MacOS binary from the latest &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;release&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Unzip the file and move Open Interface to the Applications Folder.&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_unzip_move_to_applications.png&#34; width=&#34;350&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;b&gt;Apple Silicon M-Series Macs&lt;/b&gt;&lt;/summary&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; Open Interface will ask you for Accessibility access to operate your keyboard and mouse for you, and Screen Recording access to take screenshots to assess its progress.&lt;br&gt; &lt;/li&gt; &#xA;   &lt;li&gt; In case it doesn&#39;t, manually add these permission via &lt;b&gt;System Settings&lt;/b&gt; -&amp;gt; &lt;b&gt;Privacy and Security&lt;/b&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/mac_m3_accessibility.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/mac_m3_screenrecording.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;&lt;b&gt;Intel Macs&lt;/b&gt;&lt;/summary&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; Launch the app from the Applications folder.&lt;br&gt; You might face the standard Mac &lt;i&gt;&#34;Open Interface cannot be opened&#34; error&lt;/i&gt;.&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_unverified_developer.png&#34; width=&#34;200&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt;&lt;br&gt; In that case, press &lt;b&gt;&lt;i&gt;&lt;ins&gt;&#34;Cancel&#34;&lt;/ins&gt;&lt;/i&gt;&lt;/b&gt;.&lt;br&gt; Then go to &lt;b&gt;System Preferences -&amp;gt; Security and Privacy -&amp;gt; Open Anyway.&lt;/b&gt;&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_system_preferences.png&#34; width=&#34;100&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_security.png&#34; width=&#34;100&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_open_anyway.png&#34; width=&#34;400&#34; style=&#34;border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA;   &lt;br&gt; &#xA;   &lt;li&gt; Open Interface will also need Accessibility access to operate your keyboard and mouse for you, and Screen Recording access to take screenshots to assess its progress.&lt;br&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_accessibility.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/macos_screen_recording.png&#34; width=&#34;400&#34; style=&#34;margin: 5px; border-radius: 10px;&#xA;    border: 3px solid black;&#34;&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA; &lt;/details&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Lastly, checkout the &lt;a href=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/#setup&#34;&gt;Setup&lt;/a&gt; section to connect Open Interface to LLMs (OpenAI GPT-4V)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/TuxFlat.svg/640px-TuxFlat.svg.png&#34; alt=&#34;Linux Logo&#34; width=&#34;15&#34; height=&#34;15&#34;&gt; &lt;b&gt;Linux&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Linux binary has been tested on Ubuntu 20.04 so far.&lt;/li&gt; &#xA;  &lt;li&gt;Download the Linux zip file from the latest &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;release&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt; Extract the executable and checkout the &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#setup&#34;&gt;Setup&lt;/a&gt; section to connect Open Interface to LLMs, such as OpenAI GPT-4V.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/5/5f/Windows_logo_-_2012.svg?sanitize=true&#34; alt=&#34;Linux Logo&#34; width=&#34;15&#34; height=&#34;15&#34;&gt; &lt;b&gt;Windows&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Windows binary has been tested on Windows 10.&lt;/li&gt; &#xA;  &lt;li&gt;Download the Windows zip file from the latest &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface/releases/latest&#34;&gt;release&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Unzip the folder, move the exe to the desired location, double click to open, and voila.&lt;/li&gt; &#xA;  &lt;li&gt;Checkout the &lt;a href=&#34;https://github.com/AmberSahdev/Open-Interface?tab=readme-ov-file#setup&#34;&gt;Setup&lt;/a&gt; section to connect Open Interface to LLMs (OpenAI GPT-4V)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;&lt;ins id=&#34;setup&#34;&gt;Setup&lt;/ins&gt; üõ†Ô∏è&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Set up the OpenAI API key&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Get your OpenAI API key&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Open Interface needs access to GPT-4o to perform user requests. GPT-4o keys can be downloaded from your OpenAI account at &lt;a href=&#34;https://platform.openai.com/settings/organization/api-keys&#34;&gt;platform.openai.com/settings/organization/api-keys&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://help.openai.com/en/articles/8264644-what-is-prepaid-billing&#34;&gt;Follow the steps here&lt;/a&gt; to add balance to your OpenAI account. To unlock GPT-4o a minimum payment of $5 is needed.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4&#34;&gt;More info&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Save the API key in Open Interface settings&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;In Open Interface, go to the Settings menu on the top right and enter the key you received from OpenAI into the text field like so: &lt;br&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;br&gt; &#xA;   &lt;picture&gt; &#xA;    &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/set_openai_api_key.png&#34; align=&#34;middle&#34; alt=&#34;Set API key in settings&#34; width=&#34;400&#34;&gt; &#xA;   &lt;/picture&gt;&lt;br&gt; &lt;br&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;After setting the API key for the first time you&#39;ll need to &lt;b&gt;restart the app&lt;/b&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Optional: Setup a Custom LLM&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Open Interface supports using other OpenAI API style LLMs (such as Llava) as a backend and can be configured easily in the Advanced Settings window.&lt;/li&gt; &#xA;  &lt;li&gt;Enter the custom base url and model name in the Advanced Settings window and the API key in the Settings window as needed. &lt;br&gt; &#xA;   &lt;picture&gt; &#xA;    &lt;img src=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/assets/advanced_settings.png&#34; align=&#34;middle&#34; alt=&#34;Set API key in settings&#34; width=&#34;400&#34;&gt; &#xA;   &lt;/picture&gt;&lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;li&gt;If your LLM does not support an OpenAI style API, you can use a library like &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;this&lt;/a&gt; to convert it to one.&lt;/li&gt; &#xA;  &lt;li&gt;You will need to restart the app after these changes.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Stuff It‚Äôs Error-Prone At, For Now&lt;/ins&gt; üò¨&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accurate spatial-reasoning and hence clicking buttons.&lt;/li&gt; &#xA; &lt;li&gt;Keeping track of itself in tabular contexts, like Excel and Google Sheets, for similar reasons as stated above.&lt;/li&gt; &#xA; &lt;li&gt;Navigating complex GUI-rich applications like Counter-Strike, Spotify, Garage Band, etc due to heavy reliance on cursor actions.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;ins&gt;The Future&lt;/ins&gt; üîÆ&lt;/h3&gt; &#xA;&lt;p&gt;(&lt;em&gt;with better models trained on video walkthroughs like Youtube tutorials&lt;/em&gt;)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Create a couple of bass samples for me in Garage Band for my latest project.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Read this design document for a new feature, edit the code on Github, and submit it for review.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Find my friends&#39; music taste from Spotify and create a party playlist for tonight&#39;s event.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Take the pictures from my Tahoe trip and make a White Lotus type montage in iMovie.&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Notes&lt;/ins&gt; üìù&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cost Estimation: $0.0005 - $0.002 per LLM request depending on the model used.&lt;br&gt; (User requests can require between two to a few dozen LLM backend calls depending on the request&#39;s complexity.)&lt;/li&gt; &#xA; &lt;li&gt;You can interrupt the app anytime by pressing the Stop button, or by dragging your cursor to any of the screen corners.&lt;/li&gt; &#xA; &lt;li&gt;Open Interface can only see your primary display when using multiple monitors. Therefore, if the cursor/focus is on a secondary screen, it might keep retrying the same actions as it is unable to see its progress.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;System Diagram&lt;/ins&gt; üñºÔ∏è&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;+----------------------------------------------------+&#xA;| App                                                |&#xA;|                                                    |&#xA;|    +-------+                                       |&#xA;|    |  GUI  |                                       |&#xA;|    +-------+                                       |&#xA;|        ^                                           |&#xA;|        |                                           |&#xA;|        v                                           |&#xA;|  +-----------+  (Screenshot + Goal)  +-----------+ |&#xA;|  |           | --------------------&amp;gt; |           | |&#xA;|  |    Core   |                       |    LLM    | |&#xA;|  |           | &amp;lt;-------------------- |  (GPT-4o) | |&#xA;|  +-----------+    (Instructions)     +-----------+ |&#xA;|        |                                           |&#xA;|        v                                           |&#xA;|  +-------------+                                   |&#xA;|  | Interpreter |                                   |&#xA;|  +-------------+                                   |&#xA;|        |                                           |&#xA;|        v                                           |&#xA;|  +-------------+                                   |&#xA;|  |   Executer  |                                   |&#xA;|  +-------------+                                   |&#xA;+----------------------------------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Star History&lt;/ins&gt; ‚≠êÔ∏è&lt;/h3&gt; &#xA;&lt;picture&gt; &#xA; &lt;img src=&#34;https://api.star-history.com/svg?repos=AmberSahdev/Open-Interface&amp;amp;type=Date&#34; alt=&#34;Star History&#34; width=&#34;720&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h3&gt;&lt;ins&gt;Links&lt;/ins&gt; üîó&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out more of my projects at &lt;a href=&#34;https://AmberSah.dev&#34;&gt;AmberSah.dev&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Other demos and press kit can be found at &lt;a href=&#34;https://raw.githubusercontent.com/AmberSahdev/Open-Interface/main/MEDIA.md&#34;&gt;MEDIA.md&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.producthunt.com/posts/open-interface-2?embed=true&amp;amp;utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-open-interface-2&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=790479&amp;amp;theme=light&amp;amp;t=1737074650306&#34; alt=&#34;Open Interface - Control Your Computer Using LLMs | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/AmberSahdev/Open-Interface&#34;&gt; &#xA; &lt;!-- &lt;br&gt;&#xA;&#x9;&lt;img alt=&#34;GitHub followers&#34; src=&#34;https://img.shields.io/github/followers/AmberSahdev?style=flat-square&#34;&gt;--&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>instructor-ai/instructor</title>
    <updated>2025-01-17T01:34:37Z</updated>
    <id>tag:github.com,2025-01-17:/instructor-ai/instructor</id>
    <link href="https://github.com/instructor-ai/instructor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;structured outputs for llms&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Instructor, The Most Popular Library for Simple Structured Outputs&lt;/h1&gt; &#xA;&lt;p&gt;Instructor is the most popular Python library for working with structured outputs from large language models (LLMs), boasting over 1 million monthly downloads. Built on top of Pydantic, it provides a simple, transparent, and user-friendly API to manage validation, retries, and streaming responses. Get ready to supercharge your LLM workflows with the community&#39;s top choice!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/jxnlco&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/jxnlco?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/bD9YE9JArw&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1192334452110659664?label=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/instructor&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/instructor.svg?sanitize=true&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Want your logo on our website?&lt;/h2&gt; &#xA;&lt;p&gt;If your company uses Instructor a lot, we&#39;d love to have your logo on our website! Please fill out &lt;a href=&#34;https://q7gjsgfstrp.typeform.com/to/wluQlVVQ&#34;&gt;this form&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Response Models&lt;/strong&gt;: Specify Pydantic models to define the structure of your LLM outputs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Retry Management&lt;/strong&gt;: Easily configure the number of retry attempts for your requests&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Validation&lt;/strong&gt;: Ensure LLM responses conform to your expectations with Pydantic validation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streaming Support&lt;/strong&gt;: Work with Lists and Partial responses effortlessly&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Backends&lt;/strong&gt;: Seamlessly integrate with various LLM providers beyond OpenAI&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support in many Languages&lt;/strong&gt;: We support many languages including &lt;a href=&#34;https://python.useinstructor.com&#34;&gt;Python&lt;/a&gt;, &lt;a href=&#34;https://js.useinstructor.com&#34;&gt;TypeScript&lt;/a&gt;, &lt;a href=&#34;https://ruby.useinstructor.com&#34;&gt;Ruby&lt;/a&gt;, &lt;a href=&#34;https://go.useinstructor.com&#34;&gt;Go&lt;/a&gt;, and &lt;a href=&#34;https://hex.pm/packages/instructor&#34;&gt;Elixir&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started in Minutes&lt;/h2&gt; &#xA;&lt;p&gt;Install Instructor with a single command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U instructor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, let&#39;s see Instructor in action with a simple example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import instructor&#xA;from pydantic import BaseModel&#xA;from openai import OpenAI&#xA;&#xA;&#xA;# Define your desired output structure&#xA;class UserInfo(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;# Patch the OpenAI client&#xA;client = instructor.from_openai(OpenAI())&#xA;&#xA;# Extract structured data from natural language&#xA;user_info = client.chat.completions.create(&#xA;    model=&#34;gpt-4o-mini&#34;,&#xA;    response_model=UserInfo,&#xA;    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;John Doe is 30 years old.&#34;}],&#xA;)&#xA;&#xA;print(user_info.name)&#xA;#&amp;gt; John Doe&#xA;print(user_info.age)&#xA;#&amp;gt; 30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Hooks&lt;/h3&gt; &#xA;&lt;p&gt;Instructor provides a powerful hooks system that allows you to intercept and log various stages of the LLM interaction process. Here&#39;s a simple example demonstrating how to use hooks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import instructor&#xA;from openai import OpenAI&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class UserInfo(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;# Initialize the OpenAI client with Instructor&#xA;client = instructor.from_openai(OpenAI())&#xA;&#xA;&#xA;# Define hook functions&#xA;def log_kwargs(**kwargs):&#xA;    print(f&#34;Function called with kwargs: {kwargs}&#34;)&#xA;&#xA;&#xA;def log_exception(exception: Exception):&#xA;    print(f&#34;An exception occurred: {str(exception)}&#34;)&#xA;&#xA;&#xA;client.on(&#34;completion:kwargs&#34;, log_kwargs)&#xA;client.on(&#34;completion:error&#34;, log_exception)&#xA;&#xA;user_info = client.chat.completions.create(&#xA;    model=&#34;gpt-4o-mini&#34;,&#xA;    response_model=UserInfo,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Extract the user name: &#39;John is 20 years old&#39;&#34;}&#xA;    ],&#xA;)&#xA;&#xA;&#34;&#34;&#34;&#xA;{&#xA;        &#39;args&#39;: (),&#xA;        &#39;kwargs&#39;: {&#xA;            &#39;messages&#39;: [&#xA;                {&#xA;                    &#39;role&#39;: &#39;user&#39;,&#xA;                    &#39;content&#39;: &#34;Extract the user name: &#39;John is 20 years old&#39;&#34;,&#xA;                }&#xA;            ],&#xA;            &#39;model&#39;: &#39;gpt-4o-mini&#39;,&#xA;            &#39;tools&#39;: [&#xA;                {&#xA;                    &#39;type&#39;: &#39;function&#39;,&#xA;                    &#39;function&#39;: {&#xA;                        &#39;name&#39;: &#39;UserInfo&#39;,&#xA;                        &#39;description&#39;: &#39;Correctly extracted `UserInfo` with all the required parameters with correct types&#39;,&#xA;                        &#39;parameters&#39;: {&#xA;                            &#39;properties&#39;: {&#xA;                                &#39;name&#39;: {&#39;title&#39;: &#39;Name&#39;, &#39;type&#39;: &#39;string&#39;},&#xA;                                &#39;age&#39;: {&#39;title&#39;: &#39;Age&#39;, &#39;type&#39;: &#39;integer&#39;},&#xA;                            },&#xA;                            &#39;required&#39;: [&#39;age&#39;, &#39;name&#39;],&#xA;                            &#39;type&#39;: &#39;object&#39;,&#xA;                        },&#xA;                    },&#xA;                }&#xA;            ],&#xA;            &#39;tool_choice&#39;: {&#39;type&#39;: &#39;function&#39;, &#39;function&#39;: {&#39;name&#39;: &#39;UserInfo&#39;}},&#xA;        },&#xA;    }&#xA;&#34;&#34;&#34;&#xA;&#xA;print(f&#34;Name: {user_info.name}, Age: {user_info.age}&#34;)&#xA;#&amp;gt; Name: John, Age: 20&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example demonstrates:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A pre-execution hook that logs all kwargs passed to the function.&lt;/li&gt; &#xA; &lt;li&gt;An exception hook that logs any exceptions that occur during execution.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The hooks provide valuable insights into the function&#39;s inputs and any errors, enhancing debugging and monitoring capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Using Anthropic Models&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import instructor&#xA;from anthropic import Anthropic&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;client = instructor.from_anthropic(Anthropic())&#xA;&#xA;# note that client.chat.completions.create will also work&#xA;resp = client.messages.create(&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;    max_tokens=1024,&#xA;    system=&#34;You are a world class AI that excels at extracting user data from a sentence&#34;,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Extract Jason is 25 years old.&#34;,&#xA;        }&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&#xA;assert isinstance(resp, User)&#xA;assert resp.name == &#34;Jason&#34;&#xA;assert resp.age == 25&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Cohere Models&lt;/h3&gt; &#xA;&lt;p&gt;Make sure to install &lt;code&gt;cohere&lt;/code&gt; and set your system environment variable with &lt;code&gt;export CO_API_KEY=&amp;lt;YOUR_COHERE_API_KEY&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install cohere&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import instructor&#xA;import cohere&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;client = instructor.from_cohere(cohere.Client())&#xA;&#xA;# note that client.chat.completions.create will also work&#xA;resp = client.chat.completions.create(&#xA;    model=&#34;command-r-plus&#34;,&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Extract Jason is 25 years old.&#34;,&#xA;        }&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&#xA;assert isinstance(resp, User)&#xA;assert resp.name == &#34;Jason&#34;&#xA;assert resp.age == 25&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Gemini Models&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you &lt;a href=&#34;https://ai.google.dev/api/python/google/generativeai#setup&#34;&gt;install&lt;/a&gt; the Google AI Python SDK. You should set a &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; environment variable with your API key. Gemini tool calling also requires &lt;code&gt;jsonref&lt;/code&gt; to be installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install google-generativeai jsonref&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import instructor&#xA;import google.generativeai as genai&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;# genai.configure(api_key=os.environ[&#34;API_KEY&#34;]) # alternative API key configuration&#xA;client = instructor.from_gemini(&#xA;    client=genai.GenerativeModel(&#xA;        model_name=&#34;models/gemini-1.5-flash-latest&#34;,  # model defaults to &#34;gemini-pro&#34;&#xA;    ),&#xA;    mode=instructor.Mode.GEMINI_JSON,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can &lt;a href=&#34;https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-gemini-using-openai-library#python&#34;&gt;call Gemini from the OpenAI client&lt;/a&gt;. You&#39;ll have to setup &lt;a href=&#34;https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev&#34;&gt;&lt;code&gt;gcloud&lt;/code&gt;&lt;/a&gt;, get setup on Vertex AI, and install the Google Auth library.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install google-auth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import google.auth&#xA;import google.auth.transport.requests&#xA;import instructor&#xA;from openai import OpenAI&#xA;from pydantic import BaseModel&#xA;&#xA;creds, project = google.auth.default()&#xA;auth_req = google.auth.transport.requests.Request()&#xA;creds.refresh(auth_req)&#xA;&#xA;# Pass the Vertex endpoint and authentication to the OpenAI SDK&#xA;PROJECT = &#39;PROJECT_ID&#39;&#xA;LOCATION = (&#xA;    &#39;LOCATION&#39;  # https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations&#xA;)&#xA;base_url = f&#39;https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT}/locations/{LOCATION}/endpoints/openapi&#39;&#xA;&#xA;client = instructor.from_openai(&#xA;    OpenAI(base_url=base_url, api_key=creds.token), mode=instructor.Mode.JSON&#xA;)&#xA;&#xA;&#xA;# JSON mode is req&#39;d&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;resp = client.chat.completions.create(&#xA;    model=&#34;google/gemini-1.5-flash-001&#34;,&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Extract Jason is 25 years old.&#34;,&#xA;        }&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&#xA;assert isinstance(resp, User)&#xA;assert resp.name == &#34;Jason&#34;&#xA;assert resp.age == 25&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Litellm&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import instructor&#xA;from litellm import completion&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;client = instructor.from_litellm(completion)&#xA;&#xA;resp = client.chat.completions.create(&#xA;    model=&#34;claude-3-opus-20240229&#34;,&#xA;    max_tokens=1024,&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;Extract Jason is 25 years old.&#34;,&#xA;        }&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&#xA;assert isinstance(resp, User)&#xA;assert resp.name == &#34;Jason&#34;&#xA;assert resp.age == 25&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Types are inferred correctly&lt;/h2&gt; &#xA;&lt;p&gt;This was the dream of Instructor but due to the patching of OpenAI, it wasn&#39;t possible for me to get typing to work well. Now, with the new client, we can get typing to work well! We&#39;ve also added a few &lt;code&gt;create_*&lt;/code&gt; methods to make it easier to create iterables and partials, and to access the original completion.&lt;/p&gt; &#xA;&lt;h3&gt;Calling &lt;code&gt;create&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import instructor&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;client = instructor.from_openai(openai.OpenAI())&#xA;&#xA;user = client.chat.completions.create(&#xA;    model=&#34;gpt-4-turbo-preview&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Create a user&#34;},&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now if you use an IDE, you can see the type is correctly inferred.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/instructor-ai/instructor/main/docs/blog/posts/img/type.png&#34; alt=&#34;type&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Handling async: &lt;code&gt;await create&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This will also work correctly with asynchronous clients.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import instructor&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;client = instructor.from_openai(openai.AsyncOpenAI())&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;async def extract():&#xA;    return await client.chat.completions.create(&#xA;        model=&#34;gpt-4-turbo-preview&#34;,&#xA;        messages=[&#xA;            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Create a user&#34;},&#xA;        ],&#xA;        response_model=User,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice that simply because we return the &lt;code&gt;create&lt;/code&gt; method, the &lt;code&gt;extract()&lt;/code&gt; function will return the correct user type.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/instructor-ai/instructor/main/docs/blog/posts/img/async_type.png&#34; alt=&#34;async&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Returning the original completion: &lt;code&gt;create_with_completion&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can also return the original completion object&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import instructor&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;client = instructor.from_openai(openai.OpenAI())&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;user, completion = client.chat.completions.create_with_completion(&#xA;    model=&#34;gpt-4-turbo-preview&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Create a user&#34;},&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/instructor-ai/instructor/main/docs/blog/posts/img/with_completion.png&#34; alt=&#34;with_completion&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Streaming Partial Objects: &lt;code&gt;create_partial&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;In order to handle streams, we still support &lt;code&gt;Iterable[T]&lt;/code&gt; and &lt;code&gt;Partial[T]&lt;/code&gt; but to simplify the type inference, we&#39;ve added &lt;code&gt;create_iterable&lt;/code&gt; and &lt;code&gt;create_partial&lt;/code&gt; methods as well!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import instructor&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;client = instructor.from_openai(openai.OpenAI())&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;user_stream = client.chat.completions.create_partial(&#xA;    model=&#34;gpt-4-turbo-preview&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Create a user&#34;},&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&#xA;for user in user_stream:&#xA;    print(user)&#xA;    #&amp;gt; name=None age=None&#xA;    #&amp;gt; name=None age=None&#xA;    #&amp;gt; name=None age=None&#xA;    #&amp;gt; name=None age=None&#xA;    #&amp;gt; name=None age=None&#xA;    #&amp;gt; name=None age=None&#xA;    #&amp;gt; name=&#39;John Doe&#39; age=None&#xA;    #&amp;gt; name=&#39;John Doe&#39; age=None&#xA;    #&amp;gt; name=&#39;John Doe&#39; age=None&#xA;    #&amp;gt; name=&#39;John Doe&#39; age=30&#xA;    #&amp;gt; name=&#39;John Doe&#39; age=30&#xA;    # name=None age=None&#xA;    # name=&#39;&#39; age=None&#xA;    # name=&#39;John&#39; age=None&#xA;    # name=&#39;John Doe&#39; age=None&#xA;    # name=&#39;John Doe&#39; age=30&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice now that the type inferred is &lt;code&gt;Generator[User, None]&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/instructor-ai/instructor/main/docs/blog/posts/img/generator.png&#34; alt=&#34;generator&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Streaming Iterables: &lt;code&gt;create_iterable&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We get an iterable of objects when we want to extract multiple objects.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import instructor&#xA;from pydantic import BaseModel&#xA;&#xA;&#xA;client = instructor.from_openai(openai.OpenAI())&#xA;&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    age: int&#xA;&#xA;&#xA;users = client.chat.completions.create_iterable(&#xA;    model=&#34;gpt-4-turbo-preview&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Create 2 users&#34;},&#xA;    ],&#xA;    response_model=User,&#xA;)&#xA;&#xA;for user in users:&#xA;    print(user)&#xA;    #&amp;gt; name=&#39;John Doe&#39; age=30&#xA;    #&amp;gt; name=&#39;Jane Doe&#39; age=28&#xA;    # User(name=&#39;John Doe&#39;, age=30)&#xA;    # User(name=&#39;Jane Smith&#39;, age=25)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/instructor-ai/instructor/main/docs/blog/posts/img/iterable.png&#34; alt=&#34;iterable&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/jxnl/instructor/tree/main/tests/llm/test_openai/evals#how-to-contribute-writing-and-running-evaluation-tests&#34;&gt;Evals&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We invite you to contribute to evals in &lt;code&gt;pytest&lt;/code&gt; as a way to monitor the quality of the OpenAI models and the &lt;code&gt;instructor&lt;/code&gt; library. To get started check out the evals for &lt;a href=&#34;https://github.com/jxnl/instructor/raw/main/tests/llm/test_anthropic/evals/test_simple.py&#34;&gt;Anthropic&lt;/a&gt; and &lt;a href=&#34;https://github.com/jxnl/instructor/tree/main/tests/llm/test_openai/evals#how-to-contribute-writing-and-running-evaluation-tests&#34;&gt;OpenAI&lt;/a&gt; and contribute your own evals in the form of pytest tests. These evals will be run once a week and the results will be posted.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you want to help, checkout some of the issues marked as &lt;code&gt;good-first-issue&lt;/code&gt; or &lt;code&gt;help-wanted&lt;/code&gt; found &lt;a href=&#34;https://github.com/jxnl/instructor/labels/good%20first%20issue&#34;&gt;here&lt;/a&gt;. They could be anything from code improvements, a guest blog post, or a new cookbook.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s a quick list of commands that you can run to get started. We&#39;re using &lt;code&gt;uv&lt;/code&gt; to manage our dependencies so make sure you have that installed.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;uv sync --all-extras --group &amp;lt;dependency groups you&#39;d like to install&amp;gt;&lt;/code&gt;: This should install all the dependencies for the project using &lt;code&gt;uv&lt;/code&gt;, make sure to install the specific dependencies that you&#39;d like to install&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;uv run pytest&lt;/code&gt; : This runs the tests in &lt;code&gt;pytest&lt;/code&gt;. If you&#39;re pushing up a new PR, make sure that you&#39;ve written some tests and that they&#39;re passing locally for you&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We use &lt;code&gt;ruff&lt;/code&gt; and &lt;code&gt;pyright&lt;/code&gt; for linting and type checking so make sure those are passing when you push up a PR. You can check pyright by running &lt;code&gt;uv run pyright&lt;/code&gt; and ruff with &lt;code&gt;uv run ruff check&lt;/code&gt; locally.&lt;/p&gt; &#xA;&lt;h2&gt;CLI&lt;/h2&gt; &#xA;&lt;p&gt;We also provide some added CLI functionality for easy convenience:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;instructor jobs&lt;/code&gt; : This helps with the creation of fine-tuning jobs with OpenAI. Simple use &lt;code&gt;instructor jobs create-from-file --help&lt;/code&gt; to get started creating your first fine-tuned GPT-3.5 model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;instructor files&lt;/code&gt; : Manage your uploaded files with ease. You&#39;ll be able to create, delete and upload files all from the command line&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;instructor usage&lt;/code&gt; : Instead of heading to the OpenAI site each time, you can monitor your usage from the CLI and filter by date and time period. Note that usage often takes ~5-10 minutes to update from OpenAI&#39;s side&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the terms of the MIT License.&lt;/p&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;a href=&#34;https://github.com/instructor-ai/instructor/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=instructor-ai/instructor&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>ansible/ansible-lint</title>
    <updated>2025-01-17T01:34:37Z</updated>
    <id>tag:github.com,2025-01-17:/ansible/ansible-lint</id>
    <link href="https://github.com/ansible/ansible-lint" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ansible-lint checks playbooks for practices and behavior that could potentially be improved and can fix some of the most common ones for you&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/ansible-lint&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/ansible-lint.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ansible.readthedocs.io/projects/lint/rules/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Ansible--lint-rules-blue.svg?sanitize=true&#34; alt=&#34;Ansible-lint rules explanation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://forum.ansible.com/tag/ansible-lint&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discussions-gray.svg?sanitize=true&#34; alt=&#34;Discussions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pre-commit/pre-commit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Ansible-lint&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;ansible-lint&lt;/code&gt; checks playbooks for practices and behavior that could potentially be improved. As a community-backed project ansible-lint supports only the last two major versions of Ansible.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ansible.readthedocs.io/projects/lint/&#34;&gt;Visit the Ansible Lint docs site&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Using ansible-lint as a GitHub Action&lt;/h1&gt; &#xA;&lt;p&gt;This action allows you to run &lt;code&gt;ansible-lint&lt;/code&gt; on your codebase without having to install it yourself.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# .github/workflows/ansible-lint.yml&#xA;name: ansible-lint&#xA;on:&#xA;  pull_request:&#xA;    branches: [&#34;main&#34;, &#34;stable&#34;, &#34;release/v*&#34;]&#xA;jobs:&#xA;  build:&#xA;    name: Ansible Lint # Naming the build is important to use it as a status check&#xA;    runs-on: ubuntu-24.04&#xA;    steps:&#xA;      - uses: actions/checkout@v4&#xA;      - name: Run ansible-lint&#xA;        uses: ansible/ansible-lint@main&#xA;        # optional (see below):&#xA;        with:&#xA;          args: &#34;&#34;&#xA;          setup_python: &#34;true&#34;&#xA;          working_directory: &#34;&#34;&#xA;          requirements_file: &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All the arguments are optional and most users should not need them:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;args&lt;/code&gt;: Arguments to be passed to ansible-lint command.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;setup_python&lt;/code&gt;: If python should be installed. Default is &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;working_directory&lt;/code&gt;: The directory where to run ansible-lint from. Default is &lt;code&gt;github.workspace&lt;/code&gt;. That might be needed if you want to lint only a subset of your repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;requirements_file&lt;/code&gt;: Path to the requirements.yml file to install role and collection dependencies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://ansible.readthedocs.io/projects/lint/installing/#installing-from-source-code&#34;&gt;ansible-lint-action&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Communication&lt;/h1&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://ansible.readthedocs.io/projects/lint/contributing/#talk-to-us&#34;&gt;Talk to us&lt;/a&gt; section of the Contributing guide to find out how to get in touch with us.&lt;/p&gt; &#xA;&lt;p&gt;You can also find more information in the &lt;a href=&#34;https://docs.ansible.com/ansible/devel/community/communication.html&#34;&gt;Ansible communication guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://ansible.readthedocs.io/projects/lint/contributing&#34;&gt;Contribution guidelines&lt;/a&gt; if you wish to contribute.&lt;/p&gt; &#xA;&lt;h1&gt;Code of Conduct&lt;/h1&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://docs.ansible.com/ansible/latest/community/code_of_conduct.html&#34;&gt;Ansible Community Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Licensing&lt;/h1&gt; &#xA;&lt;p&gt;The ansible-lint project is distributed as &lt;a href=&#34;https://github.com/ansible/ansible-lint/raw/main/COPYING&#34;&gt;GPLv3&lt;/a&gt; due to use of &lt;a href=&#34;https://github.com/ansible/ansible-lint/raw/main/COPYING&#34;&gt;GPLv3&lt;/a&gt; runtime dependencies, like &lt;code&gt;ansible&lt;/code&gt; and &lt;code&gt;yamllint&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For historical reasons, its own code-base remains licensed under a more liberal &lt;a href=&#34;https://github.com/ansible/ansible-lint/raw/main/docs/licenses/LICENSE.mit.txt&#34;&gt;MIT&lt;/a&gt; license and any contributions made are accepted as being made under original &lt;a href=&#34;https://github.com/ansible/ansible-lint/raw/main/docs/licenses/LICENSE.mit.txt&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;p&gt;ansible-lint was created by &lt;a href=&#34;https://github.com/willthames&#34;&gt;Will Thames&lt;/a&gt; and is now maintained as part of the &lt;a href=&#34;https://ansible.com&#34;&gt;Ansible&lt;/a&gt; by &lt;a href=&#34;https://redhat.com&#34;&gt;Red Hat&lt;/a&gt; project.&lt;/p&gt;</summary>
  </entry>
</feed>