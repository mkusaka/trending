<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-13T01:34:21Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>confident-ai/deepeval</title>
    <updated>2024-11-13T01:34:21Z</updated>
    <id>tag:github.com,2024-11-13:/confident-ai/deepeval</id>
    <link href="https://github.com/confident-ai/deepeval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The LLM Evaluation Framework&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/confident-ai/deepeval/raw/main/docs/static/img/deepeval.png&#34; alt=&#34;DeepEval Logo&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;The LLM Evaluation Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.com/invite/a3K9c8GRGt&#34;&gt; &lt;img alt=&#34;discord-invite&#34; src=&#34;https://dcbadge.vercel.app/api/server/a3K9c8GRGt?style=flat&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://docs.confident-ai.com/docs/getting-started?utm_source=GitHub&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/#-metrics-and-features&#34;&gt;Metrics and Features&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/#-quickstart&#34;&gt;Getting Started&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/#-integrations&#34;&gt;Integrations&lt;/a&gt; | &lt;a href=&#34;https://confident-ai.com?utm_source=GitHub&#34;&gt;Confident AI&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/confident-ai/deepeval/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&#34;&gt; &lt;img alt=&#34;Try Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/confident-ai/deepeval/raw/master/LICENSE.md&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepEval&lt;/strong&gt; is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt; for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;Whether your application is implemented via RAG or fine-tuning, LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal hyperparameters to improve your RAG pipeline, prevent prompt drifting, or even transition from OpenAI to hosting your own Llama2 with confidence.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Want to talk LLM evaluation? &lt;a href=&#34;https://discord.com/invite/a3K9c8GRGt&#34;&gt;Come join our discord.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üî• Metrics and Features&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ÄºÔ∏è You can now run DeepEval&#39;s metrics on the cloud for free directly on &lt;a href=&#34;https://confident-ai.com?utm_source=GitHub&#34;&gt;Confident AI&lt;/a&gt;&#39;s infrastructure ü•≥&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by &lt;strong&gt;ANY&lt;/strong&gt; LLM of your choice, statistical methods, or NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;G-Eval&lt;/li&gt; &#xA;   &lt;li&gt;Summarization&lt;/li&gt; &#xA;   &lt;li&gt;Answer Relevancy&lt;/li&gt; &#xA;   &lt;li&gt;Faithfulness&lt;/li&gt; &#xA;   &lt;li&gt;Contextual Recall&lt;/li&gt; &#xA;   &lt;li&gt;Contextual Precision&lt;/li&gt; &#xA;   &lt;li&gt;RAGAS&lt;/li&gt; &#xA;   &lt;li&gt;Hallucination&lt;/li&gt; &#xA;   &lt;li&gt;etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.confident-ai.com/docs/red-teaming-introduction&#34;&gt;Red team your LLM application&lt;/a&gt; for 40+ safety vulnerabilities in a few lines of code, including: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Toxicity&lt;/li&gt; &#xA;   &lt;li&gt;Bias&lt;/li&gt; &#xA;   &lt;li&gt;SQL Injection&lt;/li&gt; &#xA;   &lt;li&gt;etc., using advanced 10+ attack enhancement strategies such as prompt injections.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Evaluate your entire dataset in bulk in under 20 lines of Python code &lt;strong&gt;in parallel&lt;/strong&gt;. Do this via the CLI in a Pytest-like manner, or through our &lt;code&gt;evaluate()&lt;/code&gt; function.&lt;/li&gt; &#xA; &lt;li&gt;Create your own custom metrics that are automatically integrated with DeepEval&#39;s ecosystem by inheriting DeepEval&#39;s base metric class.&lt;/li&gt; &#xA; &lt;li&gt;Integrates seamlessly with &lt;strong&gt;ANY&lt;/strong&gt; CI/CD environment.&lt;/li&gt; &#xA; &lt;li&gt;Easily benchmark &lt;strong&gt;ANY&lt;/strong&gt; LLM on popular LLM benchmarks in &lt;a href=&#34;https://docs.confident-ai.com/docs/benchmarks-introduction?utm_source=GitHub&#34;&gt;under 10 lines of code.&lt;/a&gt;, which includes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;MMLU&lt;/li&gt; &#xA;   &lt;li&gt;HellaSwag&lt;/li&gt; &#xA;   &lt;li&gt;DROP&lt;/li&gt; &#xA;   &lt;li&gt;BIG-Bench Hard&lt;/li&gt; &#xA;   &lt;li&gt;TruthfulQA&lt;/li&gt; &#xA;   &lt;li&gt;HumanEval&lt;/li&gt; &#xA;   &lt;li&gt;GSM8K&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.confident-ai.com?utm_source=GitHub&#34;&gt;Automatically integrated with Confident AI&lt;/a&gt; for continous evaluation throughout the lifetime of your LLM (app): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;log evaluation results and analyze metrics pass / fails&lt;/li&gt; &#xA;   &lt;li&gt;compare and pick the optimal hyperparameters (eg. prompt templates, chunk size, models used, etc.) based on evaluation results&lt;/li&gt; &#xA;   &lt;li&gt;debug evaluation results via LLM traces&lt;/li&gt; &#xA;   &lt;li&gt;manage evaluation test cases / datasets in one place&lt;/li&gt; &#xA;   &lt;li&gt;track events to identify live LLM responses in production&lt;/li&gt; &#xA;   &lt;li&gt;real-time evaluation in production&lt;/li&gt; &#xA;   &lt;li&gt;add production events to existing evaluation datasets to strength evals over time&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(Note that while some metrics are for RAG, others are better for a fine-tuning use case. Make sure to consult our docs to pick the right metric.)&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üîå Integrations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü¶Ñ LlamaIndex, to &lt;a href=&#34;https://docs.confident-ai.com/docs/integrations-llamaindex?utm_source=GitHub&#34;&gt;&lt;strong&gt;unit test RAG applications in CI/CD&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ü§ó Hugging Face, to &lt;a href=&#34;https://docs.confident-ai.com/docs/integrations-huggingface?utm_source=GitHub&#34;&gt;&lt;strong&gt;enable real-time evaluations during LLM fine-tuning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üöÄ QuickStart&lt;/h1&gt; &#xA;&lt;p&gt;Let&#39;s pretend your LLM application is a RAG based customer support chatbot; here&#39;s how DeepEval can help test what you&#39;ve built.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U deepeval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Create an account (highly recommended)&lt;/h2&gt; &#xA;&lt;p&gt;Although optional, creating an account on our platform will allow you to log test results, enabling easy tracking of changes and performances over iterations. This step is optional, and you can run test cases even without logging in, but we highly recommend giving it a try.&lt;/p&gt; &#xA;&lt;p&gt;To login, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepeval login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy &lt;a href=&#34;https://docs.confident-ai.com/docs/data-privacy?utm_source=GitHub&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Writing your first test case&lt;/h2&gt; &#xA;&lt;p&gt;Create a test file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;touch test_chatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &lt;code&gt;test_chatbot.py&lt;/code&gt; and write your first test case using DeepEval:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pytest&#xA;from deepeval import assert_test&#xA;from deepeval.metrics import AnswerRelevancyMetric&#xA;from deepeval.test_case import LLMTestCase&#xA;&#xA;def test_case():&#xA;    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)&#xA;    test_case = LLMTestCase(&#xA;        input=&#34;What if these shoes don&#39;t fit?&#34;,&#xA;        # Replace this with the actual output from your LLM application&#xA;        actual_output=&#34;We offer a 30-day full refund at no extra costs.&#34;,&#xA;        retrieval_context=[&#34;All customers are eligible for a 30 day full refund at no extra costs.&#34;]&#xA;    )&#xA;    assert_test(test_case, [answer_relevancy_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; as an environment variable (you can also evaluate using your own custom model, for more details visit &lt;a href=&#34;https://docs.confident-ai.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub&#34;&gt;this part of our docs&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=&#34;...&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And finally, run &lt;code&gt;test_chatbot.py&lt;/code&gt; in the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepeval test run test_chatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your test should have passed ‚úÖ&lt;/strong&gt; Let&#39;s breakdown what happened.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The variable &lt;code&gt;input&lt;/code&gt; mimics user input, and &lt;code&gt;actual_output&lt;/code&gt; is a placeholder for your chatbot&#39;s intended output based on this query.&lt;/li&gt; &#xA; &lt;li&gt;The variable &lt;code&gt;retrieval_context&lt;/code&gt; contains the relevant information from your knowledge base, and &lt;code&gt;AnswerRelevancyMetric(threshold=0.5)&lt;/code&gt; is an out-of-the-box metric provided by DeepEval. It helps evaluate the relevancy of your LLM output based on the provided context.&lt;/li&gt; &#xA; &lt;li&gt;The metric score ranges from 0 - 1. The &lt;code&gt;threshold=0.5&lt;/code&gt; threshold ultimately determines whether your test has passed or not.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.confident-ai.com/docs/getting-started?utm_source=GitHub&#34;&gt;Read our documentation&lt;/a&gt; for more information on how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Evaluating Without Pytest Integration&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepeval import evaluate&#xA;from deepeval.metrics import AnswerRelevancyMetric&#xA;from deepeval.test_case import LLMTestCase&#xA;&#xA;answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)&#xA;test_case = LLMTestCase(&#xA;    input=&#34;What if these shoes don&#39;t fit?&#34;,&#xA;    # Replace this with the actual output from your LLM application&#xA;    actual_output=&#34;We offer a 30-day full refund at no extra costs.&#34;,&#xA;    retrieval_context=[&#34;All customers are eligible for a 30 day full refund at no extra costs.&#34;]&#xA;)&#xA;evaluate([test_case], [answer_relevancy_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using Standalone Metrics&lt;/h2&gt; &#xA;&lt;p&gt;DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepeval.metrics import AnswerRelevancyMetric&#xA;from deepeval.test_case import LLMTestCase&#xA;&#xA;answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)&#xA;test_case = LLMTestCase(&#xA;    input=&#34;What if these shoes don&#39;t fit?&#34;,&#xA;    # Replace this with the actual output from your LLM application&#xA;    actual_output=&#34;We offer a 30-day full refund at no extra costs.&#34;,&#xA;    retrieval_context=[&#34;All customers are eligible for a 30 day full refund at no extra costs.&#34;]&#xA;)&#xA;&#xA;answer_relevancy_metric.measure(test_case)&#xA;print(answer_relevancy_metric.score)&#xA;# Most metrics also offer an explanation&#xA;print(answer_relevancy_metric.reason)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluating a Dataset / Test Cases in Bulk&lt;/h2&gt; &#xA;&lt;p&gt;In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pytest&#xA;from deepeval import assert_test&#xA;from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric&#xA;from deepeval.test_case import LLMTestCase&#xA;from deepeval.dataset import EvaluationDataset&#xA;&#xA;first_test_case = LLMTestCase(input=&#34;...&#34;, actual_output=&#34;...&#34;, context=[&#34;...&#34;])&#xA;second_test_case = LLMTestCase(input=&#34;...&#34;, actual_output=&#34;...&#34;, context=[&#34;...&#34;])&#xA;&#xA;dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])&#xA;&#xA;@pytest.mark.parametrize(&#xA;    &#34;test_case&#34;,&#xA;    dataset,&#xA;)&#xA;def test_customer_chatbot(test_case: LLMTestCase):&#xA;    hallucination_metric = HallucinationMetric(threshold=0.3)&#xA;    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)&#xA;    assert_test(test_case, [hallucination_metric, answer_relevancy_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run this in the CLI, you can also add an optional -n flag to run tests in parallel&#xA;deepeval test run test_&amp;lt;filename&amp;gt;.py -n 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Alternatively, although we recommend using &lt;code&gt;deepeval test run&lt;/code&gt;, you can evaluate a dataset/test cases without using our Pytest integration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepeval import evaluate&#xA;...&#xA;&#xA;evaluate(dataset, [answer_relevancy_metric])&#xA;# or&#xA;dataset.evaluate([answer_relevancy_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Real-time Evaluations on Confident AI&lt;/h1&gt; &#xA;&lt;p&gt;We offer a &lt;a href=&#34;https://app.confident-ai.com?utm_source=Github&#34;&gt;web platform&lt;/a&gt; for you to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Log and view all the test results / metrics data from DeepEval&#39;s test runs.&lt;/li&gt; &#xA; &lt;li&gt;Debug evaluation results via LLM traces.&lt;/li&gt; &#xA; &lt;li&gt;Compare and pick the optimal hyperparameteres (prompt templates, models, chunk size, etc.).&lt;/li&gt; &#xA; &lt;li&gt;Create, manage, and centralize your evaluation datasets.&lt;/li&gt; &#xA; &lt;li&gt;Track events in production and augment your evaluation dataset for continous evaluation.&lt;/li&gt; &#xA; &lt;li&gt;Track events in production, view evaluation results and historical insights.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Everything on Confident AI, including how to use Confident is available &lt;a href=&#34;https://docs.confident-ai.com/docs/confident-ai-introduction?utm_source=GitHub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To begin, login from the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepeval login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the instructions to log in, create your account, and paste your API key into the CLI.&lt;/p&gt; &#xA;&lt;p&gt;Now, run your test file again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepeval test run test_chatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://d2lsxfc3p6r9rv.cloudfront.net/confident-test-cases.png&#34; alt=&#34;ok&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://github.com/confident-ai/deepeval/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement G-Eval&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Referenceless Evaluation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Production Evaluation &amp;amp; Logging&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Evaluation Dataset Creation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Integrations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; lLamaIndex&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; langChain&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Guidance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Guardrails&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; EmbedChain&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;p&gt;Built by the founders of Confident AI. Contact &lt;a href=&#34;mailto:jeffreyip@confident-ai.com&#34;&gt;jeffreyip@confident-ai.com&lt;/a&gt; for all enquiries.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;DeepEval is licensed under Apache 2.0 - see the &lt;a href=&#34;https://github.com/confident-ai/deepeval/raw/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dockur/macos</title>
    <updated>2024-11-13T01:34:21Z</updated>
    <id>tag:github.com,2024-11-13:/dockur/macos</id>
    <link href="https://github.com/dockur/macos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OSX (macOS) inside a Docker container.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;OSX&lt;br&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://github.com/dockur/macos/&#34;&gt;&lt;img src=&#34;https://github.com/dockur/macos/raw/master/.github/logo.png&#34; title=&#34;Logo&#34; style=&#34;max-width:100%;&#34; width=&#34;128&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/dockur/macos/&#34;&gt;&lt;img src=&#34;https://github.com/dockur/macos/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/dockurr/macos/tags&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/dockurr/macos/latest?arch=amd64&amp;amp;sort=semver&amp;amp;color=066da5&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/dockurr/macos/tags&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/image-size/dockurr/macos/latest?color=066da5&amp;amp;label=size&#34; alt=&#34;Size&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dockur/macos/pkgs/container/macos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fdockur%2Fmacos%2Fmacos.json&amp;amp;query=%24.downloads&amp;amp;logo=github&amp;amp;style=flat&amp;amp;color=066da5&amp;amp;label=pulls&#34; alt=&#34;Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/dockurr/macos/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/dockurr/macos.svg?style=flat&amp;amp;label=pulls&amp;amp;logo=docker&#34; alt=&#34;Pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt;&lt;/h1&gt; &#xA;&lt;p&gt;OSX (macOS) inside a Docker container.&lt;/p&gt; &#xA;&lt;h2&gt;Features ‚ú®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KVM acceleration&lt;/li&gt; &#xA; &lt;li&gt;Web-based viewer&lt;/li&gt; &#xA; &lt;li&gt;Automatic download&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage üê≥&lt;/h2&gt; &#xA;&lt;p&gt;Via Docker Compose:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;services:&#xA;  macos:&#xA;    image: dockurr/macos&#xA;    container_name: macos&#xA;    environment:&#xA;      VERSION: &#34;13&#34;&#xA;    devices:&#xA;      - /dev/kvm&#xA;    cap_add:&#xA;      - NET_ADMIN&#xA;    ports:&#xA;      - 8006:8006&#xA;      - 5900:5900/tcp&#xA;      - 5900:5900/udp&#xA;    stop_grace_period: 2m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Via Docker CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -p 8006:8006 --device=/dev/kvm --cap-add NET_ADMIN --stop-timeout 120 dockurr/macos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Via Kubernetes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl apply -f kubernetes.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ üí¨&lt;/h2&gt; &#xA;&lt;h3&gt;How do I use it?&lt;/h3&gt; &#xA;&lt;p&gt;Very simple! These are the steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the container and connect to &lt;a href=&#34;http://localhost:8006&#34;&gt;port 8006&lt;/a&gt; using your web browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Choose &lt;code&gt;Disk Utility&lt;/code&gt; and then select the largest &lt;code&gt;Apple Inc. VirtIO Block Media&lt;/code&gt; disk.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;code&gt;Erase&lt;/code&gt; button to format the disk, and give it any recognizable name you like.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Close the current window and proceed the installation by clicking &lt;code&gt;Reinstall macOS&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When prompted where you want to install it, select the disk you just created previously.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After all files are copied, select your region, language, and account settings.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Enjoy your brand new machine, and don&#39;t forget to star this repo!&lt;/p&gt; &#xA;&lt;h3&gt;How do I select the macOS version?&lt;/h3&gt; &#xA;&lt;p&gt;By default, macOS 13 (Ventura) will be installed. But you can add the &lt;code&gt;VERSION&lt;/code&gt; environment variable to your compose file, in order to specify an alternative macOS version to be downloaded:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  VERSION: &#34;13&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Select from the values below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;15&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 15&lt;/td&gt; &#xA;   &lt;td&gt;Sequoia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;14&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 14&lt;/td&gt; &#xA;   &lt;td&gt;Sonoma&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;13&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 13&lt;/td&gt; &#xA;   &lt;td&gt;Ventura&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;12&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 12&lt;/td&gt; &#xA;   &lt;td&gt;Monterey&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;11&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 11&lt;/td&gt; &#xA;   &lt;td&gt;Big Sur&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How do I change the storage location?&lt;/h3&gt; &#xA;&lt;p&gt;To change the storage location, include the following bind mount in your compose file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;volumes:&#xA;  - /var/osx:/storage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace the example path &lt;code&gt;/var/osx&lt;/code&gt; with the desired storage folder.&lt;/p&gt; &#xA;&lt;h3&gt;How do I change the size of the disk?&lt;/h3&gt; &#xA;&lt;p&gt;To expand the default size of 64 GB, add the &lt;code&gt;DISK_SIZE&lt;/code&gt; setting to your compose file and set it to your preferred capacity:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  DISK_SIZE: &#34;256G&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] This can also be used to resize the existing disk to a larger capacity without any data loss.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;How do I change the amount of CPU or RAM?&lt;/h3&gt; &#xA;&lt;p&gt;By default, the container will be allowed to use a maximum of 2 CPU cores and 4 GB of RAM.&lt;/p&gt; &#xA;&lt;p&gt;If you want to adjust this, you can specify the desired amount using the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  RAM_SIZE: &#34;8G&#34;&#xA;  CPU_CORES: &#34;4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How do I pass-through a USB device?&lt;/h3&gt; &#xA;&lt;p&gt;To pass-through a USB device, first lookup its vendor and product id via the &lt;code&gt;lsusb&lt;/code&gt; command, then add them to your compose file like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  ARGUMENTS: &#34;-device usb-host,vendorid=0x1234,productid=0x1234&#34;&#xA;devices:&#xA;  - /dev/bus/usb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How do I verify if my system supports KVM?&lt;/h3&gt; &#xA;&lt;p&gt;Only Linux and Windows 11 support KVM virtualization, macOS and Windows 10 do not unfortunately.&lt;/p&gt; &#xA;&lt;p&gt;You can run the following commands in Linux to check your system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install cpu-checker&#xA;sudo kvm-ok&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you receive an error from &lt;code&gt;kvm-ok&lt;/code&gt; indicating that KVM cannot be used, please check whether:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;the virtualization extensions (&lt;code&gt;Intel VT-x&lt;/code&gt; or &lt;code&gt;AMD SVM&lt;/code&gt;) are enabled in your BIOS.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;you enabled &#34;nested virtualization&#34; if you are running the container inside a virtual machine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;you are not using a cloud provider, as most of them do not allow nested virtualization for their VPS&#39;s.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you do not receive any error from &lt;code&gt;kvm-ok&lt;/code&gt; but the container still complains about KVM, please check whether:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;you are not using &#34;Docker Desktop for Linux&#34; as it does not support KVM, instead make use of Docker Engine directly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;it could help to add &lt;code&gt;privileged: true&lt;/code&gt; to your compose file (or &lt;code&gt;sudo&lt;/code&gt; to your &lt;code&gt;docker run&lt;/code&gt; command), to rule out any permission issue.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How do I run Windows in a container?&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;https://github.com/dockur/windows&#34;&gt;dockur/windows&lt;/a&gt; for that. It shares many of the same features, and even has completely automatic installation.&lt;/p&gt; &#xA;&lt;h3&gt;Is this project legal?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, this project contains only open-source code and does not distribute any copyrighted material. Neither does it try to circumvent any copyright protection measures. So under all applicable laws, this project will be considered legal.&lt;/p&gt; &#xA;&lt;p&gt;However, by installing Apple&#39;s macOS, you must accept their end-user license agreement, which does not permit installation on non-official hardware. So only run this container on hardware sold by Apple, as any other use will be a violation of their terms and conditions.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements üôè&lt;/h2&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://github.com/seitenca&#34;&gt;seitenca&lt;/a&gt;, this project would not exist without her invaluable work.&lt;/p&gt; &#xA;&lt;h2&gt;Stars üåü&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/dockur/macos&#34;&gt;&lt;img src=&#34;https://starchart.cc/dockur/macos.svg?variant=adaptive&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer ‚öñÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Only run this container on Apple hardware, any other use is not permitted by their EULA. The product names, logos, brands, and other trademarks referred to within this project are the property of their respective trademark holders. This project is not affiliated, sponsored, or endorsed by Apple Inc.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>home-assistant/supervisor</title>
    <updated>2024-11-13T01:34:21Z</updated>
    <id>tag:github.com,2024-11-13:/home-assistant/supervisor</id>
    <link href="https://github.com/home-assistant/supervisor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üè° Home Assistant Supervisor&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Home Assistant Supervisor&lt;/h1&gt; &#xA;&lt;h2&gt;First private cloud solution for home automation&lt;/h2&gt; &#xA;&lt;p&gt;Home Assistant (former Hass.io) is a container-based system for managing your Home Assistant Core installation and related applications. The system is controlled via Home Assistant which communicates with the Supervisor. The Supervisor provides an API to manage the installation. This includes changing network settings or installing and updating software.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Installation instructions can be found at &lt;a href=&#34;https://home-assistant.io/getting-started&#34;&gt;https://home-assistant.io/getting-started&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;For small changes and bugfixes you can just follow this, but for significant changes open a RFC first. Development instructions can be found &lt;a href=&#34;https://developers.home-assistant.io/docs/supervisor/development&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;Releases are done in 3 stages (channels) with this structure:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pull requests are merged to the &lt;code&gt;main&lt;/code&gt; branch.&lt;/li&gt; &#xA; &lt;li&gt;A new build is pushed to the &lt;code&gt;dev&lt;/code&gt; stage.&lt;/li&gt; &#xA; &lt;li&gt;Releases are published.&lt;/li&gt; &#xA; &lt;li&gt;A new build is pushed to the &lt;code&gt;beta&lt;/code&gt; stage.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/home-assistant/version/raw/master/stable.json&#34;&gt;&lt;code&gt;stable.json&lt;/code&gt;&lt;/a&gt; file is updated.&lt;/li&gt; &#xA; &lt;li&gt;The build that was pushed to &lt;code&gt;beta&lt;/code&gt; will now be pushed to &lt;code&gt;stable&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.openhomefoundation.org/&#34;&gt;&lt;img src=&#34;https://www.openhomefoundation.org/badges/home-assistant.png&#34; alt=&#34;Home Assistant - A project from the Open Home Foundation&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>