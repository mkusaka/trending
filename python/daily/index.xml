<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-29T01:31:49Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xinghaochen/TinySAM</title>
    <updated>2023-12-29T01:31:49Z</updated>
    <id>tag:github.com,2023-12-29:/xinghaochen/TinySAM</id>
    <link href="https://github.com/xinghaochen/TinySAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official PyTorch implementation of &#34;TinySAM: Pushing the Envelope for Efficient Segment Anything Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TinySAM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;TinySAM: Pushing the Envelope for Efficient Segment Anything Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yihao Chen, Houqiang Li, Yunhe Wang, Xinghao Chen&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;arXiv 2023&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2312.13789&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#citation&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/merve/tinysam&#34;&gt;&lt;code&gt;Hugging Face Demo&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img width=&#34;300&#34; alt=&#34;compare&#34; src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/tinysam_point.gif&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img width=&#34;300&#34; alt=&#34;compare&#34; src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/tinysam_box.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/27&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/merve/tinysam&#34;&gt;Models&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/merve/tinysam&#34;&gt;demo&lt;/a&gt; of TinySAM are now available in Hugging Face. Thanks for &lt;a href=&#34;https://github.com/merveenoyan&#34;&gt;merveenoyan&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/27&lt;/strong&gt;: Pre-trained models and codes of &lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#usage&#34;&gt;Q-TinySAM&lt;/a&gt; (quantized variant) are released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/27&lt;/strong&gt;: &lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt; codes for zero-shot instance segmentation task on COCO are released.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/22&lt;/strong&gt;: Pre-trained models and codes of TinySAM are released both in &lt;a href=&#34;https://github.com/xinghaochen/TinySAM&#34;&gt;Pytorch&lt;/a&gt; and &lt;a href=&#34;https://gitee.com/mindspore/models/tree/master/research/cv/TinySAM&#34;&gt;Mindspore&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;We propose a framework to obtain a tiny segment anything model (&lt;strong&gt;TinySAM&lt;/strong&gt;) while maintaining the strong zero-shot performance. We first propose a full-stage knowledge distillation method with online hard prompt sampling strategy to distill a lightweight student model. We also adapt the post-training quantization to the promptable segmentation task and further reducing the computational cost. Moreover, a hierarchical segmenting everything strategy is proposed to accelerate the everything inference by with almost no performance degradation. With all these proposed methods, our TinySAM leads to orders of magnitude computational reduction and pushes the envelope for efficient segment anything task. Extensive experiments on various zero-shot transfer tasks demonstrate the significantly advantageous performance of our TinySAM against counterpart methods.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/framework.png&#34; alt=&#34;framework&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;Figure 1: Overall framework and zero-shot results of TinySAM.&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/everything.png&#34; alt=&#34;everything&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;Figure 2: Our hierarchical strategy for everything mode.&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/fig/vis.png&#34; alt=&#34;vis&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;Figure 3: Visualization results of TinySAM.&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.7&lt;/code&gt; and we use &lt;code&gt;torch==1.10.2&lt;/code&gt; and &lt;code&gt;torchvision==0.11.3&lt;/code&gt;. To visualize the results, &lt;code&gt;matplotlib&amp;gt;=3.5.1&lt;/code&gt; is also required.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.7&lt;/li&gt; &#xA; &lt;li&gt;pytorch == 1.10.2&lt;/li&gt; &#xA; &lt;li&gt;torchvision == 0.11.3&lt;/li&gt; &#xA; &lt;li&gt;matplotlib==3.5.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download &lt;a href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/#evaluation&#34;&gt;checkpoints&lt;/a&gt; into the directory of &lt;em&gt;weights&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the demo code for single prompt of point or box.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the demo code for hierarchical segment everything strategy.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo_hierachical_everything.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Run the demo code for quantization inference.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python demo_quant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We follow the setting of original &lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;SAM&lt;/a&gt; paper and evaluate the zero-shot instance segmentaion on COCO and LVIS dataset. The experiment results are described as followed.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;FLOPs (G)&lt;/th&gt; &#xA;   &lt;th&gt;COCO AP (%)&lt;/th&gt; &#xA;   &lt;th&gt;LVIS AP (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-H&lt;/td&gt; &#xA;   &lt;td&gt;3166&lt;/td&gt; &#xA;   &lt;td&gt;46.5&lt;/td&gt; &#xA;   &lt;td&gt;44.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-L&lt;/td&gt; &#xA;   &lt;td&gt;1681&lt;/td&gt; &#xA;   &lt;td&gt;45.5&lt;/td&gt; &#xA;   &lt;td&gt;43.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SAM-B&lt;/td&gt; &#xA;   &lt;td&gt;677&lt;/td&gt; &#xA;   &lt;td&gt;41.0&lt;/td&gt; &#xA;   &lt;td&gt;40.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td&gt;344&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;   &lt;td&gt;34.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MobileSAM&lt;/td&gt; &#xA;   &lt;td&gt;232&lt;/td&gt; &#xA;   &lt;td&gt;41.0&lt;/td&gt; &#xA;   &lt;td&gt;37.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;TinySAM&lt;/strong&gt; &lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/1.0/tinysam.pth&#34;&gt;[ckpt]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;232&lt;/td&gt; &#xA;   &lt;td&gt;41.9&lt;/td&gt; &#xA;   &lt;td&gt;38.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Q-TinySAM&lt;/strong&gt; &lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/2.0/tinysam_w8a8.pth&#34;&gt;[ckpt]&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;61&lt;/td&gt; &#xA;   &lt;td&gt;41.3&lt;/td&gt; &#xA;   &lt;td&gt;37.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;First download the detection boxes (&lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/2.0/coco_instances_results_vitdet.json&#34;&gt;&lt;code&gt;coco_instances_results_vitdet.json&lt;/code&gt;&lt;/a&gt;) produced by ViTDet model, as well as the ground-truth instance segmentation labels(&lt;a href=&#34;https://github.com/xinghaochen/TinySAM/releases/download/2.0/instances_val2017.json&#34;&gt;&lt;code&gt;instances_val2017.json&lt;/code&gt;&lt;/a&gt;) and put them into &lt;code&gt;eval/json_files&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run the following code to perform evaluation for zero-shot instance segmentation on COCO dataset.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd eval; sh eval_coco.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results should be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.419&#xA;Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.683&#xA;Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.436&#xA;Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.260&#xA;Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.456&#xA;Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.583&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.325&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.511&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.532&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.390&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.577&#xA;Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.671&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank the following projects: &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;MobileSAM&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/Cream&#34;&gt;TinyViT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{tinysam,&#xA;  title={TinySAM: Pushing the Envelope for Efficient Segment Anything Model},&#xA;  author={Shu, Han and Li, Wenshuo and Tang, Yehui and Zhang, Yiman and Chen, Yihao and Li, Houqiang and Wang, Yunhe and Chen, Xinghao},&#xA;  journal={arXiv preprint arXiv:2312.13789},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under &lt;a rel=&#34;license&#34; href=&#34;https://raw.githubusercontent.com/xinghaochen/TinySAM/main/License.txt&#34;&gt; Apache License 2.0&lt;/a&gt;. Redistribution and use should follow this license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yangxy/PASD</title>
    <updated>2023-12-29T01:31:49Z</updated>
    <id>tag:github.com,2023-12-29:/yangxy/PASD</id>
    <link href="https://github.com/yangxy/PASD" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.14469&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cg.cs.tsinghua.edu.cn/people/~tyang&#34;&gt;Tao Yang&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, Peiran Ren&lt;sup&gt;1&lt;/sup&gt;, Xuansong Xie&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://www4.comp.polyu.edu.hk/~cslzhang&#34;&gt;Lei Zhang&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;br&gt; &lt;em&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;a href=&#34;https://damo.alibaba.com&#34;&gt;DAMO Academy, Alibaba Group&lt;/a&gt;, Hangzhou, China&lt;/em&gt;&lt;br&gt; &lt;em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;a href=&#34;http://www.comp.polyu.edu.hk&#34;&gt;Department of Computing, The Hong Kong Polytechnic University&lt;/a&gt;, Hong Kong, China&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Our model can do various tasks. Hope you can enjoy it.&lt;/h2&gt; &#xA;&lt;h2&gt;Realistic Image SR&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/frog.gif&#34; width=&#34;390px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/house.gif&#34; width=&#34;390px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Old photo restoration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/629e4da70703193b.gif&#34; width=&#34;390px&#34; height=&#34;520&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/27d38eeb2dbbe7c9.gif&#34; width=&#34;390px&#34; height=&#34;520&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Personalized Stylization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000020x2.gif&#34; width=&#34;390px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000067x2.gif&#34; width=&#34;390px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Colorization&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000004x2.gif&#34; width=&#34;390px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/yangxy/PASD/main/samples/000080x2.gif&#34; width=&#34;390px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;(2023-10-20) Add additional noise level via &lt;code&gt;--added_noise_level&lt;/code&gt; and the SR result achieves a great balance between &#34;extremely-detailed&#34; and &#34;over-smoothed&#34;. Very interesting!. You can control the SR&#39;s detail level freely.&lt;/p&gt; &#xA;&lt;p&gt;(2023-10-18) Completely solved the &lt;a href=&#34;https://github.com/yangxy/PASD/issues/16&#34;&gt;issues&lt;/a&gt; by initializing latents with input LR images. Interestingly, the SR results also become much more stable.&lt;/p&gt; &#xA;&lt;p&gt;(2023-10-11) &lt;a href=&#34;https://colab.research.google.com/drive/1lZ_-rSGcmreLCiRniVT973x6JLjFiC-b?usp=sharing&#34;&gt;Colab demo&lt;/a&gt; is now available. Credits to &lt;a href=&#34;https://github.com/MasahideOkada&#34;&gt;Masahide Okada&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;(2023-10-09) Add training dataset.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-28) Add tiled latent to allow upscaling ultra high-resolution images. Please carefully set &lt;code&gt;latent_tiled_size&lt;/code&gt; as well as &lt;code&gt;--decoder_tiled_size&lt;/code&gt; when upscaling large images.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-12) Add Gradio demo.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-11) Upload pre-trained models.&lt;/p&gt; &#xA;&lt;p&gt;(2023-09-07) Upload source codes.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/yangxy/PASD.git&#xA;cd PASD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download SD1.5 models from &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;huggingface&lt;/a&gt; and put them into &lt;code&gt;checkpoints/stable-diffusion-v1-5&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Prepare training datasets. Please check &lt;code&gt;dataloader/localdataset.py&lt;/code&gt; and &lt;code&gt;dataloader/webdataset.py&lt;/code&gt; carefully and set the paths correctly. We highly recommend to use &lt;code&gt;dataloader/webdataset.py&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our training dataset. &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV2K_train_HR.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200038&amp;amp;Signature=pS4wwrAMm3wdlpU%2BxYKUsOkrgjA%3D&#34;&gt;DIV2K_train_HR&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-0.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200091&amp;amp;Signature=JJBUqbfNoOLnzGp9mFNDFJsUh%2Fk%3D&#34;&gt;DIV8K-0&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-1.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200121&amp;amp;Signature=ou0ooaSaGVtMx0tFN3rZEx236s8%3D&#34;&gt;DIV8K-1&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-2.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200150&amp;amp;Signature=5FQeeqxX%2Fzb9%2FhnwTklz8N34hKI%3D&#34;&gt;DIV8K-2&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-3.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200175&amp;amp;Signature=h08kXBnZ9%2FTFpxU%2F5apBQvVMuic%3D&#34;&gt;DIV8K-3&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-4.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200201&amp;amp;Signature=6wz8PREaNkykhZ%2BAZeoeGO3Jm4Y%3D&#34;&gt;DIV8K-4&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/DIV8K-5.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200226&amp;amp;Signature=uCWbd2jaJsAoYFKtr7nlWQ3WiOY%3D&#34;&gt;DIV8K-5&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/FFHQ_5K.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200266&amp;amp;Signature=GBDovtNCqRR2nNz%2B4UKlmpVfJtE%3D&#34;&gt;FFHQ_5K&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Flickr2K_HR-0.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200304&amp;amp;Signature=vu7uOdZdLB5uSbdBKbsBMnmnjvQ%3D&#34;&gt;Flickr2K_HR-0&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Flickr2K_HR-1.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200331&amp;amp;Signature=9ID3bgjXfR7xt5zUDfNkRG50DQc%3D&#34;&gt;Flickr2K_HR-1&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Flickr2K_HR-2.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200371&amp;amp;Signature=Zww60FOUIX%2Bysg%2FJaaM%2BvdK5ePk%3D&#34;&gt;Flickr2K_HR-2&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_animal.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200402&amp;amp;Signature=QnIJVqzwBITZW%2FNTatRxaKiyjaY%3D&#34;&gt;OST_animal&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_building.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200430&amp;amp;Signature=W9AYnjnoftY8YF2NII6hx9Xf%2B0o%3D&#34;&gt;OST_building&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_grass.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200460&amp;amp;Signature=qoy%2FNAJUxOdVqYb6CpL4gt9aYxo%3D&#34;&gt;OST_grass&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_mountain.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200477&amp;amp;Signature=si3mNDvz2ZxIyoLbyuZmIOvzctE%3D&#34;&gt;OST_mountain&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_plant.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200510&amp;amp;Signature=jz%2BZeVmeoi6Lu9CQHId6XZhyyk8%3D&#34;&gt;OST_plant&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_sky.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200533&amp;amp;Signature=YWdVSXe9gKAVSm2pZgtBr1NQyp4%3D&#34;&gt;OST_sky&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/OST_water.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200558&amp;amp;Signature=%2FGtrRmYFt6xOURPFR836IqXO7Q0%3D&#34;&gt;OST_water&lt;/a&gt; | &lt;a href=&#34;http://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/data/SR_data/tars/pngtxt/Unsplash2K.tar.gz?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&amp;amp;Expires=2012200614&amp;amp;Signature=SF1XLJp8swA3O2Rr1eYI%2FLCNg2U%3D&#34;&gt;Unsplash2K&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train a PASD.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./train_pasd.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;if you want to train pasd_light, use &lt;code&gt;--use_pasd_light&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Test PASD.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Download our pre-trained models &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd.zip&#34;&gt;pasd&lt;/a&gt; | &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd_rrdb.zip&#34;&gt;pasd_rrdb&lt;/a&gt; | &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd_light.zip&#34;&gt;pasd_light&lt;/a&gt; | &lt;a href=&#34;https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/PASD/pasd_light_rrdb.zip&#34;&gt;pasd_light_rrdb&lt;/a&gt;, and put them into &lt;code&gt;runs/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python test_pasd.py # --use_pasd_light --use_personalized_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please read the arguments in &lt;code&gt;test_pasd.py&lt;/code&gt; carefully. We adopt the tiled vae method proposed by &lt;a href=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111&#34;&gt;multidiffusion-upscaler-for-automatic1111&lt;/a&gt; to save GPU memory.&lt;/p&gt; &#xA;&lt;p&gt;Please try &lt;code&gt;--use_personalized_model&lt;/code&gt; for personalized stylizetion, old photo restoration and real-world SR. Set &lt;code&gt;--conditioning_scale&lt;/code&gt; for different stylized strength.&lt;/p&gt; &#xA;&lt;p&gt;We use personalized models including &lt;a href=&#34;https://civitai.com/models/43331/&#34;&gt;majicMIX realistic&lt;/a&gt;(for SR and restoration), &lt;a href=&#34;https://civitai.com/models/30240/&#34;&gt;ToonYou&lt;/a&gt;(for stylization) and &lt;a href=&#34;https://huggingface.co/nitrosocke/mo-di-diffusion&#34;&gt;modern disney style&lt;/a&gt;(&lt;code&gt;unet&lt;/code&gt; only, for stylization). You can download more from communities and put them into &lt;code&gt;checkpoints/personalized_models&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If the default setting does not yield good results, try different &lt;code&gt;--pasd_model_path&lt;/code&gt;, &lt;code&gt;--seed&lt;/code&gt;, &lt;code&gt;--prompt&lt;/code&gt;, &lt;code&gt;--upscale&lt;/code&gt;, or &lt;code&gt;--high_level_info&lt;/code&gt; to get better performance.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradio Demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_pasd.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work is useful for your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{yang2023pasd,&#xA;    title={Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization},&#xA;    author={Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang},&#xA;    booktitle={Arxiv},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;¬© Alibaba, 2023. For academic and non-commercial use only.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;Our project is based on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions about this paper, feel free to reach me at &lt;a href=&#34;mailto:yangtao9009@gmail.com&#34;&gt;yangtao9009@gmail.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mouredev/roadmap-retos-programacion</title>
    <updated>2023-12-29T01:31:49Z</updated>
    <id>tag:github.com,2023-12-29:/mouredev/roadmap-retos-programacion</id>
    <link href="https://github.com/mouredev/roadmap-retos-programacion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ruta de estudio basada en ejercicios de c√≥digo semanales en 2024 de la comunidad MoureDev para aprender y practicar l√≥gica usando cualquier lenguaje de programaci√≥n.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Images/header.jpg&#34; alt=&#34;https://retosdeprogramacion.com&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Roadmap retos de programaci√≥n semanales 2024&lt;/h1&gt; &#xA;&lt;h3&gt;Ruta de estudio con ejercicios para mejorar tu l√≥gica de programaci√≥n y aprender cualquier lenguaje. Gratis, a tu ritmo y en comunidad.&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://retosdeprogramacion.com/roadmap&#34;&gt;https://retosdeprogramacion.com/roadmap&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/retos-programacion-web&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/retos-programacion-web?label=Web%20Retos%20Programaci%C3%B3n&amp;amp;style=social&#34; alt=&#34;Retos programaci√≥n web&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Informaci√≥n importante&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cada semana se publica un nuevo reto y se corrige en directo desde &lt;strong&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/strong&gt; el ejercicio de la semana pasada.&lt;/li&gt; &#xA; &lt;li&gt;En la secci√≥n &#34;Eventos&#34; de nuestro servidor de &lt;strong&gt;&lt;a href=&#34;https://discord.gg/mouredev&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt; encontrar√°s el d√≠a y horario por pa√≠s de los directos.&lt;/li&gt; &#xA; &lt;li&gt;Puedes utilizar &lt;strong&gt;cualquier lenguaje de programaci√≥n&lt;/strong&gt;, y encontrar tanto mis correcciones como las de la comunidad en el directorio de cada reto.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;¬øQuieres participar?&lt;/strong&gt; Te lo explico en la secci√≥n &lt;strong&gt;&lt;a href=&#34;https://github.com/mouredev/roadmap-retos-programacion#instrucciones&#34;&gt;Instrucciones&lt;/a&gt;&lt;/strong&gt; en este mismo documento.&lt;/li&gt; &#xA; &lt;li&gt;Los retos siguen un orden basado en su ruta de estudio pero si ya tienes conocimientos puedes resolverlos de manera totalmente independiente. Simplemente revisa su nivel de dificultad.&lt;/li&gt; &#xA; &lt;li&gt;Una vez se haya cumplido la semana de publicaci√≥n del reto, podr√°s consultar mi correcci√≥n y las de la comunidad en cualquier lenguaje de programaci√≥n.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Consulta la &lt;a href=&#34;https://retosdeprogramacion.com/roadmap&#34;&gt;web&lt;/a&gt; para m√°s informaci√≥n.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Correcci√≥n y pr√≥ximo ejercicio&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;h4&gt;Martes 2 de Enero de 2024 a las 20:00 (hora Espa√±a) desde &lt;strong&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; &#xA; &lt;h4&gt;Consulta el &lt;strong&gt;&lt;a href=&#34;https://discord.gg/C83vqurv?event=1189147970021642271&#34;&gt;horario&lt;/a&gt;&lt;/strong&gt; por pa√≠s y crea un &lt;strong&gt;&lt;a href=&#34;https://discord.gg/C83vqurv?event=1189147970021642271&#34;&gt;recordatorio&lt;/a&gt;&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;#&lt;/th&gt; &#xA;   &lt;th&gt;Ejercicio&lt;/th&gt; &#xA;   &lt;th&gt;Correcci√≥n&lt;/th&gt; &#xA;   &lt;th&gt;V√≠deo&lt;/th&gt; &#xA;   &lt;th&gt;Comunidad&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;00&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Roadmap/00%20-%20SINTAXIS,%20VARIABLES,%20TIPOS%20DE%20DATOS%20Y%20HOLA%20MUNDO/ejercicio.md&#34;&gt;SINTAXIS, VARIABLES, TIPOS DE DATOS Y HOLA MUNDO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discord.gg/C83vqurv?event=1189147970021642271&#34;&gt;üóìÔ∏è 02/01/24&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Roadmap/00%20-%20SINTAXIS,%20VARIABLES,%20TIPOS%20DE%20DATOS%20Y%20HOLA%20MUNDO/&#34;&gt;üë•&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Instrucciones&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Haz un &lt;a href=&#34;https://github.com/mouredev/roadmap-retos-programacion/fork&#34;&gt;FORK&lt;/a&gt; del proyecto y trabaja con Git para ir sincronizando las actualizaciones.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;En el proyecto tienes un directorio para cada ejercicio en la carpeta &lt;a href=&#34;https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/Roadmap&#34;&gt;Roadmap&lt;/a&gt;. Dentro de cada directorio encontrar√°s un fichero llamado &lt;strong&gt;ejercicio.md&lt;/strong&gt; con el enunciado de cada reto.&lt;/li&gt; &#xA; &lt;li&gt;Si quieres compartir tu propia soluci√≥n de un ejercicio con la comunidad, crea un fichero de c√≥digo con tu nombre y extensi√≥n, y realiza una &lt;a href=&#34;https://docs.github.com/es/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request&#34;&gt;&lt;strong&gt;PULL REQUEST&lt;/strong&gt;&lt;/a&gt; contra el repositorio.&lt;/li&gt; &#xA; &lt;li&gt;El fichero de c√≥digo debe situarse dentro del directorio del reto, en la carpeta correspondiente al lenguaje de programaci√≥n utilizado (si no existe la carpeta del lenguaje, cr√©ala con todas sus letras en min√∫sculas). Por ejemplo, si has resuelto el reto #00 utilizando el lenguaje de programaci√≥n Python y tu usuario de GitHub se llama &#34;mouredev&#34;, tu correcci√≥n deber√° estar en &lt;strong&gt;&#34;Roadmap/#00/python/mouredev.py&#34;&lt;/strong&gt;. El t√≠tulo de la Pull Request tambi√©n debe seguir este formato: &lt;strong&gt;&#34;#[n√∫mero] - [lenguaje_utilizado]&#34;&lt;/strong&gt;. En el ejemplo anterior ser√≠a &lt;strong&gt;&#34;#00 - Python&#34;&lt;/strong&gt;. Se rechazar√°n las Pull Request que no sigan este formato o contengan ficheros adicionales.&lt;/li&gt; &#xA; &lt;li&gt;Cada &lt;strong&gt;SEMANA&lt;/strong&gt; (consulta el d√≠a en el ejercicio correspondiente) realizar√© una transmisi√≥n en directo desde &lt;strong&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;Twitch&lt;/a&gt;&lt;/strong&gt; corrigiendo el reto, revisando soluciones de la comunidad y publicando un nuevo ejercicio.&lt;/li&gt; &#xA; &lt;li&gt;Si necesitas ayuda o quieres comentar cualquier cosa sobre los retos semanales, tienes el canal &#34;reto-semanal‚Äù en nuestro servidor de &lt;strong&gt;&lt;a href=&#34;https://discord.gg/mouredev&#34;&gt;Discord&lt;/a&gt;&lt;/strong&gt; (tambi√©n el d√≠a y horario de correcci√≥n en la secci√≥n &#34;Eventos&#34;).&lt;/li&gt; &#xA; &lt;li&gt;Puedes proponer Pull Request con propuestas o correcciones sobre ejercicios del resto de la comunidad si estos poseen errores. De esta manera colaboraremos para crear un repositorio cada vez m√°s valioso.&lt;/li&gt; &#xA; &lt;li&gt;Si se te solicita un cambio/correcci√≥n en una Pull Request, y al cabo de 2 semanas no se muestra nueva actividad, se cerrar√° esa petici√≥n para mantener el repositorio limpio. Por supuesto, puedes volver a enviar la Pull Request cuando quieras.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Aclaraciones&lt;/h2&gt; &#xA;&lt;p&gt;Si tienes dudas con el nombre del directorio de alg√∫n lenguaje, intenta consultar el nombre que se ha empleado en ejercicios anteriores. Algunos ejemplos que puedes llegar a dudar:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;c#, no csharp&lt;/li&gt; &#xA; &lt;li&gt;c++, no cplusplus&lt;/li&gt; &#xA; &lt;li&gt;go, no golang&lt;/li&gt; &#xA; &lt;li&gt;javascript, no js&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Gu√≠a r√°pida Git y GitHub&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Realiza un &lt;a href=&#34;https://github.com/mouredev/roadmap-retos-programacion/fork&#34;&gt;FORK&lt;/a&gt; del repositorio de retos semanales desde GitHub.&lt;/li&gt; &#xA; &lt;li&gt;CLONA ese repositorio a tu m√°quina local &lt;code&gt;git clone [TU-REPOSITORIO]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Crea una RAMA para la soluci√≥n y despl√°zate a ella &lt;code&gt;git checkout -b [EL-NOMBRE-DE-TU-RAMA]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A√±ade el fichero de tu soluci√≥n al STAGE &lt;code&gt;git add [FICHERO-DE-TU-RETO]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Haz COMMIT con el mensaje de la soluci√≥n &lt;code&gt;git commit -m &#34;#[N√öMERO-RETO] - [LENGUAJE-UTILIZADO]&#34;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Haz PUSH &lt;code&gt;git push [EL-NOMBRE-DE-TU-RAMA]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;En el repositorio principal debes ir a la rama y hacer &lt;a href=&#34;https://docs.github.com/es/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request&#34;&gt;PULL REQUEST&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;CONTRIBUTE.&lt;/li&gt; &#xA; &lt;li&gt;CREATE PULL REQUEST (cubre la plantilla que te aparecer√°).&lt;/li&gt; &#xA; &lt;li&gt;Si el proceso de entrega se ha realizado de forma correcta, se a√±adir√° tu correcci√≥n al repositorio. En caso contrario, se te notificar√°n los cambios a realizar o los motivos del rechazo.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;He creado un curso completo gratis para aprender a trabajar con Git y GitHub desde cero.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/hello-git&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/hello-git?label=Curso%20Git%20GitHub&amp;amp;style=social&#34; alt=&#34;Curso Git y GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;M√°s retos de programaci√≥n&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consulta los 101 retos de programaci√≥n resueltos y las 12 aplicaciones para tu portfolio que ya hemos desarrollado.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev/retos-programacion-2023&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/retos-programacion-2023?label=Retos%20Programaci%C3%B3n%202023&amp;amp;style=social&#34; alt=&#34;Retos programaci√≥n 2023&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mouredev/Weekly-Challenge-2022-Kotlin&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/Weekly-Challenge-2022-Kotlin?label=Retos%20Semanales%202022&amp;amp;style=social&#34; alt=&#34;Retos programaci√≥n 2022&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mouredev/Monthly-App-Challenge-2022&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mouredev/Monthly-App-Challenge-2022?label=Aplicaciones%20portafolio&amp;amp;style=social&#34; alt=&#34;Aplicaciones portafolio&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png&#34; alt=&#34;https://mouredev.com&#34;&gt; Hola, mi nombre es Brais Moure.&lt;/h2&gt; &#xA;&lt;h3&gt;Freelance full-stack iOS &amp;amp; Android engineer&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitch.tv/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitch-Retos_en_directo-9146FF?style=for-the-badge&amp;amp;logo=twitch&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Twitch&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Chat_comunidad-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://moure.dev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Links_de_inter%C3%A9s-moure.dev-39E09B?style=for-the-badge&amp;amp;logo=Linktree&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Link&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtube.com/mouredevapps?sub_confirmation=1&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/subscribers/UCxPD7bsocoAMq8Dj18kmGyQ?style=social&#34; alt=&#34;YouTube Channel Subscribers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitch.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitch/status/mouredev?style=social&#34; alt=&#34;Twitch Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mouredev.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/729672926432985098?style=social&amp;amp;label=Discord&amp;amp;logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/mouredev?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/followers/mouredev?style=social&#34; alt=&#34;GitHub Followers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/mouredev?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Soy ingeniero de software desde 2010. Desde 2018 a√±os combino mi trabajo desarrollando Apps con la creaci√≥n de contenido formativo sobre programaci√≥n y tecnolog√≠a en diferentes redes sociales como &lt;strong&gt;&lt;a href=&#34;https://moure.dev&#34;&gt;@mouredev&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;En mi perfil de GitHub tienes m√°s informaci√≥n&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mouredev&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-MoureDev-14a1f0?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=101010&#34; alt=&#34;Web&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>