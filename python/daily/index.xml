<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-08-06T01:32:16Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVIDIAGameWorks/kaolin-wisp</title>
    <updated>2022-08-06T01:32:16Z</updated>
    <id>tag:github.com,2022-08-06:/NVIDIAGameWorks/kaolin-wisp</id>
    <link href="https://github.com/NVIDIAGameWorks/kaolin-wisp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVIDIA Kaolin Wisp is a PyTorch library powered by NVIDIA Kaolin Core to work with neural fields (including NeRFs, NGLOD, instant-ngp and VQAD).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kaolin Wisp: A PyTorch Library and Engine for Neural Fields Research&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/media/demo.jpg&#34; alt=&#34;drawing&#34; width=&#34;1000&#34;&gt; &#xA;&lt;p&gt;NVIDIA Kaolin Wisp is a PyTorch library powered by &lt;a href=&#34;https://github.com/NVIDIAGameWorks/kaolin&#34;&gt;NVIDIA Kaolin Core&lt;/a&gt; to work with neural fields (including NeRFs, &lt;a href=&#34;https://nv-tlabs.github.io/nglod&#34;&gt;NGLOD&lt;/a&gt;, &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/&#34;&gt;instant-ngp&lt;/a&gt; and &lt;a href=&#34;https://nv-tlabs.github.io/vqad&#34;&gt;VQAD&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;NVIDIA Kaolin Wisp aims to provide a set of common utility functions for performing research on neural fields. This includes datasets, image I/O, mesh processing, and ray utility functions. Wisp also comes with building blocks like differentiable renderers and differentiable data structures (like octrees, hash grids, triplanar features) which are useful to build complex neural fields. It also includes debugging visualization tools, interactive rendering and training, logging, and trainer classes.&lt;/p&gt; &#xA;&lt;p&gt;For an overview on neural fields, we recommend you checkout the EG STAR report: &lt;a href=&#34;https://arxiv.org/abs/2111.11426&#34;&gt;Neural Fields for Visual Computing and Beyond&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License and Citation&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is licensed under the NVIDIA Source Code License. Commercial licenses are also available, free of charge. Please apply using this link (use &#34;Other&#34; and specify Kaolin Wisp): &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;https://www.nvidia.com/en-us/research/inquiries/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find the NVIDIA Kaolin Wisp library useful for your research, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{KaolinWispLibrary,&#xA;      author = {Towaki Takikawa and Or Perel and Clement Fuji Tsang and Charles Loop and Joey Litalien and Jonathan Tremblay and Maria Shugrina and Sanja Fidler},&#xA;      title = {Kaolin Wisp: A PyTorch Library and Engine for Neural Fields Research},&#xA;      year = {2022},&#xA;      howpublished={\url{https://github.com/NVIDIAGameWorks/kaolin-wisp}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/media/blocks.jpg&#34; alt=&#34;drawing&#34; width=&#34;750&#34;&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Differentiable feature grids &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Octree grids (from NGLOD)&lt;/li&gt; &#xA;   &lt;li&gt;Hash grids (from Instant-NGP)&lt;/li&gt; &#xA;   &lt;li&gt;Triplanar texture grids (from ConvOccNet, EG3D)&lt;/li&gt; &#xA;   &lt;li&gt;Codebook grids (from VQAD)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Acceleration structures for fast raytracing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Octree acceleration structures based on Kaolin Core SPC&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Tracers to trace rays against neural fields &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PackedSDFTracer for SDFs&lt;/li&gt; &#xA;   &lt;li&gt;PackedRFTracer for radiance fields (NeRFs)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Various datasets for common neural fields &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Standard Instant-NGP compatible datasets&lt;/li&gt; &#xA;   &lt;li&gt;RTMV dataset&lt;/li&gt; &#xA;   &lt;li&gt;SDF sampled from meshes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;An interactive renderer where you can train and visualize neural fields&lt;/li&gt; &#xA; &lt;li&gt;A set of core framework features (&lt;code&gt;wisp.core&lt;/code&gt;) for convenience&lt;/li&gt; &#xA; &lt;li&gt;A set of utility functions (&lt;code&gt;wisp.ops&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Have a feature request? Leave a GitHub issue!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;1. Create an anaconda environment&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to get started is to create a virtual Python 3.8 Anaconda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get update&#xA;sudo apt-get install libopenexr-dev &#xA;conda create -n wisp python=3.8&#xA;conda activate wisp&#xA;pip install --upgrade pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Install PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;You should first install PyTorch by following the &lt;a href=&#34;https://pytorch.org/&#34;&gt;official instructions&lt;/a&gt;. The code has been tested with &lt;code&gt;1.9.1&lt;/code&gt; to &lt;code&gt;1.12.0&lt;/code&gt; on Ubuntu 20.04.&lt;/p&gt; &#xA;&lt;h3&gt;3. Install Kaolin&lt;/h3&gt; &#xA;&lt;p&gt;You should also install Kaolin, following the &lt;a href=&#34;https://kaolin.readthedocs.io/en/latest/notes/installation.html&#34;&gt;instructions here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;4. Install the rest of the dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install the rest of the dependencies from &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/requirements.txt&#34;&gt;requirements&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. Installing the interactive renderer (optional)&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to use the interactive renderer and training visualizer, you will need additional dependencies. Note that you need to have OpenGL available on your system.&lt;/p&gt; &#xA;&lt;p&gt;To install (&lt;strong&gt;make sure you have the CUDA_HOME environment variable set!&lt;/strong&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/inducer/pycuda&#xA;cd pycuda&#xA;python configure.py --cuda-root=$CUDA_HOME --cuda-enable-gl&#xA;python setup.py develop&#xA;cd ..&#xA;pip install -r requirements_app.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;6. Installing Wisp&lt;/h3&gt; &#xA;&lt;p&gt;To install wisp, simply execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in the main wisp directory. You should now be able to run some examples!&lt;/p&gt; &#xA;&lt;h2&gt;Training &amp;amp; Rendering with Wisp&lt;/h2&gt; &#xA;&lt;h3&gt;Training NGLOD-NeRF from multiview RGB-D data&lt;/h3&gt; &#xA;&lt;p&gt;You will first need to download some sample data to run NGLOD-NeRF. Go to this &lt;a href=&#34;https://drive.google.com/file/d/18hY0DpX2bK-q9iY_cog5Q0ZI7YEjephE/view?usp=sharing&#34;&gt;Google Drive link&lt;/a&gt; to download a cool Lego V8 engine from the &lt;a href=&#34;http://www.cs.umd.edu/~mmeshry/projects/rtmv/&#34;&gt;RTMV dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have downloaded and extracted the data somewhere, you can train a NeRF using &lt;a href=&#34;https://nv-tlabs.github.io/nglod/&#34;&gt;NGLOD&lt;/a&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --dataset-num-workers 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate logs inside &lt;code&gt;_results/logs/runs/test-nglod-nerf&lt;/code&gt; in which you can find the trained checkpoint, and &lt;code&gt;EXR&lt;/code&gt; images of validation outputs. We highly recommend that you install &lt;a href=&#34;https://github.com/Tom94/tev&#34;&gt;tev&lt;/a&gt; as the default application to open EXRs. Note that the &lt;code&gt;--dataset-num-workers&lt;/code&gt; argument is used here to control the multiprocessing used to load ground truth images. To disable the multiprocessing, you can pass in &lt;code&gt;--dataset-num-workers -1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To view the logs with TensorBoard:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensorboard --logdir _results/logs/runs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Want to run the code with different options? Our configuration system makes this very easy. If you want to run with a different number of levels of details:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/nglod_nerf.yaml --dataset-path /path/to/V8 --num-lods 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Take a look at &lt;code&gt;wisp/config_parser.py&lt;/code&gt; for the list of different options you can pass in, and &lt;code&gt;configs/nglod_nerf.yaml&lt;/code&gt; for the options that are already passed in.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive training&lt;/h3&gt; &#xA;&lt;p&gt;To run the training task interactively using the renderer engine, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;WISP_HEADLESS=0 python3 app/main_interactive.py --config configs/nglod_nerf_interactive.yaml --dataset-path /path/to/V8 --dataset-num-workers 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Every config file that we ship has a &lt;code&gt;*_interactive.yaml&lt;/code&gt; counterpart that can be used for better settings (in terms of user experience) for the interactive training app. The later examples we show can all be run interactively with &lt;code&gt;WISP_HEADLESS=1 python3 app/main_interactive.py&lt;/code&gt; and the corresponding configs.&lt;/p&gt; &#xA;&lt;h3&gt;Using &lt;code&gt;wisp&lt;/code&gt; in headless mode&lt;/h3&gt; &#xA;&lt;p&gt;To disable interactive mode, and run wisp &lt;em&gt;without&lt;/em&gt; loading the graphics API, set the env variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;WISP_HEADLESS=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Toggling this flag is useful for debugging on machines without a display. This is also needed if you opt to avoid installing the interactive renderer requirements.&lt;/p&gt; &#xA;&lt;h3&gt;Training NGLOD-SDF from meshes&lt;/h3&gt; &#xA;&lt;p&gt;We also support training neural SDFs from meshes. You will first need to download a mesh. Go to this &lt;a href=&#34;https://github.com/alecjacobson/common-3d-test-models/raw/master/data/spot.obj&#34;&gt;link&lt;/a&gt; to download an OBJ file of the Spot cow.&lt;/p&gt; &#xA;&lt;p&gt;Then, run the SDF training with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/nglod_sdf.yaml --dataset-path /path/to/spot.obj&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Currently the SDF sampler we have shipped with our code can be quite slow for larger meshes. We plan to release a more optimized version of the SDF sampler soon.&lt;/p&gt; &#xA;&lt;h3&gt;Training NGP for forward facing scenes&lt;/h3&gt; &#xA;&lt;p&gt;Lastly, we also show an example of training a forward-facing scene: the &lt;code&gt;fox&lt;/code&gt; scene from &lt;code&gt;instant-ngp&lt;/code&gt;. To train a version of the &lt;a href=&#34;https://nvlabs.github.io/instant-ngp/&#34;&gt;Instant-NGP&lt;/a&gt;, first download the &lt;code&gt;fox&lt;/code&gt; dataset from the &lt;code&gt;instant-ngp&lt;/code&gt; repository somewhere. Then, run the training with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 app/main.py --config configs/ngp_nerf.yaml --multiview-dataset-format standard --mip 0 --dataset-path /path/to/fox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our code supports any &#34;standard&#34; NGP-format datasets that has been converted with the scripts from the &lt;code&gt;instant-ngp&lt;/code&gt; library. We pass in the &lt;code&gt;--multiview-dataset-format&lt;/code&gt; argument to specify the dataset type, which in this case is different from the RTMV dataset type used for the other examples.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--mip&lt;/code&gt; argument controls the amount of downscaling that happens on the images when they get loaded. This is useful for datasets with very high resolution images to prevent overload on system memory, but is usually not necessary for the fox dataset.&lt;/p&gt; &#xA;&lt;p&gt;Note that our architecture, training, and implementation details still have slight differences from the published Instant-NGP.&lt;/p&gt; &#xA;&lt;h3&gt;Configuration System&lt;/h3&gt; &#xA;&lt;p&gt;Wisp accepts configuration from both the command line interface (CLI) and a &lt;code&gt;yaml&lt;/code&gt; config file (examples in &lt;code&gt;configs&lt;/code&gt;). Whatever config file you pass in through the &lt;code&gt;--config&lt;/code&gt; option will be checked against the options in &lt;code&gt;wisp/options.py&lt;/code&gt; and serve as the &lt;em&gt;default arguments&lt;/em&gt;. This means any CLI argument you additionally pass in will overwrite the options you pass in through the &lt;code&gt;--config&lt;/code&gt;. The order of arguments does not matter.&lt;/p&gt; &#xA;&lt;p&gt;Wisp also supports hierarchical configs, by using the &lt;code&gt;parent&lt;/code&gt; argument in the config to set a parent config file path in relative path from the config location or with an absolute path. Note however that only a single level of hierarchy is allowed to keep the indirection manageable.&lt;/p&gt; &#xA;&lt;p&gt;If you get any errors from loading in config files, you likely made a typo in your field names. Check against &lt;code&gt;wisp/options.py&lt;/code&gt; as your source of truth. (Or pass in &lt;code&gt;-h&lt;/code&gt; for help).&lt;/p&gt; &#xA;&lt;h2&gt;What is &#34;wisp&#34;?&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin-wisp/main/media/wisp.jpg&#34; alt=&#34;drawing&#34; height=&#34;300&#34;&gt; &#xA;&lt;p&gt;Our library is named after the atmospheric ghost light, will-o&#39;-the-wisp, which are volumetric ghosts that are harder to model with common standard geometry representations like meshes. We provide a &lt;a href=&#34;https://drive.google.com/file/d/1jKIkqm4XhdeEQwXTqbKlZw-9dO7kJfsZ/view&#34;&gt;multiview dataset&lt;/a&gt; of the wisp as a reference dataset for a volumetric object. We also provide the &lt;a href=&#34;https://drive.google.com/drive/folders/1Via1TOsnG-3mUkkGteEoRJdEYJEx3wgf?usp=sharing&#34;&gt;blender file and rendering scripts&lt;/a&gt; if you want to generate specific data with this scene, please refer to the &lt;a href=&#34;https://drive.google.com/file/d/1IrWKjxxrJOlD3C5lDYvejaSXiPtm_XI_/view?usp=sharing&#34;&gt;readme.md&lt;/a&gt; for greater details on how to generate the data.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;We thank James Lucas, Jonathan Tremblay, Valts Blukis, Anita Hu, and Nishkrit Desai for giving us early feedback and testing out the code at various stages throughout development. We thank Rogelio Olguin and Jonathan Tremblay for the Wisp reference data.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ifzhang/FairMOT</title>
    <updated>2022-08-06T01:32:16Z</updated>
    <id>tag:github.com,2022-08-06:/ifzhang/FairMOT</id>
    <link href="https://github.com/ifzhang/FairMOT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[IJCV-2021] FairMOT: On the Fairness of Detection and Re-Identification in Multi-Object Tracking&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FairMOT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-2dmot15-1?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-2dmot15-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot16?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-mot16&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot17?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-mot17&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=a-simple-baseline-for-multi-object-tracking&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-simple-baseline-for-multi-object-tracking/multi-object-tracking-on-mot20-1&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A simple baseline for one-shot multi-object tracking: &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/pipeline.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://arxiv.org/abs/2004.01888&#34;&gt;&lt;strong&gt;FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking&lt;/strong&gt;&lt;/a&gt;,&lt;br&gt; Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu,&lt;br&gt; &lt;em&gt;IJCV2021 (&lt;a href=&#34;http://arxiv.org/abs/2004.01888&#34;&gt;arXiv 2004.01888&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problems. It remarkably outperforms the state-of-the-arts on the MOT challenge datasets at 30 FPS. We hope this baseline could inspire and help evaluate new ideas in this field.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2021.08.03) Our paper is accepted by IJCV!&lt;/li&gt; &#xA; &lt;li&gt;(2021.06.01) A &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.1/configs/mot&#34;&gt;nice re-implementation&lt;/a&gt; by Baidu &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleDetection&#34;&gt;PaddleDetection&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;(2021.05.24) A light version of FairMOT using yolov5s backbone is released!&lt;/li&gt; &#xA; &lt;li&gt;(2020.09.10) A new version of FairMOT is released! (73.7 MOTA on MOT17)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Main updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We pretrain FairMOT on the CrowdHuman dataset using a weakly-supervised learning approach.&lt;/li&gt; &#xA; &lt;li&gt;To detect bounding boxes outside the image, we use left, top, right and bottom (4 channel) to replace the WH head (2 channel).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tracking performance&lt;/h2&gt; &#xA;&lt;h3&gt;Results on MOT challenge test set&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDS&lt;/th&gt; &#xA;   &lt;th&gt;MT&lt;/th&gt; &#xA;   &lt;th&gt;ML&lt;/th&gt; &#xA;   &lt;th&gt;FPS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2DMOT15&lt;/td&gt; &#xA;   &lt;td&gt;60.6&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;591&lt;/td&gt; &#xA;   &lt;td&gt;47.6%&lt;/td&gt; &#xA;   &lt;td&gt;11.0%&lt;/td&gt; &#xA;   &lt;td&gt;30.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT16&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;72.8&lt;/td&gt; &#xA;   &lt;td&gt;1074&lt;/td&gt; &#xA;   &lt;td&gt;44.7%&lt;/td&gt; &#xA;   &lt;td&gt;15.9%&lt;/td&gt; &#xA;   &lt;td&gt;25.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT17&lt;/td&gt; &#xA;   &lt;td&gt;73.7&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;3303&lt;/td&gt; &#xA;   &lt;td&gt;43.2%&lt;/td&gt; &#xA;   &lt;td&gt;17.3%&lt;/td&gt; &#xA;   &lt;td&gt;25.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT20&lt;/td&gt; &#xA;   &lt;td&gt;61.8&lt;/td&gt; &#xA;   &lt;td&gt;67.3&lt;/td&gt; &#xA;   &lt;td&gt;5243&lt;/td&gt; &#xA;   &lt;td&gt;68.8%&lt;/td&gt; &#xA;   &lt;td&gt;7.6%&lt;/td&gt; &#xA;   &lt;td&gt;13.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All of the results are obtained on the &lt;a href=&#34;https://motchallenge.net&#34;&gt;MOT challenge&lt;/a&gt; evaluation server under the â€œprivate detectorâ€ protocol. We rank first among all the trackers on 2DMOT15, MOT16, MOT17 and MOT20. The tracking speed of the entire system can reach up to &lt;strong&gt;30 FPS&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Video demos on MOT challenge test set&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT15.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT16.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT17.gif&#34; width=&#34;400&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ifzhang/FairMOT/master/assets/MOT20.gif&#34; width=&#34;400&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repo, and we&#39;ll call the directory that you cloned as ${FAIRMOT_ROOT}&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies. We use python 3.8 and pytorch &amp;gt;= 1.7.0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n FairMOT&#xA;conda activate FairMOT&#xA;conda install pytorch==1.7.0 torchvision==0.8.0 cudatoolkit=10.2 -c pytorch&#xA;cd ${FAIRMOT_ROOT}&#xA;pip install cython&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use &lt;a href=&#34;https://github.com/ifzhang/DCNv2/tree/pytorch_1.7&#34;&gt;DCNv2_pytorch_1.7&lt;/a&gt; in our backbone network (pytorch_1.7 branch). Previous versions can be found in &lt;a href=&#34;https://github.com/CharlesShang/DCNv2&#34;&gt;DCNv2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone -b pytorch_1.7 https://github.com/ifzhang/DCNv2.git&#xA;cd DCNv2&#xA;./make.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In order to run the code for demos, you also need to install &lt;a href=&#34;https://www.ffmpeg.org/&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CrowdHuman&lt;/strong&gt; The CrowdHuman dataset can be downloaded from their &lt;a href=&#34;https://www.crowdhuman.org&#34;&gt;official webpage&lt;/a&gt;. After downloading, you should prepare the data in the following structure:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;crowdhuman&#xA;   |â€”â€”â€”â€”â€”â€”images&#xA;   |        â””â€”â€”â€”â€”â€”â€”train&#xA;   |        â””â€”â€”â€”â€”â€”â€”val&#xA;   â””â€”â€”â€”â€”â€”â€”labels_with_ids&#xA;   |         â””â€”â€”â€”â€”â€”â€”train(empty)&#xA;   |         â””â€”â€”â€”â€”â€”â€”val(empty)&#xA;   â””------annotation_train.odgt&#xA;   â””------annotation_val.odgt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to pretrain on CrowdHuman (we train Re-ID on CrowdHuman), you can change the paths in src/gen_labels_crowd_id.py and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python gen_labels_crowd_id.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to add CrowdHuman to the MIX dataset (we do not train Re-ID on CrowdHuman), you can change the paths in src/gen_labels_crowd_det.py and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python gen_labels_crowd_det.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;MIX&lt;/strong&gt; We use the same training data as &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT&#34;&gt;JDE&lt;/a&gt; in this part and we call it &#34;MIX&#34;. Please refer to their &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT/raw/master/DATASET_ZOO.md&#34;&gt;DATA ZOO&lt;/a&gt; to download and prepare all the training data including Caltech Pedestrian, CityPersons, CUHK-SYSU, PRW, ETHZ, MOT17 and MOT16.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2DMOT15 and MOT20&lt;/strong&gt; &lt;a href=&#34;https://motchallenge.net/data/2D_MOT_2015/&#34;&gt;2DMOT15&lt;/a&gt; and &lt;a href=&#34;https://motchallenge.net/data/MOT20/&#34;&gt;MOT20&lt;/a&gt; can be downloaded from the official webpage of MOT challenge. After downloading, you should prepare the data in the following structure:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;MOT15&#xA;   |â€”â€”â€”â€”â€”â€”images&#xA;   |        â””â€”â€”â€”â€”â€”â€”train&#xA;   |        â””â€”â€”â€”â€”â€”â€”test&#xA;   â””â€”â€”â€”â€”â€”â€”labels_with_ids&#xA;            â””â€”â€”â€”â€”â€”â€”train(empty)&#xA;MOT20&#xA;   |â€”â€”â€”â€”â€”â€”images&#xA;   |        â””â€”â€”â€”â€”â€”â€”train&#xA;   |        â””â€”â€”â€”â€”â€”â€”test&#xA;   â””â€”â€”â€”â€”â€”â€”labels_with_ids&#xA;            â””â€”â€”â€”â€”â€”â€”train(empty)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can change the seq_root and label_root in src/gen_labels_15.py and src/gen_labels_20.py and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python gen_labels_15.py&#xA;python gen_labels_20.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to generate the labels of 2DMOT15 and MOT20. The seqinfo.ini files of 2DMOT15 can be downloaded here &lt;a href=&#34;https://drive.google.com/open?id=1kJYySZy7wyETH4fKMzgJrYUrTfxKlN1w&#34;&gt;[Google]&lt;/a&gt;, &lt;a href=&#34;https://pan.baidu.com/s/1zb5tBW7-YTzWOXpd9IzS0g&#34;&gt;[Baidu],code:8o0w&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained models and baseline model&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pretrained models&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;DLA-34 COCO pretrained model: &lt;a href=&#34;https://drive.google.com/file/d/1pl_-ael8wERdUREEnaIfqOV_VF2bEVRT/view&#34;&gt;DLA-34 official&lt;/a&gt;. HRNetV2 ImageNet pretrained model: &lt;a href=&#34;https://1drv.ms/u/s!Aus8VCZ_C_33cMkPimlmClRvmpw&#34;&gt;HRNetV2-W18 official&lt;/a&gt;, &lt;a href=&#34;https://1drv.ms/u/s!Aus8VCZ_C_33dYBMemi9xOUFR0w&#34;&gt;HRNetV2-W32 official&lt;/a&gt;. After downloading, you should put the pretrained models in the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${FAIRMOT_ROOT}&#xA;   â””â€”â€”â€”â€”â€”â€”models&#xA;           â””â€”â€”â€”â€”â€”â€”ctdet_coco_dla_2x.pth&#xA;           â””â€”â€”â€”â€”â€”â€”hrnetv2_w32_imagenet_pretrained.pth&#xA;           â””â€”â€”â€”â€”â€”â€”hrnetv2_w18_imagenet_pretrained.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Baseline model&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our baseline FairMOT model (DLA-34 backbone) is pretrained on the CrowdHuman for 60 epochs with the self-supervised learning approach and then trained on the MIX dataset for 30 epochs. The models can be downloaded here: crowdhuman_dla34.pth &lt;a href=&#34;https://drive.google.com/file/d/1SFOhg_vos_xSYHLMTDGFVZBYjo8cr2fG/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1JZMCVDyQnQCa5veO73YaMw&#34;&gt;[Baidu, code:ggzx ]&lt;/a&gt; &lt;a href=&#34;https://microsoftapc-my.sharepoint.com/:u:/g/personal/v-yifzha_microsoft_com/EUsj0hkTNuhKkj9bo9kE7ZsBpmHvqDz6DylPQPhm94Y08w?e=3OF4XN&#34;&gt;[Onedrive]&lt;/a&gt;. fairmot_dla34.pth &lt;a href=&#34;https://drive.google.com/file/d/1iqRQjsG9BawIl8SlFomMg5iwkb6nqSpi/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1H1Zp8wrTKDk20_DSPAeEkg&#34;&gt;[Baidu, code:uouv]&lt;/a&gt; &lt;a href=&#34;https://microsoftapc-my.sharepoint.com/:u:/g/personal/v-yifzha_microsoft_com/EWHN_RQA08BDoEce_qFW-ogBNUsb0jnxG3pNS3DJ7I8NmQ?e=p0Pul1&#34;&gt;[Onedrive]&lt;/a&gt;. (This is the model we get 73.7 MOTA on the MOT17 test set. ) After downloading, you should put the baseline model in the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;${FAIRMOT_ROOT}&#xA;   â””â€”â€”â€”â€”â€”â€”models&#xA;           â””â€”â€”â€”â€”â€”â€”fairmot_dla34.pth&#xA;           â””â€”â€”â€”â€”â€”â€”...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the training data&lt;/li&gt; &#xA; &lt;li&gt;Change the dataset root directory &#39;root&#39; in src/lib/cfg/data.json and &#39;data_dir&#39; in src/lib/opts.py&lt;/li&gt; &#xA; &lt;li&gt;Pretrain on CrowdHuman and train on MIX:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/crowdhuman_dla34.sh&#xA;sh experiments/mix_ft_ch_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Only train on MIX:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mix_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Only train on MOT17:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mot17_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finetune on 2DMOT15 using the baseline model:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mot15_ft_mix_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Train on MOT20: The data annotation of MOT20 is a little different from MOT17, the coordinates of the bounding boxes are all inside the image, so we need to uncomment line 313 to 316 in the dataset file src/lib/datasets/dataset/jde.py:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;#np.clip(xy[:, 0], 0, width, out=xy[:, 0])&#xA;#np.clip(xy[:, 2], 0, width, out=xy[:, 2])&#xA;#np.clip(xy[:, 1], 0, height, out=xy[:, 1])&#xA;#np.clip(xy[:, 3], 0, height, out=xy[:, 3])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, we can train on the mix dataset and finetune on MOT20:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/crowdhuman_dla34.sh&#xA;sh experiments/mix_ft_ch_dla34.sh&#xA;sh experiments/mot20_ft_mix_dla34.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The MOT20 model &#39;mot20_fairmot.pth&#39; can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1HVzDTrYSSZiVqExqG9rou3zZXX1-GGQn/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1bpMtu972ZszsBx4TzIT_CA&#34;&gt;[Baidu, code:jmce]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For ablation study, we use MIX and half of MOT17 as training data, you can use different backbones such as ResNet, ResNet-FPN, HRNet and DLA::&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/mix_mot17_half_dla34.sh&#xA;sh experiments/mix_mot17_half_hrnet18.sh&#xA;sh experiments/mix_mot17_half_res34.sh&#xA;sh experiments/mix_mot17_half_res34fpn.sh&#xA;sh experiments/mix_mot17_half_res50.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The ablation study model &#39;mix_mot17_half_dla34.pth&#39; can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1dJDGSa6-FMq33XY-cOd_nYxuilv30YDM/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://microsoftapc-my.sharepoint.com/:u:/g/personal/v-yifzha_microsoft_com/ESh1SlUvZudKgUX4A8E3yksBhfRHIf2AsKaaPJ-v_5lVAw?e=NB6UHR&#34;&gt;[Onedrive]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1RQD8ik1labWuwd8jJ-0ukQ&#34;&gt;[Baidu, code:iifa]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Performance on the test set of MOT17 when using different training data:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training Data&lt;/th&gt; &#xA;   &lt;th&gt;MOTA&lt;/th&gt; &#xA;   &lt;th&gt;IDF1&lt;/th&gt; &#xA;   &lt;th&gt;IDS&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MOT17&lt;/td&gt; &#xA;   &lt;td&gt;69.8&lt;/td&gt; &#xA;   &lt;td&gt;69.9&lt;/td&gt; &#xA;   &lt;td&gt;3996&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MIX&lt;/td&gt; &#xA;   &lt;td&gt;72.9&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;3345&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CrowdHuman + MIX&lt;/td&gt; &#xA;   &lt;td&gt;73.7&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;3303&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use CrowdHuman, MIX and MOT17 to train the light version of FairMOT using yolov5s as backbone:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh experiments/all_yolov5s.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The pretrained model of yolov5s on the COCO dataset can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1Ur3_pa9r3KRY-5qM2cdFhFJ5exghRJvh/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1JHjN_l1nkMnRHRF5TcHYXg&#34;&gt;[Baidu, code:wh9h]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The model of the light version &#39;fairmot_yolov5s&#39; can be downloaded here: &lt;a href=&#34;https://drive.google.com/file/d/1MEvsRPyoAqYSCdKaS5Ofrl7ZfKbBZ1Jb/view?usp=sharing&#34;&gt;[Google]&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1dyBEeiGpRfZhqae0c264rg&#34;&gt;[Baidu, code:2y3a]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tracking&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The default settings run tracking on the validation dataset from 2DMOT15. Using the baseline model, you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --load_model ../models/fairmot_dla34.pth --conf_thres 0.6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to see the tracking results (76.5 MOTA and 79.3 IDF1 using the baseline model). You can also set save_images=True in src/track.py to save the visualization results of each frame.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For ablation study, we evaluate on the other half of the training set of MOT17, you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track_half.py mot --load_model ../exp/mot/mix_mot17_half_dla34.pth --conf_thres 0.4 --val_mot17 True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use our pretrained model &#39;mix_mot17_half_dla34.pth&#39;, you can get 69.1 MOTA and 72.8 IDF1.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To get the txt results of the test set of MOT16 or MOT17, you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --test_mot17 True --load_model ../models/fairmot_dla34.pth --conf_thres 0.4&#xA;python track.py mot --test_mot16 True --load_model ../models/fairmot_dla34.pth --conf_thres 0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run tracking using the light version of FairMOT (68.5 MOTA on the test of MOT17), you can run:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --test_mot17 True --load_model ../models/fairmot_yolov5s.pth --conf_thres 0.4 --arch yolo --reid_dim 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and send the txt files to the &lt;a href=&#34;https://motchallenge.net&#34;&gt;MOT challenge&lt;/a&gt; evaluation server to get the results. (You can get the SOTA results 73+ MOTA on MOT17 test set using the baseline model &#39;fairmot_dla34.pth&#39;.)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To get the SOTA results of 2DMOT15 and MOT20, run the tracking code:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python track.py mot --test_mot15 True --load_model your_mot15_model.pth --conf_thres 0.3&#xA;python track.py mot --test_mot20 True --load_model your_mot20_model.pth --conf_thres 0.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Results of the test set all need to be evaluated on the MOT challenge server. You can see the tracking results on the training set by setting --val_motxx True and run the tracking code. We set &#39;conf_thres&#39; 0.4 for MOT16 and MOT17. We set &#39;conf_thres&#39; 0.3 for 2DMOT15 and MOT20.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src&#xA;python demo.py mot --load_model ../models/fairmot_dla34.pth --conf_thres 0.4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can change --input-video and --output-root to get the demos of your own videos. --conf_thres can be set from 0.3 to 0.7 depending on your own videos.&lt;/p&gt; &#xA;&lt;h2&gt;Train on custom dataset&lt;/h2&gt; &#xA;&lt;p&gt;You can train FairMOT on custom dataset by following several steps bellow:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate one txt label file for one image. Each line of the txt label file represents one object. The format of the line is: &#34;class id x_center/img_width y_center/img_height w/img_width h/img_height&#34;. You can modify src/gen_labels_16.py to generate label files for your custom dataset.&lt;/li&gt; &#xA; &lt;li&gt;Generate files containing image paths. The example files are in src/data/. Some similar code can be found in src/gen_labels_crowd.py&lt;/li&gt; &#xA; &lt;li&gt;Create a json file for your custom dataset in src/lib/cfg/. You need to specify the &#34;root&#34; and &#34;train&#34; keys in the json file. You can find some examples in src/lib/cfg/.&lt;/li&gt; &#xA; &lt;li&gt;Add --data_cfg &#39;../src/lib/cfg/your_dataset.json&#39; when training.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;A large part of the code is borrowed from &lt;a href=&#34;https://github.com/Zhongdao/Towards-Realtime-MOT&#34;&gt;Zhongdao/Towards-Realtime-MOT&lt;/a&gt; and &lt;a href=&#34;https://github.com/xingyizhou/CenterNet&#34;&gt;xingyizhou/CenterNet&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zhang2021fairmot,&#xA;  title={Fairmot: On the fairness of detection and re-identification in multiple object tracking},&#xA;  author={Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},&#xA;  journal={International Journal of Computer Vision},&#xA;  volume={129},&#xA;  pages={3069--3087},&#xA;  year={2021},&#xA;  publisher={Springer}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tr0uble-mAker/POC-bomber</title>
    <updated>2022-08-06T01:32:16Z</updated>
    <id>tag:github.com,2022-08-06:/tr0uble-mAker/POC-bomber</id>
    <link href="https://github.com/tr0uble-mAker/POC-bomber" rel="alternate"></link>
    <summary type="html">&lt;p&gt;åˆ©ç”¨å¤§é‡é«˜å¨èƒpoc/expå¿«é€Ÿè·å–ç›®æ ‡æƒé™ï¼Œç”¨äºæ¸—é€å’Œçº¢é˜Ÿå¿«é€Ÿæ‰“ç‚¹&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ”¥ POC-bomber&lt;/h1&gt; &#xA;&lt;p&gt;ğŸ¦„ &lt;strong&gt;POC bomber æ˜¯ä¸€æ¬¾æ¼æ´æ£€æµ‹/åˆ©ç”¨å·¥å…·ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§é‡é«˜å±å®³æ¼æ´çš„POC/EXPå¿«é€Ÿè·å–ç›®æ ‡æœåŠ¡å™¨æƒé™&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æ”¶é›†äº’è”ç½‘å„ç§å±å®³æ€§å¤§çš„ RCE Â· ä»»æ„æ–‡ä»¶ä¸Šä¼  Â· ååºåˆ—åŒ– Â· sqlæ³¨å…¥ ç­‰é«˜å±å®³ä¸”èƒ½å¤Ÿè·å–åˆ°æœåŠ¡å™¨æ ¸å¿ƒæƒé™çš„æ¼æ´POC/EXPï¼Œå¹¶é›†æˆåœ¨ POC bomber æ­¦å™¨åº“ä¸­ï¼Œåˆ©ç”¨å¤§é‡é«˜å±å®³POCå¯¹å•ä¸ªæˆ–å¤šä¸ªç›®æ ‡è¿›è¡Œæ¨¡ç³Šæµ‹è¯•ï¼Œä»¥æ­¤åœ¨å¤§é‡èµ„äº§ä¸­å¿«é€Ÿè·å–å‘ç°è„†å¼±æ€§ç›®æ ‡ï¼Œè·å–ç›®æ ‡æœåŠ¡å™¨æƒé™ã€‚é€‚ç”¨åœºæ™¯åŒ…æ‹¬ä½†ä¸ä»…é™äºä»¥ä¸‹:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;çº¢è“å¯¹æŠ—æˆ–hvvä¸­å¸®åŠ©çº¢é˜Ÿåœ¨å¤§é‡èµ„äº§ä¸­å¿«é€Ÿæ‰¾åˆ°çªç ´å£è¿›å…¥å†…ç½‘&lt;/li&gt; &#xA; &lt;li&gt;å†…ç½‘å®‰å…¨æµ‹è¯•ï¼Œæ¨ªå‘ç§»åŠ¨&lt;/li&gt; &#xA; &lt;li&gt;åˆ©ç”¨æ–°å‹0dayå¯¹ä¼ä¸šèµ„äº§è¿›è¡Œæ‰¹é‡è¯„ä¼°&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ“ ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;POC bomber çš„pocæ”¯æŒweblogicï¼Œtomcatï¼Œapacheï¼Œjbossï¼Œnginxï¼Œstruct2ï¼Œthinkphp2x3x5xï¼Œspringï¼Œredisï¼Œjenkinsï¼Œphpè¯­è¨€æ¼æ´ï¼Œshiroï¼Œæ³›å¾®OAï¼Œè‡´è¿œOAï¼Œé€šè¾¾OAç­‰æ˜“å—æ”»å‡»ç»„ä»¶çš„æ¼æ´æ£€æµ‹ï¼Œæ”¯æŒè°ƒç”¨dnslogå¹³å°æ£€æµ‹æ— å›æ˜¾çš„rce(åŒ…æ‹¬log4j2çš„æ£€æµ‹)ï¼Œæ”¯æŒå•ä¸ªç›®æ ‡æ£€æµ‹å’Œæ‰¹é‡æ£€æµ‹ï¼Œç¨‹åºé‡‡ç”¨é«˜å¹¶å‘çº¿ç¨‹æ± ï¼Œæ”¯æŒè‡ªå®šä¹‰å¯¼å…¥poc/expï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆæ¼æ´æŠ¥å‘Š&lt;br&gt; POC bomberé»˜è®¤ä½¿ç”¨éªŒè¯æ¨¡å¼è¿›è¡Œpocçš„éªŒè¯ï¼Œå¦‚è¿”å›ç»“æœä¸­attackçš„å€¼ä¸ºTrueæ—¶ï¼Œå¯ä»¥åŠ å‚æ•°(--attack)è¿›å…¥æ”»å‡»æ¨¡å¼ç›´æ¥è°ƒç”¨expè¿›è¡Œæ”»å‡»(éœ€è¦æŒ‡å®špocæ–‡ä»¶å)ï¼Œè¾¾åˆ°ä¸€é”®getshell&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ’» Screenshots&lt;/h2&gt; &#xA;&lt;h4&gt;ğŸ† éªŒè¯æ¨¡å¼&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python3 pocbomber.py -u http://xxx.xxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/71172892/168099161-f46a54f7-562b-4ba5-a751-1d65492b17d9.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/147481630-f8b94566-572f-4d89-a874-dc01f5041377.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/148684886-98b0f1ff-76f5-48d3-8d2d-932635392a33.gif&#34; alt=&#34;verifyæ¨¡è¯•æ¼”ç¤º&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;âš¡ï¸ æ”»å‡»æ¨¡å¼&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;    python3 pocbomber.py -u http://xxx.xxx --poc=&#34;thinkphp2_rce.py&#34; --attack&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/71172892/147629887-def9d18e-f6aa-466a-ab2c-2538752b82aa.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/148206720-86f77246-301c-481f-a16c-b36047f72d7c.png&#34; alt=&#34;image&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/71172892/148684097-67b59320-6758-458d-ac6b-ae219c327924.gif&#34; alt=&#34;attackæ¨¡å¼æ¼”ç¤º&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”§ å®‰è£…&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;  git clone https://github.com/tr0uble-mAker/POC-bomber.git            &#xA;  cd POC-bomber&#xA;  pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸš€ ç”¨æ³•&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;    æŸ¥çœ‹ç”¨æ³•:     python3 pocbomber.py&#xA;    &#xA;    æ¨¡å¼:&#xA;            è·å–poc/expä¿¡æ¯:   python3 pocbomber.py --show&#xA;            å•ç›®æ ‡æ£€æµ‹:        python3 pocbomber.py -u http://xxx.xxx.xx&#xA;            æ‰¹é‡æ£€æµ‹:          python3 pocbomber.py -f url.txt -o report.txt &#xA;            æŒ‡å®špocæ£€æµ‹:       python3 pocbomber.py -f url.txt --poc=&#34;thinkphp2_rce.py&#34;&#xA;            expæ”»å‡»æ¨¡å¼:       python3 pocbomber.py -u ç›®æ ‡url --poc=&#34;æŒ‡å®špocæ–‡ä»¶&#34; --attack&#xA;    å‚æ•°:&#xA;            -u  --url      ç›®æ ‡url&#xA;            -f  --file     æŒ‡å®šç›®æ ‡urlæ–‡ä»¶   &#xA;            -o  --output   æŒ‡å®šç”ŸæˆæŠ¥å‘Šçš„æ–‡ä»¶(é»˜è®¤ä¸ç”ŸæˆæŠ¥å‘Š)&#xA;            -p  --poc      æŒ‡å®šå•ä¸ªæˆ–å¤šä¸ªpocè¿›è¡Œæ£€æµ‹, ç›´æ¥ä¼ å…¥pocæ–‡ä»¶å, å¤šä¸ªpocç”¨(,)åˆ†å¼€&#xA;            -t  --thread   æŒ‡å®šçº¿ç¨‹æ± æœ€å¤§å¹¶å‘æ•°é‡(é»˜è®¤30)&#xA;            --show         å±•ç¤ºpoc/expè¯¦ç»†ä¿¡æ¯&#xA;            --attack       ä½¿ç”¨pocæ–‡ä»¶ä¸­çš„expè¿›è¡Œæ”»å‡»&#xA;            --dnslog       ä½¿ç”¨dnslogå¹³å°æ£€æµ‹æ— å›æ˜¾æ¼æ´(é»˜è®¤ä¸å¯ç”¨dnslog,å¯åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ”† é…ç½®æ–‡ä»¶&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;  /inc/config.py   &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;âš ï¸ å¸¸è§é—®é¢˜&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ç¨‹åºä¸å®‰è£…requirements.txtå°±å¯ä»¥ç›´æ¥è¿è¡Œï¼Œåªä¾èµ–requestsç¬¬ä¸‰æ–¹åº“ï¼Œå…¶ä»–åº“å®‰è£…ä¸ä¸Šä¸å½±å“ç¨‹åºè¿è¡Œï¼Œä½†æœ‰äº›pocä¼šä¸èƒ½æ£€æµ‹&lt;/li&gt; &#xA; &lt;li&gt;log4j2å‘½ä»¤æ‰§è¡Œæ¼æ´çš„æ£€æµ‹ï¼šéœ€è¦æ·»åŠ  --dnslog å‚æ•°&lt;/li&gt; &#xA; &lt;li&gt;æ— å›æ˜¾æ¼æ´æ£€æµ‹é»˜è®¤ä½¿ç”¨ dnslog.cn å¹³å°ä¸”é»˜è®¤å…³é—­, è¦å¼€å¯éœ€å‰å¾€é…ç½®æ–‡ä»¶å°† dnslog_flag å¼€å…³ç½®ä¸ºTrue&lt;/li&gt; &#xA; &lt;li&gt;éœ€è¦æŒ‡å®šä¸€ä¸ªpocæ‰èƒ½è°ƒç”¨--attackæ”»å‡»æ¨¡å¼&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ğŸ“ ç›®å½•ç»“æ„:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;  +--------- poc_bomber.py (å¯åŠ¨ POC-bomber)&#xA;  | &#xA;  +--------- inc(å­˜æ”¾æ”¯æ’‘ POC-bomber æ¡†æ¶è¿è¡Œçš„æ ¸å¿ƒæ–‡ä»¶)&#xA;  |&#xA;  \--------- pocs(POCå­˜æ”¾åˆ—è¡¨)----------- framework(å­˜æ”¾æ¡†æ¶æ¼æ´POC)&#xA;                                  |&#xA;                                  |------ middleware(å­˜æ”¾ä¸­é—´ä»¶æ¼æ´POC)&#xA;                                  |&#xA;                                  |------ ports(å­˜æ”¾å¸¸è§ç«¯å£æ¼æ´,ä¸»æœºæœåŠ¡æ¼æ´POC)&#xA;                                  |&#xA;                                   \----- webs(å­˜æ”¾å¸¸è§webé¡µé¢æ¼æ´POC)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ“– POCç¼–å†™è§„åˆ™&lt;/h2&gt; &#xA;&lt;p&gt;POC bomberæ”¯æŒè‡ªå®šä¹‰ç¼–å†™poc&lt;br&gt; pocç»Ÿä¸€è¦æ±‚python3ç¼–å†™ï¼Œå…·æœ‰verifyå’Œattack(éå¿…é¡»)ä¸¤ä¸ªå‡½æ•°åˆ†åˆ«è¿›è¡ŒéªŒè¯å’Œæ”»å‡»,&lt;/p&gt; &#xA;&lt;h4&gt;ğŸ‘» æ¼æ´éªŒè¯å‡½æ•°(verify)ç¼–å†™åº”è¯¥æ»¡è¶³ä»¥ä¸‹æ¡ä»¶:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;å‡½æ•°åä¸º verify ï¼Œ å‚æ•°æ¥æ”¶ç›®æ ‡urlçš„å‚æ•°&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å‡½æ•°çš„è¿”å›ç»“æœä»¥å­—å…¸çš„å½¢å¼è¿”å›å¹¶ä¸”å…·æœ‰nameå’Œvulnerableä¸¤ä¸ªé”®å€¼ï¼Œnameè¯´æ˜æ¼æ´åç§°ï¼Œvulnerableé€šè¿‡Trueå’ŒFalseçš„çŠ¶æ€è¡¨æ˜æ¼æ´æ˜¯å¦å­˜åœ¨&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¦‚æœå­˜åœ¨æ¼æ´è¦å°†è¿”å›å­—å…¸ä¸­vulnerableçš„å€¼ç½®ä¸ºTrue, å¹¶æ·»åŠ ç›®æ ‡url, æ¼æ´åˆ©ç”¨ç›¸å…³ç½‘é¡µç­‰ä¿¡æ¯&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç”¨tryæ–¹æ³•å°è¯•éªŒè¯ï¼Œä½¿ç”¨requestç­‰å‘é€æ•°æ®åŒ…æ—¶è¦è®¾ç½®è¶…æ—¶æ—¶é—´, é¿å…pocä¼šå¡æ­»&lt;/p&gt; &lt;pre&gt;&lt;code&gt; def verify(url):                        &#xA;     relsult = {                                            &#xA;         &#39;name&#39;: &#39;Thinkphp5 5.0.22/5.1.29 Remote Code Execution Vulnerability&#39;,                          &#xA;         &#39;vulnerable&#39;: Falseï¼Œ&#xA;         &#39;attack&#39;ï¼š Falseï¼Œ        # å¦‚æœæœ‰expæ”¯æŒattackæ¨¡å¼å°†attackçš„å€¼ç½®ä¸ºTrue&#xA;     }              &#xA;     try:                    &#xA;         ......        &#xA;         (ç”¨ä»»æ„æ–¹æ³•æ£€æµ‹æ¼æ´)             &#xA;         ......&#xA;         if å­˜åœ¨æ¼æ´:&#xA;             relsult[&#39;vulnerable&#39;] = True     # å°†relsultçš„vulnerableçš„å€¼ç½®ä¸ºTrue&#xA;             relsult[&#39;url&#39;] = url             # è¿”å›éªŒè¯çš„url&#xA;             relust[&#39;xxxxx&#39;] = &#39;xxxxx&#39;        # å¯ä»¥æ·»åŠ è¯¥æ¼æ´ç›¸å…³æ¥æºç­‰ä¿¡æ¯   &#xA;             ......           &#xA;             return relsult     # å°†vulnerableå€¼ä¸ºTrueçš„relsultè¿”å›                   &#xA;         else:  # ä¸å­˜åœ¨æ¼æ´           &#xA;             return relsult    # è‹¥ä¸å­˜åœ¨æ¼æ´å°†vulnerableå€¼ä¸ºFalseçš„relsultè¿”å›&#xA;&#xA;     execpt:&#xA;         return relsult&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;å¦‚æœæœ‰expå¯ä»¥ç¼–å†™ attack å‡½æ•°ä½œä¸ºexpæ”»å‡»å‡½æ•°ï¼Œ&lt;/p&gt; &#xA;&lt;h4&gt;ğŸƒ æ¼æ´æ”»å‡»å‡½æ•°(attack)ç¼–å†™åº”è¯¥æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;å‡½æ•°åä¸º attack ï¼Œ å‚æ•°æ¥æ”¶ç›®æ ‡urlçš„å‚æ•°&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¹¶åœ¨tryä¸­ç¼–å†™expä»£ç è¿›è¡Œæ”»å‡», å¯ä»¥ä¸ç”¨æˆ·äº¤äº’è¾“å…¥&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ç¼–å†™å®Œæˆåå°†è¯¥æ¼æ´çš„verifyå‡½æ•°è¿”å›å­—å…¸ä¸­attackå€¼ç½®ä¸ºTrue&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;æ”»å‡»æˆåŠŸåè¿”å›Trueï¼Œå…¶ä»–åŸå› å¤±è´¥çš„è¯è¿”å›Falseå³å¯&lt;/p&gt; &lt;pre&gt;&lt;code&gt; def attack(url):    &#xA;   try:            &#xA;       ........................................            &#xA;         æ”»å‡»ä»£ç (æ‰§è¡Œå‘½ä»¤æˆ–åå¼¹shellä¸Šä¼ æœ¨é©¬ç­‰)             &#xA;       ........................................&#xA;       return True&#xA;   except:               &#xA;       return False    &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ç¼–å†™å®Œæˆåçš„pocç›´æ¥æ”¾å…¥ /pocs ç›®å½•ä¸‹ä»»æ„ä½ç½®å³å¯è¢«é€’å½’è°ƒç”¨!&lt;/p&gt; &#xA;&lt;p&gt;é¡¹ç›®æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿å„ä½å¸ˆå‚…è´¡çŒ®pocå…±ç­‘ç½‘ç»œå®‰å…¨ï¼&lt;br&gt; æœ‰é—®é¢˜æ¬¢è¿issuesç•™è¨€: &lt;a href=&#34;https://github.com/tr0uble-mAker/POC-bomber/issues&#34;&gt;https://github.com/tr0uble-mAker/POC-bomber/issues&lt;/a&gt;&lt;br&gt; è”ç³»: &lt;a href=&#34;mailto:929305053@qq.com&#34;&gt;929305053@qq.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>