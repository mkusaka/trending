<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-16T01:33:08Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>andrewyng/translation-agent</title>
    <updated>2024-08-16T01:33:08Z</updated>
    <id>tag:github.com,2024-08-16:/andrewyng/translation-agent</id>
    <link href="https://github.com/andrewyng/translation-agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Translation Agent: Agentic translation using reflection workflow&lt;/h1&gt; &#xA;&lt;p&gt;This is a Python demonstration of a reflection agentic workflow for machine translation. The main steps are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prompt an LLM to translate a text from &lt;code&gt;source_language&lt;/code&gt; to &lt;code&gt;target_language&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Have the LLM reflect on the translation to come up with constructive suggestions for improving it;&lt;/li&gt; &#xA; &lt;li&gt;Use the suggestions to improve the translation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Customizability&lt;/h2&gt; &#xA;&lt;p&gt;By using an LLM as the heart of the translation engine, this system is highly steerable. For example, by changing the prompts, it is easier using this workflow than a traditional machine translation (MT) system to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modify the output&#39;s style, such as formal/informal.&lt;/li&gt; &#xA; &lt;li&gt;Specify how to handle idioms and special terms like names, technical terms, and acronyms. For example, including a glossary in the prompt lets you make sure particular terms (such as open source, H100 or GPU) are translated consistently.&lt;/li&gt; &#xA; &lt;li&gt;Specify specific regional use of the language, or specific dialects, to serve a target audience. For example, Spanish spoken in Latin America is different from Spanish spoken in Spain; French spoken in Canada is different from how it is spoken in France.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is not mature software&lt;/strong&gt;, and is the result of Andrew playing around with translations on weekends the past few months, plus collaborators (Joaquin Dominguez, Nedelina Teneva, John Santerre) helping refactor the code.&lt;/p&gt; &#xA;&lt;p&gt;According to our evaluations using BLEU score on traditional translation datasets, this workflow is sometimes competitive with, but also sometimes worse than, leading commercial offerings. However, we’ve also occasionally gotten fantastic results (superior to commercial offerings) with this approach. We think this is just a starting point for agentic translations, and that this is a promising direction for translation, with significant headroom for further improvement, which is why we’re releasing this demonstration to encourage more discussion, experimentation, research and open-source contributions.&lt;/p&gt; &#xA;&lt;p&gt;If agentic translations can generate better results than traditional architectures (such as an end-to-end transformer that inputs a text and directly outputs a translation) -- which are often faster/cheaper to run than our approach here -- this also provides a mechanism to automatically generate training data (parallel text corpora) that can be used to further train and improve traditional algorithms. (See also &lt;a href=&#34;https://www.deeplearning.ai/the-batch/building-models-that-learn-from-themselves/&#34;&gt;this article in The Batch&lt;/a&gt; on using LLMs to generate training data.)&lt;/p&gt; &#xA;&lt;p&gt;Comments and suggestions for how to improve this are very welcome!&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with &lt;code&gt;translation-agent&lt;/code&gt;, follow these steps:&lt;/p&gt; &#xA;&lt;h3&gt;Installation:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The Poetry package manager is required for installation. &lt;a href=&#34;https://python-poetry.org/docs/#installation&#34;&gt;Poetry Installation&lt;/a&gt; Depending on your environment, this might work:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install poetry&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A .env file with a OPENAI_API_KEY is required to run the workflow. See the .env.sample file as an example.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/andrewyng/translation-agent.git&#xA;cd translation-agent&#xA;poetry install&#xA;poetry shell # activates virtual environment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import translation_agent as ta&#xA;source_lang, target_lang, country = &#34;English&#34;, &#34;Spanish&#34;, &#34;Mexico&#34;&#xA;translation = ta.translate(source_lang, target_lang, source_text, country)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See examples/example_script.py for an example script to try out.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Translation Agent is released under the &lt;strong&gt;MIT License&lt;/strong&gt;. You are free to use, modify, and distribute the code for both commercial and non-commercial purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Ideas for extensions&lt;/h2&gt; &#xA;&lt;p&gt;Here are ideas we haven’t had time to experiment with but that we hope the open-source community will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Try other LLMs.&lt;/strong&gt; We prototyped this primarily using gpt-4-turbo. We would love for others to experiment with other LLMs as well as other hyperparameter choices and see if some do better than others for particular language pairs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Glossary Creation.&lt;/strong&gt; What’s the best way to efficiently build a glossary -- perhaps using an LLM -- of the most important terms that we want translated consistently? For example, many businesses use specialized terms that are not widely used on the internet and that LLMs thus don’t know about, and there are also many terms that can be translated in multiple ways. For example, ”open source” in Spanish can be “Código abierto” or “Fuente abierta”; both are fine, but it’d better to pick one and stick with it for a single document.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Glossary Usage and Implementation.&lt;/strong&gt; Given a glossary, what’s the best way to include it in the prompt?&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluations on different languages.&lt;/strong&gt; How does its performance vary in different languages? Are there changes that make it work better for particular source or target languages? (Note that for very high levels of performance, which MT systems are approaching, we’re not sure if BLEU is a great metric.) Also, its performance on lower resource languages needs further study.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Error analysis.&lt;/strong&gt; We’ve found that specifying a language and a country/region (e.g., “Spanish as colloquially spoken in Mexico”) does a pretty good job for our applications. Where does the current approach fall short? We’re also particularly interested in understanding its performance on specialized topics (like law, medicine) or special types of text (like movie subtitles) to understand its limitations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Better evals.&lt;/strong&gt; Finally, we think better evaluations (evals) is a huge and important research topic. As with other LLM applications that generate free text, current evaluation metrics appear to fall short. For example, we found that even on documents where our agentic workflow captures context and terminology better, resulting in translations that our human raters prefer over current commercial offerings, evaluation at the sentence level (using the &lt;a href=&#34;https://github.com/facebookresearch/flores&#34;&gt;FLORES&lt;/a&gt; dataset) resulted in the agentic system scoring lower on BLEU. Can we design better metrics (perhaps using an LLM to evaluate translations?) that capture translation quality at a document level that correlates better with human preferences?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Related work&lt;/h2&gt; &#xA;&lt;p&gt;A few academic research groups are also starting to look at LLM-based and agentic translation. We think it’s early days for this field!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;ChatGPT MT: Competitive for High- (but not Low-) Resource Languages&lt;/em&gt;, Robinson et al. (2023), &lt;a href=&#34;https://arxiv.org/pdf/2309.07423&#34;&gt;https://arxiv.org/pdf/2309.07423&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;How to Design Translation Prompts for ChatGPT: An Empirical Study&lt;/em&gt;, Gao et al. (2023), &lt;a href=&#34;https://arxiv.org/pdf/2304.02182v2&#34;&gt;https://arxiv.org/pdf/2304.02182v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts&lt;/em&gt;, Wu et al. (2024), &lt;a href=&#34;https://arxiv.org/pdf/2405.11804&#34;&gt;https://arxiv.org/pdf/2405.11804&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>pyodide/pyodide</title>
    <updated>2024-08-16T01:33:08Z</updated>
    <id>tag:github.com,2024-08-16:/pyodide/pyodide</id>
    <link href="https://github.com/pyodide/pyodide" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pyodide is a Python distribution for the browser and Node.js based on WebAssembly&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/pyodide/pyodide&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pyodide/pyodide/main/docs/_static/img/pyodide-logo-readme.png&#34; alt=&#34;Pyodide&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/pyodide&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/pyodide&#34; alt=&#34;NPM Latest Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pyodide-py/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pyodide-py.svg?sanitize=true&#34; alt=&#34;PyPI Latest Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/pyodide/pyodide&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/pyodide/pyodide.png&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pyodide.readthedocs.io/?badge=stable&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pyodide/badge/?version=stable&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pyodide is a Python distribution for the browser and Node.js based on WebAssembly.&lt;/p&gt; &#xA;&lt;h2&gt;What is Pyodide?&lt;/h2&gt; &#xA;&lt;p&gt;Pyodide is a port of CPython to WebAssembly/&lt;a href=&#34;https://emscripten.org/&#34;&gt;Emscripten&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Pyodide makes it possible to install and run Python packages in the browser with &lt;a href=&#34;https://micropip.pyodide.org/&#34;&gt;micropip&lt;/a&gt;. Any pure Python package with a wheel available on PyPi is supported. Many packages with C extensions have also been ported for use with Pyodide. These include many general-purpose packages such as regex, PyYAML, lxml and scientific Python packages including NumPy, pandas, SciPy, Matplotlib, and scikit-learn.&lt;/p&gt; &#xA;&lt;p&gt;Pyodide comes with a robust Javascript ⟺ Python foreign function interface so that you can freely mix these two languages in your code with minimal friction. This includes full support for error handling, async/await, and much more.&lt;/p&gt; &#xA;&lt;p&gt;When used inside a browser, Python has full access to the Web APIs.&lt;/p&gt; &#xA;&lt;h2&gt;Try Pyodide (no installation needed)&lt;/h2&gt; &#xA;&lt;p&gt;Try Pyodide in a &lt;a href=&#34;https://pyodide.org/en/stable/console.html&#34;&gt;REPL&lt;/a&gt; directly in your browser. For further information, see the &lt;a href=&#34;https://pyodide.org/en/stable/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Pyodide offers three different ways to get started depending on your needs and technical resources. These include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use a hosted distribution of Pyodide: see the &lt;a href=&#34;https://pyodide.org/en/stable/usage/quickstart.html&#34;&gt;Getting Started&lt;/a&gt; documentation.&lt;/li&gt; &#xA; &lt;li&gt;Download a version of Pyodide from the &lt;a href=&#34;https://github.com/pyodide/pyodide/releases/&#34;&gt;releases page&lt;/a&gt; and serve it with a web server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pyodide.org/en/stable/development/building-from-sources.html&#34;&gt;Build Pyodide from source&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Build natively with &lt;code&gt;make&lt;/code&gt;: primarily for Linux users who want to experiment or contribute back to the project.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://pyodide.org/en/stable/development/building-from-sources.html#using-docker&#34;&gt;Use a Docker image&lt;/a&gt;: recommended for Windows and macOS users and for Linux users who prefer a Debian-based Docker image with the dependencies already installed.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;History&lt;/h2&gt; &#xA;&lt;p&gt;Pyodide was created in 2018 by &lt;a href=&#34;https://github.com/mdboom&#34;&gt;Michael Droettboom&lt;/a&gt; at Mozilla as part of the &lt;a href=&#34;https://github.com/iodide-project/iodide&#34;&gt;Iodide project&lt;/a&gt;. Iodide is an experimental web-based notebook environment for literate scientific computing and communication.&lt;/p&gt; &#xA;&lt;p&gt;Iodide is no longer maintained. If you want to use Pyodide in an interactive client-side notebook, see &lt;a href=&#34;https://pyodide.org/en/stable/project/related-projects.html#notebook-environments-ides-repls&#34;&gt;Pyodide notebook environments&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please view the &lt;a href=&#34;https://pyodide.org/en/stable/development/contributing.html&#34;&gt;contributing guide&lt;/a&gt; for tips on filing issues, making changes, and submitting pull requests. Pyodide is an independent and community-driven open-source project. The decision-making process is outlined in the &lt;a href=&#34;https://pyodide.org/en/stable/project/governance.html&#34;&gt;Project governance&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Blog: &lt;a href=&#34;https://blog.pyodide.org/&#34;&gt;blog.pyodide.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mailing list: &lt;a href=&#34;https://mail.python.org/mailman3/lists/pyodide.python.org/&#34;&gt;mail.python.org/mailman3/lists/pyodide.python.org/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/pyodide&#34;&gt;twitter.com/pyodide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stack Overflow: &lt;a href=&#34;https://stackoverflow.com/questions/tagged/pyodide&#34;&gt;stackoverflow.com/questions/tagged/pyodide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord: &lt;a href=&#34;https://discord.gg/cRxMCG5kJQ&#34;&gt;Pyodide Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Pyodide uses the &lt;a href=&#34;https://choosealicense.com/licenses/mpl-2.0/&#34;&gt;Mozilla Public License Version 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>speechbrain/speechbrain</title>
    <updated>2024-08-16T01:33:08Z</updated>
    <id>tag:github.com,2024-08-16:/speechbrain/speechbrain</id>
    <link href="https://github.com/speechbrain/speechbrain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A PyTorch-based Speech Toolkit&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/speechbrain/speechbrain/develop/docs/images/speechbrain-logo.svg?sanitize=true&#34; alt=&#34;SpeechBrain Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://git.io/typing-svg&#34;&gt;&lt;img src=&#34;https://readme-typing-svg.demolab.com?font=Fira+Code&amp;amp;size=40&amp;amp;duration=7000&amp;amp;pause=1000&amp;amp;random=false&amp;amp;width=1200&amp;amp;height=100&amp;amp;lines=Simplify+Conversational+AI+Development&#34; alt=&#34;Typing SVG&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;| 📘 &lt;a href=&#34;https://speechbrain.github.io/tutorial_basics.html&#34;&gt;Tutorials&lt;/a&gt; | 🌐 &lt;a href=&#34;https://speechbrain.github.io/&#34;&gt;Website&lt;/a&gt; | 📚 &lt;a href=&#34;https://speechbrain.readthedocs.io/en/latest/index.html&#34;&gt;Documentation&lt;/a&gt; | 🤝 &lt;a href=&#34;https://speechbrain.readthedocs.io/en/latest/contributing.html&#34;&gt;Contributing&lt;/a&gt; | 🤗 &lt;a href=&#34;https://huggingface.co/speechbrain&#34;&gt;HuggingFace&lt;/a&gt; | ▶️ &lt;a href=&#34;https://www.youtube.com/@SpeechBrainProject&#34;&gt;YouTube&lt;/a&gt; | 🐦 &lt;a href=&#34;https://twitter.com/SpeechBrain1&#34;&gt;X&lt;/a&gt; |&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/speechbrain/speechbrain?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;em&gt;Please, help our community project. Star on GitHub!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exciting News (January, 2024):&lt;/strong&gt; Discover what is new in SpeechBrain 1.0 &lt;a href=&#34;https://colab.research.google.com/drive/1IEPfKRuvJRSjoxu22GZhb3czfVHsAy0s?usp=sharing&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;🗣️💬 What SpeechBrain Offers&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;SpeechBrain is an &lt;strong&gt;open-source&lt;/strong&gt; &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; toolkit that accelerates &lt;strong&gt;Conversational AI&lt;/strong&gt; development, i.e., the technology behind &lt;em&gt;speech assistants&lt;/em&gt;, &lt;em&gt;chatbots&lt;/em&gt;, and &lt;em&gt;large language models&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It is crafted for fast and easy creation of advanced technologies for &lt;strong&gt;Speech&lt;/strong&gt; and &lt;strong&gt;Text&lt;/strong&gt; Processing.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🌐 Vision&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;With the rise of &lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;deep learning&lt;/a&gt;, once-distant domains like speech processing and NLP are now very close. A well-designed neural network and large datasets are all you need.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We think it is now time for a &lt;strong&gt;holistic toolkit&lt;/strong&gt; that, mimicking the human brain, jointly supports diverse technologies for complex Conversational AI systems.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This spans &lt;em&gt;speech recognition&lt;/em&gt;, &lt;em&gt;speaker recognition&lt;/em&gt;, &lt;em&gt;speech enhancement&lt;/em&gt;, &lt;em&gt;speech separation&lt;/em&gt;, &lt;em&gt;language modeling&lt;/em&gt;, &lt;em&gt;dialogue&lt;/em&gt;, and beyond.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📚 Training Recipes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We share over 200 competitive training &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes&#34;&gt;recipes&lt;/a&gt; on more than 40 datasets supporting 20 speech and text processing tasks (see below).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We support both training from scratch and fine-tuning pretrained models such as &lt;a href=&#34;https://huggingface.co/openai/whisper-large&#34;&gt;Whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/wav2vec2&#34;&gt;Wav2Vec2&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/wavlm&#34;&gt;WavLM&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/hubert&#34;&gt;Hubert&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/gpt2&#34;&gt;GPT2&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/llama2&#34;&gt;Llama2&lt;/a&gt;, and beyond. The models on &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace&lt;/a&gt; can be easily plugged in and fine-tuned.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For any task, you train the model using these commands:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python train.py hparams/train.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The hyperparameters are encapsulated in a YAML file, while the training process is orchestrated through a Python script.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We maintained a consistent code structure across different tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For better replicability, training logs and checkpoints are hosted on Dropbox.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://huggingface.co/speechbrain&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://huggingface.co/front/assets/huggingface_logo.svg?sanitize=true&#34; alt=&#34;drawing&#34; width=&#34;40&#34;&gt; &lt;/a&gt; Pretrained Models and Inference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Access over 100 pretrained models hosted on &lt;a href=&#34;https://huggingface.co/speechbrain&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Each model comes with a user-friendly interface for seamless inference. For example, transcribing speech using a pretrained model requires just three lines of code:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from speechbrain.inference import EncoderDecoderASR&#xA;&#xA;asr_model = EncoderDecoderASR.from_hparams(source=&#34;speechbrain/asr-conformer-transformerlm-librispeech&#34;, savedir=&#34;pretrained_models/asr-transformer-transformerlm-librispeech&#34;)&#xA;asr_model.transcribe_file(&#34;speechbrain/asr-conformer-transformerlm-librispeech/example.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://speechbrain.github.io/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/1200px-Google_Colaboratory_SVG_Logo.svg.png&#34; alt=&#34;drawing&#34; width=&#34;50&#34;&gt; &lt;/a&gt; Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We are deeply dedicated to promoting inclusivity and education.&lt;/li&gt; &#xA; &lt;li&gt;We have authored over 30 &lt;a href=&#34;https://speechbrain.github.io/&#34;&gt;tutorials&lt;/a&gt; on Google Colab that not only describe how SpeechBrain works but also help users familiarize themselves with Conversational AI.&lt;/li&gt; &#xA; &lt;li&gt;Every class or function has clear explanations and examples that you can run. Check out the &lt;a href=&#34;https://speechbrain.readthedocs.io/en/latest/index.html&#34;&gt;documentation&lt;/a&gt; for more details 📚.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎯 Use Cases&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🚀 &lt;strong&gt;Research Acceleration&lt;/strong&gt;: Speeding up academic and industrial research. You can develop and integrate new models effortlessly, comparing their performance against our baselines.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;⚡️ &lt;strong&gt;Rapid Prototyping&lt;/strong&gt;: Ideal for quick prototyping in time-sensitive projects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🎓 &lt;strong&gt;Educational Tool&lt;/strong&gt;: SpeechBrain&#39;s simplicity makes it a valuable educational resource. It is used by institutions like &lt;a href=&#34;https://mila.quebec/en/&#34;&gt;Mila&lt;/a&gt;, &lt;a href=&#34;https://www.concordia.ca/&#34;&gt;Concordia University&lt;/a&gt;, &lt;a href=&#34;https://univ-avignon.fr/en/&#34;&gt;Avignon University&lt;/a&gt;, and many others for student training.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;🚀 Quick Start&lt;/h1&gt; &#xA;&lt;p&gt;To get started with SpeechBrain, follow these simple steps:&lt;/p&gt; &#xA;&lt;h2&gt;🛠️ Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install via PyPI&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install SpeechBrain using PyPI:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install speechbrain&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Access SpeechBrain in your Python code:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import speechbrain as sb&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Install from GitHub&lt;/h3&gt; &#xA;&lt;p&gt;This installation is recommended for users who wish to conduct experiments and customize the toolkit according to their needs.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the GitHub repository and install the requirements:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/speechbrain/speechbrain.git&#xA;cd speechbrain&#xA;pip install -r requirements.txt&#xA;pip install --editable .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Access SpeechBrain in your Python code:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import speechbrain as sb&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Any modifications made to the &lt;code&gt;speechbrain&lt;/code&gt; package will be automatically reflected, thanks to the &lt;code&gt;--editable&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;h2&gt;✔️ Test Installation&lt;/h2&gt; &#xA;&lt;p&gt;Ensure your installation is correct by running the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pytest tests&#xA;pytest --doctest-modules speechbrain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🏃‍♂️ Running an Experiment&lt;/h2&gt; &#xA;&lt;p&gt;In SpeechBrain, you can train a model for any task using the following steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cd recipes/&amp;lt;dataset&amp;gt;/&amp;lt;task&amp;gt;/&#xA;python experiment.py params.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results will be saved in the &lt;code&gt;output_folder&lt;/code&gt; specified in the YAML file.&lt;/p&gt; &#xA;&lt;h2&gt;📘 Learning SpeechBrain&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Website:&lt;/strong&gt; Explore general information on the &lt;a href=&#34;https://speechbrain.github.io&#34;&gt;official website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tutorials:&lt;/strong&gt; Start with &lt;a href=&#34;https://speechbrain.github.io/tutorial_basics.html&#34;&gt;basic tutorials&lt;/a&gt; covering fundamental functionalities. Find advanced tutorials and topics in the Tutorials menu on the &lt;a href=&#34;https://speechbrain.github.io&#34;&gt;SpeechBrain website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; Detailed information on the SpeechBrain API, contribution guidelines, and code is available in the &lt;a href=&#34;https://speechbrain.readthedocs.io/en/latest/index.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;🔧 Supported Technologies&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SpeechBrain is a versatile framework designed for implementing a wide range of technologies within the field of Conversational AI.&lt;/li&gt; &#xA; &lt;li&gt;It excels not only in individual task implementations but also in combining various technologies into complex pipelines.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎙️ Speech/Audio Processing&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tasks&lt;/th&gt; &#xA;   &lt;th&gt;Datasets&lt;/th&gt; &#xA;   &lt;th&gt;Technologies/Models&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/AISHELL-1&#34;&gt;AISHELL-1&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice&#34;&gt;CommonVoice&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/DVoice&#34;&gt;DVoice&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/KsponSpeech&#34;&gt;KsponSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech&#34;&gt;LibriSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/MEDIA&#34;&gt;MEDIA&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/RescueSpeech&#34;&gt;RescueSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/Switchboard&#34;&gt;Switchboard&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/TIMIT&#34;&gt;TIMIT&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/Tedlium2&#34;&gt;Tedlium2&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/Voicebank&#34;&gt;Voicebank&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~graves/icml_2006.pdf&#34;&gt;CTC&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1211.3711.pdf?origin=publication_detail&#34;&gt;Transducers&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformers&lt;/a&gt;, &lt;a href=&#34;http://zhaoshuaijiang.com/file/Hybrid_CTC_Attention_Architecture_for_End-to-End_Speech_Recognition.pdf&#34;&gt;Seq2Seq&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1911.01629.pdf&#34;&gt;Beamsearch techniques for CTC&lt;/a&gt;,&lt;a href=&#34;https://arxiv.org/abs/1904.02619.pdf&#34;&gt;seq2seq&lt;/a&gt;,&lt;a href=&#34;https://www.merl.com/publications/docs/TR2017-190.pdf&#34;&gt;transducers&lt;/a&gt;), &lt;a href=&#34;https://arxiv.org/pdf/1612.02695.pdf&#34;&gt;Rescoring&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2005.08100&#34;&gt;Conformer&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2207.02971&#34;&gt;Branchformer&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2305.18281&#34;&gt;Hyperconformer&lt;/a&gt;, &lt;a href=&#34;https://github.com/k2-fsa/k2&#34;&gt;Kaldi2-FST&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speaker Recognition&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/VoxCeleb&#34;&gt;VoxCeleb&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.07143&#34;&gt;ECAPA-TDNN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1910.12592.pdf&#34;&gt;ResNET&lt;/a&gt;, &lt;a href=&#34;https://www.danielpovey.com/files/2018_icassp_xvectors.pdf&#34;&gt;Xvectors&lt;/a&gt;, &lt;a href=&#34;https://ieeexplore.ieee.org/document/6639151&#34;&gt;PLDA&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1051200499903603&#34;&gt;Score Normalization&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech Separation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/WSJ0Mix&#34;&gt;WSJ0Mix&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriMix&#34;&gt;LibriMix&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/WHAMandWHAMR&#34;&gt;WHAM!&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/WHAMandWHAMR&#34;&gt;WHAMR!&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/Aishell1Mix&#34;&gt;Aishell1Mix&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/BinauralWSJ0Mix&#34;&gt;BinauralWSJ0Mix&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.13154&#34;&gt;SepFormer&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.09507&#34;&gt;RESepFormer&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2201.10800&#34;&gt;SkiM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1910.06379&#34;&gt;DualPath RNN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1809.07454&#34;&gt;ConvTasNET&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech Enhancement&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/DNS&#34;&gt;DNS&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/Voicebank&#34;&gt;Voicebank&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.13154&#34;&gt;SepFormer&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1905.04874&#34;&gt;MetricGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2110.05866&#34;&gt;MetricGAN-U&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1703.09452&#34;&gt;SEGAN&lt;/a&gt;, &lt;a href=&#34;http://staff.ustc.edu.cn/~jundu/Publications/publications/Trans2015_Xu.pdf&#34;&gt;spectral masking&lt;/a&gt;, &lt;a href=&#34;http://staff.ustc.edu.cn/~jundu/Publications/publications/Trans2015_Xu.pdf&#34;&gt;time masking&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Interpretability&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/ESC50&#34;&gt;ESC50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.13086&#34;&gt;Listenable Maps for Audio Classifiers (L-MAC)&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/e53280d73dd5389e820f4a6250365b0e-Paper-Conference.pdf&#34;&gt;Learning-to-Interpret (L2I)&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/e53280d73dd5389e820f4a6250365b0e-Paper-Conference.pdf&#34;&gt;Non-Negative Matrix Factorization (NMF)&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2303.12659&#34;&gt;PIQ&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech Generation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/AudioMNIST&#34;&gt;AudioMNIST&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;Diffusion&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Latent Diffusion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-to-Speech&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LJSpeech&#34;&gt;LJSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriTTS&#34;&gt;LibriTTS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1712.05884&#34;&gt;Tacotron2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2112.02418&#34;&gt;Zero-Shot Multi-Speaker Tacotron2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34;&gt;FastSpeech2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vocoding&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LJSpeech&#34;&gt;LJSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriTTS&#34;&gt;LibriTTS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;HiFiGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2009.09761&#34;&gt;DiffWave&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Spoken Language Understanding&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/MEDIA&#34;&gt;MEDIA&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/SLURP&#34;&gt;SLURP&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/fluent-speech-commands&#34;&gt;Fluent Speech Commands&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/timers-and-such&#34;&gt;Timers-and-Such&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.01604&#34;&gt;Direct SLU&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2104.01604&#34;&gt;Decoupled SLU&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2104.01604&#34;&gt;Multistage SLU&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech-to-Speech Translation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/CVSS&#34;&gt;CVSS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.07447.pdf&#34;&gt;Discrete Hubert&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;HiFiGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2006.11477&#34;&gt;wav2vec2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech Translation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/Fisher-Callhome-Spanish&#34;&gt;Fisher CallHome (Spanish)&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/IWSLT22_lowresource&#34;&gt;IWSLT22(lowresource)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11477&#34;&gt;wav2vec2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Emotion Classification&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/IEMOCAP&#34;&gt;IEMOCAP&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/ZaionEmotionDataset&#34;&gt;ZaionEmotionDataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.07143&#34;&gt;ECAPA-TDNN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2006.11477&#34;&gt;wav2vec2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2306.12991&#34;&gt;Emotion Diarization&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Language Identification&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/VoxLingua107&#34;&gt;VoxLingua107&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonLanguage&#34;&gt;CommonLanguage&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.07143&#34;&gt;ECAPA-TDNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Voice Activity Detection&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriParty&#34;&gt;LibriParty&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.04624&#34;&gt;CRDNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sound Classification&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/ESC50&#34;&gt;ESC50&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/UrbanSound8k&#34;&gt;UrbanSound&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ranchlai/sound_classification&#34;&gt;CNN14&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2005.07143&#34;&gt;ECAPA-TDNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Self-Supervised Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice&#34;&gt;CommonVoice&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech&#34;&gt;LibriSpeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.11477&#34;&gt;wav2vec2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Metric Learning&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/REAL-M/sisnr-estimation&#34;&gt;REAL-M&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/Voicebank&#34;&gt;Voicebank&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.08909&#34;&gt;Blind SNR-Estimation&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2110.05866&#34;&gt;PESQ Learning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/TIMIT&#34;&gt;TIMIT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~graves/icml_2006.pdf&#34;&gt;CTC&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf&#34;&gt;Viterbi&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf&#34;&gt;Forward Forward&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Diarization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/AMI&#34;&gt;AMI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.07143&#34;&gt;ECAPA-TDNN&lt;/a&gt;, &lt;a href=&#34;https://www.danielpovey.com/files/2018_icassp_xvectors.pdf&#34;&gt;X-vectors&lt;/a&gt;, &lt;a href=&#34;http://www.ifp.illinois.edu/~hning2/papers/Ning_spectral.pdf&#34;&gt;Spectral Clustering&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;📝 Text Processing&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tasks&lt;/th&gt; &#xA;   &lt;th&gt;Datasets&lt;/th&gt; &#xA;   &lt;th&gt;Technologies/Models&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Language Modeling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice&#34;&gt;CommonVoice&lt;/a&gt;, &lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/unstable-v0.6/recipes/LibriSpeech&#34;&gt;LibriSpeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://web.stanford.edu/~jurafsky/slp3/3.pdf&#34;&gt;n-grams&lt;/a&gt;, &lt;a href=&#34;https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf&#34;&gt;RNNLM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;TransformerLM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Response Generation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/unstable-v0.6/recipes/MultiWOZ/response_generation&#34;&gt;MultiWOZ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;GPT2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Grapheme-to-Phoneme&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech&#34;&gt;LibriSpeech&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.13703&#34;&gt;RNN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2207.13703&#34;&gt;Transformer&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2207.13703&#34;&gt;Curriculum Learning&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2207.13703&#34;&gt;Homograph loss&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🔍 Additional Features&lt;/h2&gt; &#xA;&lt;p&gt;SpeechBrain includes a range of native functionalities that enhance the development of Conversational AI technologies. Here are some examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training Orchestration:&lt;/strong&gt; The &lt;code&gt;Brain&lt;/code&gt; class serves as a fully customizable tool for managing training and evaluation loops over data. It simplifies training loops while providing the flexibility to override any part of the process.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hyperparameter Management:&lt;/strong&gt; A YAML-based hyperparameter file specifies all hyperparameters, from individual numbers (e.g., learning rate) to complete objects (e.g., custom models). This elegant solution drastically simplifies the training script.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dynamic Dataloader:&lt;/strong&gt; Enables flexible and efficient data reading.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;GPU Training:&lt;/strong&gt; Supports single and multi-GPU training, including distributed training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dynamic Batching:&lt;/strong&gt; On-the-fly dynamic batching enhances the efficient processing of variable-length signals.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mixed-Precision Training:&lt;/strong&gt; Accelerates training through mixed-precision techniques.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient Data Reading:&lt;/strong&gt; Reads large datasets efficiently from a shared Network File System (NFS) via &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;WebDataset&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hugging Face Integration:&lt;/strong&gt; Interfaces seamlessly with &lt;a href=&#34;https://huggingface.co/speechbrain&#34;&gt;HuggingFace&lt;/a&gt; for popular models such as wav2vec2 and Hubert.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Orion Integration:&lt;/strong&gt; Interfaces with &lt;a href=&#34;https://github.com/Epistimio/orion&#34;&gt;Orion&lt;/a&gt; for hyperparameter tuning.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Speech Augmentation Techniques:&lt;/strong&gt; Includes SpecAugment, Noise, Reverberation, and more.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Preparation Scripts:&lt;/strong&gt; Includes scripts for preparing data for supported datasets.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;SpeechBrain is rapidly evolving, with ongoing efforts to support a growing array of technologies in the future.&lt;/p&gt; &#xA;&lt;h2&gt;📊 Performance&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;SpeechBrain integrates a variety of technologies, including those that achieves competitive or state-of-the-art performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For a comprehensive overview of the achieved performance across different tasks, datasets, and technologies, please visit &lt;a href=&#34;https://github.com/speechbrain/speechbrain/raw/develop/PERFORMANCE.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;📜 License&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SpeechBrain is released under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, version 2.0&lt;/a&gt;, a popular BSD-like license.&lt;/li&gt; &#xA; &lt;li&gt;You are free to redistribute SpeechBrain for both free and commercial purposes, with the condition of retaining license headers. Unlike the GPL, the Apache License is not viral, meaning you are not obligated to release modifications to the source code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;🔮Future Plans&lt;/h1&gt; &#xA;&lt;p&gt;We have ambitious plans for the future, with a focus on the following priorities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scale Up:&lt;/strong&gt; We aim to provide comprehensive recipes and technologies for training massive models on extensive datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scale Down:&lt;/strong&gt; While scaling up delivers unprecedented performance, we recognize the challenges of deploying large models in production scenarios. We are focusing on real-time, streamable, and small-footprint Conversational AI.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;🤝 Contributing&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SpeechBrain is a community-driven project, led by a core team with the support of numerous international collaborators.&lt;/li&gt; &#xA; &lt;li&gt;We welcome contributions and ideas from the community. For more information, check &lt;a href=&#34;https://speechbrain.github.io/contributing.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;🙏 Sponsors&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SpeechBrain is an academically driven project and relies on the passion and enthusiasm of its contributors.&lt;/li&gt; &#xA; &lt;li&gt;As we cannot rely on the resources of a large company, we deeply appreciate any form of support, including donations or collaboration with the core team.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re interested in sponsoring SpeechBrain, please reach out to us at &lt;a href=&#34;mailto:speechbrainproject@gmail.com&#34;&gt;speechbrainproject@gmail.com&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A heartfelt thank you to all our sponsors, including the current ones:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://speechbrain.github.io/img/hf.ico&#34;&gt;&lt;img src=&#34;https://huggingface.co/front/assets/huggingface_logo.svg?sanitize=true&#34; alt=&#34;Image 1&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;https://viadialog.com/en/&#34;&gt;&lt;img src=&#34;https://speechbrain.github.io/img/sponsors/logo_vd.png&#34; alt=&#34;Image 3&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;https://europe.naverlabs.com/&#34;&gt;&lt;img src=&#34;https://speechbrain.github.io/img/sponsors/logo_nle.png&#34; alt=&#34;Image 4&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ovhcloud.com/en-ca/&#34;&gt;&lt;img src=&#34;https://speechbrain.github.io/img/sponsors/logo_ovh.png&#34; alt=&#34;Image 5&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;https://usa.baidu.com/&#34;&gt;&lt;img src=&#34;https://speechbrain.github.io/img/sponsors/logo_badu.png&#34; alt=&#34;Image 2&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;https://research.samsung.com/aicenter_cambridge&#34;&gt;&lt;img src=&#34;https://speechbrain.github.io/img/sponsors/samsung_official.png&#34; alt=&#34;Image 6&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mila.quebec/en/&#34;&gt;&lt;img src=&#34;https://speechbrain.github.io/img/sponsors/logo_mila_small.png&#34; alt=&#34;Image 7&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;https://www.concordia.ca/&#34;&gt;&lt;img src=&#34;https://www.concordia.ca/content/dam/common/logos/Concordia-logo.jpeg&#34; alt=&#34;Image 9&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp; &lt;a href=&#34;https://lia.univ-avignon.fr/&#34;&gt;&lt;img src=&#34;https://speechbrain.github.io/img/partners/logo_lia.png&#34; alt=&#34;Image 8&#34; width=&#34;250&#34;&gt;&lt;/a&gt; &amp;nbsp; &amp;nbsp;&lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;h1&gt;📖 Citing SpeechBrain&lt;/h1&gt; &#xA;&lt;p&gt;If you use SpeechBrain in your research or business, please cite it using the following BibTeX entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ravanelli2024opensourceconversationalaispeechbrain,&#xA;      title={Open-Source Conversational AI with SpeechBrain 1.0},&#xA;      author={Mirco Ravanelli and Titouan Parcollet and Adel Moumen and Sylvain de Langen and Cem Subakan and Peter Plantinga and Yingzhi Wang and Pooneh Mousavi and Luca Della Libera and Artem Ploujnikov and Francesco Paissan and Davide Borra and Salah Zaiem and Zeyu Zhao and Shucong Zhang and Georgios Karakasidis and Sung-Lin Yeh and Pierre Champion and Aku Rouhe and Rudolf Braun and Florian Mai and Juan Zuluaga-Gomez and Seyed Mahed Mousavi and Andreas Nautsch and Xuechen Liu and Sangeet Sagar and Jarod Duret and Salima Mdhaffar and Gaelle Laperriere and Mickael Rouvier and Renato De Mori and Yannick Esteve},&#xA;      year={2024},&#xA;      eprint={2407.00463},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.LG},&#xA;      url={https://arxiv.org/abs/2407.00463},&#xA;}&#xA;@misc{speechbrain,&#xA;  title={{SpeechBrain}: A General-Purpose Speech Toolkit},&#xA;  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},&#xA;  year={2021},&#xA;  eprint={2106.04624},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={eess.AS},&#xA;  note={arXiv:2106.04624}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>