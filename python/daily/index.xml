<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-17T01:37:04Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pixeltable/pixeltable</title>
    <updated>2025-06-17T01:37:04Z</updated>
    <id>tag:github.com,2025-06-17:/pixeltable/pixeltable</id>
    <link href="https://github.com/pixeltable/pixeltable" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pixeltable ‚Äî AI Data infrastructure providing a declarative, incremental approach for multimodal workloads.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/pixeltable-logo-large.png&#34; alt=&#34;Pixeltable Logo&#34; width=&#34;50%&#34;&gt; &#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;h2&gt;Declarative Data Infrastructure for Multimodal AI Apps&lt;/h2&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-0530AD.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/pixeltable?logo=python&amp;amp;logoColor=white&amp;amp;&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/platform-Linux%20%7C%20macOS%20%7C%20Windows-E5DDD4&#34; alt=&#34;Platform Support&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/pixeltable/pixeltable/actions/workflows/pytest.yml&#34;&gt;&lt;img src=&#34;https://github.com/pixeltable/pixeltable/actions/workflows/pytest.yml/badge.svg?sanitize=true&#34; alt=&#34;tests status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pixeltable/pixeltable/actions/workflows/nightly.yml&#34;&gt;&lt;img src=&#34;https://github.com/pixeltable/pixeltable/actions/workflows/nightly.yml/badge.svg?sanitize=true&#34; alt=&#34;tests status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pixeltable/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pixeltable?color=4D148C&#34; alt=&#34;PyPI Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/QPyqFYx2UN&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%92%AC-Discord-%235865F2.svg?sanitize=true&#34; alt=&#34;My Discord (1306431018890166272)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/overview/installation&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://docs.pixeltable.com/docs/overview/quick-start&#34;&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://docs.pixeltable.com/&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://pixeltable.github.io/pixeltable/&#34;&gt;&lt;strong&gt;API Reference&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://docs.pixeltable.com/docs/examples/use-cases&#34;&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/QPyqFYx2UN&#34;&gt;&lt;strong&gt;Discord Community&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Pixeltable is the only Python framework that provides incremental storage, transformation, indexing, and orchestration of your multimodal data.&lt;/p&gt; &#xA;&lt;h2&gt;üò© Maintaining Production-Ready Multimodal AI Apps is Still Too Hard&lt;/h2&gt; &#xA;&lt;p&gt;Building robust AI applications, especially &lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/bringing-data&#34;&gt;multimodal&lt;/a&gt; ones, requires stitching together numerous tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ETL pipelines for data loading and transformation.&lt;/li&gt; &#xA; &lt;li&gt;Vector databases for semantic search.&lt;/li&gt; &#xA; &lt;li&gt;Feature stores for ML models.&lt;/li&gt; &#xA; &lt;li&gt;Orchestrators for scheduling.&lt;/li&gt; &#xA; &lt;li&gt;Model serving infrastructure for inference.&lt;/li&gt; &#xA; &lt;li&gt;Separate systems for parallelization, caching, versioning, and lineage tracking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This complex &#34;data plumbing&#34; slows down development, increases costs, and makes applications brittle and hard to reproduce.&lt;/p&gt; &#xA;&lt;h2&gt;üíæ Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install pixeltable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pixeltable is a database.&lt;/strong&gt; It stores metadata and computed results persistently, typically in a &lt;code&gt;.pixeltable&lt;/code&gt; directory in your workspace. See &lt;a href=&#34;https://docs.pixeltable.com/docs/overview/configuration&#34;&gt;configuration&lt;/a&gt; options for your setup.&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® What is Pixeltable?&lt;/h2&gt; &#xA;&lt;p&gt;With Pixeltable, you define your &lt;em&gt;entire&lt;/em&gt; data processing and AI workflow declaratively using &lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/computed-columns&#34;&gt;computed columns&lt;/a&gt;&lt;/strong&gt; on &lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/tables-and-operations&#34;&gt;tables&lt;/a&gt;&lt;/strong&gt;. Pixeltable&#39;s engine then automatically handles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Ingestion &amp;amp; Storage:&lt;/strong&gt; References &lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/bringing-data&#34;&gt;files&lt;/a&gt; (images, videos, audio, docs) in place, handles structured data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transformation &amp;amp; Processing:&lt;/strong&gt; Applies &lt;em&gt;any&lt;/em&gt; Python function (&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/custom-functions&#34;&gt;UDFs&lt;/a&gt;) or built-in operations (&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/iterators&#34;&gt;chunking, frame extraction&lt;/a&gt;) automatically.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI Model Integration:&lt;/strong&gt; Runs inference (&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/embedding-index&#34;&gt;embeddings&lt;/a&gt;, &lt;a href=&#34;https://docs.pixeltable.com/docs/examples/vision/yolox&#34;&gt;object detection&lt;/a&gt;, &lt;a href=&#34;https://docs.pixeltable.com/docs/integrations/frameworks#cloud-llm-providers&#34;&gt;LLMs&lt;/a&gt;) as part of the data pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Indexing &amp;amp; Retrieval:&lt;/strong&gt; Creates and manages vector indexes for fast &lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/embedding-index#phase-3%3A-query&#34;&gt;semantic search&lt;/a&gt; alongside traditional filtering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Incremental Computation:&lt;/strong&gt; Only &lt;a href=&#34;https://docs.pixeltable.com/docs/overview/quick-start&#34;&gt;recomputes&lt;/a&gt; what&#39;s necessary when data or code changes, saving time and cost.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versioning &amp;amp; Lineage:&lt;/strong&gt; Automatically tracks data and schema changes for reproducibility.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Focus on your application logic, not the infrastructure.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/tables-and-operations&#34;&gt;Unified Multimodal Interface:&lt;/a&gt;&lt;/strong&gt; &lt;code&gt;pxt.Image&lt;/code&gt;, &lt;code&gt;pxt.Video&lt;/code&gt;, &lt;code&gt;pxt.Audio&lt;/code&gt;, &lt;code&gt;pxt.Document&lt;/code&gt;, etc. ‚Äì manage diverse data consistently.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t = pxt.create_table(&#xA;  &#39;media&#39;, &#xA;  {&#xA;      &#39;img&#39;: pxt.Image, &#xA;      &#39;video&#39;: pxt.Video&#xA;  }&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/computed-columns&#34;&gt;Declarative Computed Columns:&lt;/a&gt;&lt;/strong&gt; Define processing steps once; they run automatically on new/updated data.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t.add_computed_column(&#xA;  classification=huggingface.vit_for_image_classification(&#xA;      t.image&#xA;  )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/embedding-index&#34;&gt;Built-in Vector Search:&lt;/a&gt;&lt;/strong&gt; Add embedding indexes and perform similarity searches directly on tables/views.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t.add_embedding_index(&#xA;  &#39;img&#39;, &#xA;  embedding=clip.using(&#xA;      model_id=&#39;openai/clip-vit-base-patch32&#39;&#xA;  )&#xA;)&#xA;&#xA;sim = t.img.similarity(&#34;cat playing with yarn&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/views&#34;&gt;On-the-Fly Data Views:&lt;/a&gt;&lt;/strong&gt; Create virtual tables using iterators for efficient processing without data duplication.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;frames = pxt.create_view(&#xA;  &#39;frames&#39;, &#xA;  videos, &#xA;  iterator=FrameIterator.create(&#xA;      video=videos.video, &#xA;      fps=1&#xA;  )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/integrations/frameworks&#34;&gt;Seamless AI Integration:&lt;/a&gt;&lt;/strong&gt; Built-in functions for OpenAI, Anthropic, Hugging Face, CLIP, YOLOX, and more.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t.add_computed_column(&#xA;  response=openai.chat_completions(&#xA;      messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: t.prompt}]&#xA;  )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/custom-functions&#34;&gt;Bring Your Own Code:&lt;/a&gt;&lt;/strong&gt; Extend Pixeltable with simple Python User-Defined Functions.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@pxt.udf&#xA;def format_prompt(context: list, question: str) -&amp;gt; str:&#xA;    return f&#34;Context: {context}\nQuestion: {question}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/examples/chat/tools&#34;&gt;Agentic Workflows / Tool Calling:&lt;/a&gt;&lt;/strong&gt; Register &lt;code&gt;@pxt.udf&lt;/code&gt; or &lt;code&gt;@pxt.query&lt;/code&gt; functions as tools and orchestrate LLM-based tool use (incl. multimodal).&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Example tools: a UDF and a Query function for RAG&#xA;tools = pxt.tools(get_weather_udf, search_context_query)&#xA;&#xA;# LLM decides which tool to call; Pixeltable executes it&#xA;t.add_computed_column(&#xA;     tool_output=invoke_tools(tools, t.llm_tool_choice)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/tables-and-operations#data-operations&#34;&gt;Persistent &amp;amp; Versioned:&lt;/a&gt;&lt;/strong&gt; All data, metadata, and computed results are automatically stored.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t.revert()  # Revert to a previous version&#xA;stored_table = pxt.get_table(&#39;my_existing_table&#39;)  # Retrieve persisted table&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pixeltable.com/docs/datastore/filtering-and-selecting&#34;&gt;SQL-like Python Querying:&lt;/a&gt;&lt;/strong&gt; Familiar syntax combined with powerful AI capabilities.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = (&#xA;  t.where(t.score &amp;gt; 0.8)&#xA;  .order_by(t.timestamp)&#xA;  .select(t.image, score=t.score)&#xA;  .limit(10)&#xA;  .collect()&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üí° Key Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;(See the &lt;a href=&#34;https://docs.pixeltable.com/docs/overview/quick-start&#34;&gt;Full Quick Start&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/pixeltable/pixeltable/main/#-notebook-gallery&#34;&gt;Notebook Gallery&lt;/a&gt; for more details)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Multimodal Data Store and Data Transformation (Computed Column):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pixeltable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pixeltable as pxt&#xA;&#xA;# Create a table&#xA;t = pxt.create_table(&#xA;    &#39;films&#39;, &#xA;    {&#39;name&#39;: pxt.String, &#39;revenue&#39;: pxt.Float, &#39;budget&#39;: pxt.Float}, &#xA;    if_exists=&#34;replace&#34;&#xA;)&#xA;&#xA;t.insert([&#xA;  {&#39;name&#39;: &#39;Inside Out&#39;, &#39;revenue&#39;: 800.5, &#39;budget&#39;: 200.0},&#xA;  {&#39;name&#39;: &#39;Toy Story&#39;, &#39;revenue&#39;: 1073.4, &#39;budget&#39;: 200.0}&#xA;])&#xA;&#xA;# Add a computed column for profit - runs automatically!&#xA;t.add_computed_column(profit=(t.revenue - t.budget), if_exists=&#34;replace&#34;)&#xA;&#xA;# Query the results&#xA;print(t.select(t.name, t.profit).collect())&#xA;# Output includes the automatically computed &#39;profit&#39; column&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Object Detection with &lt;a href=&#34;https://github.com/pixeltable/pixeltable-yolox&#34;&gt;YOLOX&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pixeltable pixeltable-yolox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import PIL&#xA;import pixeltable as pxt&#xA;from yolox.models import Yolox&#xA;from yolox.data.datasets import COCO_CLASSES&#xA;&#xA;t = pxt.create_table(&#39;image&#39;, {&#39;image&#39;: pxt.Image}, if_exists=&#39;replace&#39;)&#xA;&#xA;# Insert some images&#xA;prefix = &#39;https://upload.wikimedia.org/wikipedia/commons&#39;&#xA;paths = [&#xA;    &#39;/1/15/Cat_August_2010-4.jpg&#39;,&#xA;    &#39;/e/e1/Example_of_a_Dog.jpg&#39;,&#xA;    &#39;/thumb/b/bf/Bird_Diversity_2013.png/300px-Bird_Diversity_2013.png&#39;&#xA;]&#xA;t.insert({&#39;image&#39;: prefix + p} for p in paths)&#xA;&#xA;@pxt.udf&#xA;def detect(image: PIL.Image.Image) -&amp;gt; list[str]:&#xA;    model = Yolox.from_pretrained(&#34;yolox_s&#34;)&#xA;    result = model([image])&#xA;    coco_labels = [COCO_CLASSES[label] for label in result[0][&#34;labels&#34;]]&#xA;    return coco_labels&#xA;&#xA;t.add_computed_column(classification=detect(t.image))&#xA;&#xA;print(t.select().collect())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Image Similarity Search (CLIP Embedding Index):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pixeltable sentence-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pixeltable as pxt&#xA;from pixeltable.functions.huggingface import clip&#xA;&#xA;# Create image table and add sample images&#xA;images = pxt.create_table(&#39;my_images&#39;, {&#39;img&#39;: pxt.Image}, if_exists=&#39;replace&#39;)&#xA;images.insert([&#xA;    {&#39;img&#39;: &#39;https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/1920px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg&#39;},&#xA;    {&#39;img&#39;: &#39;https://upload.wikimedia.org/wikipedia/commons/d/d5/Retriever_in_water.jpg&#39;}&#xA;])&#xA;&#xA;# Add CLIP embedding index for similarity search&#xA;images.add_embedding_index(&#xA;    &#39;img&#39;,&#xA;    embedding=clip.using(model_id=&#39;openai/clip-vit-base-patch32&#39;)&#xA;)&#xA;&#xA;# Text-based image search&#xA;query_text = &#34;a dog playing fetch&#34;&#xA;sim_text = images.img.similarity(query_text)&#xA;results_text = images.order_by(sim_text, asc=False).limit(3).select(&#xA;    image=images.img, similarity=sim_text&#xA;).collect()&#xA;print(&#34;--- Text Query Results ---&#34;)&#xA;print(results_text)&#xA;&#xA;# Image-based image search&#xA;query_image_url = &#39;https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Huskiesatrest.jpg/2880px-Huskiesatrest.jpg&#39;&#xA;sim_image = images.img.similarity(query_image_url)&#xA;results_image = images.order_by(sim_image, asc=False).limit(3).select(&#xA;    image=images.img, similarity=sim_image&#xA;).collect()&#xA;print(&#34;--- Image URL Query Results ---&#34;)&#xA;print(results_image)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;4. Multimodal/Incremental RAG Workflow (Document Chunking &amp;amp; LLM Call):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pixeltable openai spacy sentence-transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m spacy download en_core_web_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pixeltable as pxt&#xA;import pixeltable.functions as pxtf&#xA;from pixeltable.functions import openai, huggingface&#xA;from pixeltable.iterators import DocumentSplitter&#xA;&#xA;# Manage your tables by directories&#xA;directory = &#34;my_docs&#34;&#xA;pxt.drop_dir(directory, if_not_exists=&#34;ignore&#34;, force=True)&#xA;pxt.create_dir(&#34;my_docs&#34;)&#xA;&#xA;# Create a document table and add a PDF&#xA;docs = pxt.create_table(f&#39;{directory}.docs&#39;, {&#39;doc&#39;: pxt.Document})&#xA;docs.insert([{&#39;doc&#39;: &#39;https://github.com/pixeltable/pixeltable/raw/release/docs/resources/rag-demo/Jefferson-Amazon.pdf&#39;}])&#xA;&#xA;# Create chunks view with sentence-based splitting&#xA;chunks = pxt.create_view(&#xA;    &#39;doc_chunks&#39;,&#xA;    docs,&#xA;    iterator=DocumentSplitter.create(document=docs.doc, separators=&#39;sentence&#39;)&#xA;)&#xA;&#xA;# Explicitly create the embedding function object&#xA;embed_model = huggingface.sentence_transformer.using(model_id=&#39;all-MiniLM-L6-v2&#39;)&#xA;# Add embedding index using the function object&#xA;chunks.add_embedding_index(&#39;text&#39;, string_embed=embed_model)&#xA;&#xA;# Define query function for retrieval - Returns a DataFrame expression&#xA;@pxt.query&#xA;def get_relevant_context(query_text: str, limit: int = 3):&#xA;    sim = chunks.text.similarity(query_text)&#xA;    # Return a list of strings (text of relevant chunks)&#xA;    return chunks.order_by(sim, asc=False).limit(limit).select(chunks.text)&#xA;&#xA;# Build a simple Q&amp;amp;A table&#xA;qa = pxt.create_table(f&#39;{directory}.qa_system&#39;, {&#39;prompt&#39;: pxt.String})&#xA;&#xA;# 1. Add retrieved context (now a list of strings)&#xA;qa.add_computed_column(context=get_relevant_context(qa.prompt))&#xA;&#xA;# 2. Format the prompt with context&#xA;qa.add_computed_column(&#xA;    final_prompt=pxtf.string.format(&#xA;        &#34;&#34;&#34;&#xA;        PASSAGES: &#xA;        {0}&#xA;        &#xA;        QUESTION: &#xA;        {1}&#xA;        &#34;&#34;&#34;, &#xA;        qa.context, &#xA;        qa.prompt&#xA;    )&#xA;)&#xA;&#xA;# 4. Generate the answer using the well-formatted prompt column&#xA;qa.add_computed_column(&#xA;    answer=openai.chat_completions(&#xA;        model=&#39;gpt-4o-mini&#39;,&#xA;        messages=[{&#xA;            &#39;role&#39;: &#39;user&#39;,&#xA;            &#39;content&#39;: qa.final_prompt&#xA;        }]&#xA;    ).choices[0].message.content&#xA;)&#xA;&#xA;# Ask a question and get the answer&#xA;qa.insert([{&#39;prompt&#39;: &#39;What can you tell me about Amazon?&#39;}])&#xA;print(&#34;--- Final Answer ---&#34;)&#xA;print(qa.select(qa.answer).collect())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìö Notebook Gallery&lt;/h2&gt; &#xA;&lt;p&gt;Explore Pixeltable&#39;s capabilities interactively:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notebook&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Notebook&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Fundamentals&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Integrations&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;10-Min Tour&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/pixeltable-basics.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OpenAI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Tables &amp;amp; Ops&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Anthropic&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;UDFs&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Together AI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Embedding Index&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-and-vector-indexes.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Label Studio&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://docs.pixeltable.com/docs/cookbooks/vision/label-studio&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%93%9A%2520Docs-013056&#34; alt=&#34;Visit Docs&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;External Files&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Mistral&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/mistralai/cookbook/blob/main/third_party/Pixeltable/incremental_prompt_engineering_and_model_comparison.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Github&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Sample Apps&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;RAG Demo&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-demo.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Multimodal Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://huggingface.co/spaces/Pixeltable/Multimodal-Powerhouse&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%2520Demo-FF7D04&#34; alt=&#34;HF Space&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Object Detection&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/object-detection-in-videos.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Image/Text Search&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://github.com/pixeltable/pixeltable/tree/main/docs/sample-apps/text-and-image-similarity-search-nextjs-fastapi&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%96%A5%EF%B8%8F%2520App-black.svg?sanitize=true&#34; alt=&#34;GitHub App&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Audio Transcription&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/audio-transcriptions.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Discord Bot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://github.com/pixeltable/pixeltable/raw/main/docs/sample-apps/context-aware-discord-bot&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%92%AC%20Bot-%235865F2.svg?sanitize=true&#34; alt=&#34;GitHub App&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üîÆ Roadmap (2025)&lt;/h2&gt; &#xA;&lt;h3&gt;Cloud Infrastructure and Deployment&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;re working on a hosted Pixeltable service that will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Enable Multimodal Data Sharing of Pixeltable Tables and Views&lt;/li&gt; &#xA; &lt;li&gt;Provide a persistent cloud instance&lt;/li&gt; &#xA; &lt;li&gt;Turn Pixeltable workflows (Tables, Queries, UDFs) into API endpoints/&lt;a href=&#34;https://github.com/pixeltable/pixeltable-mcp-server&#34;&gt;MCP Servers&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We love contributions! Whether it&#39;s reporting bugs, suggesting features, improving documentation, or submitting code changes, please check out our &lt;a href=&#34;https://raw.githubusercontent.com/pixeltable/pixeltable/main/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; and join the &lt;a href=&#34;https://github.com/pixeltable/pixeltable/discussions&#34;&gt;Discussions&lt;/a&gt; or our &lt;a href=&#34;https://discord.gg/QPyqFYx2UN&#34;&gt;Discord Server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üè¢ License&lt;/h2&gt; &#xA;&lt;p&gt;Pixeltable is licensed under the Apache 2.0 License.&lt;/p&gt;</summary>
  </entry>
</feed>