<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-10T01:44:24Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>THUDM/GLM-130B</title>
    <updated>2023-01-10T01:44:24Z</updated>
    <id>tag:github.com,2023-01-10:/THUDM/GLM-130B</id>
    <link href="https://github.com/THUDM/GLM-130B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GLM-130B: An Open Bilingual Pre-Trained Model&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/resources/7D6433A42D189E2E6FBC62BE066BCE91.png&#34;&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üåê &lt;a href=&#34;http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/&#34; target=&#34;_blank&#34;&gt;Blog&lt;/a&gt; ‚Ä¢ ‚è¨ &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform&#34; target=&#34;_blank&#34;&gt;Download Model&lt;/a&gt; ‚Ä¢ ü™ß &lt;a href=&#34;https://huggingface.co/spaces/THUDM/GLM-130B&#34; target=&#34;_blank&#34;&gt;Demo&lt;/a&gt; ‚Ä¢ ‚úâÔ∏è &lt;a href=&#34;mailto:glm-130b@googlegroups.com&#34;&gt;Email&lt;/a&gt; ‚Ä¢ üìÉ &lt;a href=&#34;https://arxiv.org/abs/2210.02414&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;&lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üí¨ &lt;a href=&#34;https://groups.google.com/g/glm-130b-forum&#34; target=&#34;_blank&#34;&gt;Google Group&lt;/a&gt; (Updates) or &lt;a href=&#34;https://github.com/THUDM/GLM-130B/raw/main/resources/WechatGroup.jpeg&#34; target=&#34;_blank&#34;&gt;Wechat Group&lt;/a&gt; or &lt;a href=&#34;https://join.slack.com/t/glm-130b/shared_invite/zt-1f2ih11xy-EAuDComTAr~XVB3MywE9Cg&#34; target=&#34;_blank&#34;&gt;Slack channel&lt;/a&gt; (Discussions) &lt;/p&gt; &#xA;&lt;h1&gt;GLM-130B: An Open Bilingual Pre-Trained Model&lt;/h1&gt; &#xA;&lt;p&gt;GLM-130B is an open bilingual (English &amp;amp; Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the algorithm of &lt;a href=&#34;https://aclanthology.org/2022.acl-long.26&#34;&gt;General Language Model (GLM)&lt;/a&gt;. It is designed to support inference tasks with the 130B parameters on &lt;strong&gt;a single A100 (40G * 8)&lt;/strong&gt; or &lt;strong&gt;V100 (32G * 8) server&lt;/strong&gt;. With INT4 quantization, the hardware requirements can further be reduced to &lt;strong&gt;a single server with 4 * RTX 3090 (24G)&lt;/strong&gt; with &lt;strong&gt;almost no performance degradation&lt;/strong&gt;. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) and it has the following unique features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bilingual:&lt;/strong&gt; supports both English and Chinese.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Performance (EN):&lt;/strong&gt; better than GPT-3 175B (+4.0%), OPT-175B (+5.5%), and BLOOM-176B (+13.0%) on LAMBADA and slightly better than GPT-3 175B (+0.9%) on MMLU.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Performance (CN):&lt;/strong&gt; significantly better than ERNIE TITAN 3.0 260B on 7 zero-shot CLUE datasets (+24.26%) and 5 zero-shot FewCLUE datasets (+12.75%).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast Inference:&lt;/strong&gt; supports fast inference on both &lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt; and &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt; (up to 2.5X faster) with a single A100 server.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reproducibility:&lt;/strong&gt; all results (30+ tasks) can be easily reproduced with open-sourced code and model checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-Platform:&lt;/strong&gt; supports training and inference on NVIDIA, Hygon DCU, Ascend 910, and Sunway (Will be released soon).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repository mainly focus on the evaluation of GLM-130B, the training part can be found at &lt;a href=&#34;https://github.com/THUDM/LargeScale&#34;&gt;this repo&lt;/a&gt;. If you find our work and our open-sourced efforts useful, ‚≠êÔ∏è to encourage our following development! :)&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2022.10.06]&lt;/strong&gt; Our &lt;a href=&#34;http://arxiv.org/abs/2210.02414&#34;&gt;paper&lt;/a&gt; for GLM-130B is out!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2022.08.24]&lt;/strong&gt; We are proud to publish the quantized version for GLM-130B. While preserving the activation precision as FP16, the model weights can be quantized to as low as &lt;strong&gt;INT4 with almost no degradation of performance&lt;/strong&gt;, further reducing the hardware requirements of the GLM-130B to &lt;strong&gt;a single server with 4 * RTX 3090 (24G)&lt;/strong&gt;! See &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/docs/quantization.md&#34;&gt;Quantization of GLM-130B&lt;/a&gt; for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For smaller models, please find &lt;a href=&#34;https://github.com/THUDM/GLM&#34;&gt;monolingual GLMs&lt;/a&gt; (English: 10B/2B/515M/410M/335M/110M, Chinese: 10B/335M) and an &lt;a href=&#34;https://github.com/THUDM/Multilingual-GLM&#34;&gt;1B multilingual GLM&lt;/a&gt; (104 languages).&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;h4&gt;Hardware&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;GPU Memory&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Quantization&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Weight Offload&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8 * A100&lt;/td&gt; &#xA;   &lt;td&gt;40 GB&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8 * V100&lt;/td&gt; &#xA;   &lt;td&gt;32 GB&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Yes (BMInf)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8 * V100&lt;/td&gt; &#xA;   &lt;td&gt;32 GB&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8 * RTX 3090&lt;/td&gt; &#xA;   &lt;td&gt;24 GB&lt;/td&gt; &#xA;   &lt;td&gt;INT8&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4 * RTX 3090&lt;/td&gt; &#xA;   &lt;td&gt;24 GB&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8 * RTX 2080 Ti&lt;/td&gt; &#xA;   &lt;td&gt;11 GB&lt;/td&gt; &#xA;   &lt;td&gt;INT4&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;It is recommended to use the an A100 (40G * 8) server, as all GLM-130B evaluation results (~30 tasks) reported can be easily reproduced with a single A100 server in about half a day. With INT8/INT4 quantization, efficient inference on &lt;strong&gt;a single server with 4 * RTX 3090 (24G)&lt;/strong&gt; is possible, see &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/docs/quantization.md&#34;&gt;Quantization of GLM-130B&lt;/a&gt; for details. Combining quantization and weight offloading techniques, GLM-130B can also be inferenced on servers with even smaller GPU memory, see &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/docs/low-resource-inference.md&#34;&gt;Low-Resource Inference&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h4&gt;Software&lt;/h4&gt; &#xA;&lt;p&gt;The GLM-130B code is built on the top of &lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;. We recommend using &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt; to manage your environment and installing additional dependencies via &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;. Here are the recommended environment configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.9+ / CUDA 11+ / PyTorch 1.10+ / DeepSpeed 0.6+ / Apex (&lt;strong&gt;installation with CUDA and C++ extensions is required, see &lt;a href=&#34;https://github.com/NVIDIA/apex/#linux&#34;&gt;here&lt;/a&gt;&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;SwissArmyTransformer&amp;gt;=0.2.11 is required for quantization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Model weights&lt;/h4&gt; &#xA;&lt;p&gt;Download the GLM-130B‚Äôs model checkpoint from &lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform?usp=sf_link&#34;&gt;here&lt;/a&gt;, make sure all 60 chunks are downloaded completely, then use the following command to merge them into a single archive file and extract it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat glm-130b-sat.tar.part_* &amp;gt; glm-130b-sat.tar&#xA;tar xvf glm-130b-sat.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; in &lt;code&gt;configs/model_glm_130b.sh&lt;/code&gt; to the path of the extracted folder. Since the checkpoint file is up to 260G, it is recommended to use the SSD or RAM disk to reduce the checkpoint loading time. Since the checkpoint we distribute is in 8-way tensor parallel, a conversion scripts is also provided if you need to change the tensor parallel dimension.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/convert_tp.py \&#xA;    --input-folder &amp;lt;SRC_CKPT_PATH&amp;gt;  \&#xA;    --output-folder &amp;lt;DST_CKPT_PATH&amp;gt; \&#xA;    --target-tp &amp;lt;TARGET_TP&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Left-To-Right Generation / Blank Filling&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/generate.sh --input-source interactive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify an input file by &lt;code&gt;--input-source input.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;GLM-130B uses two different mask tokens: &lt;code&gt;[MASK]&lt;/code&gt; for short blank filling and &lt;code&gt;[gMASK]&lt;/code&gt; for left-to-right long text generation. When the input does not contain any MASK token, &lt;code&gt;[gMASK]&lt;/code&gt; will be automatically appended to the end of the text.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Left-To-Right Generation Examples&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;h4&gt;Example 1&lt;/h4&gt; &#xA; &lt;p&gt;Input: Who is the greatest artist? The greatest artist is [gMASK]&lt;/p&gt; &#xA; &lt;p&gt;Output: Who is the greatest artist? The greatest artist is not the one who paints the most beautiful picture, or writes the best poem, or plays the best symphony. The greatest artist is the one whose work has the greatest impact on the lives of others. The Greatest Artist is the artist who inspires us to be our best selves. The Great Artist is not the artist whose work is flawless. The great artist is not one who is perfect. The true great artist has the ability to make us feel something. The artist who makes us feel is the great artist. The art of the great artists is to make you feel something that you have never felt before. The greatness of an artist is in the ability of their work to touch the lives and hearts of others, and to make them feel something they have never experienced before. This is the true greatness of the artist. 2. The Significance of the Great Artist The significance of the greatest artist cannot be overstated. The significance and impact of the art of a great artist cannot even be measured. The impact of a single work of art can change the course of history. A single work can change a person‚Äôs life. A work of great art can make you laugh, make you cry, make your heart race, or make you think. A great work of artistic expression has the power to change the way you look at the world. The work of a truly great artist can change your life forever. 3. The Impact of the Greatest Artist The greatest artists have the ability and the power of their art to change people‚Äôs lives. They have the power and the ability, through their art, to make people feel something new. The power of the work of the truly great artists cannot be measured, but it can be felt. The greater the impact of an artists work, the greater the power it has to change lives. 4. The Influence of the Art of the Artist The art and the work created by the greatest artists has the potential to change a life. The influence of the artists work cannot be quantified, but is felt by those who experience it. The truly great work has an ability to touch people‚Äô lives in a way that no other work can.&lt;/p&gt; &#xA; &lt;h4&gt;Example 2 (Chinese)&lt;/h4&gt; &#xA; &lt;p&gt;Input: ÈóÆÈ¢òÔºöÂÜ¨Â§©Ôºå‰∏≠ÂõΩÂì™Â∫ßÂüéÂ∏ÇÊúÄÈÄÇÂêàÈÅøÂØíÔºüÈóÆÈ¢òÊèèËø∞ÔºöËÉΩÊé®Ëçê‰∏Ä‰∫õÂõΩÂÜÖÈÄÇÂêàÂÜ¨Â§©ÈÅøÂØíÁöÑÂüéÂ∏ÇÂêóÔºüÂõûÁ≠îÁî®Êà∑ÔºöÊóÖÊ∏∏Áà±Â•ΩËÄÖ ÂõûÁ≠îÔºö [gMASK]&lt;/p&gt; &#xA; &lt;p&gt;Output: ÈóÆÈ¢ò:ÂÜ¨Â§©,‰∏≠ÂõΩÂì™Â∫ßÂüéÂ∏ÇÊúÄÈÄÇÂêàÈÅøÂØí?ÈóÆÈ¢òÊèèËø∞:ËÉΩÊé®Ëçê‰∏Ä‰∫õÂõΩÂÜÖÈÄÇÂêàÂÜ¨Â§©ÈÅøÂØíÁöÑÂüéÂ∏ÇÂêó?ÂõûÁ≠îÁî®Êà∑:ÊóÖÊ∏∏Áà±Â•ΩËÄÖ ÂõûÁ≠î:ÈÅøÂØíÁöÑËØù,Âª∫ËÆÆÂéªÊµ∑Âçó‰∏â‰∫ö„ÄÇ ‰∏â‰∫ö‰Ωç‰∫éÊµ∑ÂçóÂ≤õÁöÑÊúÄÂçóÁ´Ø,ÊòØ‰∏≠ÂõΩÊúÄÂçóÈÉ®ÁöÑÁÉ≠Â∏¶Êª®Êµ∑ÊóÖÊ∏∏ÂüéÂ∏Ç,‰∏≠ÂõΩÊµ∑Êª®ÂüéÂ∏Ç,ÊòØ‰∏≠ÂõΩÁ©∫Ê∞îË¥®ÈáèÊúÄÂ•ΩÁöÑÂüéÂ∏Ç„ÄÅÂÖ®ÂõΩÊúÄÈïøÂØøÂú∞Âå∫(Âπ≥ÂùáÂØøÂëΩ80Â≤Å)„ÄÇ‰∏â‰∫öÂ∏ÇÂà´Áß∞ÈπøÂüé,ÂèàË¢´Áß∞‰∏∫&#34;‰∏úÊñπÂ§èÂ®ÅÂ§∑&#34;,‰ΩçÂ±Ö‰∏≠ÂõΩÂõõÂ§ß‰∏ÄÁ∫øÊóÖÊ∏∏ÂüéÂ∏Ç&#34;‰∏âÂ®ÅÊù≠Âé¶&#34;‰πãÈ¶ñ,Êã•ÊúâÂÖ®Â≤õÊúÄÁæé‰∏ΩÁöÑÊµ∑Êª®È£éÂÖâ„ÄÇ‰∏â‰∫öÊã•ÊúâË¢´Êó†Êï∞ÂüéÂ∏ÇÂ´âÂ¶íÁöÑÊ∏ÖÊñ∞Á©∫Ê∞î,ÊüîÂíåÊµ∑Êª©„ÄÇÂú®Ê≤ôÊª©ÊÇ†Èó≤Êï£Ê≠•„ÄÅÊ≤êÊµ¥ÂÇçÊôöÊ∏©ÂíåÈò≥ÂÖâ,Âú®Êµ∑ËæπÁé©ËÄç,Âú®Èõ®ÊûóÈáåÈù¢ÂëºÂê∏ÂÅ•Â∫∑,Ê¨£ËµèËá™ÁÑ∂Â•áËßÇ,‰∏ÄÂàáÈÉΩÊòØÈÇ£‰πà‰ª§‰∫∫‰∫´Âèó„ÄÇ Ê∞îÂÄô ‰∏â‰∫öÂú∞Â§ÑÁÉ≠Â∏¶,Â±ûÁÉ≠Â∏¶Â≠£È£éÊ∞îÂÄôÂå∫Âüü,ÁªàÂπ¥Ê∞îÊ∏©È´ò,ÂØíÊöëÂèòÂåñ‰∏çÂ§ß,Âπ¥Âπ≥ÂùáÊ∞îÊ∏©25.5Â∫¶„ÄÇ‰∏â‰∫öÊúÄÁÉ≠ÁöÑÊúà‰ªΩÊòØ6Êúà,Âπ≥ÂùáÊ∞îÊ∏©‰∏∫28.5¬∞C,ÊûÅÁ´ØÊúÄÈ´òÊ∞îÊ∏©‰∏∫35.7¬∞C ,‰∏â‰∫öÊ≤°ÊúâÊòéÊòæÁöÑÂÜ¨Â≠£,ËÄå‰∏îÂÜ¨Â≠£Êõ¥ÊòØÂà∞Êµ∑Êª®ÊóÖÊ∏∏ÁöÑÊúÄÂ•ΩÊó∂ÂÄô,ÊâÄ‰ª•‰∏â‰∫öÊòØÂÖ®Âπ¥ÂÖ®Â§©ÂÄôÈÅøÂØí„ÄÅÊ∂àÊöë„ÄÅÂ∫¶ÂÅá„ÄÅÊóÖÊ∏∏ÁöÑÂ•ΩÂú∞Êñπ„ÄÇ‰∏çËøá,‰∏â‰∫öÊúÄÊó∫ÁöÑÊóÖÊ∏∏Êó∂Èó¥ÊòØ‰ªé10Êúà‰ªΩÂà∞ÂÜúÂéÜÊò•ËäÇ,Ëøô‰∏™Êó∂ÂÄôÁöÑ‰∫∫ÁâπÂà´Â§ö,ÊàøÈó¥‰πü‰∏çÂ•ΩËÆ¢,Âª∫ËÆÆÊúÄÂ•ΩÈÅøÂºÄÈ´òÂ≥∞„ÄÇ‰∏â‰∫öÁöÑ‰∏â‰∏™ÊóÖÊ∏∏È´òÂ≥∞ÊúüÊòØ&#34;‰∫î‰∏Ä&#34;„ÄÅ&#34;ÂçÅ‰∏Ä&#34;„ÄÅ&#34;Êò•ËäÇ&#34;,Ëøô‰∏â‰∏™ËäÇÊó•ÊúüÈó¥ÁöÑÊàø‰ª∑ÈÉΩ‰ºö‰∏äÊµÆÂá†ÂÄç,Â¶ÇÊûú‰Ω†ÈÄâÊã©‰∫ÜËøô‰∏â‰∏™ÂÅáÊúüÊù•‰∏â‰∫öÁöÑËØùË¶ÅÊèêÂâç‰∏Ä‰∏™ÊòüÊúüÂÆöÊàøÂíåÂí®ËØ¢„ÄÇ&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Blank Filling Examples&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;h4&gt;Example 1&lt;/h4&gt; &#xA; &lt;p&gt;Input: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.&lt;/p&gt; &#xA; &lt;p&gt;Output: Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.&lt;br&gt;&lt;/p&gt; &#xA; &lt;h4&gt;Example 2 (Chinese)&lt;/h4&gt; &#xA; &lt;p&gt;Input: ÂáØÊóãÈó®‰Ωç‰∫éÊÑèÂ§ßÂà©Á±≥ÂÖ∞Â∏ÇÂè§ÂüéÂ†°ÊóÅ„ÄÇ1807Âπ¥‰∏∫Á∫™Âøµ[MASK]ËÄåÂª∫ÔºåÈó®È´ò25Á±≥ÔºåÈ°∂‰∏äÁüóÁ´ã‰∏§Ê≠¶Â£´ÈùíÈìúÂè§ÂÖµËΩ¶Èì∏ÂÉè„ÄÇ&lt;/p&gt; &#xA; &lt;p&gt;Output: ÂáØÊóãÈó®‰Ωç‰∫éÊÑèÂ§ßÂà©Á±≥ÂÖ∞Â∏ÇÂè§ÂüéÂ†°ÊóÅ„ÄÇ1807Âπ¥‰∏∫Á∫™ÂøµÊãøÁ†¥‰ªëËÉúÂà©ËÄåÂª∫,Èó®È´ò25Á±≥,È°∂‰∏äÁüóÁ´ã‰∏§Ê≠¶Â£´ÈùíÈìúÂè§ÂÖµËΩ¶Èì∏ÂÉè„ÄÇ&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Arguments Useful in Generation&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--input-source [path] or &#34;interactive&#34;&lt;/code&gt; The input file&#39;s path. It can also be &#34;interactive&#34;, which will launch a CLI.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;‚Äî-output-path [path]&lt;/code&gt; The folder containing the results.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;‚Äî-out-seq-length [int]&lt;/code&gt; The maximum sequence length for generation (including context).&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;‚Äî-min-gen-length [int]&lt;/code&gt; The minimum generation length for each MASK.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;‚Äî-sampling-strategy &#34;BaseStrategy&#34; or &#34;BeamSearchStrategy&#34;&lt;/code&gt;. The sampling strategy used. &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;For BeamSearchStrategy: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;code&gt;‚Äî-num-beams [int]&lt;/code&gt; The number of beams.&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;‚Äî-length-penalty [float]&lt;/code&gt; The maximum sequence length for generation (including context).&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;‚Äî-no-repeat-ngram-size [int]&lt;/code&gt; Prohibit repeated n-gram generation.&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;‚Äî-print-all-beam&lt;/code&gt; Print the generated results for all beams.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;For BaseStrategy: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;code&gt;‚Äî-top-k [int]&lt;/code&gt; Top k sampling.&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;‚Äî-top-p [float]&lt;/code&gt; Top p sampling.&lt;/li&gt; &#xA;      &lt;li&gt;&lt;code&gt;‚Äî-temperature [float]&lt;/code&gt; The sampling temperature.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We use the YAML file to define tasks. Specifically, you can add multiple tasks or folders at a time for evaluation, and the evaluation script will automatically collect all YAML files under those folders recursively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/evaluate.sh task1.yaml task2.yaml dir1 dir2 ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download our evaluation dataset &lt;a href=&#34;https://cloud.tsinghua.edu.cn/f/9257ee84045644b8ac06/&#34;&gt;here&lt;/a&gt;, and set &lt;code&gt;DATA_PATH&lt;/code&gt; in &lt;code&gt;scripts/evaluate.sh&lt;/code&gt; to your local dataset directory. The task folder contains the YAML files for 30+ tasks we evaluated for GLM-130B. Take the &lt;a href=&#34;https://nyu-mll.github.io/CoLA/&#34;&gt;CoLA&lt;/a&gt; task for example, run &lt;code&gt;bash scripts/evaluate.sh tasks/bloom/glue_cola.yaml&lt;/code&gt;, which outputs an accuracy of ~65% for the best prompt and ~57% for the median.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Expected Output&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;MultiChoiceTaskConfig(name=&#39;glue_cola&#39;, type=&amp;lt;TaskType.MULTICHOICE: &#39;mul&#39;&amp;gt;, path=&#39;/thudm/LargeScale/data/zeroshot/bloom/glue_cola&#39;, module=None, metrics=[&#39;Accuracy&#39;], use_task_mask=False, use_multitask_encoding=False, unidirectional=False, max_seq_length=2048, file_pattern={&#39;validation&#39;: &#39;**/validation.jsonl&#39;}, micro_batch_size=8)&#xA;Evaluating task glue_cola:&#xA;  Evaluating group validation:&#xA;      Finish Following_sentence_acceptable/mul/validation.jsonl, Accuracy = 42.665&#xA;      Finish Make_sense_yes_no/mul/validation.jsonl, Accuracy = 56.951&#xA;      Finish Previous_sentence_acceptable/mul/validation.jsonl, Accuracy = 65.197&#xA;      Finish editing/mul/validation.jsonl, Accuracy = 57.622&#xA;      Finish is_this_correct/mul/validation.jsonl, Accuracy = 65.197&#xA;Evaluation results of task glue_cola:&#xA;  Group validation Accuracy: max = 65.197, median = 57.622, average = 57.526&#xA;Finish task glue_cola in 101.2s. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Multi-node evaluation can be configured by setting &lt;code&gt;HOST_FILE_PATH&lt;/code&gt;(required by the &lt;a href=&#34;https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node&#34;&gt;DeepSpeed lanucher&lt;/a&gt;) in &lt;code&gt;scripts/evaluate_multiple_node.sh&lt;/code&gt;. Set &lt;code&gt;DATA_PATH&lt;/code&gt; in &lt;code&gt;scripts/evaluate_multiple_node.sh&lt;/code&gt; and run the following command to evaluate all the tasks in &lt;code&gt;./task&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/evaluate_multiple_node.sh ./tasks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/docs/evaluate-your-own-tasks.md&#34;&gt;Evaluate Your Own Tasks&lt;/a&gt; for details on how to add new tasks.&lt;/p&gt; &#xA;&lt;h3&gt;2.5X faster Inference using FasterTransformer&lt;/h3&gt; &#xA;&lt;p&gt;By adapting the GLM-130B model to &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransfomer&lt;/a&gt;, a highly optimized transformer model library by NVIDIA, we can reach up to 2.5X speedup on generation, see &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/docs/inference-with-fastertransformer.md&#34;&gt;Inference with FasterTransformer&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Acknowledgement&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;br&gt; This project is supported by the National Science Foundation for Distinguished Young Scholars (No. 61825602). &#xA; &lt;h3&gt;Lead Contributors&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Sengxian&#34;&gt;Aohan Zeng (Tsinghua KEG)&lt;/a&gt;, &lt;a href=&#34;https://github.com/xiao9905&#34;&gt;Xiao Liu (Tsinghua KEG)&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Contributors&lt;/h3&gt; &#xA; &lt;h4&gt;Tsinghua KEG---the Knowledge Engineering Group at Tsinghua&lt;/h4&gt; &#xA; &lt;p&gt;Zhengxiao Du, Ming Ding, Qinkai Zheng, Hanyu Lai, Zihan Wang, Zhuoyi Yang, Jifan Yu, Xiaohan Zhang, Wendi Zheng, Xiao Xia, Yifan Xu, Weng Lam Tam, Yuxiao Dong, Jie Tang&lt;/p&gt; &#xA; &lt;h4&gt;Tsinghua PACMAN---the Parallel Architecture &amp;amp; Compiler technology of Mobile, Accelerated, and Networked systems Group at Tsinghua&lt;/h4&gt; &#xA; &lt;p&gt;Zixuan Ma, Jiaao He, Zhenbo Sun, Jidong Zhai, Wenguang Chen&lt;/p&gt; &#xA; &lt;h4&gt;Tsinghua NLP (BMInf)---the Natural Language Processing Group at Tsinghua&lt;/h4&gt; &#xA; &lt;p&gt;Guoyang Zeng, Xu Han, Weilin Zhao, Zhiyuan Liu&lt;/p&gt; &#xA; &lt;h4&gt;Zhipu.AI---an AI startup that aims to teach machines to think like humans&lt;/h4&gt; &#xA; &lt;p&gt;Yufei Xue, Shan Wang, Jiecai Shan, Haohan Jiang, Zhengang Guo, Peng Zhang&lt;/p&gt; &#xA; &lt;h3&gt;Computation Sponsor&lt;/h3&gt; &#xA; &lt;p&gt;Zhipu.AI&lt;/p&gt; &#xA; &lt;h3&gt;Project Leader&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;http://keg.cs.tsinghua.edu.cn/jietang/&#34;&gt;Jie Tang (Tsinghua KEG &amp;amp; BAAI)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This repository is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/LICENSE&#34;&gt;Apache-2.0 license&lt;/a&gt;. The use of GLM-130B model weights is subject to the &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/GLM-130B/main/MODEL_LICENSE&#34;&gt;Model License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful, please consider citing GLM-130B:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zeng2022glm130b,&#xA;  title={GLM-130B: An Open Bilingual Pre-trained Model},&#xA;  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},&#xA;  journal={arXiv preprint arXiv:2210.02414},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also consider GLM&#39;s original work in your reference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{du2022glm,&#xA;  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},&#xA;  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},&#xA;  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},&#xA;  pages={320--335},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>cuifengcn/TAICHI-flet</title>
    <updated>2023-01-10T01:44:24Z</updated>
    <id>tag:github.com,2023-01-10:/cuifengcn/TAICHI-flet</id>
    <link href="https://github.com/cuifengcn/TAICHI-flet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Âü∫‰∫éfletÁöÑ‰∏ÄÊ¨æwindowsÊ°åÈù¢Â∫îÁî®ÔºåÂÆûÁé∞‰∫ÜÁà¨ÂèñÂõæÁâá„ÄÅÈü≥‰πê„ÄÅÂ∞èËØ¥„ÄÅÁ£ÅÂäõÈìæÊé•ÁöÑÂäüËÉΩ„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TAICHI-flet&lt;/h1&gt; &#xA;&lt;p&gt;Âü∫‰∫éfletÁöÑ‰∏ÄÊ¨æwindowsÊ°åÈù¢Â∫îÁî®ÔºåÂÆûÁé∞‰∫ÜÁà¨ÂèñÂõæÁâá„ÄÅÈü≥‰πê„ÄÅÂ∞èËØ¥„ÄÅÁ£ÅÂäõÈìæÊé•ÁöÑÂäüËÉΩ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÁâπÁÇπÔºöÂ§öÂäüËÉΩÂ®±‰πêËΩØ‰ª∂ÔºåÁïåÈù¢ÁæéËßÇ„ÄÅÁÆÄÊ¥Å„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;‰ºöÊåÅÁª≠Êõ¥Êñ∞&lt;/p&gt; &#xA;&lt;h2&gt;‰ΩøÁî®ÊñπÊ≥ï&lt;/h2&gt; &#xA;&lt;p&gt;ÊñπÊ≥ï1. ‰∏ãËΩΩÊ∫ê‰ª£Á†ÅÔºåÂÆâË£Ö&lt;code&gt;requirements.txt&lt;/code&gt;‰∏≠ÁöÑÂåÖÔºåËøêË°å&lt;code&gt;ui.py&lt;/code&gt;Êñá‰ª∂„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ÊñπÊ≥ï2. ‰ΩøÁî®pyinstallerÊâìÂåÖ&lt;/p&gt; &#xA;&lt;h2&gt;‰∏ªÈ°µ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cuifengcn/TAICHI-flet/main/docs/%E4%B8%BB%E9%A1%B5.png&#34; alt=&#34;‰∏ªÈ°µ&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ËßÇÂ±±‚Äî‚ÄîÂ¶πÂ≠êÂõæÊµèËßà&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cuifengcn/TAICHI-flet/main/docs/%E5%9B%BE%E7%89%87.png&#34; alt=&#34;ËßÇÂ±±&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Âê¨Èõ®‚Äî‚ÄîÈü≥‰πêÊí≠Êîæ‰∏ãËΩΩ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cuifengcn/TAICHI-flet/main/docs/%E9%9F%B3%E4%B9%90.png&#34; alt=&#34;Âê¨Èõ®&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‰øÆ‰ªô‚Äî‚ÄîÂ∞èËØ¥ÈòÖËØª&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cuifengcn/TAICHI-flet/main/docs/%E5%B0%8F%E8%AF%B4.png&#34; alt=&#34;‰øÆ‰ªô&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ÊäöÁê¥‚Äî‚ÄîÁ£ÅÂäõÈìæÊé•ËÅöÂêàÊêúÁ¥¢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cuifengcn/TAICHI-flet/main/docs/%E7%A3%81%E5%8A%9B.png&#34; alt=&#34;ÊäöÁê¥&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ÁôæÂÆùÁÆ±‚Äî‚ÄîÁõÆÂâçÂè™ËÉΩpdfËΩ¨word&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cuifengcn/TAICHI-flet/main/docs/%E5%B7%A5%E5%85%B7.png&#34; alt=&#34;ÁôæÂÆùÁÆ±&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ËóèÁªèÈòÅ‚Äî‚ÄîÂ•ΩÁî®ÁöÑËΩØ‰ª∂Á≠âÊé®Ëçê&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cuifengcn/TAICHI-flet/main/docs/%E8%BD%AF%E4%BB%B6.png&#34; alt=&#34;ËóèÁªèÈòÅ&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>kohya-ss/sd-webui-additional-networks</title>
    <updated>2023-01-10T01:44:24Z</updated>
    <id>tag:github.com,2023-01-10:/kohya-ss/sd-webui-additional-networks</id>
    <link href="https://github.com/kohya-ss/sd-webui-additional-networks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Additional Networks for generating images&lt;/h2&gt; &#xA;&lt;p&gt;Êó•Êú¨Ë™û„ÅÆÊñáÁ´†„ÅØ‰∏ã„ÅÆ„Åª„ÅÜ„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;Updates / Êõ¥Êñ∞ÊÉÖÂ†±&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Jan 9 2023&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The method of selecting a model has changed.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Place models files in the folder &lt;code&gt;extensions\sd-webui-additional-networks\models\lora&lt;/code&gt;&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;All models, including in subfolders, are listed in the drop-down.&lt;/li&gt; &#xA;     &lt;li&gt;You can add an extra folder at &lt;code&gt;Settings&lt;/code&gt; tab -&amp;gt; &lt;code&gt;Additional Networks&lt;/code&gt; on left bottom -&amp;gt; &lt;code&gt;Extra path to scan for LoRA models:&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Generated PNGs now have settings about LoRA in the infotext, which can be restored by sending it from &lt;code&gt;PNG Info&lt;/code&gt; tab by &lt;code&gt;txt2img&lt;/code&gt; button.&lt;/li&gt; &#xA;   &lt;li&gt;Thanks to space-nuko for this great contribution!&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2023/1/9:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;„É¢„Éá„É´ÈÅ∏ÊäûÊñπÊ≥ï„ÅåÂ§â„Çè„Çä„Åæ„Åó„Åü„ÄÇ&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;„É¢„Éá„É´„Éï„Ç°„Ç§„É´„Çí Web UI „ÅÆ&lt;code&gt;extensions\sd-webui-additional-networks\models\lora&lt;/code&gt; „Éï„Ç©„É´„ÉÄ„Å´ÁΩÆ„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;„Çµ„Éñ„Éï„Ç©„É´„ÉÄ„Å´„ÅÇ„Çã„ÇÇ„ÅÆ„ÇÇÂê´„ÇÅ„Åô„Åπ„Å¶„ÅÆ„É¢„Éá„É´„Åå„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„Å´Ë°®Á§∫„Åï„Çå„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA;     &lt;li&gt;„Çπ„Ç≠„É£„É≥„Åô„Çã„Éï„Ç©„É´„ÉÄ„ÇíËøΩÂä†„Åß„Åç„Åæ„Åô„ÄÇ &lt;code&gt;Settings&lt;/code&gt; „Çø„Éñ„ÅÆ &lt;code&gt;Additional Networks&lt;/code&gt; ÔºàÂ∑¶‰∏ã„Å´„ÅÇ„Çä„Åæ„ÅôÔºâ„ÇíÈÅ∏Êäû„Åó„ÄÅ &lt;code&gt;Extra path to scan for LoRA models:&lt;/code&gt; „ÅßË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;ÁîüÊàê„Åï„Çå„Åü PNG „ÅÆ infotext „Å´Ë®≠ÂÆö„Åå‰øùÂ≠ò„Åï„Çå„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ &lt;code&gt;PNG Info&lt;/code&gt; „Çø„Éñ„Åã„Çâ &lt;code&gt;txt2img&lt;/code&gt; „Éú„Çø„É≥„ÅßÂæ©ÂÖÉ„Åï„Çå„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA;   &lt;li&gt;Á¥†Êô¥„Çâ„Åó„ÅÑ„Éó„É´„É™„ÇØ„Çí„ÅÑ„Åü„Å†„ÅÑ„Åü space-nuko Ê∞è„Å´ÊÑüË¨ù„Åó„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jan 8 2023, 2023/1/8:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Supports medvram/lowvram in web UI. Thanks for ddvarpdd!&lt;/li&gt; &#xA;   &lt;li&gt;Web UI „Å´ medvram/lowvram „Ç™„Éó„Ç∑„Éß„É≥„Çí‰ªò„Åë„ÅüÂ†¥Âêà„Åß„ÇÇÂãï‰Ωú„Åô„Çã„Çà„ÅÜ‰øÆÊ≠£„Åó„Åæ„Åó„Åü„ÄÇddvarpdd Ê∞è„Å´ÊÑüË¨ù„Åó„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jan 6 2023, 2023/1/6:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fixed a bug that broke the model were broken when switching enable-&amp;gt;disable-&amp;gt;enable...&lt;/li&gt; &#xA;   &lt;li&gt;SD 2.x „ÅÆ„É¢„Éá„É´„ÅßÊúâÂäπ„ÉªÁÑ°Âäπ„ÇíÁπ∞„ÇäËøî„ÅóÂàá„ÇäÊõø„Åà„Çã„Å®„É¢„Éá„É´„ÅåÂ£ä„Çå„Å¶„ÅÑ„Åè‰∏çÂÖ∑Âêà„Çí‰øÆÊ≠£„Åó„Åæ„Åó„Åü„ÄÇ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;This extension is for &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111&#39;s Stable Diffusion web UI&lt;/a&gt;, allows the Web UI to add some networks (e.g. LoRA) to the original Stable Diffusion model to generate images. Currently LoRA is supported. The addition is on-the-fly, the merging is not required.&lt;/p&gt; &#xA;&lt;p&gt;This extension supports the LoRA models (*.ckpt or *.safetensors) trained by our scripts in &lt;a href=&#34;https://github.com/kohya-ss/sd-scripts&#34;&gt;sd-scripts&lt;/a&gt;. The models from other LoRA implementations are not supported.&lt;/p&gt; &#xA;&lt;p&gt;This extension does not support training.&lt;/p&gt; &#xA;&lt;p&gt;Other networks other than LoRA may be supported in the future.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open &#34;Extensions&#34; tab.&lt;/li&gt; &#xA; &lt;li&gt;Open &#34;Install from URL&#34; tab in the tab.&lt;/li&gt; &#xA; &lt;li&gt;Enter URL of this repo to &#34;URL for extension&#39;s git repository&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Press &#34;Install&#34; button.&lt;/li&gt; &#xA; &lt;li&gt;Restart Web UI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;p&gt;Put the LoRA models (&lt;code&gt;*.pt&lt;/code&gt;, &lt;code&gt;*.ckpt&lt;/code&gt; or &lt;code&gt;*.safetensors&lt;/code&gt;) inside the &lt;code&gt;sd-webui-additional-networks/models/LoRA&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;Open &lt;strong&gt;&#34;Additional Networks&#34;&lt;/strong&gt; panel from the left bottom of Web UI.&lt;/p&gt; &#xA;&lt;p&gt;Press &lt;strong&gt;&#34;Refresh models&#34;&lt;/strong&gt; to update the models list.&lt;/p&gt; &#xA;&lt;p&gt;Select &lt;strong&gt;&#34;LoRA&#34;&lt;/strong&gt; for &lt;strong&gt;&#34;Network module 1&#34;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Choose &lt;strong&gt;the name of the LoRA model file&lt;/strong&gt; in &lt;strong&gt;&#34;Model 1&#34;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Set &lt;strong&gt;the weight&lt;/strong&gt; of the model (negative weight might be working but unexpected.)&lt;/p&gt; &#xA;&lt;p&gt;Repeat them for the module/model/weight 2 to 5 if you have other models. Models are applied in the order of 1 to 5.&lt;/p&gt; &#xA;&lt;p&gt;You can generate images with the model with these additional networks.&lt;/p&gt; &#xA;&lt;h2&gt;„Åì„ÅÆ Web UI Êã°Âºµ„Å´„Å§„ÅÑ„Å¶&lt;/h2&gt; &#xA;&lt;p&gt;LoRA „Å™„Å©„ÅÆ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíÂÖÉ„ÅÆ Stable Diffusion „Å´ËøΩÂä†„Åó„ÄÅÁîªÂÉèÁîüÊàê„ÇíË°å„ÅÜ„Åü„ÇÅ„ÅÆÊã°Âºµ„Åß„Åô„ÄÇÁèæÂú®„ÅØ LoRA „ÅÆ„ÅøÂØæÂøú„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;„Åì„ÅÆÊã°Âºµ„Åß‰Ωø„Åà„Çã„ÅÆ„ÅØ&lt;a href=&#34;https://github.com/kohya-ss/sd-scripts&#34;&gt;sd-scripts&lt;/a&gt;„É™„Éù„Ç∏„Éà„É™„ÅßÂ≠¶Áøí„Åó„Åü LoRA „ÅÆ„É¢„Éá„É´Ôºà*.ckpt „Åæ„Åü„ÅØ *.safetensorsÔºâ„Åß„Åô„ÄÇ‰ªñ„ÅÆ LoRA „É™„Éù„Ç∏„Éà„É™„ÅßÂ≠¶Áøí„Åó„Åü„É¢„Éá„É´„ÅØÂØæÂøú„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;„Åì„ÅÆÊã°ÂºµÂçò‰Ωì„Åß„ÅØÂ≠¶Áøí„ÅØ„Åß„Åç„Åæ„Åõ„Çì„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Â∞ÜÊù•ÁöÑ„Å´ LoRA ‰ª•Â§ñ„ÅÆ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å´„Å§„ÅÑ„Å¶„ÇÇ„Çµ„Éù„Éº„Éà„Åô„Çã„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ&lt;/p&gt; &#xA;&lt;h2&gt;„Ç§„É≥„Çπ„Éà„Éº„É´&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Web UI „Åß &#34;Extensions&#34; „Çø„Éñ„ÇíÈñã„Åç„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;„Åï„Çâ„Å´ &#34;Install from URL&#34; „Çø„Éñ„ÇíÈñã„Åç„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;&#34;URL for extension&#39;s git repository&#34; Ê¨Ñ„Å´„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅÆ URL „ÇíÂÖ•„Çå„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;&#34;Install&#34;„Éú„Çø„É≥„ÇíÊäº„Åó„Å¶„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Åæ„Åô„ÄÇ&lt;/li&gt; &#xA; &lt;li&gt;Web UI „ÇíÂÜçËµ∑Âãï„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;‰ΩøÁî®Ê≥ï&lt;/h2&gt; &#xA;&lt;p&gt;Â≠¶Áøí„Åó„Åü LoRA „ÅÆ„É¢„Éá„É´(&lt;code&gt;*.pt&lt;/code&gt;, &lt;code&gt;*.ckpt&lt;/code&gt;, &lt;code&gt;*.safetensors&lt;/code&gt;)„Çí&lt;code&gt;sd-webui-additional-networks/models/LoRA&lt;/code&gt;„Å´ÁΩÆ„Åç„Åæ„Åô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;Web UI „ÅÆÂ∑¶‰∏ã„ÅÆ„Åª„ÅÜ„ÅÆ &lt;strong&gt;&#34;Additional Networks&#34;&lt;/strong&gt; „ÅÆ„Éë„Éç„É´„ÇíÈñã„Åç„Åæ„Åô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Network module 1&#34;&lt;/strong&gt; „Åß &lt;strong&gt;&#34;LoRA&#34;&lt;/strong&gt; „ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Refresh models&#34;&lt;/strong&gt; „Åß LoRA „É¢„Éá„É´„ÅÆ„É™„Çπ„Éà„ÇíÊõ¥Êñ∞„Åó„Åæ„Åô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Model 1&#34;&lt;/strong&gt; „Å´Â≠¶Áøí„Åó„Åü LoRA „ÅÆ„É¢„Éá„É´Âêç„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Weight&#34;&lt;/strong&gt; „Å´„Åì„ÅÆ„É¢„Éá„É´„ÅÆ &lt;strong&gt;Èáç„Åø&lt;/strong&gt; „ÇíÊåáÂÆö„Åó„Åæ„ÅôÔºàË≤†„ÅÆÂÄ§„ÇÇÊåáÂÆö„Åß„Åç„Åæ„Åô„Åå„Å©„Çì„Å™ÂäπÊûú„Åå„ÅÇ„Çã„Åã„ÅØÊú™Áü•Êï∞„Åß„ÅôÔºâ„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;ËøΩÂä†„ÅÆ„É¢„Éá„É´„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ 2ÔΩû5 „Å´ÊåáÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„É¢„Éá„É´„ÅØ 1~5 „ÅÆÈ†ÜÁï™„ÅßÈÅ©Áî®„Åï„Çå„Åæ„Åô„ÄÇ&lt;/p&gt; &#xA;&lt;p&gt;‰ª•‰∏ä„ÇíÊåáÂÆö„Åô„Çã„Å®„ÄÅ„Åù„Çå„Åû„Çå„ÅÆ„É¢„Éá„É´„ÅåÈÅ©Áî®„Åï„Çå„ÅüÁä∂ÊÖã„ÅßÁîªÂÉèÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ&lt;/p&gt;</summary>
  </entry>
</feed>