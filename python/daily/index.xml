<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-07T01:34:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>coleam00/Archon</title>
    <updated>2025-06-07T01:34:54Z</updated>
    <id>tag:github.com,2025-06-07:/coleam00/Archon</id>
    <link href="https://github.com/coleam00/Archon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Archon is an AI agent that is able to create other AI agents using an advanced agentic coding workflow and framework knowledge base to unlock a new frontier of automated agents.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Archon - AI Agent Builder&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/public/Archon.png&#34; alt=&#34;Archon Logo&#34;&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top: 20px;margin-bottom: 30px&#34;&gt; &#xA; &lt;h3&gt;🚀 **CURRENT VERSION** 🚀&lt;/h3&gt; &#xA; &lt;p&gt;&lt;strong&gt;[ V6 - Tool Library and MCP Integration ]&lt;/strong&gt; &lt;em&gt;Prebuilt tools, examples, and MCP server integration&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;🔄 IMPORTANT UPDATE (March 31st)&lt;/strong&gt;: Archon now includes a library of prebuilt tools, examples, and MCP server integrations. Archon can now incorporate these resources when building new agents, significantly enhancing capabilities and reducing hallucinations. Note that the examples/tool library for Archon is just starting out. Please feel free to contribute examples, MCP servers, and prebuilt tools!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Archon is the world&#39;s first &lt;strong&gt;&#34;Agenteer&#34;&lt;/strong&gt;, an AI agent designed to autonomously build, refine, and optimize other AI agents.&lt;/p&gt; &#xA;&lt;p&gt;It serves both as a practical tool for developers and as an educational framework demonstrating the evolution of agentic systems. Archon will be developed in iterations, starting with just a simple Pydantic AI agent that can build other Pydantic AI agents, all the way to a full agentic workflow using LangGraph that can build other AI agents with any framework. Through its iterative development, Archon showcases the power of planning, feedback loops, and domain-specific knowledge in creating robust AI agents.&lt;/p&gt; &#xA;&lt;h2&gt;Important Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The current version of Archon is V6 as mentioned above - see &lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v6-tool-library-integration/README.md&#34;&gt;V6 Documentation&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I &lt;strong&gt;just&lt;/strong&gt; created the &lt;a href=&#34;https://thinktank.ottomator.ai/c/archon/30&#34;&gt;Archon community&lt;/a&gt; forum over in the oTTomator Think Tank! Please post any questions you have there!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/users/coleam00/projects/1&#34;&gt;GitHub Kanban board&lt;/a&gt; for feature implementation and bug squashing.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Vision&lt;/h2&gt; &#xA;&lt;p&gt;Archon demonstrates three key principles in modern AI development:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agentic Reasoning&lt;/strong&gt;: Planning, iterative feedback, and self-evaluation overcome the limitations of purely reactive systems&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Domain Knowledge Integration&lt;/strong&gt;: Seamless embedding of frameworks like Pydantic AI and LangGraph within autonomous workflows&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalable Architecture&lt;/strong&gt;: Modular design supporting maintainability, cost optimization, and ethical AI practices&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Getting Started with V6 (current version)&lt;/h2&gt; &#xA;&lt;p&gt;Since V6 is the current version of Archon, all the code for V6 is in both the main directory and &lt;code&gt;archon/iterations/v6-tool-library-integration&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Note that the examples/tool library for Archon is just starting out. Please feel free to contribute examples, MCP servers, and prebuilt tools!&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker (optional but preferred)&lt;/li&gt; &#xA; &lt;li&gt;Python 3.11+&lt;/li&gt; &#xA; &lt;li&gt;Supabase account (for vector database)&lt;/li&gt; &#xA; &lt;li&gt;OpenAI/Anthropic/OpenRouter API key or Ollama for local LLMs (note that only OpenAI supports streaming in the Streamlit UI currently)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;h4&gt;Option 1: Docker (Recommended)&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/coleam00/archon.git&#xA;cd archon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the Docker setup script:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# This will build both containers and start Archon&#xA;python run_docker.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Access the Streamlit UI at &lt;a href=&#34;http://localhost:8501&#34;&gt;http://localhost:8501&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;run_docker.py&lt;/code&gt; will automatically:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Build the MCP server container&lt;/li&gt; &#xA;  &lt;li&gt;Build the main Archon container&lt;/li&gt; &#xA;  &lt;li&gt;Run Archon with the appropriate port mappings&lt;/li&gt; &#xA;  &lt;li&gt;Use environment variables from &lt;code&gt;.env&lt;/code&gt; file if it exists&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Option 2: Local Python Installation&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/coleam00/archon.git&#xA;cd archon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;source venv/bin/activate  # On Windows: venv\Scripts\activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Start the Streamlit UI:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;streamlit run streamlit_ui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Access the Streamlit UI at &lt;a href=&#34;http://localhost:8501&#34;&gt;http://localhost:8501&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup Process&lt;/h3&gt; &#xA;&lt;p&gt;After installation, follow the guided setup process in the Intro section of the Streamlit UI:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Configure your API keys and model settings - all stored in &lt;code&gt;workbench/env_vars.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: Set up your Supabase vector database&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Crawl and index the Pydantic AI documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Service&lt;/strong&gt;: Start the agent service for generating agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Interact with Archon to create AI agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt; (optional): Configure integration with AI IDEs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Streamlit interface will guide you through each step with clear instructions and interactive elements. There are a good amount of steps for the setup but it goes quick!&lt;/p&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;If you encounter any errors when using Archon, please first check the logs in the &#34;Agent Service&#34; tab. Logs specifically for MCP are also logged to &lt;code&gt;workbench/logs.txt&lt;/code&gt; (file is automatically created) so please check there. The goal is for you to have a clear error message before creating a bug here in the GitHub repo&lt;/p&gt; &#xA;&lt;h3&gt;Updating Archon&lt;/h3&gt; &#xA;&lt;h4&gt;Option 1: Docker&lt;/h4&gt; &#xA;&lt;p&gt;To get the latest updates for Archon when using Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Pull the latest changes from the repository (from within the archon directory)&#xA;git pull&#xA;&#xA;# Rebuild and restart the containers with the latest changes&#xA;python run_docker.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;run_docker.py&lt;/code&gt; script will automatically:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Detect and remove any existing Archon containers (whether running or stopped)&lt;/li&gt; &#xA; &lt;li&gt;Rebuild the containers with the latest code&lt;/li&gt; &#xA; &lt;li&gt;Start fresh containers with the updated version&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Option 2: Local Python Installation&lt;/h4&gt; &#xA;&lt;p&gt;To get the latest updates for Archon when using local Python installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Pull the latest changes from the repository (from within the archon directory)&#xA;git pull&#xA;&#xA;# Install any new dependencies&#xA;source venv/bin/activate  # On Windows: venv\Scripts\activate&#xA;pip install -r requirements.txt&#xA;&#xA;# Restart the Streamlit UI&#xA;# (If you&#39;re already running it, stop with Ctrl+C first)&#xA;streamlit run streamlit_ui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This ensures you&#39;re always running the most recent version of Archon with all the latest features and bug fixes.&lt;/p&gt; &#xA;&lt;h2&gt;Project Evolution&lt;/h2&gt; &#xA;&lt;h3&gt;V1: Single-Agent Foundation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic RAG-powered agent using Pydantic AI&lt;/li&gt; &#xA; &lt;li&gt;Supabase vector database for documentation storage&lt;/li&gt; &#xA; &lt;li&gt;Simple code generation without validation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v1-single-agent/README.md&#34;&gt;Learn more about V1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;V2: Agentic Workflow (LangGraph)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-agent system with planning and execution separation&lt;/li&gt; &#xA; &lt;li&gt;Reasoning LLM (O3-mini/R1) for architecture planning&lt;/li&gt; &#xA; &lt;li&gt;LangGraph for workflow orchestration&lt;/li&gt; &#xA; &lt;li&gt;Support for local LLMs via Ollama&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v2-agentic-workflow/README.md&#34;&gt;Learn more about V2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;V3: MCP Support&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integration with AI IDEs like Windsurf and Cursor&lt;/li&gt; &#xA; &lt;li&gt;Automated file creation and dependency management&lt;/li&gt; &#xA; &lt;li&gt;FastAPI service for agent generation&lt;/li&gt; &#xA; &lt;li&gt;Improved project structure and organization&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v3-mcp-support/README.md&#34;&gt;Learn more about V3&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;V4: Streamlit UI Overhaul&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker support&lt;/li&gt; &#xA; &lt;li&gt;Comprehensive Streamlit interface for managing all aspects of Archon&lt;/li&gt; &#xA; &lt;li&gt;Guided setup process with interactive tabs&lt;/li&gt; &#xA; &lt;li&gt;Environment variable management through the UI&lt;/li&gt; &#xA; &lt;li&gt;Database setup and documentation crawling simplified&lt;/li&gt; &#xA; &lt;li&gt;Agent service control and monitoring&lt;/li&gt; &#xA; &lt;li&gt;MCP configuration through the UI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v4-streamlit-ui-overhaul/README.md&#34;&gt;Learn more about V4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;V5: Multi-Agent Coding Workflow&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Specialized refiner agents for different autonomously improving the initially generated agent&lt;/li&gt; &#xA; &lt;li&gt;Prompt refiner agent for optimizing system prompts&lt;/li&gt; &#xA; &lt;li&gt;Tools refiner agent for specialized tool implementation&lt;/li&gt; &#xA; &lt;li&gt;Agent refiner for optimizing agent configuration and dependencies&lt;/li&gt; &#xA; &lt;li&gt;Cohesive initial agent structure before specialized refinement&lt;/li&gt; &#xA; &lt;li&gt;Improved workflow orchestration with LangGraph&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v5-parallel-specialized-agents/README.md&#34;&gt;Learn more about V5&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;V6: Current - Tool Library and MCP Integration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Comprehensive library of prebuilt tools, examples, and agent templates&lt;/li&gt; &#xA; &lt;li&gt;Integration with MCP servers for massive amounts of prebuilt tools&lt;/li&gt; &#xA; &lt;li&gt;Advisor agent that recommends relevant tools and examples based on user requirements&lt;/li&gt; &#xA; &lt;li&gt;Automatic incorporation of prebuilt components into new agents&lt;/li&gt; &#xA; &lt;li&gt;Specialized tools refiner agent also validates and optimizes MCP server configurations&lt;/li&gt; &#xA; &lt;li&gt;Streamlined access to external services through MCP integration&lt;/li&gt; &#xA; &lt;li&gt;Reduced development time through component reuse&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v6-tool-library-integration/README.md&#34;&gt;Learn more about V6&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Future Iterations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;V7: LangGraph Documentation - Allow Archon to build Pydantic AI AND LangGraph agents&lt;/li&gt; &#xA; &lt;li&gt;V8: Self-Feedback Loop - Automated validation and error correction&lt;/li&gt; &#xA; &lt;li&gt;V9: Self Agent Execution - Testing and iterating on agents in an isolated environment&lt;/li&gt; &#xA; &lt;li&gt;V10: Multi-Framework Support - Framework-agnostic agent generation&lt;/li&gt; &#xA; &lt;li&gt;V11: Autonomous Framework Learning - Self-updating framework adapters&lt;/li&gt; &#xA; &lt;li&gt;V12: Advanced RAG Techniques - Enhanced retrieval and incorporation of framework documentation&lt;/li&gt; &#xA; &lt;li&gt;V13: MCP Agent Marketplace - Integrating Archon agents as MCP servers and publishing to marketplaces&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Future Integrations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LangSmith&lt;/li&gt; &#xA; &lt;li&gt;MCP marketplace&lt;/li&gt; &#xA; &lt;li&gt;Other frameworks besides Pydantic AI&lt;/li&gt; &#xA; &lt;li&gt;Other vector databases besides Supabase&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/coleam00/local-ai-packaged&#34;&gt;Local AI package&lt;/a&gt; for the agent environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Archon Agents Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The below diagram from the LangGraph studio is a visual representation of the Archon agent graph.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/public/ArchonGraph.png&#34; alt=&#34;Archon Graph&#34;&gt; &#xA;&lt;p&gt;The flow works like this:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You describe the initial AI agent you want to create&lt;/li&gt; &#xA; &lt;li&gt;The reasoner LLM creates the high level scope for the agent&lt;/li&gt; &#xA; &lt;li&gt;The primary coding agent uses the scope and documentation to create the initial agent&lt;/li&gt; &#xA; &lt;li&gt;Control is passed back to you to either give feedback or ask Archon to &#39;refine&#39; the agent autonomously&lt;/li&gt; &#xA; &lt;li&gt;If refining autonomously, the specialized agents are invoked to improve the prompt, tools, and agent configuration&lt;/li&gt; &#xA; &lt;li&gt;The primary coding agent is invoked again with either user or specialized agent feedback&lt;/li&gt; &#xA; &lt;li&gt;The process goes back to step 4 until you say the agent is complete&lt;/li&gt; &#xA; &lt;li&gt;Once the agent is complete, Archon spits out the full code again with instructions for running it&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;File Architecture&lt;/h2&gt; &#xA;&lt;h3&gt;Core Files&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;streamlit_ui.py&lt;/code&gt;: Comprehensive web interface for managing all aspects of Archon&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;graph_service.py&lt;/code&gt;: FastAPI service that handles the agentic workflow&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;run_docker.py&lt;/code&gt;: Script to build and run Archon Docker containers&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt;: Container definition for the main Archon application&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;MCP Integration&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mcp/&lt;/code&gt;: Model Context Protocol server implementation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;mcp_server.py&lt;/code&gt;: MCP server script for AI IDE integration&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt;: Container definition for the MCP server&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Archon Package&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;archon/&lt;/code&gt;: Core agent and workflow implementation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;archon_graph.py&lt;/code&gt;: LangGraph workflow definition and agent coordination&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pydantic_ai_coder.py&lt;/code&gt;: Main coding agent with RAG capabilities&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;refiner_agents/&lt;/code&gt;: Specialized agents for refining different aspects of the created agent &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;prompt_refiner_agent.py&lt;/code&gt;: Optimizes system prompts&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;tools_refiner_agent.py&lt;/code&gt;: Specializes in tool implementation&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;agent_refiner_agent.py&lt;/code&gt;: Refines agent configuration and dependencies&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;crawl_pydantic_ai_docs.py&lt;/code&gt;: Documentation crawler and processor&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Utilities&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;utils/&lt;/code&gt;: Utility functions and database setup &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt;: Shared utility functions&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;site_pages.sql&lt;/code&gt;: Database setup commands&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Workbench&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;workbench/&lt;/code&gt;: Created at runtime, files specific to your environment &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;env_vars.json&lt;/code&gt;: Environment variables defined in the UI are stored here (included in .gitignore, file is created automatically)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;logs.txt&lt;/code&gt;: Low level logs for all Archon processes go here&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;scope.md&lt;/code&gt;: The detailed scope document created by the reasoner model at the start of each Archon execution&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment Options&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docker Containers&lt;/strong&gt;: Run Archon in isolated containers with all dependencies included &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Main container: Runs the Streamlit UI and graph service&lt;/li&gt; &#xA;   &lt;li&gt;MCP container: Provides MCP server functionality for AI IDEs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local Python&lt;/strong&gt;: Run directly on your system with a Python virtual environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Docker Architecture&lt;/h3&gt; &#xA;&lt;p&gt;The Docker implementation consists of two containers:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Main Archon Container&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Runs the Streamlit UI on port 8501&lt;/li&gt; &#xA;   &lt;li&gt;Hosts the Graph Service on port 8100&lt;/li&gt; &#xA;   &lt;li&gt;Built from the root Dockerfile&lt;/li&gt; &#xA;   &lt;li&gt;Handles all agent functionality and user interactions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MCP Container&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Implements the Model Context Protocol for AI IDE integration&lt;/li&gt; &#xA;   &lt;li&gt;Built from the mcp/Dockerfile&lt;/li&gt; &#xA;   &lt;li&gt;Communicates with the main container&#39;s Graph Service&lt;/li&gt; &#xA;   &lt;li&gt;Provides a standardized interface for AI IDEs like Windsurf, Cursor, Cline, and Roo Code&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When running with Docker, the &lt;code&gt;run_docker.py&lt;/code&gt; script automates building and starting both containers with the proper configuration.&lt;/p&gt; &#xA;&lt;h2&gt;Database Setup&lt;/h2&gt; &#xA;&lt;p&gt;The Supabase database uses the following schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE site_pages (&#xA;    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),&#xA;    url TEXT,&#xA;    chunk_number INTEGER,&#xA;    title TEXT,&#xA;    summary TEXT,&#xA;    content TEXT,&#xA;    metadata JSONB,&#xA;    embedding VECTOR(1536) -- Adjust dimensions as necessary (i.e. 768 for nomic-embed-text)&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Streamlit UI provides an interface to set up this database structure automatically.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Whether you&#39;re fixing bugs, adding features, or improving documentation, please feel free to submit a Pull Request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For version-specific details:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v1-single-agent/README.md&#34;&gt;V1 Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v2-agentic-workflow/README.md&#34;&gt;V2 Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v3-mcp-support/README.md&#34;&gt;V3 Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v4-streamlit-ui-overhaul/README.md&#34;&gt;V4 Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v5-parallel-specialized-agents/README.md&#34;&gt;V5 Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/coleam00/Archon/main/iterations/v6-tool-library-integration/README.md&#34;&gt;V6 Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>deepsense-ai/ragbits</title>
    <updated>2025-06-07T01:34:54Z</updated>
    <id>tag:github.com,2025-06-07:/deepsense-ai/ragbits</id>
    <link href="https://github.com/deepsense-ai/ragbits" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Building blocks for rapid development of GenAI applications&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;🐰 Ragbits&lt;/h1&gt; &#xA; &lt;p&gt;&lt;em&gt;Building blocks for rapid development of GenAI applications&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://deepsense.ai/rd-hub/ragbits/&#34;&gt;Homepage&lt;/a&gt; | &lt;a href=&#34;https://ragbits.deepsense.ai&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://deepsense.ai/contact/&#34;&gt;Contact&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/ragbits&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/ragbits&#34; alt=&#34;PyPI - License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/ragbits&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/ragbits&#34; alt=&#34;PyPI - Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/ragbits&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/ragbits&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;🔨 Build Reliable &amp;amp; Scalable GenAI Apps&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Swap LLMs anytime&lt;/strong&gt; – Switch between &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/llms/use_llms/&#34;&gt;100+ LLMs via LiteLLM&lt;/a&gt; or run &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/llms/use_local_llms/&#34;&gt;local models&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type-safe LLM calls&lt;/strong&gt; – Use Python generics to &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/prompts/use_prompting/#how-to-configure-prompts-output-data-type&#34;&gt;enforce strict type safety&lt;/a&gt; in model interactions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Bring your own vector store&lt;/strong&gt; – Connect to &lt;a href=&#34;https://ragbits.deepsense.ai/api_reference/core/vector-stores/#ragbits.core.vector_stores.qdrant.QdrantVectorStore&#34;&gt;Qdrant&lt;/a&gt;, &lt;a href=&#34;https://ragbits.deepsense.ai/api_reference/core/vector-stores/#ragbits.core.vector_stores.pgvector.PgVectorStore&#34;&gt;PgVector&lt;/a&gt;, and more with built-in support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Developer tools included&lt;/strong&gt; – &lt;a href=&#34;https://ragbits.deepsense.ai/cli/main/#ragbits-vector-store&#34;&gt;Manage vector stores&lt;/a&gt;, query pipelines, and &lt;a href=&#34;https://ragbits.deepsense.ai/quickstart/quickstart1_prompts/#testing-the-prompt-from-the-cli&#34;&gt;test prompts from your terminal&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modular installation&lt;/strong&gt; – Install only what you need, reducing dependencies and improving performance.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;📚 Fast &amp;amp; Flexible RAG Processing&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ingest 20+ formats&lt;/strong&gt; – Process PDFs, HTML, spreadsheets, presentations, and more. Process data using &lt;a href=&#34;https://github.com/docling-project/docling&#34;&gt;Docling&lt;/a&gt;, &lt;a href=&#34;https://github.com/Unstructured-IO/unstructured&#34;&gt;Unstructured&lt;/a&gt; or create a custom parser.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Handle complex data&lt;/strong&gt; – Extract tables, images, and structured content with built-in VLMs support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Connect to any data source&lt;/strong&gt; – Use prebuilt connectors for S3, GCS, Azure, or implement your own.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scale ingestion&lt;/strong&gt; – Process large datasets quickly with &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/document_search/distributed_ingestion/#how-to-ingest-documents-in-a-distributed-fashion&#34;&gt;Ray-based parallel processing&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🚀 Deploy &amp;amp; Monitor with Confidence&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time observability&lt;/strong&gt; – Track performance with &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/project/use_tracing/#opentelemetry-trace-handler&#34;&gt;OpenTelemetry&lt;/a&gt; and &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/project/use_tracing/#cli-trace-handler&#34;&gt;CLI insights&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Built-in testing&lt;/strong&gt; – Validate prompts &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/prompts/promptfoo/&#34;&gt;with promptfoo&lt;/a&gt; before deployment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Auto-optimization&lt;/strong&gt; – Continuously evaluate and refine model performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat UI&lt;/strong&gt; – Deploy &lt;a href=&#34;https://ragbits.deepsense.ai/how-to/chatbots/api/&#34;&gt;chatbot interface&lt;/a&gt; with API, persistance and user feedback.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To get started quickly, you can install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install ragbits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is a starter bundle of packages, containing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-core&#34;&gt;&lt;code&gt;ragbits-core&lt;/code&gt;&lt;/a&gt; - fundamental tools for working with prompts, LLMs and vector databases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-agents&#34;&gt;&lt;code&gt;ragbits-agents&lt;/code&gt;&lt;/a&gt; - abstractions for building agentic systems.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-document-search&#34;&gt;&lt;code&gt;ragbits-document-search&lt;/code&gt;&lt;/a&gt; - retrieval and ingestion piplines for knowledge bases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-evaluate&#34;&gt;&lt;code&gt;ragbits-evaluate&lt;/code&gt;&lt;/a&gt; - unified evaluation framework for Ragbits components.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-chat&#34;&gt;&lt;code&gt;ragbits-chat&lt;/code&gt;&lt;/a&gt; - full-stack infrastructure for building conversational AI applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-cli&#34;&gt;&lt;code&gt;ragbits-cli&lt;/code&gt;&lt;/a&gt; - &lt;code&gt;ragbits&lt;/code&gt; shell command for interacting with Ragbits components.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Alternatively, you can use individual components of the stack by installing their respective packages.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Basics&lt;/h3&gt; &#xA;&lt;p&gt;To define a prompt and run LLM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from pydantic import BaseModel&#xA;from ragbits.core.llms import LiteLLM&#xA;from ragbits.core.prompt import Prompt&#xA;&#xA;class QuestionAnswerPromptInput(BaseModel):&#xA;    question: str&#xA;&#xA;class QuestionAnswerPromptOutput(BaseModel):&#xA;    answer: str&#xA;&#xA;class QuestionAnswerPrompt(Prompt[QuestionAnswerPromptInput, QuestionAnswerPromptOutput]):&#xA;    system_prompt = &#34;&#34;&#34;&#xA;    You are a question answering agent. Answer the question to the best of your ability.&#xA;    &#34;&#34;&#34;&#xA;    user_prompt = &#34;&#34;&#34;&#xA;    Question: {{ question }}&#xA;    &#34;&#34;&#34;&#xA;&#xA;llm = LiteLLM(model_name=&#34;gpt-4.1-nano&#34;, use_structured_output=True)&#xA;&#xA;async def main() -&amp;gt; None:&#xA;    prompt = QuestionAnswerPrompt(QuestionAnswerPromptInput(question=&#34;What are high memory and low memory on linux?&#34;))&#xA;    response = await llm.generate(prompt)&#xA;    print(response.answer)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Document Search&lt;/h3&gt; &#xA;&lt;p&gt;To build and query a simple vector store index:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from ragbits.core.embeddings import LiteLLMEmbedder&#xA;from ragbits.core.vector_stores import InMemoryVectorStore&#xA;from ragbits.document_search import DocumentSearch&#xA;&#xA;embedder = LiteLLMEmbedder(model_name=&#34;text-embedding-3-small&#34;)&#xA;vector_store = InMemoryVectorStore(embedder=embedder)&#xA;document_search = DocumentSearch(vector_store=vector_store)&#xA;&#xA;async def run() -&amp;gt; None:&#xA;    await document_search.ingest(&#34;web://https://arxiv.org/pdf/1706.03762&#34;)&#xA;    result = await document_search.search(&#34;What are the key findings presented in this paper?&#34;)&#xA;    print(result)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(run())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retrieval-Augmented Generation&lt;/h3&gt; &#xA;&lt;p&gt;To build a simple RAG pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import asyncio&#xA;from pydantic import BaseModel&#xA;from ragbits.core.embeddings import LiteLLMEmbedder&#xA;from ragbits.core.llms import LiteLLM&#xA;from ragbits.core.prompt import Prompt&#xA;from ragbits.core.vector_stores import InMemoryVectorStore&#xA;from ragbits.document_search import DocumentSearch&#xA;&#xA;class QuestionAnswerPromptInput(BaseModel):&#xA;    question: str&#xA;    context: list[str]&#xA;&#xA;class QuestionAnswerPromptOutput(BaseModel):&#xA;    answer: str&#xA;&#xA;class QuestionAnswerPrompt(Prompt[QuestionAnswerPromptInput, QuestionAnswerPromptOutput]):&#xA;    system_prompt = &#34;&#34;&#34;&#xA;    You are a question answering agent. Answer the question that will be provided using context.&#xA;    If in the given context there is not enough information refuse to answer.&#xA;    &#34;&#34;&#34;&#xA;    user_prompt = &#34;&#34;&#34;&#xA;    Question: {{ question }}&#xA;    Context: {% for item in context %}&#xA;        {{ item }}&#xA;    {%- endfor %}&#xA;    &#34;&#34;&#34;&#xA;&#xA;embedder = LiteLLMEmbedder(model_name=&#34;text-embedding-3-small&#34;)&#xA;vector_store = InMemoryVectorStore(embedder=embedder)&#xA;document_search = DocumentSearch(vector_store=vector_store)&#xA;llm = LiteLLM(model_name=&#34;gpt-4.1-nano&#34;, use_structured_output=True)&#xA;&#xA;async def run() -&amp;gt; None:&#xA;    question = &#34;What are the key findings presented in this paper?&#34;&#xA;&#xA;    await document_search.ingest(&#34;web://https://arxiv.org/pdf/1706.03762&#34;)&#xA;    result = await document_search.search(question)&#xA;&#xA;    prompt = QuestionAnswerPrompt(QuestionAnswerPromptInput(&#xA;        question=question,&#xA;        context=[element.text_representation for element in result],&#xA;    ))&#xA;    response = await llm.generate(prompt)&#xA;    print(response.answer)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    asyncio.run(run())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chatbot interface with UI&lt;/h3&gt; &#xA;&lt;p&gt;To expose your RAG application through Ragbits UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections.abc import AsyncGenerator&#xA;&#xA;from pydantic import BaseModel&#xA;&#xA;from ragbits.chat.api import RagbitsAPI&#xA;from ragbits.chat.interface import ChatInterface&#xA;from ragbits.chat.interface.types import ChatContext, ChatResponse&#xA;from ragbits.core.embeddings import LiteLLMEmbedder&#xA;from ragbits.core.llms import LiteLLM&#xA;from ragbits.core.prompt import Prompt&#xA;from ragbits.core.prompt.base import ChatFormat&#xA;from ragbits.core.vector_stores import InMemoryVectorStore&#xA;from ragbits.document_search import DocumentSearch&#xA;&#xA;&#xA;class QuestionAnswerPromptInput(BaseModel):&#xA;    question: str&#xA;    context: list[str]&#xA;&#xA;&#xA;class QuestionAnswerPrompt(Prompt[QuestionAnswerPromptInput, str]):&#xA;    system_prompt = &#34;&#34;&#34;&#xA;    You are a question answering agent. Answer the question that will be provided using context.&#xA;    If in the given context there is not enough information refuse to answer.&#xA;    &#34;&#34;&#34;&#xA;    user_prompt = &#34;&#34;&#34;&#xA;    Question: {{ question }}&#xA;    Context: {% for item in context %}{{ item }}{%- endfor %}&#xA;    &#34;&#34;&#34;&#xA;&#xA;&#xA;class MyChat(ChatInterface):&#xA;    &#34;&#34;&#34;Chat interface for fullapp application.&#34;&#34;&#34;&#xA;&#xA;    async def setup(self) -&amp;gt; None:&#xA;        self.embedder = LiteLLMEmbedder(model_name=&#34;text-embedding-3-small&#34;)&#xA;        self.vector_store = InMemoryVectorStore(embedder=self.embedder)&#xA;        self.document_search = DocumentSearch(vector_store=self.vector_store)&#xA;        self.llm = LiteLLM(model_name=&#34;gpt-4.1-nano&#34;, use_structured_output=True)&#xA;&#xA;        await self.document_search.ingest(&#34;web://https://arxiv.org/pdf/1706.03762&#34;)&#xA;&#xA;    async def chat(&#xA;        self,&#xA;        message: str,&#xA;        history: ChatFormat | None = None,&#xA;        context: ChatContext | None = None,&#xA;    ) -&amp;gt; AsyncGenerator[ChatResponse, None]:&#xA;        # Search for relevant documents&#xA;        result = await self.document_search.search(message)&#xA;&#xA;        prompt = QuestionAnswerPrompt(&#xA;            QuestionAnswerPromptInput(&#xA;                question=message,&#xA;                context=[element.text_representation for element in result],&#xA;            )&#xA;        )&#xA;&#xA;        # Stream the response from the LLM&#xA;        async for chunk in self.llm.generate_streaming(prompt):&#xA;            yield self.create_text_response(chunk)&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    RagbitsAPI(MyChat).run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Rapid development&lt;/h2&gt; &#xA;&lt;p&gt;Create Ragbits projects from templates:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uvx create-ragbits-app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Explore &lt;code&gt;create-ragbits-app&lt;/code&gt; repo &lt;a href=&#34;https://github.com/deepsense-ai/create-ragbits-app&#34;&gt;here&lt;/a&gt;. If you have a new idea for a template, feel free to contribute!&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragbits.deepsense.ai/quickstart/quickstart1_prompts/&#34;&gt;Quickstart&lt;/a&gt; - Get started with Ragbits in a few minutes&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragbits.deepsense.ai/how-to/prompts/use_prompting/&#34;&gt;How-to&lt;/a&gt; - Learn how to use Ragbits in your projects&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragbits.deepsense.ai/cli/main/&#34;&gt;CLI&lt;/a&gt; - Learn how to run Ragbits in your terminal&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ragbits.deepsense.ai/api_reference/core/prompt/&#34;&gt;API reference&lt;/a&gt; - Explore the underlying Ragbits API&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! Please read &lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Ragbits is licensed under the &lt;a href=&#34;https://github.com/deepsense-ai/ragbits/tree/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>