<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-25T01:34:40Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>allenai/open-instruct</title>
    <updated>2024-11-25T01:34:40Z</updated>
    <id>tag:github.com,2024-11-25:/allenai/open-instruct</id>
    <link href="https://github.com/allenai/open-instruct" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Training Open Instruction-Following Language Models&lt;/h1&gt; &#xA;&lt;p&gt;This repo serves as an open effort on instruction-tuning popular pretrained language models on publicly available datasets. We release this repo and will keep updating it with:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Code for finetuning language models with latest techniques and instruction datasets in a unified format.&lt;/li&gt; &#xA; &lt;li&gt;Code for running standard evaluation on a range of benchmarks, targeting for differnt capabilities of these language models.&lt;/li&gt; &#xA; &lt;li&gt;Checkpoints or other useful artifacts that we build in our exploration.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Please see our first paper &lt;a href=&#34;https://arxiv.org/abs/2306.04751&#34;&gt;How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources&lt;/a&gt; for more thoughts behind this project and our initial findings. Please see our second paper &lt;a href=&#34;https://arxiv.org/abs/2311.10702&#34;&gt;Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2&lt;/a&gt; for results using Llama-2 models and direct preference optimization. We are still working on more models. For more recent results involving PPO and DPO please see our third paper &lt;a href=&#34;https://arxiv.org/abs/2406.09279&#34;&gt;Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/assets/images/tulu_logo.png&#34; alt=&#34;Tülu (a hybrid camel) represents a suite of LLaMa models that we built by fully-finetuning them on a strong mix of datasets.&#34; style=&#34;width: 20%; min-width: 200px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024-07-01] We released &lt;a href=&#34;https://arxiv.org/abs/2406.09279&#34;&gt;Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback&lt;/a&gt; and have majorly updated our codebase to support new models and package versions.&lt;/li&gt; &#xA; &lt;li&gt;[2023-11-27] We released &lt;a href=&#34;https://arxiv.org/abs/2311.10702&#34;&gt;Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2&lt;/a&gt;. Check out our models &lt;a href=&#34;https://huggingface.co/collections/allenai/tulu-v2-suite-6551b56e743e6349aab45101&#34;&gt;here&lt;/a&gt;. We have added a DPO finetuning script for replicating our results.&lt;/li&gt; &#xA; &lt;li&gt;[2023-09-26] We switched to use the official &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval&#34;&gt;alpaca-eval&lt;/a&gt; library to run AlpacaFarm evaluation but use regenerated longer reference outputs. This will change our numbers reported in the paper. We will update the paper soon.&lt;/li&gt; &#xA; &lt;li&gt;[2023-09-25] Supported using &lt;a href=&#34;https://github.com/vllm-project/vllm/&#34;&gt;vLLM&lt;/a&gt; for our evaluations, which speeds up the evaluation by 10x.&lt;/li&gt; &#xA; &lt;li&gt;[2023-09-17] Supported &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA&lt;/a&gt; finetuning. See &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/#parameter-efficient-finetuning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[2023-08-18] Added support for &lt;a href=&#34;https://github.com/microsoft/TOXIGEN&#34;&gt;ToxiGen&lt;/a&gt;/&lt;a href=&#34;https://github.com/sylinrl/TruthfulQA&#34;&gt;TruthfulQA&lt;/a&gt; evaluation. Check our &lt;code&gt;scripts/eval/&lt;/code&gt; for examples of running them.&lt;/li&gt; &#xA; &lt;li&gt;[2023-08-08] Supported several new instruction dataset, including &lt;a href=&#34;https://huggingface.co/datasets/GAIR/lima&#34;&gt;LIMA&lt;/a&gt; / &lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/OpenOrca&#34;&gt;Open-Orca&lt;/a&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/scripts/data/prepare_train_data.sh&#34;&gt;preparation script&lt;/a&gt; for details. Performance hasn&#39;t been evaluated yet.&lt;/li&gt; &#xA; &lt;li&gt;[2023-08-06] Supported LLaMa 2 finetuning and FlashAttention-2 by bumping the version of transformers and many other dependencies.&lt;/li&gt; &#xA; &lt;li&gt;[2023-06-29] Added &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/#licensing&#34;&gt;licensing info&lt;/a&gt; for our released models.&lt;/li&gt; &#xA; &lt;li&gt;[2023-06-09] Released Tülu (a suite of LLaMa models fully-finetuned on a strong mix of datasets) and many other checkpoints on HuggingFace &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/#released-checkpoints&#34;&gt;[Links]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023-06-09] Initial release of the codebase containing the training and evaluation code for our &lt;a href=&#34;https://arxiv.org/abs/2306.04751&#34;&gt;arxiv paper&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our setup mostly follows our &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt;, which uses Python 3.10. &lt;em&gt;Note that Open Instruct is a research codebase and does not guarantee backward compatibility.&lt;/em&gt; We offer two installation strategies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local installation&lt;/strong&gt;: This is the recommended way to install Open Instruct. You can install the dependencies by running the following commands:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip &#34;setuptools&amp;lt;70.0.0&#34; wheel &#xA;# TODO, unpin setuptools when this issue in flash attention is resolved&#xA;pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121&#xA;pip install packaging&#xA;pip install flash-attn==2.6.3 --no-build-isolation&#xA;pip install -r requirements.txt&#xA;python -m nltk.downloader punkt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docker installation&lt;/strong&gt;: You can also use the Dockerfile to build a Docker image. You can build the image with the following command:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build --build-arg CUDA=12.1.0 --build-arg TARGET=cudnn8-devel --build-arg DIST=ubuntu20.04 . -t open_instruct_dev&#xA;# if you are interally at AI2, you can create an image like this:&#xA;beaker image delete $(whoami)/open_instruct_dev &#xA;beaker image create open_instruct_dev -n open_instruct_dev -w ai2/$(whoami)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are internally at AI2, you may launch experiments using our always-up-to-date auto-built image &lt;code&gt;nathanl/open_instruct_auto&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;After having setup the environment, you are ready to launch some experiments. We provide a few examples below. To learn more about how to reproduce the Tulu 3 models, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/docs/tulu3.md&#34;&gt;Tulu 3 README&lt;/a&gt;. The instructions and documentations for Tulu 1 and Tulu 2 are in &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/docs/tulu1_tulu2.md&#34;&gt;Tulu 1 and 2 README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;You can run the following commands for getting started:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# quick debugging run using 1 GPU&#xA;sh scripts/finetune_with_accelerate_config.sh 1 configs/train_configs/sft/mini.yaml&#xA;# train an 8B tulu3 model using 8 GPU&#xA;sh scripts/finetune_with_accelerate_config.sh 8 configs/train_configs/tulu3/tulu3_sft.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Preference Tuning&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# quick debugging run using 1 GPU&#xA;sh scripts/dpo_train_with_accelerate_config.sh 1 configs/train_configs/dpo/mini.yaml&#xA;# train an 8B tulu3 model using 8 GPU&#xA;sh scripts/finetune_with_accelerate_config.sh 8 configs/train_configs/tulu3/tulu3_dpo_8b.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;RLVR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# quick debugging run using 2 GPU (1 for inference, 1 for training)&#xA;# here we are using `HuggingFaceTB/SmolLM2-360M-Instruct`; it&#39;s prob not&#xA;# gonna work, but it&#39;s easy to test run and print stuff.&#xA;python open_instruct/ppo_vllm_thread_ray_gtrl.py \&#xA;    --dataset_mixer &#39;{&#34;ai2-adapt-dev/gsm8k_math_ifeval_ground_truth_mixed&#34;: 1.0}&#39; \&#xA;    --dataset_train_splits train \&#xA;    --dataset_eval_mixer &#39;{&#34;ai2-adapt-dev/gsm8k_math_ground_truth&#34;: 1.0}&#39; \&#xA;    --dataset_eval_splits test \&#xA;    --max_token_length 2048 \&#xA;    --max_prompt_token_length 2048 \&#xA;    --response_length 2048 \&#xA;    --model_name_or_path HuggingFaceTB/SmolLM2-360M-Instruct \&#xA;    --reward_model_path HuggingFaceTB/SmolLM2-360M-Instruct \&#xA;    --non_stop_penalty \&#xA;    --stop_token eos \&#xA;    --temperature 1.0 \&#xA;    --ground_truths_key ground_truth \&#xA;    --chat_template tulu \&#xA;    --sft_messages_key messages \&#xA;    --learning_rate 3e-7 \&#xA;    --total_episodes 10000 \&#xA;    --penalty_reward_value -10.0 \&#xA;    --deepspeed_stage 3 \&#xA;    --per_device_train_batch_size 2 \&#xA;    --local_rollout_forward_batch_size 2 \&#xA;    --local_mini_batch_size 32 \&#xA;    --local_rollout_batch_size 32 \&#xA;    --num_epochs 1 \&#xA;    --actor_num_gpus_per_node 1 \&#xA;    --vllm_tensor_parallel_size 1 \&#xA;    --beta 0.05 \&#xA;    --apply_verifiable_reward true \&#xA;    --output_dir output/rlvr_1b \&#xA;    --seed 3 \&#xA;    --num_evals 3 \&#xA;    --save_freq 100 \&#xA;    --reward_model_multiplier 0.0 \&#xA;    --gradient_checkpointing \&#xA;    --with_tracking&#xA;&#xA;# train an 8B tulu3 model using 8 GPU (1 for inference, 7 for training)&#xA;python open_instruct/ppo_vllm_thread_ray_gtrl.py \&#xA;    --dataset_mixer &#39;{&#34;ai2-adapt-dev/gsm8k_math_ifeval_ground_truth_mixed&#34;: 1.0}&#39; \&#xA;    --dataset_train_splits train \&#xA;    --dataset_eval_mixer &#39;{&#34;ai2-adapt-dev/gsm8k_math_ground_truth&#34;: 1.0}&#39; \&#xA;    --dataset_eval_splits test \&#xA;    --max_token_length 2048 \&#xA;    --max_prompt_token_length 2048 \&#xA;    --response_length 2048 \&#xA;    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \&#xA;    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \&#xA;    --non_stop_penalty \&#xA;    --stop_token eos \&#xA;    --temperature 1.0 \&#xA;    --ground_truths_key ground_truth \&#xA;    --chat_template tulu \&#xA;    --sft_messages_key messages \&#xA;    --learning_rate 3e-7 \&#xA;    --total_episodes 10000000 \&#xA;    --penalty_reward_value -10.0 \&#xA;    --deepspeed_stage 3 \&#xA;    --per_device_train_batch_size 2 \&#xA;    --local_rollout_forward_batch_size 2 \&#xA;    --local_mini_batch_size 32 \&#xA;    --local_rollout_batch_size 32 \&#xA;    --actor_num_gpus_per_node 7 \&#xA;    --vllm_tensor_parallel_size 1 \&#xA;    --beta 0.05 \&#xA;    --apply_verifiable_reward true \&#xA;    --output_dir output/rlvr_8b \&#xA;    --seed 3 \&#xA;    --num_evals 3 \&#xA;    --save_freq 100 \&#xA;    --reward_model_multiplier 0.0 \&#xA;    --gradient_checkpointing \&#xA;    --with_tracking&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contamination checks&lt;/h2&gt; &#xA;&lt;p&gt;We release our scripts for measuring the overlap between instruction tuning datasets and evaluation datasets in &lt;code&gt;./decontamination&lt;/code&gt;. See the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/decontamination/README.md&#34;&gt;README&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Developing&lt;/h3&gt; &#xA;&lt;p&gt;When submitting a PR to this repo, we check the core code in &lt;code&gt;open_instruct/&lt;/code&gt; for style with the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make style&#xA;make quality&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Repo structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── assets/                     &amp;lt;- Images, licenses, etc.&#xA;├── configs/                    &#xA;|     ├── beaker_configs/       &amp;lt;- AI2 Beaker configs&#xA;|     ├── ds_configs/           &amp;lt;- DeepSpeed configs&#xA;|     └── train_configs/        &amp;lt;- Training configs&#xA;├── decontamination/            &amp;lt;- Scripts for measuring train-eval overlap&#xA;├── eval/                       &amp;lt;- Evaluation suite for fine-tuned models&#xA;├── human_eval/                 &amp;lt;- Human evaluation interface (not maintained)&#xA;├── open_instruct/              &amp;lt;- Source code (flat)&#xA;├── quantize/                   &amp;lt;- Scripts for quantization&#xA;├── scripts/                    &amp;lt;- Core training and evaluation scripts&#xA;└── Dockerfile                  &amp;lt;- Dockerfile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;This codebase is licensed under Apache 2.0 as given in &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The license we use for V1 models released (along with the base model licenses) can be found in &lt;a href=&#34;https://raw.githubusercontent.com/allenai/open-instruct/main/assets/model_licenses/tulu_license.txt&#34;&gt;assets/model_licenses/tulu_license.txt&lt;/a&gt; - just replace &lt;code&gt;&amp;lt;MODELNAME&amp;gt;&lt;/code&gt; with the actual model name (i.e., the name on HuggingFace).&lt;/p&gt; &#xA;&lt;p&gt;V2 models are licensed under the &lt;a href=&#34;https://allenai.org/licenses/impact-lr&#34;&gt;low-risk AI2 ImpACT license&lt;/a&gt;. See &lt;a href=&#34;https://allenai.org/impact-license&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Open Instruct is a project that benefitd from many open-source projects and libraries. We would like to particularly thank the folloiwng projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;HuggingFace Transformers&lt;/a&gt;: We adapted Hugging Face&#39;s Trainer for our finetuning scripts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;HuggingFace TRL&lt;/a&gt; and &lt;a href=&#34;https://github.com/eric-mitchell/direct-preference-optimization&#34;&gt;eric-mitchell/direct-preference-optimization&lt;/a&gt;: our preference tuning code is adapted from TRL and from Eric Mitchell&#39;s DPO code.&lt;/li&gt; &#xA; &lt;li&gt;OpenAI&#39;s &lt;a href=&#34;https://github.com/openai/lm-human-preferences&#34;&gt;lm-human-preferences&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/summarize-from-feedback&#34;&gt;summarize-from-feedback&lt;/a&gt;, and &lt;a href=&#34;https://github.com/vwxyzjn/summarize_from_feedback_details&#34;&gt;vwxyzjn/summarize_from_feedback_details&lt;/a&gt;: Our core PPO code is adapted from OpenAI&#39;s original RLHF code and &lt;a href=&#34;https://openreview.net/forum?id=kHO2ZTa8e3&#34;&gt;Huang et al (2024)&#39;s reproduction work&lt;/a&gt; of OpenAI&#39;s summarize from feedback work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenRLHF/OpenRLHF&#34;&gt;OpenRLHF&lt;/a&gt;: We adapted OpenRLHF&#39;s Ray + vLLM distributed code for scaling up PPO RLVR training into the 70B scale.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you used this repository or our models, please cite our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{wang2023far,&#xA;   title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}, &#xA;   author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},&#xA;   year={2023},&#xA;   eprint={2306.04751},&#xA;   archivePrefix={arXiv},&#xA;   primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ivison2023camels,&#xA;      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, &#xA;      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},&#xA;      year={2023},&#xA;      eprint={2311.10702},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{ivison2024unpacking,&#xA;      title={Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback}, &#xA;      author={Hamish Ivison and Yizhong Wang and Jiacheng Liu and Zeqiu Wu and Valentina Pyatkin and Nathan Lambert and Noah A. Smith and Yejin Choi and Hannaneh Hajishirzi},&#xA;      year={2024},&#xA;      eprint={2406.09279},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>