<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-04T01:33:48Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mandiant/capa</title>
    <updated>2024-10-04T01:33:48Z</updated>
    <id>tag:github.com,2024-10-04:/mandiant/capa</id>
    <link href="https://github.com/mandiant/capa" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The FLARE team&#39;s open-source tool to identify capabilities in executable files.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://mandiant.github.io/capa/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/mandiant/capa/raw/master/.github/logo.png&#34;&gt; &lt;/a&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://mandiant.github.io/capa/&#34; target=&#34;_blank&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://github.com/mandiant/capa/releases/latest&#34; target=&#34;_blank&#34;&gt;Download&lt;/a&gt; | &lt;a href=&#34;https://mandiant.github.io/capa/explorer/&#34; target=&#34;_blank&#34;&gt;Web Interface&lt;/a&gt; &lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://pypi.org/project/flare-capa&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/flare-capa&#34; alt=&#34;PyPI - Python Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mandiant/capa/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/mandiant/capa&#34; alt=&#34;Last release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mandiant/capa-rules&#34;&gt;&lt;img src=&#34;https://gist.githubusercontent.com/capa-bot/6d7960e911f48b3b74916df8988cf0f3/raw/rules_badge.svg?sanitize=true&#34; alt=&#34;Number of rules&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mandiant/capa/actions?query=workflow%3ACI+event%3Apush+branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/mandiant/capa/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mandiant/capa/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/mandiant/capa/total&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/mandiant/capa/master/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;capa detects capabilities in executable files. You run it against a PE, ELF, .NET module, shellcode file, or a sandbox report and it tells you what it thinks the program can do. For example, it might suggest that the file is a backdoor, is capable of installing services, or relies on HTTP to communicate.&lt;/p&gt; &#xA;&lt;p&gt;To interactively inspect capa results in your browser use the &lt;a href=&#34;https://mandiant.github.io/capa/explorer/&#34;&gt;capa Explorer Web&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want to inspect or write capa rules, head on over to the &lt;a href=&#34;https://github.com/mandiant/capa-rules&#34;&gt;capa-rules repository&lt;/a&gt;. Otherwise, keep reading.&lt;/p&gt; &#xA;&lt;p&gt;Below you find a list of &lt;a href=&#34;https://raw.githubusercontent.com/mandiant/capa/master/#blog-posts&#34;&gt;our capa blog posts with more details.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;example capa output&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ capa.exe suspicious.exe&#xA;&#xA;+------------------------+--------------------------------------------------------------------------------+&#xA;| ATT&amp;amp;CK Tactic          | ATT&amp;amp;CK Technique                                                               |&#xA;|------------------------+--------------------------------------------------------------------------------|&#xA;| DEFENSE EVASION        | Obfuscated Files or Information [T1027]                                        |&#xA;| DISCOVERY              | Query Registry [T1012]                                                         |&#xA;|                        | System Information Discovery [T1082]                                           |&#xA;| EXECUTION              | Command and Scripting Interpreter::Windows Command Shell [T1059.003]           |&#xA;|                        | Shared Modules [T1129]                                                         |&#xA;| EXFILTRATION           | Exfiltration Over C2 Channel [T1041]                                           |&#xA;| PERSISTENCE            | Create or Modify System Process::Windows Service [T1543.003]                   |&#xA;+------------------------+--------------------------------------------------------------------------------+&#xA;&#xA;+-------------------------------------------------------+-------------------------------------------------+&#xA;| CAPABILITY                                            | NAMESPACE                                       |&#xA;|-------------------------------------------------------+-------------------------------------------------|&#xA;| check for OutputDebugString error                     | anti-analysis/anti-debugging/debugger-detection |&#xA;| read and send data from client to server              | c2/file-transfer                                |&#xA;| execute shell command and capture output              | c2/shell                                        |&#xA;| receive data (2 matches)                              | communication                                   |&#xA;| send data (6 matches)                                 | communication                                   |&#xA;| connect to HTTP server (3 matches)                    | communication/http/client                       |&#xA;| send HTTP request (3 matches)                         | communication/http/client                       |&#xA;| create pipe                                           | communication/named-pipe/create                 |&#xA;| get socket status (2 matches)                         | communication/socket                            |&#xA;| receive data on socket (2 matches)                    | communication/socket/receive                    |&#xA;| send data on socket (3 matches)                       | communication/socket/send                       |&#xA;| connect TCP socket                                    | communication/socket/tcp                        |&#xA;| encode data using Base64                              | data-manipulation/encoding/base64               |&#xA;| encode data using XOR (6 matches)                     | data-manipulation/encoding/xor                  |&#xA;| run as a service                                      | executable/pe                                   |&#xA;| get common file path (3 matches)                      | host-interaction/file-system                    |&#xA;| read file                                             | host-interaction/file-system/read               |&#xA;| write file (2 matches)                                | host-interaction/file-system/write              |&#xA;| print debug messages (2 matches)                      | host-interaction/log/debug/write-event          |&#xA;| resolve DNS                                           | host-interaction/network/dns/resolve            |&#xA;| get hostname                                          | host-interaction/os/hostname                    |&#xA;| create a process with modified I/O handles and window | host-interaction/process/create                 |&#xA;| create process                                        | host-interaction/process/create                 |&#xA;| create registry key                                   | host-interaction/registry/create                |&#xA;| create service                                        | host-interaction/service/create                 |&#xA;| create thread                                         | host-interaction/thread/create                  |&#xA;| persist via Windows service                           | persistence/service                             |&#xA;+-------------------------------------------------------+-------------------------------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;download and usage&lt;/h1&gt; &#xA;&lt;p&gt;Download stable releases of the standalone capa binaries &lt;a href=&#34;https://github.com/mandiant/capa/releases&#34;&gt;here&lt;/a&gt;. You can run the standalone binaries without installation. capa is a command line tool that should be run from the terminal.&lt;/p&gt; &#xA;&lt;p&gt;To use capa as a library or integrate with another tool, see &lt;a href=&#34;https://github.com/mandiant/capa/raw/master/doc/installation.md&#34;&gt;doc/installation.md&lt;/a&gt; for further setup instructions.&lt;/p&gt; &#xA;&lt;h1&gt;capa Explorer Web&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://mandiant.github.io/capa/explorer/&#34;&gt;capa Explorer Web&lt;/a&gt; enables you to interactively explore capa results in your web browser. Besides the online version you can download a standalone HTML file for local offline usage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mandiant/capa/raw/master/doc/img/capa_web_explorer.png&#34; alt=&#34;capa Explorer Web screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More details on the web UI is available in the &lt;a href=&#34;https://github.com/mandiant/capa/raw/master/web/explorer/README.md&#34;&gt;capa Explorer Web README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;example&lt;/h1&gt; &#xA;&lt;p&gt;In the above sample output, we run capa against an unknown binary (&lt;code&gt;suspicious.exe&lt;/code&gt;), and the tool reports that the program can send HTTP requests, decode data via XOR and Base64, install services, and spawn new processes. Taken together, this makes us think that &lt;code&gt;suspicious.exe&lt;/code&gt; could be a persistent backdoor. Therefore, our next analysis step might be to run &lt;code&gt;suspicious.exe&lt;/code&gt; in a sandbox and try to recover the command and control server.&lt;/p&gt; &#xA;&lt;h2&gt;detailed results&lt;/h2&gt; &#xA;&lt;p&gt;By passing the &lt;code&gt;-vv&lt;/code&gt; flag (for very verbose), capa reports exactly where it found evidence of these capabilities. This is useful for at least two reasons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;it helps explain why we should trust the results, and enables us to verify the conclusions, and&lt;/li&gt; &#xA; &lt;li&gt;it shows where within the binary an experienced analyst might study with IDA Pro&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ capa.exe suspicious.exe -vv&#xA;...&#xA;execute shell command and capture output&#xA;namespace   c2/shell&#xA;author      matthew.williams@mandiant.com&#xA;scope       function&#xA;att&amp;amp;ck      Execution::Command and Scripting Interpreter::Windows Command Shell [T1059.003]&#xA;references  https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/ns-processthreadsapi-startupinfoa&#xA;function @ 0x4011C0&#xA;  and:&#xA;    match: create a process with modified I/O handles and window @ 0x4011C0&#xA;      and:&#xA;        number: 257 = STARTF_USESTDHANDLES | STARTF_USESHOWWINDOW @ 0x4012B8&#xA;        or:&#xA;          number: 68 = StartupInfo.cb (size) @ 0x401282&#xA;        or: = API functions that accept a pointer to a STARTUPINFO structure&#xA;          api: kernel32.CreateProcess @ 0x401343&#xA;    match: create pipe @ 0x4011C0&#xA;      or:&#xA;        api: kernel32.CreatePipe @ 0x40126F, 0x401280&#xA;    optional:&#xA;      match: create thread @ 0x40136A, 0x4013BA&#xA;        or:&#xA;          and:&#xA;            os: windows&#xA;            or:&#xA;              api: kernel32.CreateThread @ 0x4013D7&#xA;        or:&#xA;          and:&#xA;            os: windows&#xA;            or:&#xA;              api: kernel32.CreateThread @ 0x401395&#xA;    or:&#xA;      string: &#34;cmd.exe&#34; @ 0x4012FD&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;capa also supports dynamic capabilities detection for multiple sandboxes including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kevoreilly/CAPEv2&#34;&gt;CAPE&lt;/a&gt; (supported report formats: &lt;code&gt;.json&lt;/code&gt;, &lt;code&gt;.json_&lt;/code&gt;, &lt;code&gt;.json.gz&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CERT-Polska/drakvuf-sandbox/&#34;&gt;DRAKVUF&lt;/a&gt; (supported report formats: &lt;code&gt;.log&lt;/code&gt;, &lt;code&gt;.log.gz&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.vmray.com/&#34;&gt;VMRay&lt;/a&gt; (supported report formats: analysis archive &lt;code&gt;.zip&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To use this feature, submit your file to a supported sandbox and then download and run capa against the generated report file. This feature enables capa to match capabilities against dynamic and static features that the sandbox captured during execution.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example of running capa against a packed file, and then running capa against the CAPE report generated for the same packed file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ capa 05be49819139a3fdcdbddbdefd298398779521f3d68daa25275cc77508e42310.exe&#xA;WARNING:capa.capabilities.common:--------------------------------------------------------------------------------&#xA;WARNING:capa.capabilities.common: This sample appears to be packed.&#xA;WARNING:capa.capabilities.common: &#xA;WARNING:capa.capabilities.common: Packed samples have often been obfuscated to hide their logic.&#xA;WARNING:capa.capabilities.common: capa cannot handle obfuscation well using static analysis. This means the results may be misleading or incomplete.&#xA;WARNING:capa.capabilities.common: If possible, you should try to unpack this input file before analyzing it with capa.&#xA;WARNING:capa.capabilities.common: Alternatively, run the sample in a supported sandbox and invoke capa against the report to obtain dynamic analysis results.&#xA;WARNING:capa.capabilities.common: &#xA;WARNING:capa.capabilities.common: Identified via rule: (internal) packer file limitation&#xA;WARNING:capa.capabilities.common: &#xA;WARNING:capa.capabilities.common: Use -v or -vv if you really want to see the capabilities identified by capa.&#xA;WARNING:capa.capabilities.common:--------------------------------------------------------------------------------&#xA;&#xA;$ capa 05be49819139a3fdcdbddbdefd298398779521f3d68daa25275cc77508e42310.json&#xA;&#xA;┍━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑&#xA;│ ATT&amp;amp;CK Tactic          │ ATT&amp;amp;CK Technique                                                                   │&#xA;┝━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥&#xA;│ CREDENTIAL ACCESS      │ Credentials from Password Stores T1555                                             │&#xA;├────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┤&#xA;│ DEFENSE EVASION        │ File and Directory Permissions Modification T1222                                  │&#xA;│                        │ Modify Registry T1112                                                              │&#xA;│                        │ Obfuscated Files or Information T1027                                              │&#xA;│                        │ Virtualization/Sandbox Evasion::User Activity Based Checks T1497.002               │&#xA;├────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┤&#xA;│ DISCOVERY              │ Account Discovery T1087                                                            │&#xA;│                        │ Application Window Discovery T1010                                                 │&#xA;│                        │ File and Directory Discovery T1083                                                 │&#xA;│                        │ Query Registry T1012                                                               │&#xA;│                        │ System Information Discovery T1082                                                 │&#xA;│                        │ System Location Discovery::System Language Discovery T1614.001                     │&#xA;│                        │ System Owner/User Discovery T1033                                                  │&#xA;├────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┤&#xA;│ EXECUTION              │ System Services::Service Execution T1569.002                                       │&#xA;├────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┤&#xA;│ PERSISTENCE            │ Boot or Logon Autostart Execution::Registry Run Keys / Startup Folder T1547.001    │&#xA;│                        │ Boot or Logon Autostart Execution::Winlogon Helper DLL T1547.004                   │&#xA;│                        │ Create or Modify System Process::Windows Service T1543.003                         │&#xA;┕━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙&#xA;&#xA;┍━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑&#xA;│ Capability                                           │ Namespace                                            │&#xA;┝━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥&#xA;│ check for unmoving mouse cursor (3 matches)          │ anti-analysis/anti-vm/vm-detection                   │&#xA;│ gather bitkinex information                          │ collection/file-managers                             │&#xA;│ gather classicftp information                        │ collection/file-managers                             │&#xA;│ gather filezilla information                         │ collection/file-managers                             │&#xA;│ gather total-commander information                   │ collection/file-managers                             │&#xA;│ gather ultrafxp information                          │ collection/file-managers                             │&#xA;│ resolve DNS (23 matches)                             │ communication/dns                                    │&#xA;│ initialize Winsock library (7 matches)               │ communication/socket                                 │&#xA;│ act as TCP client (3 matches)                        │ communication/tcp/client                             │&#xA;│ create new key via CryptAcquireContext               │ data-manipulation/encryption                         │&#xA;│ encrypt or decrypt via WinCrypt                      │ data-manipulation/encryption                         │&#xA;│ hash data via WinCrypt                               │ data-manipulation/hashing                            │&#xA;│ initialize hashing via WinCrypt                      │ data-manipulation/hashing                            │&#xA;│ hash data with MD5                                   │ data-manipulation/hashing/md5                        │&#xA;│ generate random numbers via WinAPI                   │ data-manipulation/prng                               │&#xA;│ extract resource via kernel32 functions (2 matches)  │ executable/resource                                  │&#xA;│ interact with driver via control codes (2 matches)   │ host-interaction/driver                              │&#xA;│ get Program Files directory (18 matches)             │ host-interaction/file-system                         │&#xA;│ get common file path (575 matches)                   │ host-interaction/file-system                         │&#xA;│ create directory (2 matches)                         │ host-interaction/file-system/create                  │&#xA;│ delete file                                          │ host-interaction/file-system/delete                  │&#xA;│ get file attributes (122 matches)                    │ host-interaction/file-system/meta                    │&#xA;│ set file attributes (8 matches)                      │ host-interaction/file-system/meta                    │&#xA;│ move file                                            │ host-interaction/file-system/move                    │&#xA;│ find taskbar (3 matches)                             │ host-interaction/gui/taskbar/find                    │&#xA;│ get keyboard layout (12 matches)                     │ host-interaction/hardware/keyboard                   │&#xA;│ get disk size                                        │ host-interaction/hardware/storage                    │&#xA;│ get hostname (4 matches)                             │ host-interaction/os/hostname                         │&#xA;│ allocate or change RWX memory (3 matches)            │ host-interaction/process/inject                      │&#xA;│ query or enumerate registry key (3 matches)          │ host-interaction/registry                            │&#xA;│ query or enumerate registry value (8 matches)        │ host-interaction/registry                            │&#xA;│ delete registry key                                  │ host-interaction/registry/delete                     │&#xA;│ start service                                        │ host-interaction/service/start                       │&#xA;│ get session user name                                │ host-interaction/session                             │&#xA;│ persist via Run registry key                         │ persistence/registry/run                             │&#xA;│ persist via Winlogon Helper DLL registry key         │ persistence/registry/winlogon-helper                 │&#xA;│ persist via Windows service (2 matches)              │ persistence/service                                  │&#xA;┕━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;capa rules&lt;/h1&gt; &#xA;&lt;p&gt;capa uses a collection of rules to identify capabilities within a program. These rules are easy to write, even for those new to reverse engineering. By authoring rules, you can extend the capabilities that capa recognizes. In some regards, capa rules are a mixture of the OpenIOC, Yara, and YAML formats.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example rule used by capa:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;rule:&#xA;  meta:&#xA;    name: create TCP socket&#xA;    namespace: communication/socket/tcp&#xA;    authors:&#xA;      - william.ballenthin@mandiant.com&#xA;      - joakim@intezer.com&#xA;      - anushka.virgaonkar@mandiant.com&#xA;    scopes:&#xA;      static: basic block&#xA;      dynamic: call&#xA;    mbc:&#xA;      - Communication::Socket Communication::Create TCP Socket [C0001.011]&#xA;    examples:&#xA;      - Practical Malware Analysis Lab 01-01.dll_:0x10001010&#xA;  features:&#xA;    - or:&#xA;      - and:&#xA;        - number: 6 = IPPROTO_TCP&#xA;        - number: 1 = SOCK_STREAM&#xA;        - number: 2 = AF_INET&#xA;        - or:&#xA;          - api: ws2_32.socket&#xA;          - api: ws2_32.WSASocket&#xA;          - api: socket&#xA;      - property/read: System.Net.Sockets.TcpClient::Client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/mandiant/capa-rules&#34;&gt;github.com/mandiant/capa-rules&lt;/a&gt; repository contains hundreds of standard rules that are distributed with capa. Please learn to write rules and contribute new entries as you find interesting techniques in malware.&lt;/p&gt; &#xA;&lt;h1&gt;IDA Pro plugin: capa explorer&lt;/h1&gt; &#xA;&lt;p&gt;If you use IDA Pro, then you can use the &lt;a href=&#34;https://github.com/mandiant/capa/tree/master/capa/ida/plugin&#34;&gt;capa explorer&lt;/a&gt; plugin. capa explorer helps you identify interesting areas of a program and build new capa rules using features extracted directly from your IDA Pro database. It also uses your local changes to the .idb to extract better features, such as when you rename a global variable that contains a dynamically resolved API address.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mandiant/capa/raw/master/doc/img/explorer_expanded.png&#34; alt=&#34;capa + IDA Pro integration&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Ghidra integration&lt;/h1&gt; &#xA;&lt;p&gt;If you use Ghidra, then you can use the &lt;a href=&#34;https://raw.githubusercontent.com/mandiant/capa/master/capa/ghidra/&#34;&gt;capa + Ghidra integration&lt;/a&gt; to run capa&#39;s analysis directly on your Ghidra database and render the results in Ghidra&#39;s user interface.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/mandiant/capa/assets/66766340/eeae33f4-99d4-42dc-a5e8-4c1b8c661492&#34; width=&#34;300&#34;&gt; &#xA;&lt;h1&gt;blog posts&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mandiant.com/resources/blog/dynamic-capa-executable-behavior-cape-sandbox&#34;&gt;Dynamic capa: Exploring Executable Run-Time Behavior with the CAPE Sandbox&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mandiant.com/resources/blog/capa-v4-casting-wider-net&#34;&gt;capa v4: casting a wider .NET&lt;/a&gt; (.NET support)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mandiant.com/resources/elfant-in-the-room-capa-v3&#34;&gt;ELFant in the Room – capa v3&lt;/a&gt; (ELF support)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mandiant.com/resources/capa-2-better-stronger-faster&#34;&gt;capa 2.0: Better, Stronger, Faster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mandiant.com/resources/capa-automatically-identify-malware-capabilities&#34;&gt;capa: Automatically Identify Malware Capabilities&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;further information&lt;/h1&gt; &#xA;&lt;h2&gt;capa&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mandiant/capa/raw/master/doc/installation.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mandiant/capa/raw/master/doc/usage.md&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mandiant/capa/raw/master/doc/limitations.md&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mandiant/capa/raw/master/.github/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;capa rules&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mandiant/capa-rules&#34;&gt;capa-rules repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mandiant/capa-rules/raw/master/doc/format.md&#34;&gt;capa-rules rule format&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;capa testfiles&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/mandiant/capa-testfiles&#34;&gt;capa-testfiles repository&lt;/a&gt; contains the data we use to test capa&#39;s code and rules&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fedirz/faster-whisper-server</title>
    <updated>2024-10-04T01:33:48Z</updated>
    <id>tag:github.com,2024-10-04:/fedirz/faster-whisper-server</id>
    <link href="https://github.com/fedirz/faster-whisper-server" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Faster Whisper Server&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;faster-whisper-server&lt;/code&gt; is an OpenAI API-compatible transcription server which uses &lt;a href=&#34;https://github.com/SYSTRAN/faster-whisper&#34;&gt;faster-whisper&lt;/a&gt; as its backend. Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU and CPU support.&lt;/li&gt; &#xA; &lt;li&gt;Easily deployable using Docker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configurable through environment variables (see &lt;a href=&#34;https://raw.githubusercontent.com/fedirz/faster-whisper-server/master/src/faster_whisper_server/config.py&#34;&gt;config.py&lt;/a&gt;)&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API compatible.&lt;/li&gt; &#xA; &lt;li&gt;Streaming support (transcription is sent via &lt;a href=&#34;https://en.wikipedia.org/wiki/Server-sent_events&#34;&gt;SSE&lt;/a&gt; as the audio is transcribed. You don&#39;t need to wait for the audio to fully be transcribed before receiving it).&lt;/li&gt; &#xA; &lt;li&gt;Live transcription support (audio is sent via websocket as it&#39;s generated).&lt;/li&gt; &#xA; &lt;li&gt;Dynamic model loading / offloading. Just specify which model you want to use in the request and it will be loaded automatically. It will then be unloaded after a period of inactivity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please create an issue if you find a bug, have a question, or a feature suggestion.&lt;/p&gt; &#xA;&lt;h2&gt;OpenAI API Compatibility ++&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://platform.openai.com/docs/api-reference/audio&#34;&gt;OpenAI API reference&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Audio file transcription via &lt;code&gt;POST /v1/audio/transcriptions&lt;/code&gt; endpoint. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Unlike OpenAI&#39;s API, &lt;code&gt;faster-whisper-server&lt;/code&gt; also supports streaming transcriptions(and translations). This is useful for when you want to process large audio files and would rather receive the transcription in chunks as they are processed rather than waiting for the whole file to be transcribed. It works similarly to chat messages when chatting with LLMs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Audio file translation via &lt;code&gt;POST /v1/audio/translations&lt;/code&gt; endpoint.&lt;/li&gt; &#xA; &lt;li&gt;Live audio transcription via &lt;code&gt;WS /v1/audio/transcriptions&lt;/code&gt; endpoint. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LocalAgreement2 (&lt;a href=&#34;https://aclanthology.org/2023.ijcnlp-demo.3.pdf&#34;&gt;paper&lt;/a&gt; | &lt;a href=&#34;https://github.com/ufal/whisper_streaming&#34;&gt;original implementation&lt;/a&gt;) algorithm is used for live transcription.&lt;/li&gt; &#xA;   &lt;li&gt;Only transcription of a single channel, 16000 sample rate, raw, 16-bit little-endian audio is supported.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Iatalking/fast-whisper-server&#34;&gt;Hugging Face Space&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/fedirz/faster-whisper-server/assets/76551385/6d215c52-ded5-41d2-89a5-03a6fd113aa0&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using Docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus=all --publish 8000:8000 --volume ~/.cache/huggingface:/root/.cache/huggingface fedirz/faster-whisper-server:latest-cuda&#xA;# or&#xA;docker run --publish 8000:8000 --volume ~/.cache/huggingface:/root/.cache/huggingface fedirz/faster-whisper-server:latest-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using Docker Compose&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sO https://raw.githubusercontent.com/fedirz/faster-whisper-server/master/compose.yaml&#xA;docker compose up --detach faster-whisper-server-cuda&#xA;# or&#xA;docker compose up --detach faster-whisper-server-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using Kubernetes: &lt;a href=&#34;https://substratus.ai/blog/deploying-faster-whisper-on-k8s&#34;&gt;tutorial&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;If you are looking for a step-by-step walkthrough, check out &lt;a href=&#34;https://www.youtube.com/watch?app=desktop&amp;amp;v=vSN-oAl6LVs&#34;&gt;this&lt;/a&gt; YouTube video.&lt;/p&gt; &#xA;&lt;h3&gt;OpenAI API CLI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#34;cant-be-empty&#34;&#xA;export OPENAI_BASE_URL=http://localhost:8000/v1/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openai api audio.transcriptions.create -m Systran/faster-distil-whisper-large-v3 -f audio.wav --response-format text&#xA;&#xA;openai api audio.translations.create -m Systran/faster-distil-whisper-large-v3 -f audio.wav --response-format verbose_json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OpenAI API Python SDK&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI(api_key=&#34;cant-be-empty&#34;, base_url=&#34;http://localhost:8000/v1/&#34;)&#xA;&#xA;audio_file = open(&#34;audio.wav&#34;, &#34;rb&#34;)&#xA;transcript = client.audio.transcriptions.create(&#xA;    model=&#34;Systran/faster-distil-whisper-large-v3&#34;, file=audio_file&#xA;)&#xA;print(transcript.text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;cURL&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# If `model` isn&#39;t specified, the default model is used&#xA;curl http://localhost:8000/v1/audio/transcriptions -F &#34;file=@audio.wav&#34;&#xA;curl http://localhost:8000/v1/audio/transcriptions -F &#34;file=@audio.mp3&#34;&#xA;curl http://localhost:8000/v1/audio/transcriptions -F &#34;file=@audio.wav&#34; -F &#34;stream=true&#34;&#xA;curl http://localhost:8000/v1/audio/transcriptions -F &#34;file=@audio.wav&#34; -F &#34;model=Systran/faster-distil-whisper-large-v3&#34;&#xA;# It&#39;s recommended that you always specify the language as that will reduce the transcription time&#xA;curl http://localhost:8000/v1/audio/transcriptions -F &#34;file=@audio.wav&#34; -F &#34;language=en&#34;&#xA;&#xA;curl http://localhost:8000/v1/audio/translations -F &#34;file=@audio.wav&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Live Transcription (using Web Socket)&lt;/h3&gt; &#xA;&lt;p&gt;From &lt;a href=&#34;https://raw.githubusercontent.com/fedirz/faster-whisper-server/master/examples/live-audio&#34;&gt;live-audio&lt;/a&gt; example&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fedirz/faster-whisper-server/assets/76551385/e334c124-af61-41d4-839c-874be150598f&#34;&gt;https://github.com/fedirz/faster-whisper-server/assets/76551385/e334c124-af61-41d4-839c-874be150598f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vi/websocat?tab=readme-ov-file#installation&#34;&gt;websocat&lt;/a&gt; installation is required. Live transcribing audio data from a microphone.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ffmpeg -loglevel quiet -f alsa -i default -ac 1 -ar 16000 -f s16le - | websocat --binary ws://localhost:8000/v1/audio/transcriptions&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>lightly-ai/lightly</title>
    <updated>2024-10-04T01:33:48Z</updated>
    <id>tag:github.com,2024-10-04:/lightly-ai/lightly</id>
    <link href="https://github.com/lightly-ai/lightly" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A python library for self-supervised learning on images.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lightly-ai/lightly/master/docs/logos/lightly_SSL_logo_crop.png&#34; alt=&#34;LightlySSL self-supervised learning Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/lightly-ai/lightly&#34; alt=&#34;GitHub&#34;&gt; &lt;img src=&#34;https://github.com/lightly-ai/lightly/workflows/Unit%20Tests/badge.svg?sanitize=true&#34; alt=&#34;Unit Tests&#34;&gt; &lt;a href=&#34;https://pypi.org/project/lightly/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lightly&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/lightly&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/lightly&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/xvNJW94&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/752876370337726585?logo=discord&amp;amp;logoColor=white&amp;amp;label=discord&amp;amp;color=7289da&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lightly&lt;strong&gt;SSL&lt;/strong&gt; is a computer vision framework for self-supervised learning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lightly-ai/lightly&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/xvNJW94&#34;&gt;Discord&lt;/a&gt; (We have weekly paper sessions!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For a commercial version with more features, including Docker support and pretraining models for embedding, classification, detection, and segmentation tasks with a single command, please contact &lt;a href=&#34;mailto:sales@lightly.ai&#34;&gt;sales@lightly.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve also built a whole platform on top, with additional features for active learning and &lt;a href=&#34;https://docs.lightly.ai/docs/what-is-lightly&#34;&gt;data curation&lt;/a&gt;. If you&#39;re interested in the Lightly Worker Solution to easily process millions of samples and run &lt;a href=&#34;https://docs.lightly.ai/docs/customize-a-selection&#34;&gt;powerful algorithms&lt;/a&gt; on your data, check out &lt;a href=&#34;https://www.lightly.ai&#34;&gt;lightly.ai&lt;/a&gt;. It&#39;s free to get started!&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;This self-supervised learning framework offers the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modular framework, which exposes low-level building blocks such as loss functions and model heads.&lt;/li&gt; &#xA; &lt;li&gt;Easy to use and written in a PyTorch like style.&lt;/li&gt; &#xA; &lt;li&gt;Supports custom backbone models for self-supervised pre-training.&lt;/li&gt; &#xA; &lt;li&gt;Support for distributed training using PyTorch Lightning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/models.html&#34;&gt;find sample code for all the supported models here.&lt;/a&gt; We provide PyTorch, PyTorch Lightning, and PyTorch Lightning distributed examples for all models to kickstart your project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AIM, 2024 &lt;a href=&#34;https://arxiv.org/abs/2401.08541&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/aim.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/aim.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/aim.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Barlow Twins, 2021 &lt;a href=&#34;https://arxiv.org/abs/2103.03230&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/barlowtwins.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/barlowtwins.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/barlowtwins.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;BYOL, 2020 &lt;a href=&#34;https://arxiv.org/abs/2006.07733&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/byol.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/byol.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/byol.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DCL &amp;amp; DCLW, 2021 &lt;a href=&#34;https://arxiv.org/abs/2110.06848&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/dcl.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/dcl.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/dcl.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DenseCL, 2021 &lt;a href=&#34;https://arxiv.org/abs/2011.09157&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/densecl.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/densecl.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/densecl.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DINO, 2021 &lt;a href=&#34;https://arxiv.org/abs/2104.14294&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/dino.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/dino.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/dino.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MAE, 2021 &lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/mae.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/mae.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/mae.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MSN, 2022 &lt;a href=&#34;https://arxiv.org/abs/2204.07141&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/msn.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/msn.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/msn.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MoCo, 2019 &lt;a href=&#34;https://arxiv.org/abs/1911.05722&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/moco.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/moco.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/moco.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NNCLR, 2021 &lt;a href=&#34;https://arxiv.org/abs/2104.14548&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/nnclr.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/nnclr.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/nnclr.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PMSN, 2022 &lt;a href=&#34;https://arxiv.org/abs/2210.07277&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/pmsn.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/pmsn.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/pmsn.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SimCLR, 2020 &lt;a href=&#34;https://arxiv.org/abs/2002.05709&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/simclr.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simclr.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simclr.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SimMIM, 2021 &lt;a href=&#34;https://arxiv.org/abs/2111.09886&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/simmim.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simmim.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simmim.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SimSiam, 2021 &lt;a href=&#34;https://arxiv.org/abs/2011.10566&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/simsiam.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simsiam.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simsiam.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SMoG, 2022 &lt;a href=&#34;https://arxiv.org/abs/2207.06167&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/smog.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/smog.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/smog.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SwaV, 2020 &lt;a href=&#34;https://arxiv.org/abs/2006.09882&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/swav.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/swav.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/swav.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TiCo, 2022 &lt;a href=&#34;https://arxiv.org/abs/2206.10698&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/tico.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/tico.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/tico.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VICReg, 2022 &lt;a href=&#34;https://arxiv.org/abs/2105.04906&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/vicreg.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/vicreg.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/vicreg.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VICRegL, 2022 &lt;a href=&#34;https://arxiv.org/abs/2210.01571&#34;&gt;paper&lt;/a&gt; &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/vicregl.html&#34;&gt;docs&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/vicregl.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/vicregl.ipynb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tutorials&lt;/h2&gt; &#xA;&lt;p&gt;Want to jump to the tutorials and see Lightly in action?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_moco_memory_bank.html&#34;&gt;Train MoCo on CIFAR-10&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html&#34;&gt;Train SimCLR on Clothing Data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simsiam_esa.html&#34;&gt;Train SimSiam on Satellite Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_custom_augmentations.html&#34;&gt;Use Lightly with Custom Augmentations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_pretrain_detectron2.html&#34;&gt;Pre-train a Detectron2 Backbone with Lightly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_checkpoint_finetuning.html&#34;&gt;Finetuning Lightly Checkpoints&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_timm_backbone.html&#34;&gt;Using timm Models as Backbones&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Community and partner projects:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ARM-software/EndpointAI/tree/master/ProofOfConcepts/Vision/OpenMvMaskDefaults&#34;&gt;On-Device Deep Learning with Lightly on an ARM microcontroller&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Lightly requires &lt;strong&gt;Python 3.7+&lt;/strong&gt;. We recommend installing Lightly in a &lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;OSX&lt;/strong&gt; environment. Python 3.12 is not yet supported, as PyTorch itself lacks Python 3.12 compatibility.&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Due to the modular nature of the Lightly package some modules can be used with older versions of dependencies. However, to use all features as of today lightly requires the following dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;&amp;gt;=1.11.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/vision/stable/index.html&#34;&gt;Torchvision&lt;/a&gt;&amp;gt;=0.12.0&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pytorchlightning.ai/index.html&#34;&gt;PyTorch Lightning&lt;/a&gt;&amp;gt;=1.7.1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Lightly is compatible with PyTorch and PyTorch Lightning v2.0+!&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;You can install Lightly and its dependencies from PyPI with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install lightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We strongly recommend that you install Lightly in a dedicated virtualenv, to avoid conflicting with your system packages.&lt;/p&gt; &#xA;&lt;h3&gt;Lightly in Action&lt;/h3&gt; &#xA;&lt;p&gt;With Lightly, you can use the latest self-supervised learning methods in a modular way using the full power of PyTorch. Experiment with different backbones, models, and loss functions. The framework has been designed to be easy to use from the ground up. &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/models.html&#34;&gt;Find more examples in our docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import torchvision&#xA;&#xA;from lightly import loss&#xA;from lightly import transforms&#xA;from lightly.data import LightlyDataset&#xA;from lightly.models.modules import heads&#xA;&#xA;&#xA;# Create a PyTorch module for the SimCLR model.&#xA;class SimCLR(torch.nn.Module):&#xA;    def __init__(self, backbone):&#xA;        super().__init__()&#xA;        self.backbone = backbone&#xA;        self.projection_head = heads.SimCLRProjectionHead(&#xA;            input_dim=512,  # Resnet18 features have 512 dimensions.&#xA;            hidden_dim=512,&#xA;            output_dim=128,&#xA;        )&#xA;&#xA;    def forward(self, x):&#xA;        features = self.backbone(x).flatten(start_dim=1)&#xA;        z = self.projection_head(features)&#xA;        return z&#xA;&#xA;&#xA;# Use a resnet backbone from torchvision.&#xA;backbone = torchvision.models.resnet18()&#xA;# Ignore the classification head as we only want the features.&#xA;backbone.fc = torch.nn.Identity()&#xA;&#xA;# Build the SimCLR model.&#xA;model = SimCLR(backbone)&#xA;&#xA;# Prepare transform that creates multiple random views for every image.&#xA;transform = transforms.SimCLRTransform(input_size=32, cj_prob=0.5)&#xA;&#xA;&#xA;# Create a dataset from your image folder.&#xA;dataset = LightlyDataset(input_dir=&#34;./my/cute/cats/dataset/&#34;, transform=transform)&#xA;&#xA;# Build a PyTorch dataloader.&#xA;dataloader = torch.utils.data.DataLoader(&#xA;    dataset,  # Pass the dataset to the dataloader.&#xA;    batch_size=128,  # A large batch size helps with the learning.&#xA;    shuffle=True,  # Shuffling is important!&#xA;)&#xA;&#xA;# Lightly exposes building blocks such as loss functions.&#xA;criterion = loss.NTXentLoss(temperature=0.5)&#xA;&#xA;# Get a PyTorch optimizer.&#xA;optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-6)&#xA;&#xA;# Train the model.&#xA;for epoch in range(10):&#xA;    for (view0, view1), targets, filenames in dataloader:&#xA;        z0 = model(view0)&#xA;        z1 = model(view1)&#xA;        loss = criterion(z0, z1)&#xA;        loss.backward()&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        print(f&#34;loss: {loss.item():.5f}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can easily use another model like SimSiam by swapping the model and the loss function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# PyTorch module for the SimSiam model.&#xA;class SimSiam(torch.nn.Module):&#xA;    def __init__(self, backbone):&#xA;        super().__init__()&#xA;        self.backbone = backbone&#xA;        self.projection_head = heads.SimSiamProjectionHead(512, 512, 128)&#xA;        self.prediction_head = heads.SimSiamPredictionHead(128, 64, 128)&#xA;&#xA;    def forward(self, x):&#xA;        features = self.backbone(x).flatten(start_dim=1)&#xA;        z = self.projection_head(features)&#xA;        p = self.prediction_head(z)&#xA;        z = z.detach()&#xA;        return z, p&#xA;&#xA;&#xA;model = SimSiam(backbone)&#xA;&#xA;# Use the SimSiam loss function.&#xA;criterion = loss.NegativeCosineSimilarity()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/simsiam.html&#34;&gt;find a more complete example for SimSiam here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use PyTorch Lightning to train the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pytorch_lightning import LightningModule, Trainer&#xA;&#xA;class SimCLR(LightningModule):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        resnet = torchvision.models.resnet18()&#xA;        resnet.fc = torch.nn.Identity()&#xA;        self.backbone = resnet&#xA;        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)&#xA;        self.criterion = loss.NTXentLoss()&#xA;&#xA;    def forward(self, x):&#xA;        features = self.backbone(x).flatten(start_dim=1)&#xA;        z = self.projection_head(features)&#xA;        return z&#xA;&#xA;    def training_step(self, batch, batch_index):&#xA;        (view0, view1), _, _ = batch&#xA;        z0 = self.forward(view0)&#xA;        z1 = self.forward(view1)&#xA;        loss = self.criterion(z0, z1)&#xA;        return loss&#xA;&#xA;    def configure_optimizers(self):&#xA;        optim = torch.optim.SGD(self.parameters(), lr=0.06)&#xA;        return optim&#xA;&#xA;&#xA;model = SimCLR()&#xA;trainer = Trainer(max_epochs=10, devices=1, accelerator=&#34;gpu&#34;)&#xA;trainer.fit(model, dataloader)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/examples/simclr.html&#34;&gt;our docs for a full PyTorch Lightning example.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or train the model on 4 GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;# Use distributed version of loss functions.&#xA;criterion = loss.NTXentLoss(gather_distributed=True)&#xA;&#xA;trainer = Trainer(&#xA;    max_epochs=10,&#xA;    devices=4,&#xA;    accelerator=&#34;gpu&#34;,&#xA;    strategy=&#34;ddp&#34;,&#xA;    sync_batchnorm=True,&#xA;    use_distributed_sampler=True,  # or replace_sampler_ddp=True for PyTorch Lightning &amp;lt;2.0&#xA;)&#xA;trainer.fit(model, dataloader)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide multi-GPU training examples with distributed gather and synchronized BatchNorm. &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/getting_started/distributed_training.html&#34;&gt;Have a look at our docs regarding distributed training.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;Implemented models and their performance on various datasets. Hyperparameters are not tuned for maximum accuracy. For detailed results and more information about the benchmarks click &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet1k&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenet1k&#34;&gt;ImageNet1k benchmarks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Evaluation settings are based on these papers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linear: &lt;a href=&#34;https://arxiv.org/abs/2002.05709&#34;&gt;SimCLR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Finetune: &lt;a href=&#34;https://arxiv.org/abs/2002.05709&#34;&gt;SimCLR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;KNN: &lt;a href=&#34;https://arxiv.org/abs/1805.01978&#34;&gt;InstDisc&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/lightly-ai/lightly/master/benchmarks/imagenet/resnet50/&#34;&gt;benchmarking scripts&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Backbone&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Epochs&lt;/th&gt; &#xA;   &lt;th&gt;Linear Top1&lt;/th&gt; &#xA;   &lt;th&gt;Finetune Top1&lt;/th&gt; &#xA;   &lt;th&gt;kNN Top1&lt;/th&gt; &#xA;   &lt;th&gt;Tensorboard&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BarlowTwins&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;62.9&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;45.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_barlowtwins_2023-08-18_00-11-03/pretrain/version_0/events.out.tfevents.1692310273.Machine2.569794.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_barlowtwins_2023-08-18_00-11-03/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BYOL&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;62.5&lt;/td&gt; &#xA;   &lt;td&gt;74.5&lt;/td&gt; &#xA;   &lt;td&gt;46.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_byol_2024-02-14_16-10-09/pretrain/version_0/events.out.tfevents.1707923418.Machine2.3205.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_byol_2024-02-14_16-10-09/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DINO&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;68.2&lt;/td&gt; &#xA;   &lt;td&gt;72.5&lt;/td&gt; &#xA;   &lt;td&gt;49.9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dino_2023-06-06_13-59-48/pretrain/version_0/events.out.tfevents.1686052799.Machine2.482599.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dino_2023-06-06_13-59-48/pretrain/version_0/checkpoints/epoch%3D99-step%3D1000900.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAE&lt;/td&gt; &#xA;   &lt;td&gt;ViT-B/16&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;46.0&lt;/td&gt; &#xA;   &lt;td&gt;81.3&lt;/td&gt; &#xA;   &lt;td&gt;11.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vitb16_mae_2024-02-25_19-57-30/pretrain/version_0/events.out.tfevents.1708887459.Machine2.1092409.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vitb16_mae_2024-02-25_19-57-30/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MoCoV2&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;74.3&lt;/td&gt; &#xA;   &lt;td&gt;41.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_mocov2_2024-02-18_10-29-14/pretrain/version_0/events.out.tfevents.1708248562.Machine2.439033.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_mocov2_2024-02-18_10-29-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SimCLR*&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;63.2&lt;/td&gt; &#xA;   &lt;td&gt;73.9&lt;/td&gt; &#xA;   &lt;td&gt;44.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-06-22_09-11-13/pretrain/version_0/events.out.tfevents.1687417883.Machine2.33270.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-06-22_09-11-13/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SimCLR* + DCL&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;73.5&lt;/td&gt; &#xA;   &lt;td&gt;49.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dcl_2023-07-04_16-51-40/pretrain/version_0/events.out.tfevents.1688482310.Machine2.247807.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dcl_2023-07-04_16-51-40/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SimCLR* + DCLW&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;48.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dclw_2023-07-07_14-57-13/pretrain/version_0/events.out.tfevents.1688734645.Machine2.3176.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dclw_2023-07-07_14-57-13/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SwAV&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;67.2&lt;/td&gt; &#xA;   &lt;td&gt;75.4&lt;/td&gt; &#xA;   &lt;td&gt;49.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_swav_2023-05-25_08-29-14/pretrain/version_0/events.out.tfevents.1684996168.Machine2.1445108.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_swav_2023-05-25_08-29-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TiCo&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;72.7&lt;/td&gt; &#xA;   &lt;td&gt;26.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_tico_2024-01-07_18-40-57/pretrain/version_0/events.out.tfevents.1704649265.Machine2.1604956.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_tico_2024-01-07_18-40-57/pretrain/version_0/checkpoints/epoch%3D99-step%3D250200.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VICReg&lt;/td&gt; &#xA;   &lt;td&gt;Res50&lt;/td&gt; &#xA;   &lt;td&gt;256&lt;/td&gt; &#xA;   &lt;td&gt;100&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;73.7&lt;/td&gt; &#xA;   &lt;td&gt;46.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_vicreg_2023-09-11_10-53-08/pretrain/version_0/events.out.tfevents.1694422401.Machine2.556563.0&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_vicreg_2023-09-11_10-53-08/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;*We use square root learning rate scaling instead of linear scaling as it yields better results for smaller batch sizes. See Appendix B.1 in the &lt;a href=&#34;https://arxiv.org/abs/2002.05709&#34;&gt;SimCLR paper&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ImageNet100&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenet100&#34;&gt;ImageNet100 benchmarks detailed results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Imagenette&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenette&#34;&gt;Imagenette benchmarks detailed results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;CIFAR-10&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#cifar-10&#34;&gt;CIFAR-10 benchmarks detailed results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Terminology&lt;/h2&gt; &#xA;&lt;p&gt;Below you can see a schematic overview of the different concepts in the package. The terms in bold are explained in more detail in our &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lightly-ai/lightly/master/docs/source/getting_started/images/lightly_overview.png&#34; alt=&#34;Overview of the Lightly pip package&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Next Steps&lt;/h3&gt; &#xA;&lt;p&gt;Head to the &lt;a href=&#34;https://docs.lightly.ai/self-supervised-learning/&#34;&gt;documentation&lt;/a&gt; and see the things you can achieve with Lightly!&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;To install dev dependencies (for example to contribute to the framework) you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -e &#34;.[dev]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about how to contribute have a look &lt;a href=&#34;https://raw.githubusercontent.com/lightly-ai/lightly/master/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Running Tests&lt;/h3&gt; &#xA;&lt;p&gt;Unit tests are within the &lt;a href=&#34;https://raw.githubusercontent.com/lightly-ai/lightly/master/tests/&#34;&gt;tests directory&lt;/a&gt; and we recommend running them using &lt;a href=&#34;https://docs.pytest.org/en/stable/&#34;&gt;pytest&lt;/a&gt;. There are two test configurations available. By default, only a subset will be run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make test-fast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run all tests (including the slow ones) you can use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test a specific file or directory use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pytest &amp;lt;path to file or directory&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Code Formatting&lt;/h3&gt; &#xA;&lt;p&gt;To format code with &lt;a href=&#34;https://black.readthedocs.io/en/stable/&#34;&gt;black&lt;/a&gt; and &lt;a href=&#34;https://docs.pytest.org&#34;&gt;isort&lt;/a&gt; run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make format&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Further Reading&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Have a look at our &lt;a href=&#34;https://discord.com/channels/752876370337726585/815153188487299083&#34;&gt;#papers channel on discord&lt;/a&gt; for the newest self-supervised learning papers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.12210&#34;&gt;A Cookbook of Self-Supervised Learning, 2023&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;Masked Autoencoders Are Scalable Vision Learners, 2021&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.14294&#34;&gt;Emerging Properties in Self-Supervised Vision Transformers, 2021&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.09882&#34;&gt;Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, 2021&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.05659&#34;&gt;What Should Not Be Contrastive in Contrastive Learning, 2020&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.05709&#34;&gt;A Simple Framework for Contrastive Learning of Visual Representations, 2020&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.05722&#34;&gt;Momentum Contrast for Unsupervised Visual Representation Learning, 2020&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Why should I care about self-supervised learning? Aren&#39;t pre-trained models from ImageNet much better for transfer learning?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Self-supervised learning has become increasingly popular among scientists over the last years because the learned representations perform extraordinarily well on downstream tasks. This means that they capture the important information in an image better than other types of pre-trained models. By training a self-supervised model on &lt;em&gt;your&lt;/em&gt; dataset, you can make sure that the representations have all the necessary information about your images.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;How can I contribute?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Create an issue if you encounter bugs or have ideas for features we should implement. You can also add your own code by forking this repository and creating a PR. More details about how to contribute with code is in our &lt;a href=&#34;https://raw.githubusercontent.com/lightly-ai/lightly/master/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Is this framework for free?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yes, this framework is completely free to use and we provide the source code. We believe that we need to make training deep learning models more data efficient to achieve widespread adoption. One step to achieve this goal is by leveraging self-supervised learning. The company behind Lightly is committed to keep this framework open-source.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If this framework is free, how is the company behind Lightly making money?&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Training self-supervised models is only one part of our solution. &lt;a href=&#34;https://lightly.ai/&#34;&gt;The company behind Lightly&lt;/a&gt; focuses on processing and analyzing embeddings created by self-supervised models. By building, what we call a self-supervised active learning loop we help companies understand and work with their data more efficiently. As the &lt;a href=&#34;https://docs.lightly.ai&#34;&gt;Lightly Solution&lt;/a&gt; is a freemium product, you can try it out for free. However, we will charge for some features.&lt;/li&gt; &#xA;   &lt;li&gt;In any case this framework will always be free to use, even for commercial purposes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lightly in Research&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.15614&#34;&gt;Reverse Engineering Self-Supervised Learning, 2023&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.12248.pdf&#34;&gt;Learning Visual Representations via Language-Guided Sampling, 2023&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mdpi.com/2075-4418/12/5/1237&#34;&gt;Self-Supervised Learning Methods for Label-Efficient Dental Caries Classification, 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://assets.researchsquare.com/files/rs-1516950/v1_covered.pdf?c=1654486158&#34;&gt;DPCL: Constrative Representation Learning with Differential Privacy, 2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.06848&#34;&gt;Decoupled Contrastive Learning, 2021&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jmlr.org/papers/volume23/21-1155/21-1155.pdf&#34;&gt;solo-learn: A Library of Self-supervised Methods for Visual Representation Learning, 2021&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Company behind this Open Source Framework&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.lightly.ai&#34;&gt;Lightly&lt;/a&gt; is a spin-off from ETH Zurich that helps companies build efficient active learning pipelines to select the most relevant data for their models.&lt;/p&gt; &#xA;&lt;p&gt;You can find out more about the company and it&#39;s services by following the links below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.lightly.ai&#34;&gt;Homepage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.lightly.ai&#34;&gt;Web-App&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.lightly.ai/&#34;&gt;Lightly Solution Documentation (Lightly Worker &amp;amp; API)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>