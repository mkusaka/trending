<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-12T01:41:26Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>randaller/llama-chat</title>
    <updated>2023-03-12T01:41:26Z</updated>
    <id>tag:github.com,2023-03-12:/randaller/llama-chat</id>
    <link href="https://github.com/randaller/llama-chat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with Meta&#39;s LLaMA models at home made easy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chat with Meta&#39;s LLaMA models at home made easy&lt;/h1&gt; &#xA;&lt;p&gt;This repository is a chat example with &lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;LLaMA&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;arXiv&lt;/a&gt;) models running on a typical home PC. You will just need a NVIDIA videocard and some RAM to chat with model.&lt;/p&gt; &#xA;&lt;p&gt;This repo is heavily based on Meta&#39;s original repo: &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;https://github.com/facebookresearch/llama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And on Steve Manuatu&#39;s repo: &lt;a href=&#34;https://github.com/venuatu/llama&#34;&gt;https://github.com/venuatu/llama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And on Shawn Presser&#39;s repo: &lt;a href=&#34;https://github.com/shawwn/llama&#34;&gt;https://github.com/shawwn/llama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Examples of chats here&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama/issues/162&#34;&gt;https://github.com/facebookresearch/llama/issues/162&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Share your best prompts, chats or generations here in this issue: &lt;a href=&#34;https://github.com/randaller/llama-chat/issues/7&#34;&gt;https://github.com/randaller/llama-chat/issues/7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;System requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Modern enough CPU&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA graphics card&lt;/li&gt; &#xA; &lt;li&gt;64 or better 128 Gb of RAM (192 or 256 would be perfect)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;One may run with 32 Gb of RAM, but inference will be slow (with the speed of your swap file reading)&lt;/p&gt; &#xA;&lt;p&gt;I am running this on 12700k/128 Gb RAM/NVIDIA 3070ti 8Gb/fast huge nvme and getting one token from 30B model in a few seconds.&lt;/p&gt; &#xA;&lt;p&gt;For example, &lt;strong&gt;30B model uses around 70 Gb of RAM&lt;/strong&gt;. 7B model fits into 18 Gb. 13B model uses 48 Gb.&lt;/p&gt; &#xA;&lt;p&gt;If you do not have powerful videocard, you may use another repo for cpu-only inference: &lt;a href=&#34;https://github.com/randaller/llama-cpu&#34;&gt;https://github.com/randaller/llama-cpu&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Conda Environment Setup Example for Windows 10+&lt;/h3&gt; &#xA;&lt;p&gt;Download and install Anaconda Python &lt;a href=&#34;https://www.anaconda.com&#34;&gt;https://www.anaconda.com&lt;/a&gt; and run Anaconda Prompt&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n llama python=3.10&#xA;conda activate llama&#xA;conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;In a conda env with pytorch / cuda available, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in this repository&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download tokenizer and models&lt;/h3&gt; &#xA;&lt;p&gt;magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;amp;dn=LLaMA&lt;/p&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;p&gt;magnet:xt=urn:btih:b8287ebfa04f879b048d4d4404108cf3e8014352&amp;amp;dn=LLaMA&amp;amp;tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&lt;/p&gt; &#xA;&lt;h3&gt;Prepare model&lt;/h3&gt; &#xA;&lt;p&gt;First, you need to unshard model checkpoints to a single file. Let&#39;s do this for 30B model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python merge-weights.py --input_dir D:\Downloads\LLaMA --model_size 30B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, D:\Downloads\LLaMA is a root folder of downloaded torrent with weights.&lt;/p&gt; &#xA;&lt;p&gt;This will create merged.pth file in the root folder of this repo.&lt;/p&gt; &#xA;&lt;p&gt;Place this file and corresponding (torrentroot)/30B/params.json of model into [/model] folder.&lt;/p&gt; &#xA;&lt;p&gt;So you should end up with two files in [/model] folder: merged.pth and params.json.&lt;/p&gt; &#xA;&lt;p&gt;Place (torrentroot)/tokenizer.model file to the [/tokenizer] folder of this repo. Now you are ready to go.&lt;/p&gt; &#xA;&lt;h3&gt;Run the chat&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python example-chat.py ./model ./tokenizer/tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generation parameters&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/22396871/224481306-0079dc71-a659-46f2-96a3-38d8a0b8bafc.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Temperature&lt;/strong&gt; is one of the key parameters of generation. You may wish to play with temperature. The more temperature is, the model will use more &#34;creativity&#34;, and the less temperature instruct model to be &#34;less creative&#34;, but following your prompt stronger.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Repetition penalty&lt;/strong&gt; is a feature implemented by Shawn Presser. With this, the model will be fined, when it would like to enter to repetion loop state. Set this parameter to 1.0, if you wish to disable this feature.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Samplers&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;By default, Meta provided us with top_p sampler only. Again, Shawn added an alternate top_k sampler, which (in my tests) performs pretty well. If you wish to switch to top_k sampler, use the following parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;temperature: float = 0.7,&#xA;top_p: float = 0.0,&#xA;top_k: int = 40,&#xA;sampler: str = &#39;top_k&#39;,&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For sure, you may play with all the values to get different outputs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Launch examples&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;One may modify these hyperparameters straight in the code. But it is better to leave the defaults in code and set the parameters of experiments in the launch line.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Run with top_p sampler, with temperature 0.75, with top_p value 0.95, repetition penalty disabled&#xA;python example-chat.py ./model ./tokenizer/tokenizer.model 0.75 0.95 0 1.0 top_p&#xA;&#xA;# Run with top_k sampler, with temperature 0.7, with top_k value 40, default repetition penalty value&#xA;python example-chat.py ./model ./tokenizer/tokenizer.model 0.7 0.0 40 1.17 top_k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Of course, this is also applicable to a [python example.py] as well (see below).&lt;/p&gt; &#xA;&lt;h3&gt;Enable multi-line answers&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to stop generation not by &#34;\n&#34; sign, but by another signature, like &#34;User:&#34; (which is also good idea), or any other, make the following modification in the llama/generation.py:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/22396871/224122767-227deda4-a718-4774-a7f9-786c07d379cf.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;-5 means to remove last 5 chars from resulting context, which is length of your stop signature, &#34;User:&#34; in this example.&lt;/p&gt; &#xA;&lt;h3&gt;Share the best with community&lt;/h3&gt; &#xA;&lt;p&gt;Share your best prompts and generations with others here: &lt;a href=&#34;https://github.com/randaller/llama-chat/issues/7&#34;&gt;https://github.com/randaller/llama-chat/issues/7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Typical generation with prompt (not a chat)&lt;/h3&gt; &#xA;&lt;p&gt;Simply comment three lines in llama/generation.py to turn it to a generator back.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/22396871/224283389-e29de04e-28d1-4ccd-bf6b-81b29828d3eb.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python example.py ./model ./tokenizer/tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Confirming that 30B model is able to generate code and fix errors in code: &lt;a href=&#34;https://github.com/randaller/llama-chat/issues/7&#34;&gt;https://github.com/randaller/llama-chat/issues/7&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Confirming that 30B model is able to generate prompts for Stable Diffusion: &lt;a href=&#34;https://github.com/randaller/llama-chat/issues/7#issuecomment-1463691554&#34;&gt;https://github.com/randaller/llama-chat/issues/7#issuecomment-1463691554&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Confirming that 7B and 30B model support Arduino IDE: &lt;a href=&#34;https://github.com/randaller/llama-chat/issues/7#issuecomment-1464179944&#34;&gt;https://github.com/randaller/llama-chat/issues/7#issuecomment-1464179944&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/visual-chatgpt</title>
    <updated>2023-03-12T01:41:26Z</updated>
    <id>tag:github.com,2023-03-12:/microsoft/visual-chatgpt</id>
    <link href="https://github.com/microsoft/visual-chatgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VisualChatGPT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Visual ChatGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual ChatGPT&lt;/strong&gt; connects ChatGPT and a series of Visual Foundation Models to enable &lt;strong&gt;sending&lt;/strong&gt; and &lt;strong&gt;receiving&lt;/strong&gt; images during chatting.&lt;/p&gt; &#xA;&lt;p&gt;See our paper: &lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;&lt;font size=&#34;5&#34;&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/font&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/visual-chatgpt/main/assets/demo_short.gif&#34; width=&#34;750&#34;&gt; &#xA;&lt;h2&gt;System Architecture&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/visual-chatgpt/main/assets/figure.jpg&#34; alt=&#34;Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# create a new environment&#xA;conda create -n visgpt python=3.8&#xA;&#xA;# activate the new environment&#xA;conda activate visgpt&#xA;&#xA;#  prepare the basic environments&#xA;pip install -r requirement.txt&#xA;&#xA;# download the visual foundation models&#xA;bash download.sh&#xA;&#xA;# prepare your private openAI private key&#xA;export OPENAI_API_KEY={Your_Private_Openai_Key}&#xA;&#xA;# create a folder to save images&#xA;mkdir ./image&#xA;&#xA;# Start Visual ChatGPT !&#xA;python visual_chatgpt.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;GPU memory usage&lt;/h2&gt; &#xA;&lt;p&gt;Here we list the GPU memory usage of each visual foundation model, one can modify &lt;code&gt;self.tools&lt;/code&gt; with fewer visual foundation models to save your GPU memory:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Foundation Model&lt;/th&gt; &#xA;   &lt;th&gt;Memory Usage (MB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageEditing&lt;/td&gt; &#xA;   &lt;td&gt;6667&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ImageCaption&lt;/td&gt; &#xA;   &lt;td&gt;1755&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T2I&lt;/td&gt; &#xA;   &lt;td&gt;6677&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;canny2image&lt;/td&gt; &#xA;   &lt;td&gt;5540&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;line2image&lt;/td&gt; &#xA;   &lt;td&gt;6679&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hed2image&lt;/td&gt; &#xA;   &lt;td&gt;6679&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;scribble2image&lt;/td&gt; &#xA;   &lt;td&gt;6679&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;pose2image&lt;/td&gt; &#xA;   &lt;td&gt;6681&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BLIPVQA&lt;/td&gt; &#xA;   &lt;td&gt;2709&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;seg2image&lt;/td&gt; &#xA;   &lt;td&gt;5540&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;depth2image&lt;/td&gt; &#xA;   &lt;td&gt;6677&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;normal2image&lt;/td&gt; &#xA;   &lt;td&gt;3974&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InstructPix2Pix&lt;/td&gt; &#xA;   &lt;td&gt;2795&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate the open source of the following projects:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface&#34;&gt;Hugging Face&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;LangChain&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;InstructPix2Pix&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/timojl/clipseg&#34;&gt;CLIPSeg&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/salesforce/BLIP&#34;&gt;BLIP&lt;/a&gt; ‚ÄÇ&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>arc53/DocsGPT</title>
    <updated>2023-03-12T01:41:26Z</updated>
    <id>tag:github.com,2023-03-12:/arc53/DocsGPT</id>
    <link href="https://github.com/arc53/DocsGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPT-powered chat for documentation search &amp; assistance.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; DocsGPT ü¶ñ &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;Open-Source Documentation Assistant&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;strong&gt;DocsGPT&lt;/strong&gt; is a cutting-edge open-source solution that streamlines the process of finding information in project documentation. With its integration of the powerful &lt;strong&gt;GPT&lt;/strong&gt; models, developers can easily ask questions about a project and receive accurate answers. &lt;/p&gt;&#xA;&lt;p&gt;Say goodbye to time-consuming manual searches, and let &lt;strong&gt;DocsGPT&lt;/strong&gt; help you quickly find the information you need. Try it out and see how it revolutionizes your project documentation experience. Contribute to its development and be a part of the future of AI-powered assistance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.gg/n5BX8dh8rU&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/arc53/docsgpt?style=social&#34; alt=&#34;example1&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/n5BX8dh8rU&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/arc53/docsgpt?style=social&#34; alt=&#34;example2&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/n5BX8dh8rU&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/arc53/docsgpt&#34; alt=&#34;example3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/n5BX8dh8rU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1070046503302877216&#34; alt=&#34;example3&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/17906039/220427472-2644cff4-7666-46a5-819f-fc4a521f63c7.png&#34; alt=&#34;Group 9&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;You can find our &lt;a href=&#34;https://github.com/orgs/arc53/projects/2&#34;&gt;Roadmap&lt;/a&gt; here, please don&#39;t hesitate contributing or creating issues, it helps us make DocsGPT better!&lt;/p&gt; &#xA;&lt;h2&gt;Preview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://d3dg1063dc54p9.cloudfront.net/videos/demov2.gif&#34; alt=&#34;video-example-of-docs-gpt&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://docsgpt.arc53.com/&#34;&gt;Live preview&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://discord.gg/n5BX8dh8rU&#34;&gt;Join Our Discord&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;Project structure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Application - flask app (main application)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Extensions - chrome extension&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Scripts - script that creates similarity search index and store for other libraries.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;QuickStart&lt;/h2&gt; &#xA;&lt;p&gt;Please note: current vector database uses pandas Python documentation, thus responses will be related to it, if you want to use other docs please follow a guide below&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to &lt;code&gt;/application&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prepare .env file Copy .env_sample and create .env with your openai api token&lt;/li&gt; &#xA; &lt;li&gt;Run the app &lt;code&gt;python app.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To start frontend&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to &lt;code&gt;/frontend&lt;/code&gt; folder&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;In the file &lt;code&gt;.env.development&lt;/code&gt; instead of &lt;code&gt;VITE_API_HOST = https://docsapi.arc53.com&lt;/code&gt; use &lt;code&gt;VITE_API_HOST=http://localhost:5001&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the app&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Alternatively, you can use docker-compose to run the app via docker&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;From the root folder run &lt;code&gt;docker-compose build &amp;amp;&amp;amp; docker-compose up&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://railway.app/template/2ZSNAt?referralCode=97q7Ll&#34;&gt;&lt;img src=&#34;https://railway.app/button.svg?sanitize=true&#34; alt=&#34;Deploy on Railway&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/arc53/docsgpt/wiki#launch-chrome-extension&#34;&gt;How to install the Chrome extension&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/arc53/docsgpt/wiki&#34;&gt;Guides&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/arc53/DocsGPT/raw/main/CONTRIBUTING.md&#34;&gt;Interested in contributing?&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/arc53/docsgpt/wiki/How-to-train-on-other-documentation&#34;&gt;How to use any other documentation&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/arc53/DocsGPT/wiki/How-to-use-different-LLM&#39;s#hosting-everything-locally&#34;&gt;How to host it locally (so all data will stay on-premises)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Built with &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;ü¶úÔ∏èüîó LangChain&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>