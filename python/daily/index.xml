<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-24T01:44:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Zeqiang-Lai/DragGAN</title>
    <updated>2023-05-24T01:44:58Z</updated>
    <id>tag:github.com,2023-05-24:/Zeqiang-Lai/DragGAN</id>
    <link href="https://github.com/Zeqiang-Lai/DragGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial implementation of &#34;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DragGAN&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;a href=&#34;https://colab.research.google.com/github/Zeqiang-Lai/DragGAN/blob/master/colab.ipynb&#34;&gt;&lt;code&gt;Colab Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--  [`Online Demo`](https://6a05f355a8f139550c.gradio.live/)  --&gt; &#xA;&lt;!-- &gt; Note that the link of online demo will be updated regularly. --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note for Colab, remember to select a GPU via &lt;code&gt;Runtime/Change runtime type&lt;/code&gt; (&lt;code&gt;‰ª£Á†ÅÊâßË°åÁ®ãÂ∫è/Êõ¥ÊîπËøêË°åÊó∂Á±ªÂûã&lt;/code&gt;).&lt;/p&gt; &#xA; &lt;p&gt;Due to the limitation of GAN inversion, it is possible that your custom images are distorted. Besides, it is also possible the manipulations fail due to the limitation of our implementation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/DragGAN/&#34;&gt;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/paper.png&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tweak performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integrate into &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;InternGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Automatically determining the number of iterations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Custom Image with GAN inversion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Download generated image and generation trajectory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Controling generation process with GUI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatically download stylegan2 checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support movable region, mutliple handle points.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio and Colab Demo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Results of our implementation.&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/mouse.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/nose.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/cat.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/horse.gif&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/assets/26198430/f1516101-5667-4f73-9330-57fc45754283&#34;&gt;https://github.com/Zeqiang-Lai/DragGAN/assets/26198430/f1516101-5667-4f73-9330-57fc45754283&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you have a GPU and &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://gradio.app/quickstart/&#34;&gt;Gradio&lt;/a&gt; installed. You could install all the requirements via,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Lanuch the Gradio demo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you have any issuse for downloading the checkpoint, you could manually download it from &lt;a href=&#34;https://huggingface.co/aaronb/StyleGAN2/tree/main&#34;&gt;here&lt;/a&gt; and put it into the folder &lt;code&gt;checkpoints&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/XingangPan/DragGAN&#34;&gt;Official DragGAN&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;StyleGAN2&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;StyleGAN2-pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- https://github.com/omertov/encoder4editing --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{pan2023draggan,&#xA;    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold}, &#xA;    author={Pan, Xingang and Tewari, Ayush, and Leimk{\&#34;u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},&#xA;    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>SevaSk/ecoute</title>
    <updated>2023-05-24T01:44:58Z</updated>
    <id>tag:github.com,2023-05-24:/SevaSk/ecoute</id>
    <link href="https://github.com/SevaSk/ecoute" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ecoute is a live transcription tool that provides real-time transcripts for both the user&#39;s microphone input (You) and the user&#39;s speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI&#39;s GPT-3.5 for the user to say based on the live transcription of the conversation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üéß Ecoute&lt;/h1&gt; &#xA;&lt;p&gt;Ecoute is a live transcription tool that provides real-time transcripts for both the user&#39;s microphone input (You) and the user&#39;s speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI&#39;s GPT-3.5 for the user to say based on the live transcription of the conversation.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SevaSk/ecoute/assets/50382291/8ac48927-8a26-49fd-80e9-48f980986208&#34;&gt;https://github.com/SevaSk/ecoute/assets/50382291/8ac48927-8a26-49fd-80e9-48f980986208&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ecoute is designed to help users in their conversations by providing live transcriptions and generating contextually relevant responses. By leveraging the power of OpenAI&#39;s GPT-3.5, Ecoute aims to make communication more efficient and enjoyable.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Follow these steps to set up and run Ecoute on your local machine.&lt;/p&gt; &#xA;&lt;h3&gt;üìã Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.x&lt;/li&gt; &#xA; &lt;li&gt;An OpenAI API key&lt;/li&gt; &#xA; &lt;li&gt;Windows OS (Not tested on others)&lt;/li&gt; &#xA; &lt;li&gt;FFmpeg&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If FFmpeg is not installed in your system, you can follow the steps below to install it.&lt;/p&gt; &#xA;&lt;p&gt;First, you need to install Chocolatey, a package manager for Windows. Open your PowerShell as Administrator and run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString(&#39;https://community.chocolatey.org/install.ps1&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once Chocolatey is installed, you can install FFmpeg by running the following command in your PowerShell:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;choco install ffmpeg-full&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please ensure that you run these commands in a PowerShell window with administrator privileges. If you face any issues during the installation, you can visit the official Chocolatey and FFmpeg websites for troubleshooting.&lt;/p&gt; &#xA;&lt;h3&gt;üîß Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/SevaSk/ecoute&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;ecoute&lt;/code&gt; folder:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd ecoute&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required packages:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;keys.py&lt;/code&gt; file and add your OpenAI API key:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;echo &#39;OPENAI_API_KEY = &#34;API KEY&#34;&#39; &amp;gt; keys.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace &lt;code&gt;API KEY&lt;/code&gt; with your actual OpenAI API key.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üé¨ Running Ecoute&lt;/h3&gt; &#xA;&lt;p&gt;Run the main script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, Ecoute will start transcribing your microphone input and speaker output in real-time, and provide a suggested response based on the conversation. It may take a couple of seconds to warm up before the transcription becomes real-time.&lt;/p&gt; &#xA;&lt;h3&gt;‚ö†Ô∏è Limitations&lt;/h3&gt; &#xA;&lt;p&gt;While Ecoute provides real-time transcription and response suggestions, there are several known limitations to its functionality that you should be aware of:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Default Mic and Speaker:&lt;/strong&gt; Ecoute is currently configured to listen only to the default microphone and speaker set in your system. It will not detect sound from other devices or systems. If you wish to use a different mic or speaker, you will need to set it as your default device in your system settings.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Whisper Model&lt;/strong&gt;: We utilize the &#39;tiny&#39; version of the Whisper ASR model, due to its low resource consumption and fast response times. However, this model may not be as accurate as the larger models in transcribing certain types of speech, including accents or uncommon words.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language&lt;/strong&gt;: The Whisper model used in Ecoute is set to English. As a result, it may not accurately transcribe non-English languages or dialects. We are actively working to add multi-language support to future versions of the program.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/SevaSk/ecoute/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Feel free to open issues or submit pull requests to improve Ecoute.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>linyiLYi/snake-ai</title>
    <updated>2023-05-24T01:44:58Z</updated>
    <id>tag:github.com,2023-05-24:/linyiLYi/snake-ai</id>
    <link href="https://github.com/linyiLYi/snake-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI agent that beats the classic game &#34;Snake&#34;.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SnakeAI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/linyiLYi/snake-ai/master/README_CN.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | English&lt;/p&gt; &#xA;&lt;p&gt;This project contains the program scripts for the classic game &#34;Snake&#34; and an artificial intelligence agent that can play the game automatically. The intelligent agent is trained using deep reinforcement learning and includes two versions: an agent based on a Multi-Layer Perceptron (MLP) and an agent based on a Convolution Neural Network (CNN), with the latter having a higher average game score.&lt;/p&gt; &#xA;&lt;h3&gt;File Structure&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;‚îú‚îÄ‚îÄ‚îÄmain&#xA;‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogs&#xA;‚îÇ   ‚îú‚îÄ‚îÄ‚îÄtrained_models_cnn&#xA;‚îÇ   ‚îú‚îÄ‚îÄ‚îÄtrained_models_mlp&#xA;‚îÇ   ‚îî‚îÄ‚îÄ‚îÄscripts&#xA;‚îú‚îÄ‚îÄ‚îÄutils&#xA;‚îÇ   ‚îî‚îÄ‚îÄ‚îÄscripts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The main code folder for the project is &lt;code&gt;main/&lt;/code&gt;. It contains &lt;code&gt;logs/&lt;/code&gt;, which includes terminal text and data curves of the training process (viewable using Tensorboard); &lt;code&gt;trained_models_cnn/&lt;/code&gt; and &lt;code&gt;trained_models_mlp/&lt;/code&gt; respectively contain the model weight files for the convolutional network and perceptron models at different stages, which can be used for running tests in &lt;code&gt;test_cnn.py&lt;/code&gt; and &lt;code&gt;test_mlp.py&lt;/code&gt; to observe the actual game performance of the two intelligent agents at different training stages.&lt;/p&gt; &#xA;&lt;p&gt;The other folder &lt;code&gt;utils/&lt;/code&gt; includes two utility scripts. &lt;code&gt;check_gpu_status/&lt;/code&gt; is used to check if the GPU can be called by PyTorch; &lt;code&gt;compress_code.py&lt;/code&gt; can remove all indentation and line breaks from the code, turning it into a tightly arranged single line of text for easier communication with GPT-4 when asking for code suggestions (GPT-4&#39;s understanding of code is far superior to humans and doesn&#39;t require indentation, line breaks, etc.).&lt;/p&gt; &#xA;&lt;h2&gt;Running Guide&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the Python programming language and mainly uses external code libraries such as &lt;a href=&#34;https://www.pygame.org/news&#34;&gt;Pygame&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://github.com/openai/gym&#34;&gt;OpenAI Gym&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34;&gt;Stable-Baselines3&lt;/a&gt;. The Python version used for running the program is 3.8.16. It is recommended to use &lt;a href=&#34;https://www.anaconda.com&#34;&gt;Anaconda&lt;/a&gt; to configure the Python environment. The following setup process has been tested on the Windows 11 system. The following commands are for the console/terminal (Console/Terminal/Shell).&lt;/p&gt; &#xA;&lt;h3&gt;Environment Configuration&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a conda environment named SnakeAI with Python version 3.8.16&#xA;conda create -n SnakeAI python=3.8.16&#xA;conda activate SnakeAI&#xA;&#xA;# [Optional] To use GPU for training, manually install the full version of PyTorch&#xA;conda install pytorch=2.0.0 torchvision pytorch-cuda=11.8 -c pytorch -c nvidia&#xA;&#xA;# [Optional] Run the script to test if PyTorch can successfully call the GPU&#xA;python .\utils\check_gpu_status.py&#xA;&#xA;# Install external code libraries&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Tests&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;main/&lt;/code&gt; folder of the project contains the program scripts for the classic game &#34;Snake&#34;, based on the &lt;a href=&#34;https://www.pygame.org/news&#34;&gt;Pygame&lt;/a&gt; code library. You can directly run the following command to play the game:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;python .\snake_game.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After completing the environment configuration, you can run &lt;code&gt;test_cnn.py&lt;/code&gt; or &lt;code&gt;test_mlp.py&lt;/code&gt; in the &lt;code&gt;main/&lt;/code&gt; folder to test and observe the actual performance of the two intelligent agents at different training stages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;python test_cnn.py&#xA;python test_mlp.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weight files are stored in the &lt;code&gt;main/trained_models_cnn/&lt;/code&gt; and &lt;code&gt;main/trained_models_mlp/&lt;/code&gt; folders. Both test scripts call the trained models by default. If you want to observe the AI performance at different training stages, you can modify the &lt;code&gt;MODEL_PATH&lt;/code&gt; variable in the test scripts to point to the file path of other models.&lt;/p&gt; &#xA;&lt;h3&gt;Training Models&lt;/h3&gt; &#xA;&lt;p&gt;If you need to retrain the models, you can run &lt;code&gt;train_cnn.py&lt;/code&gt; or &lt;code&gt;train_mlp.py&lt;/code&gt; in the &lt;code&gt;main/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;python train_cnn.py&#xA;python train_mlp.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Viewing Curves&lt;/h3&gt; &#xA;&lt;p&gt;The project includes Tensorboard curve graphs of the training process. You can use Tensorboard to view detailed data. It is recommended to use the integrated Tensorboard plugin in VSCode for direct viewing, or you can use the traditional method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd [parent folder of the project]/snake-ai/main&#xA;tensorboard --logdir=logs/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open the default Tensorboard service address &lt;code&gt;http://localhost:6006/&lt;/code&gt; in your browser to view the interactive curve graphs of the training process.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The external code libraries used in this project include &lt;a href=&#34;https://www.pygame.org/news&#34;&gt;Pygame&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://github.com/openai/gym&#34;&gt;OpenAI Gym&lt;/a&gt;„ÄÅ&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34;&gt;Stable-Baselines3&lt;/a&gt;. Thanks all the software developers for their selfless dedication to the open-source community!&lt;/p&gt; &#xA;&lt;p&gt;The convolutional neural network used in this project is from the Nature paper:&lt;/p&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://www.nature.com/articles/nature14236&#34;&gt;Human-level control through deep reinforcement learning&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>