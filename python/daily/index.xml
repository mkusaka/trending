<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-17T01:42:22Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>VikParuchuri/surya</title>
    <updated>2024-01-17T01:42:22Z</updated>
    <id>tag:github.com,2024-01-17:/VikParuchuri/surya</id>
    <link href="https://github.com/VikParuchuri/surya" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Accurate line-level text detection and recognition (OCR) in any language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Surya&lt;/h1&gt; &#xA;&lt;p&gt;Surya is a multilingual document OCR toolkit. It can do:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accurate line-level text detection&lt;/li&gt; &#xA; &lt;li&gt;Text recognition (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Table and chart detection (coming soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It works on a range of documents and languages (see &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#usage&#34;&gt;usage&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#benchmarks&#34;&gt;benchmarks&lt;/a&gt; for more details).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt.png&#34; alt=&#34;New York Times Article Example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Surya is named after the &lt;a href=&#34;https://en.wikipedia.org/wiki/Surya&#34;&gt;Hindu sun god&lt;/a&gt;, who has universal vision.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg//KuZwXNGnfH&#34;&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Text Detection&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;New York Times&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hindi&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Presentation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scientific Paper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Document&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Form&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ll need python 3.9+ and PyTorch. You may need to install the CPU version of torch first if you&#39;re not using a Mac or a GPU machine. See &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install surya-ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weights will automatically download the first time you run surya.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspect the settings in &lt;code&gt;surya/settings.py&lt;/code&gt;. You can override any settings with environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;. Note that the &lt;code&gt;mps&lt;/code&gt; device has a bug (on the &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/84936&#34;&gt;Apple side&lt;/a&gt;) that may prevent it from working properly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;You can detect text lines in an image, pdf, or folder of images/pdfs with the following command. This will write out a json file with the detected bboxes, and optionally save images of the pages with the bboxes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;surya_detect DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain these keys for each page of the input document(s):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;polygons&lt;/code&gt; - polygons for each detected text line (these are more accurate than the bboxes) in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - axis-aligned rectangles for each detected text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vertical_lines&lt;/code&gt; - vertical lines detected in the document in (x1, y1, x2, y2) format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;horizontal_lines&lt;/code&gt; - horizontal lines detected in the document in (x1, y1, x2, y2) format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page_number&lt;/code&gt; - the page number of the document&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;280MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 9GB of VRAM.&lt;/p&gt; &#xA;&lt;p&gt;Depending on your CPU core count, &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; might make a difference there too - the default CPU batch size is &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can adjust &lt;code&gt;DETECTOR_NMS_THRESHOLD&lt;/code&gt; and &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; if you don&#39;t get good results. Try lowering them to detect more text, and vice versa.&lt;/p&gt; &#xA;&lt;h3&gt;From Python&lt;/h3&gt; &#xA;&lt;p&gt;You can also do text detection from code with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from PIL import Image&#xA;from surya.detection import batch_inference&#xA;from surya.model.segformer import load_model, load_processor&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;model, processor = load_model(), load_processor()&#xA;&#xA;# predictions is a list of dicts, one per image&#xA;predictions = batch_inference([image], model, processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text recognition&lt;/h2&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Table and chart detection&lt;/h2&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h1&gt;Manual install&lt;/h1&gt; &#xA;&lt;p&gt;If you want to develop surya, you can install it manually:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/VikParuchuri/surya.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd surya&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install&lt;/code&gt; # Installs main and dev dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is specialized for document OCR. It will likely not work on photos or other images.&lt;/li&gt; &#xA; &lt;li&gt;It is for printed text, not handwriting.&lt;/li&gt; &#xA; &lt;li&gt;The model has trained itself to ignore advertisements.&lt;/li&gt; &#xA; &lt;li&gt;This has worked for every language I&#39;ve tried, but languages with very different character sets may not work well.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_chart_small.png&#34; alt=&#34;Benchmark chart&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Time (s)&lt;/th&gt; &#xA;   &lt;th&gt;Time per page (s)&lt;/th&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;recall&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;surya&lt;/td&gt; &#xA;   &lt;td&gt;52.6892&lt;/td&gt; &#xA;   &lt;td&gt;0.205817&lt;/td&gt; &#xA;   &lt;td&gt;0.844426&lt;/td&gt; &#xA;   &lt;td&gt;0.937818&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tesseract&lt;/td&gt; &#xA;   &lt;td&gt;74.4546&lt;/td&gt; &#xA;   &lt;td&gt;0.290838&lt;/td&gt; &#xA;   &lt;td&gt;0.631498&lt;/td&gt; &#xA;   &lt;td&gt;0.997694&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Tesseract is CPU-based, and surya is CPU or GPU. I ran the benchmarks on a system with an A6000 GPU, and a 32 core CPU. This was the resource usage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;tesseract - 32 CPU cores, or 8 workers using 4 cores each&lt;/li&gt; &#xA; &lt;li&gt;surya - 32 batch size, for 9GB VRAM usage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Surya predicts line-level bboxes, while tesseract and others predict word-level or character-level. It&#39;s also hard to find 100% correct datasets with line-level annotations. Merging bboxes can be noisy, so I chose not to use IoU as the metric for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;I instead used coverage, which calculates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Precision - how well predicted bboxes cover ground truth bboxes&lt;/li&gt; &#xA; &lt;li&gt;Recall - how well ground truth bboxes cover predicted bboxes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First calculate coverage for each bbox, then add a small penalty for double coverage, since we want the detection to have non-overlapping bboxes. Anything with a coverage of 0.5 or higher is considered a match.&lt;/p&gt; &#xA;&lt;p&gt;Then we calculate precision and recall for the whole dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can benchmark the performance of surya on your machine.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the manual install instructions above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install --group dev&lt;/code&gt; # Installs dev dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text line detection&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate tesseract and surya for text line detection across a randomly sampled set of images from &lt;a href=&#34;https://huggingface.co/datasets/vikp/doclaynet_bench&#34;&gt;doclaynet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/detection.py --max 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images and detected bboxes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pdf_path&lt;/code&gt; will let you specify a pdf to benchmark instead of the default data&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;p&gt;This was trained on 4x A6000s for about 3 days. It used a diverse set of images as training data. It was trained from scratch using a modified segformer architecture that reduces inference RAM requirements.&lt;/p&gt; &#xA;&lt;h1&gt;Commercial usage&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text detection&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The text detection model was trained from scratch, so it&#39;s okay for commercial usage. The weights are licensed cc-by-nc-sa-4.0, but I will waive that for any organization under $5M USD in gross revenue in the most recent 12-month period.&lt;/p&gt; &#xA;&lt;p&gt;If you want to remove the GPL license requirements for inference or use the weights commercially over the revenue limit, please contact me at &lt;a href=&#34;mailto:surya@vikas.sh&#34;&gt;surya@vikas.sh&lt;/a&gt; for dual licensing.&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;This work would not have been possible without amazing open source AI work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.15203.pdf&#34;&gt;Segformer&lt;/a&gt; from NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; from huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/clovaai/CRAFT-pytorch&#34;&gt;CRAFT&lt;/a&gt;, a great scene text detection model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you to everyone who makes open source AI possible.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>drunkdream/weread-exporter</title>
    <updated>2024-01-17T01:42:22Z</updated>
    <id>tag:github.com,2024-01-17:/drunkdream/weread-exporter</id>
    <link href="https://github.com/drunkdream/weread-exporter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;将微信读书中的书籍导出成epub、pdf、mobi等格式&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;微信读书导出工具&lt;/h1&gt; &#xA;&lt;h2&gt;实现原理&lt;/h2&gt; &#xA;&lt;p&gt;通过Hook Web页面中的Canvas函数，获取绘制到Canvas中的文本及样式等信息，转换成markdown格式，保存到本地文件，然后再转换成最终的epub或pdf格式，而mobi格式则是使用kindlegen工具从epub格式转换来的。&lt;/p&gt; &#xA;&lt;h2&gt;INSTALL&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;USAGE&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ python -m weread_exporter -b $book_id -o epub -o pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;获取书籍ID的方法：在页面&lt;code&gt;https://weread.qq.com/&lt;/code&gt;搜索目标书籍，进入到书籍介绍页，URL格式为：&lt;code&gt;https://weread.qq.com/web/bookDetail/08232ac0720befa90825d88&lt;/code&gt;，这里的&lt;code&gt;08232ac0720befa90825d88&lt;/code&gt;就是书籍ID。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;code&gt;-o&lt;/code&gt;参数用于指定要保存的文件格式，目前支持的格式有：&lt;code&gt;epub&lt;/code&gt;、&lt;code&gt;pdf&lt;/code&gt;、&lt;code&gt;mobi&lt;/code&gt;，生成的文件在当前目录下的&lt;code&gt;output&lt;/code&gt;目录中。&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;epub&lt;/code&gt;格式适合手机端访问，&lt;code&gt;pdf&lt;/code&gt;格式适合电脑端访问，&lt;code&gt;mobi&lt;/code&gt;格式适合kindle访问。&lt;/p&gt; &#xA;&lt;p&gt;命令行还支持一个可选参数&lt;code&gt;--force-login&lt;/code&gt;，默认为&lt;code&gt;False&lt;/code&gt;，指定该参数时，会先进行登录操作。&lt;/p&gt; &#xA;&lt;h2&gt;免责申明&lt;/h2&gt; &#xA;&lt;p&gt;本工具仅作技术研究之用，请勿用于商业或违法用途，由于使用该工具导致的侵权或其它问题，该本工具不承担任何责任！&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>BRlkl/AGI-Samantha</title>
    <updated>2024-01-17T01:42:22Z</updated>
    <id>tag:github.com,2024-01-17:/BRlkl/AGI-Samantha</id>
    <link href="https://github.com/BRlkl/AGI-Samantha" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AGI has been achieved externally&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Samantha from the movie Her is here:&lt;/h2&gt; &#xA;&lt;p&gt;An autonomous agent for conversations capable of freely thinking and speaking, continuously. Creating an unparalleled sense of realism and dynamicity.&lt;/p&gt; &#xA;&lt;h2&gt;Features:&lt;/h2&gt; &#xA;&lt;p&gt;-Dynamic speech: Samantha can speak whenever it chooses to, influenced by its context, in stark contrast to normal LLMs which are limited to answering and reacting. It is also not limited to solving tasks, like all other autonomous agents.&lt;/p&gt; &#xA;&lt;p&gt;-Live visual capabilities: Visuals are only mentioned and acted upon directly if relevant, but always influences thoughts and behavior.&lt;/p&gt; &#xA;&lt;p&gt;-External categorized memory: Gets dynamically written and read by Samantha, which chooses the most relevant information to write, and to retrieve to context.&lt;/p&gt; &#xA;&lt;p&gt;-Evolving at every moment: Experiences that get stored in the memory can influence and shape subsequent Samantha behavior, like personality, frequency, and style of speech, etc.&lt;/p&gt; &#xA;&lt;h2&gt;How to use:&lt;/h2&gt; &#xA;&lt;p&gt;You communicate with Samantha on the terminal while you can see its inner workings on the flask website.&lt;/p&gt; &#xA;&lt;p&gt;Speak or type &#34;Stop&#34; to stop the agent and save its state.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to plug in OpenAi API and ElevenLabs API (If you want TTS).&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that as it currenly stands, the system is relatively slow and very expensive&lt;/p&gt; &#xA;&lt;h2&gt;How it works:&lt;/h2&gt; &#xA;&lt;p&gt;Orchestration of a collection of LLM calls each with a different purpose. I call each specialized LLM call a “Module”. Samantha is not a single module, but the sum of them, all working together.&lt;/p&gt; &#xA;&lt;p&gt;There are the following modules: Thought, Consciousness, Subconsciousness, Answer, Memory_Read, Memory_Write, Memory_Select. Alongside Vision. Each of them with a different system prompt, and their inputs and outputs orchestrated among themselves to simulate a basic human brain workflow.&lt;/p&gt; &#xA;&lt;p&gt;In short, the agent is a never-ending loop of thoughts and auxiliary systems, constantly receiving visual and auditory stimuli, and based on all that it decides what, when and if to say something.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/BRlkl/AGI-Samantha/assets/63427520/253edb6f-74d2-4903-aac7-58fc3b28d535&#34; alt=&#34;Screenshot_119&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The following pipeline is repeated indefinitely: A loop iteration begins with a gpt-4Vision. The subconsciousness module then processes visual and user input (User can input at any time), it also analyzes the current context of what is going on and produces a description of Samantha’s feelings and emotions. Then the memory_read gets called to analyze current context and provide only relevant memory for Samantha to keep in her context length. After that, the consciousness module gets called to analyze the context and decide on what Samantha should do, if to speak or to continue thinking, and if so, about what. Then, thought module receives the command from the consciousness module, and produces a rational piece of thought. Finally, if the consciousness module decided to speak, the answer modules receives Samantha’s thoughts and composes an answer the user will see. The memory_write module gets called to transfer information from the Short-Term Memory to the Long-Term Memory only once in a while when the Short-Term Memory length gets above a threshold.&lt;/p&gt; &#xA;&lt;p&gt;Bellow is a detailed description of the inputs and outputs of each module:&lt;/p&gt; &#xA;&lt;p&gt;Short-Term Memory is stored as a string in python while the Long-Term Memory a dictionary. The former records what the user says, what Samantha says and her thoughts. The latter groups dense knowledge and information abstracted from the former.&lt;/p&gt; &#xA;&lt;p&gt;-Thought: Receives as input the Long-Term Memory, Short-Term Memory, Subconsciousness, Consciousness, and the current time. The output will be a unit of a thought (Similar to when LLM is prompted to think step by step, the output of this module is one step)&lt;/p&gt; &#xA;&lt;p&gt;-Consciousness: Receives as input the Long-Term Memory, Short-Term Memory and Subconsciousness. The output will be a decision on whether to continue thinking or to speak, and if to continue thinking, then it will also say what to think about and why (Prompting it to say why improves coherence).&lt;/p&gt; &#xA;&lt;p&gt;-Subconsciousness: Receives as input the Long-Term Memory, Short-Term Memory, Subconsciousness alongside visual and textual input. The output will be the summary of the context of what is happening, the visual and textual stimuli (If exists), and the agents’ feelings and emotions about what is happening.&lt;/p&gt; &#xA;&lt;p&gt;-Answer: Receives as input the Long-Term Memory, Short-Term Memory and Subconsciousness. The output will be what the agent speaks out loud for the user, made as a composition of its thoughts.&lt;/p&gt; &#xA;&lt;p&gt;-Memory_Read: Receives as input the Short-Term Memory and the name of the categories of the Long-Term Memory “Keywords”. Output will be a list of the most relevant categories/keywords given the context of the Short-Term Memory. (Code then feeds the entries in the selected categories to the other modules as the relevant part of “Long-Term Memory”)&lt;/p&gt; &#xA;&lt;p&gt;-Memory_Select: Similar to Memory_Read but instead of selecting the keywords relevant for the agent to remember given the recent Short-Term Memory, this module selects the keywords relevant for the agent to store new information inside, given the oldest entries in the Short-Term Memory. Output is list of keywords. (Code expands these keywords and feeds Memory_Write).&lt;/p&gt; &#xA;&lt;p&gt;-Memory_Write: Receives as input the expanded keywords and the Short-Term Memory. Output will be the extended keywords with the additions and modifications made by the module. (Code will then update the Long-Term Memory with the modifications).&lt;/p&gt; &#xA;&lt;h2&gt;How to improve:&lt;/h2&gt; &#xA;&lt;p&gt;Better results can continuously be achieved by tweaking which modules are present, the organization of the modules, and the prompt of the modules.&lt;/p&gt; &#xA;&lt;p&gt;Think of a thing that the human mind can do that LLM &#34;Cannot&#34;, then implement it, then test it. Easy to do and to iterate. The more that is done the closer it is going to be to the human mind.&lt;/p&gt; &#xA;&lt;p&gt;But most importantly: Smaller models each trained specifically to do the job of one of the modules will greatly increase quality, and decrease cost and latency.&lt;/p&gt; &#xA;&lt;p&gt;Contact: &lt;a href=&#34;mailto:pedroschindler964@gmail.com&#34;&gt;pedroschindler964@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>