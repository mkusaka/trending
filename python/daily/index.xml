<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-09-08T01:35:56Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lucidrains/alphafold3-pytorch</title>
    <updated>2024-09-08T01:35:56Z</updated>
    <id>tag:github.com,2024-09-08:/lucidrains/alphafold3-pytorch</id>
    <link href="https://github.com/lucidrains/alphafold3-pytorch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of Alphafold 3 in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lucidrains/alphafold3-pytorch/main/alphafold3.png&#34; width=&#34;500px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Alphafold 3 - Pytorch&lt;/h2&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://www.nature.com/articles/s41586-024-07487-w&#34;&gt;Alphafold 3&lt;/a&gt; in Pytorch&lt;/p&gt; &#xA;&lt;p&gt;You can chat with other researchers about this work &lt;a href=&#34;https://discord.gg/Xsq4WaBR9S&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qjFgthkKxcA&#34;&gt;Review of the paper&lt;/a&gt; by &lt;a href=&#34;https://x.com/sokrypton&#34;&gt;Sergey&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://elanapearl.github.io/blog/2024/the-illustrated-alphafold/&#34;&gt;Illustrated guide&lt;/a&gt; by &lt;a href=&#34;https://elanapearl.github.io/&#34;&gt;Elana P. Simon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AE35XCN5NuU&#34;&gt;Talk by Max Jaderberg&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A fork with full Lightning + Hydra support is being maintained by &lt;a href=&#34;https://github.com/amorehead&#34;&gt;Alex&lt;/a&gt; at &lt;a href=&#34;https://github.com/amorehead/alphafold3-pytorch-lightning-hydra&#34;&gt;this repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A visualization of the molecules of life used in the repository can be seen and interacted with &lt;a href=&#34;https://colab.research.google.com/drive/1S9TD8WmS1Gjgwjo9xyEYTbdxgMVVZcQe?usp=sharing&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/joseph-c-kim&#34;&gt;Joseph&lt;/a&gt; for contributing the Relative Positional Encoding and the Smooth LDDT Loss!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/engelberger&#34;&gt;Felipe&lt;/a&gt; for contributing Weighted Rigid Align, Express Coordinates In Frame, Compute Alignment Error, and Centre Random Augmentation modules!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/amorehead&#34;&gt;Alex&lt;/a&gt; for fixing various issues in the transcribed algorithms&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/gitabtion&#34;&gt;Heng&lt;/a&gt; for pointing out inconsistencies with the paper and pull requesting the solutions&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/gitabtion&#34;&gt;Heng&lt;/a&gt; for finding an issue with the molecular atom indices for the distogram loss&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/luwei0917&#34;&gt;Wei Lu&lt;/a&gt; for catching a few erroneous hyperparameters&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/amorehead&#34;&gt;Alex&lt;/a&gt; for the PDB dataset preparation script!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/milot-mirdita&#34;&gt;Milot&lt;/a&gt; for optimizing the PDB dataset clustering script!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/amorehead&#34;&gt;Alex&lt;/a&gt; for basically writing the entire gargantuan flow from parsing the PDB all the way to the molecule and atomic inputs for training&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/vandrw&#34;&gt;Andrei&lt;/a&gt; for working on the weighted PDB dataset sampling!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/tanjimin&#34;&gt;Jimin&lt;/a&gt; for submitting a small fix to an issue with the coordinates being passed into &lt;code&gt;WeightedRigidAlign&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/xluo233&#34;&gt;@xluo233&lt;/a&gt; for contributing the confidence measures, clash penalty ranking, and sample ranking logic!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/sj900&#34;&gt;sj900&lt;/a&gt; for integrating and testing the &lt;code&gt;WeightedPDBSampler&lt;/code&gt; within the &lt;code&gt;PDBDataset&lt;/code&gt; and for adding initial support for MSA and template parsing!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/xluo233&#34;&gt;@xluo233&lt;/a&gt; again for contributing the logic for computing the model selection score as well as the unresolved rasa!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/wufandi&#34;&gt;Fandi&lt;/a&gt; for discovering a few inconsistencies in the elucidated atom diffusion module with the supplementary&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ptosco&#34;&gt;Paolo&lt;/a&gt; for proposing the &lt;code&gt;PDB neutral stable molecule&lt;/code&gt; hypothesis!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/dhuvik&#34;&gt;Dhuvi&lt;/a&gt; for fixing a bug related to metal ion molecule ID assignment for &lt;code&gt;Alphafold3Inputs&lt;/code&gt;!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Tom (from the Discord channel) for identifying a discrepancy between this codebase&#39;s distogram and template unit vector computations and those of OpenFold (and &lt;a href=&#34;https://github.com/vandrw&#34;&gt;Andrei&lt;/a&gt; for addressing these issues)!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Kaihui-Cheng&#34;&gt;Kaihui&lt;/a&gt; for identifying a bug in how non-standard atoms were handled in polymer residues!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/patrick-kidger&#34;&gt;Patrick&lt;/a&gt; for &lt;a href=&#34;https://docs.kidger.site/jaxtyping/&#34;&gt;jaxtyping&lt;/a&gt;, &lt;a href=&#34;https://github.com/fferflo&#34;&gt;Florian&lt;/a&gt; for &lt;a href=&#34;https://github.com/fferflo/einx&#34;&gt;einx&lt;/a&gt;, and of course, &lt;a href=&#34;https://github.com/arogozhnikov&#34;&gt;Alex&lt;/a&gt; for &lt;a href=&#34;https://einops.rocks/&#34;&gt;einops&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install alphafold3-pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from alphafold3_pytorch import Alphafold3&#xA;from alphafold3_pytorch.utils.model_utils import exclusive_cumsum&#xA;&#xA;alphafold3 = Alphafold3(&#xA;    dim_atom_inputs = 77,&#xA;    dim_template_feats = 108&#xA;)&#xA;&#xA;# mock inputs&#xA;&#xA;seq_len = 16&#xA;&#xA;molecule_atom_indices = torch.randint(0, 2, (2, seq_len)).long()&#xA;molecule_atom_lens = torch.full((2, seq_len), 2).long()&#xA;&#xA;atom_seq_len = molecule_atom_lens.sum(dim=-1).amax()&#xA;atom_offsets = exclusive_cumsum(molecule_atom_lens)&#xA;&#xA;atom_inputs = torch.randn(2, atom_seq_len, 77)&#xA;atompair_inputs = torch.randn(2, atom_seq_len, atom_seq_len, 5)&#xA;&#xA;additional_molecule_feats = torch.randint(0, 2, (2, seq_len, 5))&#xA;additional_token_feats = torch.randn(2, seq_len, 33)&#xA;is_molecule_types = torch.randint(0, 2, (2, seq_len, 5)).bool()&#xA;is_molecule_mod = torch.randint(0, 2, (2, seq_len, 4)).bool()&#xA;molecule_ids = torch.randint(0, 32, (2, seq_len))&#xA;&#xA;template_feats = torch.randn(2, 2, seq_len, seq_len, 108)&#xA;template_mask = torch.ones((2, 2)).bool()&#xA;&#xA;msa = torch.randn(2, 7, seq_len, 32)&#xA;msa_mask = torch.ones((2, 7)).bool()&#xA;&#xA;additional_msa_feats = torch.randn(2, 7, seq_len, 2)&#xA;&#xA;# required for training, but omitted on inference&#xA;&#xA;atom_pos = torch.randn(2, atom_seq_len, 3)&#xA;&#xA;distogram_atom_indices = molecule_atom_lens - 1&#xA;&#xA;distance_labels = torch.randint(0, 37, (2, seq_len, seq_len))&#xA;resolved_labels = torch.randint(0, 2, (2, atom_seq_len))&#xA;&#xA;# offset indices correctly&#xA;&#xA;distogram_atom_indices += atom_offsets&#xA;molecule_atom_indices += atom_offsets&#xA;&#xA;# train&#xA;&#xA;loss = alphafold3(&#xA;    num_recycling_steps = 2,&#xA;    atom_inputs = atom_inputs,&#xA;    atompair_inputs = atompair_inputs,&#xA;    molecule_ids = molecule_ids,&#xA;    molecule_atom_lens = molecule_atom_lens,&#xA;    additional_molecule_feats = additional_molecule_feats,&#xA;    additional_msa_feats = additional_msa_feats,&#xA;    additional_token_feats = additional_token_feats,&#xA;    is_molecule_types = is_molecule_types,&#xA;    is_molecule_mod = is_molecule_mod,&#xA;    msa = msa,&#xA;    msa_mask = msa_mask,&#xA;    templates = template_feats,&#xA;    template_mask = template_mask,&#xA;    atom_pos = atom_pos,&#xA;    distogram_atom_indices = distogram_atom_indices,&#xA;    molecule_atom_indices = molecule_atom_indices,&#xA;    distance_labels = distance_labels,&#xA;    resolved_labels = resolved_labels&#xA;)&#xA;&#xA;loss.backward()&#xA;&#xA;# after much training ...&#xA;&#xA;sampled_atom_pos = alphafold3(&#xA;    num_recycling_steps = 4,&#xA;    num_sample_steps = 16,&#xA;    atom_inputs = atom_inputs,&#xA;    atompair_inputs = atompair_inputs,&#xA;    molecule_ids = molecule_ids,&#xA;    molecule_atom_lens = molecule_atom_lens,&#xA;    additional_molecule_feats = additional_molecule_feats,&#xA;    additional_msa_feats = additional_msa_feats,&#xA;    additional_token_feats = additional_token_feats,&#xA;    is_molecule_types = is_molecule_types,&#xA;    is_molecule_mod = is_molecule_mod,&#xA;    msa = msa,&#xA;    msa_mask = msa_mask,&#xA;    templates = template_feats,&#xA;    template_mask = template_mask&#xA;)&#xA;&#xA;sampled_atom_pos.shape # (2, &amp;lt;atom_seqlen&amp;gt;, 3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example with molecule level input handling&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;from alphafold3_pytorch import (&#xA;    Alphafold3,&#xA;    Alphafold3Input,&#xA;    alphafold3_inputs_to_batched_atom_input&#xA;)&#xA;&#xA;contrived_protein = &#39;AG&#39;&#xA;&#xA;mock_atompos = [&#xA;    torch.randn(5, 3),   # alanine has 5 non-hydrogen atoms&#xA;    torch.randn(4, 3)    # glycine has 4 non-hydrogen atoms&#xA;]&#xA;&#xA;train_alphafold3_input = Alphafold3Input(&#xA;    proteins = [contrived_protein],&#xA;    atom_pos = mock_atompos&#xA;)&#xA;&#xA;eval_alphafold3_input = Alphafold3Input(&#xA;    proteins = [contrived_protein]&#xA;)&#xA;&#xA;batched_atom_input = alphafold3_inputs_to_batched_atom_input(train_alphafold3_input, atoms_per_window = 27)&#xA;&#xA;# training&#xA;&#xA;alphafold3 = Alphafold3(&#xA;    dim_atom_inputs = 3,&#xA;    dim_atompair_inputs = 5,&#xA;    atoms_per_window = 27,&#xA;    dim_template_feats = 108,&#xA;    num_dist_bins = 64,&#xA;    num_molecule_mods = 0,&#xA;    confidence_head_kwargs = dict(&#xA;        pairformer_depth = 1&#xA;    ),&#xA;    template_embedder_kwargs = dict(&#xA;        pairformer_stack_depth = 1&#xA;    ),&#xA;    msa_module_kwargs = dict(&#xA;        depth = 1&#xA;    ),&#xA;    pairformer_stack = dict(&#xA;        depth = 2&#xA;    ),&#xA;    diffusion_module_kwargs = dict(&#xA;        atom_encoder_depth = 1,&#xA;        token_transformer_depth = 1,&#xA;        atom_decoder_depth = 1,&#xA;    )&#xA;)&#xA;&#xA;loss = alphafold3(**batched_atom_input.model_forward_dict())&#xA;loss.backward()&#xA;&#xA;# sampling&#xA;&#xA;batched_eval_atom_input = alphafold3_inputs_to_batched_atom_input(eval_alphafold3_input, atoms_per_window = 27)&#xA;&#xA;alphafold3.eval()&#xA;sampled_atom_pos = alphafold3(**batched_eval_atom_input.model_forward_dict())&#xA;&#xA;assert sampled_atom_pos.shape == (1, (5 + 4), 3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;PDB dataset curation&lt;/h3&gt; &#xA;&lt;p&gt;To acquire the AlphaFold 3 PDB dataset, first download all first-assembly (and asymmetric unit) complexes in the Protein Data Bank (PDB), and then preprocess them with the script referenced below. The PDB can be downloaded from the RCSB: &lt;a href=&#34;https://www.wwpdb.org/ftp/pdb-ftp-sites#rcsbpdb&#34;&gt;https://www.wwpdb.org/ftp/pdb-ftp-sites#rcsbpdb&lt;/a&gt;. The two Python scripts below (i.e., &lt;code&gt;filter_pdb_{train,val,test}_mmcifs.py&lt;/code&gt; and &lt;code&gt;cluster_pdb_{train,val,test}_mmcifs.py&lt;/code&gt;) assume you have downloaded the PDB in the &lt;strong&gt;mmCIF file format&lt;/strong&gt;, placing its first-assembly and asymmetric unit mmCIF files at &lt;code&gt;data/pdb_data/unfiltered_assembly_mmcifs/&lt;/code&gt; and &lt;code&gt;data/pdb_data/unfiltered_asym_mmcifs/&lt;/code&gt;, respectively.&lt;/p&gt; &#xA;&lt;p&gt;For reproducibility, we recommend downloading the PDB using AWS snapshots (e.g., &lt;code&gt;20240101&lt;/code&gt;). To do so, refer to &lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html&#34;&gt;AWS&#39;s documentation&lt;/a&gt; to set up the AWS CLI locally. Alternatively, on the RCSB website, navigate down to &#34;Download Protocols&#34;, and follow the download instructions depending on your location.&lt;/p&gt; &#xA;&lt;p&gt;For example, one can use the following commands to download the PDB as two collections of mmCIF files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For `assembly1` complexes, use the PDB&#39;s `20240101` AWS snapshot:&#xA;aws s3 sync s3://pdbsnapshots/20240101/pub/pdb/data/assemblies/mmCIF/divided/ ./data/pdb_data/unfiltered_assembly_mmcifs&#xA;# Or as a fallback, use rsync:&#xA;rsync -rlpt -v -z --delete --port=33444 \&#xA;rsync.rcsb.org::ftp_data/assemblies/mmCIF/divided/ ./data/pdb_data/unfiltered_assembly_mmcifs/&#xA;&#xA;# For asymmetric unit complexes, also use the PDB&#39;s `20240101` AWS snapshot:&#xA;aws s3 sync s3://pdbsnapshots/20240101/pub/pdb/data/structures/divided/mmCIF/ ./data/pdb_data/unfiltered_asym_mmcifs&#xA;# Or as a fallback, use rsync:&#xA;rsync -rlpt -v -z --delete --port=33444 \&#xA;rsync.rcsb.org::ftp_data/structures/divided/mmCIF/ ./data/pdb_data/unfiltered_asym_mmcifs/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;WARNING: Downloading the PDB can take up to 700GB of space.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: The PDB hosts all available AWS snapshots here: &lt;a href=&#34;https://pdbsnapshots.s3.us-west-2.amazonaws.com/index.html&#34;&gt;https://pdbsnapshots.s3.us-west-2.amazonaws.com/index.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;After downloading, you should have two directories formatted like this: &lt;a href=&#34;https://files.rcsb.org/pub/pdb/data/assemblies/mmCIF/divided/&#34;&gt;https://files.rcsb.org/pub/pdb/data/assemblies/mmCIF/divided/&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://files.rcsb.org/pub/pdb/data/structures/divided/mmCIF/&#34;&gt;https://files.rcsb.org/pub/pdb/data/structures/divided/mmCIF/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;00/&#xA;01/&#xA;02/&#xA;..&#xA;zz/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For these directories, unzip all the files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find ./data/pdb_data/unfiltered_assembly_mmcifs/ -type f -name &#34;*.gz&#34; -exec gzip -d {} \;&#xA;find ./data/pdb_data/unfiltered_asym_mmcifs/ -type f -name &#34;*.gz&#34; -exec gzip -d {} \;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next run the commands&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget -P ./data/ccd_data/ https://files.wwpdb.org/pub/pdb/data/monomers/components.cif.gz&#xA;wget -P ./data/ccd_data/ https://files.wwpdb.org/pub/pdb/data/component-models/complete/chem_comp_model.cif.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;from the project&#39;s root directory to download the latest version of the PDB&#39;s Chemical Component Dictionary (CCD) and its structural models. Extract each of these files using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find data/ccd_data/ -type f -name &#34;*.gz&#34; -exec gzip -d {} \;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;PDB dataset filtering&lt;/h3&gt; &#xA;&lt;p&gt;Then run the following with &lt;code&gt;pdb_assembly_dir&lt;/code&gt;, &lt;code&gt;pdb_asym_dir&lt;/code&gt;, &lt;code&gt;ccd_dir&lt;/code&gt;, and &lt;code&gt;mmcif_output_dir&lt;/code&gt; replaced with the locations of your local copies of the first-assembly PDB, asymmetric unit PDB, CCD, and your desired dataset output directory (i.e., &lt;code&gt;./data/pdb_data/unfiltered_assembly_mmcifs/&lt;/code&gt;, &lt;code&gt;./data/pdb_data/unfiltered_asym_mmcifs/&lt;/code&gt;, &lt;code&gt;./data/ccd_data/&lt;/code&gt;, and &lt;code&gt;./data/pdb_data/{train,val,test}_mmcifs/&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/filter_pdb_train_mmcifs.py --mmcif_assembly_dir &amp;lt;pdb_assembly_dir&amp;gt; --mmcif_asym_dir &amp;lt;pdb_asym_dir&amp;gt; --ccd_dir &amp;lt;ccd_dir&amp;gt; --output_dir &amp;lt;mmcif_output_dir&amp;gt;&#xA;python scripts/filter_pdb_val_mmcifs.py --mmcif_assembly_dir &amp;lt;pdb_assembly_dir&amp;gt; --mmcif_asym_dir &amp;lt;pdb_asym_dir&amp;gt; --output_dir &amp;lt;mmcif_output_dir&amp;gt;&#xA;python scripts/filter_pdb_test_mmcifs.py --mmcif_assembly_dir &amp;lt;pdb_assembly_dir&amp;gt; --mmcif_asym_dir &amp;lt;pdb_asym_dir&amp;gt; --output_dir &amp;lt;mmcif_output_dir&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the scripts for more options. Each first-assembly mmCIF that successfully passes all processing steps will be written to &lt;code&gt;mmcif_output_dir&lt;/code&gt; within a subdirectory named according to the mmCIF&#39;s second and third PDB ID characters (e.g. &lt;code&gt;5c&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;PDB dataset clustering&lt;/h3&gt; &#xA;&lt;p&gt;Next, run the following with &lt;code&gt;mmcif_dir&lt;/code&gt; and &lt;code&gt;{train,val,test}_clustering_output_dir&lt;/code&gt; replaced, respectively, with your local output directory created using the dataset filtering script above and with your desired clustering output directories (i.e., &lt;code&gt;./data/pdb_data/{train,val,test}_mmcifs/&lt;/code&gt; and &lt;code&gt;./data/pdb_data/data_caches/{train,val,test}_clusterings/&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/cluster_pdb_train_mmcifs.py --mmcif_dir &amp;lt;mmcif_dir&amp;gt; --output_dir &amp;lt;train_clustering_output_dir&amp;gt; --clustering_filtered_pdb_dataset&#xA;python scripts/cluster_pdb_val_mmcifs.py --mmcif_dir &amp;lt;mmcif_dir&amp;gt; --reference_clustering_dir &amp;lt;train_clustering_output_dir&amp;gt; --output_dir &amp;lt;val_clustering_output_dir&amp;gt; --clustering_filtered_pdb_dataset&#xA;python scripts/cluster_pdb_test_mmcifs.py --mmcif_dir &amp;lt;mmcif_dir&amp;gt; --reference_1_clustering_dir &amp;lt;train_clustering_output_dir&amp;gt; --reference_2_clustering_dir &amp;lt;val_clustering_output_dir&amp;gt; --output_dir &amp;lt;test_clustering_output_dir&amp;gt; --clustering_filtered_pdb_dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;--clustering_filtered_pdb_dataset&lt;/code&gt; flag is recommended when clustering the filtered PDB dataset as curated using the scripts above, as this flag will enable faster runtimes in this context (since filtering leaves each chain&#39;s residue IDs 1-based). However, this flag must &lt;strong&gt;not&lt;/strong&gt; be provided when clustering other (i.e., non-PDB) datasets of mmCIF files. Otherwise, interface clustering may be performed incorrectly, as these datasets&#39; mmCIF files may not use strict 1-based residue indexing for each chain.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: One can instead download preprocessed (i.e., filtered) mmCIF (&lt;code&gt;train&lt;/code&gt;/&lt;code&gt;val&lt;/code&gt;/&lt;code&gt;test&lt;/code&gt;) files (~25GB, comprising 148k complexes) and chain/interface clustering (&lt;code&gt;train&lt;/code&gt;/&lt;code&gt;val&lt;/code&gt;/&lt;code&gt;test&lt;/code&gt;) files (~3GB) for the PDB&#39;s &lt;code&gt;20240101&lt;/code&gt; AWS snapshot via a &lt;a href=&#34;https://mailmissouri-my.sharepoint.com/:f:/g/personal/acmwhb_umsystem_edu/EqU8tjUmmKxJr-FAlq4tzaIBi2TIBtmw5Vl3k_kmgNlepA?e=mzlyv6&#34;&gt;shared OneDrive folder&lt;/a&gt;. Each of these &lt;code&gt;tar.gz&lt;/code&gt; archives should be decompressed within the &lt;code&gt;data/pdb_data/&lt;/code&gt; directory e.g., via &lt;code&gt;tar -xzf data_caches.tar.gz -C data/pdb_data/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;At the project root, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sh ./contribute.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, add your module to &lt;code&gt;alphafold3_pytorch/alphafold3.py&lt;/code&gt;, add your tests to &lt;code&gt;tests/test_af3.py&lt;/code&gt;, and submit a pull request. You can run the tests locally with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pytest tests/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker Image&lt;/h2&gt; &#xA;&lt;p&gt;The included &lt;code&gt;Dockerfile&lt;/code&gt; contains the required dependencies to run the package and to train/inference using PyTorch with GPUs.&lt;/p&gt; &#xA;&lt;p&gt;The default base image is &lt;code&gt;pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime&lt;/code&gt; and installs the latest version of this package from the &lt;code&gt;main&lt;/code&gt; GitHub branch.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Build Docker Container&#xA;docker build -t af3 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, use build arguments to rebuild the image with different software versions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;PYTORCH_TAG&lt;/code&gt;: Changes the base image and thus builds with different PyTorch, CUDA, and/or cuDNN versions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GIT_TAG&lt;/code&gt;: Changes the tag of this repo to clone and install the package.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Use build argument to change versions&#xA;docker build --build-arg &#34;PYTORCH_TAG=2.2.1-cuda12.1-cudnn8-devel&#34; --build-arg &#34;GIT_TAG=0.1.15&#34; -t af3 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run the container with GPUs and mount a local volume (for training) using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Run Container&#xA;docker run -v .:/data --gpus all -it af3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Abramson2024-fj,&#xA;  title    = &#34;Accurate structure prediction of biomolecular interactions with&#xA;              {AlphaFold} 3&#34;,&#xA;  author   = &#34;Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans,&#xA;              Richard and Green, Tim and Pritzel, Alexander and Ronneberger,&#xA;              Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick,&#xA;              Joshua and Bodenstein, Sebastian W and Evans, David A and Hung,&#xA;              Chia-Chun and O&#39;Neill, Michael and Reiman, David and&#xA;              Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e},&#xA;              Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and&#xA;              Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and&#xA;              Congreve, Miles and Cowen-Rivers, Alexander I and Cowie, Andrew&#xA;              and Figurnov, Michael and Fuchs, Fabian B and Gladman, Hannah and&#xA;              Jain, Rishub and Khan, Yousuf A and Low, Caroline M R and Perlin,&#xA;              Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and&#xA;              Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine&#xA;              and Yakneen, Sergei and Zhong, Ellen D and Zielinski, Michal and&#xA;              {\v Z}{\&#39;\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet&#xA;              and Jaderberg, Max and Hassabis, Demis and Jumper, John M&#34;,&#xA;  journal  = &#34;Nature&#34;,&#xA;  month    = &#34;May&#34;,&#xA;  year     =  2024&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Darcet2023VisionTN,&#xA;    title   = {Vision Transformers Need Registers},&#xA;    author  = {Timoth&#39;ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},&#xA;    year    = {2023},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:263134283}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Arora2024SimpleLA,&#xA;    title   = {Simple linear attention language models balance the recall-throughput tradeoff},&#xA;    author  = {Simran Arora and Sabri Eyuboglu and Michael Zhang and Aman Timalsina and Silas Alberti and Dylan Zinsley and James Zou and Atri Rudra and Christopher R&#39;e},&#xA;    journal = {ArXiv},&#xA;    year    = {2024},&#xA;    volume  = {abs/2402.18668},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:268063190}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Puny2021FrameAF,&#xA;    title   = {Frame Averaging for Invariant and Equivariant Network Design},&#xA;    author  = {Omri Puny and Matan Atzmon and Heli Ben-Hamu and Edward James Smith and Ishan Misra and Aditya Grover and Yaron Lipman},&#xA;    journal = {ArXiv},&#xA;    year    = {2021},&#xA;    volume  = {abs/2110.03336},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:238419638}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Duval2023FAENetFA,&#xA;    title   = {FAENet: Frame Averaging Equivariant GNN for Materials Modeling},&#xA;    author  = {Alexandre Duval and Victor Schmidt and Alex Hernandez Garcia and Santiago Miret and Fragkiskos D. Malliaros and Yoshua Bengio and David Rolnick},&#xA;    journal = {ArXiv},&#xA;    year    = {2023},&#xA;    volume  = {abs/2305.05577},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:258564608}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Wang2022DeepNetST,&#xA;    title   = {DeepNet: Scaling Transformers to 1, 000 Layers},&#xA;    author  = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},&#xA;    journal = {ArXiv},&#xA;    year    = {2022},&#xA;    volume  = {abs/2203.00555},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:247187905}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Ainslie2023CoLT5FL,&#xA;    title   = {CoLT5: Faster Long-Range Transformers with Conditional Computation},&#xA;    author  = {Joshua Ainslie and Tao Lei and Michiel de Jong and Santiago Ontan&#39;on and Siddhartha Brahma and Yury Zemlyanskiy and David Uthus and Mandy Guo and James Lee-Thorp and Yi Tay and Yun-Hsuan Sung and Sumit Sanghai},&#xA;    year    = {2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{Ash2019OnTD,&#xA;    title   = {On the Difficulty of Warm-Starting Neural Network Training},&#xA;    author  = {Jordan T. Ash and Ryan P. Adams},&#xA;    journal = {ArXiv},&#xA;    year    = {2019},&#xA;    volume  = {abs/1910.08475},&#xA;    url     = {https://api.semanticscholar.org/CorpusID:204788802}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>