<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-09T01:38:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Picsart-AI-Research/StreamingT2V</title>
    <updated>2024-04-09T01:38:47Z</updated>
    <id>tag:github.com,2024-04-09:/Picsart-AI-Research/StreamingT2V</id>
    <link href="https://github.com/Picsart-AI-Research/StreamingT2V" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StreamingT2V&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official implementation of &lt;a href=&#34;https://streamingt2v.github.io/&#34;&gt;StreamingT2V&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://www.linkedin.com/in/dr-ing-roberto-henschel-6aa1ba176&#34;&gt;Roberto Henschel&lt;/a&gt;*, &lt;a href=&#34;https://levon-kh.github.io/&#34;&gt;Levon Khachatryan&lt;/a&gt;*, &lt;a href=&#34;https://www.linkedin.com/in/daniil-hayrapetyan-375b05149/&#34;&gt;Daniil Hayrapetyan&lt;/a&gt;*, &lt;a href=&#34;https://www.linkedin.com/in/hayk-poghosyan-793b97198/&#34;&gt;Hayk Poghosyan&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/vtadevosian&#34;&gt;Vahram Tadevosyan&lt;/a&gt;, &lt;a href=&#34;https://www.ece.utexas.edu/people/faculty/atlas-wang&#34;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/shant-navasardyan-1302aa149&#34;&gt;Shant Navasardyan&lt;/a&gt;, &lt;a href=&#34;https://www.humphreyshi.com&#34;&gt;Humphrey Shi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- Roberto Henschel&lt;sup&gt;&amp;ast;&lt;/sup&gt;,&#xA;Levon Khachatryan&lt;sup&gt;&amp;ast;&lt;/sup&gt;,&#xA;Daniil Hayrapetyan&lt;sup&gt;&amp;ast;&lt;/sup&gt;,&#xA;Hayk Poghosyan,&#xA;Vahram Tadevosyan,&#xA;Zhangyang Wang, Shant Navasardyan, Humphrey Shi&#xA;&lt;/br&gt;&#xA;&#xA;&lt;sup&gt;&amp;ast;&lt;/sup&gt; Equal Contribution --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.14773&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-StreamingT2V-red&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://streamingt2v.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Website-orange&#34; alt=&#34;Project Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=GDPP0zmFmQg&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/PAIR/StreamingT2V&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [Paper](https://arxiv.org/abs/2403.14773) | [Video](https://twitter.com/i/status/1770909673463390414) | [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/StreamingT2V) | [Project](https://streamingt2v.github.io/) --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Picsart-AI-Research/StreamingT2V/main/__assets__/github/teaser/teaser_final.png&#34; width=&#34;800px&#34;&gt; &lt;br&gt; &lt;br&gt; &lt;em&gt;StreamingT2V is an advanced autoregressive technique that enables the creation of long videos featuring rich motion dynamics without any stagnation. It ensures temporal consistency throughout the video, aligns closely with the descriptive text, and maintains high frame-level image quality. Our demonstrations include successful examples of videos up to 1200 frames, spanning 2 minutes, and can be extended for even longer durations. Importantly, the effectiveness of StreamingT2V is not limited by the specific Text2Video model used, indicating that improvements in base models could yield even higher-quality videos.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[03/21/2024] Paper &lt;a href=&#34;https://arxiv.org/abs/2403.14773&#34;&gt;StreamingT2V&lt;/a&gt; released!&lt;/li&gt; &#xA; &lt;li&gt;[04/05/2024] Code and &lt;a href=&#34;https://huggingface.co/PAIR/StreamingT2V&#34;&gt;model&lt;/a&gt; released!&lt;/li&gt; &#xA; &lt;li&gt;[04/06/2024] The &lt;a href=&#34;https://huggingface.co/spaces/PAIR/StreamingT2V&#34;&gt;first version&lt;/a&gt; of our huggingface demo released!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and enter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/Picsart-AI-Research/StreamingT2V.git&#xA;cd StreamingT2V/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install requirements using Python 3.10 and CUDA &amp;gt;= 11.6&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n st2v python=3.10&#xA;conda activate st2v&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;(Optional) Install FFmpeg if it&#39;s missing on your system&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install conda-forge::ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download the weights from &lt;a href=&#34;https://huggingface.co/PAIR/StreamingT2V&#34;&gt;HF&lt;/a&gt; and put them into the &lt;code&gt;t2v_enhanced/checkpoints&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;For Text-to-Video&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd t2v_enhanced&#xA;python inference.py --prompt=&#34;A cat running on the street&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use other base models add the &lt;code&gt;--base_model=AnimateDiff&lt;/code&gt; argument. Use &lt;code&gt;python inference.py --help&lt;/code&gt; for more options.&lt;/p&gt; &#xA;&lt;h3&gt;For Image-to-Video&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd t2v_enhanced&#xA;python inference.py --image=../__assets__/demo/fish.jpg --base_model=SVD&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference Time&lt;/h3&gt; &#xA;&lt;h5&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;ModelscopeT2V&lt;/a&gt; as a Base Model&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Number of Frames&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference Time for Faster Preview (256x256)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference Time for Final Result (720x720)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;165 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;56 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;360 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;80 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;110 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;525 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;240 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;340 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1610 seconds (~27 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;600 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;860 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5128 seconds (~85 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1200 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1710 seconds (~28 min)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10225 seconds (~170 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h5&gt;&lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt; as a Base Model&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Number of Frames&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference Time for Faster Preview (256x256)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference Time for Final Result (720x720)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;180 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;56 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;370 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;80 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;120 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;535 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;240 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;350 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1620 seconds (~27 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;600 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;870 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5138 seconds (~85 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1200 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1720 seconds (~28 min)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10235 seconds (~170 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h5&gt;&lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;SVD&lt;/a&gt; as a Base Model&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Number of Frames&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference Time for Faster Preview (256x256)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Inference Time for Final Result (720x720)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;24 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;210 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;56 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;115 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;400 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;80 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;150 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;565 seconds&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;240 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;380 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1650 seconds (~27 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;600 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;900 seconds&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5168 seconds (~86 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1200 frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1750 seconds (~29 min)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10265 seconds (~171 min)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All measurements were conducted using the NVIDIA A100 (80 GB) GPU. Randomized blending is employed when the frame count surpasses 80. For Randomized blending, the values for &lt;code&gt;chunk_size&lt;/code&gt; and &lt;code&gt;overlap_size&lt;/code&gt; are set to 112 and 32, respectively.&lt;/p&gt; &#xA;&lt;h3&gt;Gradio&lt;/h3&gt; &#xA;&lt;p&gt;The same functionality is also available as a gradio demo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd t2v_enhanced&#xA;python gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;Detailed results can be found in the &lt;a href=&#34;https://streamingt2v.github.io/&#34;&gt;Project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our code is published under the CreativeML Open RAIL-M license.&lt;/p&gt; &#xA;&lt;p&gt;We include &lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;ModelscopeT2V&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff&lt;/a&gt;, &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;SVD&lt;/a&gt; in the demo for research purposes and to demonstrate the flexibility of the StreamingT2V framework to include different T2V/I2V models. For commercial usage of such components, please refer to their original license.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If you use our work in your research, please cite our publication:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{henschel2024streamingt2v,&#xA;  title={StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text},&#xA;  author={Henschel, Roberto and Khachatryan, Levon and Hayrapetyan, Daniil and Poghosyan, Hayk and Tadevosyan, Vahram and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},&#xA;  journal={arXiv preprint arXiv:2403.14773},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>InstantStyle/InstantStyle</title>
    <updated>2024-04-09T01:38:47Z</updated>
    <id>tag:github.com,2024-04-09:/InstantStyle/InstantStyle</id>
    <link href="https://github.com/InstantStyle/InstantStyle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation 🔥&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://haofanwang.github.io/&#34;&gt;&lt;strong&gt;Haofan Wang&lt;/strong&gt;&lt;/a&gt;&lt;sup&gt;*&lt;/sup&gt; · &lt;a href=&#34;https://github.com/cubiq&#34;&gt;&lt;strong&gt;Matteo Spinelli&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://github.com/wangqixun&#34;&gt;&lt;strong&gt;Qixun Wang&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://huggingface.co/baymin0220&#34;&gt;&lt;strong&gt;Xu Bai&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://github.com/ZekuiQin&#34;&gt;&lt;strong&gt;Zekui Qin&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://antonioo-c.github.io/&#34;&gt;&lt;strong&gt;Anthony Chen&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;InstantX Team&lt;/p&gt; &#xA; &lt;p&gt;&lt;sup&gt;*&lt;/sup&gt;corresponding authors&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/%5Bhttps://instantid.github.io/%5D(https://instantstyle.github.io/)&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.02733&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Technique-Report-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ameerazam08/InstantStyle-GPU-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-red&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InstantStyle/InstantStyle&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/InstantStyle/InstantStyle?style=social&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;InstantStyle is a general framework that employs two straightforward yet potent techniques for achieving an effective disentanglement of style and content from reference images.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/pipe.png&#34;&gt; &#xA;&lt;h2&gt;Principle&lt;/h2&gt; &#xA;&lt;p&gt;Separating Content from Image. Benefit from the good characterization of CLIP global features, after subtracting the content text fea- tures from the image features, the style and content can be explicitly decoupled. Although simple, this strategy is quite effective in mitigating content leakage.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/subtraction.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Injecting into Style Blocks Only. Empirically, each layer of a deep network captures different semantic information the key observation in our work is that there exists two specific attention layers handling style. Specifically, we find up blocks.0.attentions.1 and down blocks.2.attentions.1 capture style (color, material, atmosphere) and spatial layout (structure, composition) respectively.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/tree.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/04/08] 🔥 InstantStyle is supported in &lt;a href=&#34;https://tiger-ai-lab.github.io/AnyV2V/&#34;&gt;AnyV2V&lt;/a&gt; for stylized video-to-video editing, demo can be found &lt;a href=&#34;https://twitter.com/vinesmsuic/status/1777170927500787782&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/04/07] 🔥 We support image-based stylization, more information can be found &lt;a href=&#34;https://github.com/InstantStyle/InstantStyle/raw/main/infer_style_controlnet.py&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/04/07] 🔥 We support an experimental version for SD1.5, more information can be found &lt;a href=&#34;https://github.com/InstantStyle/InstantStyle/raw/main/infer_style_sd15.py&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/04/03] 🔥 InstantStyle is supported in &lt;a href=&#34;https://github.com/cubiq/ComfyUI_IPAdapter_plus&#34;&gt;ComfyUI_IPAdapter_plus&lt;/a&gt; developed by our co-author.&lt;/li&gt; &#xA; &lt;li&gt;[2024/04/03] 🔥 We release the &lt;a href=&#34;https://arxiv.org/abs/2404.02733&#34;&gt;technical report&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Stylized Synthesis&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/example1.png&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/example2.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Image-based Stylized Synthesis&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/example3.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Comparison with Previous Works&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/comparison.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter?tab=readme-ov-file#download-models&#34;&gt;IP-Adapter&lt;/a&gt; to download pre-trained checkpoints from &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/InstantStyle/InstantStyle.git&#xA;cd InstantStyle&#xA;&#xA;# download the models&#xA;git lfs install&#xA;git clone https://huggingface.co/h94/IP-Adapter&#xA;mv IP-Adapter/models models&#xA;mv IP-Adapter/sdxl_models sdxl_models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Our method is fully compatible with &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt;. For feature subtraction, it only works for global feature instead of patch features. For SD1.5, you can find a demo at &lt;a href=&#34;https://github.com/InstantStyle/InstantStyle/raw/main/infer_style_sd15.py&#34;&gt;infer_style_sd15.py&lt;/a&gt;, but we find that SD1.5 has weaker perception and understanding of style information, thus this demo is experimental only. All block names can be found in &lt;a href=&#34;https://github.com/InstantStyle/InstantStyle/raw/main/attn_blocks.py&#34;&gt;attn_blocks.py&lt;/a&gt; and &lt;a href=&#34;https://github.com/InstantStyle/InstantStyle/raw/main/attn_blocks_sd15.py&#34;&gt;attn_blocks_sd15.py&lt;/a&gt; for SDXL and SD1.5 respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import StableDiffusionXLPipeline&#xA;from PIL import Image&#xA;&#xA;from ip_adapter import IPAdapterXL&#xA;&#xA;base_model_path = &#34;stabilityai/stable-diffusion-xl-base-1.0&#34;&#xA;image_encoder_path = &#34;sdxl_models/image_encoder&#34;&#xA;ip_ckpt = &#34;sdxl_models/ip-adapter_sdxl.bin&#34;&#xA;device = &#34;cuda&#34;&#xA;&#xA;# load SDXL pipeline&#xA;pipe = StableDiffusionXLPipeline.from_pretrained(&#xA;    base_model_path,&#xA;    torch_dtype=torch.float16,&#xA;    add_watermarker=False,&#xA;)&#xA;&#xA;# reduce memory consumption&#xA;pipe.enable_vae_tiling()&#xA;&#xA;# load ip-adapter&#xA;# target_blocks=[&#34;block&#34;] for original IP-Adapter&#xA;# target_blocks=[&#34;up_blocks.0.attentions.1&#34;] for style blocks only&#xA;# target_blocks = [&#34;up_blocks.0.attentions.1&#34;, &#34;down_blocks.2.attentions.1&#34;] # for style+layout blocks&#xA;ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device, target_blocks=[&#34;up_blocks.0.attentions.1&#34;])&#xA;&#xA;image = &#34;./assets/0.jpg&#34;&#xA;image = Image.open(image)&#xA;image.resize((512, 512))&#xA;&#xA;# generate image variations with only image prompt&#xA;images = ip_model.generate(pil_image=image,&#xA;                            prompt=&#34;a cat, masterpiece, best quality, high quality&#34;,&#xA;                            negative_prompt= &#34;text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry&#34;,&#xA;                            scale=1.0,&#xA;                            guidance_scale=5,&#xA;                            num_samples=1,&#xA;                            num_inference_steps=30, &#xA;                            seed=42,&#xA;                            #neg_content_prompt=&#34;a rabbit&#34;,&#xA;                            #neg_content_scale=0.5,&#xA;                          )&#xA;&#xA;images[0].save(&#34;result.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Gradio Demo&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/InstantStyle/InstantStyle.git&#xA;cd ./InstantStyle/gradio_demo/&#xA;pip install -r requirements.txt&#xA;python app.py #remove spaces import from the function this for GPU Server in Huggingface ()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cubiq/ComfyUI_IPAdapter_plus&#34;&gt;InstantStyle for ComfyUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/InstantID/InstantID&#34;&gt;InstantID&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support in diffusers API, check our &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/7586&#34;&gt;PR&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support InstantID for face stylization once stars reach 1K.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Our released codes and checkpoints are for non-commercial research purposes only. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;InstantStyle is developed by the InstantX team and is highly built on &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt;, which has been unfairly compared by many other works. We at InstantStyle make IP-Adapter great again. Additionally, we acknowledge &lt;a href=&#34;https://github.com/xiaohu2015&#34;&gt;Hu Ye&lt;/a&gt; for his valuable discussion.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#InstantStyle/InstantStyle&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=InstantStyle/InstantStyle&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;If you find InstantStyle useful for your research and applications, please cite us using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wang2024instantstyle,&#xA;  title={InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation},&#xA;  author={Wang, Haofan and Wang, Qixun and Bai, Xu and Qin, Zekui and Chen, Anthony},&#xA;  journal={arXiv preprint arXiv:2404.02733},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For any question, feel free to contact us via &lt;a href=&#34;mailto:haofanwang.ai@gmail.com&#34;&gt;haofanwang.ai@gmail.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TMElyralab/MuseTalk</title>
    <updated>2024-04-09T01:38:47Z</updated>
    <id>tag:github.com,2024-04-09:/TMElyralab/MuseTalk</id>
    <link href="https://github.com/TMElyralab/MuseTalk" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MuseTalk: Real-Time High Quality Lip Synchorization with Latent Space Inpainting&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MuseTalk&lt;/h1&gt; &#xA;&lt;p&gt;MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting &lt;br&gt; Yue Zhang &lt;sup&gt;*&lt;/sup&gt;, Minhao Liu&lt;sup&gt;*&lt;/sup&gt;, Zhaokang Chen, Bin Wu&lt;sup&gt;†&lt;/sup&gt;, Yingjie He, Chao Zhan, Wenjiang Zhou (&lt;sup&gt;*&lt;/sup&gt;Equal Contribution, &lt;sup&gt;†&lt;/sup&gt;Corresponding Author, &lt;a href=&#34;mailto:benbinwu@tencent.com&#34;&gt;benbinwu@tencent.com&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/TMElyralab/MuseTalk&#34;&gt;github&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href=&#34;https://huggingface.co/TMElyralab/MuseTalk&#34;&gt;huggingface&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;Project (comming soon)&lt;/strong&gt; &lt;strong&gt;Technical report (comming soon)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We introduce &lt;code&gt;MuseTalk&lt;/code&gt;, a &lt;strong&gt;real-time high quality&lt;/strong&gt; lip-syncing model (30fps+ on an NVIDIA Tesla V100). MuseTalk can be applied with input videos, e.g., generated by &lt;a href=&#34;https://github.com/TMElyralab/MuseV&#34;&gt;MuseV&lt;/a&gt;, as a complete virtual human solution.&lt;/p&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;MuseTalk&lt;/code&gt; is a real-time high quality audio-driven lip-syncing model trained in the latent space of &lt;code&gt;ft-mse-vae&lt;/code&gt;, which&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;modifies an unseen face according to the input audio, with a size of face region of &lt;code&gt;256 x 256&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;supports audio in various languages, such as Chinese, English, and Japanese.&lt;/li&gt; &#xA; &lt;li&gt;supports real-time inference with 30fps+ on an NVIDIA Tesla V100.&lt;/li&gt; &#xA; &lt;li&gt;supports modification of the center point of the face region proposes, which &lt;strong&gt;SIGNIFICANTLY&lt;/strong&gt; affects generation results.&lt;/li&gt; &#xA; &lt;li&gt;checkpoint available trained on the HDTF dataset.&lt;/li&gt; &#xA; &lt;li&gt;training codes (comming soon).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[04/02/2024] Released MuseTalk project and pretrained models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/figs/musetalk_arc.jpg&#34; alt=&#34;Model Structure&#34;&gt; MuseTalk was trained in latent spaces, where the images were encoded by a freezed VAE. The audio was encoded by a freezed &lt;code&gt;whisper-tiny&lt;/code&gt; model. The architecture of the generation network was borrowed from the UNet of the &lt;code&gt;stable-diffusion-v1-4&lt;/code&gt;, where the audio embeddings were fused to the image embeddings by cross-attention.&lt;/p&gt; &#xA;&lt;h2&gt;Cases&lt;/h2&gt; &#xA;&lt;h3&gt;MuseV + MuseTalk make human photos alive！&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt;Image&lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt;MuseV&lt;/td&gt; &#xA;   &lt;td width=&#34;33%&#34;&gt;+MuseTalk&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/musk/musk.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/4a4bb2d1-9d14-4ca9-85c8-7f19c39f712e&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/b2a879c2-e23a-4d39-911d-51f0343218e4&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/yongen/yongen.jpeg&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/57ef9dee-a9fd-4dc8-839b-3fbbbf0ff3f4&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/94d8dcba-1bcd-4b54-9d1d-8b6fc53228f0&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/sit/sit.jpeg&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/5fbab81b-d3f2-4c75-abb5-14c76e51769e&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/f8100f4a-3df8-4151-8de2-291b09269f66&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/man/man.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/a6e7d431-5643-4745-9868-8b423a454153&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/6ccf7bc7-cb48-42de-85bd-076d5ee8a623&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/monalisa/monalisa.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/1568f604-a34f-4526-a13a-7d282aa2e773&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/a40784fc-a885-4c1f-9b7e-8f87b7caf4e0&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/sun1/sun.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/37a3a666-7b90-4244-8d3a-058cb0e44107&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/172f4ff1-d432-45bd-a5a7-a07dec33a26b&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/sun2/sun.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/37a3a666-7b90-4244-8d3a-058cb0e44107&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/85a6873d-a028-4cce-af2b-6c59a1f2971d&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The character of the last two rows, &lt;code&gt;Xinying Sun&lt;/code&gt;, is a supermodel KOL. You can follow her on &lt;a href=&#34;https://www.douyin.com/user/MS4wLjABAAAAWDThbMPN_6Xmm_JgXexbOii1K-httbu2APdG8DvDyM8&#34;&gt;douyin&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video dubbing&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td width=&#34;70%&#34;&gt;MuseTalk&lt;/td&gt; &#xA;   &lt;td width=&#34;30%&#34;&gt;Original videos&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/4d7c5fa1-3550-4d52-8ed2-52f158150f24&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/www.bilibili.com/video/BV1wT411b7HU&#34;&gt;Link&lt;/a&gt; &#xA;    &lt;href src=&#34;&#34;&gt;&lt;/href&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For video dubbing, we applied a self-developed tool which can identify the talking person.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Some interesting videos!&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr style=&#34;font-weight: bolder;text-align:center;&#34;&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt;Image&lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34;&gt;MuseV + MuseTalk&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/demo/video1/video1.png&#34; width=&#34;95%&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &#xA;    &lt;video src=&#34;https://github.com/TMElyralab/MuseTalk/assets/163980830/1f02f9c6-8b98-475e-86b8-82ebee82fe0d&#34; controls preload&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h1&gt;TODO:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; trained models and inference codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; technical report.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; training codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; online UI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; a better model (may take longer).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;We provide a detailed tutorial about the installation and the basic usage of MuseTalk for new users:&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To prepare the Python environment and install additional packages such as opencv, diffusers, mmcv, etc., please follow the steps below:&lt;/p&gt; &#xA;&lt;h3&gt;Build environment&lt;/h3&gt; &#xA;&lt;p&gt;We recommend a python version &amp;gt;=3.10 and cuda version =11.7. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;mmlab packages&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --no-cache-dir -U openmim &#xA;mim install mmengine &#xA;mim install &#34;mmcv&amp;gt;=2.0.1&#34; &#xA;mim install &#34;mmdet&amp;gt;=3.1.0&#34; &#xA;mim install &#34;mmpose&amp;gt;=1.1.0&#34; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download ffmpeg-static&lt;/h3&gt; &#xA;&lt;p&gt;Download the ffmpeg-static and&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export FFMPEG_PATH=/path/to/ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export FFMPEG_PATH=/musetalk/ffmpeg-4.4-amd64-static&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;You can download weights manually as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/TMElyralab/MuseTalk&#34;&gt;weights&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the weights of other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt&#34;&gt;whisper&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/yzd-v/DWPose/tree/main&#34;&gt;dwpose&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parse-bisent&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34;&gt;resnet18&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be organized in &lt;code&gt;models&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./models/&#xA;├── musetalk&#xA;│   └── musetalk.json&#xA;│   └── pytorch_model.bin&#xA;├── dwpose&#xA;│   └── dw-ll_ucoco_384.pth&#xA;├── face-parse-bisent&#xA;│   ├── 79999_iter.pth&#xA;│   └── resnet18-5c106cde.pth&#xA;├── sd-vae-ft-mse&#xA;│   ├── config.json&#xA;│   └── diffusion_pytorch_model.bin&#xA;└── whisper&#xA;    └── tiny.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Here, we provide the inference script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m scripts.inference --inference_config configs/inference/test.yaml &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;configs/inference/test.yaml is the path to the inference configuration file, including video_path and audio_path. The video_path should be either a video file or a directory of images.&lt;/p&gt; &#xA;&lt;p&gt;You are recommended to input video with &lt;code&gt;25fps&lt;/code&gt;, the same fps used when training the model. If your video is far less than 25fps, you are recommended to apply frame interpolation or directly convert the video to 25fps using ffmpeg.&lt;/p&gt; &#xA;&lt;h4&gt;Use of bbox_shift to have adjustable results&lt;/h4&gt; &#xA;&lt;p&gt;&lt;span&gt;🔎&lt;/span&gt; We have found that upper-bound of the mask has an important impact on mouth openness. Thus, to control the mask region, we suggest using the &lt;code&gt;bbox_shift&lt;/code&gt; parameter. Positive values (moving towards the lower half) increase mouth openness, while negative values (moving towards the upper half) decrease mouth openness.&lt;/p&gt; &#xA;&lt;p&gt;You can start by running with the default configuration to obtain the adjustable value range, and then re-run the script within this range.&lt;/p&gt; &#xA;&lt;p&gt;For example, in the case of &lt;code&gt;Xinying Sun&lt;/code&gt;, after running the default configuration, it shows that the adjustable value rage is [-9, 9]. Then, to decrease the mouth openness, we set the value to be &lt;code&gt;-7&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m scripts.inference --inference_config configs/inference/test.yaml --bbox_shift -7 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;span&gt;📌&lt;/span&gt; More technical details can be found in &lt;a href=&#34;https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/BBOX_SHIFT.md&#34;&gt;bbox_shift&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Combining MuseV and MuseTalk&lt;/h4&gt; &#xA;&lt;p&gt;As a complete solution to virtual human generation, you are suggested to first apply &lt;a href=&#34;https://github.com/TMElyralab/MuseV&#34;&gt;MuseV&lt;/a&gt; to generate a video (text-to-video, image-to-video or pose-to-video) by referring &lt;a href=&#34;https://github.com/TMElyralab/MuseV?tab=readme-ov-file#text2video&#34;&gt;this&lt;/a&gt;. Frame interpolation is suggested to increase frame rate. Then, you can use &lt;code&gt;MuseTalk&lt;/code&gt; to generate a lip-sync video by referring &lt;a href=&#34;https://github.com/TMElyralab/MuseTalk?tab=readme-ov-file#inference&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Note&lt;/h1&gt; &#xA;&lt;p&gt;If you want to launch online video chats, you are suggested to generate videos using MuseV and apply necessary pre-processing such as face detection and face parsing in advance. During online chatting, only UNet and the VAE decoder are involved, which makes MuseTalk real-time.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We thank open-source components like &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://github.com/IDEA-Research/DWPose&#34;&gt;dwpose&lt;/a&gt;, &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34;&gt;face-alignment&lt;/a&gt;, &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parsing&lt;/a&gt;, &lt;a href=&#34;https://github.com/yxlijun/S3FD.pytorch&#34;&gt;S3FD&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MuseTalk has referred much to &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; and &lt;a href=&#34;https://github.com/isaacOnline/whisper/tree/extract-embeddings&#34;&gt;isaacOnline/whisper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;MuseTalk has been built on &lt;a href=&#34;https://github.com/MRzzm/HDTF&#34;&gt;HDTF&lt;/a&gt; datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Thanks for open-sourcing!&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Resolution: Though MuseTalk uses a face region size of 256 x 256, which make it better than other open-source methods, it has not yet reached the theoretical resolution bound. We will continue to deal with this problem.&lt;br&gt; If you need higher resolution, you could apply super resolution models such as &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;GFPGAN&lt;/a&gt; in combination with MuseTalk.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Identity preservation: Some details of the original face are not well preserved, such as mustache, lip shape and color.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jitter: There exists some jitter as the current pipeline adopts single-frame generation.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{musetalk,&#xA;  title={MuseTalk: Real-Time High Quality Lip Synchorization with Latent Space Inpainting},&#xA;  author={Zhang, Yue and Liu, Minhao and Chen, Zhaokang and Wu, Bin and He, Yingjie and Zhan, Chao and Zhou, Wenjiang},&#xA;  journal={arxiv},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Disclaimer/License&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;code&lt;/code&gt;: The code of MuseTalk is released under the MIT License. There is no limitation for both academic and commercial usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: The trained model are available for any purpose, even commercially.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;other opensource model&lt;/code&gt;: Other open-source models used must comply with their license, such as &lt;code&gt;whisper&lt;/code&gt;, &lt;code&gt;ft-mse-vae&lt;/code&gt;, &lt;code&gt;dwpose&lt;/code&gt;, &lt;code&gt;S3FD&lt;/code&gt;, etc..&lt;/li&gt; &#xA; &lt;li&gt;The testdata are collected from internet, which are available for non-commercial research purposes only.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AIGC&lt;/code&gt;: This project strives to impact the domain of AI-driven video generation positively. Users are granted the freedom to create videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>