<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-01T01:39:38Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>volcengine/verl</title>
    <updated>2025-02-01T01:39:38Z</updated>
    <id>tag:github.com,2025-02-01:/volcengine/verl</id>
    <link href="https://github.com/volcengine/verl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;veRL: Volcano Engine Reinforcement Learning for LLM&lt;/p&gt;&lt;hr&gt;&lt;h1 style=&#34;text-align: center;&#34;&gt;veRL: Volcano Engine Reinforcement Learning for LLM&lt;/h1&gt; &#xA;&lt;p&gt;veRL is a flexible, efficient and production-ready RL training library for large language models (LLMs).&lt;/p&gt; &#xA;&lt;p&gt;veRL is the open-source version of &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;&lt;/strong&gt; paper.&lt;/p&gt; &#xA;&lt;p&gt;veRL is flexible and easy to use with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy extension of diverse RL algorithms&lt;/strong&gt;: The Hybrid programming model combines the strengths of single-controller and multi-controller paradigms to enable flexible representation and efficient execution of complex Post-Training dataflows. Allowing users to build RL dataflows in a few lines of code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Seamless integration of existing LLM infra with modular APIs&lt;/strong&gt;: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as PyTorch FSDP, Megatron-LM and vLLM. Moreover, users can easily extend to other LLM training and inference frameworks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible device mapping&lt;/strong&gt;: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Readily integration with popular HuggingFace models&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;veRL is fast with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State-of-the-art throughput&lt;/strong&gt;: By seamlessly integrating existing SOTA LLM training and inference frameworks, veRL achieves high generation and training throughput.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient actor model resharding with 3D-HybridEngine&lt;/strong&gt;: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA&#34;&gt;&lt;b&gt;Slack&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&#34;&gt;&lt;b&gt;Wechat&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://x.com/verl_project&#34;&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;&#34;&gt;&lt;b&gt;Slides&lt;/b&gt;&lt;/a&gt; | --&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2025/1] &lt;a href=&#34;https://team.doubao.com/zh/special/doubao_1_5_pro&#34;&gt;Doubao-1.5-pro&lt;/a&gt; is released with SOTA-level performance on LLM &amp;amp; VLM. The RL scaling preview model is trained using veRL, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt; &#xA; &lt;li&gt;[2024/12] The team presented &lt;a href=&#34;https://neurips.cc/Expo/Conferences/2024/workshop/100677&#34;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&#34;https://github.com/eric-haibin-lin/verl-data/tree/neurips&#34;&gt;Slides&lt;/a&gt; and &lt;a href=&#34;https://neurips.cc/Expo/Conferences/2024/workshop/100677&#34;&gt;video&lt;/a&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] veRL is presented at Ray Summit. &lt;a href=&#34;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;amp;index=37&#34;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;FSDP&lt;/strong&gt; and &lt;strong&gt;Megatron-LM&lt;/strong&gt; for training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt; and &lt;strong&gt;TGI&lt;/strong&gt; for rollout generation, &lt;strong&gt;SGLang&lt;/strong&gt; support coming soon.&lt;/li&gt; &#xA; &lt;li&gt;huggingface models support&lt;/li&gt; &#xA; &lt;li&gt;Supervised fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement learning from human feedback with &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/ppo_trainer&#34;&gt;PPO&lt;/a&gt; and &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/grpo_trainer&#34;&gt;GRPO&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support model-based reward and function-based reward (verifiable reward)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;flash-attention integration, sequence packing, and long context support via DeepSpeed Ulysses&lt;/li&gt; &#xA; &lt;li&gt;scales up to 70B models and hundreds of GPUs&lt;/li&gt; &#xA; &lt;li&gt;experiment tracking with wandb and mlflow&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Upcoming Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reward model training&lt;/li&gt; &#xA; &lt;li&gt;DPO training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Checkout this &lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/ppo_trainer/verl_getting_started.ipynb&#34;&gt;Jupyter Notebook&lt;/a&gt; to get started with PPO training with a single 24GB L4 GPU (&lt;strong&gt;FREE&lt;/strong&gt; GPU quota provided by &lt;a href=&#34;https://lightning.ai/hlin-verl/studios/verl-getting-started&#34;&gt;Lighting Studio&lt;/a&gt;)!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/start/install.html&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/start/quickstart.html&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running a PPO example step-by-step:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data and Reward Preparation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/preparation/prepare_data.html&#34;&gt;Prepare Data (Parquet) for Post-Training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/preparation/reward_function.html&#34;&gt;Implement Reward Function for Dataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Understanding the PPO Example &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html&#34;&gt;PPO Example Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/config.html&#34;&gt;Config Explanation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html&#34;&gt;Run GSM8K Example&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reproducible algorithm baselines:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/experiment/ppo.html&#34;&gt;PPO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For code explanation and advance usage (extension):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PPO Trainer and Workers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/workers/ray_trainer.html&#34;&gt;PPO Ray Trainer&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html&#34;&gt;PyTorch FSDP Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;Megatron-LM Backend&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Advance Usage and Extension &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/placement.html&#34;&gt;Ray API design tutorial&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/dpo_extension.html&#34;&gt;Extend to Other RL(HF) algorithms&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html&#34;&gt;Add Models with the FSDP Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/advance/megatron_extension.html&#34;&gt;Add Models with the Megatron-LM Backend&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/volcengine/verl/tree/main/examples/split_placement&#34;&gt;Deployment using Separate GPU Resources&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance Tuning Guide&lt;/h2&gt; &#xA;&lt;p&gt;The performance is essential for on-policy RL algorithm. We write a detailed performance tuning guide to allow people tune the performance. See &lt;a href=&#34;https://verl.readthedocs.io/en/latest/perf/perf_tuning.html&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution Guide&lt;/h2&gt; &#xA;&lt;p&gt;Contributions from the community are welcome!&lt;/p&gt; &#xA;&lt;h3&gt;Code formatting&lt;/h3&gt; &#xA;&lt;p&gt;We use yapf (Google style) to enforce strict code formatting when reviewing PRs. To reformat you code locally, make sure you installed &lt;strong&gt;latest&lt;/strong&gt; &lt;code&gt;yapf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install yapf --upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, make sure you are at top level of verl repo and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/format.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation and acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;If you find the project helpful, please cite:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.19256v2&#34;&gt;HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf&#34;&gt;A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@article{sheng2024hybridflow,&#xA;  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},&#xA;  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},&#xA;  year    = {2024},&#xA;  journal = {arXiv preprint arXiv: 2409.19256}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and supported by Anyscale, Bytedance, LMSys.org, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, and University of Hong Kong.&lt;/p&gt; &#xA;&lt;h2&gt;Awesome work using veRL&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.09302&#34;&gt;Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.21236&#34;&gt;Flaming-hot Initiation with Regular Execution Sampling for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PRIME-RL/PRIME/&#34;&gt;Process Reinforcement Through Implicit Rewards&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jiayi-Pan/TinyZero&#34;&gt;TinyZero&lt;/a&gt;: a reproduction of DeepSeek R1 Zero recipe for reasoning tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ZihanWang314/ragen&#34;&gt;RAGEN&lt;/a&gt;: a general-purpose reasoning agent training framework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are HIRING! Send us an &lt;a href=&#34;mailto:haibin.lin@bytedance.com&#34;&gt;email&lt;/a&gt; if you are interested in internship/FTE opportunities in MLSys/LLM reasoning/multimodal alignment.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/meridian</title>
    <updated>2025-02-01T01:39:38Z</updated>
    <id>tag:github.com,2025-02-01:/google/meridian</id>
    <link href="https://github.com/google/meridian" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Meridian is an MMM framework that enables advertisers to set up and run their own in-house models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;About Meridian&lt;/h1&gt; &#xA;&lt;p&gt;Marketing mix modeling (MMM) is a statistical analysis technique that measures the impact of marketing campaigns and activities to guide budget planning decisions and improve overall media effectiveness. MMM uses aggregated data to measure impact across marketing channels and account for non-marketing factors that impact sales and other key performance indicators (KPIs). MMM is privacy-safe and does not use any cookie or user-level information.&lt;/p&gt; &#xA;&lt;p&gt;Meridian is an MMM framework that enables advertisers to set up and run their own in-house models. Meridian helps you answer key questions such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How did the marketing channels drive my revenue or other KPI?&lt;/li&gt; &#xA; &lt;li&gt;What was my marketing return on investment (ROI)?&lt;/li&gt; &#xA; &lt;li&gt;How do I optimize my marketing budget allocation for the future?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Meridian is a highly customizable modeling framework that is based on &lt;a href=&#34;https://developers.google.com/meridian/docs/basics/bayesian-inference&#34;&gt;Bayesian causal inference&lt;/a&gt;. It is capable of handling large scale geo-level data, which is encouraged if available, but it can also be used for national-level modeling. Meridian provides clear insights and visualizations to inform business decisions around marketing budget and planning. Additionally, Meridian provides methodologies to support calibration of MMM with experiments and other prior information, and to optimize target ad frequency by utilizing reach and frequency data.&lt;/p&gt; &#xA;&lt;p&gt;If you are using LightweightMMM, see the &lt;a href=&#34;https://developers.google.com/meridian/docs/migrate&#34;&gt;migration guide&lt;/a&gt; to help you understand the differences between these MMM projects.&lt;/p&gt; &#xA;&lt;h1&gt;Install Meridian&lt;/h1&gt; &#xA;&lt;p&gt;Python 3.11 or 3.12 is required to use Meridian. We also recommend using a minimum of 1 GPU.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project has been tested on V100 and T4 GPU using 16 GB of RAM.&lt;/p&gt; &#xA;&lt;p&gt;To install Meridian, run the following command to automatically install the latest release from PyPI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install --upgrade google-meridian&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, run the following command to install the most recent, unreleased version from GitHub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install --upgrade git+https://github.com/google/meridian.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend to install Meridian in a fresh &lt;a href=&#34;https://virtualenv.pypa.io/en/latest/user_guide.html#quick-start&#34;&gt;virtual environment&lt;/a&gt; to make sure that correct versions of all the dependencies are installed, as defined in &lt;a href=&#34;https://github.com/google/meridian/raw/main/pyproject.toml&#34;&gt;pyproject.toml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to use the Meridian library&lt;/h2&gt; &#xA;&lt;p&gt;To get started with Meridian, you can run the code programmatically using sample data with the &lt;a href=&#34;https://developers.google.com/meridian/notebook/meridian-getting-started&#34;&gt;Getting Started Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Meridian model uses a holistic MCMC sampling approach called &lt;a href=&#34;https://www.tensorflow.org/probability/api_docs/python/tfp/experimental/mcmc/NoUTurnSampler&#34;&gt;No U Turn Sampler (NUTS)&lt;/a&gt; which can be compute intensive. To help with this, GPU support has been developed across the library (out-of-the-box) using tensors. We recommend running your Meridian model on GPUs to get real time optimization results and significantly reduce training time.&lt;/p&gt; &#xA;&lt;h1&gt;Meridian Documentation &amp;amp; Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;The following documentation, colab, and video resources will help you get started quickly with using Meridian:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resource&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian&#34;&gt;Meridian documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main landing page for Meridian documentation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/basics/about-the-project&#34;&gt;Meridian basics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn about Meridian features, methodologies, and the model math.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/notebook/meridian-getting-started&#34;&gt;Getting started colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Install and quickly learn how to use Meridian with this colab tutorial using sample data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/user-guide/installing&#34;&gt;User guide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A detailed walk-through of how to use Meridian and generating visualizations using your own data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/user-guide/collect-data&#34;&gt;Pre-modeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Prepare and analyze your data before modeling.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/advanced-modeling/control-variables&#34;&gt;Modeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Modeling guidance for model refinement and edge cases.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/advanced-modeling/model-fit&#34;&gt;Post-modeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Post-modeling guidance for model fit, visualizations, optimizations, refreshing the model, and debugging.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/migrate&#34;&gt;Migrate from LMMM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn about the differences between Meridian and LightweightMMM as you consider migrating.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/reference/api/meridian&#34;&gt;API Reference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API reference documentation for the Meridian package.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/reference-list&#34;&gt;Reference list&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;White papers and other referenced material.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Questions about methodology&lt;/strong&gt;: Please see the &lt;a href=&#34;https://developers.google.com/meridian/docs/basics/about-the-project&#34;&gt;Modeling&lt;/a&gt; tab in the technical documentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Issues installing or using Meridian&lt;/strong&gt;: Feel free to post questions in the &lt;a href=&#34;https://github.com/google/meridian/discussions&#34;&gt;Discussions&lt;/a&gt; or &lt;a href=&#34;https://github.com/google/meridian/issues&#34;&gt;Issues&lt;/a&gt; tabs of the Meridian GitHub repository. The Meridian team responds to these questions weekly in batches, so please be patient and don&#39;t reach out directly to your Google Account teams.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bug reports&lt;/strong&gt;: Please post bug reports to the &lt;a href=&#34;https://github.com/google/meridian/issues&#34;&gt;Issues&lt;/a&gt; tab of the Meridian GitHub repository. We also encourage the community to share tips and advice with each other on the &lt;a href=&#34;https://github.com/google/meridian/issues&#34;&gt;Issues&lt;/a&gt; tab. When our team addresses or resolves a new bug, we will notify you through the comments on the issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature requests&lt;/strong&gt;: Please post these to the &lt;a href=&#34;https://github.com/google/meridian/discussions&#34;&gt;Discussions&lt;/a&gt; tab of the Meridian GitHub repository. We have an internal roadmap for Meridian development, but would love your inputs for new feature requests so that we can prioritize them based on the roadmap.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pull requests&lt;/strong&gt;: These are appreciated but are very difficult for us to merge because the code in this repository is linked to Google internal systems and has to pass internal review. If you submit a pull request and we believe that we can incorporate a change in the base code, we will reach out to you directly about this.&lt;/p&gt; &#xA;&lt;h2&gt;Citing Meridian&lt;/h2&gt; &#xA;&lt;p&gt;To cite this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{meridian_github,&#xA;  author = {Google Meridian Marketing Mix Modeling Team},&#xA;  title = {Meridian: Marketing Mix Modeling},&#xA;  url = {https://github.com/google/meridian},&#xA;  version = {1.0.0},&#xA;  year = {2025},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>QwenLM/Qwen2.5-Math</title>
    <updated>2025-02-01T01:39:38Z</updated>
    <id>tag:github.com,2025-02-01:/QwenLM/Qwen2.5-Math</id>
    <link href="https://github.com/QwenLM/Qwen2.5-Math" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of math-specific large language models of our Qwen2 series.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/logo/qwen2.5_math.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/2024-08-qwen2.5-math-allsize.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ðŸ’œ &lt;a href=&#34;https://chat.qwenlm.ai/&#34;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ðŸ¤— &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ðŸ¤– &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;&lt;a href=&#34;https://www.kaggle.com/models/qwen-lm/qwen2-math&#34;&gt;Kaggle&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ðŸ“‘ &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-math/&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ï½œ &amp;nbsp;&amp;nbsp;ðŸ“– &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ðŸ«¨ &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click the links above). Search checkpoints with names starting with &lt;code&gt;Qwen2.5-Math-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;A month ago, we released the first series of mathematical LLMs - &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-math/&#34;&gt;Qwen2-Math&lt;/a&gt; - of our Qwen family. Today, we have upgraded it and open-sourced &lt;strong&gt;Qwen2.5-Math&lt;/strong&gt; series, including base models &lt;strong&gt;Qwen2.5-Math-1.5B/7B/72B&lt;/strong&gt;, instruction-tuned models &lt;strong&gt;Qwen2.5-Math-1.5B/7B/72B-Instruct&lt;/strong&gt;, and mathematical reward model &lt;strong&gt;Qwen2.5-Math-RM-72B&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Unlike Qwen2-Math series which only supports using Chain-of-Thught (CoT) to solve English math problems, Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English. The Qwen2.5-Math series models have achieved significant performance improvements compared to the Qwen2-Math series models on the Chinese and English mathematics benchmarks with CoT.&lt;/p&gt; &#xA;&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-math/&#34;&gt; ðŸ“‘ blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt; ðŸš¨ Qwen2.5-Math mainly supports solving English and Chinese math problems through CoT and TIR. We do not recommend using this series of models for other tasks. &lt;/b&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&amp;gt;=4.37.0&lt;/code&gt; for Qwen2.5-Math models. The latest version is recommended.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt; ðŸš¨ This is a must because `transformers` integrated Qwen2 codes since `4.37.0`. &lt;/b&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For requirements on GPU memory and the respective throughput, see similar results of Qwen2 &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Qwen2.5-Math-72B-Instruct&lt;/strong&gt; is an instruction model for chatting;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Qwen2.5-Math-72B&lt;/strong&gt; is a base model typically used for few-shot inference, serving as a better starting point for fine-tuning.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;ðŸ¤— Hugging Face Transformers&lt;/h3&gt; &#xA;&lt;p&gt;Qwen2.5-Math can be deployed and inferred in the same way as &lt;a href=&#34;https://github.com/QwenLM/Qwen2.5&#34;&gt;Qwen2.5&lt;/a&gt;. Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen2.5-Math-72B-Instruct&#34;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;prompt = &#34;Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.&#34;&#xA;&#xA;# CoT&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;Please reason step by step, and put your final answer within \\boxed{}.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;&#xA;# TIR&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\boxed{}.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;generated_ids = model.generate(&#xA;    **model_inputs,&#xA;    max_new_tokens=512&#xA;)&#xA;generated_ids = [&#xA;    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)&#xA;]&#xA;&#xA;response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This time, we also released a mathematical reward model, &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Math-RM-72B&#34;&gt;Qwen2.5-Math-RM-72B&lt;/a&gt;, based on Qwen2.5-Math-72B-Instruct. Qwen2.5-Math-RM-72B can be easily infered with HuggingFace Transformers with &lt;code&gt;trust_remote_code&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;p&gt;We use &lt;code&gt;temperature=0.7&lt;/code&gt; and &lt;code&gt;top_p=0.8&lt;/code&gt; for maj@8 and RM@8 sampling with &lt;strong&gt;Qwen2.5-Math-Instruct&lt;/strong&gt;, while &lt;code&gt;temperature=1.0&lt;/code&gt; and &lt;code&gt;top_p=0.9&lt;/code&gt; with &lt;strong&gt;Qwen2-Math-Instruct&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen2.5-Math-RM-72B&#34;&#xA;device = &#34;auto&#34; # the device to load the model onto&#xA;&#xA;model = AutoModel.from_pretrained(&#xA;    model_name, &#xA;    device_map=device, &#xA;    torch_dtype=torch.bfloat16,&#xA;    trust_remote_code=True,&#xA;).eval()&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&#xA;&#xA;chat = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;Please reason step by step, and put your final answer within \\boxed{}.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market?&#34;},&#xA;    {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;To determine how much Janet makes from selling the duck eggs at the farmers&#39; market, we need to follow these steps:\n\n1. Calculate the total number of eggs laid by the ducks each day.\n2. Determine how many eggs Janet eats and bakes for herself each day.\n3. Find out how many eggs are left to be sold.\n4. Calculate the revenue from selling the remaining eggs at $2 per egg.\n\nLet&#39;s start with the first step:\n\n1. Janet&#39;s ducks lay 16 eggs per day.\n\nNext, we calculate how many eggs Janet eats and bakes for herself each day:\n\n2. Janet eats 3 eggs for breakfast every morning.\n3. Janet bakes 4 eggs for her friends every day.\n\nSo, the total number of eggs Janet eats and bakes for herself each day is:\n\\[ 3 + 4 = 7 \\text{ eggs} \\]\n\nNow, we find out how many eggs are left to be sold:\n\\[ 16 - 7 = 9 \\text{ eggs} \\]\n\nFinally, we calculate the revenue from selling the remaining eggs at $2 per egg:\n\\[ 9 \\times 2 = 18 \\text{ dollars} \\]\n\nTherefore, Janet makes \\boxed{18} dollars every day at the farmers&#39; market.&#34;}&#xA;] # 3.75&#xA;&#xA;conversation_str = tokenizer.apply_chat_template(&#xA;    chat, &#xA;    tokenize=False, &#xA;    add_generation_prompt=False&#xA;)&#xA;&#xA;input_ids = tokenizer.encode(&#xA;    conversation_str, &#xA;    return_tensors=&#34;pt&#34;, &#xA;    add_special_tokens=False&#xA;).to(model.device)&#xA;&#xA;outputs = model(input_ids=input_ids)&#xA;print(outputs[0])&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ðŸ¤– ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;We strongly advise users, especially those in mainland China, to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;Local Demo (Qwen-Agent)&lt;/h3&gt; &#xA;&lt;p&gt;We developed a demo that supports the TIR mode in &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt;, which allows running code locally to experience Tool-Integrated Reasoning capabilities of Qwen2.5-Math.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;h3&gt;Base Models&lt;/h3&gt; &#xA;&lt;p&gt;We evaluate our Qwen2.5-Math base models on three widely used English math benchmarks GSM8K, Math, and MMLU-STEM. In addition, we also evaluate three Chinese math benchmarks CMATH, GaoKao Math Cloze, and GaoKao Math QA. All evaluations are tested with few-shot chain-of-thought prompting.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/qwen2.5-math-table.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;Compared to Qwen2-Math-1.5B/7B/72B, Qwen2.5-Math-1.5B/7B/72B have achieved significant improvements on all benchmarks. For example, Qwen2.5-Math-1.5B/7B/72B obtains 5.4, 5.0, 6.3 scores improvement on MATH, and 3.4, 12.2, 19.8 scores improvement on GaoKao Math QA.&lt;/p&gt; &#xA;&lt;h3&gt;Instruction-Tuned Models&lt;/h3&gt; &#xA;&lt;p&gt;We evaluate Qwen2.5-Math-Instruct on mathematical benchmarks in both English and Chinese. In addition to the widely-used benchmarks, such as GSM8K and Math, we also involve more exams that are more challenging to fully inspect the capabilities of Qwen2.5-Math-Instruct, such as OlympiadBench, CollegeMath, GaoKao, AIME2024, and AMC2023. For Chinese mathematical benchmarks, we use CMATH, Gaokao (Chinese College Entrance Examination 2024), and CN Middle School 24 (China High School Entrance Examination 2024).&lt;/p&gt; &#xA;&lt;p&gt;We report greedy, Maj@8 and RM@8 performance on all benchmarks in the zero-shot setting, except for the multi-choice benchmarks (including MMLU STEM and multiple-choice problems in GaoKao and CN Middle School 24) with a 5-shot setting.&lt;/p&gt; &#xA;&lt;p&gt;The Qwen2.5-Math-72B-Instruct model outperforms the Qwen2-Math-72B-Instruct model by an average margin of 4.4 and 6.1 points in English and Chinese, respectively, establishing itself as the best open-source mathematical model currently available.&lt;/p&gt; &#xA;&lt;p&gt;The flagship model, Qwen2.5-Math-72B-Instruct, significantly outperforms both open-source models and leading closed-source models (e.g., GPT-4o, Gemini Math-Specialized 1.5 Pro). Under the TIR setting of RM@8, a high score of 92.9 was achieved on MATH.&lt;/p&gt; &#xA;&lt;p&gt;With the aid of synthesized pre-training and supervised fine-tuning data from the 72B model, Qwen2.5-Math-7B-Instruct surpasses Qwen2-Math-Instruct 72B in performance. Under CoT and TIR settings, it achieves MATH scores of 83.6 and 85.3, respectively.&lt;/p&gt; &#xA;&lt;p&gt;Even our smallest 1.5B model, achieves a MATH score of around 80 when utilizing the Python Interpreter, outperforming the majority of current models in this domain.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/math_instruct_en.jpg#center&#34; width=&#34;800&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/math_instruct_zh.jpg#center&#34; width=&#34;800&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;In more complex mathematical competition evaluations such as AIME 2024 and AMC 2023, Qwen2.5-Math-Instruct also performs well across various settings, including Greedy, Maj@64, RM@64, and RM@256.&lt;/p&gt; &#xA;&lt;p&gt;With the support of the Qwen2.5-Math-RM-72B, Qwen2.5-Math-1.5B-Instruct, using the RM@256 in CoT mode, successfully solves 29 out of 40 problems on AMC 2023.&lt;/p&gt; &#xA;&lt;p&gt;Moreover, Qwen2.5-Math-72B-Instruct nearly achieves a perfect score in TIR mode, solving almost all the problems.&lt;/p&gt; &#xA;&lt;p&gt;On the extremely difficult AIME 2024 benchmark, Claude3 Opus, GPT-4 Turbo, and Gemini 1.5 Pro manage to solve only 1 or 2 questions out of 30.&lt;/p&gt; &#xA;&lt;p&gt;In contrast, Qwen2.5-Math-72B-Instruct solves 9 problems in Greedy decoding CoT mode and 12 problems in TIR mode. With the help of the RM, Qwen2.5-Math-7B-Instruct could even solve up to 21 problems, further demonstrating the outstanding mathematical problem-solving ability of Qwen2.5-Math-Instruct.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2.5/math_instruct_aime.jpg&#34; width=&#34;500&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Our evaluation is adapted from &lt;a href=&#34;https://github.com/ZubinGou/math-evaluation-harness&#34;&gt;math-evaluation-harness&lt;/a&gt;. Feel free to reproduce the results of all instruction models in the Qwen2.5-Math series with scripts in &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Math/main/evaluation&#34;&gt;evaluation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Before the evaluation, please install the required packages with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd latex2sympy&#xA;pip install -e .&#xA;cd ..&#xA;pip install -r requirements.txt &#xA;pip install vllm==0.5.1 --no-build-isolation&#xA;pip install transformers=4.42.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Strictly following the versions of requirements is essential to reproduce the reported scores.&lt;/p&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;p&gt;Evaluate Qwen2.5-Math-Instruct series model with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PROMPT_TYPE=&#34;qwen25-math-cot&#34;&#xA;&#xA;# Qwen2.5-Math-1.5B-Instruct&#xA;export CUDA_VISIBLE_DEVICES=&#34;0&#34;&#xA;MODEL_NAME_OR_PATH=&#34;Qwen/Qwen2.5-Math-1.5B-Instruct&#34;&#xA;bash sh/eval.sh $PROMPT_TYPE $MODEL_NAME_OR_PATH&#xA;&#xA;# Qwen2.5-Math-7B-Instruct&#xA;export CUDA_VISIBLE_DEVICES=&#34;0&#34;&#xA;MODEL_NAME_OR_PATH=&#34;Qwen/Qwen2.5-Math-7B-Instruct&#34;&#xA;bash sh/eval.sh $PROMPT_TYPE $MODEL_NAME_OR_PATH&#xA;&#xA;# Qwen2.5-Math-72B-Instruct&#xA;export CUDA_VISIBLE_DEVICES=&#34;0,1,2,3&#34;&#xA;MODEL_NAME_OR_PATH=&#34;Qwen/Qwen2.5-Math-72B-Instruct&#34;&#xA;bash sh/eval.sh $PROMPT_TYPE $MODEL_NAME_OR_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a citation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{yang2024qwen2,&#xA;  title={Qwen2 technical report},&#xA;  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},&#xA;  journal={arXiv preprint arXiv:2407.10671},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in leaving a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Math/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; â†‘ Back to Top â†‘ &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
</feed>