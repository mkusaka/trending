<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-07T01:33:44Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>modelscope/swift</title>
    <updated>2024-06-07T01:33:44Z</updated>
    <id>tag:github.com,2024-06-07:/modelscope/swift</id>
    <link href="https://github.com/modelscope/swift" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ms-swift: Use PEFT or Full-parameter to finetune 250+ LLMs or 35+ MLLMs. (Qwen2, GLM4, Internlm2, Yi, Llama3, Llava, Deepseek, ...)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/swift/main/resources/banner.png&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://modelscope.cn/home&#34;&gt;ModelScope Community Website&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &amp;nbsp; ÔΩú &amp;nbsp; English &amp;nbsp; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg?sanitize=true&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/pytorch-%E2%89%A51.12%20%7C%20%E2%89%A52.0-orange.svg?sanitize=true&#34;&gt; &lt;a href=&#34;https://github.com/modelscope/modelscope/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/modelscope-%E2%89%A51.9.5-5D91D4.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/ms-swift/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/ms-swift.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/swift&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/ms-swift&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/ms-swift&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/swift/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PR-welcome-55EB99.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/6427&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/6427&#34; alt=&#34;modelscope%2Fswift | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üìñ Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-news&#34;&gt;News&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#%EF%B8%8F-installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-License&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-Wechat-Group&#34;&gt;WeChat Group&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìù Introduction&lt;/h2&gt; &#xA;&lt;p&gt;SWIFT supports training, inference, evaluation and deployment of nearly &lt;strong&gt;200 LLMs and MLLMs&lt;/strong&gt; (multimodal large models). Developers can directly apply our framework to their own research and production environments to realize the complete workflow from model training and evaluation to application. In addition to supporting the lightweight training solutions provided by &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;, we also provide a complete &lt;strong&gt;Adapters library&lt;/strong&gt; to support the latest training techniques such as NEFTune, LoRA+, LLaMA-PRO, etc. This adapter library can be used directly in your own custom workflow without our training scripts.&lt;/p&gt; &#xA;&lt;p&gt;To facilitate use by users unfamiliar with deep learning, we provide a Gradio web-ui for controlling training and inference, as well as accompanying deep learning courses and best practices for beginners.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, we are expanding capabilities for other modalities. Currently, we support full-parameter training and LoRA training for AnimateDiff.&lt;/p&gt; &#xA;&lt;p&gt;SWIFT has rich documentations for users, please check &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/LLM&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SWIFT web-ui is available both on &lt;a href=&#34;https://huggingface.co/spaces/tastelikefeet/swift&#34;&gt;Huggingface space&lt;/a&gt; and &lt;a href=&#34;https://www.modelscope.cn/studios/iic/Scalable-lightWeight-Infrastructure-for-Fine-Tuning/summary&#34;&gt;ModelScope studio&lt;/a&gt;, please feel free to try!&lt;/p&gt; &#xA;&lt;h2&gt;üéâ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî•2024.06.07: Support &lt;strong&gt;Qwen2&lt;/strong&gt; series LLM, including Base and Instruct models of 0.5B, 1.5B, 7B, and 72B, as well as corresponding quantized versions gptq-int4, gptq-int8, and awq-int4. The best practice for self-cognition fine-tuning, inference and deployment of Qwen2-72B-Instruct using dual-card 80GiB A100 can be found &lt;a href=&#34;https://github.com/modelscope/swift/issues/1092&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.06.05: Support for &lt;strong&gt;glm4&lt;/strong&gt; series LLM and glm4v-9b-chat MLLM. You can refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/Multi-Modal/glm4v-best-practice.md&#34;&gt;glm4v best practice&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.06.01: Supoprts &lt;strong&gt;SimPO&lt;/strong&gt; training! See &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source_en/LLM/SimPO.md&#34;&gt;document&lt;/a&gt; to start training!&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.06.01: Support for deploying large multimodal models, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/Multi-Modal/mutlimodal-deployment.md&#34;&gt;Multimodal Deployment Documentation&lt;/a&gt; for more information.&lt;/li&gt; &#xA; &lt;li&gt;2024.05.31: Supports Mini-Internvl model, Use model_type &lt;code&gt;mini-internvl-chat-2b-v1_5&lt;/code&gt; and &lt;code&gt;mini-internvl-chat-4b-v1_5&lt;/code&gt;to train.&lt;/li&gt; &#xA; &lt;li&gt;2024.05.24: Supports Phi3-vision model, Use model_type &lt;code&gt;phi3-vision-128k-instruct&lt;/code&gt; to train.&lt;/li&gt; &#xA; &lt;li&gt;2024.05.22: Supports DeepSeek-V2-Lite series models, model_type are &lt;code&gt;deepseek-v2-lite&lt;/code&gt; and &lt;code&gt;deepseek-v2-lite-chat&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;2024.05.22: Supports TeleChat-12B-v2 model with quantized version, model_type are &lt;code&gt;telechat-12b-v2&lt;/code&gt; and &lt;code&gt;telechat-12b-v2-gptq-int4&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.05.21: Inference and fine-tuning support for MiniCPM-Llama3-V-2_5 are now available. For more details, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source/Multi-Modal/minicpm-v-2.5%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;minicpm-v-2.5 Best Practice&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.05.20: Support for inferencing and fine-tuning cogvlm2-llama3-chinese-chat-19B, cogvlm2-llama3-chat-19B. you can refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/Multi-Modal/cogvlm2-best-practice.md&#34;&gt;cogvlm2 Best Practice&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.05.17: Support peft=0.11.0. Meanwhile support 3 new tuners: &lt;code&gt;BOFT&lt;/code&gt;, &lt;code&gt;Vera&lt;/code&gt; and &lt;code&gt;Pissa&lt;/code&gt;. use &lt;code&gt;--sft_type boft/vera&lt;/code&gt; to use BOFT or Vera, use &lt;code&gt;--init_lora_weights pissa&lt;/code&gt; with &lt;code&gt;--sft_type lora&lt;/code&gt; to use Pissa.&lt;/li&gt; &#xA; &lt;li&gt;2024.05.16: Supports Llava-Next (Stronger) series models. For best practice, you can refer to &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/llava-best-practice.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.05.13: Support Yi-1.5 series modelsÔºåuse &lt;code&gt;--model_type yi-1_5-9b-chat&lt;/code&gt; to begin!&lt;/li&gt; &#xA; &lt;li&gt;2024.05.11: Support for qlora training and quantized inference using &lt;a href=&#34;https://github.com/mobiusml/hqq&#34;&gt;hqq&lt;/a&gt; and &lt;a href=&#34;https://github.com/NetEase-FuXi/EETQ&#34;&gt;eetq&lt;/a&gt;. For more information, see the &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/LLM/LLM-quantization.md&#34;&gt;LLM Quantization Documentation&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;2024.05.10: Support split a sequence to multiple GPUs to reduce memory usage. Use this feature by &lt;code&gt;pip install .[seq_parallel]&lt;/code&gt;, then add &lt;code&gt;--sequence_parallel_size n&lt;/code&gt; to your DDP script to begin!&lt;/li&gt; &#xA; &lt;li&gt;2024.05.08: Support DeepSeek-V2-Chat model, you can refer to &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/deepseek-v2-chat/lora_ddp_ds3/sft.sh&#34;&gt;this script&lt;/a&gt;.Support InternVL-Chat-V1.5-Int8 model, for best practice, you can refer to &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.05.07: Supoprts &lt;strong&gt;ORPO&lt;/strong&gt; training! See &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source_en/LLM/ORPO.md&#34;&gt;document&lt;/a&gt; to start training!&lt;/li&gt; &#xA; &lt;li&gt;2024.05.07: Supports Llava-Llama3 model from xtunerÔºåmodel_type is &lt;code&gt;llava-llama-3-8b-v1_1&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;2024.04.29: Supports inference and fine-tuning of InternVL-Chat-V1.5 model. For best practice, you can refer to &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.04.26: Support &lt;strong&gt;LISA&lt;/strong&gt; and &lt;strong&gt;unsloth&lt;/strong&gt; training! Specify &lt;code&gt;--lisa_activated_layers=2&lt;/code&gt; to use LISA(to reduce the memory cost to 30 percent!), specify &lt;code&gt;--tuner_backend unsloth&lt;/code&gt; to use unsloth to train a huge model(full or lora) with lesser memory(30 percent or lesser) and faster speed(5x)!&lt;/li&gt; &#xA; &lt;li&gt;üî•2024.04.26: Support the fine-tuning and inference of Qwen1.5-110B and Qwen1.5-110B-Chat model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/qwen1half_110b_chat/lora_ddp_ds/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;More&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;2024.04.24: Support for inference and fine-tuning of Phi3 series models. Including: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/examples/pytorch/llm/scripts/phi3_4b_4k_instruct/lora&#34;&gt;phi3-4b-4k-instruct&lt;/a&gt;, phi3-4b-128k-instruct.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.22: Support for inference, fine-tuning, and deployment of &lt;strong&gt;chinese-llama-alpaca-2&lt;/strong&gt; series models. This includesÔºöchinese-llama-2-1.3b, chinese-llama-2-7b, chinese-llama-2-13b, chinese-alpaca-2-1.3b, chinese-alpaca-2-7b and chinese-alpaca-2-13b along with their corresponding 16k and 64k long text versions.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.22: Support for inference and fine-tuning of Llama3 GPTQ-Int4, GPTQ-Int8, and AWQ series models. Support for inference and fine-tuning of chatglm3-6b-128k, Openbuddy-Llama3.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.20: Support for inference, fine-tuning, and deployment of &lt;strong&gt;Atom&lt;/strong&gt; series models. This includes: Atom-7B and Atom-7B-Chat. use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/atom_7b_chat/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to train.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.19: Support for single-card, DDP, ZeRO2, and ZeRO3 training and inference with NPU, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/NPU-best-practice.md&#34;&gt;NPU Inference and Fine-tuning Best Practice&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.19: Support for inference, fine-tuning, and deployment of &lt;strong&gt;Llama3&lt;/strong&gt; series models. This includes: Llama-3-8B, Llama-3-8B-Instruct, Llama-3-70B, and Llama-3-70B-Instruct. use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/llama3_8b_instruct/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to train.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.18: Supported models: wizardlm2-7b-awq, wizardlm2-8x22b, yi-6b-chat-awq, yi-6b-chat-int8, yi-34b-chat-awq, yi-34b-chat-int8. Supported &lt;code&gt;--deepspeed zero3-offload&lt;/code&gt; and provided default zero3-offload configuration file for zero3+cpu offload usage.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.18: Supported compatibility with HuggingFace ecosystem using the environment variable &lt;code&gt;USE_HF&lt;/code&gt;, switching to use models and datasets from HF. Please refer to the &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/LLM/Compat-HF.md&#34;&gt;HuggingFace ecosystem compatibility documentation&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.17: Support the evaluation for OpenAI standard interfaces. Check the &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Command-line-parameters.md#eval-parameters&#34;&gt;parameter documentation&lt;/a&gt; for details.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.04.17: Support &lt;strong&gt;CodeQwen1.5-7B&lt;/strong&gt; series: CodeQwen1.5-7B, CodeQwen1.5-7B-Chat,CodeQwen1.5-7B-Chat-AWQ, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/codeqwen1half_7b_chat/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to train.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.16: Supports inference and fine-tuning of llava-v1.6-34b model. For best practice, you can refer to &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/llava-best-practice.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.13: Support the fine-tuning and inference of Mixtral-8x22B-v0.1 model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/mixtral_moe_8x22b_v1/lora_ddp_ds/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.13: Support the newly launched &lt;strong&gt;MiniCPM&lt;/strong&gt; series: MiniCPM-V-2.0„ÄÅMiniCPM-2B-128k„ÄÅMiniCPM-MoE-8x2B and MiniCPM-1B.use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/minicpm_moe_8x2b/lora_ddp/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.04.11: Support Model Evaluation with MMLU/ARC/CEval datasets(also user custom eval datasets) with one command! Check &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/LLM-eval.md&#34;&gt;this documentation&lt;/a&gt; for details. Meanwhile, we support a trick way to do multiple ablation experiments, check &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/LLM-exp.md&#34;&gt;this documentation&lt;/a&gt; to use.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.04.11: Support &lt;strong&gt;c4ai-command-r&lt;/strong&gt; series: c4ai-command-r-plus, c4ai-command-r-v01, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/c4ai_command_r_plus/lora_mp/sft.sh&#34;&gt;this script&lt;/a&gt; to train.&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.10: Use SWIFT to fine-tune the qwen-7b-chat model to enhance its function call capabilities, and combine it with &lt;a href=&#34;https://github.com/modelscope/modelscope-agent&#34;&gt;Modelscope-Agent&lt;/a&gt; for best practices, which can be found &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/LLM/Agent-best-practice.md#Usage-with-Modelscope_Agent&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.04.09: Support ruozhiba dataset. Search &lt;code&gt;ruozhiba&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Supported-models-datasets.md&#34;&gt;this documentation&lt;/a&gt; to begin training!&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.08: Support the fine-tuning and inference of XVERSE-MoE-A4.2B model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/xverse_moe_a4_2b/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;2024.04.04: Support &lt;strong&gt;QLoRA+FSDP&lt;/strong&gt; to train a 70B model with two 24G memory GPUs, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/llama2_70b_chat/qlora_fsdp/sft.sh&#34;&gt;this script&lt;/a&gt; to train.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.04.03: Support &lt;strong&gt;Qwen1.5-32B&lt;/strong&gt; series: Qwen1.5-32B, Qwen1.5-32B-Chat, Qwen1.5-32B-Chat-GPTQ-Int4.use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/qwen1half_32b_chat/lora_mp/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.04.02: Support the fine-tuning and inference of Mengzi3-13B-Base model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/mengzi3_13b_base/lora_ddp_ds/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.04.01: Support &lt;strong&gt;dbrx&lt;/strong&gt; series: dbrx-base and dbrx-instruct, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/dbrx-instruct/lora_mp/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.29: Support &lt;strong&gt;Qwen1.5-MoE&lt;/strong&gt; series: Qwen1.5-MoE-A2.7B, Qwen1.5-MoE-A2.7B-Chat, Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.29: Support the fine-tuning and inference of &lt;strong&gt;Grok-1&lt;/strong&gt; 300B MoE, please view details &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/LLM/Grok-1-best-practice.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.25: Supports inference and fine-tuning of TeleChat-7b and TeleChat-12b model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/telechat_12b/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.20: Supports inference and fine-tuning for the &lt;strong&gt;llava&lt;/strong&gt; series. For best practice, you can refer to &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/llava-best-practice.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.12: Support inference and fine-tuning for &lt;strong&gt;deepseek-vl&lt;/strong&gt; series. Best practices can be found &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/Multi-Modal/deepseek-vl-best-practice.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.11: Support &lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore&lt;/a&gt; for effectively reducing memory usage to 1/2 of the original in full-parameter training.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.10: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Qwen1.5-best-practice.md&#34;&gt;End-to-end best practices&lt;/a&gt; from fine-tuning to deployment for Qwen1.5-7B-Chat and Qwen1.5-72B-Chat.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.03.09: Support training and inference of MAMBA model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/mamba-1.4b/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;2024.03.09: Support training and inference of AQLM quantized model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/llama2_7b_aqlm_2bit_1x16/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;2024.03.06: Support training and inference of AWQ quantized model, use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/qwen1half_7b_chat_awq/lora/sft.sh&#34;&gt;this Qwen1.5-AWQ model script&lt;/a&gt; to start training, and support training and inference of &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/yi_9b/lora_zero3&#34;&gt;yi-9b&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.02.29: Support &lt;a href=&#34;https://arxiv.org/pdf/2401.02415.pdf&#34;&gt;LLaMA PRO&lt;/a&gt;, simply use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/yi_6b_chat/llamapro/sft.sh&#34;&gt;this script&lt;/a&gt; to start training.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.02.29: Support &lt;a href=&#34;https://arxiv.org/pdf/2402.12354.pdf&#34;&gt;LoRA+&lt;/a&gt;, simply use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/yi_6b_chat/lorap/sft.sh&#34;&gt;this script&lt;/a&gt; to start training.&lt;/li&gt; &#xA;  &lt;li&gt;2024.02.25: Support &lt;code&gt;swift export&lt;/code&gt; to quantize models using &lt;strong&gt;AWQ/GPTQ&lt;/strong&gt; and push to ModelScope Hub. See documentation: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/LLM-quantization.md&#34;&gt;LLM Quantization&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2024.02.22: Support gemma series: gemma-2b, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/gemma_2b_instruct&#34;&gt;gemma-2b-instruct&lt;/a&gt;, gemma-7b, gemma-7b-instruct.&lt;/li&gt; &#xA;  &lt;li&gt;2024.02.16: Support deepseek-math series: deepseek-math-7b, deepseek-math-7b-instruct, deepseek-math-7b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.02.05: Support &lt;strong&gt;Qwen1.5&lt;/strong&gt; series models, see &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/LLM/%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86.md#%E6%A8%A1%E5%9E%8B&#34;&gt;model list&lt;/a&gt; for all supported Qwen1.5 models. Provide fine-tuning scripts for &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat&#34;&gt;qwen1half-7b-chat&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat_int8&#34;&gt;qwen1half-7b-chat-int8&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2024.02.05: Support training of diffusion models such as &lt;strong&gt;SDXL&lt;/strong&gt;, &lt;strong&gt;SD&lt;/strong&gt;, &lt;strong&gt;ControlNet&lt;/strong&gt;, as well as &lt;strong&gt;DreamBooth&lt;/strong&gt; training. See corresponding &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/sdxl/scripts&#34;&gt;training scripts&lt;/a&gt; for details.&lt;/li&gt; &#xA;  &lt;li&gt;2024.02.01: Support minicpm series: &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/minicpm_2b_sft_chat&#34;&gt;minicpm-2b-sft-chat&lt;/a&gt;, minicpm-2b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.02.01: Support dataset mixing to reduce &lt;strong&gt;catastrophic forgetting&lt;/strong&gt;. Use &lt;code&gt;--train_dataset_mix_ratio 2.0&lt;/code&gt; to enable training! We also open sourced the general knowledge dataset &lt;a href=&#34;https://www.modelscope.cn/datasets/iic/ms_bench/summary&#34;&gt;ms-bench&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.02.01: Support Agent training! Agent training algorithm is derived from this &lt;a href=&#34;https://arxiv.org/pdf/2309.00986.pdf&#34;&gt;paper&lt;/a&gt;. We also added &lt;a href=&#34;https://www.modelscope.cn/datasets/iic/ms_agent/summary&#34;&gt;ms-agent&lt;/a&gt;, a high-quality agent dataset. Use &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/examples/pytorch/llm/scripts/qwen_7b_chat/lora/sft.sh&#34;&gt;this script&lt;/a&gt; to start Agent training!&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.02.01: Support adding SFT loss in DPO training to reduce repetitive generation caused by KL divergence loss.&lt;/li&gt; &#xA;  &lt;li&gt;2024.02.01: Support using AdaLoRA and IA3 adapters in training.&lt;/li&gt; &#xA;  &lt;li&gt;2024.02.01: Support &lt;code&gt;--merge_lora&lt;/code&gt; parameter in AnimateDiff training.&lt;/li&gt; &#xA;  &lt;li&gt;2024.01.30: Support &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/internlm_xcomposer2_7b_chat&#34;&gt;internlm-xcomposer2-7b-chat&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.01.30: Support &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_14b_chat/full_ddp_zero3/&#34;&gt;ZeRO-3&lt;/a&gt;, simply specify &lt;code&gt;--deepspeed default-zero3&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2024.01.29: Support internlm2-math series: internlm2-math-7b, internlm2-math-7b-chat, internlm2-math-20b, internlm2-math-20b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.01.26: Support &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_vl_6b_chat&#34;&gt;yi-vl-6b-chat&lt;/a&gt;, yi-vl-34b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;2024.01.24: Support codefuse-codegeex2-6b-chat, codefuse-qwen-14b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;2024.01.23: Support orion series: orion-14b, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/orion_14b_chat&#34;&gt;orion-14b-chat&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2024.01.20: Support &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/xverse_13b_256k&#34;&gt;xverse-13b-256k&lt;/a&gt;, xverse-65b-v2, xverse-65b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.01.17: Support internlm2 series: internlm2-7b-base, internlm2-7b, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/internlm2_7b_sft_chat&#34;&gt;internlm2-7b-sft-chat&lt;/a&gt;, internlm2-7b-chat, internlm2-20b-base, internlm2-20b, internlm2-20b-sft-chat, internlm2-20b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;2024.01.15: Support yuan series: yuan2-2b-instruct, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yuan2_2b_janus_instruct&#34;&gt;yuan2-2b-janus-instruct&lt;/a&gt;, yuan2-51b-instruct, yuan2-102b-instruct.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.01.12: Support &lt;strong&gt;deepseek-moe&lt;/strong&gt; series: deepseek-moe-16b, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/deepseek_moe_16b_chat&#34;&gt;deepseek-moe-16b-chat&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2024.01.04: Support &lt;strong&gt;VLLM deployment&lt;/strong&gt;, compatible with &lt;strong&gt;OpenAI API&lt;/strong&gt; style, see &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source_en/LLM/VLLM-inference-acceleration-and-deployment.md#Deployment&#34;&gt;VLLM Inference Acceleration and Deployment&lt;/a&gt; for details.&lt;/li&gt; &#xA;  &lt;li&gt;2024.01.04: Update &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/LLM/Benchmark.md&#34;&gt;Benchmark&lt;/a&gt; for convenient viewing of training speed and memory usage of different models.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.12.29: Support web-ui for sft training and inference, use &lt;code&gt;swift web-ui&lt;/code&gt; after installing ms-swift to start.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.12.29: Support DPO RLHF (Reinforcement Learning from Human Feedback) and three datasets for this task: AI-ModelScope/stack-exchange-paired, AI-ModelScope/hh-rlhf and AI-ModelScope/hh_rlhf_cn. See &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source_en/LLM/DPO.md&#34;&gt;documentation&lt;/a&gt; to start training!&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.12.28: Support SCEdit! This tuner can significantly reduce memory usage in U-Net and support low-memory controllable image generation (replacing ControlNet), read the section below to learn more.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.23: Support &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/codegeex2_6b&#34;&gt;codegeex2-6b&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.19: Support &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/phi2_3b&#34;&gt;phi2-3b&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.18: Support VLLM for inference acceleration.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.15: Support deepseek, deepseek-coder series: deepseek-7b, deepseek-7b-chat, deepseek-67b, deepseek-67b-chat, openbuddy-deepseek-67b-chat, deepseek-coder-1_3b, deepseek-coder-1_3b-instruct, deepseek-coder-6_7b, deepseek-coder-6_7b-instruct, deepseek-coder-33b, deepseek-coder-33b-instruct.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.13: Support mistral-7b-instruct-v2, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/mixtral_7b_moe&#34;&gt;mixtral-moe-7b&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/mixtral_7b_moe_instruct&#34;&gt;mixtral-moe-7b-instruct&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.09: Support &lt;code&gt;freeze_parameters&lt;/code&gt; parameter as a compromise between lora and full-parameter training. Corresponding sh can be found in &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat/full_freeze_ddp&#34;&gt;full_freeze_ddp&lt;/a&gt;. Support &lt;code&gt;disable_tqdm&lt;/code&gt;, &lt;code&gt;lazy_tokenize&lt;/code&gt;, &lt;code&gt;preprocess_num_proc&lt;/code&gt; parameters, see &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/LLM/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0.md&#34;&gt;command line arguments&lt;/a&gt; for details.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.08: Support &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/sus_34b_chat&#34;&gt;sus-34b-chat&lt;/a&gt;, support yi-6b-200k, yi-34b-200k.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.07: Support &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/LLM/LLM%E5%BE%AE%E8%B0%83%E6%96%87%E6%A1%A3.md#%E4%BD%BF%E7%94%A8cli&#34;&gt;Multi-Node DDP training&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2023.12.05: Support models: zephyr-7b-beta-chat, openbuddy-zephyr-7b-chat. Support datasets: hc3-zh, hc3-en.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.12.02: &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Self-cognition-best-practice.md&#34;&gt;Self-cognition fine-tuning best practices&lt;/a&gt;, &lt;strong&gt;10 minutes to fine-tune a large model for self-cognition&lt;/strong&gt;, create your own unique large model.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.30: Support training and inference of &lt;strong&gt;qwen-1_8b&lt;/strong&gt;, &lt;strong&gt;qwen-72b&lt;/strong&gt;, &lt;strong&gt;qwen-audio&lt;/strong&gt; series models. Corresponding sh scripts can be found in &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_1_8b_chat&#34;&gt;qwen_1_8b_chat&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_72b_chat&#34;&gt;qwen_72b_chat&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_audio_chat&#34;&gt;qwen_audio_chat&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.29: Support training and inference of &lt;strong&gt;AnimateDiff&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.24: Support &lt;strong&gt;yi-34b-chat&lt;/strong&gt;, &lt;strong&gt;codefuse-codellama-34b-chat&lt;/strong&gt; models. Corresponding sh scripts can be found in &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b_chat&#34;&gt;yi_34b_chat&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/codefuse_codellama_34b_chat&#34;&gt;codefuse_codellama_34b_chat&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.18: Support &lt;strong&gt;tongyi-finance-14b&lt;/strong&gt; series models: tongyi-finance-14b, tongyi-finance-14b-chat, tongyi-finance-14b-chat-int4. Corresponding sh scripts can be found in &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/tongyi_finance_14b_chat_int4&#34;&gt;tongyi_finance_14b_chat_int4&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2023.11.16: Support &lt;strong&gt;flash attn&lt;/strong&gt; for more models: qwen series, qwen-vl series, llama series, openbuddy series, mistral series, yi series, ziya series. Please use &lt;code&gt;use_flash_attn&lt;/code&gt; parameter.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.11: Support &lt;strong&gt;NEFTune&lt;/strong&gt;, simply use &lt;code&gt;Swift.prepare_model(model, NEFTuneConfig())&lt;/code&gt; to enable.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.11: Support training and inference by &lt;strong&gt;command line&lt;/strong&gt; and inference by &lt;strong&gt;Web-UI&lt;/strong&gt;, see &lt;code&gt;Usage with Swift CLI&lt;/code&gt; section below for details.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.10: Support &lt;strong&gt;bluelm&lt;/strong&gt; series models: bluelm-7b, bluelm-7b-chat, bluelm-7b-32k, bluelm-7b-chat-32k. Corresponding sh scripts can be found in &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/bluelm_7b_chat&#34;&gt;bluelm_7b_chat&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.08: Support training and inference of &lt;strong&gt;xverse-65b&lt;/strong&gt; model, script at &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/xverse_65b&#34;&gt;xverse_65b&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.11.07: Support training and inference of &lt;strong&gt;yi-6b&lt;/strong&gt;, &lt;strong&gt;yi-34b&lt;/strong&gt; models, scripts at &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_6b&#34;&gt;yi_6b&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b&#34;&gt;yi_34b&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.10.30: Support two new tuners: &lt;strong&gt;QA-LoRA&lt;/strong&gt; and &lt;strong&gt;LongLoRA&lt;/strong&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.10.30: Support editing models using &lt;strong&gt;ROME&lt;/strong&gt; (Rank One Model Editing) to infuse new knowledge into models without training!&lt;/li&gt; &#xA;  &lt;li&gt;2023.10.30: Support &lt;strong&gt;skywork-13b&lt;/strong&gt; series models: skywork-13b, skywork-13b-chat. Corresponding sh scripts can be found in &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/skywork_13b&#34;&gt;skywork_13b&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.10.27: Support &lt;strong&gt;chatglm3&lt;/strong&gt; series models: chatglm3-6b-base, chatglm3-6b, chatglm3-6b-32k. Corresponding sh scripts can be found in &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/chatglm3_6b&#34;&gt;chatglm3_6b&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.10.17: Support SFT of &lt;strong&gt;int4&lt;/strong&gt;, &lt;strong&gt;int8&lt;/strong&gt; models: qwen-7b-chat-int4, qwen-14b-chat-int4, qwen-vl-chat-int4, baichuan2-7b-chat-int4, baichuan2-13b-chat-int4, qwen-7b-chat-int8, qwen-14b-chat-int8.&lt;/li&gt; &#xA;  &lt;li&gt;2023.10.15: Support &lt;strong&gt;ziya2-13b&lt;/strong&gt; series models: ziya2-13b, ziya2-13b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;2023.10.12: Support &lt;strong&gt;mistral-7b&lt;/strong&gt; series models: openbuddy-mistral-7b-chat, mistral-7b, mistral-7b-instruct.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.10.07: Support &lt;strong&gt;DeepSpeed ZeRO-2&lt;/strong&gt;, enabling lora (not just qlora) to run DDP on dual A10 cards.&lt;/li&gt; &#xA;  &lt;li&gt;2023.10.04: Support more math, law, SQL, code domain datasets: blossom-math-zh, school-math-zh, text2sql-en, sql-create-context-en, lawyer-llama-zh, tigerbot-law-zh, leetcode-python-en.&lt;/li&gt; &#xA;  &lt;li&gt;üî•2023.09.25: Support &lt;strong&gt;qwen-14b&lt;/strong&gt; series: qwen-14b, qwen-14b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;2023.09.18: Support &lt;strong&gt;internlm-20b&lt;/strong&gt; series: internlm-20b, internlm-20b-chat.&lt;/li&gt; &#xA;  &lt;li&gt;2023.09.12: Support &lt;strong&gt;MP+DDP&lt;/strong&gt; to accelerate full-parameter training.&lt;/li&gt; &#xA;  &lt;li&gt;2023.09.05: Support &lt;strong&gt;openbuddy-llama2-70b-chat&lt;/strong&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;2023.09.03: Support &lt;strong&gt;baichuan2&lt;/strong&gt; series: baichuan2-7b, baichuan2-7b-chat, baichuan2-13b, baichuan2-13b-chat.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; &#xA;&lt;p&gt;SWIFT runs in the Python environment. Please ensure your Python version is higher than 3.8.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Method 1: Install SWIFT using pip command:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Full capabilities&#xA;pip install &#39;ms-swift[all]&#39; -U&#xA;# LLM only&#xA;pip install &#39;ms-swift[llm]&#39; -U&#xA;# AIGC only&#xA;pip install &#39;ms-swift[aigc]&#39; -U&#xA;# Adapters only&#xA;pip install ms-swift -U&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Method 2: Install SWIFT through source code (convenient for running training and inference scripts), please run the following commands:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/modelscope/swift.git&#xA;cd swift&#xA;pip install -e &#39;.[llm]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;SWIFT depends on torch&amp;gt;=1.13, recommend torch&amp;gt;=2.0.0.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Method 3: Use SWIFT in our Docker image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# China-Hangzhou image&#xA;docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.13.1&#xA;# US-west image&#xA;docker pull registry.us-west-1.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.13.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;This section introduces basic usage, see the &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/#-documentation&#34;&gt;Documentation&lt;/a&gt; section for more ways to use.&lt;/p&gt; &#xA;&lt;h3&gt;Web-UI&lt;/h3&gt; &#xA;&lt;p&gt;Web-UI is a gradio-based interface for &lt;strong&gt;zero-threshold&lt;/strong&gt; training and deployment. It is easy to use and perfectly supports multi-GPU training and deployment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;SWIFT_UI_LANG=en swift web-ui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/resources/web-ui-en.jpg&#34; alt=&#34;image.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;h4&gt;Training Scripts&lt;/h4&gt; &#xA;&lt;p&gt;You can refer to the following scripts to customize your own training script.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;full: &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat/full&#34;&gt;qwen1half-7b-chat&lt;/a&gt; (A100), &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat/full_mp&#34;&gt;qwen-7b-chat&lt;/a&gt; (2*A100)&lt;/li&gt; &#xA; &lt;li&gt;full+ddp+zero2: &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat/full_ddp_zero2&#34;&gt;qwen-7b-chat&lt;/a&gt; (4*A100)&lt;/li&gt; &#xA; &lt;li&gt;full+ddp+zero3: &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_14b_chat/full_ddp_zero3&#34;&gt;qwen-14b-chat&lt;/a&gt; (4*A100)&lt;/li&gt; &#xA; &lt;li&gt;lora: &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/chatglm3_6b/lora&#34;&gt;chatglm3-6b&lt;/a&gt; (3090), &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/baichuan2_13b_chat/lora_mp&#34;&gt;baichuan2-13b-chat&lt;/a&gt; (2*3090), &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b_chat/lora&#34;&gt;yi-34b-chat&lt;/a&gt; (A100), &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_72b_chat/lora_mp&#34;&gt;qwen-72b-chat&lt;/a&gt; (2*A100)&lt;/li&gt; &#xA; &lt;li&gt;lora+ddp: &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/chatglm3_6b/lora_ddp&#34;&gt;chatglm3-6b&lt;/a&gt; (2*3090)&lt;/li&gt; &#xA; &lt;li&gt;lora+ddp+zero3: &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_14b_chat/lora_ddp_zero3&#34;&gt;qwen-14b-chat&lt;/a&gt; (4*3090), &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_72b_chat/lora_ddp_zero3&#34;&gt;qwen-72b-chat&lt;/a&gt; (4*A100)&lt;/li&gt; &#xA; &lt;li&gt;qlora(gptq-int4): &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat_int4/qlora&#34;&gt;qwen-7b-chat-int4&lt;/a&gt; (3090)&lt;/li&gt; &#xA; &lt;li&gt;qlora(gptq-int8): &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat_int8/qlora&#34;&gt;qwen1half-7b-chat-int8&lt;/a&gt; (3090)&lt;/li&gt; &#xA; &lt;li&gt;qlora(bnb-int4): &lt;a href=&#34;https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat/qlora&#34;&gt;qwen-7b-chat&lt;/a&gt; (3090)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Supported Training Processes&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Training Process&lt;/th&gt; &#xA;   &lt;th&gt;Training Method&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pretraining&lt;/td&gt; &#xA;   &lt;td&gt;Text Generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;Single-turn/Multi-turn&lt;br&gt;Agent Training/Self-cognition&lt;br&gt;Multi-modal Vision/Multi-modal Speech&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Human Alignment&lt;/td&gt; &#xA;   &lt;td&gt;DPO&lt;br&gt;ORPO&lt;br&gt;SimPO&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-to-Image&lt;/td&gt; &#xA;   &lt;td&gt;DreamBooth, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text-to-Video&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Single GPU Training&lt;/h4&gt; &#xA;&lt;p&gt;Start single GPU fine-tuning with the following command:&lt;/p&gt; &#xA;&lt;p&gt;LoRA:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: A100&#xA;# GPU Memory Requirement: 20GB&#xA;# Runtime: 3.1 hours&#xA;CUDA_VISIBLE_DEVICES=0 \&#xA;swift sft \&#xA;    --model_type qwen1half-7b-chat \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type lora \&#xA;    --output_dir output \&#xA;    --eval_steps 200 \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Full-parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: A100&#xA;# GPU Memory Requirement: 80GB&#xA;# Runtime: 2.5 hours&#xA;CUDA_VISIBLE_DEVICES=0 \&#xA;swift sft \&#xA;    --model_type qwen1half-7b-chat \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type full \&#xA;    --output_dir output \&#xA;    --eval_steps 500 \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Model Parallel Training&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: 2 * A100&#xA;# GPU Memory Requirement: 10GB + 13GB&#xA;# Runtime: 3.4 hours&#xA;CUDA_VISIBLE_DEVICES=0,1 \&#xA;swift sft \&#xA;    --model_type qwen1half-7b-chat \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type lora \&#xA;    --output_dir output \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Data Parallel Training&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: 4 * A100&#xA;# GPU Memory Requirement: 4 * 30GB&#xA;# Runtime: 0.8 hours&#xA;NPROC_PER_NODE=4 \&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3 \&#xA;swift sft \&#xA;    --model_type qwen1half-7b-chat \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type lora \&#xA;    --output_dir output \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Combining Model Parallelism and Data Parallelism:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: 4 * A100&#xA;# GPU Memory Requirement: 2*14GB + 2*18GB&#xA;# Runtime: 1.7 hours&#xA;NPROC_PER_NODE=2 \&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3 \&#xA;swift sft \&#xA;    --model_type qwen1half-7b-chat \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type lora \&#xA;    --output_dir output \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Deepspeed Training&lt;/h4&gt; &#xA;&lt;p&gt;Deepspeed supports training of quantized GPTQ and AWQ models.&lt;/p&gt; &#xA;&lt;p&gt;ZeRO2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: 4 * A100&#xA;# GPU Memory Requirement: 4 * 21GB&#xA;# Runtime: 0.9 hours&#xA;NPROC_PER_NODE=4 \&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3 \&#xA;swift sft \&#xA;    --model_type qwen1half-7b-chat \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type lora \&#xA;    --output_dir output \&#xA;    --deepspeed default-zero2 \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ZeRO3:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: 4 * A100&#xA;# GPU Memory Requirement: 4 * 19GB&#xA;# Runtime: 3.2 hours&#xA;NPROC_PER_NODE=4 \&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3 \&#xA;swift sft \&#xA;    --model_type qwen1half-7b-chat \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type lora \&#xA;    --output_dir output \&#xA;    --deepspeed default-zero3 \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ZeRO3-Offload:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Experimental Environment: 4 * A100&#xA;# GPU Memory Requirement: 4 * 12GB&#xA;# Runtime: 60 hours&#xA;NPROC_PER_NODE=4 \&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3 \&#xA;swift sft \&#xA;    --model_id_or_path AI-ModelScope/WizardLM-2-8x22B \&#xA;    --dataset blossom-math-zh \&#xA;    --num_train_epochs 5 \&#xA;    --sft_type lora \&#xA;    --output_dir output \&#xA;    --deepspeed zero3-offload \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi-node Multi-GPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# node0&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \&#xA;NNODES=2 \&#xA;NODE_RANK=0 \&#xA;MASTER_ADDR=127.0.0.1 \&#xA;NPROC_PER_NODE=8 \&#xA;swift sft \&#xA;    --model_id_or_path qwen1half-32b-chat \&#xA;    --sft_type full \&#xA;    --dataset blossom-math-zh \&#xA;    --output_dir output \&#xA;    --deepspeed default-zero3 \&#xA;&#xA;# node1&#xA;CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \&#xA;NNODES=2 \&#xA;NODE_RANK=1 \&#xA;MASTER_ADDR=xxx.xxx.xxx.xxx \&#xA;NPROC_PER_NODE=8 \&#xA;swift sft \&#xA;    --model_id_or_path qwen1half-32b-chat \&#xA;    --sft_type full \&#xA;    --dataset blossom-math-zh \&#xA;    --output_dir output \&#xA;    --deepspeed default-zero3 \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;AliYun-DLC multi-node training&lt;/h5&gt; &#xA;&lt;p&gt;In DLC product, WORLD_SIZE is the node number, RANK is the node index, this is different from the definition of torchrun.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;NNODES=$WORLD_SIZE \&#xA;NODE_RANK=$RANK \&#xA;swift sft \&#xA;    --model_id_or_path qwen1half-32b-chat \&#xA;    --sft_type full \&#xA;    --dataset blossom-math-zh \&#xA;    --output_dir output \&#xA;    --deepspeed default-zero3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Original model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 swift infer --model_type qwen1half-7b-chat&#xA;# use VLLM&#xA;CUDA_VISIBLE_DEVICES=0 swift infer --model_type qwen1half-7b-chat \&#xA;    --infer_backend vllm --max_model_len 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LoRA fine-tuned:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 swift infer --ckpt_dir xxx/checkpoint-xxx --load_dataset_config true&#xA;# use VLLM&#xA;CUDA_VISIBLE_DEVICES=0 swift infer \&#xA;    --ckpt_dir xxx/checkpoint-xxx --load_dataset_config true \&#xA;    --merge_lora true --infer_backend vllm --max_model_len 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 swift eval --model_type qwen1half-7b-chat --eval_dataset mmlu ceval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;Original model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 swift export --model_type qwen1half-7b-chat \&#xA;    --quant_bits 4 --quant_method awq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LoRA fine-tuned:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 swift export \&#xA;    --ckpt_dir xxx/checkpoint-xxx --load_dataset_config true \&#xA;    --quant_method awq --quant_bits 4 \&#xA;    --merge_lora true \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;The client uses the OpenAI API for invocation, for details refer to the &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source_en/LLM/VLLM-inference-acceleration-and-deployment.md&#34;&gt;LLM deployment documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Original model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 swift deploy --model_type qwen1half-7b-chat&#xA;# ‰ΩøÁî®VLLMÂä†ÈÄü&#xA;CUDA_VISIBLE_DEVICES=0 swift deploy --model_type qwen1half-7b-chat \&#xA;    --infer_backend vllm --max_model_len 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LoRA fine-tuned:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;CUDA_VISIBLE_DEVICES=0 swift deploy --ckpt_dir xxx/checkpoint-xxx&#xA;# ‰ΩøÁî®VLLMÂä†ÈÄü&#xA;CUDA_VISIBLE_DEVICES=0 swift deploy \&#xA;    --ckpt_dir xxx/checkpoint-xxx --merge_lora true \&#xA;    --infer_backend vllm --max_model_len 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Supported Models&lt;/h3&gt; &#xA;&lt;p&gt;The complete list of supported models and datasets can be found at &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Supported-models-datasets.md&#34;&gt;Supported Models and Datasets List&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;LLMs&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Model Introduction&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Model Size&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen&lt;br&gt;Qwen1.5&lt;br&gt;Qwen2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM&#34;&gt;Tongyi Qwen 1.0 and 1.5 series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;0.5B-110B&lt;br&gt;including quantized versions&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;MoE model&lt;br&gt;code model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGLM2&lt;br&gt;ChatGLM3&lt;br&gt;Codegeex2&lt;br&gt;GLM4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM&#34;&gt;Zhipu ChatGLM series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;6B-9B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;code model&lt;br&gt;long text model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan/Baichuan2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc&#34;&gt;Baichuan 1 and Baichuan 2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-13B&lt;br&gt;including quantized versions&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Yuan2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IEIT-Yuan&#34;&gt;Langchao Yuan series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;2B-102B&lt;/td&gt; &#xA;   &lt;td&gt;instruct model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XVerse&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xverse-ai&#34;&gt;XVerse series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-65B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;long text model&lt;br&gt;MoE model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA2 series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-70B&lt;br&gt;including quantized versions&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/meta-llama/llama3&#34;&gt;LLaMA3 series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;8B-70B&lt;br&gt;including quantized versions&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;br&gt;Mixtral&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mistralai/mistral-src&#34;&gt;Mistral series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-22B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;instruct model&lt;br&gt;MoE model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Yi&lt;br&gt;Yi1.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/01-ai&#34;&gt;01AI&#39;s YI series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;6B-34B&lt;br&gt;including quantized&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;long text model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternLM&lt;br&gt;InternLM2&lt;br&gt;InternLM2-Math&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM&#34;&gt;Pujiang AI Lab InternLM series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;1.8B-20B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;math model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek&lt;br&gt;DeepSeek-MoE&lt;br&gt;DeepSeek-Coder&lt;br&gt;DeepSeek-Math&lt;br&gt;DeepSeek-V2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepseek-ai&#34;&gt;DeepSeek series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-236B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;MoE model&lt;br&gt;code model&lt;br&gt;math model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MAMBA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/state-spaces/mamba&#34;&gt;MAMBA temporal convolution model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;130M-2.8B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google/gemma_pytorch&#34;&gt;Google Gemma series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;2B-7B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;instruct model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniCPM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/MiniCPM&#34;&gt;OpenBmB MiniCPM series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;2B-3B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;br&gt;MoE model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenBuddy&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBuddy/OpenBuddy&#34;&gt;OpenBuddy series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-67B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Orion&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OrionStarAI&#34;&gt;OrionStar AI series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;14B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BlueLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/vivo-ai-lab/BlueLM&#34;&gt;VIVO BlueLM large model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-CCNL/Fengshenbang-LM&#34;&gt;Fengshenbang series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Skywork&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SkyworkAI/Skywork&#34;&gt;Skywork series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Zephyr&lt;/td&gt; &#xA;   &lt;td&gt;Zephyr series models based on Mistral&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PolyLM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/DAMO-NLP-MT/PolyLM&#34;&gt;Tongyi Lab self-developed PolyLM series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multilingual&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SeqGPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Alibaba-NLP/SeqGPT&#34;&gt;Tongyi Lab self-developed text understanding model for information extraction and text classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;560M&lt;/td&gt; &#xA;   &lt;td&gt;semantic understanding model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SUS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SUSTech-IDEA/SUS-Chat&#34;&gt;Southern University of Science and Technology model fine-tuned on YI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tongyi-Finance&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;Tongyi finance series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;14B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;financial model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeFuse-CodeLLaMA&lt;br&gt;CodeFuse-Codegeex2&lt;br&gt;CodeFuse-Qwen&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/codefuse-ai&#34;&gt;Ant CodeFuse series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;6B-34B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;br&gt;code model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;phi2/phi3&lt;/td&gt; &#xA;   &lt;td&gt;Microsoft&#39;s PHI series models&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;3B/4B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;instruct model&lt;br&gt;code model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Grok&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xai-org/grok-1&#34;&gt;X-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;300B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TeleChat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tele-AI/Telechat&#34;&gt;Tele-AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-12B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dbrx&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/databricks/dbrx&#34;&gt;databricks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;132B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mengzi3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Langboat/Mengzi3&#34;&gt;Langboat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;c4ai-command-r&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cohere.com/command&#34;&gt;c4ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Multilingual&lt;/td&gt; &#xA;   &lt;td&gt;35B-104B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardLM2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM2 series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-8x22B&lt;br&gt;including quantized versions&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;br&gt;MoE model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Atom&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese&#34;&gt;Atom&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Alpaca-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese-LLaMA-Alpaca-2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-13B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;br&gt;long text model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Alpaca-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3&#34;&gt;Chinese-LLaMA-Alpaca-3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ModelScope-Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-agent&#34;&gt;ModelScope Agent series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;7B-14B&lt;/td&gt; &#xA;   &lt;td&gt;agent model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;MLLMs&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Model Introduction&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Model Size&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-VL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM&#34;&gt;Tongyi Qwen vision model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;br&gt;including quantized versions&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen-Audio&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM&#34;&gt;Tongyi Qwen speech model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;base model&lt;br&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YI-VL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/01-ai&#34;&gt;01AI&#39;s YI series vision models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;6B-34B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;XComposer2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM&#34;&gt;Pujiang AI Lab InternLM vision model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepseek-ai&#34;&gt;DeepSeek series vision models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-7B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniCPM-V&lt;br&gt;MiniCPM-V-2&lt;br&gt;MiniCPM-V-2_5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/MiniCPM&#34;&gt;OpenBmB MiniCPM vision model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;3B-9B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CogVLM&lt;br&gt;CogVLM2&lt;br&gt;CogAgent&lt;br&gt;GLM4V&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/&#34;&gt;Zhipu ChatGLM visual QA and Agent model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;9B-19B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llava&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;Llava series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;7B-34B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llava-Next&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-NeXT&#34;&gt;Llava-Next series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;8B-110B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;mPLUG-Owl&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/X-PLUG/mPLUG-Owl&#34;&gt;mPLUG-Owl series models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;11B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;InternVL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese&lt;br&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;2B-25.5B&lt;br&gt;including quantized version&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llava-llama3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-transformers&#34;&gt;xtuner&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi3-Vision&lt;/td&gt; &#xA;   &lt;td&gt;Microsoft&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;4B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaliGemma&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;chat model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Diffusion Models&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Model Introduction&lt;/th&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AnimateDiff&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;AnimateDiff animation model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;text-to-video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SD1.5/SD2.0/SDXL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Stability-AI&#34;&gt;StabilityAI series diffusion models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English&lt;/td&gt; &#xA;   &lt;td&gt;text-to-image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Supported Open Source Datasets&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset Type&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Training Task&lt;/th&gt; &#xA;   &lt;th&gt;Documentation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;General&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;üî•ruozhiba, üî•ms-bench, üî•alpaca-en(gpt4), üî•alpaca-zh(gpt4), multi-alpaca, instinwild, cot-en, cot-zh, firefly-zh, instruct-en, gpt4all-en, sharegpt, tulu-v2-sft-mixture, wikipedia-zh, open-orca, sharegpt-gpt4, deepctrl-sft, coig-cqia.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Agent&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;üî•ms-agent, üî•ms-agent-for-agentfabric, ms-agent-multirole, üî•toolbench-for-alpha-umi, damo-agent-zh, damo-agent-zh-mini, agent-instruct-all-en.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;General&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Human Alignment&lt;/td&gt; &#xA;   &lt;td&gt;hh-rlhf, üî•hh-rlhf-cn, stack-exchange-paired.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;code-alpaca-en, üî•leetcode-python-en, üî•codefuse-python-en, üî•codefuse-evol-instruction-zh.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medical&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;medical-en, medical-zh, üî•disc-med-sft-zh.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Legal&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;lawyer-llama-zh, tigerbot-law-zh, üî•disc-law-sft-zh.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Math&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;üî•blossom-math-zh, school-math-zh, open-platypus-en.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SQL&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;text2sql-en, üî•sql-create-context-en.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text Generation&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;üî•advertise-gen-zh, üî•dureader-robust-zh.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Classification&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;cmnli-zh, üî•jd-sentiment-zh, üî•hc3-zh, üî•hc3-en.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Quantization Assist&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Quantization&lt;/td&gt; &#xA;   &lt;td&gt;pileval.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Other&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;finance-en, poetry-zh, webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;coco-en, üî•coco-en-mini, coco-en-2, coco-en-2-mini, capcha-images.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fine-tuning&lt;/td&gt; &#xA;   &lt;td&gt;aishell1-zh, üî•aishell1-zh-mini.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Supported Technologies&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Technology Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•LoRA: &lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•LoRA+: &lt;a href=&#34;https://arxiv.org/pdf/2402.12354.pdf&#34;&gt;LoRA+: Efficient Low Rank Adaptation of Large Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•GaLore:&lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•LISA: &lt;a href=&#34;https://arxiv.org/abs/2403.17919&#34;&gt;LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•UnSloth: &lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•LLaMA PRO: &lt;a href=&#34;https://arxiv.org/pdf/2401.02415.pdf&#34;&gt;LLAMA PRO: Progressive LLaMA with Block Expansion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•SCEdit: &lt;a href=&#34;https://arxiv.org/abs/2312.11392&#34;&gt;SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing&lt;/a&gt; &amp;lt; &lt;a href=&#34;https://arxiv.org/abs/2312.11392&#34;&gt;arXiv&lt;/a&gt; \&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;üî•NEFTune: &lt;a href=&#34;https://arxiv.org/abs/2310.05914&#34;&gt;Noisy Embeddings Improve Instruction Finetuning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LongLoRA: &lt;a href=&#34;https://arxiv.org/abs/2309.12307&#34;&gt;Efficient Fine-tuning of Long-Context Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Adapter: &lt;a href=&#34;http://arxiv.org/abs/1902.00751&#34;&gt;Parameter-Efficient Transfer Learning for NLP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vision Prompt Tuning: &lt;a href=&#34;https://arxiv.org/abs/2203.12119&#34;&gt;Visual Prompt Tuning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Side: &lt;a href=&#34;https://arxiv.org/abs/1912.13503&#34;&gt;Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Res-Tuning: &lt;a href=&#34;https://arxiv.org/abs/2310.19859&#34;&gt;Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone&lt;/a&gt; &amp;lt; &lt;a href=&#34;https://arxiv.org/abs/2310.19859&#34;&gt;arXiv&lt;/a&gt; \&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tuners provided by &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;, such as IA3, AdaLoRA, etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Supported Hardware&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hardware Environment&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RTX 20/30/40 series, etc.&lt;/td&gt; &#xA;   &lt;td&gt;After 30 series, BF16 and FlashAttn can be used&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Computing cards T4/V100, etc.&lt;/td&gt; &#xA;   &lt;td&gt;BF16 and FlashAttn not supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Computing cards A10/A100, etc.&lt;/td&gt; &#xA;   &lt;td&gt;Support BF16 and FlashAttn&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Huawei Ascend NPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üìÉ Documentation&lt;/h2&gt; &#xA;&lt;h3&gt;Documentation Compiling&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make docs&#xA;# Check docs/build/html/index.html in web-browser&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;User Guide&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Document Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/GetStarted/Web-ui.md&#34;&gt;Using Web-UI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/GetStarted/Tuners.md&#34;&gt;Using Tuners&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/LLM-inference.md&#34;&gt;LLM Inference&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/LLM-fine-tuning.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/LLM-eval.md&#34;&gt;LLM Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/LLM-quantization.md&#34;&gt;LLM Quantization&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/VLLM-inference-acceleration-and-deployment.md&#34;&gt;LLM Deployment&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/AIGC/AnimateDiff-train-infer.md&#34;&gt;AnimateDiff Training&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Reference Documentation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Document Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Command-line-parameters.md&#34;&gt;Command Line Arguments&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Supported-models-datasets.md&#34;&gt;Supported Models and Datasets List&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Customization.md&#34;&gt;Customizing New Models and Datasets&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Benchmark.md&#34;&gt;Runtime Speed and Memory Benchmark&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Best Practices&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Best Practices Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Agent-best-practice.md&#34;&gt;Agent Fine-Tuning Best Practice&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Self-cognition-best-practice.md&#34;&gt;Self-Cognition Fine-Tuning Best Practice&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/Qwen1.5-best-practice.md&#34;&gt;Qwen1.5 Best Practice&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/Multi-Modal/index.md&#34;&gt;Multi-Modal Model Training Best Practice&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/NPU-best-practice.md&#34;&gt;NPU Best Practice&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/DPO.md&#34;&gt;DPO Human Alignment Training&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/ORPO.md&#34;&gt;ORPO Human Alignment Training&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/swift/main/docs/source_en/LLM/SimPO.md&#34;&gt;SimPO Human Alignment Training&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Deep Learning Tutorials&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Tutorial Name&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/A.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md&#34;&gt;Introduction to Deep Learning&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/B.%E9%AD%94%E6%90%AD%E7%A4%BE%E5%8C%BA%E5%92%8CLLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.md&#34;&gt;Large Model Basics&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/C.%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-prompt%20engineering.md&#34;&gt;Prompt Engineering&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/D.Transformer%E7%BB%93%E6%9E%84.md&#34;&gt;Transformer Architecture Introduction&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/E.%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B.md&#34;&gt;Training Technique Selection&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/F.%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.md&#34;&gt;Data Preprocessing&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/G.%E9%87%8F%E5%8C%96.md&#34;&gt;Quantization&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/H.%E8%AE%AD%E7%BB%83.md&#34;&gt;Training&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/I.LLM%E5%92%8C%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E9%AB%98%E6%95%88%E6%8E%A8%E7%90%86%E5%AE%9E%E8%B7%B5.md&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/J.%E9%83%A8%E7%BD%B2.md&#34;&gt;Deployment&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope-classroom/raw/main/LLM-tutorial/K.%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%87%AA%E5%8A%A8%E8%AF%84%E4%BC%B0%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E6%88%98--LLM%20Automatic%20Evaluation.md&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üèõ License&lt;/h2&gt; &#xA;&lt;p&gt;This framework is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;. For models and datasets, please refer to the original resource page and follow the corresponding License.&lt;/p&gt; &#xA;&lt;h2&gt;üìé Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@Misc{swift,&#xA;  title = {SWIFT:Scalable lightWeight Infrastructure for Fine-Tuning},&#xA;  author = {The ModelScope Team},&#xA;  howpublished = {\url{https://github.com/modelscope/swift}},&#xA;  year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚òé Wechat Group&lt;/h2&gt; &#xA;&lt;p&gt;You can contact us and communicate with us by adding our WeChat group:&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/modelscope/swift/main/asset/wechat.png&#34; width=&#34;250&#34; style=&#34;display: inline-block;&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#modelscope/swift&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=modelscope/swift&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SYSTRAN/faster-whisper</title>
    <updated>2024-06-07T01:33:44Z</updated>
    <id>tag:github.com,2024-06-07:/SYSTRAN/faster-whisper</id>
    <link href="https://github.com/SYSTRAN/faster-whisper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Faster Whisper transcription with CTranslate2&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/faster-whisper&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/faster-whisper.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Faster Whisper transcription with CTranslate2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;faster-whisper&lt;/strong&gt; is a reimplementation of OpenAI&#39;s Whisper model using &lt;a href=&#34;https://github.com/OpenNMT/CTranslate2/&#34;&gt;CTranslate2&lt;/a&gt;, which is a fast inference engine for Transformer models.&lt;/p&gt; &#xA;&lt;p&gt;This implementation is up to 4 times faster than &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;openai/whisper&lt;/a&gt; for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmark&lt;/h2&gt; &#xA;&lt;h3&gt;Whisper&lt;/h3&gt; &#xA;&lt;p&gt;For reference, here&#39;s the time and memory usage that are required to transcribe &lt;a href=&#34;https://www.youtube.com/watch?v=0u7tTptBo9I&#34;&gt;&lt;strong&gt;13 minutes&lt;/strong&gt;&lt;/a&gt; of audio using different implementations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;openai/whisper&lt;/a&gt;@&lt;a href=&#34;https://github.com/openai/whisper/commit/6dea21fd7f7253bfe450f1e2512a0fe47ee2d258&#34;&gt;6dea21fd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;@&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/commit/3b010f9bed9a6068609e9faf52383aea792b0362&#34;&gt;3b010f9&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SYSTRAN/faster-whisper&#34;&gt;faster-whisper&lt;/a&gt;@&lt;a href=&#34;https://github.com/SYSTRAN/faster-whisper/commit/cce6b53e4554f71172dad188c45f10fb100f6e3e&#34;&gt;cce6b53e&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Large-v2 model on GPU&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Beam size&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Max. GPU memory&lt;/th&gt; &#xA;   &lt;th&gt;Max. CPU memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai/whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;4m30s&lt;/td&gt; &#xA;   &lt;td&gt;11325MB&lt;/td&gt; &#xA;   &lt;td&gt;9439MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;54s&lt;/td&gt; &#xA;   &lt;td&gt;4755MB&lt;/td&gt; &#xA;   &lt;td&gt;3244MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;int8&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;59s&lt;/td&gt; &#xA;   &lt;td&gt;3091MB&lt;/td&gt; &#xA;   &lt;td&gt;3117MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Executed with CUDA 11.7.1 on a NVIDIA Tesla V100S.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Small model on CPU&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Beam size&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Max. memory&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai/whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;10m31s&lt;/td&gt; &#xA;   &lt;td&gt;3101MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;whisper.cpp&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;17m42s&lt;/td&gt; &#xA;   &lt;td&gt;1581MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;whisper.cpp&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;12m39s&lt;/td&gt; &#xA;   &lt;td&gt;873MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;fp32&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;2m44s&lt;/td&gt; &#xA;   &lt;td&gt;1675MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;faster-whisper&lt;/td&gt; &#xA;   &lt;td&gt;int8&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;2m04s&lt;/td&gt; &#xA;   &lt;td&gt;995MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Executed with 8 threads on a Intel(R) Xeon(R) Gold 6226R.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Distil-whisper&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Implementation&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;Beam size&lt;/th&gt; &#xA;   &lt;th&gt;Time&lt;/th&gt; &#xA;   &lt;th&gt;Gigaspeech WER&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;distil-whisper/distil-large-v2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;10.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Systran/faster-distil-whisper-large-v2&#34;&gt;faster-distil-large-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;10.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;distil-whisper/distil-medium.en&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;11.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Systran/faster-distil-whisper-medium.en&#34;&gt;faster-distil-medium.en&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;11.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Executed with CUDA 11.4 on a NVIDIA 3090.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;testing details (click to expand)&lt;/summary&gt; &#xA; &lt;p&gt;For &lt;code&gt;distil-whisper/distil-large-v2&lt;/code&gt;, the WER is tested with code sample from &lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v2#evaluation&#34;&gt;link&lt;/a&gt;. for &lt;code&gt;faster-distil-whisper&lt;/code&gt;, the WER is tested with setting:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faster_whisper import WhisperModel&#xA;&#xA;model_size = &#34;distil-large-v2&#34;&#xA;# model_size = &#34;distil-medium.en&#34;&#xA;# Run on GPU with FP16&#xA;model = WhisperModel(model_size, device=&#34;cuda&#34;, compute_type=&#34;float16&#34;)&#xA;segments, info = model.transcribe(&#34;audio.mp3&#34;, beam_size=5, language=&#34;en&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.8 or greater&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Unlike openai-whisper, FFmpeg does &lt;strong&gt;not&lt;/strong&gt; need to be installed on the system. The audio is decoded with the Python library &lt;a href=&#34;https://github.com/PyAV-Org/PyAV&#34;&gt;PyAV&lt;/a&gt; which bundles the FFmpeg libraries in its package.&lt;/p&gt; &#xA;&lt;h3&gt;GPU&lt;/h3&gt; &#xA;&lt;p&gt;GPU execution requires the following NVIDIA libraries to be installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cublas&#34;&gt;cuBLAS for CUDA 12&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN 8 for CUDA 12&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Latest versions of &lt;code&gt;ctranslate2&lt;/code&gt; support CUDA 12 only. For CUDA 11, the current workaround is downgrading to the &lt;code&gt;3.24.0&lt;/code&gt; version of &lt;code&gt;ctranslate2&lt;/code&gt; (This can be done with &lt;code&gt;pip install --force-reinstall ctranslate2==3.24.0&lt;/code&gt; or specifying the version in a &lt;code&gt;requirements.txt&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;There are multiple ways to install the NVIDIA libraries mentioned above. The recommended way is described in the official NVIDIA documentation, but we also suggest other installation methods below.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For all these methods below, keep in mind the above note regarding CUDA versions. Depending on your setup, you may need to install the &lt;em&gt;CUDA 11&lt;/em&gt; versions of libraries that correspond to the CUDA 12 libraries listed in the instructions below.&lt;/p&gt; &#xA; &lt;h4&gt;Use Docker&lt;/h4&gt; &#xA; &lt;p&gt;The libraries (cuBLAS, cuDNN) are installed in these official NVIDIA CUDA Docker images: &lt;code&gt;nvidia/cuda:12.0.0-runtime-ubuntu20.04&lt;/code&gt; or &lt;code&gt;nvidia/cuda:12.0.0-runtime-ubuntu22.04&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;h4&gt;Install with &lt;code&gt;pip&lt;/code&gt; (Linux only)&lt;/h4&gt; &#xA; &lt;p&gt;On Linux these libraries can be installed with &lt;code&gt;pip&lt;/code&gt;. Note that &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; must be set before launching Python.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install nvidia-cublas-cu12 nvidia-cudnn-cu12&#xA;&#xA;export LD_LIBRARY_PATH=`python3 -c &#39;import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + &#34;:&#34; + os.path.dirname(nvidia.cudnn.lib.__file__))&#39;`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Version 9+ of &lt;code&gt;nvidia-cudnn-cu12&lt;/code&gt; appears to cause issues due its reliance on cuDNN 9 (Faster-Whisper does not currently support cuDNN 9). Ensure your version of the Python package is for cuDNN 8.&lt;/p&gt; &#xA; &lt;h4&gt;Download the libraries from Purfview&#39;s repository (Windows &amp;amp; Linux)&lt;/h4&gt; &#xA; &lt;p&gt;Purfview&#39;s &lt;a href=&#34;https://github.com/Purfview/whisper-standalone-win&#34;&gt;whisper-standalone-win&lt;/a&gt; provides the required NVIDIA libraries for Windows &amp;amp; Linux in a &lt;a href=&#34;https://github.com/Purfview/whisper-standalone-win/releases/tag/libs&#34;&gt;single archive&lt;/a&gt;. Decompress the archive and place the libraries in a directory included in the &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The module can be installed from &lt;a href=&#34;https://pypi.org/project/faster-whisper/&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install faster-whisper&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; &#xA; &lt;h3&gt;Install the master branch&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --force-reinstall &#34;faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Install a specific commit&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --force-reinstall &#34;faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Faster-whisper&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faster_whisper import WhisperModel&#xA;&#xA;model_size = &#34;large-v3&#34;&#xA;&#xA;# Run on GPU with FP16&#xA;model = WhisperModel(model_size, device=&#34;cuda&#34;, compute_type=&#34;float16&#34;)&#xA;&#xA;# or run on GPU with INT8&#xA;# model = WhisperModel(model_size, device=&#34;cuda&#34;, compute_type=&#34;int8_float16&#34;)&#xA;# or run on CPU with INT8&#xA;# model = WhisperModel(model_size, device=&#34;cpu&#34;, compute_type=&#34;int8&#34;)&#xA;&#xA;segments, info = model.transcribe(&#34;audio.mp3&#34;, beam_size=5)&#xA;&#xA;print(&#34;Detected language &#39;%s&#39; with probability %f&#34; % (info.language, info.language_probability))&#xA;&#xA;for segment in segments:&#xA;    print(&#34;[%.2fs -&amp;gt; %.2fs] %s&#34; % (segment.start, segment.end, segment.text))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;segments&lt;/code&gt; is a &lt;em&gt;generator&lt;/em&gt; so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a &lt;code&gt;for&lt;/code&gt; loop:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#34;audio.mp3&#34;)&#xA;segments = list(segments)  # The transcription will actually run here.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Faster Distil-Whisper&lt;/h3&gt; &#xA;&lt;p&gt;The Distil-Whisper checkpoints are compatible with the Faster-Whisper package. In particular, the latest &lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v3&#34;&gt;distil-large-v3&lt;/a&gt; checkpoint is intrinsically designed to work with the Faster-Whisper transcription algorithm. The following code snippet demonstrates how to run inference with distil-large-v3 on a specified audio file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faster_whisper import WhisperModel&#xA;&#xA;model_size = &#34;distil-large-v3&#34;&#xA;&#xA;model = WhisperModel(model_size, device=&#34;cuda&#34;, compute_type=&#34;float16&#34;)&#xA;segments, info = model.transcribe(&#34;audio.mp3&#34;, beam_size=5, language=&#34;en&#34;, condition_on_previous_text=False)&#xA;&#xA;for segment in segments:&#xA;    print(&#34;[%.2fs -&amp;gt; %.2fs] %s&#34; % (segment.start, segment.end, segment.text))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about the distil-large-v3 model, refer to the original &lt;a href=&#34;https://huggingface.co/distil-whisper/distil-large-v3&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Word-level timestamps&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#34;audio.mp3&#34;, word_timestamps=True)&#xA;&#xA;for segment in segments:&#xA;    for word in segment.words:&#xA;        print(&#34;[%.2fs -&amp;gt; %.2fs] %s&#34; % (word.start, word.end, word.word))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;VAD filter&lt;/h3&gt; &#xA;&lt;p&gt;The library integrates the &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;Silero VAD&lt;/a&gt; model to filter out parts of the audio without speech:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#34;audio.mp3&#34;, vad_filter=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the &lt;a href=&#34;https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/vad.py&#34;&gt;source code&lt;/a&gt;. They can be customized with the dictionary argument &lt;code&gt;vad_parameters&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;segments, _ = model.transcribe(&#xA;    &#34;audio.mp3&#34;,&#xA;    vad_filter=True,&#xA;    vad_parameters=dict(min_silence_duration_ms=500),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;The library logging level can be configured like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging&#xA;&#xA;logging.basicConfig()&#xA;logging.getLogger(&#34;faster_whisper&#34;).setLevel(logging.DEBUG)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Going further&lt;/h3&gt; &#xA;&lt;p&gt;See more model and transcription options in the &lt;a href=&#34;https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/transcribe.py&#34;&gt;&lt;code&gt;WhisperModel&lt;/code&gt;&lt;/a&gt; class implementation.&lt;/p&gt; &#xA;&lt;h2&gt;Community integrations&lt;/h2&gt; &#xA;&lt;p&gt;Here is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fedirz/faster-whisper-server&#34;&gt;faster-whisper-server&lt;/a&gt; is an OpenAI compatible server using &lt;code&gt;faster-whisper&lt;/code&gt;. It&#39;s easily deployable with Docker, works with OpenAI SDKs/CLI, supports streaming, and live transcription.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/m-bain/whisperX&#34;&gt;WhisperX&lt;/a&gt; is an award-winning Python library that offers speaker diarization and accurate word-level timestamps using wav2vec2 alignment&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Softcatala/whisper-ctranslate2&#34;&gt;whisper-ctranslate2&lt;/a&gt; is a command line client based on faster-whisper and compatible with the original client from openai/whisper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MahmoudAshraf97/whisper-diarization&#34;&gt;whisper-diarize&lt;/a&gt; is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Purfview/whisper-standalone-win&#34;&gt;whisper-standalone-win&lt;/a&gt; Standalone CLI executables of faster-whisper for Windows, Linux &amp;amp; macOS.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hedrergudene/asr-sd-pipeline&#34;&gt;asr-sd-pipeline&lt;/a&gt; provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zh-plus/Open-Lyrics&#34;&gt;Open-Lyrics&lt;/a&gt; is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into &lt;code&gt;.lrc&lt;/code&gt; files in the desired language using OpenAI-GPT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/geekodour/wscribe&#34;&gt;wscribe&lt;/a&gt; is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with &lt;a href=&#34;https://github.com/geekodour/wscribe-editor&#34;&gt;wscribe-editor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BANDAS-Center/aTrain&#34;&gt;aTrain&lt;/a&gt; is a graphical user interface implementation of faster-whisper developed at the BANDAS-Center at the University of Graz for transcription and diarization in Windows (&lt;a href=&#34;https://apps.microsoft.com/detail/atrain/9N15Q44SZNS2&#34;&gt;Windows Store App&lt;/a&gt;) and Linux.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ufal/whisper_streaming&#34;&gt;Whisper-Streaming&lt;/a&gt; implements real-time mode for offline Whisper-like speech-to-text models with faster-whisper as the most recommended back-end. It implements a streaming policy with self-adaptive latency based on the actual source complexity, and demonstrates the state of the art.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/collabora/WhisperLive&#34;&gt;WhisperLive&lt;/a&gt; is a nearly-live implementation of OpenAI&#39;s Whisper which uses faster-whisper as the backend to transcribe audio in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber&#34;&gt;Faster-Whisper-Transcriber&lt;/a&gt; is a simple but reliable voice transcriber that provides a user-friendly interface.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model conversion&lt;/h2&gt; &#xA;&lt;p&gt;When loading a model from its size such as &lt;code&gt;WhisperModel(&#34;large-v3&#34;)&lt;/code&gt;, the corresponding CTranslate2 model is automatically downloaded from the &lt;a href=&#34;https://huggingface.co/Systran&#34;&gt;Hugging Face Hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.&lt;/p&gt; &#xA;&lt;p&gt;For example the command below converts the &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v3&#34;&gt;original &#34;large-v3&#34; Whisper model&lt;/a&gt; and saves the weights in FP16:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers[torch]&amp;gt;=4.23&#xA;&#xA;ct2-transformers-converter --model openai/whisper-large-v3 --output_dir whisper-large-v3-ct2&#xA;--copy_files tokenizer.json preprocessor_config.json --quantization float16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The option &lt;code&gt;--model&lt;/code&gt; accepts a model name on the Hub or a path to a model directory.&lt;/li&gt; &#xA; &lt;li&gt;If the option &lt;code&gt;--copy_files tokenizer.json&lt;/code&gt; is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Models can also be converted from the code. See the &lt;a href=&#34;https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html&#34;&gt;conversion API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Load a converted model&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Directly load the model from a local directory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = faster_whisper.WhisperModel(&#34;whisper-large-v3-ct2&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface&#34;&gt;Upload your model to the Hugging Face Hub&lt;/a&gt; and load it from its name:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = faster_whisper.WhisperModel(&#34;username/whisper-large-v3-ct2&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Comparing performance against other implementations&lt;/h2&gt; &#xA;&lt;p&gt;If you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, &lt;code&gt;model.transcribe&lt;/code&gt; uses a default beam size of 1 but here we use a default beam size of 5.&lt;/li&gt; &#xA; &lt;li&gt;When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, which can be set when running your script:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OMP_NUM_THREADS=4 python3 my_script.py&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>