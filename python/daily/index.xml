<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-30T01:32:33Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>RUC-NLPIR/FlashRAG</title>
    <updated>2024-05-30T01:32:33Z</updated>
    <id>tag:github.com,2024-05-30:/RUC-NLPIR/FlashRAG</id>
    <link href="https://github.com/RUC-NLPIR/FlashRAG" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚ö°FlashRAG: A Python Toolkit for Efficient RAG Research&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&#xA; &lt;div align=&#34;center&#34;&gt;&#xA;  ‚ö°FlashRAG: A Python Toolkit for Efficient RAG Research&#xA;  &lt;div&gt;&lt;/div&gt;&#xA; &lt;/div&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2405.13576&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/datasets/ignore/FlashRAG_datasets&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/RUC-NLPIR/FlashRAG/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/badge/LICENSE-MIT-green&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#wrench-installation&#34;&gt;Installation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#sparkles-features&#34;&gt;Features&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#running-quick-start&#34;&gt;Quick-Start&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#gear-components&#34;&gt; Components&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#robot-supporting-methods&#34;&gt; Supporting Methods&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#notebook-supporting-datasets&#34;&gt; Supporting Datasets&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#raised_hands-additional-faqs&#34;&gt; FAQs&lt;/a&gt; &lt;/p&gt; &lt;/h4&gt; &#xA;&lt;p&gt;FlashRAG is a Python toolkit for the reproduction and development of Retrieval Augmented Generation (RAG) research. Our toolkit includes 32 pre-processed benchmark RAG datasets and 12 state-of-the-art RAG algorithms.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/asset/framework.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;With FlashRAG and provided resources, you can effortless reproduce existing SOTA works in the RAG domain or implement your custom RAG processes and components.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚ú®&lt;/span&gt; Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üõ† Extensive and Customizable Framework&lt;/strong&gt;: Includes essential components for RAG scenarios such as retrievers, rerankers, generators, and compressors, allowing for flexible assembly of complex pipelines.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üóÇ Comprehensive Benchmark Datasets&lt;/strong&gt;: A collection of 32 pre-processed RAG benchmark datasets to test and validate RAG models&#39; performances.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Pre-implemented Advanced RAG Algorithms&lt;/strong&gt;: Features 12 advancing RAG algorithms with reported results, based on our framework. Easily reproducing results under different settings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß© Efficient Preprocessing Stage&lt;/strong&gt;: Simplifies the RAG workflow preparation by providing various scripts like corpus processing for retrieval, retrieval index building, and pre-retrieval of documents.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;üöÄ Optimized Execution&lt;/strong&gt;: The library&#39;s efficiency is enhanced with tools like vLLM, FastChat for LLM inference acceleration, and Faiss for vector index management.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîé&lt;/span&gt; Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;FlashRAG is still under development and there are many issues and room for improvement. We will continue to update. And we also sincerely welcome contributions on this open-source toolkit.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support multiple API based generators (e.g. ChatGPT, Claude, Gemini)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integrating sentence-transformers&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add more evaluation metrics (e.g. Unieval, name-entity F1) and benchmarks (e.g. RGB benchmark)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance code adaptability and readability&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîß&lt;/span&gt; Installation&lt;/h2&gt; &#xA;&lt;p&gt;To get started with FlashRAG, simply clone it from Github and install (requires Python 3.9+):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/RUC-NLPIR/FlashRAG.git&#xA;cd FlashRAG&#xA;pip install -e . &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;üèÉ&lt;/span&gt; Quick Start&lt;/h2&gt; &#xA;&lt;h4&gt;Toy Example&lt;/h4&gt; &#xA;&lt;p&gt;Run the following code to implement a naive RAG pipeline using provided toy datasets. The default retriever is &lt;code&gt;e5&lt;/code&gt; and default generator is &lt;code&gt;llama2-7B-chat&lt;/code&gt;. You need to fill in the corresponding model path in the following command. If you wish to use other models, please refer to the detailed instructions below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd examples/quick_start&#xA;python simple_pipeline.py \&#xA;    --model_path=&amp;lt;LLAMA2-7B-Chat-PATH&amp;gt; \&#xA;    --retriever_path=&amp;lt;E5-PATH&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the code is completed, you can view the intermediate results of the run and the final evaluation score in the output folder under the corresponding path.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This toy example is just to help test whether the entire process can run normally. Our toy retrieval document only contains 1000 pieces of data, so it may not yield good results.&lt;/p&gt; &#xA;&lt;h4&gt;Using the ready-made pipeline&lt;/h4&gt; &#xA;&lt;p&gt;You can use the pipeline class we have already built (as shown in &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/#pipelines&#34;&gt;pipelines&lt;/a&gt;) to implement the RAG process inside. In this case, you just need to configure the config and load the corresponding pipeline.&lt;/p&gt; &#xA;&lt;p&gt;Firstly, load the entire process&#39;s config, which records various hyperparameters required in the RAG process. You can input yaml files as parameters or directly as variables. The priority of variables as input is higher than that of files.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flashrag.config import Config&#xA;&#xA;config_dict = {&#39;data_dir&#39;: &#39;dataset/&#39;}&#xA;my_config = Config(config_file_path = &#39;my_config.yaml&#39;,&#xA;                config_dict = config_dict)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/flashrag/config/basic_config.yaml&#34;&gt;basic yaml file&lt;/a&gt; we provide to set your own parameters. For specific parameter names and meanings, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/flashrag/config/basic_config.yaml&#34;&gt;config parameter description&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Next, load the corresponding dataset and initialize the pipeline. The components in the pipeline will be automatically loaded.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flashrag.utils import get_dataset&#xA;from flashrag.pipeline import SequentialPipeline&#xA;from flashrag.prompt import PromptTemplate&#xA;from flashrag.config import Config&#xA;&#xA;config_dict = {&#39;data_dir&#39;: &#39;dataset/&#39;}&#xA;my_config = Config(config_file_path = &#39;my_config.yaml&#39;,&#xA;                config_dict = config_dict)&#xA;all_split = get_dataset(my_config)&#xA;test_data = all_split[&#39;test&#39;]&#xA;&#xA;pipeline = SequentialPipeline(my_config)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify your own input prompt using &lt;code&gt;PromptTemplete&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt_templete = PromptTemplate(&#xA;    config, &#xA;    system_prompt = &#34;Answer the question based on the given document. Only give me the answer and do not output any other words.\nThe following are given documents.\n\n{reference}&#34;,&#xA;    user_prompt = &#34;Question: {question}\nAnswer:&#34;&#xA;)&#xA;pipeline = SequentialPipeline(my_config, prompt_template=prompt_templete)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, execute &lt;code&gt;pipeline.run&lt;/code&gt; to obtain the final result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;output_dataset = pipeline.run(test_data, do_eval=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;output_dataset&lt;/code&gt; contains the intermediate results and metric scores for each item in the input dataset. Meanwhile, the dataset with intermediate results and the overall evaluation score will also be saved as a file (if &lt;code&gt;save_intermediate_data&lt;/code&gt; and &lt;code&gt;save_metric_score&lt;/code&gt; are specified).&lt;/p&gt; &#xA;&lt;h4&gt;Build your own pipeline&lt;/h4&gt; &#xA;&lt;p&gt;Sometimes you may need to implement more complex RAG process, and you can build your own pipeline to implement it. You just need to inherit &lt;code&gt;BasicPipeline&lt;/code&gt;, initialize the components you need, and complete the &lt;code&gt;run&lt;/code&gt; function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flashrag.pipeline import BasicPipeline&#xA;from flashrag.utils import get_retriever, get_generator&#xA;&#xA;class ToyPipeline(BasicPipeline):&#xA;  def __init__(self, config, prompt_templete=None):&#xA;    # Load your own components&#xA;    pass&#xA;&#xA;  def run(self, dataset, do_eval=True):&#xA;    # Complete your own process logic&#xA;&#xA;    # get attribute in dataset using `.`&#xA;    input_query = dataset.question&#xA;    ...&#xA;    # use `update_output` to save intermeidate data&#xA;    dataset.update_output(&#34;pred&#34;,pred_answer_list)&#xA;    dataset = self.evaluate(dataset, do_eval=do_eval)&#xA;    return dataset&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please first understand the input and output forms of the components you need to use from our &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/basic_usage.md&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Just use components&lt;/h4&gt; &#xA;&lt;p&gt;If you already have your own code and only want to use our components to embed the original code, you can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/basic_usage.md&#34;&gt;basic introduction of the components&lt;/a&gt; to obtain the input and output formats of each component.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚öô&lt;/span&gt; Components&lt;/h2&gt; &#xA;&lt;p&gt;In FlashRAG, we have built a series of common RAG components, including retrievers, generators, refiners, and more. Based on these components, we have assembled several pipelines to implement the RAG workflow, while also providing the flexibility to combine these components in custom arrangements to create your own pipeline.&lt;/p&gt; &#xA;&lt;h4&gt;RAG-Components&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Judger&lt;/td&gt; &#xA;   &lt;td&gt;SKR Judger&lt;/td&gt; &#xA;   &lt;td&gt;Judging whether to retrieve using &lt;a href=&#34;https://aclanthology.org/2023.findings-emnlp.691.pdf&#34;&gt;SKR&lt;/a&gt; method&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Retriever&lt;/td&gt; &#xA;   &lt;td&gt;Dense Retriever&lt;/td&gt; &#xA;   &lt;td&gt;Bi-encoder models such as dpr, bge, e5, using faiss for search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BM25 Retriever&lt;/td&gt; &#xA;   &lt;td&gt;Sparse retrieval method based on Lucene&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bi-Encoder Reranker&lt;/td&gt; &#xA;   &lt;td&gt;Calculate matching score using bi-Encoder&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cross-Encoder Reranker&lt;/td&gt; &#xA;   &lt;td&gt;Calculate matching score using cross-encoder&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Refiner&lt;/td&gt; &#xA;   &lt;td&gt;Extractive Refiner&lt;/td&gt; &#xA;   &lt;td&gt;Refine input by extracting important context&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Abstractive Refiner&lt;/td&gt; &#xA;   &lt;td&gt;Refine input through seq2seq model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLMLingua Refiner&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2023.emnlp-main.825/&#34;&gt;LLMLingua-series&lt;/a&gt; prompt compressor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SelectiveContext Refiner&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.06201&#34;&gt;Selective-Context&lt;/a&gt; prompt compressor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Generator&lt;/td&gt; &#xA;   &lt;td&gt;Encoder-Decoder Generator&lt;/td&gt; &#xA;   &lt;td&gt;Encoder-Decoder model, supporting &lt;a href=&#34;https://arxiv.org/abs/2007.01282&#34;&gt;Fusion-in-Decoder (FiD)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Decoder-only Generator&lt;/td&gt; &#xA;   &lt;td&gt;Native transformers implementation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastChat Generator&lt;/td&gt; &#xA;   &lt;td&gt;Accelerate with &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vllm Generator&lt;/td&gt; &#xA;   &lt;td&gt;Accelerate with &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vllm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Pipelines&lt;/h4&gt; &#xA;&lt;p&gt;Referring to a &lt;a href=&#34;https://arxiv.org/abs/2312.10997&#34;&gt;survey on retrieval-augmented generation&lt;/a&gt;, we categorized RAG methods into four types based on their inference paths.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sequential&lt;/strong&gt;: Sequential execuation of RAG process, like Query-(pre-retrieval)-retriever-(post-retrieval)-generator&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Conditional&lt;/strong&gt;: Implements different paths for different types of input queries&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Branching&lt;/strong&gt; : Executes multiple paths in parallel, merging the responses from each path&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Loop&lt;/strong&gt;: Iteratively performs retrieval and generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In each category, we have implemented corresponding common pipelines. Some pipelines have corresponding work papers.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;Sequential Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;Linear execution of query, supporting refiner, reranker&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;1&#34;&gt;Conditional&lt;/td&gt; &#xA;   &lt;td&gt;Conditional Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;With a judger module, distinct execution paths for various query types&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;Branching&lt;/td&gt; &#xA;   &lt;td&gt;REPLUG Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;Generate answer by integrating probabilities in multiple generation paths&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;SuRe Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;Ranking and merging generated results based on each document&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;4&#34;&gt;Loop&lt;/td&gt; &#xA;   &lt;td&gt;Iterative Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;Alternating retrieval and generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Self-Ask Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;Decompose complex problems into subproblems using &lt;a href=&#34;https://arxiv.org/abs/2210.03350&#34;&gt;self-ask&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Self-RAG Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;Adaptive retrieval, critique, and generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FLARE Pipeline&lt;/td&gt; &#xA;   &lt;td&gt;Dynamic retrieval during the generation process&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;ü§ñ&lt;/span&gt; Supporting Methods&lt;/h2&gt; &#xA;&lt;p&gt;We have implemented 12 works with a consistent setting of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generator:&lt;/strong&gt; LLAMA3-8B-instruct with input length of 4096&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Retriever:&lt;/strong&gt; e5-base-v2 as embedding model, retrieve 5 docs per query&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; A consistent default prompt, templete can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/flashrag/prompt/base_prompt.py&#34;&gt;code&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For open-source methods, we implemented their processes using our framework. For methods where the author did not provide source code, we will try our best to follow the methods in the original paper for implementation.&lt;/p&gt; &#xA;&lt;p&gt;For necessary settings and hyperparameters specific to some methods, we have documented them in the &lt;strong&gt;specific settings&lt;/strong&gt; column. For more details, please consult our &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/examples/methods/run_exp.py&#34;&gt;code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It‚Äôs important to note that, to ensure consistency, we have utilized a uniform setting. However, this setting may differ from the original setting of the method, leading to variations in results compared to the original outcomes.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;NQ (EM)&lt;/th&gt; &#xA;   &lt;th&gt;TriviaQA (EM)&lt;/th&gt; &#xA;   &lt;th&gt;Hotpotqa (F1)&lt;/th&gt; &#xA;   &lt;th&gt;2Wiki (F1)&lt;/th&gt; &#xA;   &lt;th&gt;PopQA (F1)&lt;/th&gt; &#xA;   &lt;th&gt;WebQA(EM)&lt;/th&gt; &#xA;   &lt;th&gt;Specific setting&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Naive Generation&lt;/td&gt; &#xA;   &lt;td&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;22.6&lt;/td&gt; &#xA;   &lt;td&gt;55.7&lt;/td&gt; &#xA;   &lt;td&gt;28.4&lt;/td&gt; &#xA;   &lt;td&gt;33.9&lt;/td&gt; &#xA;   &lt;td&gt;21.7&lt;/td&gt; &#xA;   &lt;td&gt;18.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Standard RAG&lt;/td&gt; &#xA;   &lt;td&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;35.1&lt;/td&gt; &#xA;   &lt;td&gt;58.9&lt;/td&gt; &#xA;   &lt;td&gt;35.3&lt;/td&gt; &#xA;   &lt;td&gt;21.0&lt;/td&gt; &#xA;   &lt;td&gt;36.7&lt;/td&gt; &#xA;   &lt;td&gt;15.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2023.acl-long.136.pdf&#34;&gt;AAR-contriever-kilt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;30.1&lt;/td&gt; &#xA;   &lt;td&gt;56.8&lt;/td&gt; &#xA;   &lt;td&gt;33.4&lt;/td&gt; &#xA;   &lt;td&gt;19.8&lt;/td&gt; &#xA;   &lt;td&gt;36.1&lt;/td&gt; &#xA;   &lt;td&gt;16.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2023.acl-long.136.pdf&#34;&gt;LongLLMLingua&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;32.2&lt;/td&gt; &#xA;   &lt;td&gt;59.2&lt;/td&gt; &#xA;   &lt;td&gt;37.5&lt;/td&gt; &#xA;   &lt;td&gt;25.0&lt;/td&gt; &#xA;   &lt;td&gt;38.7&lt;/td&gt; &#xA;   &lt;td&gt;17.5&lt;/td&gt; &#xA;   &lt;td&gt;Compress Ratio=0.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2023.acl-long.136.pdf&#34;&gt;RECOMP-abstractive&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;33.1&lt;/td&gt; &#xA;   &lt;td&gt;56.4&lt;/td&gt; &#xA;   &lt;td&gt;37.5&lt;/td&gt; &#xA;   &lt;td&gt;32.4&lt;/td&gt; &#xA;   &lt;td&gt;39.9&lt;/td&gt; &#xA;   &lt;td&gt;20.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.06201&#34;&gt;Selective-Context&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;30.5&lt;/td&gt; &#xA;   &lt;td&gt;55.6&lt;/td&gt; &#xA;   &lt;td&gt;34.4&lt;/td&gt; &#xA;   &lt;td&gt;18.5&lt;/td&gt; &#xA;   &lt;td&gt;33.5&lt;/td&gt; &#xA;   &lt;td&gt;17.3&lt;/td&gt; &#xA;   &lt;td&gt;Compress Ratio=0.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.01558&#34;&gt;Ret-Robust&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Sequential&lt;/td&gt; &#xA;   &lt;td&gt;42.9&lt;/td&gt; &#xA;   &lt;td&gt;68.2&lt;/td&gt; &#xA;   &lt;td&gt;35.8&lt;/td&gt; &#xA;   &lt;td&gt;43.4&lt;/td&gt; &#xA;   &lt;td&gt;57.2&lt;/td&gt; &#xA;   &lt;td&gt;33.7&lt;/td&gt; &#xA;   &lt;td&gt;Use LLAMA2-13B with trained lora&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.13081&#34;&gt;SuRe&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Branching&lt;/td&gt; &#xA;   &lt;td&gt;37.1&lt;/td&gt; &#xA;   &lt;td&gt;53.2&lt;/td&gt; &#xA;   &lt;td&gt;33.4&lt;/td&gt; &#xA;   &lt;td&gt;20.6&lt;/td&gt; &#xA;   &lt;td&gt;48.1&lt;/td&gt; &#xA;   &lt;td&gt;24.2&lt;/td&gt; &#xA;   &lt;td&gt;Use provided prompt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.12652&#34;&gt;REPLUG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Branching&lt;/td&gt; &#xA;   &lt;td&gt;28.9&lt;/td&gt; &#xA;   &lt;td&gt;57.7&lt;/td&gt; &#xA;   &lt;td&gt;31.2&lt;/td&gt; &#xA;   &lt;td&gt;21.1&lt;/td&gt; &#xA;   &lt;td&gt;27.8&lt;/td&gt; &#xA;   &lt;td&gt;20.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2023.findings-emnlp.691.pdf&#34;&gt;SKR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Conditional&lt;/td&gt; &#xA;   &lt;td&gt;25.5&lt;/td&gt; &#xA;   &lt;td&gt;55.9&lt;/td&gt; &#xA;   &lt;td&gt;29.8&lt;/td&gt; &#xA;   &lt;td&gt;28.5&lt;/td&gt; &#xA;   &lt;td&gt;24.5&lt;/td&gt; &#xA;   &lt;td&gt;18.6&lt;/td&gt; &#xA;   &lt;td&gt;Use infernece-time training data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.11511&#34;&gt;Self-RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Loop&lt;/td&gt; &#xA;   &lt;td&gt;36.4&lt;/td&gt; &#xA;   &lt;td&gt;38.2&lt;/td&gt; &#xA;   &lt;td&gt;29.6&lt;/td&gt; &#xA;   &lt;td&gt;25.1&lt;/td&gt; &#xA;   &lt;td&gt;32.7&lt;/td&gt; &#xA;   &lt;td&gt;21.9&lt;/td&gt; &#xA;   &lt;td&gt;Use trained selfrag-llama2-7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.06983&#34;&gt;FLARE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Loop&lt;/td&gt; &#xA;   &lt;td&gt;22.5&lt;/td&gt; &#xA;   &lt;td&gt;55.8&lt;/td&gt; &#xA;   &lt;td&gt;28.0&lt;/td&gt; &#xA;   &lt;td&gt;33.9&lt;/td&gt; &#xA;   &lt;td&gt;20.7&lt;/td&gt; &#xA;   &lt;td&gt;20.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.15294&#34;&gt;Iter-Retgen&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2310.05149&#34;&gt;ITRG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Loop&lt;/td&gt; &#xA;   &lt;td&gt;36.8&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;   &lt;td&gt;38.3&lt;/td&gt; &#xA;   &lt;td&gt;21.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;   &lt;td&gt;18.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìì&lt;/span&gt; Supporting Datasets &amp;amp; Document Corpus&lt;/h2&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;We have collected and processed 35 datasets widely used in RAG research, pre-processing them to ensure a consistent format for ease of use. For certain datasets (such as Wiki-asp), we have adapted them to fit the requirements of RAG tasks according to the methods commonly used within the community. All datasets are available at &lt;a href=&#34;https://huggingface.co/datasets/ignore/FlashRAG_datasets&#34;&gt;Huggingface datasets&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For each dataset, we save each split as a &lt;code&gt;jsonl&lt;/code&gt; file, and each line is a dict as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{&#xA;  &#39;id&#39;: str,&#xA;  &#39;question&#39;: str,&#xA;  &#39;golden_answers&#39;: List[str],&#xA;  &#39;metadata&#39;: dict&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Below is the list of datasets along with the corresponding sample sizes:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Task&lt;/th&gt; &#xA;   &lt;th&gt;Dataset Name&lt;/th&gt; &#xA;   &lt;th&gt;Knowledge Source&lt;/th&gt; &#xA;   &lt;th&gt;# Train&lt;/th&gt; &#xA;   &lt;th&gt;# Dev&lt;/th&gt; &#xA;   &lt;th&gt;# Test&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;NQ&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;79,168&lt;/td&gt; &#xA;   &lt;td&gt;8,757&lt;/td&gt; &#xA;   &lt;td&gt;3,610&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;TriviaQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki &amp;amp; web&lt;/td&gt; &#xA;   &lt;td&gt;78,785&lt;/td&gt; &#xA;   &lt;td&gt;8,837&lt;/td&gt; &#xA;   &lt;td&gt;11,313&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;PopQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;14,267&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;SQuAD&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;87,599&lt;/td&gt; &#xA;   &lt;td&gt;10,570&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;MSMARCO-QA&lt;/td&gt; &#xA;   &lt;td&gt;web&lt;/td&gt; &#xA;   &lt;td&gt;808,731&lt;/td&gt; &#xA;   &lt;td&gt;101,093&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;NarrativeQA&lt;/td&gt; &#xA;   &lt;td&gt;books and story&lt;/td&gt; &#xA;   &lt;td&gt;32,747&lt;/td&gt; &#xA;   &lt;td&gt;3,461&lt;/td&gt; &#xA;   &lt;td&gt;10,557&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;WikiQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;20,360&lt;/td&gt; &#xA;   &lt;td&gt;2,733&lt;/td&gt; &#xA;   &lt;td&gt;6,165&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;WebQuestions&lt;/td&gt; &#xA;   &lt;td&gt;Google Freebase&lt;/td&gt; &#xA;   &lt;td&gt;3,778&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;2,032&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;AmbigQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;10,036&lt;/td&gt; &#xA;   &lt;td&gt;2,002&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;SIQA&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;33,410&lt;/td&gt; &#xA;   &lt;td&gt;1,954&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;CommenseQA&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;9,741&lt;/td&gt; &#xA;   &lt;td&gt;1,221&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;BoolQ&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;9,427&lt;/td&gt; &#xA;   &lt;td&gt;3,270&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;PIQA&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;16,113&lt;/td&gt; &#xA;   &lt;td&gt;1,838&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;QA&lt;/td&gt; &#xA;   &lt;td&gt;Fermi&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;8,000&lt;/td&gt; &#xA;   &lt;td&gt;1,000&lt;/td&gt; &#xA;   &lt;td&gt;1,000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multi-hop QA&lt;/td&gt; &#xA;   &lt;td&gt;HotpotQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;90,447&lt;/td&gt; &#xA;   &lt;td&gt;7,405&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multi-hop QA&lt;/td&gt; &#xA;   &lt;td&gt;2WikiMultiHopQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;15,000&lt;/td&gt; &#xA;   &lt;td&gt;12,576&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multi-hop QA&lt;/td&gt; &#xA;   &lt;td&gt;Musique&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;19,938&lt;/td&gt; &#xA;   &lt;td&gt;2,417&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multi-hop QA&lt;/td&gt; &#xA;   &lt;td&gt;Bamboogle&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;125&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Long-form QA&lt;/td&gt; &#xA;   &lt;td&gt;ASQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;4,353&lt;/td&gt; &#xA;   &lt;td&gt;948&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Long-form QA&lt;/td&gt; &#xA;   &lt;td&gt;ELI5&lt;/td&gt; &#xA;   &lt;td&gt;Reddit&lt;/td&gt; &#xA;   &lt;td&gt;272,634&lt;/td&gt; &#xA;   &lt;td&gt;1,507&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-Domain Summarization&lt;/td&gt; &#xA;   &lt;td&gt;WikiASP&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;300,636&lt;/td&gt; &#xA;   &lt;td&gt;37,046&lt;/td&gt; &#xA;   &lt;td&gt;37,368&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-choice&lt;/td&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;99,842&lt;/td&gt; &#xA;   &lt;td&gt;1,531&lt;/td&gt; &#xA;   &lt;td&gt;14,042&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-choice&lt;/td&gt; &#xA;   &lt;td&gt;TruthfulQA&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;817&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-choice&lt;/td&gt; &#xA;   &lt;td&gt;HellaSWAG&lt;/td&gt; &#xA;   &lt;td&gt;ActivityNet&lt;/td&gt; &#xA;   &lt;td&gt;39,905&lt;/td&gt; &#xA;   &lt;td&gt;10,042&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-choice&lt;/td&gt; &#xA;   &lt;td&gt;ARC&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;3,370&lt;/td&gt; &#xA;   &lt;td&gt;869&lt;/td&gt; &#xA;   &lt;td&gt;3,548&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;multiple-choice&lt;/td&gt; &#xA;   &lt;td&gt;OpenBookQA&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;4,957&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;   &lt;td&gt;500&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Fact Verification&lt;/td&gt; &#xA;   &lt;td&gt;FEVER&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;104,966&lt;/td&gt; &#xA;   &lt;td&gt;10,444&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dialog Generation&lt;/td&gt; &#xA;   &lt;td&gt;WOW&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;63,734&lt;/td&gt; &#xA;   &lt;td&gt;3,054&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Entity Linking&lt;/td&gt; &#xA;   &lt;td&gt;AIDA CoNll-yago&lt;/td&gt; &#xA;   &lt;td&gt;Freebase &amp;amp; wiki&lt;/td&gt; &#xA;   &lt;td&gt;18,395&lt;/td&gt; &#xA;   &lt;td&gt;4,784&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Entity Linking&lt;/td&gt; &#xA;   &lt;td&gt;WNED&lt;/td&gt; &#xA;   &lt;td&gt;Wiki&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;8,995&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Slot Filling&lt;/td&gt; &#xA;   &lt;td&gt;T-REx&lt;/td&gt; &#xA;   &lt;td&gt;DBPedia&lt;/td&gt; &#xA;   &lt;td&gt;2,284,168&lt;/td&gt; &#xA;   &lt;td&gt;5,000&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Slot Filling&lt;/td&gt; &#xA;   &lt;td&gt;Zero-shot RE&lt;/td&gt; &#xA;   &lt;td&gt;wiki&lt;/td&gt; &#xA;   &lt;td&gt;147,909&lt;/td&gt; &#xA;   &lt;td&gt;3,724&lt;/td&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Document Corpus&lt;/h3&gt; &#xA;&lt;p&gt;Our toolkit supports jsonl format for retrieval document collections, with the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsonl&#34;&gt;{&#34;id&#34;:&#34;0&#34;, &#34;contents&#34;: &#34;....&#34;}&#xA;{&#34;id&#34;:&#34;1&#34;, &#34;contents&#34;: &#34;...&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;contents&lt;/code&gt; key is essential for building the index. For documents that include both text and title, we recommend setting the value of &lt;code&gt;contents&lt;/code&gt; to &lt;code&gt;{title}\n{text}&lt;/code&gt;. The corpus file can also contain other keys to record additional characteristics of the documents.&lt;/p&gt; &#xA;&lt;p&gt;In the academic research, Wikipedia and MS MARCO are the most commonly used retrieval document collections. For Wikipedia, we provide a &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/process-wiki.md&#34;&gt;comprehensive script&lt;/a&gt; to process any Wikipedia dump into a clean corpus. Additionally, various processed versions of the Wikipedia corpus are available in many works, and we have listed some reference links.&lt;/p&gt; &#xA;&lt;p&gt;For MS MARCO, it is already processed upon release and can be directly downloaded from its &lt;a href=&#34;https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus&#34;&gt;hosting link&lt;/a&gt; on Hugging Face.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üôå&lt;/span&gt; Additional FAQs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/process-wiki.md&#34;&gt;How to build my own corpus, such as a specific segmented Wikipedia?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/docs/building-index.md&#34;&gt;How to index my own corpus?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîñ&lt;/span&gt; License&lt;/h2&gt; &#xA;&lt;p&gt;FlashRAG is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/RUC-NLPIR/FlashRAG/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;üåü&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please kindly cite our paper if helps your research:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTex&#34;&gt;@article{FlashRAG,&#xA;    author={Jiajie Jin and&#xA;            Yutao Zhu and&#xA;            Xinyu Yang and&#xA;            Chenghao Zhang and&#xA;            Zhicheng Dou},&#xA;    title={FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research},&#xA;    journal={CoRR},&#xA;    volume={abs/2405.13576},&#xA;    year={2024},&#xA;    url={https://arxiv.org/abs/2405.13576},&#xA;    eprinttype={arXiv},&#xA;    eprint={2405.13576}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>THU-MIG/yolov10</title>
    <updated>2024-05-30T01:32:33Z</updated>
    <id>tag:github.com,2024-05-30:/THU-MIG/yolov10</id>
    <link href="https://github.com/THU-MIG/yolov10" rel="alternate"></link>
    <summary type="html">&lt;p&gt;YOLOv10: Real-Time End-to-End Object Detection&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14458&#34;&gt;YOLOv10: Real-Time End-to-End Object Detection&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Official PyTorch implementation of &lt;strong&gt;YOLOv10&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/THU-MIG/yolov10/main/figures/latency.svg?sanitize=true&#34; width=&#34;48%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/THU-MIG/yolov10/main/figures/params.svg?sanitize=true&#34; width=&#34;48%&#34;&gt; &lt;br&gt; Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs. &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14458&#34;&gt;YOLOv10: Real-Time End-to-End Object Detection&lt;/a&gt;.&lt;br&gt; Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.14458&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb#scrollTo=SaKTSzSWnG7s&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/kadirnar/Yolov10&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Xenova/yolov10-web&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Transformers.js-blue&#34; alt=&#34;Transformers.js Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;font size=&#34;+1&#34;&gt;Abstract&lt;/font&gt; &lt;/summary&gt; Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model&#39;s capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8$\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8$\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\% less latency and 25\% fewer parameters for the same performance. &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;UPDATES&lt;/strong&gt; üî•&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024/05/29: Add the gradio demo for running the models locally. Thanks to &lt;a href=&#34;https://x.com/_akhaliq&#34;&gt;AK&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024/05/27: Thanks to &lt;a href=&#34;https://raw.githubusercontent.com/THU-MIG/yolov10/main/sujanshresstha&#34;&gt;sujanshresstha&lt;/a&gt; for the integration with &lt;a href=&#34;https://github.com/sujanshresstha/YOLOv10_DeepSORT.git&#34;&gt;DeepSORT&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024/05/27: We have updated the &lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/tag/v1.1&#34;&gt;checkpoints&lt;/a&gt; with other attributes, like class names, for ease of use.&lt;/li&gt; &#xA; &lt;li&gt;2024/05/26: Thanks to &lt;a href=&#34;https://github.com/CVHub520&#34;&gt;CVHub520&lt;/a&gt; for the integration into &lt;a href=&#34;https://github.com/CVHub520/X-AnyLabeling&#34;&gt;X-AnyLabeling&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024/05/26: Thanks to &lt;a href=&#34;https://github.com/DanielSarmiento04&#34;&gt;DanielSarmiento04&lt;/a&gt; for integrate in &lt;a href=&#34;https://github.com/DanielSarmiento04/yolov10cpp&#34;&gt;c++ | ONNX | OPENCV&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024/05/25: Add &lt;a href=&#34;https://huggingface.co/spaces/Xenova/yolov10-web&#34;&gt;Transformers.js demo&lt;/a&gt; and onnx weights(yolov10&lt;a href=&#34;https://huggingface.co/onnx-community/yolov10n&#34;&gt;n&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/onnx-community/yolov10s&#34;&gt;s&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/onnx-community/yolov10m&#34;&gt;m&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/onnx-community/yolov10b&#34;&gt;b&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/onnx-community/yolov10l&#34;&gt;l&lt;/a&gt;/&lt;a href=&#34;https://huggingface.co/onnx-community/yolov10x&#34;&gt;x&lt;/a&gt;). Thanks to &lt;a href=&#34;https://github.com/xenova&#34;&gt;xenova&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024/05/25: Add &lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb#scrollTo=SaKTSzSWnG7s&#34;&gt;colab demo&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/kadirnar/Yolov10&#34;&gt;HuggingFace Demo&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/kadirnar/Yolov10&#34;&gt;HuggingFace Model Page&lt;/a&gt;. Thanks to &lt;a href=&#34;https://github.com/SkalskiP&#34;&gt;SkalskiP&lt;/a&gt; and &lt;a href=&#34;https://github.com/kadirnar&#34;&gt;kadirnar&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;COCO&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FLOPs&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;sup&gt;val&lt;/sup&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Latency&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10n.pt&#34;&gt;YOLOv10-N&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.3M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.7G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.84ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10s.pt&#34;&gt;YOLOv10-S&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.2M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.6G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.3%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.49ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10m.pt&#34;&gt;YOLOv10-M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.1G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.74ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10b.pt&#34;&gt;YOLOv10-B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.1M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;92.0G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.74ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10l.pt&#34;&gt;YOLOv10-L&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;120.3G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.2%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.28ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10x.pt&#34;&gt;YOLOv10-X&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;640&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.5M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;160.4G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.4%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.70ms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;conda&lt;/code&gt; virtual environment is recommended.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n yolov10 python=3.9&#xA;conda activate yolov10&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;wget https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10s.pt&#xA;python app.py&#xA;# Please visit http://127.0.0.1:7860&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Validation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10n.pt&#34;&gt;&lt;code&gt;yolov10n.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10s.pt&#34;&gt;&lt;code&gt;yolov10s.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10m.pt&#34;&gt;&lt;code&gt;yolov10m.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10b.pt&#34;&gt;&lt;code&gt;yolov10b.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10l.pt&#34;&gt;&lt;code&gt;yolov10l.pt&lt;/code&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10x.pt&#34;&gt;&lt;code&gt;yolov10x.pt&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;yolo val model=yolov10n/s/m/b/l/x.pt data=coco.yaml batch=256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;yolo detect train data=coco.yaml model=yolov10n/s/m/b/l/x.yaml epochs=500 batch=256 imgsz=640 device=0,1,2,3,4,5,6,7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prediction&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;yolo predict model=yolov10n/s/m/b/l/x.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Export&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;# End-to-End ONNX&#xA;yolo export model=yolov10n/s/m/b/l/x.pt format=onnx opset=13 simplify&#xA;# Predict with ONNX&#xA;yolo predict model=yolov10n/s/m/b/l/x.onnx&#xA;&#xA;# End-to-End TensorRT&#xA;yolo export model=yolov10n/s/m/b/l/x.pt format=engine half=True simplify opset=13 workspace=16&#xA;# Or&#xA;trtexec --onnx=yolov10n/s/m/b/l/x.onnx --saveEngine=yolov10n/s/m/b/l/x.engine --fp16&#xA;# Predict with TensorRT&#xA;yolo predict model=yolov10n/s/m/b/l/x.engine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;The code base is built with &lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;ultralytics&lt;/a&gt; and &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt;RT-DETR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks for the great implementations!&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our code or models help your work, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{wang2024yolov10,&#xA;      title={YOLOv10: Real-Time End-to-End Object Detection}, &#xA;      author={Ao Wang and Hui Chen and Lihao Liu and Kai Chen and Zijia Lin and Jungong Han and Guiguang Ding},&#xA;      year={2024},&#xA;      eprint={2405.14458},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ambionics/cnext-exploits</title>
    <updated>2024-05-30T01:32:33Z</updated>
    <id>tag:github.com,2024-05-30:/ambionics/cnext-exploits</id>
    <link href="https://github.com/ambionics/cnext-exploits" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Exploits for CNEXT (CVE-2024-2961), a buffer overflow in the glibc&#39;s iconv()&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CNEXT exploits&lt;/h1&gt; &#xA;&lt;p&gt;Exploits for CNEXT (CVE-2024-2961), a buffer overflow in the glibc&#39;s iconv(), by &lt;a href=&#34;https://twitter.com/cfreal_&#34;&gt;@cfreal_&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Technical analysis&lt;/h1&gt; &#xA;&lt;p&gt;The vulnerability and exploits are described in the following blogposts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ambionics.io/blog/iconv-cve-2024-2961-p1&#34;&gt;Iconv, set the charset to RCE: Exploiting the glibc to hack the PHP engine (part 1)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;To be continued...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Exploits&lt;/h1&gt; &#xA;&lt;p&gt;Exploits will become available as blogposts come out.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ambionics/cnext-exploits/main/cnext-exploit.py&#34;&gt;CNEXT: file read to RCE exploit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;To be continued...&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>