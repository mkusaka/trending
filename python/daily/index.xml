<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-26T01:42:57Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gururise/AlpacaDataCleaned</title>
    <updated>2023-03-26T01:42:57Z</updated>
    <id>tag:github.com,2023-03-26:/gururise/AlpacaDataCleaned</id>
    <link href="https://github.com/gururise/AlpacaDataCleaned" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Alpaca dataset from Stanford, cleaned and curated&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü¶ôüõÅ Cleaned Alpaca Dataset&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset Quality and its Impact on Model Performance&lt;/h2&gt; &#xA;&lt;p&gt;One possibility behind the lack of a significant improvement in performance from fine-tuning the 7B Alpaca model to the 13B model is the quality of the original dataset. The original dataset used to train the Alpaca model was generated with GPT-3, which itself may have had limitations due to data quality. More evidence pointing to poor data quality is that fine-tuning on the original dataset resulted in &lt;a href=&#34;https://twitter.com/abacaj/status/1637310768780648448&#34;&gt;poor loss curves&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The quality of the dataset plays a crucial role in determining the performance of the natural language processing models trained on it. A dataset that is noisy, inconsistent, or incomplete can result in poor performance even with the most advanced models. In contrast, a high-quality dataset can enable a model to perform well with smaller parameters.&lt;/p&gt; &#xA;&lt;p&gt;Therefore, it is possible that with better data, we could improve the performance of the models more than what would be gained by simply increasing model size.&lt;/p&gt; &#xA;&lt;h2&gt;Data Cleaning and Curation&lt;/h2&gt; &#xA;&lt;p&gt;Alpaca is a fine-tuned version of LLAMA that was trained using an Instruct Dataset generated by GPT-3. The generated dataset was designed to be &lt;a href=&#34;https://github.com/gururise/AlpacaDataCleaned/raw/main/assets/parse_analysis.png&#34;&gt;diverse&lt;/a&gt;; however, recent analysis indicates it is very US centric. The original dataset used to train the Alpaca LLM was found to have many issues that impacts its quality and usefulness for training a machine learning model.&lt;/p&gt; &#xA;&lt;h2&gt;Issues with the Original Dataset&lt;/h2&gt; &#xA;&lt;p&gt;The original dataset had several issues that likely affect the final performance of the fine-tuned model. These issues include:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hallucinations:&lt;/strong&gt; Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;instruction&#34;:&#34;Summarize the given article in 200 Words.&#34;,&#xA;&#34;input&#34;: &#34;https://www.bbc.com/news/world-51461830&#34;,&#xA;&#34;output&#34;: &#34;The recent protests in Belarus are part of a growing wave of discontent against the rule of Alexander Lukashenko, who has led the country since 1994. Demonstrations against his government have been ongoing since August, and come in the wake of an election widely seen as rigged. As police and security forces have cracked down on protesters, the movement has only grown, with hundreds of thousands of people taking to the streets to call for new leadership. Human rights activists have accused the government of using extreme violence in order prosecute protesters, leading to an international outcry and sanctions from the European Union. As the situation in Belarus continues to unfold, the international community is monitoring the situation closely - hoping for a peaceful resolution, while also standing firm against the oppressive tactics employed by the Belarusian state.&#34;&#xA;&#xA;&#34;instruction&#34;: &#34;Classify the following shape according to simple geometry.&#34;,&#xA;&#34;input&#34;: &#34;&amp;lt;Picture Attached&amp;gt;&#34;,&#xA;&#34;output&#34;: &#34;Triangle&#34;&#xA;&#xA;&#34;instruction&#34;: &#34;Given a recent news article, write a headline for it.&#34;,&#xA;&#34;input&#34;: &#34;https://www.news.example.com/article-1&#34;,&#xA;&#34;output&#34;: &#34;\&#34;NASA Launches a Mars 2020 Mission to Search for Signs of Ancient Life\&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Merged Instructions:&lt;/strong&gt; There were many instructions that were merged together in the original dataset for some reason.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;instruction&#34;: &#34;Should I invest in stocks?&#34;,&#xA;&#34;input&#34;: &#34;&#34;,&#xA;&#34;output&#34;: &#34;It depends on your individual situation. Investing in stocks carries a certain amount of risk but can also offer potential rewards. Make sure to research the different types of stocks, understand the risks and rewards, and develop a plan that works for you.\n##\n18. Instruction: Identify the noun in the following sentence:\n18. Input:\nThe large apple was delicious.\n18. Output:\nApple&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Empty outputs:&lt;/strong&gt; Some entries in the original dataset had empty outputs.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;instruction&#34;: &#34;Calculate the area of the following shape in square centimeters.&#34;,&#xA;&#34;input&#34;: &#34;rectangle of size 4 cm x 5 cm&#34;,&#xA;&#34;output&#34;: &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Empty code examples:&lt;/strong&gt; Some descriptions in the original dataset were missing code examples, making it difficult to understand the intended behavior of the code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Instructions to generate images:&lt;/strong&gt; Some descriptions in the original dataset included instructions to generate images, something obviously not possible.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;instruction&#34;: &#34;Create a graphic or logo that visually represents the word \&#34;courage\&#34;.&#34;,&#xA;&#34;input&#34;: &#34;&#34;,&#xA;&#34;output&#34;: &#34;&amp;lt;No Output&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;N/A outputs:&lt;/strong&gt; Some code snippets in the original dataset had N/A outputs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Inconsistent input field:&lt;/strong&gt; The original dataset had inconsistent usage of the input field when it was supposed to be empty.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;input&#34;:&#34;&amp;lt;no input&amp;gt;&#34;&#xA;&#34;input&#34;:&#34;No input&#34;&#xA;&#34;input&#34;:&#34;noinput&#34;&#xA;&#34;input&#34;:&#34;&amp;lt;noinput&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wrong answers:&lt;/strong&gt; Some instructions/questions in the original dataset had incorrect answers. About 80% of the math problems are estimated to have incorrect answers.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;instruction&#34;: &#34;Calculate the median of the following data set.&#34;,&#xA;&#34;input&#34;: &#34;1, 2, 4, 5, 8, 9&#34;,&#xA;&#34;output&#34;: &#34;5&#34;&#xA;&#xA;&#34;instruction&#34;: &#34;Convert 25m to km.&#34;,&#xA;&#34;input&#34;: &#34;&#34;,&#xA;&#34;output&#34;: &#34;25km&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Non-Sensical/Unclear instructions:&lt;/strong&gt; Many instructions are unclear, we try to clarify (or re-write) if instructions are non-sensical. Instructions that are slightly unclear, but where one could deduce the meaning are not altered.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;instruction&#34;: &#34;Freeze the following sample of yogurt for 10 minutes.&#34;,&#xA;&#34;input&#34;: &#34;Yogurt sample&#34;,&#xA;&#34;output&#34;: &#34;&amp;lt;noinput&amp;gt;&#34;&#xA;&#xA;&#34;instruction&#34;: &#34;Increase the font size to 12 points.&#34;,&#xA;&#34;input&#34;: &#34;&#34;,&#xA;&#34;output&#34;: &#34;The font size has been increased to 12 points.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extraneous escape and control characters:&lt;/strong&gt; The original dataset had several entries with extraneous escape and control characters.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Hugging Face Hub&lt;/h2&gt; &#xA;&lt;p&gt;The cleaned dataset is also available on the &lt;a href=&#34;https://huggingface.co/datasets/yahma/alpaca-cleaned&#34;&gt;Hugging Face Hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;With over 52k entries, several issues still exist. Please help out by submitting a pull-request.&lt;/p&gt; &#xA;&lt;h2&gt;Goals&lt;/h2&gt; &#xA;&lt;p&gt;The primary goal of this project is to provide a cleaned and curated version of the Alpaca dataset that will improve the performance of natural language processing models trained on this data. By removing errors and inconsistencies, the goal is to improve performance of the fine-tuned llama models and reduce the likelihood of hallucinations.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The original version of the Alpaca dataset was sourced from tatsu-lab&#39;s &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;github repository&lt;/a&gt;. We would like to thank the original creators of these datasets for making their data available to the public. We would also like to thank the team at Meta AI for their work in developing &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lukasHoel/text2room</title>
    <updated>2023-03-26T01:42:57Z</updated>
    <id>tag:github.com,2023-03-26:/lukasHoel/text2room</id>
    <link href="https://github.com/lukasHoel/text2room" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text2Room generates textured 3D meshes from a given text prompt using 2D text-to-image models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text2Room&lt;/h1&gt; &#xA;&lt;p&gt;Text2Room generates textured 3D meshes from a given text prompt using 2D text-to-image models.&lt;/p&gt; &#xA;&lt;p&gt;This is the official repository that contains source code for the arXiv paper &lt;a href=&#34;https://lukashoel.github.io/text-to-room/&#34;&gt;Text2Room&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2303.11989&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://lukashoel.github.io/text-to-room/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://youtu.be/fjRnFL91EZc&#34;&gt;Video&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lukasHoel/text2room/main/docs/teaser.jpg&#34; alt=&#34;Teaser&#34; title=&#34;Text2Room&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find Text2Room useful for your work please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@preprint{hoellein2023text2room,&#xA;  title={Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models},&#xA;  author={H{\&#34;o}llein, Lukas and Cao, Ang and Owens, Andrew and Johnson, Justin and Nie{\ss}ner, Matthias},&#xA;  journal={arXiv preprint arXiv:2303.11989},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prepare Environment&lt;/h2&gt; &#xA;&lt;p&gt;Create a conda environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n text2room python=3.9&#xA;conda activate text2room&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install Pytorch3D by following the &lt;a href=&#34;https://github.com/facebookresearch/pytorch3d/raw/main/INSTALL.md&#34;&gt;official instructions&lt;/a&gt;. For example, to install Pytorch3D on Linux (tested with PyTorch 1.13.1, CUDA 11.7, Pytorch3D 0.7.2):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install -c fvcore -c iopath -c conda-forge fvcore iopath&#xA;pip install &#34;git+https://github.com/facebookresearch/pytorch3d.git@stable&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the pretrained model weights for the fixed depth inpainting model, that we use:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;refer to the &lt;a href=&#34;https://github.com/baegwangbin/IronDepth&#34;&gt;official IronDepth implemention&lt;/a&gt; to download the files &lt;code&gt;normal_scannet.pt&lt;/code&gt; and &lt;code&gt;irondepth_scannet.pt&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;place the files under &lt;code&gt;text2room/checkpoints&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(Optional) Download the pretrained model weights for the text-to-image model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://huggingface.co/stabilityai/stable-diffusion-2-inpainting&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://huggingface.co/stabilityai/stable-diffusion-2-1&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ln -s &amp;lt;path/to/stable-diffusion-2-inpainting&amp;gt; checkpoints&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ln -s &amp;lt;path/to/stable-diffusion-2-1&amp;gt; checkpoints&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generate a Scene&lt;/h2&gt; &#xA;&lt;p&gt;As default, we generate a living room scene:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python generate_scene.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Outputs are stored in &lt;code&gt;text2room/output&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Generated outputs&lt;/h3&gt; &#xA;&lt;p&gt;We generate the following outputs per generated scene:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Mesh Files:&#xA;    &amp;lt;output_root&amp;gt;/fused_mesh/after_generation.ply: generated mesh after the first stage of our method&#xA;    &amp;lt;output_root&amp;gt;/fused_mesh/fused_final.ply: generated mesh after the second stage of our method&#xA;    &amp;lt;output_root&amp;gt;/fused_mesh/x_poisson_meshlab_depth_y.ply: result of applying poisson surface reconstruction on mesh x with depth y&#xA;    &amp;lt;output_root&amp;gt;/fused_mesh/x_poisson_meshlab_depth_y_quadric_z.ply: result of applying poisson surface reconstruction on mesh x with depth y and then decimating the mesh to have at least z faces&#xA;    &#xA;Renderings:&#xA;    &amp;lt;output_root&amp;gt;/output_rendering/rendering_t.png: image from pose t, that was rendered from the final mesh&#xA;    &amp;lt;output_root&amp;gt;/output_rendering/rendering_noise_t.png: image from a slightly different/noised pose t, that was rendered from the final mesh&#xA;    &amp;lt;output_root&amp;gt;/output_depth/depth_t.png: depth from pose t, that was rendered from the final mesh&#xA;    &amp;lt;output_root&amp;gt;/output_depth/depth_noise_t.png: depth from a slightly different/noised pose t, that was rendered from the final mesh&#xA;&#xA;Metadata:&#xA;    &amp;lt;output_root&amp;gt;/settings.json: all arguments used to generate the scene&#xA;    &amp;lt;output_root&amp;gt;/seen_poses.json: list of all poses in Pytorch3D convention used to render output_rendering (no noise)&#xA;    &amp;lt;output_root&amp;gt;/seen_poses_noise.json: list of all poses in Pytorch3D convention used to render output_rendering (with noise)&#xA;    &amp;lt;output_root&amp;gt;/transforms.json: a file in the standard NeRF convention (e.g. see NeRFStudio) that can be used to optimize a NeRF for the generated scene. It refers to the rendered images in output_rendering (no noise).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also generate the following intermediate outputs during generation of the scene:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    &amp;lt;output_root&amp;gt;/fused_mesh/fused_until_frame_t.ply: generated mesh using the content until pose t&#xA;    &amp;lt;output_root&amp;gt;/rendered/rendered_t.png: image from pose t, that was rendered from mesh_t&#xA;    &amp;lt;output_root&amp;gt;/mask/mask_t.png: mask from pose t, that signals unobserved regions&#xA;    &amp;lt;output_root&amp;gt;/mask/mask_eroded_dilated_t.png: mask from pose t, after applying erosion/dilation&#xA;    &amp;lt;output_root&amp;gt;/rgb/rgb_t.png: image from pose t, that was inpainted with the text-to-image model&#xA;    &amp;lt;output_root&amp;gt;/depth/rendered_depth_t.png: depth from pose t, that was rendered from mesh_t&#xA;    &amp;lt;output_root&amp;gt;/depth/depth_t.png: depth from pose t, that was predicted/aligned from rgb_t and rendered_depth_t&#xA;    &amp;lt;output_root&amp;gt;/rgbd/rgbd_t.png: combination of rgb_t and depth_t placed next to each other&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Create a scene from a fixed start-image&lt;/h3&gt; &#xA;&lt;p&gt;Already have an in-the-wild image, from which you want to start the generation? Specify it as &lt;code&gt;--input_image_path&lt;/code&gt; and the generated scene kicks-off from there.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python generate_scene.py --input_image_path sample_data/0.png&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Create a scene from another room type&lt;/h3&gt; &#xA;&lt;p&gt;Generate indoor-scenes of arbitrary rooms by specifying another &lt;code&gt;--trajectory_file&lt;/code&gt; as input:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python generate_scene.py --trajectory_file model/trajectories/examples/bedroom.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide a bunch of &lt;a href=&#34;https://raw.githubusercontent.com/lukasHoel/text2room/main/model/trajectories/examples&#34;&gt;example rooms&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Customize Generation&lt;/h3&gt; &#xA;&lt;p&gt;We provide a highly configurable method. See &lt;a href=&#34;https://raw.githubusercontent.com/lukasHoel/text2room/main/model/utils/opt.py&#34;&gt;opt.py&lt;/a&gt; for a complete list of the configuration options.&lt;/p&gt; &#xA;&lt;h3&gt;Get creative!&lt;/h3&gt; &#xA;&lt;p&gt;You can specify your own prompts and camera trajectories by simply creating your own &lt;code&gt;trajectory.json&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h4&gt;Trajectory Format&lt;/h4&gt; &#xA;&lt;p&gt;Each &lt;code&gt;trajectory.json&lt;/code&gt; file should satisfy the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;  {&#xA;    &#34;prompt&#34;: (str, optional) the prompt to use for this trajectory,&#xA;    &#34;negative_prompt&#34;: (str, optional) the negative prompt to use for this trajectory,&#xA;    &#34;n_images&#34;: (int, optional) how many images to render between start and end pose of this trajectory,&#xA;    &#34;surface_normal_threshold&#34;: (float, optional) the surface_normal_threshold to use for this trajectory&#xA;    &#34;fn_name&#34;: (str, required) the name of a trajectory_function as specified in model/trajectories/trajectory_util.py&#xA;    &#34;fn_args&#34;: (dict, optional) {&#xA;      &#34;a&#34;: value for an argument with name &#39;a&#39; of fn_name,&#xA;      &#34;b&#34;: value for an argument with name &#39;b&#39; of fn_name,&#xA;    },&#xA;    &#34;adaptive&#34;: (list, optional) [&#xA;      {&#xA;        &#34;arg&#34;: (str, required) name of an argument of fn_name that represents a float value,&#xA;        &#34;delta&#34;: (float, required) delta value to add to the argument during adaptive pose search,&#xA;        &#34;min&#34;: (float, optional) minimum value during search,&#xA;        &#34;max&#34;: (float, optional) maximum value during search&#xA;      }&#xA;    ]&#xA;  },&#xA;  &#xA;  {... next trajectory with similar structure as above ...}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Adding new trajectory functions&lt;/h4&gt; &#xA;&lt;p&gt;We provide a bunch of predefined trajectory functions in &lt;a href=&#34;https://raw.githubusercontent.com/lukasHoel/text2room/main/model/trajectories/trajectory_util.py&#34;&gt;trajectory_util.py&lt;/a&gt;. Each &lt;code&gt;trajectory.json&lt;/code&gt; file is a combination of the provided trajectory functions. You can create custom trajectories by creating new combinations of existing functions. You can also add custom trajectory functions in &lt;a href=&#34;https://raw.githubusercontent.com/lukasHoel/text2room/main/model/trajectories/trajectory_util.py&#34;&gt;trajectory_util.py&lt;/a&gt;. For automatic integration with our codebase, custom trajectory functions should have the following pattern:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;def custom_trajectory_fn(current_step, n_steps, **args):&#xA;    # n_steps: how many poses including start and end pose in this trajectory&#xA;    # current_step: pose in the current trajectory&#xA;    &#xA;    # your custom trajectory function here...&#xA;&#xA;def custom_trajectory(**args):&#xA;    return _config_fn(custom_trajectory_fn, **args)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This lets you reference &lt;code&gt;custom_trajectory&lt;/code&gt; as &lt;code&gt;fn_name&lt;/code&gt; in a &lt;code&gt;trajectory.json&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;Render an existing scene&lt;/h2&gt; &#xA;&lt;p&gt;We provide a script that renders images from a mesh at different poses:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python render_cameras.py -m &amp;lt;path/to/mesh.ply&amp;gt; -c &amp;lt;path/to/cameras.json&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;where you can provide any cameras in the Pytorch3D convention via &lt;code&gt;-c&lt;/code&gt;. For example, to re-render all poses used during generation and completion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python render_cameras.py \&#xA;-m &amp;lt;output_root&amp;gt;/fused_mesh/fused_final_poisson_meshlab_depth_12.ply \&#xA;-c &amp;lt;output_root&amp;gt;/seen_poses.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Optimize a NeRF&lt;/h2&gt; &#xA;&lt;p&gt;We provide an easy way to train a NeRF from our generated scene. We save a &lt;code&gt;transforms.json&lt;/code&gt; file in the standard NeRF convention, that can be used to optimize a NeRF for the generated scene. It refers to the rendered images in &lt;code&gt;&amp;lt;output_root&amp;gt;/output_rendering&lt;/code&gt;. It can be used with standard NeRF frameworks like &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;Instant-NGP&lt;/a&gt; or &lt;a href=&#34;https://github.com/nerfstudio-project/nerfstudio&#34;&gt;NeRFStudio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Our work builds on top of amazing open-source networks and codebases. We thank the authors for providing them.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/baegwangbin/IronDepth&#34;&gt;IronDepth&lt;/a&gt; [1]: a method for monocular depth prediction, that can be used for depth inpainting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-inpainting&#34;&gt;StableDiffusion&lt;/a&gt; [2]: a state-of-the-art text-to-image inpainting model with publicly released network weights.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;[1] IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty, BMVC 2022, Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla&lt;/p&gt; &#xA;&lt;p&gt;[2] High-Resolution Image Synthesis with Latent Diffusion Models, CVPR 2022, Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>sdatkinson/NeuralAmpModelerPlugin</title>
    <updated>2023-03-26T01:42:57Z</updated>
    <id>tag:github.com,2023-03-26:/sdatkinson/NeuralAmpModelerPlugin</id>
    <link href="https://github.com/sdatkinson/NeuralAmpModelerPlugin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Plugin for Neural Amp Modeler&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Neural Amp Modeler Plug-in&lt;/h1&gt; &#xA;&lt;p&gt;A VST3/AudioUnit plug-in* for &lt;a href=&#34;https://github.com/sdatkinson/neural-amp-modeler&#34;&gt;Neural Amp Modeler&lt;/a&gt;, built with &lt;a href=&#34;https://iplug2.github.io&#34;&gt;iPlug2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/user/RunawayThumbtack&#34;&gt;https://www.youtube.com/user/RunawayThumbtack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sdatkinson/neural-amp-modeler&#34;&gt;https://github.com/sdatkinson/neural-amp-modeler&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/sdatkinson/NeuralAmpModelerPlugin/releases&#34;&gt;Releases&lt;/a&gt; for pre-built installers for the plugin!&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;This is a cleaned up version of &lt;a href=&#34;https://github.com/sdatkinson/iPlug2&#34;&gt;the original iPlug2-based NAM plugin&lt;/a&gt; with some refactoring to adopt better practices recommended by the developers of iPlug2. (Thanks &lt;a href=&#34;https://github.com/olilarkin&#34;&gt;Oli&lt;/a&gt; for your generous suggestions!)&lt;/p&gt; &#xA;&lt;p&gt;*could also support VST2, AAX, CLAP, Linux, iOS soon.&lt;/p&gt;</summary>
  </entry>
</feed>