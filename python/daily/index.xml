<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-10T01:34:59Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>heshengtao/comfyui_LLM_party</title>
    <updated>2024-08-10T01:34:59Z</updated>
    <id>tag:github.com,2024-08-10:/heshengtao/comfyui_LLM_party</id>
    <link href="https://github.com/heshengtao/comfyui_LLM_party" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dify in comfyui is compatible with Omost,ChatTTS,access to Feishu,discord,and adapts to all models with similar openai interfaces, such as ollama, qwen, GLM, deepseek, moonshot,doubao. Adapted to local models such as llama/qwen/GLM,Linkage neo4j KG，Implemented the function of graphRAG.Supports a variety of RAG.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/img/%E5%B0%81%E9%9D%A2.png&#34; alt=&#34;图片&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://space.bilibili.com/26978344&#34;&gt;video tutorial&lt;/a&gt; · &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/how_to_use_nodes.md&#34;&gt;text tutorial&lt;/a&gt; · &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow_tutorial/&#34;&gt;workflow tutorial&lt;/a&gt; · &#xA; &lt;a href=&#34;https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&amp;amp;pwd=qyhu&#34;&gt;Baidu cloud&lt;/a&gt; · &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/img/Q%E7%BE%A4.jpg&#34;&gt;QQ group&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/English-d9d9d9&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/README_ZH.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;&lt;/h4&gt; &#xA;&lt;p&gt;C‌‌﻿​﻿‎﻿​﻿‎‏​﻿‍‎​﻿‎﻿​﻿‎‏​﻿‌‎​﻿‎‍​﻿‍‏​‍﻿‌​﻿‌‏omfyui_llm_party aims to develop a complete set of nodes for LLM workflow construction based on &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;comfyui&lt;/a&gt; as the front end. It allows users to quickly and conveniently build their own LLM workflows and easily integrate them into their existing SD workflows.&lt;/p&gt; &#xA;&lt;h2&gt;Effect display&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6&#34;&gt;https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Latest update&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can use this LLM tool maker to automatically generate LLM tools, save the tool code you generated as a python file, and then copy the code to the custom_tool folder, and then you create a new node. Example workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/LLM%E5%B7%A5%E5%85%B7%E5%88%B6%E9%80%A0%E6%9C%BA.json&#34;&gt;LLM tool generator&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;It supports duckduckgo search, but it has significant limitations. It seems that only English keywords can be entered, and multiple concepts cannot appear in keywords. The advantage is that there are no APIkey restrictions.&lt;/li&gt; &#xA; &lt;li&gt;It supports the function of calling multiple knowledge bases separately, and it is possible to specify which knowledge base is used to answer questions in the prompt word. Example workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E5%A4%9A%E7%9F%A5%E8%AF%86%E5%BA%93%E5%88%86%E5%88%AB%E8%B0%83%E7%94%A8.json&#34;&gt;multiple knowledge bases are called separately&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Support LLM input extra parameters, including advanced parameters such as json out. Example workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/LLM%E9%A2%9D%E5%A4%96%E5%8F%82%E6%95%B0eg_JSON_OUT.json&#34;&gt;LLM input extra parameters&lt;/a&gt;.&lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E7%94%A8json_out%E5%88%86%E7%A6%BB%E6%8F%90%E7%A4%BA%E8%AF%8D.json&#34;&gt;Separate prompt words with json_out&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added the function of connecting the agent to discord. (still testing)&lt;/li&gt; &#xA; &lt;li&gt;Added the function of connecting the agent to Feishu, thank you very much &lt;a href=&#34;https://github.com/guobalove&#34;&gt;guobalove&lt;/a&gt; for your contribution! Refer to the workflow &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E9%A3%9E%E4%B9%A6%E6%9C%BA%E5%99%A8%E4%BA%BA.json&#34;&gt;Feishu robot&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Added universal API call node and a large number of auxiliary nodes for constructing the request body and grabbing the information in the response.&lt;/li&gt; &#xA; &lt;li&gt;Added empty model node, you can uninstall LLM from video memory at any location!&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/2noise/ChatTTS&#34;&gt;chatTTS&lt;/a&gt; node has been added, thank you very much for the contribution of &lt;a href=&#34;https://github.com/guobalove&#34;&gt;guobalove&lt;/a&gt;! &lt;code&gt;model_path&lt;/code&gt; parameter can be empty! It is recommended to use &lt;code&gt;HF&lt;/code&gt; mode to load the model, the model will be automatically downloaded from hugging face, no need to download manually; if using &lt;code&gt;local&lt;/code&gt; loading, please put the model&#39;s&lt;code&gt;asset&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; folders in the root directory. &lt;a href=&#34;https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&amp;amp;pwd=qyhu&#34;&gt;Baidu cloud address&lt;/a&gt;, extraction code: qyhu; if using &lt;code&gt;custom&lt;/code&gt; mode to load, please put the model&#39;s &lt;code&gt;asset&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; folders under &lt;code&gt;model_path&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;User Guide&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;For the instructions for using the node, please refer to: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/how_to_use_nodes.md&#34;&gt;how to use nodes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If there are any issues with the plugin or you have other questions, feel free to join the QQ group: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/img/Q%E7%BE%A4.jpg&#34;&gt;931057213&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Please refer to the workflow tutorial: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow_tutorial/&#34;&gt;Workflow Tutorial&lt;/a&gt;, thanks to &lt;a href=&#34;https://github.com/HuangYuChuh&#34;&gt;HuangYuChuh&lt;/a&gt; for your contribution!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;More workflows please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow&#34;&gt;workflow&lt;/a&gt; folder.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Vedio tutorial&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1JZ421v7Tw/?vd_source=f229e378448918b84afab7c430c6a75b&#34;&gt;Building a Modular AI with ComfyUI×LLM: A Step-by-Step Tutorial (Super Easy!)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1JJ4m1A789/?spm_id_from=333.999.0.0&amp;amp;vd_source=f229e378448918b84afab7c430c6a75b&#34;&gt;Teach you GPT-4o access to comfyui | Make workflow call another workflow | Make LLM a tool&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1DT421a7KY/?spm_id_from=333.999.0.0&#34;&gt;Disguise your workflow as GPT to access WeChat | Omost compatible! Flexibly create your own dalle3&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV15y411q78L/?spm_id_from=333.999.0.0&amp;amp;vd_source=f229e378448918b84afab7c430c6a75b&#34;&gt;How to play interactive fiction games in comfyui&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1dS421R7Au/?spm_id_from=333.999.0.0&amp;amp;vd_source=f229e378448918b84afab7c430c6a75b&#34;&gt;AI girlfriend, and is your shape | comfyui on the implementation of graphRAG, linkage neoa4j | comfyui workflow access streamlit front-end&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Model support&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Support all API calls in openai format, base_url selection reference &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/config.ini.example&#34;&gt;config.ini.example&lt;/a&gt;, which has been tested so far:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;ollama&lt;/a&gt;(Recommended! If you are calling locally, it is highly recommended to use ollama to host your local model!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq&#34;&gt;Tongyi Qianwen /qwen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://open.bigmodel.cn/dev/api#http_auth&#34;&gt;zhipu qingyan/glm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.deepseek.com/api-docs/zh-cn/&#34;&gt;deepseek&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF&#34;&gt;kimi/moonshot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.volcengine.com/docs/82379/1263482&#34;&gt;doubao&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Most of the local models supported by the transformer library have been tested so far:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/THUDM/chatglm3-6b&#34;&gt;THUDM/chatglm3-6b&lt;/a&gt;(Due to the new calling format of GLM4, developers cannot maintain all local large model calls, because it is recommended that everyone use ollama to call locally!)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;meta-llama/llama-2-7b-chat-hf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2-7B-Instruct&#34;&gt;Qwen/Qwen2-7B-Instruct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf&#34;&gt;xtuner/llava-llama-3-8b-v1_1-gguf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits&#34;&gt;omost-llama-3-8b-4bits&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Model download&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&amp;amp;pwd=qyhu&#34;&gt;Baidu cloud address&lt;/a&gt;, extraction code: qyhu&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;Install using one of the following methods:&lt;/p&gt; &#xA;&lt;h3&gt;Method 1:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Search for comfyui_LLM_party in the &lt;a href=&#34;https://github.com/ltdrdata/ComfyUI-Manager&#34;&gt;comfyui manager&lt;/a&gt; and install it with one click.&lt;/li&gt; &#xA; &lt;li&gt;Restart comfyui.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Method 2:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to the &lt;code&gt;custom_nodes&lt;/code&gt; subfolder under the ComfyUI root folder.&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository with &lt;code&gt;git clone https://github.com/heshengtao/comfyui_LLM_party.git&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Method 3:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click &lt;code&gt;CODE&lt;/code&gt; in the upper right corner.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;download zip&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Unzip the downloaded package into the &lt;code&gt;custom_nodes&lt;/code&gt; subfolder under the ComfyUI root folder.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Environment Deployment&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to the &lt;code&gt;comfyui_LLM_party&lt;/code&gt; project folder.&lt;/li&gt; &#xA; &lt;li&gt;Enter &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; in the terminal to deploy the third-party libraries required by the project into the comfyui environment. Please ensure you are installing within the comfyui environment and pay attention to any &lt;code&gt;pip&lt;/code&gt; errors in the terminal.&lt;/li&gt; &#xA; &lt;li&gt;If you are using the comfyui launcher, you need to enter &lt;code&gt;path_in_launcher_configuration\python_embeded\python.exe -m pip install -r requirements.txt&lt;/code&gt; in the terminal to install. The &lt;code&gt;python_embeded&lt;/code&gt; folder is usually at the same level as your &lt;code&gt;ComfyUI&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;If you have some environment configuration problems, you can try to use the dependencies in &lt;code&gt;requirements_fixed.txt&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;APIKEY can be configured using one of the following methods&lt;/p&gt; &#xA;&lt;h3&gt;Method 1:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the &lt;code&gt;config.ini&lt;/code&gt; file in the project folder of the &lt;code&gt;comfyui_LLM_party&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Enter your openai_api_key, base_url in &lt;code&gt;config.ini&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you are using an ollama model, fill in &lt;code&gt;http://127.0.0.1:11434/v1/&lt;/code&gt; in &lt;code&gt;base_url&lt;/code&gt;, &lt;code&gt;ollama&lt;/code&gt; in &lt;code&gt;openai_api_key&lt;/code&gt;, and your model name in &lt;code&gt;model_name&lt;/code&gt;, for example: &lt;code&gt;llama3&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use Google search or Bing search tools, enter your &lt;code&gt;google_api_key&lt;/code&gt;, &lt;code&gt;cse_id&lt;/code&gt; or &lt;code&gt;bing_api_key&lt;/code&gt; in &lt;code&gt;config.ini&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use image input LLM, it is recommended to use image bed imgbb and enter your imgbb_api in &lt;code&gt;config.ini&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Each model can be configured separately in the &lt;code&gt;config.ini&lt;/code&gt; file, which can be filled in by referring to the &lt;code&gt;config.ini.example&lt;/code&gt; file. After you configure it, just enter &lt;code&gt;model_name&lt;/code&gt; on the node.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Method 2:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the comfyui interface.&lt;/li&gt; &#xA; &lt;li&gt;Create a Large Language Model (LLM) node and enter your openai_api_key and base_url directly in the node.&lt;/li&gt; &#xA; &lt;li&gt;If you use the ollama model, use LLM_api node, fill in &lt;code&gt;http://127.0.0.1:11434/v1/&lt;/code&gt; in &lt;code&gt;base_url&lt;/code&gt; node, fill in &lt;code&gt;ollama&lt;/code&gt; in &lt;code&gt;api_key&lt;/code&gt;, and fill in your model name in &lt;code&gt;model_name&lt;/code&gt;, for example: &lt;code&gt;llama3&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use image input LLM, it is recommended to use graph bed imgbb and enter your &lt;code&gt;imgbb_api_key&lt;/code&gt; on the node.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can right-click in the comfyui interface, select &lt;code&gt;llm&lt;/code&gt; from the context menu, and you will find the nodes for this project. &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/how_to_use_nodes.md&#34;&gt;how to use nodes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Supports API integration or local large model integration. Modular implementation for tool invocation.When entering the base_url, please use a URL that ends with &lt;code&gt;/v1/&lt;/code&gt;.You can use &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;ollama&lt;/a&gt; to manage your model. Then, enter &lt;code&gt;http://127.0.0.1:11434/v1/&lt;/code&gt; for the base_url, &lt;code&gt;ollama&lt;/code&gt; for the api_key, and your model name for the model_name, such as: llama3.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;API access sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/start_with_LLM_api&#34;&gt;start_with_LLM_api&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Local model access sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/start_with_LLM_local&#34;&gt;start_with_LLM_local&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ollama access sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/ollama.json&#34;&gt;ollama&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Local knowledge base integration with RAG support.sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E7%9F%A5%E8%AF%86%E5%BA%93RAG%E6%90%9C%E7%B4%A2.json&#34;&gt;Knowledge Base RAG Search&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ability to invoke code interpreters.&lt;/li&gt; &#xA; &lt;li&gt;Enables online queries, including Google search support.sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E7%94%B5%E5%BD%B1%E6%9F%A5%E8%AF%A2%E5%B7%A5%E4%BD%9C%E6%B5%81.json&#34;&gt;movie query workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Implement conditional statements within ComfyUI to categorize user queries and provide targeted responses.sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D.json&#34;&gt;intelligent customer service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Supports looping links for large models, allowing two large models to engage in debates.sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E7%94%B5%E8%BD%A6%E9%9A%BE%E9%A2%98%E8%BE%A9%E8%AE%BA%E8%B5%9B.json&#34;&gt;Tram Challenge Debate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Attach any persona mask, customize prompt templates.&lt;/li&gt; &#xA; &lt;li&gt;Supports various tool invocations, including weather lookup, time lookup, knowledge base, code execution, web search, and single-page search.&lt;/li&gt; &#xA; &lt;li&gt;Use LLM as a tool node.sample workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/LLM%E5%A5%97%E5%A8%83.json&#34;&gt;LLM Matryoshka dolls&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rapidly develop your own web applications using API + Streamlit.&lt;/li&gt; &#xA; &lt;li&gt;Added a dangerous omnipotent interpreter node that allows the large model to perform any task.&lt;/li&gt; &#xA; &lt;li&gt;It is recommended to use the &lt;code&gt;show_text&lt;/code&gt; node under the &lt;code&gt;function&lt;/code&gt; submenu of the right-click menu as the display output for the LLM node.&lt;/li&gt; &#xA; &lt;li&gt;Supported the visual features of GPT-4O!sample workflow:&lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/GPT-4o.json&#34;&gt;GPT-4o&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A new workflow intermediary has been added, which allows your workflow to call other workflows!sample workflow:&lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E8%B0%83%E7%94%A8%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%B7%A5%E4%BD%9C%E6%B5%81.json&#34;&gt;Invoke another workflow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adapted to all models with an interface similar to OpenAI, such as: Tongyi Qianwen/QWEN, Zhigu Qingyan/GLM, DeepSeek, Kimi/Moonshot. Please fill in the base_url, api_key, and model_name of these models into the LLM node to call them.&lt;/li&gt; &#xA; &lt;li&gt;Added an LVM loader, now you can call the LVM model locally, support &lt;a href=&#34;https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf&#34;&gt;lava-llama-3-8b-v1_1-gguf&lt;/a&gt; model, other LVM models should theoretically run if they are GUFF format.The example workflow can be found here: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/start_with_LVM.json&#34;&gt;start_with_LVM.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;I wrote a &lt;code&gt;fastapi.py&lt;/code&gt; file, and if you run it directly, you’ll get an OpenAI interface on &lt;code&gt;http://127.0.0.1:8817/v1/&lt;/code&gt;. Any application that can call GPT can now invoke your comfyui workflow! I will create a tutorial to demonstrate the details on how to do this.&lt;/li&gt; &#xA; &lt;li&gt;I’ve separated the LLM loader and the LLM chain, dividing the model loading and model configuration. This allows for sharing models across different LLM nodes!&lt;/li&gt; &#xA; &lt;li&gt;macOS and mps devices are now supported! Thanks to &lt;a href=&#34;https://github.com/bigcat88&#34;&gt;bigcat88&lt;/a&gt; for their contribution!&lt;/li&gt; &#xA; &lt;li&gt;You can build your own interactive novel game, and go to different endings according to the user&#39;s choice! Example workflow reference: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E4%BA%92%E5%8A%A8%E5%B0%8F%E8%AF%B4.json&#34;&gt;interactive_novel&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Adapted to OpenAI&#39;s whisper and tts functions, voice input and output can be realized. Example workflow reference: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E8%AF%AD%E9%9F%B3%E8%BE%93%E5%85%A5+%E8%AF%AD%E9%9F%B3%E8%BE%93%E5%87%BA.json&#34;&gt;voice_input&amp;amp;voice_output&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Compatible with &lt;a href=&#34;https://github.com/lllyasviel/Omost&#34;&gt;Omost&lt;/a&gt;!!! Please download &lt;a href=&#34;https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits&#34;&gt;omost-llama-3-8b-4bits&lt;/a&gt; to experience it now! Sample workflow reference: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/start_with_OMOST&#34;&gt;start_with_OMOST&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Added LLM tools to send messages to WeCom, DingTalk, and Feishu, as well as external functions to call.&lt;/li&gt; &#xA; &lt;li&gt;Added a new text iterator, which can output only part of the characters at a time. It is safe to split the text according to Carriage Return and chunk size, and will not be divided from the middle of the text. chunk_overlap refers to how many characters the divided text overlaps. In this way, you can enter super long text in batches, as long as you don&#39;t have a brain to click, or open the loop in comfyui to execute, it can be automatically executed. Remember to turn on the is_locked property, which can automatically lock the workflow at the end of the input and will not continue to execute. Example workflow: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E6%96%87%E6%9C%AC%E8%BF%AD%E4%BB%A3%E8%BE%93%E5%85%A5.json&#34;&gt;text iteration input&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Added the model name attribute to the local LLM loader, local llava loader. If it is empty, it will be loaded using various local paths in the node. If it is not empty, it will be loaded using the path parameters you fill in yourself in &lt;code&gt;config.ini&lt;/code&gt;. If it is not empty and not in &lt;code&gt;config.ini&lt;/code&gt;, it will be downloaded from huggingface or loaded from the model save directory of huggingface. If you want to download from huggingface, please fill in the format of for example: &lt;code&gt;THUDM/glm-4-9b-chat&lt;/code&gt;.Attention! Models loaded in this way must be adapted to the transformer library.&lt;/li&gt; &#xA; &lt;li&gt;Adapted to &lt;a href=&#34;https://github.com/FunAudioLLM/CosyVoice&#34;&gt;CosyVoice&lt;/a&gt;, now you can use the TTS function directly without downloading any model or any API key. Currently the interface is only adapted to Chinese.&lt;/li&gt; &#xA; &lt;li&gt;Added JSON file parsing node and JSON value node, which allows you to get the value of a key from a file or text. Thanks to &lt;a href=&#34;https://github.com/guobalove&#34;&gt;guobalove&lt;/a&gt; for your contribution!&lt;/li&gt; &#xA; &lt;li&gt;Improved the code of tool call. Now LLM without tool call function can also open is_tools_in_sys_prompt attribute (local LLM does not need to be opened by default, automatic adaptation). After opening, the tool information will be added to the system prompt word, so that LLM can call the tool.Related papers on implementation principles: &lt;a href=&#34;https://arxiv.org/abs/2407.04997&#34;&gt;Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A new custom_tool folder is created to store the code of the custom tool. You can refer to the code in the &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/custom_tool&#34;&gt;custom_tool&lt;/a&gt; folder, put the code of the custom tool into the custom_tool folder, and you can call the custom tool in LLM.&lt;/li&gt; &#xA; &lt;li&gt;Added Knowledge Graph tool, so that LLM and Knowledge Graph can interact perfectly. LLM can modify Knowledge Graph according to your input, and can reason on Knowledge Graph to get the answers you need. Example workflow reference: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/graphRAG_neo4j.json&#34;&gt;graphRAG_neo4j&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Added personality AI function, 0 code to develop your own girlfriend AI or boyfriend AI, unlimited dialogue, permanent memory, stable personality. Example workflow reference: &lt;a href=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/workflow/%E9%BA%A6%E6%B4%9B%E8%96%87%E4%BA%BA%E6%A0%BCAI.json&#34;&gt;Mylover Personality AI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Next Steps Plan:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;More model adaptations, at least covering the API interfaces of mainstream large models and local calls of mainstream open-source models, as well as more LVM model adaptations. Currently, I have only adapted the visual function calls of GPT-4;&lt;/li&gt; &#xA; &lt;li&gt;More ways to build agents. The work I have completed in this area includes importing an LLM as a tool to another LLM, achieving radial construction of LLM workflows, and importing one workflow as a node into another workflow. I might develop some cooler functions in this area in the future.&lt;/li&gt; &#xA; &lt;li&gt;More automation features. In the future, I will introduce more nodes that automatically push images, text, videos, and audio to other applications, as well as listening nodes that implement automatic replies to mainstream social software and forums.&lt;/li&gt; &#xA; &lt;li&gt;More knowledge base management functions. The project already supports local file search and web search. In the future, I will introduce knowledge graph search and long-term memory search. This will allow agents to think logically about professional knowledge and always remember certain key information when conversing with users.&lt;/li&gt; &#xA; &lt;li&gt;More tools, more persona. This part is the easiest to do but also requires the most accumulation. I hope that in the future, this project can have as many custom nodes as comfyui, with a multitude of tools and persona.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Disclaimer:&lt;/h2&gt; &#xA;&lt;p&gt;This open-source project and its contents (hereinafter referred to as &#34;Project&#34;) are provided for reference purposes only and do not imply any form of warranty, either expressed or implied. The contributors of the Project shall not be held responsible for the completeness, accuracy, reliability, or suitability of the Project. Any reliance you place on the Project is strictly at your own risk. In no event shall the contributors of the Project be liable for any indirect, special, or consequential damages or any damages whatsoever resulting from the use of the Project.&lt;/p&gt; &#xA;&lt;h2&gt;Special thanks:&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/bigcat88&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/13381981?v=4&#34; width=&#34;50&#34; height=&#34;50&#34; style=&#34;border-radius: 50%; overflow: hidden;&#34; alt=&#34;octocat&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://github.com/guobalove&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/171540731?v=4&#34; width=&#34;50&#34; height=&#34;50&#34; style=&#34;border-radius: 50%; overflow: hidden;&#34; alt=&#34;octocat&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://github.com/HuangYuChuh&#34;&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/167663109?v=4&#34; width=&#34;50&#34; height=&#34;50&#34; style=&#34;border-radius: 50%; overflow: hidden;&#34; alt=&#34;octocat&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Support:&lt;/h2&gt; &#xA;&lt;h3&gt;If my work has brought value to your day, consider fueling it with a coffee! Your support not only energizes the project but also warms the heart of the creator. ☕💖 Every cup makes a difference!&lt;/h3&gt; &#xA;&lt;div style=&#34;display:flex; justify-content:space-between;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/img/zhifubao.jpg&#34; style=&#34;width: 48%;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/img/wechat.jpg&#34; style=&#34;width: 48%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;If there is a problem with the plug-in or you have other questions, welcome to join the QQ group: 931057213&lt;/h3&gt; &#xA;&lt;div style=&#34;display:flex; justify-content:space-between;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/heshengtao/comfyui_LLM_party/main/img/Q%E7%BE%A4.jpg&#34; style=&#34;width: 48%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;If you want to continue to pay attention to the latest features of this project, please follow the Bilibili account: &lt;a href=&#34;https://space.bilibili.com/26978344&#34;&gt;Party host BB machine&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#heshengtao/comfyui_LLM_party&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>