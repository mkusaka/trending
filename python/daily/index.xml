<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-15T01:36:31Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>EvanZhouDev/donut-py</title>
    <updated>2023-10-15T01:36:31Z</updated>
    <id>tag:github.com,2023-10-15:/EvanZhouDev/donut-py</id>
    <link href="https://github.com/EvanZhouDev/donut-py" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üç© donut.py&lt;/h1&gt; &#xA;&lt;p&gt;The first-ever &lt;code&gt;donut.c&lt;/code&gt; Python clone that &lt;em&gt;looks like a donut&lt;/em&gt;!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;while (&#39;o_&#39; in dir()) or (A := (0)&#xA;               ) or (B := 0) or print((                           &#xA;            &#34;\x1b[2J&#34;)) or not ((sin := ((                        &#xA;         __import__(&#39;math&#39;))).sin)) or not (                     &#xA;      cos := __import__(&#39;math&#39;).cos) or (o_ := (                   &#xA;     True)): [pr() for b in [[(func(), b) for ((z                 &#xA;    )) in [[0 for _ in range(1760)]] for b in [[ (                &#xA;   &#34;\n&#34;) if ii % 80 == 79 else &#34; &#34; for ii in range(               &#xA;  1760)]] for o, D, N in [(o, D, N) for j in range((              &#xA; 0), 628, 7) for i in range(0, 628, 2) for (c, d, e,(             &#xA;f), g, l, m, n) in [(sin(     i / 100), cos(j / 100),(             &#xA;sin(A)), sin(j / 100),          cos(A), cos(i / 100) ,            &#xA;cos(B), sin(B))] for              (h) in [d + 2] for (            &#xA;D, t) in [ (1 / ( c               * h * e + f * g + (5&#xA;)), c * h * g - f *                e)] for (x, y) in [            &#xA;(int( 40 + 30 * D * (              l * h * m - t * n)),            &#xA;int( 12 + 15 * D * ( l           * h * n + t * (m))))]            &#xA; for (o, N) in [(x + 80 *     y,int(8 * (( f * e -  c              &#xA;  * d * g) * m - c * d * e - f * g - l * d * n)))] if             &#xA;  0 &amp;lt; x &amp;lt; 80 and 22 &amp;gt; y &amp;gt; 0] if D &amp;gt; z[o] for func in              &#xA;   [lambda: (z.pop((o))), lambda: z.insert((o), (D)                &#xA;    ),(lambda: (b.pop( o))), lambda: b.insert((o),                 &#xA;     &#34;.,-~:;=!*#$@&#34;[ N if N &amp;gt; 0 else 0])]][ 0][1]                 &#xA;       ] for pr in [lambda: print(&#34;\x1b[H&#34;),                    &#xA;         lambda: print(&#34;&#34;.join(b))] if (A :=&#xA;            A + 0.02) and ( B := B + 0.02)]                        &#xA;               #..--~~EvanZhouDev:~~--.#                           &#xA;                    #...,2023,...#&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 donut.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Original &lt;code&gt;donut.c&lt;/code&gt; idea from &lt;a href=&#34;https://www.a1k0n.net/2011/07/20/donut-math.html&#34;&gt;Andy Sloane&#39;s blog&lt;/a&gt;. Original one-line &lt;code&gt;donut.py&lt;/code&gt; implementation by &lt;a href=&#34;https://github.com/Julius-Syvis/DonutPy&#34;&gt;Julius Syvis&#39;s DonutPy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If the donut looks vertically stretched, then you may need to adjust your font to be a square font, or just modify line width and font spacing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>FlagAI-Open/Aquila2</title>
    <updated>2023-10-15T01:36:31Z</updated>
    <id>tag:github.com,2023-10-15:/FlagAI-Open/Aquila2</id>
    <link href="https://github.com/FlagAI-Open/Aquila2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official repo of Aquila2 series proposed by BAAI, including pretrained &amp; chat large language models.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;&amp;nbsp; ÔΩú &amp;amp;nbspEnglish&amp;nbsp; &lt;/p&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/BAAI&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://model.baai.ac.cn/models&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt; BAAI ModelHub&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/wechat-qrcode.jpg&#34;&gt;WeChat(ÂæÆ‰ø°)&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;We announce that our &lt;strong&gt;Aquila2&lt;/strong&gt; series is now open source, comprising Aquila2 (the base language models: &lt;strong&gt;Aquila2-7B&lt;/strong&gt; and &lt;strong&gt;Aquila2-34B&lt;/strong&gt;) and AquilaChat2 (the chat models, namely &lt;strong&gt;AquilaChat2-7B&lt;/strong&gt; and &lt;strong&gt;AquilaChat2-34B&lt;/strong&gt;, as well as the long-text chat models, namely &lt;strong&gt;AquilaChat2-7B-16k&lt;/strong&gt; and &lt;strong&gt;AquilaChat2-34B-16k&lt;/strong&gt;). You can find the links in the following table. Kindly click on them to access the model cards.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download Sources&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aquila2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://model.baai.ac.cn/model-detail/100118&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/BAAI/Aquila2-7B&#34;&gt;ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AquilaChat2-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://model.baai.ac.cn/model-detail/100117&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/BAAI/AquilaChat2-7B&#34;&gt;ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AquilaChat2-7B-16k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://model.baai.ac.cn/model-detail/100120&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/BAAI/AquilaChat2-7B-16K&#34;&gt;ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Aquila2-34B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://model.baai.ac.cn/model-detail/100119&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AquilaChat2-34B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://model.baai.ac.cn/model-detail/100116&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/BAAI/AquilaChat2-34B&#34;&gt;ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AquilaChat2-34B-16k&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://model.baai.ac.cn/model-detail/100121&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/BAAI/AquilaChat2-34B-16K&#34;&gt;ü§ó&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;In this repo, you can figure out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quickstart with Aquila2.&lt;/li&gt; &#xA; &lt;li&gt;Tutorials on finetuning, including full-parameter, LoRA, and Q-LoRA.&lt;/li&gt; &#xA; &lt;li&gt;Long-context understanding and evaluation&lt;/li&gt; &#xA; &lt;li&gt;License agreement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please don&#39;t hesitate to bring up issues and feel free to submit pull requests (PRs) at any time (p.s. better in English for wider comprehension) ‚Äì we&#39;re always enthusiastic about contributions! &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News and Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2023.10.12 üî• We release &lt;strong&gt;Aquila2&lt;/strong&gt; series on BAAI ModelHub and Hugging Face. &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Aquila2 series outperform the models of similar model sizes on a series of benchmark datasets.&lt;/p&gt; &#xA;&lt;h3&gt;Base Model Performance&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/base_metrics.jpeg&#34; width=&#34;1024&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h3&gt;Chat Model Performance&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/chat_metrics.jpeg&#34; width=&#34;1024&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt; In evaluating generative chat models, our team prioritizes how models autonomously respond to questions‚Äîa reflection of real-world user interactions. Guided by Stanford University&#39;s HELM [1] approach, our assessment emphasizes context understanding and instruction adherence. In some cases, models may deliver answers not in line with the instruction of input, resulting in a &#34;0&#34; score. For instance, if the model should respond with &#34;A&#34; but outputs &#34;B&#34; or &#34;The answer is A&#34;, it earns a &#34;0.&#34; Other industry methods include concatenating &#34;question+answer&#34; and assessing the combined text&#39;s probability. However, in this method, the chat model doesn&#39;t generate content but computing probability scores. Due to its divergence from real-world chat scenarios, we haven&#39;t adopted this approach in our evaluations. &lt;br&gt; [1] https://crfm.stanford.edu/helm/latest/ &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Long Context Performance&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ZH-Avg.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;EN-Avg.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;VCSUM(zh)&lt;br&gt;(Chinese)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LSHT(zh)&lt;br&gt;(Chinese)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;HotpotQA&lt;br&gt;(English)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;2WikiMQA&lt;br&gt;(English)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-3.5-Turbo-16K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AquilaChat2-34B-16K&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PI + SFT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;31.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;40.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;23.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;16.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;30.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;41.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;38.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGLM2-6B-32K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PI + SFT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AquilaChat2-7B-16K&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PI + SFT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;29.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;31.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;27.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;14.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;40.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;36.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;27.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;InternLM-7B-8K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGLM2-6B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LongChat-7B-v1.5-32K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;PI + SFT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-7B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Internlm-20B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qwen-14B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dynamic NTK&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;XGen-7B-8K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pre-train&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-7B-Chat-4K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-13B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Reasoning Tasks Performance&lt;/h3&gt; &#xA;&lt;br&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bAbI#16&lt;br&gt;(Inductive)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CLUTRR&lt;br&gt;(Inductive)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;bAbI#15&lt;br&gt;(Deductive)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;EntailmentBank&lt;br&gt;(Deductive)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Œ±NLI&lt;br&gt;(Abductive)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;E-Care&lt;br&gt;(Casual)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-7B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qwen-7B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qwen-14B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-13B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;InternLM-20B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;ChatGPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;86.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA-70B-Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;GPT-4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AquilaChat2-34B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;58.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;43.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;16.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;63.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;66.7&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AquilaChat2-34B+SFT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;73.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;16.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;70.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AquilaChat2-34B+SFT+CoT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;69.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;23.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;83.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;73.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.7&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python 3.10 and above&lt;/li&gt; &#xA; &lt;li&gt;pytorch 1.12 and above, 2.0 and above are recommended&lt;/li&gt; &#xA; &lt;li&gt;transformers 4.32 and above&lt;/li&gt; &#xA; &lt;li&gt;CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.) &lt;br&gt;&lt;br&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;We have provided a straightforward example to illustrate how to quickly get started with Aquila2.&lt;/p&gt; &#xA;&lt;p&gt;Before proceeding, ensure that your environment is properly configured and that the necessary packages have been installed. First and foremost, ensure that these prerequisites are met and then follow the instructions below to install the necessary libraries and dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;https://github.com/FlagAI-Open/FlagAI.git&#xA;(cd FlagAI/ &amp;amp;&amp;amp; python setup.py install)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If your device supports fp16 or bf16 precision, we also recommend installing flash-attention to enhance execution speed and reduce memory consumption. It&#39;s important to note that flash-attention is optional, and the project can be executed normally without it.&lt;/p&gt; &#xA;&lt;p&gt;For the installation of flash-attention, please follow the instructions in &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/&#34;&gt;https://github.com/Dao-AILab/flash-attention/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using Docker TAR File&lt;/h3&gt; &#xA;&lt;p&gt;You can also set up the environment required for Aquila2 by directly downloading &lt;a href=&#34;https://model.baai.ac.cn/model-detail/220118&#34;&gt;the Docker TAR file&lt;/a&gt;, then loading and running it. Because of all already installed dependencies, in the container you just pull all sources &lt;a href=&#34;https://github.com/FlagAI-Open/FlagAI.git&#34;&gt;FlagAI&lt;/a&gt; and &lt;a href=&#34;https://github.com/FlagAI-Open/Aquila2.git&#34;&gt;Aquila2&lt;/a&gt; and include both paths int environment variable, like export PYTHONPATH=$FLAGAI_HOME:$AQUILA2_HOME:$PYTHONPATH.&lt;/p&gt; &#xA;&lt;p&gt;Now you can use &lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;18&#34;&gt; BAAI Modelhub or ü§ó Transformers to run our model„ÄÇ&lt;/p&gt; &#xA;&lt;h3&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/baai.png&#34; width=&#34;20&#34;&gt; ModelHub&lt;/h3&gt; &#xA;&lt;p&gt;You can now use the AquilaChat2-7B model for inference as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from flagai.auto_model.auto_loader import AutoLoader&#xA;&#xA;# Model name&#xA;model_name = &#39;AquilaChat2-7B&#39;&#xA;# model_name = &#39;AquilaChat2-34B&#39;&#xA;&#xA;# Load the model and tokenizer&#xA;autoloader = AutoLoader(&#34;aquila2&#34;, model_name=model_name)&#xA;# To modify the model loading path, use the model_dir parameter&#xA;# autoloader = AutoLoader(&#34;aquila2&#34;, model_dir=&#39;./checkpoints&#39;, model_name=model_name)&#xA;# To load the LoRA module, you need to provide the path to the LoRA module&#xA;# autoloader = AutoLoader(&#34;aquila2&#34;, model_name=model_nameÔºålora_dir=&#39;./examples/checkpoints/lora/aquila2chat&#39;)&#xA;# To load the LoRA module, you need to provide the path to the LoRA module&#xA;# autoloader = AutoLoader(&#34;aquila2&#34;, model_name=model_nameÔºåqlora_dir=&#39;./examples/checkpoints/qlora/aquila2chat&#39;)&#xA;&#xA;model = autoloader.get_model()&#xA;tokenizer = autoloader.get_tokenizer()&#xA;&#xA;&#xA;# Example&#xA;test_data = [&#xA;    &#34;Write a tongue twister that&#39;s extremely difficult to pronounce.&#34;,&#xA;]&#xA;&#xA;for text in test_data:&#xA;    print(model.predict(text, tokenizer=tokenizer, model_name=model_name))&#xA;    # For Aquila2-7B or Aquila2-34BÔºåyou need to set sft=False&#xA;    # print(model.predict(text, tokenizer=tokenizer, model_name=model_name, sft=False))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results of our execution are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Harry had a harpy flight, Fred had a fiddle, and George had a gecko for breakfast.  Say that three times fast and see how long you can make it last!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ü§ó Transformers&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;import torch&#xA;device = torch.device(&#34;cuda&#34;)&#xA;model_info = &#34;BAAI/AquilaChat2-7B&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_info, trust_remote_code=True)&#xA;model = AutoModelForCausalLM.from_pretrained(model_info, trust_remote_code=True)&#xA;model.eval()&#xA;model.to(device)&#xA;text = &#34;ËØ∑ÁªôÂá∫10‰∏™Ë¶ÅÂà∞Âåó‰∫¨ÊóÖÊ∏∏ÁöÑÁêÜÁî±„ÄÇ&#34;&#xA;tokens = tokenizer.encode_plus(text)[&#39;input_ids&#39;]&#xA;tokens = torch.tensor(tokens)[None,].to(device)&#xA;stop_tokens = [&#34;###&#34;, &#34;[UNK]&#34;, &#34;&amp;lt;/s&amp;gt;&#34;]&#xA;with torch.no_grad():&#xA;    out = model.generate(tokens, do_sample=True, max_length=512, eos_token_id=100007, bad_words_ids=[[tokenizer.encode(token)[0] for token in stop_tokens]])[0]&#xA;    out = tokenizer.decode(out.cpu().numpy().tolist())&#xA;    print(out)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;p&gt;Before using quantization, BitsAndBytesConfig needs to be installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, you&#39;re all set to use the quantized models for inference!&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch &#xA;from flagai.auto_model.auto_loader import AutoLoader&#xA;from transformers import BitsAndBytesConfig&#xA;&#xA;model_name = &#39;AquilaChat2-7B&#39;&#xA;&#xA;autoloader = AutoLoader(&#34;aquila2&#34;, model_name=model_name, &#xA;    quantization_config=BitsAndBytesConfig(&#xA;        load_in_4bit=True,&#xA;        bnb_4bit_use_double_quant=True,&#xA;        bnb_4bit_quant_type=&#34;nf4&#34;,&#xA;        bnb_4bit_compute_dtype=torch.bfloat16,&#xA;    ))&#xA;&#xA;model = autoloader.get_model()&#xA;tokenizer = autoloader.get_tokenizer()&#xA;# &#xA;&#xA;test_data = [&#xA;    &#34;Write a tongue twister that&#39;s extremely difficult to pronounce.&#34;,&#xA;]&#xA;&#xA;for text in test_data:&#xA;    print(model.predict(text, tokenizer=tokenizer, model_name=model_name))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretraining&lt;/h2&gt; &#xA;&lt;p&gt;From Aquila2, we upgrade the underlying pretraining framework, which is now open-sourced as &lt;a href=&#34;https://github.com/FlagOpen/FlagScale&#34;&gt;FlagScale&lt;/a&gt;. It is based on the Megatron-LM project and aims at utilizing the computation resources efficiently for LLMs without sacrificing the numerical stability and model effectiveness.&lt;/p&gt; &#xA;&lt;p&gt;In FlagScale, we firstly provide our actually used training schemes for Aquila2-7B and Aquila2-34B, including the parallel strategies, optimizations and hyper-parameter settings. By using FlagScale, our model FLOPs utilization can achieve a very high level for both Aquila2-7B and Aquila2-34B. For now, FlagScale is still in its early stage and we will work with the community together to support different LLMs on various hardware architectures in the future.&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;We provide users with a series of fine-tuning scripts designed to adapt models to various downstream tasks using custom data. Within the comments section of the scripts, users will find detailed instructions indicating which parameters may need adjustments based on specific needs.&lt;/p&gt; &#xA;&lt;p&gt;Before initiating the fine-tuning process, you are required to have your training data prepared. All samples should be consolidated into a list and stored in a json file. Each sample should be represented as a dictionary, encompassing an ID and conversation, with the latter presented in list format. Below is an example for your reference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;id&#34;: &#34;alpaca_data.json_1&#34;,&#xA;&#x9;&#34;conversations&#34;: [{&#xA;&#x9;&#x9;&#34;from&#34;: &#34;human&#34;,&#xA;&#x9;&#x9;&#34;value&#34;: &#34;What are the three primary colors?&#34;&#xA;&#x9;}, {&#xA;&#x9;&#x9;&#34;from&#34;: &#34;gpt&#34;,&#xA;&#x9;&#x9;&#34;value&#34;: &#34;The three primary colors are red, blue, and yellow.&#34;&#xA;&#x9;}],&#xA;&#x9;&#34;instruction&#34;: &#34;&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Subsequently, you can use the variety of fine-tuning scripts we offer for different purposes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;finetune/7B/finetune.sh&lt;/code&gt; for a full parameter fine-tuning of the 7B model&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;finetune/7B/finetune_lora.sh&lt;/code&gt; for LoRA fine-tuning of the 7B model&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;finetune/7B/finetune_qlora.sh&lt;/code&gt; for Q-LoRA fine-tuning of the 7B model&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;finetune/34B/finetune.sh&lt;/code&gt; for a full parameter fine-tuning of the 34B model&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;finetune/34B/finetune_lora.sh&lt;/code&gt; for LoRA fine-tuning of the 34B model&lt;/li&gt; &#xA; &lt;li&gt;Execute &lt;code&gt;finetune/34B/finetune_qlora.sh&lt;/code&gt; for Q-LoRA fine-tuning of the 34B model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Optimization Effects&lt;/h3&gt; &#xA;&lt;p&gt;Below are the data on memory usage and training speed for the 7B and 34B models using full-parameter fine-tuning, LoRA, and QLoRA with different input lengths. The evaluation was conducted on a machine equipped with an A100-SXM4-80G GPU, utilizing CUDA 12.1 and Pytorch 2.1. The input length for the 7B model is 2048, and for the 34B model, it is 4096. All tests were performed using a batch size of 4 and a gradient accumulation of 1, and both memory usage (in GB) and training speed (in s/iter) were recorded. The specific data is as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Size&lt;/th&gt;&#xA;   &lt;th&gt;Method&lt;/th&gt;&#xA;   &lt;th&gt;Memory&lt;/th&gt;&#xA;   &lt;th&gt;speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;3&#34;&gt;7B&lt;/th&gt;&#xA;   &lt;td&gt;SFT&lt;/td&gt;&#xA;   &lt;td&gt;43.9G&lt;/td&gt;&#xA;   &lt;td&gt;2.67s/iter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA&lt;/td&gt;&#xA;   &lt;td&gt;29.4G&lt;/td&gt;&#xA;   &lt;td&gt;2.04s/iter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Q-LoRA&lt;/td&gt;&#xA;   &lt;td&gt;19.9G&lt;/td&gt;&#xA;   &lt;td&gt;2.14s/iter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;1&#34;&gt;34B&lt;/th&gt;&#xA;   &lt;td&gt;Q-LoRA&lt;/td&gt;&#xA;   &lt;td&gt;37.7G&lt;/td&gt;&#xA;   &lt;td&gt;8.22s/iter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;p&gt;Please click the link to visit the official &lt;a href=&#34;https://flagopen.baai.ac.cn&#34;&gt;FlagOpen&lt;/a&gt; website, click on &#34;Model Trial - Dialogue Model&#34; to fill out the application form. After approval, you can experience the dialogue capabilities of AquilaChat2 online.&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Long-Context Understanding&lt;/h2&gt; &#xA;&lt;p&gt;AquilaChat2-34B-16K is built on Aquila2-34B, processed by positional coding interpolation and SFT on 200k high-quality long text conversations dataset to extend the effective context window. We tested the model four Chinese and English long text quiz and summarization tasks from [LongBench](&lt;a href=&#34;https://github.com/THUDM/&#34;&gt;https://github.com/THUDM/&lt;/a&gt; LongBench). The evaluation results show that AquilaChat2-34B-16K reaches the leading level of open source long text models, close to GPT-3.5-16k.&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tokenizer&lt;/h2&gt; &#xA;&lt;p&gt;Our tokenizer of BBPE type is trained on a 50GB text dataset, mainly sampled from deduplicated Pile and WuDao corpus. We also add some special tokens for passage and conversation separation. &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;re welcome to submit your questions or share your user experience in &lt;a href=&#34;https://github.com/FlagAI-Open/Aquila2/issues&#34;&gt;GitHub Issues&lt;/a&gt; . &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;The Aquila2 project is based on the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache 2.0 license&lt;/a&gt;; The Aquila2 series models are based on the &lt;a href=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/aquila_license.pdf&#34;&gt;BAAI Aquila Model License Agreement&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested, please join our WeChat groups!&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/FlagAI-Open/Aquila2/main/assets/wechat-qrcode.jpg&#34; width=&#34;200&#34; height=&#34;200&#34; align=&#34;center&#34;&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>python/typeshed</title>
    <updated>2023-10-15T01:36:31Z</updated>
    <id>tag:github.com,2023-10-15:/python/typeshed</id>
    <link href="https://github.com/python/typeshed" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Collection of library stubs for Python, with static types&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;typeshed&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/python/typeshed/actions/workflows/tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/python/typeshed/actions/workflows/tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/python/typing?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/python/typing.svg?sanitize=true&#34; alt=&#34;Chat at https://gitter.im/python/typing&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/python/typeshed/raw/main/CONTRIBUTING.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pull%20requests-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;Pull Requests Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;Typeshed contains external type annotations for the Python standard library and Python builtins, as well as third party packages as contributed by people external to those projects.&lt;/p&gt; &#xA;&lt;p&gt;This data can e.g. be used for static analysis, type checking or type inference.&lt;/p&gt; &#xA;&lt;p&gt;For information on how to use &lt;code&gt;typeshed&lt;/code&gt;, read below. Information for contributors can be found in &lt;a href=&#34;https://raw.githubusercontent.com/python/typeshed/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;. &lt;strong&gt;Please read it before submitting pull requests; do not report issues with annotations to the project the stubs are for, but instead report them here to typeshed.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Further documentation on stub files, typeshed, and Python&#39;s typing system in general, can also be found at &lt;a href=&#34;https://typing.readthedocs.io/en/latest/&#34;&gt;https://typing.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Typeshed fully supports Python versions 3.8 and up. Support for Python 3.7 is limited: see &lt;a href=&#34;https://github.com/python/typeshed/issues/10113&#34;&gt;https://github.com/python/typeshed/issues/10113&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Using&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re just using a type checker (&lt;a href=&#34;https://github.com/python/mypy/&#34;&gt;mypy&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/pyright&#34;&gt;pyright&lt;/a&gt;, &lt;a href=&#34;https://github.com/google/pytype/&#34;&gt;pytype&lt;/a&gt;, PyCharm, ...), as opposed to developing it, you don&#39;t need to interact with the typeshed repo at all: a copy of standard library part of typeshed is bundled with type checkers. And type stubs for third party packages and modules you are using can be installed from PyPI. For example, if you are using &lt;code&gt;six&lt;/code&gt; and &lt;code&gt;requests&lt;/code&gt;, you can install the type stubs using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install types-six types-requests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These PyPI packages follow &lt;a href=&#34;http://www.python.org/dev/peps/pep-0561/&#34;&gt;PEP 561&lt;/a&gt; and are automatically released (multiple times a day, when needed) by &lt;a href=&#34;https://github.com/typeshed-internal/stub_uploader&#34;&gt;typeshed internal machinery&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Type checkers should be able to use these stub packages when installed. For more details, see the documentation for your type checker.&lt;/p&gt; &#xA;&lt;h3&gt;Package versioning for third-party stubs&lt;/h3&gt; &#xA;&lt;p&gt;Version numbers of third-party stub packages consist of at least four parts. All parts of the stub version, except for the last part, correspond to the version of the runtime package being stubbed. For example, if the &lt;code&gt;types-foo&lt;/code&gt; package has version &lt;code&gt;1.2.0.7&lt;/code&gt;, this guarantees that the &lt;code&gt;types-foo&lt;/code&gt; package contains stubs targeted against &lt;code&gt;foo==1.2.*&lt;/code&gt; and tested against the latest version of &lt;code&gt;foo&lt;/code&gt; matching that specifier. In this example, the final element of the version (7) indicates that this is the eighth revision of the stubs for &lt;code&gt;foo==1.2.*&lt;/code&gt;. If an update to the stubs were pushed (but the stubs were still aiming to provide annotations for &lt;code&gt;foo==1.2.*&lt;/code&gt;), then the version of &lt;code&gt;types-foo&lt;/code&gt; would increment to &lt;code&gt;1.2.0.8&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;At typeshed, we try to keep breaking changes to a minimum. However, due to the nature of stubs, any version bump can introduce changes that might make your code fail to type check.&lt;/p&gt; &#xA;&lt;p&gt;There are several strategies available for specifying the version of a stubs package you&#39;re using, each with its own tradeoffs:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the same bounds that you use for the package being stubbed. For example, if you use &lt;code&gt;requests&amp;gt;=2.30.0,&amp;lt;2.32&lt;/code&gt;, you can use &lt;code&gt;types-requests&amp;gt;=2.30.0,&amp;lt;2.32&lt;/code&gt;. This ensures that the stubs are compatible with the package you are using, but it carries a small risk of breaking type checking due to changes in the stubs.&lt;/p&gt; &lt;p&gt;Another risk of this strategy is that stubs often lag behind the package being stubbed. You might want to force the package being stubbed to a certain minimum version because it fixes a critical bug, but if correspondingly updated stubs have not been released, your type checking results may not be fully accurate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pin the stubs to a known good version and update the pin from time to time (either manually, or using a tool such as dependabot or renovate).&lt;/p&gt; &lt;p&gt;For example, if you use &lt;code&gt;types-requests==2.31.0.1&lt;/code&gt;, you can have confidence that upgrading dependencies will not break type checking. However, you will miss out on improvements in the stubs that could potentially improve type checking until you update the pin. This strategy also has the risk that the stubs you are using might become incompatible with the package being stubbed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Don&#39;t pin the stubs. This is the option that demands the least work from you when it comes to updating version pins, and has the advantage that you will automatically benefit from improved stubs whenever a new version of the stubs package is released. However, it carries the risk that the stubs become incompatible with the package being stubbed.&lt;/p&gt; &lt;p&gt;For example, if a new major version of the package is released, there&#39;s a chance the stubs might be updated to reflect the new version of the runtime package before you update the package being stubbed.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You can also switch between the different strategies as needed. For example, you could default to strategy (1), but fall back to strategy (2) when a problem arises that can&#39;t easily be fixed.&lt;/p&gt; &#xA;&lt;h3&gt;The &lt;code&gt;_typeshed&lt;/code&gt; package&lt;/h3&gt; &#xA;&lt;p&gt;typeshed includes a package &lt;code&gt;_typeshed&lt;/code&gt; as part of the standard library. This package and its submodules contains utility types, but is not available at runtime. For more information about how to use this package, &lt;a href=&#34;https://github.com/python/typeshed/tree/main/stdlib/_typeshed&#34;&gt;see the &lt;code&gt;stdlib/_typeshed&lt;/code&gt; directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;ve run into behavior in the type checker that suggests the type stubs for a given library are incorrect or incomplete, we want to hear from you!&lt;/p&gt; &#xA;&lt;p&gt;Our main forum for discussion is the project&#39;s &lt;a href=&#34;https://github.com/python/typeshed/issues&#34;&gt;GitHub issue tracker&lt;/a&gt;. This is the right place to start a discussion of any of the above or most any other topic concerning the project.&lt;/p&gt; &#xA;&lt;p&gt;If you have general questions about typing with Python, or you need a review of your type annotations or stubs outside of typeshed, head over to &lt;a href=&#34;https://github.com/python/typing/discussions&#34;&gt;our discussion forum&lt;/a&gt;. For less formal discussion, try the typing chat room on &lt;a href=&#34;https://gitter.im/python/typing&#34;&gt;gitter.im&lt;/a&gt;. Some typeshed maintainers are almost always present; feel free to find us there and we&#39;re happy to chat. Substantive technical discussion will be directed to the issue tracker.&lt;/p&gt;</summary>
  </entry>
</feed>