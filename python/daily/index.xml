<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-19T01:33:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JonathonLuiten/Dynamic3DGaussians</title>
    <updated>2023-10-19T01:33:47Z</updated>
    <id>tag:github.com,2023-10-19:/JonathonLuiten/Dynamic3DGaussians</id>
    <link href="https://github.com/JonathonLuiten/Dynamic3DGaussians" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://dynamic3dgaussians.github.io/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2308.09713.pdf&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2308.09713&#34;&gt;ArXiv&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/JonathonLuiten/status/1692346451668636100&#34;&gt;Tweet Thread&lt;/a&gt; | &lt;a href=&#34;https://omnomnom.vision.rwth-aachen.de/data/Dynamic3DGaussians/data.zip&#34;&gt;Data&lt;/a&gt; | &lt;a href=&#34;https://omnomnom.vision.rwth-aachen.de/data/Dynamic3DGaussians/output.zip&#34;&gt;Pretrained Models&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Official implementation of our approach for modelling the dynamic 3D world as a set of 3D Gaussians that move &amp;amp; rotate over time. This extends Gaussian Splatting to dynamic scenes, with accurate novel-view synthesis and dense 3D 6-DOF tracking.&lt;br&gt;&lt;br&gt; &lt;a href=&#34;https://dynamic3dgaussians.github.io/&#34;&gt;Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.vision.rwth-aachen.de/person/216/&#34;&gt;Jonathon Luiten&lt;/a&gt; &lt;sup&gt;1, 2&lt;/sup&gt;, &lt;a href=&#34;https://grgkopanas.github.io/&#34;&gt;Georgios Kopanas&lt;/a&gt; &lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www.vision.rwth-aachen.de/person/1/&#34;&gt;Bastian Leibe&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~deva/&#34;&gt;Deva Ramanan&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt; Carnegie Mellon University, &lt;sup&gt;2&lt;/sup&gt; RWTH Aachen University, &lt;sup&gt;3&lt;/sup&gt; Inria &amp;amp; Universite Cote d’Azur, France &lt;br&gt; International Conference on 3D Vision (3DV), 2024 &lt;br&gt; &lt;a href=&#34;mailto:jonoluiten@gmail.com&#34;&gt;jonoluiten@gmail.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p float=&#34;middle&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/JonathonLuiten/Dynamic3DGaussians/main/teaser_figure.png&#34; width=&#34;99%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install this repo (pytorch)&#xA;git clone git@github.com:JonathonLuiten/Dynamic3DGaussians.git&#xA;conda env create --file environment.yml&#xA;conda activate dynamic_gaussians&#xA;&#xA;# Install rendering code (cuda)&#xA;git clone git@github.com:JonathonLuiten/diff-gaussian-rasterization-w-depth.git&#xA;cd diff-gaussian-rasterization-w-depth&#xA;python setup.py install&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run visualizer on pretrained models&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Dynamic3DGaussians&#xA;wget https://omnomnom.vision.rwth-aachen.de/data/Dynamic3DGaussians/output.zip  # Download pretrained models&#xA;unzip output.zip&#xA;python visualize.py  # See code for visualization options&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train models yourself&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Dynamic3DGaussians&#xA;wget https://omnomnom.vision.rwth-aachen.de/data/Dynamic3DGaussians/data.zip  # Download training data&#xA;unzip data.zip&#xA;python train.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code Structure&lt;/h2&gt; &#xA;&lt;p&gt;I tried really hard to make this code really clean and useful for building upon. In my opinion it is now much nicer than the original code it was built upon. Everything is relatively &#39;functional&#39; and I tried to remove redundant classes and modules wherever possible. Almost all of the code is in &lt;a href=&#34;https://raw.githubusercontent.com/JonathonLuiten/Dynamic3DGaussians/main/train.py&#34;&gt;train.py&lt;/a&gt; in a few core functions, with the overall training loop clearly laid out. There are only a few other helper functions used, divided between &lt;a href=&#34;https://raw.githubusercontent.com/JonathonLuiten/Dynamic3DGaussians/main/helpers.py&#34;&gt;helpers.py&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/JonathonLuiten/Dynamic3DGaussians/main/external.py&#34;&gt;external.py&lt;/a&gt; (depending on license). I have split all useful variables into two dicts: &#39;params&#39; (those updated with gradient descent), and &#39;variables&#39; (those not updated by gradient descent). There is also a custom visualization codebase build using Open3D (used for the cool visuals on the website) that is entirely in &lt;a href=&#34;https://raw.githubusercontent.com/JonathonLuiten/Dynamic3DGaussians/main/visualize.py&#34;&gt;visualize.py&lt;/a&gt;. Please let me know if there is anyway you think the code could be cleaner.&lt;/p&gt; &#xA;&lt;h2&gt;Previous bugs&lt;/h2&gt; &#xA;&lt;p&gt;Before a recent &lt;a href=&#34;https://github.com/JonathonLuiten/Dynamic3DGaussians/commit/a246ec6065a86b8c3f1c83f38c66df8954ffc4bf&#34;&gt;commit&lt;/a&gt; there was a bug in this code. This has now been fixed and the code now seems like it is working bug free. On Oct 17 I also updated the pretrained model link to the most recent working code version. Please make sure to pull code to latest version AND redownload the pretrained models.&lt;/p&gt; &#xA;&lt;h2&gt;Differences to paper&lt;/h2&gt; &#xA;&lt;p&gt;This codebase contains some significant changes from the results presented in the currently public version of the paper. Both this codebase and the corresponding &lt;a href=&#34;https://arxiv.org/pdf/2308.09713.pdf&#34;&gt;paper&lt;/a&gt; are work-in-progress and likely to change in the near future. Until I find time to update the paper (ETA Dec 15th) the code here is the more up-to-date public facing version of these two.&lt;/p&gt; &#xA;&lt;p&gt;Differences:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In the paper we &#39;hard fixed&#39; the colour to be perfectly consistent over time for each Gaussian by simply not updating it after the first timestep. In this codebase the colour is only &#39;soft fixed&#39;. e.g. it is updated over time, but there is a regularization loss added which &#39;soft enforces&#39; it to be consistent over time.&lt;/li&gt; &#xA; &lt;li&gt;Because we know the ground-plane of the scenes we are using a &#39;floor loss&#39; to enforce Gassians don&#39;t go below the floor.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please let me know if there are any other differences between the paper and the code so that I can include them here and remember to include them in future version of the paper.&lt;/p&gt; &#xA;&lt;h2&gt;Partial code release&lt;/h2&gt; &#xA;&lt;p&gt;So far we have released two parts of the code: training and visualization. There are three further parts to be released in the future when I find time to clean them up (ETA Dec 15):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Evaluation code for evaluating the method for both novel-view-synthesis and tracking.&lt;/li&gt; &#xA; &lt;li&gt;Data preparation code, to create the cleaned dataset (which I have provided), from the raw CMU panoptic capture data.&lt;/li&gt; &#xA; &lt;li&gt;Code for creative editing of scenes (scene composition, etc).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Calls for contributions: Let&#39;s make this code better together!&lt;/h2&gt; &#xA;&lt;p&gt;Happy to work together to make this code better. If you want to contrib either open and issue / pull request, or send me an email.&lt;/p&gt; &#xA;&lt;h3&gt;Speeding up the code&lt;/h3&gt; &#xA;&lt;p&gt;I do a number of dumb things which slows the code down ALOT. If someone is motivated improving these could significantly speed up training time.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To do the &#39;fg/bg&#39; segmentation loss I am doing the full rendering process twice. By diving into the cuda a little, changing this could easily make training ~2x faster. However, note that for full reproducibility maybe this should only be done after the first timestep, as in the first timestep the gradients from just the colour rendering are used for densification.&lt;/li&gt; &#xA; &lt;li&gt;The current implementation of the local-rigidity loss in pytorch slows the code down MUCH more than it should. Currently it adds something like 20ms per training iter (e.g. 50 iter/s with, 100 without) however I have a compiled jax version that is much faster only adding ~1ms which wouldn&#39;t really slow down training at all). Not sure how to speed this up in pure pytorch but there might be away. Other ideas include upgrading to pytorch 2.0 and using compile, writing this part in cuda, or somehow using jax here (or everywhere).&lt;/li&gt; &#xA; &lt;li&gt;Lots of other parts of this pytorch code are not super efficient. Lots of room to make speedups.&lt;/li&gt; &#xA; &lt;li&gt;Potentially the whole code could be ported to pure cuda. E.g. see &lt;a href=&#34;https://github.com/MrNeRF/gaussian-splatting-cuda&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Visualization&lt;/h3&gt; &#xA;&lt;p&gt;In this codebase we provide an open3D based dynamic visualizer. This is makes adding 3D effects (like the track trajectories) really easy, although it definitely makes visualization slower than it should be. E.g. the code renders the scene at 800 FPS, but including open3D in order to display it on the scene (and add camera controls etc) slows it down to ~30 FPS.&lt;/p&gt; &#xA;&lt;p&gt;I have seen lots of cool renderers for Gaussians for static scenes. It would be cool to make my dynamic scenes work on these.&lt;/p&gt; &#xA;&lt;p&gt;In particular, I have seen various things that (a) somehow run on my phone and old laptop (e.g. &lt;a href=&#34;https://gsplat.tech/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/spaces/cakewalk/splat&#34;&gt;here&lt;/a&gt;) (b) run on VR headsets (e.g. &lt;a href=&#34;https://twitter.com/charshenton/status/1704358063036375548&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/s1ddok/status/1696249177250931086&#34;&gt;here&lt;/a&gt;) (c) run in commonly used tools like unity (e.g. &lt;a href=&#34;https://github.com/aras-p/UnityGaussianSplatting&#34;&gt;here&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Dylan made a helpful list that can be found &lt;a href=&#34;https://huggingface.co/spaces/dylanebert/list-of-splats&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Better (or no) FG / BG segmentations&lt;/h3&gt; &#xA;&lt;p&gt;The current FG/BG segmentations I use are REALLY bad. I made them very quickly by using simple background subtraction using a background image (image with no objects) for each camera with some smoothing. The badness of these segmentation masks causes a noticable degradation of the results. Especially around the feet of people / near the floor. It should be very easy to get much better segmentation masks (e.g. using pretrained networks), but I think it also probably isn&#39;t too hard to get the method to work without segmentation masks as all.&lt;/p&gt; &#xA;&lt;h2&gt;Further research&lt;/h2&gt; &#xA;&lt;p&gt;There are ALOT of cool things still to be done building upon Dynamic 3D Gaussians. If you&#39;re doing so (especially research projects) feel free to reach out if you want to discuss (email / issue / twitter)&lt;/p&gt; &#xA;&lt;h2&gt;Notes on license&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository (except in external.py) is licensed under the MIT licence.&lt;/p&gt; &#xA;&lt;p&gt;However, for this code to run it uses the cuda rasterizer code from &lt;a href=&#34;https://github.com/JonathonLuiten/diff-gaussian-rasterization-w-depth&#34;&gt;here&lt;/a&gt;, as well as various code in &lt;a href=&#34;https://raw.githubusercontent.com/JonathonLuiten/Dynamic3DGaussians/main/external.py&#34;&gt;external.py&lt;/a&gt; which has been taken or adapted from &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting&#34;&gt;here&lt;/a&gt;. These are required for this project, and for these a much more restrictive license from Inria applies which can be found &lt;a href=&#34;https://github.com/graphdeco-inria/gaussian-splatting/raw/main/LICENSE.md&#34;&gt;here&lt;/a&gt;. This requires express permission (licensing agreements) from Inria for use in any commercial application, but is otherwise freely distributed for research and experimentation.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{luiten2023dynamic,&#xA;  title={Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis},&#xA;  author={Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva},&#xA;  booktitle={3DV},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>RealKai42/liu-yao-divining</title>
    <updated>2023-10-19T01:33:47Z</updated>
    <id>tag:github.com,2023-10-19:/RealKai42/liu-yao-divining</id>
    <link href="https://github.com/RealKai42/liu-yao-divining" rel="alternate"></link>
    <summary type="html">&lt;p&gt;六爻游戏 + GPT 解读 / liu yao divining + interpreted by GPT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;六爻游戏&lt;/h1&gt; &#xA;&lt;h1&gt;demo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RealKai42/liu-yao-divining/master/doc/demo1.png&#34; alt=&#34;demo1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/RealKai42/liu-yao-divining/master/doc/demo2.png&#34; alt=&#34;demo2&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zhiwehu/100_plus_Python_Projects_Challenge</title>
    <updated>2023-10-19T01:33:47Z</updated>
    <id>tag:github.com,2023-10-19:/zhiwehu/100_plus_Python_Projects_Challenge</id>
    <link href="https://github.com/zhiwehu/100_plus_Python_Projects_Challenge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;100+ Python Projects Challenge&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;100+ Python Projects Challenge&lt;/h1&gt; &#xA;&lt;h2&gt;001 1 minute math&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/001_1_minute_math.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/001_1_minute_math.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;002 Chinese English Translation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/002_English_Chinese_Translation.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/002_English_Chinese_Translation.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;003 Add text watermark on pictures&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/003_Add_text_watermark_on_pictures.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/003_Add_text_watermark_on_pictures.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;004 Check your English speech&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/004_Check_your_English_speech.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/004_Check_your_English_speech.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;005 Check your English speech using baidu ai cloud ASR service&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/005_Check_your_English_speech_baidu.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/005_Check_your_English_speech_baidu.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;006 Arithmetic Formatter&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/006_Arithmetic_Formatter.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/006_Arithmetic_Formatter.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;007 OpenCV Capture Camera Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/007_OpenCV_Capture_Camera_Video.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/007_OpenCV_Capture_Camera_Video.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;008 OpenCV MediaPipe Hands Detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/008_OpenCV_MediaPipe_hand_detection.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/008_OpenCV_MediaPipe_hand_detection.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;009 OpenCV MediaPipe Hands Fingers Count&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/009_OpenCV_MediaPipe_hands_fingers_count.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/009_OpenCV_MediaPipe_hands_fingers_count.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;010 Gobang game&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/010_gobang.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/010_gobang.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;011 Snake game&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/011_snake_game.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/011_snake_game.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;012 Minesweeper game&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/012_Minesweeper.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/012_Minesweeper.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;013 Pose game&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/013_Posegame.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/013_Posegame.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;014 Tetris game&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/014_Tetris.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/014_Tetris.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;015 2048 game&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/raw/main/015_2048.md&#34;&gt;https://github.com/zhiwehu/100_plus_Python_Projects_Challenge/blob/main/015_2048.md&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>