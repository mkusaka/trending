<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-22T01:35:13Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>airtai/faststream</title>
    <updated>2024-06-22T01:35:13Z</updated>
    <id>tag:github.com,2024-06-22:/airtai/faststream</id>
    <link href="https://github.com/airtai/faststream" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FastStream is a powerful and easy-to-use Python framework for building asynchronous services interacting with event streams such as Apache Kafka, RabbitMQ, NATS and Redis.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FastStream&lt;/h1&gt; &#xA;&lt;p&gt;&lt;b&gt;Effortless event stream integration for your services&lt;/b&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/airtai/faststream/actions/workflows/pr_tests.yaml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/airtai/faststream/actions/workflows/pr_tests.yaml/badge.svg?branch=main&#34; alt=&#34;Test Passing&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://coverage-badge.samuelcolvin.workers.dev/redirect/airtai/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://coverage-badge.samuelcolvin.workers.dev/airtai/faststream.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.pepy.tech/projects/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/personalized-badge/faststream?period=month&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=green&amp;amp;left_text=downloads/month&#34; alt=&#34;Downloads&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/faststream?label=PyPI&#34; alt=&#34;Package version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/faststream&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/faststream.svg?sanitize=true&#34; alt=&#34;Supported Python versions&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/airtai/faststream/actions/workflows/pr_codeql.yml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/airtai/faststream/actions/workflows/pr_codeql.yml/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/airtai/faststream/actions/workflows/pr_dependency-review.yaml&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/airtai/faststream/actions/workflows/pr_dependency-review.yaml/badge.svg?sanitize=true&#34; alt=&#34;Dependency Review&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/airtai/faststream/raw/main/LICENSE&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/airtai/faststream.png&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/airtai/faststream/raw/main/CODE_OF_CONDUCT.md&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg?sanitize=true&#34; alt=&#34;Code of Conduct&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/qFm6aSqq59&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1085457301214855171?logo=discord&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://faststream.airt.ai/latest/&#34;&gt;&lt;strong&gt;FastStream&lt;/strong&gt;&lt;/a&gt; simplifies the process of writing producers and consumers for message queues, handling all the parsing, networking and documentation generation automatically.&lt;/p&gt; &#xA;&lt;p&gt;Making streaming microservices has never been easier. Designed with junior developers in mind, &lt;strong&gt;FastStream&lt;/strong&gt; simplifies your work while keeping the door open for more advanced use cases. Here&#39;s a look at the core features that make &lt;strong&gt;FastStream&lt;/strong&gt; a go-to framework for modern, data-centric microservices.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple Brokers&lt;/strong&gt;: &lt;strong&gt;FastStream&lt;/strong&gt; provides a unified API to work across multiple message brokers (&lt;a href=&#34;https://kafka.apache.org/&#34;&gt;&lt;strong&gt;Kafka&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.rabbitmq.com/&#34;&gt;&lt;strong&gt;RabbitMQ&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://nats.io/&#34;&gt;&lt;strong&gt;NATS&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://redis.io/&#34;&gt;&lt;strong&gt;Redis&lt;/strong&gt;&lt;/a&gt; support)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#writing-app-code&#34;&gt;&lt;strong&gt;Pydantic Validation&lt;/strong&gt;&lt;/a&gt;: Leverage &lt;a href=&#34;https://docs.pydantic.dev/&#34;&gt;&lt;strong&gt;Pydantic&#39;s&lt;/strong&gt;&lt;/a&gt; validation capabilities to serialize and validate incoming messages&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#project-documentation&#34;&gt;&lt;strong&gt;Automatic Docs&lt;/strong&gt;&lt;/a&gt;: Stay ahead with automatic &lt;a href=&#34;https://www.asyncapi.com/&#34;&gt;&lt;strong&gt;AsyncAPI&lt;/strong&gt;&lt;/a&gt; documentation&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: Full-typed editor support makes your development experience smooth, catching errors before they reach runtime&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#dependencies&#34;&gt;&lt;strong&gt;Powerful Dependency Injection System&lt;/strong&gt;&lt;/a&gt;: Manage your service dependencies efficiently with &lt;strong&gt;FastStream&lt;/strong&gt;&#39;s built-in DI system&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#testing-the-service&#34;&gt;&lt;strong&gt;Testable&lt;/strong&gt;&lt;/a&gt;: Supports in-memory tests, making your CI/CD pipeline faster and more reliable&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Use extensions for lifespans, custom serialization and middleware&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#any-framework&#34;&gt;&lt;strong&gt;Integrations&lt;/strong&gt;&lt;/a&gt;: &lt;strong&gt;FastStream&lt;/strong&gt; is fully compatible with any HTTP framework you want (&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#fastapi-plugin&#34;&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;&lt;/a&gt; especially)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/airtai/faststream/main/#code-generator&#34;&gt;&lt;strong&gt;Built for Automatic Code Generation&lt;/strong&gt;&lt;/a&gt;: &lt;strong&gt;FastStream&lt;/strong&gt; is optimized for automatic code generation using advanced models like GPT and Llama&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;That&#39;s &lt;strong&gt;FastStream&lt;/strong&gt; in a nutshell—easy, efficient, and powerful. Whether you&#39;re just starting with streaming microservices or looking to scale, &lt;strong&gt;FastStream&lt;/strong&gt; has got you covered.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&#34;https://faststream.airt.ai/latest/&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://faststream.airt.ai/latest/&#34;&gt;https://faststream.airt.ai/latest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; is a new package based on the ideas and experiences gained from &lt;a href=&#34;https://github.com/airtai/fastkafka&#34;&gt;&lt;strong&gt;FastKafka&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/lancetnik/propan&#34;&gt;&lt;strong&gt;Propan&lt;/strong&gt;&lt;/a&gt;. By joining our forces, we picked up the best from both packages and created a unified way to write services capable of processing streamed data regardless of the underlying protocol. We&#39;ll continue to maintain both packages, but new development will be in this project. If you are starting a new service, this package is the recommended way to do it.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; works on &lt;strong&gt;Linux&lt;/strong&gt;, &lt;strong&gt;macOS&lt;/strong&gt;, &lt;strong&gt;Windows&lt;/strong&gt; and most &lt;strong&gt;Unix&lt;/strong&gt;-style operating systems. You can install it with &lt;code&gt;pip&lt;/code&gt; as usual:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install faststream[kafka]&#xA;# or&#xA;pip install faststream[rabbit]&#xA;# or&#xA;pip install faststream[nats]&#xA;# or&#xA;pip install faststream[redis]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default &lt;strong&gt;FastStream&lt;/strong&gt; uses &lt;strong&gt;PydanticV2&lt;/strong&gt; written in &lt;strong&gt;Rust&lt;/strong&gt;, but you can downgrade it manually, if your platform has no &lt;strong&gt;Rust&lt;/strong&gt; support - &lt;strong&gt;FastStream&lt;/strong&gt; will work correctly with &lt;strong&gt;PydanticV1&lt;/strong&gt; as well.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Writing app code&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; brokers provide convenient function decorators &lt;code&gt;@broker.subscriber&lt;/code&gt; and &lt;code&gt;@broker.publisher&lt;/code&gt; to allow you to delegate the actual process of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;consuming and producing data to Event queues, and&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;decoding and encoding JSON-encoded messages&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These decorators make it easy to specify the processing logic for your consumers and producers, allowing you to focus on the core business logic of your application without worrying about the underlying integration.&lt;/p&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;FastStream&lt;/strong&gt; uses &lt;a href=&#34;https://docs.pydantic.dev/&#34;&gt;&lt;strong&gt;Pydantic&lt;/strong&gt;&lt;/a&gt; to parse input JSON-encoded data into Python objects, making it easy to work with structured data in your applications, so you can serialize your input messages just using type annotations.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example Python app using &lt;strong&gt;FastStream&lt;/strong&gt; that consumes data from an incoming data stream and outputs the data to another one:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faststream import FastStream&#xA;from faststream.kafka import KafkaBroker&#xA;# from faststream.rabbit import RabbitBroker&#xA;# from faststream.nats import NatsBroker&#xA;# from faststream.redis import RedisBroker&#xA;&#xA;broker = KafkaBroker(&#34;localhost:9092&#34;)&#xA;# broker = RabbitBroker(&#34;amqp://guest:guest@localhost:5672/&#34;)&#xA;# broker = NatsBroker(&#34;nats://localhost:4222/&#34;)&#xA;# broker = RedisBroker(&#34;redis://localhost:6379/&#34;)&#xA;&#xA;app = FastStream(broker)&#xA;&#xA;@broker.subscriber(&#34;in&#34;)&#xA;@broker.publisher(&#34;out&#34;)&#xA;async def handle_msg(user: str, user_id: int) -&amp;gt; str:&#xA;    return f&#34;User: {user_id} - {user} registered&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;Pydantic&lt;/strong&gt;’s &lt;a href=&#34;https://docs.pydantic.dev/usage/models/&#34;&gt;&lt;code&gt;BaseModel&lt;/code&gt;&lt;/a&gt; class allows you to define messages using a declarative syntax, making it easy to specify the fields and types of your messages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pydantic import BaseModel, Field, PositiveInt&#xA;from faststream import FastStream&#xA;from faststream.kafka import KafkaBroker&#xA;&#xA;broker = KafkaBroker(&#34;localhost:9092&#34;)&#xA;app = FastStream(broker)&#xA;&#xA;class User(BaseModel):&#xA;    user: str = Field(..., examples=[&#34;John&#34;])&#xA;    user_id: PositiveInt = Field(..., examples=[&#34;1&#34;])&#xA;&#xA;@broker.subscriber(&#34;in&#34;)&#xA;@broker.publisher(&#34;out&#34;)&#xA;async def handle_msg(data: User) -&amp;gt; str:&#xA;    return f&#34;User: {data.user} - {data.user_id} registered&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Testing the service&lt;/h2&gt; &#xA;&lt;p&gt;The service can be tested using the &lt;code&gt;TestBroker&lt;/code&gt; context managers, which, by default, puts the Broker into &#34;testing mode&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The Tester will redirect your &lt;code&gt;subscriber&lt;/code&gt; and &lt;code&gt;publisher&lt;/code&gt; decorated functions to the InMemory brokers, allowing you to quickly test your app without the need for a running broker and all its dependencies.&lt;/p&gt; &#xA;&lt;p&gt;Using pytest, the test for our service would look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Code above omitted 👆&#xA;&#xA;import pytest&#xA;import pydantic&#xA;from faststream.kafka import TestKafkaBroker&#xA;&#xA;&#xA;@pytest.mark.asyncio&#xA;async def test_correct():&#xA;    async with TestKafkaBroker(broker) as br:&#xA;        await br.publish({&#xA;            &#34;user&#34;: &#34;John&#34;,&#xA;            &#34;user_id&#34;: 1,&#xA;        }, &#34;in&#34;)&#xA;&#xA;@pytest.mark.asyncio&#xA;async def test_invalid():&#xA;    async with TestKafkaBroker(broker) as br:&#xA;        with pytest.raises(pydantic.ValidationError):&#xA;            await br.publish(&#34;wrong message&#34;, &#34;in&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the application&lt;/h2&gt; &#xA;&lt;p&gt;The application can be started using built-in &lt;strong&gt;FastStream&lt;/strong&gt; CLI command.&lt;/p&gt; &#xA;&lt;p&gt;To run the service, use the &lt;strong&gt;FastStream CLI&lt;/strong&gt; command and pass the module (in this case, the file where the app implementation is located) and the app symbol to the command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;faststream run basic:app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the command, you should see the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;INFO     - FastStream app starting...&#xA;INFO     - input_data |            - `HandleMsg` waiting for messages&#xA;INFO     - FastStream app started successfully! To exit press CTRL+C&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;FastStream&lt;/strong&gt; provides you with a great hot reload feature to improve your Development Experience&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;faststream run basic:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And multiprocessing horizontal scaling feature as well:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;faststream run basic:app --workers 3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can learn more about &lt;strong&gt;CLI&lt;/strong&gt; features &lt;a href=&#34;https://faststream.airt.ai/latest/getting-started/cli/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Project Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; automatically generates documentation for your project according to the &lt;a href=&#34;https://www.asyncapi.com/&#34;&gt;&lt;strong&gt;AsyncAPI&lt;/strong&gt;&lt;/a&gt; specification. You can work with both generated artifacts and place a web view of your documentation on resources available to related teams.&lt;/p&gt; &#xA;&lt;p&gt;The availability of such documentation significantly simplifies the integration of services: you can immediately see what channels and message formats the application works with. And most importantly, it won&#39;t cost anything - &lt;strong&gt;FastStream&lt;/strong&gt; has already created the docs for you!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/airtai/faststream/raw/main/docs/docs/assets/img/AsyncAPI-basic-html-short.png?raw=true&#34; alt=&#34;HTML-page&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastStream&lt;/strong&gt; (thanks to &lt;a href=&#34;https://lancetnik.github.io/FastDepends/&#34;&gt;&lt;strong&gt;FastDepends&lt;/strong&gt;&lt;/a&gt;) has a dependency management system similar to &lt;code&gt;pytest fixtures&lt;/code&gt; and &lt;code&gt;FastAPI Depends&lt;/code&gt; at the same time. Function arguments declare which dependencies you want are needed, and a special decorator delivers them from the global Context object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from faststream import Depends, Logger&#xA;&#xA;async def base_dep(user_id: int) -&amp;gt; bool:&#xA;    return True&#xA;&#xA;@broker.subscriber(&#34;in-test&#34;)&#xA;async def base_handler(user: str,&#xA;                       logger: Logger,&#xA;                       dep: bool = Depends(base_dep)):&#xA;    assert dep is True&#xA;    logger.info(user)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;HTTP Frameworks integrations&lt;/h2&gt; &#xA;&lt;h3&gt;Any Framework&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;strong&gt;FastStream&lt;/strong&gt; &lt;code&gt;MQBrokers&lt;/code&gt; without a &lt;code&gt;FastStream&lt;/code&gt; application. Just &lt;em&gt;start&lt;/em&gt; and &lt;em&gt;stop&lt;/em&gt; them according to your application&#39;s lifespan.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from aiohttp import web&#xA;&#xA;from faststream.kafka import KafkaBroker&#xA;&#xA;broker = KafkaBroker(&#34;localhost:9092&#34;)&#xA;&#xA;@broker.subscriber(&#34;test&#34;)&#xA;async def base_handler(body):&#xA;    print(body)&#xA;&#xA;async def start_broker(app):&#xA;    await broker.start()&#xA;&#xA;async def stop_broker(app):&#xA;    await broker.close()&#xA;&#xA;async def hello(request):&#xA;    return web.Response(text=&#34;Hello, world&#34;)&#xA;&#xA;app = web.Application()&#xA;app.add_routes([web.get(&#34;/&#34;, hello)])&#xA;app.on_startup.append(start_broker)&#xA;app.on_cleanup.append(stop_broker)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    web.run_app(app)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;FastAPI&lt;/strong&gt; Plugin&lt;/h3&gt; &#xA;&lt;p&gt;Also, &lt;strong&gt;FastStream&lt;/strong&gt; can be used as part of &lt;strong&gt;FastAPI&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Just import a &lt;strong&gt;StreamRouter&lt;/strong&gt; you need and declare the message handler with the same &lt;code&gt;@router.subscriber(...)&lt;/code&gt; and &lt;code&gt;@router.publisher(...)&lt;/code&gt; decorators.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastapi import FastAPI&#xA;from pydantic import BaseModel&#xA;&#xA;from faststream.kafka.fastapi import KafkaRouter&#xA;&#xA;router = KafkaRouter(&#34;localhost:9092&#34;)&#xA;&#xA;class Incoming(BaseModel):&#xA;    m: dict&#xA;&#xA;@router.subscriber(&#34;test&#34;)&#xA;@router.publisher(&#34;response&#34;)&#xA;async def hello(m: Incoming):&#xA;    return {&#34;response&#34;: &#34;Hello, world!&#34;}&#xA;&#xA;app = FastAPI(lifespan=router.lifespan_context)&#xA;app.include_router(router)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More integration features can be found &lt;a href=&#34;https://faststream.airt.ai/latest/getting-started/integrations/fastapi/&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Code generator&lt;/h2&gt; &#xA;&lt;p&gt;As evident, &lt;strong&gt;FastStream&lt;/strong&gt; is an incredibly user-friendly framework. However, we&#39;ve taken it a step further and made it even more user-friendly! Introducing &lt;a href=&#34;https://faststream-gen.airt.ai&#34;&gt;faststream-gen&lt;/a&gt;, a Python library that harnesses the power of generative AI to effortlessly generate &lt;strong&gt;FastStream&lt;/strong&gt; applications. Simply describe your application requirements, and &lt;a href=&#34;https://faststream-gen.airt.ai&#34;&gt;faststream-gen&lt;/a&gt; will generate a production-grade &lt;strong&gt;FastStream&lt;/strong&gt; project that is ready to deploy in no time.&lt;/p&gt; &#xA;&lt;p&gt;Save application description inside &lt;code&gt;description.txt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Create a FastStream application using localhost broker for testing and use the&#xA;default port number.&#xA;&#xA;It should consume messages from the &#39;input_data&#39; topic, where each message is a&#xA;JSON encoded object containing a single attribute: &#39;data&#39;.&#xA;&#xA;While consuming from the topic, increment the value of the data attribute by 1.&#xA;&#xA;Finally, send message to the &#39;output_data&#39; topic.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and run the following command to create a new &lt;strong&gt;FastStream&lt;/strong&gt; project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;faststream_gen -i description.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;✨  Generating a new FastStream application!&#xA; ✔ Application description validated.&#xA; ✔ FastStream app skeleton code generated. Takes around 15 to 45 seconds)...&#xA; ✔ The app and the tests are generated.  around 30 to 90 seconds)...&#xA; ✔ New FastStream project created.&#xA; ✔ Integration tests were successfully completed.&#xA; Tokens used: 10768&#xA; Total Cost (USD): $0.03284&#xA;✨  All files were successfully generated!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tutorial&lt;/h3&gt; &#xA;&lt;p&gt;We also invite you to explore our tutorial, where we will guide you through the process of utilizing the &lt;a href=&#34;https://faststream-gen.airt.ai&#34;&gt;faststream-gen&lt;/a&gt; Python library to effortlessly create &lt;strong&gt;FastStream&lt;/strong&gt; applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://faststream-gen.airt.ai/Tutorial/Cryptocurrency_Tutorial/&#34;&gt;Cryptocurrency analysis with FastStream&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Stay in touch&lt;/h2&gt; &#xA;&lt;p&gt;Please show your support and stay in touch by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;giving our &lt;a href=&#34;https://github.com/airtai/faststream/&#34;&gt;GitHub repository&lt;/a&gt; a star, and&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;joining our &lt;a href=&#34;https://discord.gg/qFm6aSqq59&#34;&gt;Discord server&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Your support helps us to stay in touch with you and encourages us to continue developing and improving the framework. Thank you for your support!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to all of these amazing people who made the project better!&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/airtai/faststream/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=airtai/faststream&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>DigitalPhonetics/IMS-Toucan</title>
    <updated>2024-06-22T01:35:13Z</updated>
    <id>tag:github.com,2024-06-22:/DigitalPhonetics/IMS-Toucan</id>
    <link href="https://github.com/DigitalPhonetics/IMS-Toucan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Multilingual and Controllable Text-to-Speech Toolkit of the Speech and Language Technologies Group at the University of Stuttgart.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/DigitalPhonetics/IMS-Toucan/MassiveScaleToucan/Utility/toucan.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;IMS Toucan is a toolkit for teaching, training and using state-of-the-art Speech Synthesis models, developed at the &lt;strong&gt;Institute for Natural Language Processing (IMS), University of Stuttgart, Germany&lt;/strong&gt;. Everything is pure Python and PyTorch based to keep it as simple and beginner-friendly, yet powerful as possible.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Links 🦚&lt;/h2&gt; &#xA;&lt;h3&gt;Pre-Generated Audios&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://poetictts.github.io/&#34;&gt;Human-in-the-loop edited poetry for German literary studies&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://toucanprosodycloningdemo.github.io&#34;&gt;Cloning prosody across speakers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://multilingualtoucan.github.io/&#34;&gt;Multi-lingual and multi-speaker audios&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://anondemos.github.io/MMDemo&#34;&gt;Massively-Multi-Lingual audios and study setup&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Interactive Demos&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Flux9665/MassivelyMultilingualTTS&#34;&gt;Check out our massively-multi-lingual demo on Huggingface🤗&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Flux9665/SpeechCloning&#34;&gt;Check out our demo on exact style cloning on Huggingface🤗&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Flux9665/PoeticTTS&#34;&gt;Check out our human-in-the-loop poetry reading demo on Huggingface🤗&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Flux9665/ThisSpeakerDoesNotExist&#34;&gt;You can also design the voice of a speaker who doesn&#39;t exist on Huggingface🤗&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/datasets/Flux9665/BibleMMS&#34;&gt;We have also published a massively multilingual TTS dataset on Huggingface🤗&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation 🦉&lt;/h2&gt; &#xA;&lt;p&gt;These instructions should work for most cases, but I heard of some instances where espeak behaves weird, which are sometimes resolved after a re-install and sometimes not. Also, M1 and M2 MacBooks require a very different installation process, with which I am unfortunately not familiar.&lt;/p&gt; &#xA;&lt;h4&gt;Basic Requirements&lt;/h4&gt; &#xA;&lt;p&gt;To install this toolkit, clone it onto the machine you want to use it on (should have at least one cuda enabled GPU if you intend to train models on that machine. For inference, you don&#39;t need a GPU). Navigate to the directory you have cloned. We recommend creating and activating a &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;virtual environment&lt;/a&gt; to install the basic requirements into. The commands below summarize everything you need to do under Linux. If you are running Windows, the second line needs to be changed, please have a look at the &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;venv documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m venv &amp;lt;path_to_where_you_want_your_env_to_be&amp;gt;&#xA;&#xA;source &amp;lt;path_to_where_you_want_your_env_to_be&amp;gt;/bin/activate&#xA;&#xA;pip install --no-cache-dir -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the second line everytime you start using the tool again to activate the virtual environment again, if you e.g. logged out in the meantime. To make use of a GPU, you don&#39;t need to do anything else on a Linux machine. On a Windows machine, have a look at &lt;a href=&#34;https://pytorch.org/&#34;&gt;the official PyTorch website&lt;/a&gt; for the install-command that enables GPU support.&lt;/p&gt; &#xA;&lt;h4&gt;Storage configuration&lt;/h4&gt; &#xA;&lt;p&gt;If you don&#39;t want the pretrained and trained models as well as the cache files resulting from preprocessing your datasets to be stored in the default subfolders, you can set corresponding directories globally by editing &lt;code&gt;Utility/storage_config.py&lt;/code&gt; to suit your needs (the path can be relative to the repository root directory or absolute).&lt;/p&gt; &#xA;&lt;h4&gt;Pretrained Models&lt;/h4&gt; &#xA;&lt;p&gt;You don&#39;t need to use pretrained models, but it can speed things up tremendously. Run the &lt;code&gt;run_model_downloader.py&lt;/code&gt; script to automatically download them from the release page and put them into their appropriate locations with appropriate names.&lt;/p&gt; &#xA;&lt;h4&gt;[optional] eSpeak-NG&lt;/h4&gt; &#xA;&lt;p&gt;eSpeak-NG is an optional requirement, that handles lots of special cases in many languages, so it&#39;s good to have.&lt;/p&gt; &#xA;&lt;p&gt;On most &lt;strong&gt;Linux&lt;/strong&gt; environments it will be installed already, and if it is not, and you have the sufficient rights, you can install it by simply running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apt-get install espeak-ng&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;strong&gt;Windows&lt;/strong&gt;, they provide a convenient .msi installer file &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng/releases&#34;&gt;on their GitHub release page&lt;/a&gt;. After installation on non-linux systems, you&#39;ll also need to tell the phonemizer library where to find your espeak installation by setting the &lt;code&gt;PHONEMIZER_ESPEAK_LIBRARY&lt;/code&gt; environment variable, which is discussed in &lt;a href=&#34;https://github.com/bootphon/phonemizer/issues/44#issuecomment-1008449718&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For &lt;strong&gt;Mac&lt;/strong&gt; it&#39;s unfortunately a lot more complicated. Thanks to Sang Hyun Park, here is a guide for installing it on Mac: For M1 Macs, the most convenient method to install espeak-ng onto your system is via a &lt;a href=&#34;https://ports.macports.org/port/espeak-ng/&#34;&gt;MacPorts port of espeak-ng&lt;/a&gt;. MacPorts itself can be installed from the &lt;a href=&#34;https://www.macports.org/install.php&#34;&gt;MacPorts website&lt;/a&gt;, which also requires Apple&#39;s &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;XCode&lt;/a&gt;. Once XCode and MacPorts have been installed, you can install the port of espeak-ng via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo port install espeak-n&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As stated in the Windows install instructions, the espeak-ng installation will need to be set as a variable for the phonemizer library. The environment variable is &lt;code&gt;PHONEMIZER_ESPEAK_LIBRARY&lt;/code&gt; as given in the &lt;a href=&#34;https://github.com/bootphon/phonemizer/issues/44#issuecomment-1008449718&#34;&gt;GitHub thread&lt;/a&gt; linked above. However, the espeak-ng installation file you need to set this variable to is a .dylib file rather than a .dll file on Mac. In order to locate the espeak-ng library file, you can run &lt;code&gt;port contents espeak-ng&lt;/code&gt;. The specific file you are looking for is named &lt;code&gt;libespeak-ng.dylib&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Inference 🦢&lt;/h2&gt; &#xA;&lt;p&gt;You can load your trained models, or the pretrained provided one, using the &lt;code&gt;InferenceInterfaces/ToucanTTSInterface.py&lt;/code&gt;. Simply create an object from it with the proper directory handle identifying the model you want to use. The rest should work out in the background. You might want to set a language embedding or a speaker embedding using the &lt;em&gt;set_language&lt;/em&gt; and &lt;em&gt;set_speaker_embedding&lt;/em&gt; functions. Most things should be self-explanatory.&lt;/p&gt; &#xA;&lt;p&gt;An &lt;em&gt;InferenceInterface&lt;/em&gt; contains two methods to create audio from text. They are &lt;em&gt;read_to_file&lt;/em&gt; and &lt;em&gt;read_aloud&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;read_to_file&lt;/em&gt; takes as input a list of strings and a filename. It will synthesize the sentences in the list and concatenate them with a short pause inbetween and write them to the filepath you supply as the other argument.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;read_aloud&lt;/em&gt; takes just a string, which it will then convert to speech and immediately play using the system&#39;s speakers. If you set the optional argument &lt;em&gt;view&lt;/em&gt; to &lt;em&gt;True&lt;/em&gt;, a visualization will pop up, that you need to close for the program to continue.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Their use is demonstrated in &lt;em&gt;run_interactive_demo.py&lt;/em&gt; and &lt;em&gt;run_text_to_file_reader.py&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are simple scaling parameters to control the duration, the variance of the pitch curve and the variance of the energy curve. You can either change them in the code when using the interactive demo or the reader, or you can simply pass them to the interface when you use it in your own code.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Creating a new Training Pipeline 🐣&lt;/h2&gt; &#xA;&lt;p&gt;In the directory called &lt;em&gt;Utility&lt;/em&gt; there is a file called &lt;code&gt;path_to_transcript_dicts.py&lt;/code&gt;. In this file you should write a function that returns a dictionary that has all the absolute paths to each of the audio files in your dataset as strings as the keys and the textual transcriptions of the corresponding audios as the values.&lt;/p&gt; &#xA;&lt;p&gt;Then go to the directory &lt;em&gt;TrainingInterfaces/TrainingPipelines&lt;/em&gt;. In there, make a copy of the &lt;code&gt;finetuning_example_simple.py&lt;/code&gt; file if you just want to finetune on a single dataset or &lt;code&gt;finetuning_example_multilingual.py&lt;/code&gt; if you want to finetune on multiple datasets, potentially even multiple languages. We will use this copy as reference and only make the necessary changes to use the new dataset. Find the call(s) to the &lt;em&gt;prepare_tts_corpus&lt;/em&gt; function. Replace the path_to_transcript_dict used there with the one(s) you just created. Then change the name of the corresponding cache directory to something that makes sense for the dataset. Also look out for the variable &lt;em&gt;save_dir&lt;/em&gt;, which is where the checkpoints will be saved to. This is a default value, you can overwrite it when calling the pipeline later using a command line argument, in case you want to fine-tune from a checkpoint and thus save into a different directory. Finally, change the &lt;em&gt;lang&lt;/em&gt; argument in the creation of the dataset and in the call to the train loop function to the ISO 639-3 language ID that matches your data.&lt;/p&gt; &#xA;&lt;p&gt;The arguments that are given to the train loop in the finetuning examples are meant for the case of finetuning from a pretrained model. If you want to train from scratch, have a look at a different pipeline that has ToucanTTS in its name and look at the arguments used there.&lt;/p&gt; &#xA;&lt;p&gt;Once this is complete, we are almost done, now we just need to make it available to the &lt;code&gt;run_training_pipeline.py&lt;/code&gt; file in the top level. In said file, import the &lt;em&gt;run&lt;/em&gt; function from the pipeline you just created and give it a meaningful name. Now in the &lt;em&gt;pipeline_dict&lt;/em&gt;, add your imported function as value and use as key a shorthand that makes sense.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Training a Model 🦜&lt;/h2&gt; &#xA;&lt;p&gt;Once you have a training pipeline built, training is super easy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_training_pipeline.py &amp;lt;shorthand of the pipeline&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can supply any of the following arguments, but don&#39;t have to (although for training you should definitely specify at least a GPU ID).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--gpu_id &amp;lt;ID of the GPU you wish to use, as displayed with nvidia-smi, default is cpu. If multiple GPUs are provided (comma separated), then distributed training will be used, but the script has to be started with torchrun.&amp;gt; &#xA;&#xA;--resume_checkpoint &amp;lt;path to a checkpoint to load&amp;gt;&#xA;&#xA;--resume (if this is present, the furthest checkpoint available will be loaded automatically)&#xA;&#xA;--finetune (if this is present, the provided checkpoint will be fine-tuned on the data from this pipeline)&#xA;&#xA;--model_save_dir &amp;lt;path to a directory where the checkpoints should be saved&amp;gt;&#xA;&#xA;--wandb (if this is present, the logs will be synchronized to your weights&amp;amp;biases account, if you are logged in on the command line)&#xA;&#xA;--wandb_resume_id &amp;lt;the id of the run you want to resume, if you are using weights&amp;amp;biases (you can find the id in the URL of the run)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For multi-GPU training, you have to supply multiple GPU ids (comma separated) and start the script with torchrun. You also have to specify the number of GPUs. This has to match the number of IDs that you supply. Careful: torchrun is incompatible with nohup! Use tmux instead to keep the script running after you log out of the shell.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --standalone --nproc_per_node=4 --nnodes=1 run_training_pipeline.py &amp;lt;shorthand of the pipeline&amp;gt; --gpu_id &#34;0,1,2,3&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After every epoch (or alternatively after certain step counts), some logs will be written to the console and to the Weights and Biases website, if you are logged in and set the flag. If you get cuda out of memory errors, you need to decrease the batchsize in the arguments of the call to the training_loop in the pipeline you are running. Try decreasing the batchsize in small steps until you get no more out of cuda memory errors.&lt;/p&gt; &#xA;&lt;p&gt;In the directory you specified for saving, checkpoint files and spectrogram visualization data will appear. Since the checkpoints are quite big, only the five most recent ones will be kept. The amount of training steps highly depends on the data you are using and whether you&#39;re finetuning from a pretrained checkpoint or training from scratch. The fewer data you have, the fewer steps you should take to prevent a possible collapse. If you want to stop earlier, just kill the process, since everything is daemonic all the child-processes should die with it. In case there are some ghost-processes left behind, you can use the following command to find them and kill them manually.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;fuser -v /dev/nvidia*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Whenever a checkpoint is saved, a compressed version that can be used for inference is also created, which is named &lt;em&gt;best.py&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;FAQ 🐓&lt;/h2&gt; &#xA;&lt;p&gt;Here are a few points that were brought up by users:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How can I figure out if my data has outliers or similar problems? -- There is a scorer that can find and even remove samples from your dataset cache that have extraordinarily high loss values, have a look at &lt;code&gt;run_scorer.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;My error message shows GPU0, even though I specified a different GPU -- The way GPU selection works is that the specified GPU is set as the only visible device, in order to avoid backend stuff running accidentally on different GPUs. So internally the program will name the device GPU0, because it is the only GPU it can see. It is actually running on the GPU you specified.&lt;/li&gt; &#xA; &lt;li&gt;read_to_file produces strange outputs -- Check if you&#39;re passing a list to the method or a string. Since strings can be iterated over, it might not throw an error, but a list of strings is expected.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;UserWarning: Detected call of lr_scheduler.step() before optimizer.step().&lt;/code&gt; -- We use a custom scheduler, and torch incorrectly thinks that we call the scheduler and the optimizer in the wrong order. Just ignore this warning, it is completely meaningless.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;WARNING[XFORMERS]: xFormers can&#39;t load C++/CUDA extensions. [...]&lt;/code&gt; -- Another meaningless warning. We actually don&#39;t use xFormers ourselves, it is just part of the dependencies of one of our dependencies, but it is not used at any place.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;The torchaudio backend is switched to &#39;soundfile&#39;. Note that &#39;sox_io&#39; is not supported on Windows. [...]&lt;/code&gt; -- Just happens under Windows and doesn&#39;t affect anything.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1) [...]&lt;/code&gt; -- We have no idea why espeak started giving out this warning, however it doesn&#39;t seem to affect anything, so it seems safe to ignore.&lt;/li&gt; &#xA; &lt;li&gt;Loss turns to &lt;code&gt;NaN&lt;/code&gt; -- The default learning rates work on clean data. If your data is less clean, try using the scorer to find problematic samples, or reduce the learning rate. The most common problem is there being pauses in the speech, but nothing that hints at them in the text. That&#39;s why ASR corpora, which leave out punctuation, are usually difficult to use for TTS.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Disclaimer 🦆&lt;/h2&gt; &#xA;&lt;p&gt;The basic PyTorch modules of FastSpeech 2 and GST are taken from &lt;a href=&#34;https://github.com/espnet/espnet&#34;&gt;ESPnet&lt;/a&gt;, the PyTorch modules of HiFi-GAN are taken from the &lt;a href=&#34;https://github.com/kan-bayashi/ParallelWaveGAN&#34;&gt;ParallelWaveGAN repository&lt;/a&gt;. Some modules related to the Glow based PostNet as outlined in PortaSpeech are taken from the &lt;a href=&#34;https://github.com/NATSpeech/NATSpeech&#34;&gt;official PortaSpeech codebase&lt;/a&gt;. We use audio watermarking from &lt;a href=&#34;https://github.com/facebookresearch/audioseal&#34;&gt;audioseal&lt;/a&gt;. For grapheme-to-phoneme conversion, we rely on the aforementioned eSpeak-NG as well as &lt;a href=&#34;https://github.com/xinjli/transphone&#34;&gt;transphone&lt;/a&gt;. We use &lt;a href=&#34;https://github.com/yangdongchao/AcademiCodec&#34;&gt;encodec, a neural audio codec&lt;/a&gt; as intermediate representation for caching the train data to save space.&lt;/p&gt; &#xA;&lt;h2&gt;Citation 🐧&lt;/h2&gt; &#xA;&lt;h3&gt;Introduction of the Toolkit &lt;a href=&#34;https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v1.0&#34;&gt;[associated code and models]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{lux2021toucan,&#xA;  year         = 2021,&#xA;  title        = {{The IMS Toucan system for the Blizzard Challenge 2021}},&#xA;  author       = {Florian Lux and Julia Koch and Antje Schweitzer and Ngoc Thang Vu},&#xA;  booktitle    = {Blizzard Challenge Workshop},&#xA;  publisher    = {ISCA Speech Synthesis SIG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding Articulatory Features and Meta-Learning Pretraining &lt;a href=&#34;https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v1.1&#34;&gt;[associated code and models]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{lux2022laml,&#xA;  year         = 2022,&#xA;  title        = {{Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features}},&#xA;  author       = {Florian Lux and Ngoc Thang Vu},&#xA;  booktitle    = {ACL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding Exact Prosody-Cloning Capabilities &lt;a href=&#34;https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v2.2&#34;&gt;[associated code and models]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{lux2022cloning,&#xA;  year         = 2022,&#xA;  title        = {{Exact Prosody Cloning in Zero-Shot Multispeaker Text-to-Speech}},&#xA;  author       = {Lux, Florian and Koch, Julia and Vu, Ngoc Thang},&#xA;  booktitle    = {SLT},&#xA;  publisher    = {IEEE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding Language Embeddings and Word Boundaries &lt;a href=&#34;https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v2.2&#34;&gt;[associated code and models]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{lux2022lrms,&#xA;  year         = 2022,&#xA;  title        = {{Low-Resource Multilingual and Zero-Shot Multispeaker TTS}},&#xA;  author       = {Florian Lux and Julia Koch and Ngoc Thang Vu},&#xA;  booktitle    = {AACL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding Controllable Speaker Embedding Generation &lt;a href=&#34;https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v2.3&#34;&gt;[associated code and models]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{lux2023controllable,&#xA;  year         = 2023,&#xA;  title        = {{Low-Resource Multilingual and Zero-Shot Multispeaker TTS}},&#xA;  author       = {Florian Lux and Pascal Tilli and Sarina Meyer and Ngoc Thang Vu},&#xA;  booktitle    = {Interspeech}&#xA;  publisher    = {ISCA}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Our Contribution to the Blizzard Challenge 2023 &lt;a href=&#34;https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v2.b&#34;&gt;[associated code and models]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{lux2023controllable,&#xA;  year         = 2023,&#xA;  title        = {{The IMS Toucan System for the Blizzard Challenge 2023}},&#xA;  author       = {Florian Lux and Julia Koch and Sarina Meyer and Thomas Bott and Nadja Schauffler and Pavel Denisov and Antje Schweitzer and Ngoc Thang Vu},&#xA;  booktitle    = {Blizzard Challenge Workshop},&#xA;  publisher    = {ISCA Speech Synthesis SIG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Introducing the first TTS System in over 7000 languages &lt;a href=&#34;https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v3.0&#34;&gt;[associated code and models]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{lux2024massive,&#xA;  year         = 2024,&#xA;  title        = {{Meta Learning Text-to-Speech Synthesis in over 7000 Languages}},&#xA;  author       = {Florian Lux and Sarina Meyer and Lyonel Behringer and Frank Zalkow and Phat Do and Matt Coler and  Emanuël A. P. Habets and Ngoc Thang Vu},&#xA;  booktitle    = {Interspeech}&#xA;  publisher    = {ISCA}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>