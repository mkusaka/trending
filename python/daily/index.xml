<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-06T01:31:39Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pydantic/logfire</title>
    <updated>2024-05-06T01:31:39Z</updated>
    <id>tag:github.com,2024-05-06:/pydantic/logfire</id>
    <link href="https://github.com/pydantic/logfire" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Uncomplicated Observability for Python and beyond! 🪵🔥&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pydantic Logfire — Uncomplicated Observability&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pydantic/logfire/actions?query=event%3Apush+branch%3Amain+workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/pydantic/logfire/actions/workflows/main.yml/badge.svg?event=push&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/pydantic/logfire&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/pydantic/logfire/graph/badge.svg?token=735CNGCGFD&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.python.org/pypi/logfire&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/logfire.svg?sanitize=true&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pydantic/logfire/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/pydantic/logfire.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pydantic/logfire&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/logfire.svg?sanitize=true&#34; alt=&#34;versions&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;From the team behind Pydantic, &lt;strong&gt;Logfire&lt;/strong&gt; is an observability platform built on the same belief as our open source library — that the most powerful tools can be easy to use.&lt;/p&gt; &#xA;&lt;p&gt;What sets Logfire apart:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple and Powerful:&lt;/strong&gt; Logfire&#39;s dashboard is simple relative to the power it provides, ensuring your entire engineering team will actually use it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Python-centric Insights:&lt;/strong&gt; From rich display of Python objects, to event-loop telemetry, to profiling Python code and database queries, Logfire gives you unparalleled visibility into your Python application&#39;s behavior.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SQL:&lt;/strong&gt; Query your data using standard SQL — all the control and (for many) nothing new to learn. Using SQL also means you can query your data with existing BI tools and database querying libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenTelemetry:&lt;/strong&gt; Logfire is an opinionated wrapper around OpenTelemetry, allowing you to leverage existing tooling, infrastructure, and instrumentation for many common Python packages, and enabling support for virtually any language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pydantic Integration:&lt;/strong&gt; Understand the data flowing through your Pydantic models and get built-in analytics on validations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://docs.pydantic.dev/logfire/&#34;&gt;documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feel free to report issues and ask any questions about Logfire in this repository!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains the Python SDK for &lt;code&gt;logfire&lt;/code&gt; and documentation; the server application for recording and displaying data is closed source.&lt;/p&gt; &#xA;&lt;h2&gt;Using Logfire&lt;/h2&gt; &#xA;&lt;p&gt;This is a very brief overview of how to use Logfire, the &lt;a href=&#34;https://docs.pydantic.dev/logfire/&#34;&gt;documentation&lt;/a&gt; has much more detail.&lt;/p&gt; &#xA;&lt;h3&gt;Install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install logfire&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.pydantic.dev/logfire/guides/first_steps/#install&#34;&gt;&lt;em&gt;(learn more)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Authenticate&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;logfire auth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.pydantic.dev/logfire/guides/first_steps/#authentication&#34;&gt;&lt;em&gt;(learn more)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Manual tracing&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s a simple manual tracing (aka logging) example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logfire&#xA;from datetime import date&#xA;&#xA;logfire.info(&#39;Hello, {name}!&#39;, name=&#39;world&#39;)&#xA;&#xA;with logfire.span(&#39;Asking the user their {question}&#39;, question=&#39;age&#39;):&#xA;    user_input = input(&#39;How old are you [YYYY-mm-dd]? &#39;)&#xA;    dob = date.fromisoformat(user_input)&#xA;    logfire.debug(&#39;{dob=} {age=!r}&#39;, dob=dob, age=date.today() - dob)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.pydantic.dev/logfire/guides/onboarding_checklist/03_add_manual_tracing/&#34;&gt;&lt;em&gt;(learn more)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Integration&lt;/h3&gt; &#xA;&lt;p&gt;Or you can also avoid manual instrumentation and instead integrate with &lt;a href=&#34;https://docs.pydantic.dev/logfire/integrations/&#34;&gt;lots of popular packages&lt;/a&gt;, here&#39;s an example of integrating with FastAPI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import logfire&#xA;from pydantic import BaseModel&#xA;from fastapi import FastAPI&#xA;&#xA;app = FastAPI()&#xA;&#xA;logfire.configure()&#xA;logfire.instrument_fastapi(app)&#xA;# next, instrument your database connector, http library etc. and add the logging handler&#xA;&#xA;class User(BaseModel):&#xA;    name: str&#xA;    country_code: str&#xA;&#xA;@app.post(&#39;/&#39;)&#xA;async def add_user(user: User):&#xA;    # we would store the user here&#xA;    return {&#39;message&#39;: f&#39;{user.name} added&#39;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.pydantic.dev/logfire/integrations/fastapi/&#34;&gt;&lt;em&gt;(learn more)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Logfire gives you a view into how your code is running like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://docs.pydantic.dev/logfire/images/index/logfire-screenshot-fastapi-200.png&#34; alt=&#34;Logfire screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;d love anyone interested to contribute to the Logfire SDK and documentation, see the &lt;a href=&#34;https://raw.githubusercontent.com/pydantic/logfire/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Reporting a Security Vulnerability&lt;/h2&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://github.com/pydantic/logfire/security&#34;&gt;security policy&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>prometheus-eval/prometheus-eval</title>
    <updated>2024-05-06T01:31:39Z</updated>
    <id>tag:github.com,2024-05-06:/prometheus-eval/prometheus-eval</id>
    <link href="https://github.com/prometheus-eval/prometheus-eval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Evaluate your LLM&#39;s response with Prometheus 💯&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/assets/logo.png&#34; alt=&#34;Prometheus-Logo&#34; style=&#34;width: 15%; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;🔥 Prometheus-Eval 🔥&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.01535&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2405.01535-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/prometheus-eval/Preference-Collection&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-Dataset-ffd21e&#34; alt=&#34;Hugging Face Datasets&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging%20Face-Model-ff9d00&#34; alt=&#34;Hugging Face Model&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/prometheus-eval/prometheus-eval/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/prometheus-eval/prometheus-eval.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/prometheus-eval/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/prometheus-eval.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ⚡ A repository for evaluating LLMs in generation tasks 🚀 ⚡ &lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latest News&lt;/strong&gt; 🔥&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024/05] We release Prometheus 2 (7B &amp;amp; 8x7B) models!&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prometheus 2 (8x7B)&lt;/strong&gt; is an open-source state-of-the-art evaluator language model!&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Compared to Prometheus 1 (13B), Prometheus 2 (8x7B) shows improved evaluation performances &amp;amp; supports assessing in pairwise ranking (relative grading) formats as well!&lt;/li&gt; &#xA;     &lt;li&gt;It achieves a Pearson correlation of 0.6 to 0.7 with GPT-4-1106 on a 5-point Likert scale across multiple direct assessment benchmarks, including &lt;a href=&#34;https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge/data/vicuna_bench&#34;&gt;VicunaBench&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge/data/mt_bench&#34;&gt;MT-Bench&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kaistAI/FLASK&#34;&gt;FLASK&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;It also scores a 72% to 85% agreement with human judgments across multiple pairwise ranking benchmarks, including &lt;a href=&#34;https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/hhh_alignment&#34;&gt;HHH Alignment&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/lmsys/mt_bench_human_judgments&#34;&gt;MT Bench Human Judgment&lt;/a&gt;, and &lt;a href=&#34;https://github.com/GAIR-NLP/auto-j/raw/main/data/test/testdata_pairwise.jsonl&#34;&gt;Auto-J Eval&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prometheus 2 (7B)&lt;/strong&gt; is a lighter version of Prometheus 2 (8x7B) model with reasonable performances (outperforming Llama-2-70B &amp;amp; on par with Mixtral-8x7B).&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;It achieves at least 80% of the evaluation statistics or performances of Prometheus 2 (8x7B)&lt;/li&gt; &#xA;     &lt;li&gt;It requires only 16 GB of VRAM, making it suitable for running on consumer GPUs.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⏩ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;code&gt;prometheus-eval&lt;/code&gt; library is currently in the beta stage. If you encounter any issues, please let us know by creating an issue on the repository.&lt;/p&gt; &#xA;&lt;p&gt;Installation with pip:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(Optional) We encourage you to install &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash attention&lt;/a&gt; for efficient inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install flash-attn --no-build-isolation&#xA;pip install prometheus-eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;With &lt;code&gt;prometheus-eval&lt;/code&gt;, evaluating &lt;em&gt;any&lt;/em&gt; instruction and response pair is as simple as:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Absolute Grading: Outputs score of 1 to 5&#xA;&#xA;from prometheus_eval import PrometheusEval&#xA;from prometheus_eval.prompts import ABSOLUTE_PROMPT, SCORE_RUBRIC_TEMPLATE&#xA;&#xA;judge = PrometheusEval(model_id=&#34;prometheus-eval/prometheus-7b-v2.0&#34;, absolute_grade_template=ABSOLUTE_PROMPT)&#xA;&#xA;instruction = &#34;Struggling with a recent break-up, a person opens up about the intense feelings of loneliness and sadness. They ask for advice on how to cope with the heartbreak and move forward in life.&#34;,&#xA;response = &#34;I&#39;m genuinely sorry to hear about your break-up. This can be an immensely challenging time filled with intense emotions of loneliness and sorrow. It&#39;s important to remember that it&#39;s normal to experience these feelings; they are a testament to the deep connection you once had. Firstly, don&#39;t hesitate to embrace your emotions, let them flow freely. Ignoring them often complicates the healing process. It&#39;s natural to be upset, to shed tears, or to feel angry. This is all part of the journey towards healing. Self-care can be a healing balm in these tough times. This could mean indulging in your favourite book, journaling your thoughts, or even just a quiet cup of tea. Small acts of kindness to oneself can significantly improve your emotional well-being. Also, don&#39;t isolate yourself. Surround yourself with supportive individuals who can offer comfort and understanding. This might include friends, family, or even a mental health professional. They can provide much-needed emotional support and strategies to help you cope. Lastly, remember there&#39;s no set timeline for healing. It&#39;s different for everyone and it&#39;s okay to take as long as you need. Keep your focus on the future and the potential it holds. Though it may not seem possible now, you will emerge stronger from this experience. It&#39;s okay to ask for help, it&#39;s okay to grieve, and it&#39;s okay to take it one day at a time. You&#39;re not alone in this journey, and there are people ready and willing to help. It&#39;s okay to heal at your own pace.&#34;,&#xA;reference_answer = &#34;I can only imagine how difficult this time must be for you. Heartbreak can feel like an overwhelming wave that washes over you, making you feel lost and alone. It&#39;s important to remember that it&#39;s okay to feel this way; your feelings are valid and it&#39;s natural to grieve the loss of a relationship. Firstly, it&#39;s crucial to allow yourself to feel the emotions fully. Suppressing or denying them often only prolongs the healing process. It&#39;s okay to cry, to feel sad, or to be angry. These are all part of the healing journey. Engaging in self-care activities can also provide a much-needed boost. This could be anything from taking a warm bath, writing in a journal, or simply drinking a cup of your favorite tea. Small acts of self-love can make a big difference in how you feel. Next, try to surround yourself with supportive people who understand your situation and provide comfort. Friends and family can be a great source of strength in times of heartbreak. If you feel comfortable, you might want to consider seeking professional help. Therapists and counselors are trained to provide assistance and tools to navigate through difficult times like these. Lastly, it&#39;s important to remember that it&#39;s okay to take your time to heal. Everyone has their own pace and there&#39;s no rush. Try to focus on the future and the possibilities it holds. While it may not seem like it now, you will come out stronger and more resilient from this experience. Remember, it&#39;s okay to ask for help and it&#39;s okay to feel the way you feel. You are not alone in this journey and there are people who care about you and want to help. It&#39;s okay to take one day at a time. Healing is a process, and it&#39;s okay to move through it at your own pace.&#34;,&#xA;&#xA;rubric_data = {&#xA;  &#34;criteria&#34;:&#34;Is the model proficient in applying empathy and emotional intelligence to its responses when the user conveys emotions or faces challenging circumstances?&#34;,&#xA;  &#34;score1_description&#34;:&#34;The model neglects to identify or react to the emotional tone of user inputs, giving responses that are unfitting or emotionally insensitive.&#34;,&#xA;  &#34;score2_description&#34;:&#34;The model intermittently acknowledges emotional context but often responds without sufficient empathy or emotional understanding.&#34;,&#xA;  &#34;score3_description&#34;:&#34;The model typically identifies emotional context and attempts to answer with empathy, yet the responses might sometimes miss the point or lack emotional profundity.&#34;,&#xA;  &#34;score4_description&#34;:&#34;The model consistently identifies and reacts suitably to emotional context, providing empathetic responses. Nonetheless, there may still be sporadic oversights or deficiencies in emotional depth.&#34;,&#xA;  &#34;score5_description&#34;:&#34;The model excels in identifying emotional context and persistently offers empathetic, emotionally aware responses that demonstrate a profound comprehension of the user&#39;s emotions or situation.&#34;&#xA;}&#xA;&#xA;score_rubric = SCORE_RUBRIC_TEMPLATE.format(**rubric_data)&#xA;&#xA;&#xA;feedback, score = judge.single_absolute_grade(&#xA;    instruction=instruction,&#xA;    response=response,&#xA;    rubric=score_rubric,&#xA;    reference_answer=reference_answer&#xA;)&#xA;&#xA;print(&#34;Feedback:&#34;, feedback)&#xA;print(&#34;Score:&#34;, score)&#xA;&#xA;# Output&#xA;# Feedback: The response provided shows a high level of empathy and emotional intelligence. It effectively addresses the emotional distress expressed by the user. It acknowledges the user&#39;s pain and validates their feelings of loneliness and sadness, which is a crucial aspect of providing empathetic advice. The response also suggests practical steps for coping, such as embracing emotions, practicing self-care, and seeking support from friends, family, or professionals. Furthermore, the response reassures the user that healing is a personal process with no fixed timeline, offering comfort and understanding. It emphasizes the user&#39;s worth and potential to overcome the situation, which demonstrates a profound comprehension of the user&#39;s emotions and situation. By comparing the score rubric with the provided response, it is clear that the model exhibits an excellent ability to apply empathy and emotional intelligence. The response does not have any deficiencies in emotional depth and successfully meets the criteria for a score of 5.&#xA;# Score: 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Relative Grading: Outputs A or B&#xA;&#xA;from prometheus_eval import PrometheusEval&#xA;from prometheus_eval.prompts import RELATIVE_PROMPT&#xA;&#xA;judge = PrometheusEval(model_id=&#34;prometheus-eval/prometheus-7b-v2.0&#34;, relative_grade_template=RELATIVE_PROMPT)&#xA;&#xA;&#xA;data = {&#xA;  &#34;instruction&#34;: &#34;A group of historians are conducting a debate on the factors that led to the fall of the Roman Empire. One historian argues that the primary reason for the fall was the constant pressure from barbarian invasions. Another one believes it was because of economic troubles and overreliance on slave labor. A third one suggests it was due to moral decay and political instability. Each historian needs to provide evidence to support their claims. How would the historian arguing for economic troubles and overreliance on slave labor present their case?&#34;,&#xA;  &#34;response_A&#34;: &#34;The historian arguing that economic troubles and overreliance on slave labor led to the fall of the Roman Empire would say this: The Empire&#39;s economy was heavily affected by the devaluation of Roman currency. This currency debasement resulted in rampant inflation, disrupting the stability of the economy. Additionally, the Roman Empire heavily depended on slave labor. This caused unemployment among free citizens because maintaining slaves was cheaper than hiring free citizens. The decline in employment opportunities resulted in economic instability. On top of these, the empire&#39;s expansion towards the east made them reliant on imports, like grain from Egypt. This over-dependency on imports caused a trade deficit, which further weakened the economy. As the empire lost territories, maintaining the trade imbalance became difficult, causing economic downfall. Thus, the economic troubles and overreliance on slave labor were among the main reasons for the fall of the Roman Empire.&#34;,&#xA;  &#34;response_B&#34;: &#34;The historian arguing for economic troubles and overreliance on slave labor would present their case citing key economic factors that contributed to the decline of the Roman Empire. Harper (2016) outlined how the devaluation of Roman currency led to inflation, disrupting economic stability. Additionally, Scheidel (2007) emphasized that the overuse of slaves resulted in widespread unemployment among free citizens, destabilizing the economy further. The empire&#39;s dependency on grain imports from Egypt, creating a trade deficit as highlighted by Temin (2006), also contributed to the economic decline. Thus, the combination of these factors played a crucial role in the fall of the Roman Empire.&#34;,&#xA;  &#34;reference_answer&#34;: &#34;This argument focuses on the economic troubles and overreliance on slave labor as primary reasons for the fall of the Roman Empire. To start with, one of the significant pieces of evidence is the devaluation of Roman currency. As highlighted by Harper (2016), the empire suffered from severe inflation due to the constant debasement of their currency, making it difficult for the economy to remain stable. Moreover, the overreliance on slave labor also played a detrimental role. As pointed out by Scheidel (2007), the dependence on slaves led to unemployment among free Roman citizens. This is because slaves were significantly cheaper to maintain compared to hiring free citizens, leading to a decline in job opportunities, which in turn resulted in economic instability. Furthermore, the empire&#39;s expansion to the east made them highly dependent on imports, for instance, grain from Egypt. As noted by Temin (2006), this created a trade deficit that further weakened the Roman economy. When the empire began to lose its territories, it became increasingly difficult to maintain this trade imbalance, leading to economic decline. In conclusion, it can be argued that the economic troubles, mainly due to the devaluation of currency and overreliance on slave labor, were significant contributing factors to the fall of the Roman Empire. The evidence provided, which includes scholarly references to Harper (2016), Scheidel (2007), and Temin (2006), supports this thesis.&#34;,&#xA;  &#34;rubric&#34;: &#34;Is the answer well supported with evidence, including citations/attributions wherever relevant?&#34;&#xA;}&#xA;&#xA;&#xA;feedback, score = judge.single_relative_grade(**data)&#xA;&#xA;print(&#34;Feedback:&#34;, feedback)&#xA;print(&#34;Score:&#34;, score)&#xA;&#xA;# Output&#xA;# Feedback: Both Response A and Response B correctly identify economic troubles and overreliance on slave labor as significant contributing factors to the fall of the Roman Empire. However, Response B is more effective in presenting the historian&#39;s argument due to its inclusion of scholarly sources to back up its claims. Specifically, it references works by Harper, Scheidel, and Temin, which adds credibility to the historian&#39;s argument and aligns well with the score rubric&#39;s emphasis on evidence and citations. While Response A provides a similar argument, it lacks any form of citations or attributions, which lessens the strength of the evidence presented. Therefore, based on the provided rubric, Response B is the superior response due to its use of scholarly evidence to support the historian&#39;s claims.&#xA;# Score: B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤔 What is Prometheus-Eval?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prometheus-Eval&lt;/strong&gt;🔥 is a repository that provides a collection of tools for training, evaluating, and using language models specialized in evaluating other language models. The repository includes the following components:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The &lt;code&gt;prometheus-eval&lt;/code&gt; Python package, which provides a simple interface for evaluating instruction-response pairs using Prometheus.&lt;/li&gt; &#xA; &lt;li&gt;Collection of evaluation datasets for training and evaluating Prometheus models.&lt;/li&gt; &#xA; &lt;li&gt;Scripts for training Prometheus models or fine-tuning on custom datasets.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Prometheus&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prometheus&lt;/strong&gt;🔥 is a family of open-source language models specialized in evaluating other language models. By effectively simulating human judgments and proprietary LM-based evaluations, we aim to resolve the following issues:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Fairness&lt;/em&gt;: Not relying on closed-source models for evaluations!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Controllability&lt;/em&gt;: You don’t have to worry about GPT version updates or sending your private data to OpenAI by constructing internal evaluation pipelines&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;Affordability&lt;/em&gt;: If you already have GPUs, it is free to use!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;finegrained-eval&#34; src=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/assets/finegrained_eval.png&#34; width=&#34;550&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🚀 What&#39;s special about Prometheus?&lt;/h2&gt; &#xA;&lt;p&gt;Compared to the Prometheus 1 models, the Prometheus 2 models support both &lt;strong&gt;direct assessment&lt;/strong&gt; (absolute grading) and &lt;strong&gt;pairwise ranking&lt;/strong&gt; (relative grading).&lt;/p&gt; &#xA;&lt;p&gt;You could switch modes by providing a different input prompt format and system prompt. Within the prompt, you should fill in the instruction, response(s), and score rubrics with your own data. Optionally, you could also add a reference answer which leads to better performance!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;formats&#34; src=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/assets/formats.png&#34; width=&#34;700&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;🏃 Running Prometheus-Eval&lt;/h2&gt; &#xA;&lt;h3&gt;Using the package &lt;code&gt;prometheus-eval&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;prometheus-eval&lt;/code&gt; package provides a simple interface for evaluating instruction-response pairs using Prometheus. The package includes the following methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;absolute_grade&lt;/code&gt;: Evaluates a single response based on a given instruction, reference answer, and score rubric. Outputs a score between 1 and 5.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;relative_grade&lt;/code&gt;: Evaluates two responses based on a given instruction and score rubric. Outputs &#39;A&#39; or &#39;B&#39; based on the better response.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Using the weights from Huggingface Hub 🤗&lt;/h3&gt; &#xA;&lt;p&gt;If you prefer directly working with the weights uploaded in Huggingface Hub, you can directly download the model weights!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;prometheus-eval/prometheus-7b-v2.0&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;prometheus-eval/prometheus-7b-v2.0&#34;)&#xA;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is your favourite condiment?&#34;},&#xA;    {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;Well, I&#39;m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I&#39;m cooking up in the kitchen!&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Do you have mayonnaise recipes?&#34;}&#xA;]&#xA;&#xA;encodeds = tokenizer.apply_chat_template(messages, return_tensors=&#34;pt&#34;)&#xA;&#xA;model_inputs = encodeds.to(device)&#xA;model.to(device)&#xA;&#xA;generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)&#xA;decoded = tokenizer.batch_decode(generated_ids)&#xA;print(decoded[0])&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📚 Learn more&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Section&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/docs/library.md&#34;&gt;Documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/docs/custom_eval.md&#34;&gt;Custom Benchmark Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/docs/eval_for_eval_lm.md&#34;&gt;Evaluation for Evaluator LMs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/docs/train_prometheus.md&#34;&gt;Training Prometheus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/docs/prompts.md&#34;&gt;Collection of Prompts&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WIP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;👏 Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The underlying codebase for training originates from Huggingface&#39;s &lt;a href=&#34;https://github.com/huggingface/alignment-handbook&#34;&gt;Alignment Handbook&lt;/a&gt; and &lt;a href=&#34;https://github.com/martyn/safetensors-merge-supermario&#34;&gt;Super Mario Merging&lt;/a&gt; repository. Also, for inference, it heavily utilizes the &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vllm&lt;/a&gt; and the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformer&lt;/a&gt; library. Huge thanks to all the contributors for these awesome repositories!! 🙌&lt;/p&gt; &#xA;&lt;h2&gt;⭐ Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#prometheus-eval/prometheus-eval&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=prometheus-eval/prometheus-eval&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work useful, please consider citing our paper!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{kim2024prometheus,&#xA;      title={Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models}, &#xA;      author={Seungone Kim and Juyoung Suk and Shayne Longpre and Bill Yuchen Lin and Jamin Shin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},&#xA;      year={2024},&#xA;      eprint={2405.01535},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/DoRA</title>
    <updated>2024-05-06T01:31:39Z</updated>
    <id>tag:github.com,2024-05-06:/NVlabs/DoRA</id>
    <link href="https://github.com/NVlabs/DoRA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[ICML2024] Official PyTorch implementation of DoRA: Weight-Decomposed Low-Rank Adaptation&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;p&gt;[ICML2024] DoRA: Weight-Decomposed Low-Rank Adaptation&lt;/p&gt; &lt;/h1&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/DoRA/main/imgs/dora.png&#34; width=&#34;600&#34;&gt; &lt;/h1&gt; &#xA;&lt;p&gt;The Official PyTorch implementation of [ICML2024] &lt;a href=&#34;https://arxiv.org/abs/2402.09353&#34;&gt;&lt;strong&gt;DoRA: Weight-Decomposed Low-Rank Adaptation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVlabs/DoRA/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NVlabs/DoRA.svg?style=social&#34; alt=&#34;Star on GitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nbasyl.github.io/&#34;&gt;Shih-Yang Liu&lt;/a&gt;, &lt;a href=&#34;https://chienyiwang.github.io/&#34;&gt;Chien-Yi Wang&lt;/a&gt;, &lt;a href=&#34;https://hongxu-yin.github.io/&#34;&gt;Hongxu Yin&lt;/a&gt;, &lt;a href=&#34;https://www.pmolchanov.com/&#34;&gt;Pavlo Molchanov&lt;/a&gt;, &lt;a href=&#34;http://vllab.ee.ntu.edu.tw/ycwang.html&#34;&gt;Yu-Chiang Frank Wang&lt;/a&gt;, &lt;a href=&#34;https://seng.hkust.edu.hk/about/people/faculty/tim-kwang-ting-cheng&#34;&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href=&#34;https://minhungchen.netlify.app/&#34;&gt;Min-Hung Chen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2402.09353&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://nbasyl.github.io/DoRA-project-page/&#34;&gt;&lt;code&gt;Website&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/DoRA/main/#citation&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding.&lt;/p&gt; &#xA;&lt;h2&gt;💥 News 💥&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[05.02.2024]&lt;/strong&gt; 🔥🔥 DoRA is accepted to &lt;a href=&#34;https://icml.cc/Conferences/2024&#34;&gt;&lt;strong&gt;ICML 2024&lt;/strong&gt;&lt;/a&gt;!! See you in Vienna!!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[04.27.2024]&lt;/strong&gt; 🔥🔥 We have added the source code and the DoRA weight for finetuning LLaMA2-7B and LLaMA3-8B on commonsense reasoning tasks!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[04.22.2024]&lt;/strong&gt; 🔥🔥 Check out a awesome blog post &lt;a href=&#34;https://www.answer.ai/posts/2024-04-26-fsdp-qdora-llama3.html&#34;&gt;FSDP/QDoRA&lt;/a&gt; from Answer.ai which shows that QDoRA siginificantly outperforms QLoRA and even edges out full finetuning!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[04.18.2024]&lt;/strong&gt; 🔥🔥 We have released the source code and the DoRA weight for reproducing the results in our paper!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[03.20.2024]&lt;/strong&gt; 🔥🔥 DoRA is now fully supported by the HuggingFace PEFT package and can now support Linear, Conv1d, and Conv2d layers, as well as linear layers quantized with bitsandbytes!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Useful Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An amazing tutorial about implementing DoRA from scratch by Sebastian Raschka, see &lt;a href=&#34;https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch&#34;&gt;https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An amazing blog post from Answer.AI about QDoRA/FSDP which allow finetuning LLMs on consumer-level GPUs, see &lt;a href=&#34;https://www.answer.ai/posts/2024-04-26-fsdp-qdora-llama3.html&#34;&gt;https://www.answer.ai/posts/2024-04-26-fsdp-qdora-llama3.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start and some tricks regarding finetuning with DoRA&lt;/h2&gt; &#xA;&lt;h3&gt;HuggingFace PEFT&lt;/h3&gt; &#xA;&lt;p&gt;DoRA is now supported by the &lt;a href=&#34;https://github.com/huggingface/peft/releases/tag/v0.10.0&#34;&gt;Huggingface PEFT package&lt;/a&gt;. You can install the PEFT package using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/peft.git -q&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After PEFT is installed, you can simply set the &lt;code&gt;use_dora&lt;/code&gt; argument of &lt;code&gt;LoraConfig()&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; for applying DoRA.&lt;/p&gt; &#xA;&lt;p&gt;An example could be as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;from peft import LoraConfig&#xA;&#xA;# Initialize DoRA configuration&#xA;config = (&#xA;    use_dora=True, ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the official &lt;a href=&#34;https://huggingface.co/docs/peft/en/developer_guides/lora#weight-decomposed-low-rank-adaptation-dora&#34;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;HuggingFace Diffusers&lt;/h3&gt; &#xA;&lt;p&gt;You can also toy with DoRA on finetuning diffusion models. See &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/advanced_diffusion_training#dora-training&#34;&gt;huggingface/diffusers&lt;/a&gt;. Another good tutorial would be this &lt;a href=&#34;https://colab.research.google.com/drive/134mt7bCMKtCYyYzETfEGKXT1J6J50ydT?usp=sharing#scrollTo=23d6bb49-3469-4e23-baf5-25b2344b599d&#34;&gt;Colab notebook&lt;/a&gt; from &lt;a href=&#34;https://twitter.com/linoy_tsaban&#34;&gt;Linoy Tsaban&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In general, DoRA finetuning on diffusion models is still &lt;em&gt;experimental&lt;/em&gt; and is likely to require different hyperparameter values to perform best compared to LoRA.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Specifically, people have noticed 2 differences to take into account in your training:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;strong&gt;LoRA seem to converge faster than DoRA&lt;/strong&gt; (so a set of parameters that may lead to overfitting when training a LoRA may be working well for a DoRA)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;DoRA quality superior to LoRA especially in lower ranks&lt;/strong&gt;: The difference in quality of DoRA of rank 8 and LoRA of rank 8 appears to be more significant than when training ranks of 32 or 64 for example.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Some DoRA vs. LoRA diffusion finetuning results&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example From &lt;a href=&#34;https://twitter.com/linoy_tsaban&#34;&gt;Linoy Tsaban&lt;/a&gt;(Images generated by DoRA are on the left and LoRA on the right):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/DoRA/main/imgs/dora_lora_yoda_emoji.jpg&#34; width=&#34;500&#34;&gt; &lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Example From &lt;a href=&#34;https://twitter.com/mervenoyann&#34;&gt;merve&lt;/a&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/DoRA/main/imgs/dora_lora_lego.jpeg&#34; width=&#34;500&#34;&gt; &lt;/h1&gt; &#xA;&lt;h3&gt;DoRA hyperparameters settings&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] 💡 While fine-tuning with DoRA, utilizing the configuration of LoRA can already achieve better results most of the time, achieving optimal performance compared to LoRA still requires adjustments to the hyperparameters.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We suggest starting with a slightly lower learning rate than that of LoRA, and users may also experiment with varying LoRA dropout ratios.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;User may also start with half of the rank of the LoRA configuration which oftentimes can already result in comparable or even superior accuracy compared to that of LoRA.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Reproducing the results in the paper&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains four directories:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;./commonsense_reasoning&lt;/code&gt; contains the code to finetune LLaMA-7B/13B using DoRA on the commonsense reasoning tasks. This directory is modified based on &lt;a href=&#34;https://github.com/AGI-Edgerunners/LLM-Adapters&#34;&gt;LLM-Adapter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;./instruction_tuning&lt;/code&gt; contains the code to finetune LLaMA-7B and LLaMA2-7B using DoRA and DVoRA (DoRA+VeRA) with the cleaned Alpaca instruction tuning dataset. This directory is modified based on &lt;a href=&#34;https://openreview.net/attachment?id=NjNfLdxr3A&amp;amp;name=supplementary_material&#34;&gt;VeRA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;./image_video_text_understanding&lt;/code&gt; contains the code to finetune VL-BART using DoRA for the image/video-text understanding tasks. This directory is modified based on &lt;a href=&#34;https://github.com/ylsung/VL_adapter&#34;&gt;VL-Adapter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;./visual_instruction_tuning&lt;/code&gt; contains the code to finetune LLaVA-1.5-7B on the visual instruction tuning tasks with DoRA. This directory is modified based on &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;DoRA vs LoRA on the commonsense reasoning tasks&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;r&lt;/th&gt; &#xA;   &lt;th&gt;BoolQ&lt;/th&gt; &#xA;   &lt;th&gt;PIQA&lt;/th&gt; &#xA;   &lt;th&gt;SIQA&lt;/th&gt; &#xA;   &lt;th&gt;HellaS&lt;/th&gt; &#xA;   &lt;th&gt;WinoG&lt;/th&gt; &#xA;   &lt;th&gt;ARC-e&lt;/th&gt; &#xA;   &lt;th&gt;ARC-c&lt;/th&gt; &#xA;   &lt;th&gt;OBQA&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;67.5&lt;/td&gt; &#xA;   &lt;td&gt;80.8&lt;/td&gt; &#xA;   &lt;td&gt;78.2&lt;/td&gt; &#xA;   &lt;td&gt;83.4&lt;/td&gt; &#xA;   &lt;td&gt;80.4&lt;/td&gt; &#xA;   &lt;td&gt;78.0&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;79.1&lt;/td&gt; &#xA;   &lt;td&gt;76.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B-DoRA(ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1cKCXN168uv1bWkI00d20FvyVeZTMU8Ky?usp=drive_link&#34;&gt;16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70.0&lt;/td&gt; &#xA;   &lt;td&gt;82.6&lt;/td&gt; &#xA;   &lt;td&gt;79.7&lt;/td&gt; &#xA;   &lt;td&gt;83.2&lt;/td&gt; &#xA;   &lt;td&gt;80.6&lt;/td&gt; &#xA;   &lt;td&gt;80.6&lt;/td&gt; &#xA;   &lt;td&gt;65.4&lt;/td&gt; &#xA;   &lt;td&gt;77.6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;77.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B-DoRA(ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1Kz27h5BqNv3NOLdH2UhDf12C2JtwJe0Q?usp=drive_link&#34;&gt;32&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;69.7&lt;/td&gt; &#xA;   &lt;td&gt;83.4&lt;/td&gt; &#xA;   &lt;td&gt;78.6&lt;/td&gt; &#xA;   &lt;td&gt;87.2&lt;/td&gt; &#xA;   &lt;td&gt;81.0&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;66.2&lt;/td&gt; &#xA;   &lt;td&gt;79.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;78.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2-7B-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;69.8&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;79.5&lt;/td&gt; &#xA;   &lt;td&gt;83.6&lt;/td&gt; &#xA;   &lt;td&gt;82.6&lt;/td&gt; &#xA;   &lt;td&gt;79.8&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;81.0&lt;/td&gt; &#xA;   &lt;td&gt;77.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2-7B-DoRA(ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1lMn7WKLw5aQQqwnFnuDpsM3c9FsQtpl2?usp=drive_link&#34;&gt;16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;72.0&lt;/td&gt; &#xA;   &lt;td&gt;83.1&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;89.1&lt;/td&gt; &#xA;   &lt;td&gt;83.0&lt;/td&gt; &#xA;   &lt;td&gt;84.5&lt;/td&gt; &#xA;   &lt;td&gt;71.0&lt;/td&gt; &#xA;   &lt;td&gt;81.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;80.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA2-7B-DoRA(ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1x2qamDlNRgNtBBi-tPrZ3UTYXdObtskE?usp=drive_link&#34;&gt;32&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;71.8&lt;/td&gt; &#xA;   &lt;td&gt;83.7&lt;/td&gt; &#xA;   &lt;td&gt;76.0&lt;/td&gt; &#xA;   &lt;td&gt;89.1&lt;/td&gt; &#xA;   &lt;td&gt;82.6&lt;/td&gt; &#xA;   &lt;td&gt;83.7&lt;/td&gt; &#xA;   &lt;td&gt;68.2&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;79.7&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA3-8B-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;70.8&lt;/td&gt; &#xA;   &lt;td&gt;85.2&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;91.7&lt;/td&gt; &#xA;   &lt;td&gt;84.3&lt;/td&gt; &#xA;   &lt;td&gt;84.2&lt;/td&gt; &#xA;   &lt;td&gt;71.2&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;80.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA3-8B-DoRA(ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1WHH_c5sGIdybPZt2Cuk0uEQrKtUOAk5v?usp=drive_link&#34;&gt;16&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.5&lt;/td&gt; &#xA;   &lt;td&gt;88.8&lt;/td&gt; &#xA;   &lt;td&gt;80.3&lt;/td&gt; &#xA;   &lt;td&gt;95.5&lt;/td&gt; &#xA;   &lt;td&gt;84.7&lt;/td&gt; &#xA;   &lt;td&gt;90.1&lt;/td&gt; &#xA;   &lt;td&gt;79.1&lt;/td&gt; &#xA;   &lt;td&gt;87.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA3-8B-DoRA(ours)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/107-Qjf-odzG7q7uMonLy_ulwzhE5URgb?usp=drive_link&#34;&gt;32&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.6&lt;/td&gt; &#xA;   &lt;td&gt;89.3&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;95.5&lt;/td&gt; &#xA;   &lt;td&gt;85.6&lt;/td&gt; &#xA;   &lt;td&gt;90.5&lt;/td&gt; &#xA;   &lt;td&gt;80.4&lt;/td&gt; &#xA;   &lt;td&gt;85.8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;85.2&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#NVlabs/DoRA&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=NVlabs/DoRA&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Shih-Yang Liu: &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/DoRA/main/shihyangl@nvidia.com&#34;&gt;shihyangl@nvidia.com&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/DoRA/main/sliuau@connect.ust.hk&#34;&gt;sliuau@connect.ust.hk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find DoRA useful, please consider giving a star and citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2024dora,&#xA;  title={DoRA: Weight-Decomposed Low-Rank Adaptation},&#xA;  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},&#xA;  journal={arXiv preprint arXiv:2402.09353},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;Copyright © 2024, NVIDIA Corporation. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;This work is made available under the NVIDIA Source Code License-NC. Click &lt;a href=&#34;https://github.com/nbasyl/DoRA/LICENSE&#34;&gt;here&lt;/a&gt; to view a copy of this license.&lt;/p&gt;</summary>
  </entry>
</feed>