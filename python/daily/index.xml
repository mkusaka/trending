<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-13T01:36:17Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>beetbox/beets</title>
    <updated>2025-01-13T01:36:17Z</updated>
    <id>tag:github.com,2025-01-13:/beetbox/beets</id>
    <link href="https://github.com/beetbox/beets" rel="alternate"></link>
    <summary type="html">&lt;p&gt;music library manager and MusicBrainz tagger&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/pypi/v/beets.svg&#34;&gt;https://img.shields.io/pypi/v/beets.svg&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.python.org/pypi/beets&#34;&gt;https://pypi.python.org/pypi/beets&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/codecov/c/github/beetbox/beets.svg&#34;&gt;https://img.shields.io/codecov/c/github/beetbox/beets.svg&lt;/a&gt; :target: &lt;a href=&#34;https://codecov.io/github/beetbox/beets&#34;&gt;https://codecov.io/github/beetbox/beets&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/beetbox/beets/workflows/ci/badge.svg?branch=master&#34;&gt;https://github.com/beetbox/beets/workflows/ci/badge.svg?branch=master&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/beetbox/beets/actions&#34;&gt;https://github.com/beetbox/beets/actions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://repology.org/badge/tiny-repos/beets.svg&#34;&gt;https://repology.org/badge/tiny-repos/beets.svg&lt;/a&gt; :target: &lt;a href=&#34;https://repology.org/project/beets/versions&#34;&gt;https://repology.org/project/beets/versions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;beets&lt;/h1&gt; &#xA;&lt;p&gt;Beets is the media library management system for obsessive music geeks.&lt;/p&gt; &#xA;&lt;p&gt;The purpose of beets is to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes. It then provides a bouquet of tools for manipulating and accessing your music.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s an example of beets&#39; brainy tag corrector doing its thing::&lt;/p&gt; &#xA;&lt;p&gt;$ beet import ~/music/ladytron Tagging: Ladytron - Witching Hour (Similarity: 98.4%)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Last One Standing -&amp;gt; The Last One Standing&lt;/li&gt; &#xA; &lt;li&gt;Beauty -&amp;gt; Beauty*2&lt;/li&gt; &#xA; &lt;li&gt;White Light Generation -&amp;gt; Whitelightgenerator&lt;/li&gt; &#xA; &lt;li&gt;All the Way -&amp;gt; All the Way...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Because beets is designed as a library, it can do almost anything you can imagine for your music collection. Via &lt;code&gt;plugins&lt;/code&gt;_, beets becomes a panacea:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fetch or calculate all the metadata you could possibly need: &lt;code&gt;album art&lt;/code&gt;&lt;em&gt;, &lt;code&gt;lyrics&lt;/code&gt;&lt;/em&gt;, &lt;code&gt;genres&lt;/code&gt;&lt;em&gt;, &lt;code&gt;tempos&lt;/code&gt;&lt;/em&gt;, &lt;code&gt;ReplayGain&lt;/code&gt;_ levels, or &lt;code&gt;acoustic fingerprints&lt;/code&gt;_.&lt;/li&gt; &#xA; &lt;li&gt;Get metadata from &lt;code&gt;MusicBrainz&lt;/code&gt;&lt;em&gt;, &lt;code&gt;Discogs&lt;/code&gt;&lt;/em&gt;, and &lt;code&gt;Beatport&lt;/code&gt;_. Or guess metadata using songs&#39; filenames or their acoustic fingerprints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Transcode audio&lt;/code&gt;_ to any format you like.&lt;/li&gt; &#xA; &lt;li&gt;Check your library for &lt;code&gt;duplicate tracks and albums&lt;/code&gt;_ or for &lt;code&gt;albums that are missing tracks&lt;/code&gt;_.&lt;/li&gt; &#xA; &lt;li&gt;Clean up crufty tags left behind by other, less-awesome tools.&lt;/li&gt; &#xA; &lt;li&gt;Embed and extract album art from files&#39; metadata.&lt;/li&gt; &#xA; &lt;li&gt;Browse your music library graphically through a Web browser and play it in any browser that supports &lt;code&gt;HTML5 Audio&lt;/code&gt;_.&lt;/li&gt; &#xA; &lt;li&gt;Analyze music files&#39; metadata from the command line.&lt;/li&gt; &#xA; &lt;li&gt;Listen to your library with a music player that speaks the &lt;code&gt;MPD&lt;/code&gt;_ protocol and works with a staggering variety of interfaces.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If beets doesn&#39;t do what you want yet, &lt;code&gt;writing your own plugin&lt;/code&gt;_ is shockingly simple if you know a little Python.&lt;/p&gt; &#xA;&lt;p&gt;.. _plugins: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/&#34;&gt;https://beets.readthedocs.org/page/plugins/&lt;/a&gt; .. _MPD: &lt;a href=&#34;https://www.musicpd.org/&#34;&gt;https://www.musicpd.org/&lt;/a&gt; .. _MusicBrainz music collection: &lt;a href=&#34;https://musicbrainz.org/doc/Collections/&#34;&gt;https://musicbrainz.org/doc/Collections/&lt;/a&gt; .. _writing your own plugin: &lt;a href=&#34;https://beets.readthedocs.org/page/dev/plugins.html&#34;&gt;https://beets.readthedocs.org/page/dev/plugins.html&lt;/a&gt; .. _HTML5 Audio: &lt;a href=&#34;https://html.spec.whatwg.org/multipage/media.html#the-audio-element&#34;&gt;https://html.spec.whatwg.org/multipage/media.html#the-audio-element&lt;/a&gt; .. _albums that are missing tracks: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/missing.html&#34;&gt;https://beets.readthedocs.org/page/plugins/missing.html&lt;/a&gt; .. _duplicate tracks and albums: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/duplicates.html&#34;&gt;https://beets.readthedocs.org/page/plugins/duplicates.html&lt;/a&gt; .. _Transcode audio: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/convert.html&#34;&gt;https://beets.readthedocs.org/page/plugins/convert.html&lt;/a&gt; .. _Discogs: &lt;a href=&#34;https://www.discogs.com/&#34;&gt;https://www.discogs.com/&lt;/a&gt; .. _acoustic fingerprints: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/chroma.html&#34;&gt;https://beets.readthedocs.org/page/plugins/chroma.html&lt;/a&gt; .. _ReplayGain: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/replaygain.html&#34;&gt;https://beets.readthedocs.org/page/plugins/replaygain.html&lt;/a&gt; .. _tempos: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/acousticbrainz.html&#34;&gt;https://beets.readthedocs.org/page/plugins/acousticbrainz.html&lt;/a&gt; .. _genres: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/lastgenre.html&#34;&gt;https://beets.readthedocs.org/page/plugins/lastgenre.html&lt;/a&gt; .. _album art: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/fetchart.html&#34;&gt;https://beets.readthedocs.org/page/plugins/fetchart.html&lt;/a&gt; .. _lyrics: &lt;a href=&#34;https://beets.readthedocs.org/page/plugins/lyrics.html&#34;&gt;https://beets.readthedocs.org/page/plugins/lyrics.html&lt;/a&gt; .. _MusicBrainz: &lt;a href=&#34;https://musicbrainz.org/&#34;&gt;https://musicbrainz.org/&lt;/a&gt; .. _Beatport: &lt;a href=&#34;https://www.beatport.com&#34;&gt;https://www.beatport.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;p&gt;You can install beets by typing &lt;code&gt;pip install beets&lt;/code&gt; or directly from Github (see details &lt;code&gt;here&lt;/code&gt;&lt;em&gt;). Beets has also been packaged in the &lt;code&gt;software repositories&lt;/code&gt;&lt;/em&gt; of several distributions. Check out the &lt;code&gt;Getting Started&lt;/code&gt;_ guide for more information.&lt;/p&gt; &#xA;&lt;p&gt;.. _here: &lt;a href=&#34;https://beets.readthedocs.io/en/latest/faq.html#run-the-latest-source-version-of-beets&#34;&gt;https://beets.readthedocs.io/en/latest/faq.html#run-the-latest-source-version-of-beets&lt;/a&gt; .. _Getting Started: &lt;a href=&#34;https://beets.readthedocs.org/page/guides/main.html&#34;&gt;https://beets.readthedocs.org/page/guides/main.html&lt;/a&gt; .. _software repositories: &lt;a href=&#34;https://repology.org/project/beets/versions&#34;&gt;https://repology.org/project/beets/versions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for considering contributing to &lt;code&gt;beets&lt;/code&gt;! Whether you&#39;re a programmer or not, you should be able to find all the info you need at &lt;code&gt;CONTRIBUTING.rst&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;.. _CONTRIBUTING.rst: &lt;a href=&#34;https://github.com/beetbox/beets/raw/master/CONTRIBUTING.rst&#34;&gt;https://github.com/beetbox/beets/blob/master/CONTRIBUTING.rst&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Read More&lt;/h2&gt; &#xA;&lt;p&gt;Learn more about beets at &lt;code&gt;its Web site&lt;/code&gt;&lt;em&gt;. Follow &lt;code&gt;@b33ts&lt;/code&gt;&lt;/em&gt; on Mastodon for news and updates.&lt;/p&gt; &#xA;&lt;p&gt;.. _its Web site: &lt;a href=&#34;https://beets.io/&#34;&gt;https://beets.io/&lt;/a&gt; .. _@b33ts: &lt;a href=&#34;https://fosstodon.org/@beets&#34;&gt;https://fosstodon.org/@beets&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Encountered a bug you&#39;d like to report? Check out our &lt;code&gt;issue tracker&lt;/code&gt;_! &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If your issue hasn&#39;t already been reported, please &lt;code&gt;open a new ticket&lt;/code&gt;_ and we&#39;ll be in touch with you shortly.&lt;/li&gt; &#xA;   &lt;li&gt;If you&#39;d like to vote on a feature/bug, simply give a &lt;span&gt;üëç&lt;/span&gt; on issues you&#39;d like to see prioritized over others.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Need help/support, would like to start a discussion, have an idea for a new feature, or would just like to introduce yourself to the team? Check out &lt;code&gt;GitHub Discussions&lt;/code&gt;_!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;.. _GitHub Discussions: &lt;a href=&#34;https://github.com/beetbox/beets/discussions&#34;&gt;https://github.com/beetbox/beets/discussions&lt;/a&gt; .. _issue tracker: &lt;a href=&#34;https://github.com/beetbox/beets/issues&#34;&gt;https://github.com/beetbox/beets/issues&lt;/a&gt; .. _open a new ticket: &lt;a href=&#34;https://github.com/beetbox/beets/issues/new/choose&#34;&gt;https://github.com/beetbox/beets/issues/new/choose&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p&gt;Beets is by &lt;code&gt;Adrian Sampson&lt;/code&gt;_ with a supporting cast of thousands.&lt;/p&gt; &#xA;&lt;p&gt;.. _Adrian Sampson: &lt;a href=&#34;https://www.cs.cornell.edu/~asampson/&#34;&gt;https://www.cs.cornell.edu/~asampson/&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/Sana</title>
    <updated>2025-01-13T01:36:17Z</updated>
    <id>tag:github.com,2025-01-13:/NVlabs/Sana</id>
    <link href="https://github.com/NVlabs/Sana" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; style=&#34;border-radius: 10px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/logo.png&#34; width=&#34;35%&#34; alt=&#34;logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;‚ö°Ô∏èSana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://nvlabs.github.io/Sana/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Project&amp;amp;message=Github&amp;amp;color=blue&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://hanlab.mit.edu/projects/sana/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Page&amp;amp;message=MIT&amp;amp;color=darkred&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://arxiv.org/abs/2410.10629&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Arxiv&amp;amp;message=Sana&amp;amp;color=red&amp;amp;logo=arxiv&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://nv-sana.mit.edu/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo:8x3090&amp;amp;message=MIT&amp;amp;color=yellow&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://replicate.com/chenxwh/sana&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=API:H100&amp;amp;message=Replicate&amp;amp;color=pink&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://discord.gg/rde6eaE5Ta&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Discuss&amp;amp;message=Discord&amp;amp;color=purple&amp;amp;logo=discord&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34; border-raduis=&#34;10px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/Sana.jpg&#34; width=&#34;90%&#34; alt=&#34;teaser_page1&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üí° Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096 √ó 4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include:&lt;/p&gt; &#xA;&lt;p&gt;(1) &lt;a href=&#34;https://hanlab.mit.edu/projects/dc-ae&#34;&gt;&lt;strong&gt;DC-AE&lt;/strong&gt;&lt;/a&gt;: unlike traditional AEs, which compress images only 8√ó, we trained an AE that can compress images 32√ó, effectively reducing the number of latent tokens. &lt;br&gt; (2) &lt;strong&gt;Linear DiT&lt;/strong&gt;: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. &lt;br&gt; (3) &lt;strong&gt;Decoder-only text encoder&lt;/strong&gt;: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. &lt;br&gt; (4) &lt;strong&gt;Efficient training and sampling&lt;/strong&gt;: we propose &lt;strong&gt;Flow-DPM-Solver&lt;/strong&gt; to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence.&lt;/p&gt; &#xA;&lt;p&gt;As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024 √ó 1024 resolution image. Sana enables content creation at low cost.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; border-raduis=&#34;10px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/model-incremental.jpg&#34; width=&#34;90%&#34; alt=&#34;teaser_page2&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üî•üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/12] DC-AE tiling makes Sana-4K inferences 4096x4096px images within 22GB GPU memory.&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md#-3-4k-models&#34;&gt;[Guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/11] Sana code-base license changed to Apache 2.0.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/10] Inference Sana with 8bit quantization.&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/8bit_sana.md#quantization&#34;&gt;[Guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/8] 4K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; is supported in &lt;a href=&#34;https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels&#34;&gt;Sana-ComfyUI&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/Sana_FlowEuler_4K.json&#34;&gt;work flow&lt;/a&gt; is also prepared. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/comfyui.md&#34;&gt;[4K guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/8] 1.6B 4K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; are released: &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_4Kpx_BF16&#34;&gt;[BF16 pth]&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_4Kpx_BF16_diffusers&#34;&gt;[BF16 diffusers]&lt;/a&gt;. üöÄ Get your 4096x4096 resolution images within 20 seconds! Find more samples in &lt;a href=&#34;https://nvlabs.github.io/Sana/&#34;&gt;Sana page&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/Fanghua-Yu/SUPIR&#34;&gt;SUPIR&lt;/a&gt; for their wonderful work and support.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/2] Bug in the &lt;code&gt;diffusers&lt;/code&gt; pipeline is solved. &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/10431&#34;&gt;Solved PR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/2] 2K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; is supported in &lt;a href=&#34;https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels&#34;&gt;Sana-ComfyUI&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/Sana_FlowEuler_2K.json&#34;&gt;work flow&lt;/a&gt; is also prepared.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/20] 1.6B 2K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; are released: &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_2Kpx_BF16&#34;&gt;[BF16 pth]&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_2Kpx_BF16_diffusers&#34;&gt;[BF16 diffusers]&lt;/a&gt;. üöÄ Get your 2K resolution images within 4 seconds! Find more samples in &lt;a href=&#34;https://nvlabs.github.io/Sana/&#34;&gt;Sana page&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/Fanghua-Yu/SUPIR&#34;&gt;SUPIR&lt;/a&gt; for their wonderful work and support.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/18] &lt;code&gt;diffusers&lt;/code&gt; supports Sana-LoRA fine-tuning! Sana-LoRA&#39;s training and convergence speed is supper fast. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/sana_lora_dreambooth.md&#34;&gt;[Guidance]&lt;/a&gt; or &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/examples/dreambooth/README_sana.md&#34;&gt;[diffusers docs]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/13] &lt;code&gt;diffusers&lt;/code&gt; has Sana! &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/sana-673efba2a57ed99843f11f9e&#34;&gt;All Sana models in diffusers safetensors&lt;/a&gt; are released and diffusers pipeline &lt;code&gt;SanaPipeline&lt;/code&gt;, &lt;code&gt;SanaPAGPipeline&lt;/code&gt;, &lt;code&gt;DPMSolverMultistepScheduler(with FlowMatching)&lt;/code&gt; are all supported now. We prepare a &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model Card&lt;/a&gt; for you to choose.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/10] 1.6B BF16 &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_BF16&#34;&gt;Sana model&lt;/a&gt; is released for stable fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/9] We release the &lt;a href=&#34;https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels&#34;&gt;ComfyUI node&lt;/a&gt; for Sana. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/comfyui.md&#34;&gt;[Guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] All multi-linguistic (Emoji &amp;amp; Chinese &amp;amp; English) SFT models are released: &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_512px_MultiLing&#34;&gt;1.6B-512px&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_MultiLing&#34;&gt;1.6B-1024px&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_600M_512px&#34;&gt;600M-512px&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px&#34;&gt;600M-1024px&lt;/a&gt;. The metric performance is shown &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#performance&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] Sana Replicate API is launching at &lt;a href=&#34;https://replicate.com/chenxwh/sana&#34;&gt;Sana-API&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] 1.6B &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/sana-673efba2a57ed99843f11f9e&#34;&gt;Sana models&lt;/a&gt; are released.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] Training &amp;amp; Inference &amp;amp; Metrics code are released.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] Working on &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/9982&#34;&gt;&lt;code&gt;diffusers&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] &lt;a href=&#34;https://nv-sana.mit.edu/&#34;&gt;Demo&lt;/a&gt; is released.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] &lt;a href=&#34;https://github.com/mit-han-lab/efficientvit/raw/master/applications/dc_ae/README.md&#34;&gt;DC-AE Code&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/collections/mit-han-lab/dc-ae-670085b9400ad7197bb1009b&#34;&gt;weights&lt;/a&gt; are released!&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] &lt;a href=&#34;https://arxiv.org/abs/2410.10629&#34;&gt;Paper&lt;/a&gt; is on Arxiv!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Methods (1024x1024)&lt;/th&gt; &#xA;   &lt;th&gt;Throughput (samples/s)&lt;/th&gt; &#xA;   &lt;th&gt;Latency (s)&lt;/th&gt; &#xA;   &lt;th&gt;Params (B)&lt;/th&gt; &#xA;   &lt;th&gt;Speedup&lt;/th&gt; &#xA;   &lt;th&gt;FID üëá&lt;/th&gt; &#xA;   &lt;th&gt;CLIP üëÜ&lt;/th&gt; &#xA;   &lt;th&gt;GenEval üëÜ&lt;/th&gt; &#xA;   &lt;th&gt;DPG üëÜ&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FLUX-dev&lt;/td&gt; &#xA;   &lt;td&gt;0.04&lt;/td&gt; &#xA;   &lt;td&gt;23.0&lt;/td&gt; &#xA;   &lt;td&gt;12.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;   &lt;td&gt;10.15&lt;/td&gt; &#xA;   &lt;td&gt;27.47&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;0.67&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;84.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Sana-0.6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;   &lt;td&gt;39.5√ó&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;5.81&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;28.36&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;83.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px&#34;&gt;Sana-0.6B-MultiLing&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;   &lt;td&gt;39.5√ó&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.61&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;28.80&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;0.68&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;84.2&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Sana-1.6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;23.3√ó&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;5.76&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;28.67&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_MultiLing&#34;&gt;Sana-1.6B-MultiLing&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;23.3√ó&lt;/td&gt; &#xA;   &lt;td&gt;5.92&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;28.94&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.69&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;84.5&lt;/u&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Click to show all&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Methods&lt;/th&gt; &#xA;    &lt;th&gt;Throughput (samples/s)&lt;/th&gt; &#xA;    &lt;th&gt;Latency (s)&lt;/th&gt; &#xA;    &lt;th&gt;Params (B)&lt;/th&gt; &#xA;    &lt;th&gt;Speedup&lt;/th&gt; &#xA;    &lt;th&gt;FID üëÜ&lt;/th&gt; &#xA;    &lt;th&gt;CLIP üëÜ&lt;/th&gt; &#xA;    &lt;th&gt;GenEval üëÜ&lt;/th&gt; &#xA;    &lt;th&gt;DPG üëÜ&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;em&gt;&lt;strong&gt;512 √ó 512 resolution&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PixArt-Œ±&lt;/td&gt; &#xA;    &lt;td&gt;1.5&lt;/td&gt; &#xA;    &lt;td&gt;1.2&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.14&lt;/td&gt; &#xA;    &lt;td&gt;27.55&lt;/td&gt; &#xA;    &lt;td&gt;0.48&lt;/td&gt; &#xA;    &lt;td&gt;71.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PixArt-Œ£&lt;/td&gt; &#xA;    &lt;td&gt;1.5&lt;/td&gt; &#xA;    &lt;td&gt;1.2&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;6.34&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;27.62&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;0.52&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;79.5&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-0.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;6.7&lt;/td&gt; &#xA;    &lt;td&gt;0.8&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;5.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;5.67&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;27.92&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;0.64&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;84.3&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-1.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;3.8&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;1.6&lt;/td&gt; &#xA;    &lt;td&gt;2.5√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;5.16&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;28.19&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;0.66&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;85.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;em&gt;&lt;strong&gt;1024 √ó 1024 resolution&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;LUMINA-Next&lt;/td&gt; &#xA;    &lt;td&gt;0.12&lt;/td&gt; &#xA;    &lt;td&gt;9.1&lt;/td&gt; &#xA;    &lt;td&gt;2.0&lt;/td&gt; &#xA;    &lt;td&gt;2.8√ó&lt;/td&gt; &#xA;    &lt;td&gt;7.58&lt;/td&gt; &#xA;    &lt;td&gt;26.84&lt;/td&gt; &#xA;    &lt;td&gt;0.46&lt;/td&gt; &#xA;    &lt;td&gt;74.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;SDXL&lt;/td&gt; &#xA;    &lt;td&gt;0.15&lt;/td&gt; &#xA;    &lt;td&gt;6.5&lt;/td&gt; &#xA;    &lt;td&gt;2.6&lt;/td&gt; &#xA;    &lt;td&gt;3.5√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.63&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;29.03&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;0.55&lt;/td&gt; &#xA;    &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PlayGroundv2.5&lt;/td&gt; &#xA;    &lt;td&gt;0.21&lt;/td&gt; &#xA;    &lt;td&gt;5.3&lt;/td&gt; &#xA;    &lt;td&gt;2.6&lt;/td&gt; &#xA;    &lt;td&gt;4.9√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;6.09&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;29.13&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;0.56&lt;/td&gt; &#xA;    &lt;td&gt;75.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Hunyuan-DiT&lt;/td&gt; &#xA;    &lt;td&gt;0.05&lt;/td&gt; &#xA;    &lt;td&gt;18.2&lt;/td&gt; &#xA;    &lt;td&gt;1.5&lt;/td&gt; &#xA;    &lt;td&gt;1.2√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.54&lt;/td&gt; &#xA;    &lt;td&gt;28.19&lt;/td&gt; &#xA;    &lt;td&gt;0.63&lt;/td&gt; &#xA;    &lt;td&gt;78.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PixArt-Œ£&lt;/td&gt; &#xA;    &lt;td&gt;0.4&lt;/td&gt; &#xA;    &lt;td&gt;2.7&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;9.3√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.15&lt;/td&gt; &#xA;    &lt;td&gt;28.26&lt;/td&gt; &#xA;    &lt;td&gt;0.54&lt;/td&gt; &#xA;    &lt;td&gt;80.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DALLE3&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;0.67&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;83.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;SD3-medium&lt;/td&gt; &#xA;    &lt;td&gt;0.28&lt;/td&gt; &#xA;    &lt;td&gt;4.4&lt;/td&gt; &#xA;    &lt;td&gt;2.0&lt;/td&gt; &#xA;    &lt;td&gt;6.5√ó&lt;/td&gt; &#xA;    &lt;td&gt;11.92&lt;/td&gt; &#xA;    &lt;td&gt;27.83&lt;/td&gt; &#xA;    &lt;td&gt;0.62&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;84.1&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;FLUX-dev&lt;/td&gt; &#xA;    &lt;td&gt;0.04&lt;/td&gt; &#xA;    &lt;td&gt;23.0&lt;/td&gt; &#xA;    &lt;td&gt;12.0&lt;/td&gt; &#xA;    &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;10.15&lt;/td&gt; &#xA;    &lt;td&gt;27.47&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;0.67&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;84.0&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;FLUX-schnell&lt;/td&gt; &#xA;    &lt;td&gt;0.5&lt;/td&gt; &#xA;    &lt;td&gt;2.1&lt;/td&gt; &#xA;    &lt;td&gt;12.0&lt;/td&gt; &#xA;    &lt;td&gt;11.6√ó&lt;/td&gt; &#xA;    &lt;td&gt;7.94&lt;/td&gt; &#xA;    &lt;td&gt;28.14&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;0.71&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-0.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;1.7&lt;/td&gt; &#xA;    &lt;td&gt;0.9&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;39.5√ó&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;5.81&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;28.36&lt;/td&gt; &#xA;    &lt;td&gt;0.64&lt;/td&gt; &#xA;    &lt;td&gt;83.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-1.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;1.0&lt;/td&gt; &#xA;    &lt;td&gt;1.2&lt;/td&gt; &#xA;    &lt;td&gt;1.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;23.3√ó&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;5.76&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;28.67&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;0.66&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-1-dependencies-and-installation&#34;&gt;Env&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-2-how-to-play-with-sana-inference&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-3-how-to-train-sana&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-4-metric-toolkit&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#to-do-list&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#bibtex&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üîß 1. Dependencies and Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.10.0 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 2.0.1+cu12.1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/NVlabs/Sana.git&#xA;cd Sana&#xA;&#xA;./environment_setup.sh sana&#xA;# or you can install each components step by step following environment_setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üíª 2. How to Play with Sana (Inference)&lt;/h1&gt; &#xA;&lt;h2&gt;üí∞Hardware requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;9GB VRAM is required for 0.6B model and 12GB VRAM for 1.6B model. Our later quantization version will require less than 8GB for inference.&lt;/li&gt; &#xA; &lt;li&gt;All the tests are done on A100 GPUs. Different GPU version may be different.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîõ Choose your model: &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model card&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;üîõ Quick start with &lt;a href=&#34;https://www.gradio.app/guides/quickstart&#34;&gt;Gradio&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# official online demo&#xA;DEMO_PORT=15432 \&#xA;python app/app_sana.py \&#xA;    --share \&#xA;    --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;    --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;    --image_size=1024&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;1. How to use &lt;code&gt;SanaPipeline&lt;/code&gt; with &lt;code&gt;üß®diffusers&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Upgrade your &lt;code&gt;diffusers&amp;gt;=0.32.0.dev&lt;/code&gt; to make the &lt;code&gt;SanaPipeline&lt;/code&gt; and &lt;code&gt;SanaPAGPipeline&lt;/code&gt; available!&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/diffusers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Make sure to specify &lt;code&gt;pipe.transformer&lt;/code&gt; to default &lt;code&gt;torch_dtype&lt;/code&gt; and &lt;code&gt;variant&lt;/code&gt; according to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model Card&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Set &lt;code&gt;pipe.text_encoder&lt;/code&gt; to BF16 and &lt;code&gt;pipe.vae&lt;/code&gt; to FP32 or BF16. For more info, &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/sana#sanapipeline&#34;&gt;docs&lt;/a&gt; are here.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers&#xA;import torch&#xA;from diffusers import SanaPipeline&#xA;&#xA;pipe = SanaPipeline.from_pretrained(&#xA;    &#34;Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers&#34;,&#xA;    variant=&#34;bf16&#34;,&#xA;    torch_dtype=torch.bfloat16,&#xA;)&#xA;pipe.to(&#34;cuda&#34;)&#xA;&#xA;pipe.vae.to(torch.bfloat16)&#xA;pipe.text_encoder.to(torch.bfloat16)&#xA;&#xA;prompt = &#39;a cyberpunk cat with a neon sign that says &#34;Sana&#34;&#39;&#xA;image = pipe(&#xA;    prompt=prompt,&#xA;    height=1024,&#xA;    width=1024,&#xA;    guidance_scale=4.5,&#xA;    num_inference_steps=20,&#xA;    generator=torch.Generator(device=&#34;cuda&#34;).manual_seed(42),&#xA;)[0]&#xA;&#xA;image[0].save(&#34;sana.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. How to use &lt;code&gt;SanaPAGPipeline&lt;/code&gt; with &lt;code&gt;üß®diffusers&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers&#xA;import torch&#xA;from diffusers import SanaPAGPipeline&#xA;&#xA;pipe = SanaPAGPipeline.from_pretrained(&#xA;  &#34;Efficient-Large-Model/Sana_1600M_1024px_diffusers&#34;,&#xA;  variant=&#34;fp16&#34;,&#xA;  torch_dtype=torch.float16,&#xA;  pag_applied_layers=&#34;transformer_blocks.8&#34;,&#xA;)&#xA;pipe.to(&#34;cuda&#34;)&#xA;&#xA;pipe.text_encoder.to(torch.bfloat16)&#xA;pipe.vae.to(torch.bfloat16)&#xA;&#xA;prompt = &#39;a cyberpunk cat with a neon sign that says &#34;Sana&#34;&#39;&#xA;image = pipe(&#xA;    prompt=prompt,&#xA;    guidance_scale=5.0,&#xA;    pag_scale=2.0,&#xA;    num_inference_steps=20,&#xA;    generator=torch.Generator(device=&#34;cuda&#34;).manual_seed(42),&#xA;)[0]&#xA;image[0].save(&#39;sana.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;3. How to use Sana in this repo&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from app.sana_pipeline import SanaPipeline&#xA;from torchvision.utils import save_image&#xA;&#xA;device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;generator = torch.Generator(device=device).manual_seed(42)&#xA;&#xA;sana = SanaPipeline(&#34;configs/sana_config/1024ms/Sana_1600M_img1024.yaml&#34;)&#xA;sana.from_pretrained(&#34;hf://Efficient-Large-Model/Sana_1600M_1024px_BF16/checkpoints/Sana_1600M_1024px_BF16.pth&#34;)&#xA;prompt = &#39;a cyberpunk cat with a neon sign that says &#34;Sana&#34;&#39;&#xA;&#xA;image = sana(&#xA;    prompt=prompt,&#xA;    height=1024,&#xA;    width=1024,&#xA;    guidance_scale=5.0,&#xA;    pag_guidance_scale=2.0,&#xA;    num_inference_steps=18,&#xA;    generator=generator,&#xA;)&#xA;save_image(image, &#39;output/sana.png&#39;, nrow=1, normalize=True, value_range=(-1, 1))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;4. Run Sana (Inference) with Docker&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;# Pull related models&#xA;huggingface-cli download google/gemma-2b-it&#xA;huggingface-cli download google/shieldgemma-2b&#xA;huggingface-cli download mit-han-lab/dc-ae-f32c32-sana-1.0&#xA;huggingface-cli download Efficient-Large-Model/Sana_1600M_1024px&#xA;&#xA;# Run with docker&#xA;docker build . -t sana&#xA;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \&#xA;    -v ~/.cache:/root/.cache \&#xA;    sana&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üîõ Run inference with TXT or JSON files&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run samples in a txt file&#xA;python scripts/inference.py \&#xA;      --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;      --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;      --txt_file=asset/samples_mini.txt&#xA;&#xA;# Run samples in a json file&#xA;python scripts/inference.py \&#xA;      --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;      --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;      --json_file=asset/samples_mini.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where each line of &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/samples_mini.txt&#34;&gt;&lt;code&gt;asset/samples_mini.txt&lt;/code&gt;&lt;/a&gt; contains a prompt to generate&lt;/p&gt; &#xA;&lt;h1&gt;üî• 3. How to Train Sana&lt;/h1&gt; &#xA;&lt;h2&gt;üí∞Hardware requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;32GB VRAM is required for both 0.6B and 1.6B model&#39;s training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1). Train with image-text pairs in directory&lt;/h3&gt; &#xA;&lt;p&gt;We provide a training example here and you can also select your desired config file from &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/configs/sana_config&#34;&gt;config files dir&lt;/a&gt; based on your data structure.&lt;/p&gt; &#xA;&lt;p&gt;To launch Sana training, you will first need to prepare data in the following formats. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/example_data&#34;&gt;Here&lt;/a&gt; is an example for the data structure for reference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;asset/example_data&#xA;‚îú‚îÄ‚îÄ AAA.txt&#xA;‚îú‚îÄ‚îÄ AAA.png&#xA;‚îú‚îÄ‚îÄ BCC.txt&#xA;‚îú‚îÄ‚îÄ BCC.png&#xA;‚îú‚îÄ‚îÄ ......&#xA;‚îú‚îÄ‚îÄ CCC.txt&#xA;‚îî‚îÄ‚îÄ CCC.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then Sana&#39;s training can be launched via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example of training Sana 0.6B with 512x512 resolution from scratch&#xA;bash train_scripts/train.sh \&#xA;  configs/sana_config/512ms/Sana_600M_img512.yaml \&#xA;  --data.data_dir=&#34;[asset/example_data]&#34; \&#xA;  --data.type=SanaImgDataset \&#xA;  --model.multi_scale=false \&#xA;  --train.train_batch_size=32&#xA;&#xA;# Example of fine-tuning Sana 1.6B with 1024x1024 resolution&#xA;bash train_scripts/train.sh \&#xA;  configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;  --data.data_dir=&#34;[asset/example_data]&#34; \&#xA;  --data.type=SanaImgDataset \&#xA;  --model.load_from=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;  --model.multi_scale=false \&#xA;  --train.train_batch_size=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2). Train with image-text pairs in directory&lt;/h3&gt; &#xA;&lt;p&gt;We also provide conversion scripts to convert your data to the required format. You can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/data_conversion_scripts&#34;&gt;data conversion scripts&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/convert_ImgDataset_to_WebDatasetMS_format.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then Sana&#39;s training can be launched via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example of training Sana 0.6B with 512x512 resolution from scratch&#xA;bash train_scripts/train.sh \&#xA;  configs/sana_config/512ms/Sana_600M_img512.yaml \&#xA;  --data.data_dir=&#34;[asset/example_data_tar]&#34; \&#xA;  --data.type=SanaWebDatasetMS \&#xA;  --model.multi_scale=true \&#xA;  --train.train_batch_size=32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üíª 4. Metric toolkit&lt;/h1&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/metrics_toolkit.md&#34;&gt;Toolkit Manual&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;üí™To-Do List&lt;/h1&gt; &#xA;&lt;p&gt;We will try our best to release&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[‚úÖ] Training code&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] Inference code&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] Model zoo&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] ComfyUI&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] DC-AE Diffusers&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] Sana merged in Diffusers(&lt;a href=&#34;https://github.com/huggingface/diffusers/pull/9982&#34;&gt;https://github.com/huggingface/diffusers/pull/9982&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] LoRA training by &lt;a href=&#34;https://github.com/sayakpaul&#34;&gt;@paul&lt;/a&gt;(&lt;code&gt;diffusers&lt;/code&gt;: &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/10234&#34;&gt;https://github.com/huggingface/diffusers/pull/10234&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] 2K/4K resolution models.(Thanks &lt;a href=&#34;https://github.com/Fanghua-Yu/SUPIR&#34;&gt;@SUPIR&lt;/a&gt; to provide a 4K super-resolution model)&lt;/li&gt; &#xA; &lt;li&gt;[üíª] ControlNet (train &amp;amp; inference &amp;amp; models)&lt;/li&gt; &#xA; &lt;li&gt;[üíª] 8bit / 4bit Laptop development&lt;/li&gt; &#xA; &lt;li&gt;[üíª] Larger model size&lt;/li&gt; &#xA; &lt;li&gt;[üíª] Better re-construction F32/F64 VAEs.&lt;/li&gt; &#xA; &lt;li&gt;[üíª] &lt;strong&gt;Sana1.5 (Focus on: Human body / Human face / Text rendering / Realism / Efficiency)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ü§óAcknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ±&lt;/a&gt;, &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-sigma&#34;&gt;PixArt-Œ£&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/efficientvit&#34;&gt;Efficient-ViT&lt;/a&gt;, &lt;a href=&#34;https://github.com/city96/ComfyUI_ExtraModels&#34;&gt;ComfyUI_ExtraModels&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; for their wonderful work and codebase!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìñBibTeX&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{xie2024sana,&#xA;      title={Sana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer},&#xA;      author={Enze Xie and Junsong Chen and Junyu Chen and Han Cai and Haotian Tang and Yujun Lin and Zhekai Zhang and Muyang Li and Ligeng Zhu and Yao Lu and Song Han},&#xA;      year={2024},&#xA;      eprint={2410.10629},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2410.10629},&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>