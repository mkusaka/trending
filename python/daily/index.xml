<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-08T01:39:12Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lukas-blecher/LaTeX-OCR</title>
    <updated>2023-11-08T01:39:12Z</updated>
    <id>tag:github.com,2023-11-08:/lukas-blecher/LaTeX-OCR</id>
    <link href="https://github.com/lukas-blecher/LaTeX-OCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;pix2tex: Using a ViT to convert images of equations into LaTeX code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pix2tex - LaTeX OCR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pix2tex.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pix2tex/badge/?version=latest&#34; alt=&#34;Documentation Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pix2tex?logo=pypi&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/pix2tex?logo=pypi&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&amp;amp;logo=github&#34; alt=&#34;GitHub all releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/lukasblecher/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/lukbl/LaTeX-OCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Using the model&lt;/h2&gt; &#xA;&lt;p&gt;To run the model you need Python 3.7+&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t have PyTorch installed. Follow their instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Install the package &lt;code&gt;pix2tex&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;pix2tex[gui]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model checkpoints will be downloaded automatically.&lt;/p&gt; &#xA;&lt;p&gt;There are three ways to get a prediction from an image.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use the command line tool by calling &lt;code&gt;pix2tex&lt;/code&gt;. Here you can parse already existing images from the disk and images in your clipboard.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/katie-lim&#34;&gt;@katie-lim&lt;/a&gt;, you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with &lt;code&gt;latexocr&lt;/code&gt;. From here you can take a screenshot and the predicted latex code is rendered using &lt;a href=&#34;https://www.mathjax.org/&#34;&gt;MathJax&lt;/a&gt; and copied to your clipboard.&lt;/p&gt; &lt;p&gt;Under linux, it is possible to use the GUI with &lt;code&gt;gnome-screenshot&lt;/code&gt; (which comes with multiple monitor support) if &lt;code&gt;gnome-screenshot&lt;/code&gt; was installed beforehand. For Wayland, &lt;code&gt;grim&lt;/code&gt; and &lt;code&gt;slurp&lt;/code&gt; will be used when they are both available. Note that &lt;code&gt;gnome-screenshot&lt;/code&gt; is not compatible with wlroots-based Wayland compositors. Since &lt;code&gt;gnome-screenshot&lt;/code&gt; will be preferred when available, you may have to set the environment variable &lt;code&gt;SCREENSHOT_TOOL&lt;/code&gt; to &lt;code&gt;grim&lt;/code&gt; in this case (other available values are &lt;code&gt;gnome-screenshot&lt;/code&gt; and &lt;code&gt;pil&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &lt;p&gt;If the model is unsure about the what&#39;s in the image it might output a different prediction every time you click &#34;Retry&#34;. With the &lt;code&gt;temperature&lt;/code&gt; parameter you can control this behavior (low temperature will produce the same result).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can use an API. This has additional dependencies. Install via &lt;code&gt;pip install -U &#34;pix2tex[api]&#34;&lt;/code&gt; and run&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pix2tex.api.run&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to start a &lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt; demo that connects to the API at port 8502. There is also a docker image available for the API: &lt;a href=&#34;https://hub.docker.com/r/lukasblecher/pix2tex&#34;&gt;https://hub.docker.com/r/lukasblecher/pix2tex&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/lukasblecher/pix2tex&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker&#34; alt=&#34;Docker Image Size (latest by date)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull lukasblecher/pix2tex:api&#xA;docker run --rm -p 8502:8502 lukasblecher/pix2tex:api&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To also run the streamlit demo run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and navigate to &lt;a href=&#34;http://localhost:8501/&#34;&gt;http://localhost:8501/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use from within Python&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from PIL import Image&#xA;from pix2tex.cli import LatexOCR&#xA;&#xA;img = Image.open(&#39;path/to/image.png&#39;)&#xA;model = LatexOCR()&#xA;print(model(img))&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The model works best with images of smaller resolution. That&#39;s why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it&#39;s not perfect and might not be able to handle huge images optimally, so don&#39;t zoom in all the way before taking a picture.&lt;/p&gt; &#xA;&lt;p&gt;Always double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Want to use the package?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m trying to compile a documentation right now.&lt;/p&gt; &#xA;&lt;p&gt;Visit here: &lt;a href=&#34;https://pix2tex.readthedocs.io/&#34;&gt;https://pix2tex.readthedocs.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training the model &lt;a href=&#34;https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Install a couple of dependencies &lt;code&gt;pip install &#34;pix2tex[train]&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use your own tokenizer pass it via &lt;code&gt;--tokenizer&lt;/code&gt; (See below).&lt;/p&gt; &#xA;&lt;p&gt;You can find my generated training data on the &lt;a href=&#34;https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO&#34;&gt;Google Drive&lt;/a&gt; as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Edit the &lt;code&gt;data&lt;/code&gt; (and &lt;code&gt;valdata&lt;/code&gt;) entry in the config file to the newly generated &lt;code&gt;.pkl&lt;/code&gt; file. Change other hyperparameters if you want to. See &lt;code&gt;pix2tex/model/settings/config.yaml&lt;/code&gt; for a template.&lt;/li&gt; &#xA; &lt;li&gt;Now for the actual training run&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pix2tex.train --config path_to_config_file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use your own data you might be interested in creating your own tokenizer with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t forget to update the path to the tokenizer in the config file and set &lt;code&gt;num_tokens&lt;/code&gt; to your vocabulary size.&lt;/p&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;The model consist of a ViT [&lt;a href=&#34;https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References&#34;&gt;1&lt;/a&gt;] encoder with a ResNet backbone and a Transformer [&lt;a href=&#34;https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References&#34;&gt;2&lt;/a&gt;] decoder.&lt;/p&gt; &#xA;&lt;h3&gt;Performance&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;BLEU score&lt;/th&gt; &#xA;   &lt;th&gt;normed edit distance&lt;/th&gt; &#xA;   &lt;th&gt;token accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;   &lt;td&gt;0.10&lt;/td&gt; &#xA;   &lt;td&gt;0.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Data&lt;/h2&gt; &#xA;&lt;p&gt;We need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. &lt;a href=&#34;https://www.wikipedia.org&#34;&gt;wikipedia&lt;/a&gt;, &lt;a href=&#34;https://www.arxiv.org&#34;&gt;arXiv&lt;/a&gt;. We also use the formulae from the &lt;a href=&#34;https://zenodo.org/record/56198#.V2px0jXT6eA&#34;&gt;im2latex-100k&lt;/a&gt; [&lt;a href=&#34;https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References&#34;&gt;3&lt;/a&gt;] dataset. All of it can be found &lt;a href=&#34;https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Requirements&lt;/h3&gt; &#xA;&lt;p&gt;In order to render the math in many different fonts we use XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ctan.org/pkg/xetex&#34;&gt;XeLaTeX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://imagemagick.org/&#34;&gt;ImageMagick&lt;/a&gt; with &lt;a href=&#34;https://www.ghostscript.com/index.html&#34;&gt;Ghostscript&lt;/a&gt;. (for converting pdf to png)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/&#34;&gt;Node.js&lt;/a&gt; to run &lt;a href=&#34;https://github.com/KaTeX/KaTeX&#34;&gt;KaTeX&lt;/a&gt; (for normalizing Latex code)&lt;/li&gt; &#xA; &lt;li&gt;Python 3.7+ &amp;amp; dependencies (specified in &lt;code&gt;setup.py&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Fonts&lt;/h3&gt; &#xA;&lt;p&gt;Latin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; add more evaluation metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; create a GUI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; add beam search&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; support handwritten formulae (kinda done, see training colab notebook)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; reduce model size (distillation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; find optimal hyperparameters&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; tweak model structure&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; fix data scraping and scrape more data&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; trace the model (&lt;a href=&#34;https://github.com/lukas-blecher/LaTeX-OCR/issues/2&#34;&gt;#2&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions of any kind are welcome.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &#xA;&lt;p&gt;Code taken and modified from &lt;a href=&#34;https://github.com/lucidrains&#34;&gt;lucidrains&lt;/a&gt;, &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34;&gt;rwightman&lt;/a&gt;, &lt;a href=&#34;https://github.com/harvardnlp/im2markup&#34;&gt;im2markup&lt;/a&gt;, &lt;a href=&#34;https://github.com/soskek/arxiv_leaks&#34;&gt;arxiv_leaks&lt;/a&gt;, &lt;a href=&#34;https://github.com/pkra/MathJax-single-file&#34;&gt;pkra: Mathjax&lt;/a&gt;, &lt;a href=&#34;https://github.com/harupy/snipping-tool&#34;&gt;harupy: snipping tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;[1] &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;An Image is Worth 16x16 Words&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[3] &lt;a href=&#34;https://arxiv.org/abs/1609.04938v2&#34;&gt;Image-to-Markup Generation with Coarse-to-Fine Attention&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Genesis-Embodied-AI/RoboGen</title>
    <updated>2023-11-08T01:39:12Z</updated>
    <id>tag:github.com,2023-11-08:/Genesis-Embodied-AI/RoboGen</id>
    <link href="https://github.com/Genesis-Embodied-AI/RoboGen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A generative and self-guided robotic agent that endlessly propose and master new skills.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;500px&#34; src=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/imgs/logo.png&#34;&gt; &#xA; &lt;h1&gt;RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/imgs/teaser.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;http://arxiv.org/abs/2311.01455&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Paper-arXiv-green?style=plastic&amp;amp;logo=arXiv&amp;amp;logoColor=green&#34; alt=&#34;Paper arXiv&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://robogen-ai.github.io/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Project-Page-blue?style=plastic&amp;amp;logo=Google%20chrome&amp;amp;logoColor=blue&#34; alt=&#34;Project Page&#34;&gt; &lt;/a&gt; &lt;/p&gt; This is the official repo for the paper: &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://robogen-ai.github.io/&#34;&gt;RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://yufeiwang63.github.io/&#34;&gt;Yufei Wang*&lt;/a&gt;, &lt;a href=&#34;https://zhou-xian.com/&#34;&gt;Zhou Xian*&lt;/a&gt;, &lt;a href=&#34;https://robogen-ai.github.io/&#34;&gt;Feng Chen*&lt;/a&gt;, &lt;a href=&#34;https://zswang666.github.io/&#34;&gt;Tsun-Hsuan Wang&lt;/a&gt;, &lt;a href=&#34;https://wangyian-me.github.io/&#34;&gt;Yian Wang&lt;/a&gt;, &lt;a href=&#34;https://www.cs.cmu.edu/~katef/&#34;&gt; Katerina Fragkiadaki&lt;/a&gt;, &lt;a href=&#34;https://zackory.com/&#34;&gt;Zackory Erickson&lt;/a&gt;, &lt;a href=&#34;https://davheld.github.io/&#34;&gt;David Held&lt;/a&gt;, &lt;a href=&#34;https://people.csail.mit.edu/ganchuang/&#34;&gt;Chuang Gan&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;RoboGen is a &lt;strong&gt;self-guided&lt;/strong&gt; and &lt;strong&gt;generative&lt;/strong&gt; robotic agent that autonomously proposes &lt;strong&gt;new tasks&lt;/strong&gt;, generates corresponding &lt;strong&gt;environments&lt;/strong&gt;, and acquires &lt;strong&gt;new robotic skills&lt;/strong&gt; continuously.&lt;/p&gt; &#xA;&lt;p&gt;RoboGen is powered by &lt;a href=&#34;https://github.com/Genesis-Embodied-AI/Genesis&#34;&gt;Genesis&lt;/a&gt;, a multi-material multi-solver generative simulation engine for general-purpose robot learning. Genesis is still under active development and will be released soon. This repo contains a re-implementation of RoboGen using PyBullet, containing generation and learning of rigid manipulation and locomotion tasks. Our full pipeline containing soft-body manipulation and more tasks will be released later together with Genesis.&lt;/p&gt; &#xA;&lt;p&gt;We are still in the process of cleaning the code &amp;amp; testing, please stay tuned!&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/#RoboGen&#34;&gt;RoboGen&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/#Open-Motion-Planning-Library&#34;&gt;OMPL&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/#lets-rock&#34;&gt;Let&#39;s Rock!&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/#One-click-for-all&#34;&gt;Automated Task Generation &amp;amp; Skill Learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/Generate-tasks&#34;&gt;Generate Tasks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/Learn-skills&#34;&gt;Learn Skills&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Genesis-Embodied-AI/RoboGen/main/Pre-generated-tasks&#34;&gt;Pre-generated Tasks&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;RoboGen&lt;/h3&gt; &#xA;&lt;p&gt;Clone this git repo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/Genesis-Embodied-AI/RoboGen.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend working with a conda environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yaml&#xA;conda activate robogen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If installing from this yaml file doesn&#39;t work, manual installation of missing packages should also work.&lt;/p&gt; &#xA;&lt;h3&gt;Open Motion Planning Library&lt;/h3&gt; &#xA;&lt;p&gt;RoboGen leverages &lt;a href=&#34;https://ompl.kavrakilab.org/&#34;&gt;Open Motion Planning Library (OMPL)&lt;/a&gt; for motion planning as part of the pipeline to solve the generated task. To install OMPL, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./install_ompl_1.5.2.sh --python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which will install the ompl with system-wide python. Then, export the installation to the conda environment to be used with RoboGen:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;echo &#34;path_to_your_ompl_installation_from_last_step/OMPL/ompl-1.5.2/py-bindings&#34; &amp;gt;&amp;gt; ~/miniconda3/envs/robogen/lib/python3.9/site-packages/ompl.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;remember to change the path to be your ompl installed path and conda environment path.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;p&gt;RoboGen uses &lt;a href=&#34;https://sapien.ucsd.edu/browse&#34;&gt;PartNet-Mobility&lt;/a&gt; for task generation and scene population. We provide a parsed version &lt;a href=&#34;https://drive.google.com/file/d/1d-1txzcg_ke17NkHKAolXlfDnmPePFc6/view?usp=sharing&#34;&gt;here&lt;/a&gt; (which parses the urdf to extract the articulation tree as a shortened input to GPT-4). After downloading, please unzip it and put it in the &lt;code&gt;data&lt;/code&gt; folder, so it looks like &lt;code&gt;data/dataset&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For retrieving objects from objaverse, we embed object descriptions from objaverse using &lt;a href=&#34;https://www.sbert.net/&#34;&gt;SentenceBert&lt;/a&gt;. If you want to generate these embeddings by yourself, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python objaverse_utils/embed_all_annotations.py&#xA;python objaverse_utils/embed_cap3d.py&#xA;python objaverse_utils/embed_partnet_annotations.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide the embeddings &lt;a href=&#34;https://drive.google.com/file/d/1dFDpG3tlckTUSy7VYdfkNqtfVctpn3T6/view?usp=sharing&#34;&gt;here&lt;/a&gt; if generation takes too much time. After downloading, unzip and put it under &lt;code&gt;objaverse_utils/data/&lt;/code&gt; folder, so it looks like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;objaverse_utils/data/default_tag_embeddings_*.pt&#xA;objaverse_utils/data/default_tag_names_*.pt&#xA;objaverse_utils/data/default_tag_uids_*.pt&#xA;objaverse_utils/data/cap3d_sentence_bert_embeddings.pt&#xA;objaverse_utils/data/partnet_mobility_category_embeddings.pt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Let&#39;s Rock!&lt;/h2&gt; &#xA;&lt;h3&gt;One click for all&lt;/h3&gt; &#xA;&lt;p&gt;Put your OpenAI API key at the top of &lt;code&gt;gpt_4/query.py&lt;/code&gt;, and simply run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;RoboGen will then generate the task, build the scene in pybullet, and solve it to learn the corresponding skill.&lt;br&gt; If you wish to generate manipulation tasks relevant to a specific object, e.g., microwave, you can run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --category Microwave&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate tasks&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to just generate the tasks, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --train 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which will generate the tasks, scene config yaml files, and training supervisions. The generated tasks will be stored at &lt;code&gt;data/generated_tasks_release/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Learn skills&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to just learn the skill with a generated task, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python execute.py --task_config_path [PATH_TO_THE_GENERATED_TASK_CONFIG] # for manipulation tasks&#xA;python execute_locomotion.py --task_config_path [PATH_TO_THE_GENERATED_TASK_CONFIG] # for locomotion tasks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python execute.py --task_config_path example_tasks/Change_Lamp_Direction/Change_Lamp_Direction_The_robotic_arm_will_alter_the_lamps_light_direction_by_manipulating_the_lamps_head.yaml  &#xA;python execute_locomotion.py --task_config_path example_tasks/task_Turn_right/Turn_right.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-generated tasks&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;example_tasks&lt;/code&gt; we include a number of generated tasks from RoboGen. We hope this could be useful for, e.g., language conditioned multi-task learning &amp;amp; transfer learning &amp;amp; low-level skill learning. We hope to keep updating the list!&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The interface between OMPL and pybullet is based on &lt;a href=&#34;https://github.com/lyfkyle/pybullet_ompl&#34;&gt;pybullet_ompl&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Part of the objaverse annotations are from &lt;a href=&#34;https://arxiv.org/abs/2306.07279&#34;&gt;Scalable 3D Captioning with Pretrained Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>kylesargent/ZeroNVS</title>
    <updated>2023-11-08T01:39:12Z</updated>
    <id>tag:github.com,2023-11-08:/kylesargent/ZeroNVS</id>
    <link href="https://github.com/kylesargent/ZeroNVS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ZeroNVS&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://kylesargent.github.io/zeronvs/&#34;&gt;Webpage (with video results)&lt;/a&gt; | &lt;a href=&#34;http://arxiv.org/abs/2310.17994&#34;&gt;Paper&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This is the offical code release for ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kylesargent/ZeroNVS/main/zeronvs_teaser.png&#34; alt=&#34;teaser image&#34; title=&#34;ZeroNVS results.&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;What is in this repository: 3D SDS distillation code, evaluation code, trained models&lt;/h3&gt; &#xA;&lt;p&gt;In this repository, we currently provide code to reproduce our main evaluations and also to run ZeroNVS to distill NeRFs from your own images. This includes scripts to reproduce the main metrics on DTU and Mip-NeRF 360 datasets.&lt;/p&gt; &#xA;&lt;h3&gt;How do I train my own diffusion models?&lt;/h3&gt; &#xA;&lt;p&gt;Check out the companion repository, &lt;a href=&#34;https://github.com/kylesargent/zeronvs_diffusion&#34;&gt;https://github.com/kylesargent/zeronvs_diffusion&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;This codebase is heavily built off existing codebases for 3D-aware diffusion model training and 3D SDS distillation, namely &lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Zero-1-to-3&lt;/a&gt; and &lt;a href=&#34;https://github.com/threestudio-project/threestudio&#34;&gt;threestudio&lt;/a&gt;. If you use ZeroNVS, please consider also citing these great contributions.&lt;/p&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;p&gt;The code has been tested on an A100 GPU with 40GB of memory.&lt;/p&gt; &#xA;&lt;p&gt;To get the code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/kylesargent/zeronvs.git&#xA;cd zeronvs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To set up the environment, use the following sequence of commands. The exact setup that will work for you might be platform dependent. Note: it&#39;s normal for installing tiny-cuda-nn to take a long time.&lt;/p&gt; &#xA;&lt;!-- export TCNN_CUDA_ARCHITECTURES=80 --&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n zeronvs python=3.8 pip&#xA;conda activate zeronvs&#xA;&#xA;pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118&#xA;conda install -c &#34;nvidia/label/cuda-11.8.0&#34; cuda-toolkit&#xA;pip install ninja git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&#xA;&#xA;pip install -r requirements-zeronvs.txt&#xA;pip install nerfacc -f https://nerfacc-bucket.s3.us-west-2.amazonaws.com/whl/torch-2.0.0_cu118.html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, be sure to initialize and pull the code in the &lt;code&gt;zeronvs_diffusion&lt;/code&gt; submodule.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd zeronvs_diffusion&#xA;git submodule init&#xA;git submodule update&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Data and models&lt;/h1&gt; &#xA;&lt;p&gt;Since we have experimented with a variety of datasets in ZeroNVS, the codebase consumes a few different types of data formats.&lt;/p&gt; &#xA;&lt;p&gt;To download all the relevant data and models, you can run the following commands within the zeronvs conda environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gdown --fuzzy https://drive.google.com/file/d/1q0oMpp2Vy09-0LA-JXpo_ZoX2PH5j8oP/view?usp=sharing&#xA;gdown --fuzzy https://drive.google.com/file/d/1aTSmJa8Oo2qCc2Ce2kT90MHEA6UTSBKj/view?usp=drive_link&#xA;gdown --fuzzy https://drive.google.com/file/d/17WEMfs2HABJcdf4JmuIM3ti0uz37lSZg/view?usp=sharing&#xA;&#xA;unzip dtu_dataset.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;MipNeRF360 dataset&lt;/h2&gt; &#xA;&lt;p&gt;You can download it &lt;a href=&#34;https://drive.google.com/file/d/1q0oMpp2Vy09-0LA-JXpo_ZoX2PH5j8oP/view?usp=sharing&#34;&gt;here&lt;/a&gt;. Be sure to set the appropriate path in &lt;code&gt;resources.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;DTU dataset&lt;/h2&gt; &#xA;&lt;p&gt;Download it &lt;a href=&#34;https://drive.google.com/file/d/1aTSmJa8Oo2qCc2Ce2kT90MHEA6UTSBKj/view?usp=drive_link&#34;&gt;here&lt;/a&gt; (hosted by the PixelNeRF authors). Be sure to unzip it and then set the approriate path in &lt;code&gt;resources.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Your own images&lt;/h2&gt; &#xA;&lt;p&gt;Store them as 256x256 png images and pass them to &lt;code&gt;launch_inference.sh&lt;/code&gt; (details below).&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;We release our main model, trained with our $\mathbf{M}_{\mathrm{6DoF+1,~viewer}}$ parameterization on CO3D, RealEstate10K, and ACID. You can download it &lt;a href=&#34;https://drive.google.com/file/d/17WEMfs2HABJcdf4JmuIM3ti0uz37lSZg/view?usp=sharing&#34;&gt;here&lt;/a&gt;. We use this one model for all our main results.&lt;/p&gt; &#xA;&lt;h1&gt;Inference&lt;/h1&gt; &#xA;&lt;p&gt;Evaluation is performed by distilling a NeRF for each of the scenes in the dataset. DTU has 15 scenes and the Mip-NeRF 360 dataset has 7 scenes. Since NeRF distillation takes ~3 hours, running the full eval can take quite some time, especially if you only have 1 GPU.&lt;/p&gt; &#xA;&lt;p&gt;Note that you can still achieve good performance with much faster config options; for instance, reduced resolution, batch size, number of training steps, or some combination. The code as-is is just intended to reproduce the results from the paper.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the data and models, you can run the evals via either &lt;code&gt;launch_eval_dtu.sh&lt;/code&gt; or &lt;code&gt;launch_eval_mipnerf360&lt;/code&gt;. The metrics for each scene will be saved in &lt;code&gt;metrics.json&lt;/code&gt; files which you must average to get the final performance.&lt;/p&gt; &#xA;&lt;p&gt;We provide the expected performance for individual scenes in the tables below. Note that there is some randomness inherent in SDS distillation, so you may not get exactly these numbers (though the performance should be quite close, especially on average).&lt;/p&gt; &#xA;&lt;h2&gt;DTU (expected performance)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ssim&lt;/th&gt; &#xA;   &lt;th&gt;psnr&lt;/th&gt; &#xA;   &lt;th&gt;lpips&lt;/th&gt; &#xA;   &lt;th&gt;scene_uid&lt;/th&gt; &#xA;   &lt;th&gt;manual_gt_to_pred_scale&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.6094&lt;/td&gt; &#xA;   &lt;td&gt;13.2329&lt;/td&gt; &#xA;   &lt;td&gt;0.2988&lt;/td&gt; &#xA;   &lt;td&gt;8.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.1739&lt;/td&gt; &#xA;   &lt;td&gt;8.4278&lt;/td&gt; &#xA;   &lt;td&gt;0.5783&lt;/td&gt; &#xA;   &lt;td&gt;21.0&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.6311&lt;/td&gt; &#xA;   &lt;td&gt;14.1864&lt;/td&gt; &#xA;   &lt;td&gt;0.2332&lt;/td&gt; &#xA;   &lt;td&gt;30.0&lt;/td&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.2992&lt;/td&gt; &#xA;   &lt;td&gt;8.9569&lt;/td&gt; &#xA;   &lt;td&gt;0.5117&lt;/td&gt; &#xA;   &lt;td&gt;31.0&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.3862&lt;/td&gt; &#xA;   &lt;td&gt;14.049&lt;/td&gt; &#xA;   &lt;td&gt;0.3611&lt;/td&gt; &#xA;   &lt;td&gt;34.0&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.3495&lt;/td&gt; &#xA;   &lt;td&gt;12.6771&lt;/td&gt; &#xA;   &lt;td&gt;0.4659&lt;/td&gt; &#xA;   &lt;td&gt;38.0&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.4612&lt;/td&gt; &#xA;   &lt;td&gt;12.2447&lt;/td&gt; &#xA;   &lt;td&gt;0.3729&lt;/td&gt; &#xA;   &lt;td&gt;40.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.4657&lt;/td&gt; &#xA;   &lt;td&gt;12.5998&lt;/td&gt; &#xA;   &lt;td&gt;0.3794&lt;/td&gt; &#xA;   &lt;td&gt;41.0&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.369&lt;/td&gt; &#xA;   &lt;td&gt;11.241&lt;/td&gt; &#xA;   &lt;td&gt;0.4441&lt;/td&gt; &#xA;   &lt;td&gt;45.0&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.4456&lt;/td&gt; &#xA;   &lt;td&gt;17.0177&lt;/td&gt; &#xA;   &lt;td&gt;0.4322&lt;/td&gt; &#xA;   &lt;td&gt;55.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.5724&lt;/td&gt; &#xA;   &lt;td&gt;12.6056&lt;/td&gt; &#xA;   &lt;td&gt;0.2639&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.5384&lt;/td&gt; &#xA;   &lt;td&gt;12.1564&lt;/td&gt; &#xA;   &lt;td&gt;0.2725&lt;/td&gt; &#xA;   &lt;td&gt;82.0&lt;/td&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.5434&lt;/td&gt; &#xA;   &lt;td&gt;16.0902&lt;/td&gt; &#xA;   &lt;td&gt;0.3811&lt;/td&gt; &#xA;   &lt;td&gt;103.0&lt;/td&gt; &#xA;   &lt;td&gt;1.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.6353&lt;/td&gt; &#xA;   &lt;td&gt;19.5588&lt;/td&gt; &#xA;   &lt;td&gt;0.349&lt;/td&gt; &#xA;   &lt;td&gt;110.0&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.5529&lt;/td&gt; &#xA;   &lt;td&gt;18.2336&lt;/td&gt; &#xA;   &lt;td&gt;0.3613&lt;/td&gt; &#xA;   &lt;td&gt;114.0&lt;/td&gt; &#xA;   &lt;td&gt;1.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Mip-NeRF 360 (expected performance)&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ssim&lt;/th&gt; &#xA;   &lt;th&gt;psnr&lt;/th&gt; &#xA;   &lt;th&gt;lpips&lt;/th&gt; &#xA;   &lt;th&gt;scene_uid&lt;/th&gt; &#xA;   &lt;th&gt;manual_gt_to_pred_scale&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.1707&lt;/td&gt; &#xA;   &lt;td&gt;13.184&lt;/td&gt; &#xA;   &lt;td&gt;0.6536&lt;/td&gt; &#xA;   &lt;td&gt;bicycle&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.3164&lt;/td&gt; &#xA;   &lt;td&gt;13.1137&lt;/td&gt; &#xA;   &lt;td&gt;0.6122&lt;/td&gt; &#xA;   &lt;td&gt;bonsai&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.2473&lt;/td&gt; &#xA;   &lt;td&gt;12.2189&lt;/td&gt; &#xA;   &lt;td&gt;0.6823&lt;/td&gt; &#xA;   &lt;td&gt;counter&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.207&lt;/td&gt; &#xA;   &lt;td&gt;15.2817&lt;/td&gt; &#xA;   &lt;td&gt;0.5366&lt;/td&gt; &#xA;   &lt;td&gt;garden&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.254&lt;/td&gt; &#xA;   &lt;td&gt;13.2983&lt;/td&gt; &#xA;   &lt;td&gt;0.6245&lt;/td&gt; &#xA;   &lt;td&gt;kitchen&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.3431&lt;/td&gt; &#xA;   &lt;td&gt;11.8591&lt;/td&gt; &#xA;   &lt;td&gt;0.5928&lt;/td&gt; &#xA;   &lt;td&gt;room&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0.1396&lt;/td&gt; &#xA;   &lt;td&gt;13.124&lt;/td&gt; &#xA;   &lt;td&gt;0.6717&lt;/td&gt; &#xA;   &lt;td&gt;stump&lt;/td&gt; &#xA;   &lt;td&gt;1.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Running on your own images&lt;/h2&gt; &#xA;&lt;p&gt;Use the script &lt;code&gt;launch_inference.sh&lt;/code&gt;. You will need to specify the image path, field-of-view, camera elevation, and content scale. These don&#39;t need to be exact, but badly wrong values will cause convergence failure.&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you use ZeroNVS, please cite via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{zeronvs,&#xA;    author = {Sargent, Kyle and Li, Zizhang and Shah, Tanmay and Herrmann, Charles and Yu, Hong-Xing and Zhang, Yunzhi and Chan, Eric Ryan and Lagun, Dmitry and Fei-Fei, Li and Sun, Deqing and Wu, Jiajun},       &#xA;    title = {{ZeroNVS}: Zero-Shot 360-Degree View Synthesis from a Single Real Image},&#xA;    journal={arXiv preprint arXiv:2310.17994},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>