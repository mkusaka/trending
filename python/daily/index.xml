<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-28T01:37:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openvinotoolkit/open_model_zoo</title>
    <updated>2022-10-28T01:37:58Z</updated>
    <id>tag:github.com,2022-10-28:/openvinotoolkit/open_model_zoo</id>
    <link href="https://github.com/openvinotoolkit/open_model_zoo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pre-trained Deep Learning models and demos (high quality and extremely fast)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/index.html&#34;&gt;OpenVINO™ Toolkit&lt;/a&gt; - Open Model Zoo repository&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/open_model_zoo/releases/tag/2022.2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-2022.2.0-green.svg?sanitize=true&#34; alt=&#34;Stable release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gitter.im/open_model_zoo/community&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/gitterHQ/gitter.png&#34; alt=&#34;Gitter chat&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Apache License Version 2.0&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository includes optimized deep learning models and a set of demos to expedite development of high-performance deep learning inference applications. Use these free pre-trained models instead of training your own models to speed-up the development and production deployment process.&lt;/p&gt; &#xA;&lt;p&gt;Intel is committed to the respect of human rights and avoiding complicity in human rights abuses, a policy reflected in the &lt;a href=&#34;https://www.intel.com/content/www/us/en/policy/policy-human-rights.html&#34;&gt;Intel Global Human Rights Principles&lt;/a&gt;. Accordingly, by accessing the Intel material on this platform you agree that you will not use the material in a product or application that causes or contributes to a violation of an internationally recognized human right.&lt;/p&gt; &#xA;&lt;h2&gt;Repository Components:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/models/intel/index.md&#34;&gt;Intel Pre-Trained Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/models/public/index.md&#34;&gt;Public Pre-Trained Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/tools/model_tools/README.md&#34;&gt;Model Downloader&lt;/a&gt; and other automation tools&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/demos/README.md&#34;&gt;Demos&lt;/a&gt; that demonstrate models usage with OpenVINO™ Toolkit&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/tools/accuracy_checker/README.md&#34;&gt;Accuracy Checker&lt;/a&gt; tool for models accuracy validation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Open Model Zoo is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/LICENSE&#34;&gt;Apache License Version 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Online Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/articles/OpenVINO-RelNotes&#34;&gt;OpenVINO™ Release Notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/model_zoo.html&#34;&gt;Pre-Trained Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.openvino.ai/latest/omz_demos.html&#34;&gt;Demos and Samples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other Usage Examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/open-visual-cloud.html&#34;&gt;Open Visual Cloud&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/OpenVisualCloud/Ad-Insertion-Sample/wiki/Tutorial:-Running-AD-Insertion-on-Public-Cloud&#34;&gt;Tutorial: Running AD Insertion on Public Cloud&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/OpenVisualCloud/Ad-Insertion-Sample&#34;&gt;GitHub Repo for Ad Insertion Sample&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/incluit/OpenVino-For-SmartCity&#34;&gt;OpenVINO for Smart City&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/incluit/OpenVino-Driver-Behaviour&#34;&gt;OpenVINO Driver Behavior&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/openvino_notebooks/raw/main/README.md&#34;&gt;OpenVINO Python* Notebooks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;We welcome community contributions to the Open Model Zoo repository. If you have an idea how to improve the product, please share it with us doing the following steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure you can build the product and run all the demos with your patch.&lt;/li&gt; &#xA; &lt;li&gt;In case of a larger feature, provide a relevant demo.&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request at &lt;a href=&#34;https://github.com/openvinotoolkit/open_model_zoo/pulls&#34;&gt;https://github.com/openvinotoolkit/open_model_zoo/pulls&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can find additional information about model contribution &lt;a href=&#34;https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We will review your contribution and, if any additional fixes or modifications are needed, may give you feedback to guide you. When accepted, your pull request will be merged into the GitHub* repositories.&lt;/p&gt; &#xA;&lt;p&gt;Open Model Zoo is licensed under Apache License, Version 2.0. By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Please report questions, issues and suggestions using:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/search?q=%23open_model_zoo&#34;&gt;#open_model_zoo&lt;/a&gt; tag on StackOverflow*&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openvinotoolkit/open_model_zoo/issues&#34;&gt;GitHub* Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit&#34;&gt;Forum&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitter.im/open_model_zoo/community&#34;&gt;Gitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;* Other names and brands may be claimed as the property of others.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pyca/cryptography</title>
    <updated>2022-10-28T01:37:58Z</updated>
    <id>tag:github.com,2022-10-28:/pyca/cryptography</id>
    <link href="https://github.com/pyca/cryptography" rel="alternate"></link>
    <summary type="html">&lt;p&gt;cryptography is a package designed to expose cryptographic primitives and recipes to Python developers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pyca/cryptography&lt;/h1&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://img.shields.io/pypi/v/cryptography.svg&#34;&gt;https://img.shields.io/pypi/v/cryptography.svg&lt;/a&gt; :target: &lt;a href=&#34;https://pypi.org/project/cryptography/&#34;&gt;https://pypi.org/project/cryptography/&lt;/a&gt; :alt: Latest Version&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://readthedocs.org/projects/cryptography/badge/?version=latest&#34;&gt;https://readthedocs.org/projects/cryptography/badge/?version=latest&lt;/a&gt; :target: &lt;a href=&#34;https://cryptography.io&#34;&gt;https://cryptography.io&lt;/a&gt; :alt: Latest Docs&lt;/p&gt; &#xA;&lt;p&gt;.. image:: &lt;a href=&#34;https://github.com/pyca/cryptography/workflows/CI/badge.svg?branch=main&#34;&gt;https://github.com/pyca/cryptography/workflows/CI/badge.svg?branch=main&lt;/a&gt; :target: &lt;a href=&#34;https://github.com/pyca/cryptography/actions?query=workflow%3ACI+branch%3Amain&#34;&gt;https://github.com/pyca/cryptography/actions?query=workflow%3ACI+branch%3Amain&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;cryptography&lt;/code&gt; is a package which provides cryptographic recipes and primitives to Python developers. Our goal is for it to be your &#34;cryptographic standard library&#34;. It supports Python 3.6+ and PyPy3 7.2+.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;cryptography&lt;/code&gt; includes both high level recipes and low level interfaces to common cryptographic algorithms such as symmetric ciphers, message digests, and key derivation functions. For example, to encrypt something with &lt;code&gt;cryptography&lt;/code&gt;&#39;s high level symmetric encryption recipe:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: pycon&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from cryptography.fernet import Fernet&#xA;&amp;gt;&amp;gt;&amp;gt; # Put this somewhere safe!&#xA;&amp;gt;&amp;gt;&amp;gt; key = Fernet.generate_key()&#xA;&amp;gt;&amp;gt;&amp;gt; f = Fernet(key)&#xA;&amp;gt;&amp;gt;&amp;gt; token = f.encrypt(b&#34;A really secret message. Not for prying eyes.&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; token&#xA;&#39;...&#39;&#xA;&amp;gt;&amp;gt;&amp;gt; f.decrypt(token)&#xA;&#39;A really secret message. Not for prying eyes.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find more information in the &lt;code&gt;documentation&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;You can install &lt;code&gt;cryptography&lt;/code&gt; with:&lt;/p&gt; &#xA;&lt;p&gt;.. code-block:: console&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ pip install cryptography&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For full details see &lt;code&gt;the installation documentation&lt;/code&gt;_.&lt;/p&gt; &#xA;&lt;p&gt;Discussion&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;If you run into bugs, you can file them in our `issue tracker`_.&#xA;&#xA;We maintain a `cryptography-dev`_ mailing list for development discussion.&#xA;&#xA;You can also join ``#pyca`` on ``irc.libera.chat`` to ask questions or get&#xA;involved.&#xA;&#xA;Security&#xA;~~~~~~~~&#xA;&#xA;Need to report a security issue? Please consult our `security reporting`_&#xA;documentation.&#xA;&#xA;&#xA;.. _`documentation`: https://cryptography.io/&#xA;.. _`the installation documentation`: https://cryptography.io/en/latest/installation/&#xA;.. _`issue tracker`: https://github.com/pyca/cryptography/issues&#xA;.. _`cryptography-dev`: https://mail.python.org/mailman/listinfo/cryptography-dev&#xA;.. _`security reporting`: https://cryptography.io/en/latest/security/&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>bumble-tech/private-detector</title>
    <updated>2022-10-28T01:37:58Z</updated>
    <id>tag:github.com,2022-10-28:/bumble-tech/private-detector</id>
    <link href="https://github.com/bumble-tech/private-detector" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bumble&#39;s Private Detector - a pretrained model for detecting lewd images&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Private Detector&lt;/h1&gt; &#xA;&lt;p&gt;This is the repo for Bumble&#39;s &lt;em&gt;Private Detector&lt;/em&gt;™ model - an image classifier that can detect lewd images.&lt;/p&gt; &#xA;&lt;p&gt;The internal repo has been heavily refactored and released as a fully open-source project to allow for the wider community to use and finetune a Private Detector model of their own. You can download the pretrained SavedModel and checkpoint &lt;a href=&#34;https://storage.googleapis.com/private_detector/private_detector.zip&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;The SavedModel can be found in &lt;code&gt;saved_model/&lt;/code&gt; within &lt;code&gt;private_detector.zip&lt;/code&gt; above&lt;/p&gt; &#xA;&lt;p&gt;The model is based on Efficientnet-v2 and trained on our internal dataset of lewd images - more information can be found at the whitepaper &lt;a href=&#34;https://bumble.com/en/the-buzz/bumble-open-source-private-detector-ai-cyberflashing-dick-pics&#34;&gt;here&lt;/a&gt; or &lt;a href=&#34;https://medium.com/bumble-tech/bumble-inc-open-sources-private-detector-and-makes-another-step-towards-a-safer-internet-for-women-8e6cdb111d81&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Inference is pretty simple and an example has been given in &lt;code&gt;inference.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 inference.py \&#xA;    --model saved_model/ \&#xA;    --image_paths \&#xA;        Yes_samples/1.jpg \&#xA;        Yes_samples/2.jpg \&#xA;        Yes_samples/3.jpg \&#xA;        Yes_samples/4.jpg \&#xA;        Yes_samples/5.jpg \&#xA;        No_samples/1.jpg \&#xA;        No_samples/2.jpg \&#xA;        No_samples/3.jpg \&#xA;        No_samples/4.jpg \&#xA;        No_samples/5.jpg \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Sample Output&lt;/summary&gt; &#xA; &lt;code&gt; &lt;pre&gt;&lt;code&gt;Probability: 93.71% - Yes_samples/1.jpg&#xA;Probability: 93.43% - Yes_samples/2.jpg&#xA;Probability: 94.06% - Yes_samples/3.jpg&#xA;Probability: 94.08% - Yes_samples/4.jpg&#xA;Probability: 91.01% - Yes_samples/5.jpg&#xA;Probability: 9.76% - No_samples/1.jpg&#xA;Probability: 7.14% - No_samples/2.jpg&#xA;Probability: 8.83% - No_samples/3.jpg&#xA;Probability: 4.87% - No_samples/4.jpg&#xA;Probability: 5.29% - No_samples/5.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/code&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Additional Training&lt;/h2&gt; &#xA;&lt;p&gt;You can finetune the model yourself on your own data, to do so is fairly simple - though you will need the checkpoint files as can be found in &lt;code&gt;saved_checkpoint/&lt;/code&gt; in &lt;code&gt;private_detector.zip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Set up a JSON file with links to your image path lists for each class:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;Yes&#34;: {&#xA;        &#34;path&#34;: &#34;/home/sofarrell/private_detector/Yes.txt&#34;,&#xA;        &#34;label&#34;: 0&#xA;    },&#xA;    &#34;No&#34;: {&#xA;         &#34;path&#34;: &#34;/home/sofarrell/private_detector/No.txt&#34;,&#xA;         &#34;label&#34;: 1&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With each &lt;code&gt;.txt&lt;/code&gt; file listing off the image paths to your images&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;/home/sofarrell/private_detector_images/Yes/1093840880_309463828.jpg&#xA;/home/sofarrell/private_detector_images/Yes/657954182_3459624.jpg&#xA;/home/sofarrell/private_detector_images/Yes/1503714421_3048734.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can create the training environment with conda:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;conda env create -f environment.yaml&#xA;conda activate private_detector&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then retrain like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 ./train.py \&#xA;    --train_json /home/sofarrell/private_detector/train_classes.json \&#xA;    --eval_json /home/sofarrell/private_detector/eval_classes.json \&#xA;    --checkpoint_dir saved_checkpoint/ \&#xA;    --train_id retrained_private_detector&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The training script has several parameters that can be tweaked:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;train_id&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ID for this particular training run&lt;/td&gt; &#xA;   &lt;td&gt;str&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;train_json&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JSON file(s) which describes classes and contains lists of filenames of data files&lt;/td&gt; &#xA;   &lt;td&gt;List[str]&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;eval_json&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Validation json file which describes classes and contains lists of filenames of data files&lt;/td&gt; &#xA;   &lt;td&gt;str&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;num_epochs&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of epochs to train for&lt;/td&gt; &#xA;   &lt;td&gt;int&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of images to process in a batch&lt;/td&gt; &#xA;   &lt;td&gt;int&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;64&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;checkpoint_dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to store checkpoints in&lt;/td&gt; &#xA;   &lt;td&gt;str&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;model_dir&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to store graph in&lt;/td&gt; &#xA;   &lt;td&gt;str&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;.&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;data_format&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Data format: [channels_first, channels_last]&lt;/td&gt; &#xA;   &lt;td&gt;str&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;channels_last&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;initial_learning_rate&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Initial learning rate&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1e-4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;min_learning_rate&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Minimal learning rate&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1e-6&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;min_eval_metric&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Minimal evaluation metric to start saving models&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.01&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;float_dtype&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Float Dtype to use in image tensors: [16, 32]&lt;/td&gt; &#xA;   &lt;td&gt;int&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;steps_per_train_epoch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of steps per train epoch&lt;/td&gt; &#xA;   &lt;td&gt;int&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;800&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;steps_per_eval_epoch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of steps per evaluation epoch&lt;/td&gt; &#xA;   &lt;td&gt;int&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reset_on_lr_update&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Whether to reset to the best model after learning rate update&lt;/td&gt; &#xA;   &lt;td&gt;bool&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;rotation_augmentation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Rotation augmentation angle, value &amp;lt;= 0 disables it&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;use_augmentation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Add speckle, v0, random or color distortion augmentation&lt;/td&gt; &#xA;   &lt;td&gt;str&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;scale_crop_augmentation&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Resize image to the model&#39;s size times this scale and then randomly crop needed size&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;1.4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;reg_loss_weight&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;L2 regularization weight&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;skip_saving_epochs&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Do not save good checkpoint and update best metric for this number of the first epochs&lt;/td&gt; &#xA;   &lt;td&gt;int&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;sequential&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use sequential run over randomly shuffled filenames vs equal sampling from each class&lt;/td&gt; &#xA;   &lt;td&gt;bool&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;eval_threshold&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Threshold above which to consider a prediction positive for evaluation&lt;/td&gt; &#xA;   &lt;td&gt;float&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;epochs_lr_update&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum number of epochs without improvement used to reset/decrease learning rate&lt;/td&gt; &#xA;   &lt;td&gt;int&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;20&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>