<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-23T01:34:50Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bytedance/monolith</title>
    <updated>2024-12-23T01:34:50Z</updated>
    <id>tag:github.com,2024-12-23:/bytedance/monolith</id>
    <link href="https://github.com/bytedance/monolith" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ByteDance&#39;s Recommendation System&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Monolith&lt;/p&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.07663&#34;&gt;Monolith&lt;/a&gt; is a deep learning framework for large scale recommendation modeling. It introduces two important features which are crucial for advanced recommendation system:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;collisionless embedding tables guarantees unique represeantion for different id features&lt;/li&gt; &#xA; &lt;li&gt;real time training captures the latest hotspots and help users to discover new intersts rapidly&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Monolith is built on the top of TensorFlow and supports batch/real-time training and serving.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion Group&lt;/h2&gt; &#xA;&lt;h3&gt;Join us at Discord&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/QYTDeKxGMX&#34;&gt;https://discord.gg/QYTDeKxGMX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;p&gt;Currently, we only support compilation on the Linux.&lt;/p&gt; &#xA;&lt;p&gt;First, download bazel 3.1.0&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/bazelbuild/bazel/releases/download/3.1.0/bazel-3.1.0-installer-linux-x86_64.sh &amp;amp;&amp;amp; \&#xA;  chmod +x bazel-3.1.0-installer-linux-x86_64.sh &amp;amp;&amp;amp; \&#xA;  ./bazel-3.1.0-installer-linux-x86_64.sh &amp;amp;&amp;amp; \&#xA;  rm bazel-3.1.0-installer-linux-x86_64.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, prepare a python environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U --user pip numpy wheel packaging requests opt_einsum&#xA;pip install -U --user keras_preprocessing --no-deps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, you can build any target in the monolith. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bazel run //monolith/native_training:demo --output_filter=IGNORE_LOGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Demo and tutorials&lt;/h3&gt; &#xA;&lt;p&gt;There are a tutorial in &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/monolith/master/markdown/demo&#34;&gt;markdown/demo&lt;/a&gt; on how to run distributed async training, and few guides on how to use the &lt;code&gt;MonolithModel&lt;/code&gt; API &lt;a href=&#34;https://raw.githubusercontent.com/bytedance/monolith/master/markdown&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>codelion/optillm</title>
    <updated>2024-12-23T01:34:50Z</updated>
    <id>tag:github.com,2024-12-23:/codelion/optillm</id>
    <link href="https://github.com/codelion/optillm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Optimizing inference proxy for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;optillm&lt;/h1&gt; &#xA;&lt;p&gt;optillm is an OpenAI API compatible optimizing inference proxy which implements several state-of-the-art techniques that can improve the accuracy and performance of LLMs. The current focus is on implementing techniques that improve reasoning over coding, logical and mathematical queries. It is possible to beat the frontier models using these techniques across diverse tasks by doing additional compute at inference time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/codelion/optillm&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1SpuUb8d9xAoTh32M-9wJsB50AOH54EaH?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/codelion/optillm/discussions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/discussions/codelion/optillm&#34; alt=&#34;GitHub Discussions&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Using pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install optillm&#xA;optillm             &#xA;2024-10-22 07:45:05,612 - INFO - Loaded plugin: privacy&#xA;2024-10-22 07:45:06,293 - INFO - Loaded plugin: memory&#xA;2024-10-22 07:45:06,293 - INFO - Starting server with approach: auto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install from source&lt;/h3&gt; &#xA;&lt;p&gt;Clone the repository with &lt;code&gt;git&lt;/code&gt; and use &lt;code&gt;pip install&lt;/code&gt; to setup the dependencies.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/codelion/optillm.git&#xA;cd optillm&#xA;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set up the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable (for OpenAI) or the &lt;code&gt;AZURE_OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;AZURE_API_VERSION&lt;/code&gt; and &lt;code&gt;AZURE_API_BASE&lt;/code&gt; environment variables (for Azure OpenAI) or the &lt;code&gt;AZURE_API_VERSION&lt;/code&gt; and &lt;code&gt;AZURE_API_BASE&lt;/code&gt; environment variables and login using &lt;code&gt;az login&lt;/code&gt; for Azure OpenAI with managed identity (see &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You can then run the optillm proxy as follows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python optillm.py&#xA;2024-09-06 07:57:14,191 - INFO - Starting server with approach: auto&#xA;2024-09-06 07:57:14,191 - INFO - Server configuration: {&#39;approach&#39;: &#39;auto&#39;, &#39;mcts_simulations&#39;: 2, &#39;mcts_exploration&#39;: 0.2, &#39;mcts_depth&#39;: 1, &#39;best_of_n&#39;: 3, &#39;model&#39;: &#39;gpt-4o-mini&#39;, &#39;rstar_max_depth&#39;: 3, &#39;rstar_num_rollouts&#39;: 5, &#39;rstar_c&#39;: 1.4, &#39;base_url&#39;: &#39;&#39;}&#xA; * Serving Flask app &#39;optillm&#39;&#xA; * Debug mode: off&#xA;2024-09-06 07:57:14,212 - INFO - WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.&#xA; * Running on all addresses (0.0.0.0)&#xA; * Running on http://127.0.0.1:8000&#xA; * Running on http://192.168.10.48:8000&#xA;2024-09-06 07:57:14,212 - INFO - Press CTRL+C to quit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Once the proxy is running, you can use it as a drop in replacement for an OpenAI client by setting the &lt;code&gt;base_url&lt;/code&gt; as &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;from openai import OpenAI&#xA;&#xA;OPENAI_KEY = os.environ.get(&#34;OPENAI_API_KEY&#34;)&#xA;OPENAI_BASE_URL = &#34;http://localhost:8000/v1&#34;&#xA;client = OpenAI(api_key=OPENAI_KEY, base_url=OPENAI_BASE_URL)&#xA;&#xA;response = client.chat.completions.create(&#xA;  model=&#34;moa-gpt-4o&#34;,&#xA;  messages=[&#xA;    {&#xA;      &#34;role&#34;: &#34;user&#34;,&#xA;      &#34;content&#34;: &#34;Write a Python program to build an RL model to recite text from any position that the user provides, using only numpy.&#34;&#xA;    }&#xA;  ],&#xA;  temperature=0.2&#xA;)&#xA;&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The code above applies to both OpenAI and Azure OpenAI, just remember to populate the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; env variable with the proper key. There are multiple ways to control the optimization techniques, they are applied in the follow order of preference:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can control the technique you use for optimization by prepending the slug to the model name &lt;code&gt;{slug}-model-name&lt;/code&gt;. E.g. in the above code we are using &lt;code&gt;moa&lt;/code&gt; or mixture of agents as the optimization approach. In the proxy logs you will see the following showing the &lt;code&gt;moa&lt;/code&gt; is been used with the base model as &lt;code&gt;gpt-4o-mini&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;2024-09-06 08:35:32,597 - INFO - Using approach moa, with gpt-4o-mini&#xA;2024-09-06 08:35:35,358 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions &#34;HTTP/1.1 200 OK&#34;&#xA;2024-09-06 08:35:39,553 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions &#34;HTTP/1.1 200 OK&#34;&#xA;2024-09-06 08:35:44,795 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions &#34;HTTP/1.1 200 OK&#34;&#xA;2024-09-06 08:35:44,797 - INFO - 127.0.0.1 - - [06/Sep/2024 08:35:44] &#34;POST /v1/chat/completions HTTP/1.1&#34; 200 -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Or, you can pass the slug in the &lt;code&gt;optillm_approach&lt;/code&gt; field in the &lt;code&gt;extra_body&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;response = client.chat.completions.create(&#xA;  model=&#34;gpt-4o-mini&#34;,&#xA;  messages=[{ &#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;&#34; }],&#xA;  temperature=0.2,&#xA;  extra_body={&#34;optillm_approach&#34;: &#34;bon|moa|mcts&#34;}&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Or, you can just mention the approach in either your &lt;code&gt;system&lt;/code&gt; or &lt;code&gt;user&lt;/code&gt; prompt, within &lt;code&gt;&amp;lt;optillm_approach&amp;gt; &amp;lt;/optillm_approach&amp;gt;&lt;/code&gt; tags.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;response = client.chat.completions.create(&#xA;  model=&#34;gpt-4o-mini&#34;,&#xA;  messages=[{ &#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;&amp;lt;optillm_approach&amp;gt;re2&amp;lt;/optillm_approach&amp;gt; How many r&#39;s are there in strawberry?&#34; }],&#xA;  temperature=0.2&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You can also combine different techniques either by using symbols &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt;. When you use &lt;code&gt;&amp;amp;&lt;/code&gt; the techniques are processed in the order from left to right in a pipeline with response from previous stage used as request to the next. While, with &lt;code&gt;|&lt;/code&gt; we run all the requests in parallel and generate multiple responses that are returned as a list.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Please note that the convention described above works only when the optillm server has been started with inference approach set to &lt;code&gt;auto&lt;/code&gt;. Otherwise, the &lt;code&gt;model&lt;/code&gt; attribute in the client request must be set with the model name only.&lt;/p&gt; &#xA;&lt;p&gt;We now suport all LLM providers (by wrapping around the &lt;a href=&#34;https://docs.litellm.ai/docs/#litellm-python-sdk&#34;&gt;LiteLLM sdk&lt;/a&gt;). E.g. you can use the Gemini Flash model with &lt;code&gt;moa&lt;/code&gt; by setting passing the api key in the environment variable &lt;code&gt;os.environ[&#39;GEMINI_API_KEY&#39;]&lt;/code&gt; and then calling the model &lt;code&gt;moa-gemini/gemini-1.5-flash-002&lt;/code&gt;. In the output you will then see that LiteLLM is being used to call the base model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;9:43:21 - LiteLLM:INFO: utils.py:2952 - &#xA;LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini&#xA;2024-09-29 19:43:21,011 - INFO - &#xA;LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini&#xA;2024-09-29 19:43:21,481 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-002:generateContent?key=[redacted] &#34;HTTP/1.1 200 OK&#34;&#xA;19:43:21 - LiteLLM:INFO: utils.py:988 - Wrapper: Completed Call, calling success_handler&#xA;2024-09-29 19:43:21,483 - INFO - Wrapper: Completed Call, calling success_handler&#xA;19:43:21 - LiteLLM:INFO: utils.py:2952 - &#xA;LiteLLM completion() model= gemini-1.5-flash-002; provider = gemini&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] optillm is a transparent proxy and will work with any LLM API or provider that has an OpenAI API compatible chat completions endpoint, and in turn, optillm also exposes the same OpenAI API compatible chat completions endpoint. This should allow you to integrate it into any existing tools or frameworks easily. If the LLM you want to use doesn&#39;t have an OpenAI API compatible endpoint (like Google or Anthropic) you can use &lt;a href=&#34;https://docs.litellm.ai/docs/proxy/quick_start&#34;&gt;LiteLLM proxy server&lt;/a&gt; that supports most LLMs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The following sequence diagram illustrates how the request and responses go through optillm.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/codelion/optillm/main/optillm-sequence-diagram.png&#34; alt=&#34;Sequance diagram showing optillm in use&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the diagram:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;A&lt;/code&gt; is an existing tool (like &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/&#34;&gt;oobabooga&lt;/a&gt;), framework (like &lt;a href=&#34;https://github.com/patched-codes/patchwork&#34;&gt;patchwork&lt;/a&gt;) or your own code where you want to use the results from optillm. You can use it directly using any OpenAI client sdk.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;B&lt;/code&gt; is the optillm service (running directly or in a docker container) that will send requests to the &lt;code&gt;base_url&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;C&lt;/code&gt; is any service providing an OpenAI API compatible chat completions endpoint.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Local inference server&lt;/h3&gt; &#xA;&lt;p&gt;We support loading any HuggingFace model or LoRA directly in optillm. To use the built-in inference server set the &lt;code&gt;OPTILLM_API_KEY&lt;/code&gt; to any value (e.g. &lt;code&gt;export OPTILLM_API_KEY=&#34;optillm&#34;&lt;/code&gt;) and then use the same in your OpenAI client. You can pass any HuggingFace model in model field. If it is a private model make sure you set the &lt;code&gt;HF_TOKEN&lt;/code&gt; environment variable with your HuggingFace key. We also support adding any number of LoRAs on top of the model by using the &lt;code&gt;+&lt;/code&gt; separator.&lt;/p&gt; &#xA;&lt;p&gt;E.g. The following code loads the base model &lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt; and then adds two LoRAs on top - &lt;code&gt;patched-codes/Llama-3.2-1B-FixVulns&lt;/code&gt; and &lt;code&gt;patched-codes/Llama-3.2-1B-FastApply&lt;/code&gt;. You can specify which LoRA to use using the &lt;code&gt;active_adapter&lt;/code&gt; param in &lt;code&gt;extra_args&lt;/code&gt; field of OpenAI SDK client. By default we will load the last specified adapter.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;OPENAI_BASE_URL = &#34;http://localhost:8000/v1&#34;&#xA;OPENAI_KEY = &#34;optillm&#34;&#xA;response = client.chat.completions.create(&#xA;  model=&#34;meta-llama/Llama-3.2-1B-Instruct+patched-codes/Llama-3.2-1B-FastApply+patched-codes/Llama-3.2-1B-FixVulns&#34;,&#xA;  messages=messages,&#xA;  temperature=0.2,&#xA;  logprobs = True,&#xA;  top_logprobs = 3,&#xA;  extra_body={&#34;active_adapter&#34;: &#34;patched-codes/Llama-3.2-1B-FastApply&#34;},&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use the alternate decoding techniques like &lt;code&gt;cot_decoding&lt;/code&gt; and &lt;code&gt;entropy_decoding&lt;/code&gt; directly with the local inference server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;response = client.chat.completions.create(&#xA;  model=&#34;meta-llama/Llama-3.2-1B-Instruct&#34;,&#xA;  messages=messages,&#xA;  temperature=0.2,&#xA;  extra_body={&#xA;        &#34;decoding&#34;: &#34;cot_decoding&#34;,  # or &#34;entropy_decoding&#34;&#xA;        # CoT specific params&#xA;        &#34;k&#34;: 10,&#xA;        &#34;aggregate_paths&#34;: True,&#xA;        # OR Entropy specific params&#xA;        &#34;top_k&#34;: 27,&#xA;        &#34;min_p&#34;: 0.03,&#xA;    }&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Starting the optillm proxy with an external server (e.g. llama.cpp or ollama)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; env variable to a placeholder value &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;e.g. &lt;code&gt;export OPENAI_API_KEY=&#34;sk-no-key&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;./llama-server -c 4096 -m path_to_model&lt;/code&gt; to start the server with the specified model and a context length of 4096 tokens&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;python3 optillm.py --base_url base_url&lt;/code&gt; to start the proxy &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;e.g. for llama.cpp, run &lt;code&gt;python3 optillm.py --base_url http://localhost:8080/v1&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Note that the Anthropic API, llama-server (and ollama) currently does not support sampling multiple responses from a model, which limits the available approaches to the following: &lt;code&gt;cot_reflection&lt;/code&gt;, &lt;code&gt;leap&lt;/code&gt;, &lt;code&gt;plansearch&lt;/code&gt;, &lt;code&gt;rstar&lt;/code&gt;, &lt;code&gt;rto&lt;/code&gt;, &lt;code&gt;self_consistency&lt;/code&gt;, &lt;code&gt;re2&lt;/code&gt;, and &lt;code&gt;z3&lt;/code&gt;. For models on HuggingFace, you can use the built-in local inference server as it supports multiple responses.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Implemented techniques&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Approach&lt;/th&gt; &#xA;   &lt;th&gt;Slug&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CoT with Reflection&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;cot_reflection&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements chain-of-thought reasoning with &amp;lt;thinking&amp;gt;, &amp;lt;reflection&amp;gt; and &amp;lt;output&amp;gt; sections&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PlanSearch&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;plansearch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements a search algorithm over candidate plans for solving a problem in natural language&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ReRead&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;re2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements rereading to improve reasoning by processing queries twice&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Self-Consistency&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;self_consistency&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements an advanced self-consistency method&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Z3 Solver&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;z3&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Utilizes the Z3 theorem prover for logical reasoning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;R* Algorithm&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;rstar&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements the R* algorithm for problem-solving&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LEAP&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;leap&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learns task-specific principles from few shot examples&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Round Trip Optimization&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;rto&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Optimizes responses through a round-trip process&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Best of N Sampling&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;bon&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Generates multiple responses and selects the best one&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixture of Agents&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;moa&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Combines responses from multiple critiques&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Monte Carlo Tree Search&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;mcts&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Uses MCTS for decision-making in chat responses&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PV Game&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;pvg&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Applies a prover-verifier game approach at inference time&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CoT Decoding&lt;/td&gt; &#xA;   &lt;td&gt;N/A for proxy&lt;/td&gt; &#xA;   &lt;td&gt;Implements chain-of-thought decoding to elicit reasoning without explicit prompting&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Entropy Decoding&lt;/td&gt; &#xA;   &lt;td&gt;N/A for proxy&lt;/td&gt; &#xA;   &lt;td&gt;Implements adaptive sampling based on the uncertainty of tokens during generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Implemented plugins&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Plugin&lt;/th&gt; &#xA;   &lt;th&gt;Slug&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Router&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;router&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Uses the &lt;a href=&#34;https://huggingface.co/codelion/optillm-bert-uncased&#34;&gt;optillm-bert-uncased&lt;/a&gt; model to route requests to different approaches based on the user prompt&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chain-of-Code&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;coc&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements a chain of code approach that combines CoT with code execution and LLM based code simulation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Memory&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;memory&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implements a short term memory layer, enables you to use unbounded context length with any LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Privacy&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;privacy&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Anonymize PII data in request and deanonymize it back to original value in response&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Read URLs&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;readurls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Reads all URLs found in the request, fetches the content at the URL and adds it to the context&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Execute Code&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;executecode&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enables use of code interpreter to execute python code in requests and LLM generated responses&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Available parameters&lt;/h2&gt; &#xA;&lt;p&gt;optillm supports various command-line arguments and environment variables for configuration.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--approach&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Inference approach to use&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;auto&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--simulations&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of MCTS simulations&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--exploration&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Exploration weight for MCTS&lt;/td&gt; &#xA;   &lt;td&gt;0.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--depth&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Simulation depth for MCTS&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--best-of-n&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of samples for best_of_n approach&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI model to use&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;gpt-4o-mini&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--base-url&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Base URL for OpenAI compatible endpoint&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rstar-max-depth&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum depth for rStar algorithm&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rstar-num-rollouts&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of rollouts for rStar algorithm&lt;/td&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rstar-c&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Exploration constant for rStar algorithm&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--n&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Number of final responses to be returned&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--return-full-response&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Return the full response including the CoT with &#xA;    &lt;thinking&gt;&#xA;      tags&#xA;    &lt;/thinking&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Specify the port to run the proxy&lt;/td&gt; &#xA;   &lt;td&gt;8000&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--optillm-api-key&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Optional API key for client authentication to optillm&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;When using Docker, these can be set as environment variables prefixed with &lt;code&gt;OPTILLM_&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running with Docker&lt;/h2&gt; &#xA;&lt;p&gt;optillm can optionally be built and run using Docker and the provided &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/Dockerfile&#34;&gt;Dockerfile&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using Docker Compose&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure you have Docker and Docker Compose installed on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Either update the environment variables in the docker-compose.yaml file or create a &lt;code&gt;.env&lt;/code&gt; file in the project root directory and add any environment variables you want to set. For example, to set the OpenAI API key, add the following line to the &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=your_openai_api_key_here&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the following command to start optillm:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will build the Docker image if it doesn&#39;t exist and start the optillm service.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;optillm will be available at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When using Docker, you can set these parameters as environment variables. For example, to set the approach and model, you would use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPTILLM_APPROACH=mcts&#xA;OPTILLM_MODEL=gpt-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To secure the optillm proxy with an API key, set the &lt;code&gt;OPTILLM_API_KEY&lt;/code&gt; environment variable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPTILLM_API_KEY=your_secret_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the API key is set, clients must include it in their requests using the &lt;code&gt;Authorization&lt;/code&gt; header:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plain&#34;&gt;Authorization: Bearer your_secret_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;SOTA results on benchmarks with optillm&lt;/h2&gt; &#xA;&lt;h3&gt;coc-claude-3-5-sonnet-20241022 on AIME 2024 pass@1 (Nov 2024)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;o1-mini&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;coc-claude-3-5-sonnet-20241022&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;46.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;coc-gemini/gemini-exp-1121&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;46.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;o1-preview&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;40.00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemini-exp-1114&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;36.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;claude-3-5-sonnet-20241022&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;20.00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemini-1.5-pro-002&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;20.00&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gemini-1.5-flash-002&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;16.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;readurls&amp;amp;memory-gpt-4o-mini on Google FRAMES Benchmark (Oct 2024)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;readurls&amp;amp;memory-gpt-4o-mini&lt;/td&gt; &#xA;   &lt;td&gt;61.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-4o-mini&lt;/td&gt; &#xA;   &lt;td&gt;50.61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;readurls&amp;amp;memory-Gemma2-9b&lt;/td&gt; &#xA;   &lt;td&gt;30.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma2-9b&lt;/td&gt; &#xA;   &lt;td&gt;5.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma2-27b&lt;/td&gt; &#xA;   &lt;td&gt;30.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemini Flash 1.5&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemini Pro 1.5&lt;/td&gt; &#xA;   &lt;td&gt;72.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;plansearch-gpt-4o-mini on LiveCodeBench (Sep 2024)&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;pass@1&lt;/th&gt; &#xA;   &lt;th&gt;pass@5&lt;/th&gt; &#xA;   &lt;th&gt;pass@10&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;plansearch-gpt-4o-mini&lt;/td&gt; &#xA;   &lt;td&gt;44.03&lt;/td&gt; &#xA;   &lt;td&gt;59.31&lt;/td&gt; &#xA;   &lt;td&gt;63.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-4o-mini&lt;/td&gt; &#xA;   &lt;td&gt;43.9&lt;/td&gt; &#xA;   &lt;td&gt;50.61&lt;/td&gt; &#xA;   &lt;td&gt;53.25&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;claude-3.5-sonnet&lt;/td&gt; &#xA;   &lt;td&gt;51.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-4o-2024-05-13&lt;/td&gt; &#xA;   &lt;td&gt;45.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-4-turbo-2024-04-09&lt;/td&gt; &#xA;   &lt;td&gt;44.2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;moa-gpt-4o-mini on Arena-Hard-Auto (Aug 2024)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/codelion/optillm/main/moa-results.png&#34; alt=&#34;Results showing Mixture of Agents approach using gpt-4o-mini on Arena Hard Auto Benchmark&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;optillm with Patchwork (July 2024)&lt;/h3&gt; &#xA;&lt;p&gt;Since optillm is a drop-in replacement for OpenAI API you can easily integrate it with existing tools and frameworks using the OpenAI client. We used optillm with &lt;a href=&#34;https://github.com/patched-codes/patchwork&#34;&gt;patchwork&lt;/a&gt; which is an open-source framework that automates development gruntwork like PR reviews, bug fixing, security patching using workflows called patchflows. We saw huge performance gains across all the supported patchflows as shown below when using the mixture of agents approach (moa).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/codelion/optillm/main/moa-patchwork-results.png&#34; alt=&#34;Results showing optillm mixture of agents approach used with patchflows&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.04474&#34;&gt;Chain of Code: Reasoning with a Language Model-Augmented Code Emulator&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/plugins/coc_plugin.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xjdr-alt/entropix&#34;&gt;Entropy Based Sampling and Parallel CoT Decoding&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/entropy_decoding.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.12941&#34;&gt;Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/scripts/eval_frames_benchmark.py&#34;&gt;Evaluation script&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.arxiv.org/abs/2408.14906&#34;&gt;Writing in the Margins: Better Inference Pattern for Long Context Retrieval&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/plugins/memory_plugin.py&#34;&gt;Inspired the implementation of the memory plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.10200&#34;&gt;Chain-of-Thought Reasoning Without Prompting&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/cot_decoding.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.06275&#34;&gt;Re-Reading Improves Reasoning in Large Language Models&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/reread.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.05403&#34;&gt;In-Context Principle Learning from Mistakes&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/leap.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.03733&#34;&gt;Planning In Natural Language Improves LLM Search For Code Generation&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/plansearch.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.11171&#34;&gt;Self-Consistency Improves Chain of Thought Reasoning in Language Models&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/self_consistency.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2408.06195&#34;&gt;Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/rstar.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.04692&#34;&gt;Mixture-of-Agents Enhances Large Language Model Capabilities&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/moa.py&#34;&gt;Inspired the implementation of moa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.13692&#34;&gt;Prover-Verifier Games improve legibility of LLM outputs&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/pvg.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.00451&#34;&gt;Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/mcts.py&#34;&gt;Inspired the implementation of mcts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.08699&#34;&gt;Unsupervised Evaluation of Code LLMs with Round-Trip Correctness&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/rto.py&#34;&gt;Inspired the implementation of rto&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.18521&#34;&gt;Patched MOA: optimizing inference for diverse software development tasks&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/moa.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.16557&#34;&gt;Patched RTC: evaluating LLMs for diverse software development tasks&lt;/a&gt; - &lt;a href=&#34;https://github.com/codelion/optillm/raw/main/optillm/rto.py&#34;&gt;Implementation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>haosulab/ManiSkill</title>
    <updated>2024-12-23T01:34:50Z</updated>
    <id>tag:github.com,2024-12-23:/haosulab/ManiSkill</id>
    <link href="https://github.com/haosulab/ManiSkill" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SAPIEN Manipulation Skill Framework, a open source GPU parallelized robotics simulator and benchmark, led by Hillbot, Inc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ManiSkill 3 (Beta)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/haosulab/ManiSkill/main/figures/teaser.jpg&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p style=&#34;text-align: center; font-size: 0.8rem; color: #999;margin-top: -1rem;&#34;&gt;Sample of environments/robots rendered with ray-tracing. Scene datasets sourced from AI2THOR and ReplicaCAD&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/mani_skill&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/mani_skill&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/haosulab/ManiSkill/blob/main/examples/tutorials/1_quickstart.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/mani-skill&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/mani-skill.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://maniskill.readthedocs.io/en/latest/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-passing-brightgreen.svg?sanitize=true&#34; alt=&#34;Docs status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/x8yUZe5AdN&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/996566046414753822?logo=discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ManiSkill is a powerful unified framework for robot simulation and training powered by &lt;a href=&#34;https://sapien.ucsd.edu/&#34;&gt;SAPIEN&lt;/a&gt;, with a strong focus on manipulation skills. The entire tech stack is as open-source as possible and ManiSkill v3 is in beta release now. Among its features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU parallelized visual data collection system. On the high end you can collect RGBD + Segmentation data at 30,000+ FPS with a 4090 GPU!&lt;/li&gt; &#xA; &lt;li&gt;GPU parallelized simulation, enabling high throughput state-based synthetic data collection in simulation&lt;/li&gt; &#xA; &lt;li&gt;GPU parallelized heterogeneous simulation, where every parallel environment has a completely different scene/set of objects&lt;/li&gt; &#xA; &lt;li&gt;Example tasks cover a wide range of different robot embodiments (humanoids, mobile manipulators, single-arm robots) as well as a wide range of different tasks (table-top, drawing/cleaning, dextrous manipulation)&lt;/li&gt; &#xA; &lt;li&gt;Flexible and simple task building API that abstracts away much of the complex GPU memory management code via an object oriented design&lt;/li&gt; &#xA; &lt;li&gt;Real2sim environments for scalably evaluating real-world policies 100x faster via GPU simulation.&lt;/li&gt; &#xA; &lt;li&gt;Many tuned robot learning baselines in Reinforcement Learning (e.g. PPO, SAC, &lt;a href=&#34;https://github.com/nicklashansen/tdmpc2&#34;&gt;TD-MPC2&lt;/a&gt;), Imitation Learning (e.g. Behavior Cloning, &lt;a href=&#34;https://github.com/real-stanford/diffusion_policy&#34;&gt;Diffusion Policy&lt;/a&gt;), and large Vision Language Action (VLA) models (e.g. &lt;a href=&#34;https://github.com/octo-models/octo&#34;&gt;Octo&lt;/a&gt;, &lt;a href=&#34;https://github.com/thu-ml/RoboticsDiffusionTransformer&#34;&gt;RDT-1B&lt;/a&gt;, &lt;a href=&#34;https://robotics-transformer-x.github.io/&#34;&gt;RT-x&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details we encourage you to take a look at our &lt;a href=&#34;https://arxiv.org/abs/2410.00425&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are more features to be added to ManiSkill 3, see &lt;a href=&#34;https://maniskill.readthedocs.io/en/latest/roadmap/index.html&#34;&gt;our roadmap&lt;/a&gt; for planned features that will be added over time before the official v3 is released.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://maniskill.readthedocs.io/en/latest/user_guide&#34;&gt;documentation&lt;/a&gt; to learn more information from tutorials on building tasks to data collection.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This project currently is in a &lt;strong&gt;beta release&lt;/strong&gt;, so not all features have been added in yet and there may be some bugs. If you find any bugs or have any feature requests please post them to our &lt;a href=&#34;https://github.com/haosulab/ManiSkill/issues/&#34;&gt;GitHub issues&lt;/a&gt; or discuss about them on &lt;a href=&#34;https://github.com/haosulab/ManiSkill/discussions/&#34;&gt;GitHub discussions&lt;/a&gt;. We also have a &lt;a href=&#34;https://discord.gg/x8yUZe5AdN&#34;&gt;Discord Server&lt;/a&gt; through which we make announcements and discuss about ManiSkill.&lt;/p&gt; &#xA;&lt;p&gt;Users looking for the original ManiSkill2 can find the commit for that codebase at the &lt;a href=&#34;https://github.com/haosulab/ManiSkill/tree/v0.5.3&#34;&gt;v0.5.3 tag&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Installation of ManiSkill is extremely simple, you only need to run a few pip installs and setup Vulkan for rendering.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install the package&#xA;pip install --upgrade mani_skill&#xA;# install a version of torch that is compatible with your system&#xA;pip install torch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally you also need to set up Vulkan with &lt;a href=&#34;https://maniskill.readthedocs.io/en/latest/user_guide/getting_started/installation.html#vulkan&#34;&gt;instructions here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details about installation (e.g. from source, or doing troubleshooting) see &lt;a href=&#34;https://maniskill.readthedocs.io/en/latest/user_guide/getting_started/installation.html&#34;&gt;the documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started, check out the quick start documentation: &lt;a href=&#34;https://maniskill.readthedocs.io/en/latest/user_guide/getting_started/quickstart.html&#34;&gt;https://maniskill.readthedocs.io/en/latest/user_guide/getting_started/quickstart.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also have a quick start &lt;a href=&#34;https://colab.research.google.com/github/haosulab/ManiSkill/blob/main/examples/tutorials/1_quickstart.ipynb&#34;&gt;colab notebook&lt;/a&gt; that lets you try out GPU parallelized simulation without needing your own hardware. Everything is runnable on Colab free tier.&lt;/p&gt; &#xA;&lt;p&gt;For a full list of example scripts you can run, see &lt;a href=&#34;https://maniskill.readthedocs.io/en/latest/user_guide/demos/index.html&#34;&gt;the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;System Support&lt;/h2&gt; &#xA;&lt;p&gt;We currently best support Linux based systems. There is limited support for windows and no support for MacOS at the moment. We are working on trying to support more features on other systems but this may take some time. Most constraints stem from what the &lt;a href=&#34;https://github.com/haosulab/SAPIEN/&#34;&gt;SAPIEN&lt;/a&gt; package is capable of supporting.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System / GPU&lt;/th&gt; &#xA;   &lt;th&gt;CPU Sim&lt;/th&gt; &#xA;   &lt;th&gt;GPU Sim&lt;/th&gt; &#xA;   &lt;th&gt;Rendering&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Linux / NVIDIA GPU&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows / NVIDIA GPU&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows / AMD GPU&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WSL / Anything&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MacOS / Anything&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use ManiSkill3 (versions &lt;code&gt;mani_skill&amp;gt;=3.0.0&lt;/code&gt;) in your work please cite our &lt;a href=&#34;https://arxiv.org/abs/2410.00425&#34;&gt;ManiSkill3 paper&lt;/a&gt; as so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{taomaniskill3,&#xA;  title={ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI},&#xA;  author={Stone Tao and Fanbo Xiang and Arth Shukla and Yuzhe Qin and Xander Hinrichsen and Xiaodi Yuan and Chen Bao and Xinsong Lin and Yulin Liu and Tse-kai Chan and Yuan Gao and Xuanlin Li and Tongzhou Mu and Nan Xiao and Arnav Gurha and Zhiao Huang and Roberto Calandra and Rui Chen and Shan Luo and Hao Su},&#xA;  journal = {arXiv preprint arXiv:2410.00425},&#xA;  year={2024},&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use ManiSkill2 (version &lt;code&gt;mani_skill==0.5.3&lt;/code&gt; or lower) in your work please cite the ManiSkill2 paper as so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{gu2023maniskill2,&#xA;  title={ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills},&#xA;  author={Gu, Jiayuan and Xiang, Fanbo and Li, Xuanlin and Ling, Zhan and Liu, Xiqiang and Mu, Tongzhou and Tang, Yihe and Tao, Stone and Wei, Xinyue and Yao, Yunchao and Yuan, Xiaodi and Xie, Pengwei and Huang, Zhiao and Chen, Rui and Su, Hao},&#xA;  booktitle={International Conference on Learning Representations},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that some other assets, algorithms, etc. in ManiSkill are from other sources/research. We try our best to include the correct citation bibtex where possible when introducing the different components provided by ManiSkill.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All rigid body environments in ManiSkill are licensed under fully permissive licenses (e.g., Apache-2.0).&lt;/p&gt; &#xA;&lt;p&gt;The assets are licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/legalcode&#34;&gt;CC BY-NC 4.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>