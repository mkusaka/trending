<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-13T01:40:58Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>butaixianran/Stable-Diffusion-Webui-Civitai-Helper</title>
    <updated>2023-03-13T01:40:58Z</updated>
    <id>tag:github.com,2023-03-13:/butaixianran/Stable-Diffusion-Webui-Civitai-Helper</id>
    <link href="https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion Webui Extension for Civitai, to manage your model much more easily.&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Language&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/README.cn.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Notice&lt;/h1&gt; &#xA;&lt;p&gt;After install or update to new version, you need to shutdown SD webui and re-launch. Just Reload UI won&#39;t work!&lt;/p&gt; &#xA;&lt;h1&gt;Stable-Diffusion-Webui-Civitai-Helper&lt;/h1&gt; &#xA;&lt;p&gt;Stable Diffusion Webui Extension for Civitai, to handle your models much more easily.&lt;/p&gt; &#xA;&lt;p&gt;Civitai: &lt;a href=&#34;https://civitai.com/models/16768/civitai-helper-sd-webui-civitai-extension&#34;&gt;Civitai Url&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scans all models to download model information and preview images from Civitai.&lt;/li&gt; &#xA; &lt;li&gt;Link local model to a civitai model by civitai model&#39;s url&lt;/li&gt; &#xA; &lt;li&gt;Download a model(with info+preview) by Civitai Url into SD&#39;s model folder or subfolder.&lt;/li&gt; &#xA; &lt;li&gt;Downloading can resume at break-point, which is good for large file.&lt;/li&gt; &#xA; &lt;li&gt;Checking all your local model&#39;s new version from Civitai&lt;/li&gt; &#xA; &lt;li&gt;Download a new version directly into SD model folder (with info+preview)&lt;/li&gt; &#xA; &lt;li&gt;Modified Built-in &#34;Extra Network&#34; cards, to add the following buttons on each card: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üñº: Modified &#34;replace preview&#34; text into this icon&lt;/li&gt; &#xA;   &lt;li&gt;üåê: Open this model&#39;s Civitai url in a new tab&lt;/li&gt; &#xA;   &lt;li&gt;üí°: Add this model&#39;s trigger words to prompt&lt;/li&gt; &#xA;   &lt;li&gt;üè∑: Use this model&#39;s preview image&#39;s prompt&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Above buttons support thumbnail mode of Extra Network&lt;/li&gt; &#xA; &lt;li&gt;Option to always show additional buttons, to work with touchscreen.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;p&gt;Go to SD webui&#39;s extension tab, go to &lt;code&gt;Install from url&lt;/code&gt; sub-tab. Copy this project&#39;s url into it, click install.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, download this project as a zip file, and unzip it to &lt;code&gt;Your SD webui folder/extensions&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then shutdown SD Webui and Relaunch it. Just &#34;Reload UI&#34; won&#39;t work for this extension.&lt;/p&gt; &#xA;&lt;p&gt;Done.&lt;/p&gt; &#xA;&lt;h1&gt;How to Use&lt;/h1&gt; &#xA;&lt;h2&gt;Update Your SD Webui&lt;/h2&gt; &#xA;&lt;p&gt;This extension need to get extra network&#39;s cards id. &lt;strong&gt;Which is added to SD webui since 2023-02-06.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;So, if you are using a version earlier than this, you need to update your SD Webui!&lt;/p&gt; &#xA;&lt;h2&gt;Scanning Models&lt;/h2&gt; &#xA;&lt;p&gt;Go to extension tab &#34;Civitai Helper&#34;. There is a button called &#34;Scan model&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/extension_tab.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Click it and the extension will scan all your models to generate SHA256 hashes, using them to retreive model information and preview images from Civitai.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scanning takes time, just wait it finish&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For each model, it will create a json file to save all model info from Civitai. This model info file will be &#34;Your_model_name.civitai.info&#34; in your model folder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/model_info_file.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If a model info file already exists, it will be skipped. If a model cannot be found in Civitai, it will create an empty model info file, so the model won&#39;t be scanned twice.&lt;/p&gt; &#xA;&lt;h3&gt;Adding New Models&lt;/h3&gt; &#xA;&lt;p&gt;When you have some new models, just click scan button again, to get new model&#39;s information and preview images. It won&#39;t scan the same model twice.&lt;/p&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Use this only after scanning finished)&lt;/strong&gt; Open SD webui&#39;s build-in &#34;Extra Network&#34; tab, to show model cards.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/extra_network.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Move your mouse on to the bottom of a model card. It will show 4 icon buttons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üñº: Modified &#34;replace preview&#34; text into this icon&lt;/li&gt; &#xA; &lt;li&gt;üåê: Open this model&#39;s Civitai url in a new tab&lt;/li&gt; &#xA; &lt;li&gt;üí°: Add this model&#39;s trigger words to prompt&lt;/li&gt; &#xA; &lt;li&gt;üè∑: Use this model&#39;s preview image&#39;s prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/model_card.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If these additional buttons are not there&lt;/strong&gt;, click the &lt;code&gt;Refresh Civitai Helper&lt;/code&gt; button to bring them back.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/refresh_ch.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt; Everytime after Extra Network tab refreshed, it will remove all these additional buttons. So, you need to click &lt;code&gt;Refresh Civitai Helper&lt;/code&gt; button to bring them back.&lt;/p&gt; &#xA;&lt;h3&gt;Thumbnail Mode&lt;/h3&gt; &#xA;&lt;p&gt;Additional buttons work on thumbnail too, but due to SD webui&#39;s CSS issue, for now, they must be always displayed on thumbnail or don&#39;t display at all.&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/thumb_mode.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;To download a model by Civitai Model Page&#39;s Url, you need 3 steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fill url, click button to get model info&lt;/li&gt; &#xA; &lt;li&gt;It will show model name and type automatically. Just choose sub-folder and model version&lt;/li&gt; &#xA; &lt;li&gt;Click download. &lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/download_model.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Detail will be displayed on console log, with a progress bar.&lt;br&gt; Downloading can resume from break-point, so no fear for large file.&lt;/p&gt; &#xA;&lt;h2&gt;Checking Model&#39;s New Version&lt;/h2&gt; &#xA;&lt;p&gt;You can checking your local model&#39;s new version from civitai by model types. You can select multiple model types.&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/check_model_new_version.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The checking process has a &#34;1 second delay&#34; after each model&#39;s new version checking request. So it is a little slow.&lt;/p&gt; &#xA;&lt;p&gt;This is to protect Civitai from facing issue like DDos from this extension. Some cloud service provider has a rule as &#34;no more than 1 API request in a second for free user&#34;. Civitai doesn&#39;t have this rule yet, but we still need to protect it. There is no good for us if it is down.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;After checking process done&lt;/strong&gt;, it will display all new version&#39;s information on UI.&lt;/p&gt; &#xA;&lt;p&gt;There are 3 urls for each new version.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First one is model&#39;s page.&lt;/li&gt; &#xA; &lt;li&gt;Second one is new version&#39;s download url.&lt;/li&gt; &#xA; &lt;li&gt;Third one is a button to download it into your SD&#39;s model folder with python.&lt;br&gt; With this one, output information is on &#34;Download Model&#34; section&#39;s log and console log. One task at a time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/check_model_new_version_output.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Model Info By Url&lt;/h2&gt; &#xA;&lt;p&gt;If a model&#39;s SHA256 can not be found in civitai, but you still want to link it to a civitai model. You can choose this model from list, then offer a civitai model page&#39;s url you want to link.&lt;/p&gt; &#xA;&lt;p&gt;After clicking button, extension will download that civitai model&#39;s model info for this local model file you picked.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/get_one_model_info.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Other Setting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Save Setting button, will save both &#34;Scan Model&#34;&#39;s setting and other setting.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Always Display Button&#34; is good for touch screen.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Show Buttons on Thumb Mode&#34; will turn on/off additional Buttons on thumbnail.&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper/main/img/other_setting.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preview Image&lt;/h2&gt; &#xA;&lt;p&gt;Extra network uses both &lt;code&gt;model_file.png&lt;/code&gt; and &lt;code&gt;model_file.preview.png&lt;/code&gt; as preview image. But &lt;code&gt;model_file.png&lt;/code&gt; has higher priority, because it is created by yourself.&lt;/p&gt; &#xA;&lt;p&gt;When you don&#39;t have the higher priority one, it will use the other automatically.&lt;/p&gt; &#xA;&lt;h2&gt;Prompt&lt;/h2&gt; &#xA;&lt;p&gt;When you click the button &#34;Use prompt from preview image&#34;, it does not use the prompt from your own preview image. It uses the one from civitai&#39;s preview image.&lt;/p&gt; &#xA;&lt;p&gt;On civitai, a model&#39;s preview images may not has prompt. This extension will check this model&#39;s all civitai preview images&#39; information and use the first one has prompt in it.&lt;/p&gt; &#xA;&lt;h2&gt;SHA256&lt;/h2&gt; &#xA;&lt;p&gt;To create a file SHA256, it need to read the whole file to generate a hash code. It gonna be slow for large files.&lt;/p&gt; &#xA;&lt;p&gt;Also, extension uses Memory Optimized SHA256, which won&#39;t stuck your system and works with colab.&lt;/p&gt; &#xA;&lt;p&gt;There are 2 cases this hash code can not find the model on civitai:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Some old models, which do not have SHA256 code on civitai.&lt;/li&gt; &#xA; &lt;li&gt;The model&#39;s owner changed file on civitai, but does not change version name and description. So, the file on civitai is actually not the one on your manchine.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In these cases, you can always link a model to civitai by filling its URL in this extension.&lt;/p&gt; &#xA;&lt;h2&gt;Civitai API Security Check&lt;/h2&gt; &#xA;&lt;p&gt;Since last website down of civitai, it has more strict checking for request&#39;s frequency. Once it thinks this extension is sending requests to quickly, it gonna re-direct extension&#39;s API request to Cloudflare&#39;s real human checking page. Then this extension can not get any information back.&lt;/p&gt; &#xA;&lt;p&gt;In that case, you have to wait until civitai unblock you. It could take hours.&lt;/p&gt; &#xA;&lt;p&gt;Since v1.5.1, TI scanning is forced to delay 1 second to avoid this case. So you should be find if you are using 1.5.1 or later.&lt;/p&gt; &#xA;&lt;h2&gt;Feature Request&lt;/h2&gt; &#xA;&lt;p&gt;No new feature for v1.x after v1.5. All new feature will go to 2.x.&lt;/p&gt; &#xA;&lt;p&gt;2.x will focus on custom model information and may change name to &#34;Model Info Helper&#34;, because it is not just focus on Civitai anymore.&lt;/p&gt; &#xA;&lt;p&gt;From v1.5, v1.x goes into maintenance phase.&lt;/p&gt; &#xA;&lt;p&gt;Enjoy!&lt;/p&gt; &#xA;&lt;h1&gt;Change Log&lt;/h1&gt; &#xA;&lt;h2&gt;v1.5.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Force TI scanning delay 1 second to prevent from civitai treating this extension&#39;s requests as attacking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.5.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download a model by Civitai model page&#39;s url&lt;/li&gt; &#xA; &lt;li&gt;Resume downloading from break-point&lt;/li&gt; &#xA; &lt;li&gt;Download new version into SD Webui&#39;s model folder&lt;/li&gt; &#xA; &lt;li&gt;Addtional button now works on thumbnail mode&lt;/li&gt; &#xA; &lt;li&gt;Option to always show addtion button, for touch screen.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.4.2&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ignore .vae file in model folder when scanning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.4.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When checking new versions, also searching and ignore already existed ones.&lt;/li&gt; &#xA; &lt;li&gt;Add version number to the bottom of this extension&#39;s tab&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.4&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support checking model&#39;s new version, display the result in UI and offer download url&lt;/li&gt; &#xA; &lt;li&gt;Remove addintional sub tabs on extension tab. make ui simpler.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.3&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open url at client side&lt;/li&gt; &#xA; &lt;li&gt;Link selected model to civitai by url or model id&lt;/li&gt; &#xA; &lt;li&gt;Save and load extension setting to file&lt;/li&gt; &#xA; &lt;li&gt;Show button action&#39;s output to UI&lt;/li&gt; &#xA; &lt;li&gt;Code refactoring&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.2.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add more error checking to work with different versions of SD webui.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.2&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support customer model folder&lt;/li&gt; &#xA; &lt;li&gt;Support readable model info file&lt;/li&gt; &#xA; &lt;li&gt;Support download preview image with max size&lt;/li&gt; &#xA; &lt;li&gt;Remove card buttons when extra network is in thumbnail mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;v1.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support subfolders&lt;/li&gt; &#xA; &lt;li&gt;Check if refresh is needed when clicking &#34;Refresh Civitai Helper&#34;&lt;/li&gt; &#xA; &lt;li&gt;Add space when adding trigger words&lt;/li&gt; &#xA; &lt;li&gt;Add memory Optimized sha256 as an option&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>pkuliyi2015/multidiffusion-upscaler-for-automatic1111</title>
    <updated>2023-03-13T01:40:58Z</updated>
    <id>tag:github.com,2023-03-13:/pkuliyi2015/multidiffusion-upscaler-for-automatic1111</id>
    <link href="https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MultiDiffusion implementation with VAE VRAM optimize&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MultiDiffusion with Tiled VAE&lt;/h1&gt; &#xA;&lt;p&gt;EnglishÔΩú&lt;a href=&#34;https://raw.githubusercontent.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/main/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains two scripts that enable &lt;strong&gt;ultra-large image generation&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The MultiDiffusion comes from existing work. Please refer to their paper and GitHub page &lt;a href=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/multidiffusion.github.io&#34;&gt;MultiDiffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The Tiled VAE is my original algorithm, which is &lt;strong&gt;very powerful&lt;/strong&gt; in VRAM saving&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update on 2023.3.7&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added Fast Mode for Tiled VAE, which increase the speed by 5X and eliminated the need for extra RAM.&lt;/li&gt; &#xA; &lt;li&gt;Now you can use 16GB GPU for 8K images, and the encoding/decoding process will be around 25 seconds. For 4k images, the process completes almost instantly.&lt;/li&gt; &#xA; &lt;li&gt;If you encountered VAE NaN or black image output: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Use the OpenAI provided 840000 VAE weights. This usually solves the problem.&lt;/li&gt; &#xA;   &lt;li&gt;Use --no-half-vae on startup is also effective.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MultiDiffusion&lt;/h2&gt; &#xA;&lt;p&gt;Note: &lt;a href=&#34;https://energy-based-model.github.io/reduce-reuse-recycle/&#34;&gt;The latest sampler by Google&lt;/a&gt; seems to achieve &lt;strong&gt;theoretically&lt;/strong&gt; better results in local control than MultiDiffusion. We are investigating their differences and may provide an implementation in this repo.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast ultra-large images refinement (img2img)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MultiDiffusion is especially good at adding details to upscaled images.&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Faster than highres.fix&lt;/strong&gt; with proper params&lt;/li&gt; &#xA;   &lt;li&gt;Much finer results than SD Upscaler &amp;amp; Ultimate SD Upscaler&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How to use:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;The checkpoint is crucial&lt;/strong&gt;. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;MultiDiffusion works very similar to highres.fix, so it highly relies on your checkpoint.&lt;/li&gt; &#xA;     &lt;li&gt;A checkpoint that good at drawing details (e.g., trained on high resolution images) can add amazing details to your image.&lt;/li&gt; &#xA;     &lt;li&gt;Some friends have found that using a &lt;strong&gt;full checkpoint&lt;/strong&gt; instead of a pruned one yields much finer results.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Don&#39;t include any concrete objects in your positive prompts.&lt;/strong&gt; Otherwise the results get ruined. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Just use something like &#34;highres, masterpiece, best quality, ultra-detailed unity 8k wallpaper, extremely clear&#34;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;You don&#39;t need too large tile size, large overlap and many denoising steps, or it can be slow. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Latent tile size=64 - 96, Overlap=32 - 48, and steps=20 - 25 are recommended. &lt;strong&gt;If you find seams, please increase overlap.&lt;/strong&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CFG scale can significantly affect the details&lt;/strong&gt;, together with a proper sampler. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;A large CFG scale (e.g., 14) gives you much more details. For samplers,I personally prefer Euler a and DPM++ SDE Karras.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;You can control how much you want to change the original image with &lt;strong&gt;denoising strength from 0.1 - 0.6&lt;/strong&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;If your results are still not as satisfying as mine, &lt;a href=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/issues/3&#34;&gt;see our discussions here.&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Example: 1024 * 800 -&amp;gt; 4096 * 3200 image, denoise=0.4, steps=20, Sampler=DPM++ SDE Karras, Upscaler=RealESRGAN++, Negative Prompts=EasyNegative&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Before:&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/lowres.jpg?raw=true&#34; alt=&#34;lowres&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;After: 4x upscale.&lt;/li&gt; &#xA;   &lt;li&gt;1min12s on NVIDIA Testla V100. (If 2x, it completes in 10s)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/highres.jpeg?raw=true&#34; alt=&#34;highres&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wide Image Generation (txt2img)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;txt2img panorama generation, as mentioned in MultiDiffusion.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;All tiles share the same prompt currently.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Please use simple positive prompts to get good results&lt;/strong&gt;, otherwise the result will be pool.&lt;/li&gt; &#xA;   &lt;li&gt;We are urgently working on the rectangular &amp;amp; fine-grained prompt control.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Example - masterpiece, best quality, highres, city skyline, night.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/city_panorama.jpeg?raw=true&#34; alt=&#34;panorama&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;It can cooperate with ControlNet&lt;/strong&gt; to produce wide images with control.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You cannot use complex positive prompts currently. However, you can use ControlNet.&lt;/li&gt; &#xA; &lt;li&gt;Canny edge seems to be the best as it provides sufficient local controls.&lt;/li&gt; &#xA; &lt;li&gt;Example: 22020 x 1080 ultra-wide image conversion &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Masterpiece, best quality, highres, ultra-detailed 8k unity wallpaper, bird&#39;s-eye view, trees, ancient architectures, stones, farms, crowd, pedestrians&lt;/li&gt; &#xA;   &lt;li&gt;Before: &lt;a href=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/ancient_city_origin.jpeg&#34;&gt;click for the raw image&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/ancient_city_origin.jpeg?raw=true&#34; alt=&#34;ancient city origin&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;After: &lt;a href=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/ancient_city.jpeg&#34;&gt;click for the raw image&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/ancient_city.jpeg?raw=true&#34; alt=&#34;ancient city&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Example: 2560 * 1280 large image drawing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ControlNet canny edge&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/yourname_canny.jpeg?raw=true&#34; alt=&#34;Your Name&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/yourname.jpeg?raw=true&#34; alt=&#34;yourname&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Advantages&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Draw super large resolution (2k~8k) image in both txt2img and img2img&lt;/li&gt; &#xA; &lt;li&gt;Seamless output without any post-processing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Drawbacks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We haven&#39;t optimized it much, so it can be &lt;strong&gt;slow especially for very large images&lt;/strong&gt; (8k) and with ControlNet.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt control is weak.&lt;/strong&gt; It will produce repeated patterns with strong positive prompts, and the result may not be usable.&lt;/li&gt; &#xA; &lt;li&gt;The gradient calculation is not compatible with this hack. It will break any backward() or torch.autograd.grad() that passes UNet.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How it works (so simple!)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The latent image is split into tiles.&lt;/li&gt; &#xA; &lt;li&gt;The tiles are denoised by the original sampler for one time step.&lt;/li&gt; &#xA; &lt;li&gt;The tiles are added together but divided by how many times each pixel is added.&lt;/li&gt; &#xA; &lt;li&gt;Repeat 2-3 until all timesteps are completed.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Tiled VAE&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;This script is currently production-ready&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;vae_optimize.py&lt;/code&gt; script is a wild hack that splits the image into tiles, encodes each tile separately, and merges the result back together. This process allows the VAE to work with giant images on limited VRAM (~10 GB for 8K images!).&lt;/p&gt; &#xA;&lt;p&gt;Remove --lowvram and --medvram to enjoy!&lt;/p&gt; &#xA;&lt;h3&gt;Advantages&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The tiled VAE work with giant images on limited VRAM (~12 GB for 8K images!)&lt;/li&gt; &#xA; &lt;li&gt;Unlike &lt;a href=&#34;https://github.com/Kahsolt/stable-diffusion-webui-vae-tile-infer&#34;&gt;my friend&#39;s implementation&lt;/a&gt; and the HuggingFace diffuser&#39;s VAE tiling options that averages the tile borders, this VAE tiling removed attention blocks and use padding tricks. The decoding results are mathematically identical to that of not tiling, i.e., &lt;strong&gt;it will not produce seams at all.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;The script is extremely optimized with tons of tricks. Cannot be faster!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Drawbacks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NaNs occassionally appear. We are figuring out the root cause and trying to fix.&lt;/li&gt; &#xA; &lt;li&gt;Similarly, the gradient calculation is not compatible with this hack. It will break any backward() or torch.autograd.grad() that passes VAE.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How it works&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;The image is split into tiles and padded with 11/32 pixels&#39; in decoder/encoder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When Fast Mode is disabled:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The original VAE forward is decomposed into a task queue and a task worker, which start to process each tile.&lt;/li&gt; &#xA;   &lt;li&gt;When GroupNorm is needed, it suspends, stores current GroupNorm mean and var, send everything to RAM, and turns to the next tile.&lt;/li&gt; &#xA;   &lt;li&gt;After all GroupNorm mean and var parameters are summarized, it applies group norm to tiles and continues.&lt;/li&gt; &#xA;   &lt;li&gt;A zigzag execution order is used to reduce unnecessary data transfer.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When Fast Mode is enabled:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The original input is downsampled and passed to a separate task queue.&lt;/li&gt; &#xA;   &lt;li&gt;Its group norm parameters are recorded and used by all tiles&#39; task queues.&lt;/li&gt; &#xA;   &lt;li&gt;Each tile is separately processed without any RAM-VRAM data transfer.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After all tiles are processed, tiles are written to a result buffer and returned.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open Automatic1111 WebUI -&amp;gt; Click Tab &#34;Extensions&#34; -&amp;gt; Click Tab &#34;Install from URL&#34; -&amp;gt; type in the link of this repo -&amp;gt; Click &#34;Install&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/installation.png?raw=true&#34; alt=&#34;installation&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;After restart your WebUI, you shall see the following two tabs:&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111/raw/docs/imgs/Tab.png?raw=true&#34; alt=&#34;Tab&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;MultiDiffusion Params&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Latent tile width &amp;amp; height: Basically, MultiDiffusion draws images tile by tile and each tile is a rectangle. Hence, this controls how large is the latent rectangle, each is 1/8 size of the actual image. Shouldn&#39;t be too large or too small (normally 64-128 is OK. but you can try other values.)&lt;/li&gt; &#xA; &lt;li&gt;Latent tile overlap: MultiDiffusion uses overlapping to prevent seams and fuses two latent images. So this controls how long should two tiles be overlapped at one side. The larger this value is, the slower the process, but the result will contain fewer seams and be more natural.&lt;/li&gt; &#xA; &lt;li&gt;Latent tile batch size: allow UNet to process tiles in a batched manner. Larger values can speed up the UNet at the cost of more VRAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tiled VAE param&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Move to GPU&lt;/strong&gt;: when you are running under --lowvram or medvram, this option will help to move the VAE to GPU temporarily and move it back later. It needs several seconds.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The two tile size params&lt;/strong&gt; control how large the tile should be we split for VAE encoder and decoder. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Basically, larger size brings faster speed at the cost of more VRAM usage. We will dynamicly shrink the size to make it faster.&lt;/li&gt; &#xA;   &lt;li&gt;You don&#39;t need to change the params at the first time of using. It will recommend a set of parameters based on hand-crafted rules. However, the recommended params may not be good to fit your device.&lt;/li&gt; &#xA;   &lt;li&gt;Please adjust according to the GPU used in the console output. If you have more VRAM, turn it larger, or vice versus.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast Decoder&lt;/strong&gt;: We use a small latent image to estimate the decoder params and then comput very fast. By default, it is enabled. Not recommend to disable it. If you disable it, a large amount of CPU RAM and time will be consumed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast Encoder&lt;/strong&gt;: We use a small image to estimate the encoder params; However, this is not accurate when your tile is very small and is further compressed by the encoder. Hence, it may do harm to your image&#39;s quality, especially colors.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Encoder Color Fix&lt;/strong&gt;: To fix the above problem, we provide a semi-fast mode that only estimate the params before downsampling. When you enable this, the slowest steps will be done in fast mode, and the remaining steps will run in legacy mode. Only enable this when you see visible color artifacts in pictures.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enjoy!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Current Progress&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We are investigating Google&#39;s latest sampler (which seems to yield better results than MultiDiffusion? We are not sure.)&lt;/li&gt; &#xA; &lt;li&gt;Local prompt control is in progress. Automatic regional prompting is in consideration.&lt;/li&gt; &#xA; &lt;li&gt;Video translation via MultiDiffusion frame interpolation still need proof-of-concept.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;These scripts are licensed under the MIT License. If you find them useful, please give me a star.&lt;/p&gt; &#xA;&lt;p&gt;Thank you!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>WassimTenachi/PhySO</title>
    <updated>2023-03-13T01:40:58Z</updated>
    <id>tag:github.com,2023-03-13:/WassimTenachi/PhySO</id>
    <link href="https://github.com/WassimTenachi/PhySO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Physical Symbolic Optimization&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;$\Phi$-SO : Physical Symbolic Optimization&lt;/h1&gt; &#xA;&lt;p&gt;The physical symbolic regression ( $\Phi$-SO ) package &lt;code&gt;physo&lt;/code&gt; is a symbolic regression package that fully leverages physical units constraints. For more details see: &lt;a href=&#34;https://arxiv.org/abs/2303.03192&#34;&gt;[Tenachi et al 2023]&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h3&gt;Virtual environment&lt;/h3&gt; &#xA;&lt;p&gt;The package has been tested on Unix and OSX. To install the package it is recommend to first create a conda virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n PhySO python=3.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And activate it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate PhySO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;From the repository root:&lt;/p&gt; &#xA;&lt;p&gt;Installing essential dependencies :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Installing optional dependencies (for monitoring plots) :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements_display.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Side note for ARM users:&lt;/h5&gt; &#xA;&lt;p&gt;The file &lt;code&gt;requirements_display.txt&lt;/code&gt; contains dependencies that can be installed via pip only. However, it also contains &lt;code&gt;pygraphviz&lt;/code&gt; which can be installed via conda which avoids compiler issues on ARM.&lt;/p&gt; &#xA;&lt;p&gt;It is recommended to run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pygraphviz==1.9&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;before running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements_display.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installing $\Phi$-SO&lt;/h3&gt; &#xA;&lt;p&gt;Installing &lt;code&gt;physo&lt;/code&gt; (from the repository root):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Testing install&lt;/h3&gt; &#xA;&lt;h5&gt;Import test:&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3&#xA;&amp;gt;&amp;gt;&amp;gt; import physo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should result in &lt;code&gt;physo&lt;/code&gt; being successfully imported.&lt;/p&gt; &#xA;&lt;h5&gt;Unit tests:&lt;/h5&gt; &#xA;&lt;p&gt;From the repository root:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m unittest discover -p &#34;*UnitTest.py&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should result in all tests being successfully passed (except for plots tests if dependencies were not installed).&lt;/p&gt; &#xA;&lt;h1&gt;Getting started&lt;/h1&gt; &#xA;&lt;h3&gt;Symbolic regression with default hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;[Coming soon] In the meantime you can have a look a our demo folder ! :)&lt;/p&gt; &#xA;&lt;h3&gt;Symbolic regression&lt;/h3&gt; &#xA;&lt;p&gt;[Coming soon]&lt;/p&gt; &#xA;&lt;h3&gt;Custom symbolic optimization task&lt;/h3&gt; &#xA;&lt;p&gt;[Coming soon]&lt;/p&gt; &#xA;&lt;h3&gt;Using custom functions&lt;/h3&gt; &#xA;&lt;p&gt;[Coming soon]&lt;/p&gt; &#xA;&lt;h3&gt;Open training loop&lt;/h3&gt; &#xA;&lt;p&gt;[Coming soon]&lt;/p&gt; &#xA;&lt;h1&gt;Citing this work&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@ARTICLE{2023arXiv230303192T,&#xA;       author = {{Tenachi}, Wassim and {Ibata}, Rodrigo and {Diakogiannis}, Foivos I.},&#xA;        title = &#34;{Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws}&#34;,&#xA;      journal = {arXiv e-prints},&#xA;     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Physics - Computational Physics},&#xA;         year = 2023,&#xA;        month = mar,&#xA;          eid = {arXiv:2303.03192},&#xA;        pages = {arXiv:2303.03192},&#xA;          doi = {10.48550/arXiv.2303.03192},&#xA;archivePrefix = {arXiv},&#xA;       eprint = {2303.03192},&#xA; primaryClass = {astro-ph.IM},&#xA;       adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230303192T},&#xA;      adsnote = {Provided by the SAO/NASA Astrophysics Data System}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>