<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-12T01:34:07Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>p0dalirius/Coercer</title>
    <updated>2022-07-12T01:34:07Z</updated>
    <id>tag:github.com,2022-07-12:/p0dalirius/Coercer</id>
    <link href="https://github.com/p0dalirius/Coercer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A python script to automatically coerce a Windows server to authenticate on an arbitrary machine through 9 methods.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/p0dalirius/Coercer/master/.github/banner.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; A python script to automatically coerce a Windows server to authenticate on an arbitrary machine through 9 methods. &lt;br&gt; &lt;img alt=&#34;GitHub release (latest by date)&#34; src=&#34;https://img.shields.io/github/v/release/p0dalirius/Coercer&#34;&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=podalirius_&#34; title=&#34;Follow&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/podalirius_?label=Podalirius&amp;amp;style=social&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/c/Podalirius_?sub_confirmation=1&#34; title=&#34;Subscribe&#34;&gt;&lt;img alt=&#34;YouTube Channel Subscribers&#34; src=&#34;https://img.shields.io/youtube/channel/subscribers/UCF_x5O7CSfr82AfNVTKOv_A?style=social&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatically detects open SMB pipes on the remote machine.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Calls one by one all the vulnerable RPC functions to coerce the server to authenticate on an arbitrary machine.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Analyze mode with &lt;code&gt;--analyze&lt;/code&gt;, which only lists the vulnerable protocols and functions listening, without performing a coerced authentication.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Perform coerce attack on a list of targets from a file with &lt;code&gt;--targets-file&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Coerce to a WebDAV target with &lt;code&gt;--webdav-host&lt;/code&gt; and &lt;code&gt;--webdav-port&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./coercer.py -h                                                                                                  &#xA;&#xA;       ______                              &#xA;      / ____/___  ___  _____________  _____&#xA;     / /   / __ \/ _ \/ ___/ ___/ _ \/ ___/&#xA;    / /___/ /_/ /  __/ /  / /__/  __/ /      v1.4&#xA;    \____/\____/\___/_/   \___/\___/_/       by @podalirius_&#xA;&#xA;usage: coercer.py [-h] [-u USERNAME] [-p PASSWORD] [-d DOMAIN] [--hashes [LMHASH]:NTHASH] [--no-pass] [-v] [-a] [-k] [--dc-ip ip address] [-l LISTENER] [-wh WEBDAV_HOST] [-wp WEBDAV_PORT]&#xA;                  (-t TARGET | -f TARGETS_FILE) [--target-ip ip address]&#xA;&#xA;Automatic windows authentication coercer over various RPC calls.&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;  -u USERNAME, --username USERNAME&#xA;                        Username to authenticate to the endpoint.&#xA;  -p PASSWORD, --password PASSWORD&#xA;                        Password to authenticate to the endpoint. (if omitted, it will be asked unless -no-pass is specified)&#xA;  -d DOMAIN, --domain DOMAIN&#xA;                        Windows domain name to authenticate to the endpoint.&#xA;  --hashes [LMHASH]:NTHASH&#xA;                        NT/LM hashes (LM hash can be empty)&#xA;  --no-pass             Don&#39;t ask for password (useful for -k)&#xA;  -v, --verbose         Verbose mode (default: False)&#xA;  -a, --analyze         Analyze mode (default: Attack mode)&#xA;  -k, --kerberos        Use Kerberos authentication. Grabs credentials from ccache file (KRB5CCNAME) based on target parameters. If valid credentials cannot be found, it will use the ones specified in the&#xA;                        command line&#xA;  --dc-ip ip address    IP Address of the domain controller. If omitted it will use the domain part (FQDN) specified in the target parameter&#xA;  -t TARGET, --target TARGET&#xA;                        IP address or hostname of the target machine&#xA;  -f TARGETS_FILE, --targets-file TARGETS_FILE&#xA;                        IP address or hostname of the target machine&#xA;  --target-ip ip address&#xA;                        IP Address of the target machine. If omitted it will use whatever was specified as target. This is useful when target is the NetBIOS name or Kerberos name and you cannot resolve it&#xA;&#xA;  -l LISTENER, --listener LISTENER&#xA;                        IP address or hostname of the listener machine&#xA;  -wh WEBDAV_HOST, --webdav-host WEBDAV_HOST&#xA;                        WebDAV IP of the server to authenticate to.&#xA;  -wp WEBDAV_PORT, --webdav-port WEBDAV_PORT&#xA;                        WebDAV port of the server to authenticate to.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Coerced SMB authentication demonstration&lt;/h2&gt; &#xA;&lt;p&gt;Here is a video demonstration of the attack mode against a target:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/79218792/177647814-bb04f728-96bb-4048-a3ad-f83b250c05bf.mp4&#34;&gt;https://user-images.githubusercontent.com/79218792/177647814-bb04f728-96bb-4048-a3ad-f83b250c05bf.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Coerced WebDAV authentication demonstration&lt;/h2&gt; &#xA;&lt;p&gt;If you want to trigger an HTTP authentication, you can use WebDAV with &lt;code&gt;--webdav-host&lt;/code&gt; and the netdbios name of your attacking machine! Here is an example:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/79218792/178027554-a0b084d8-10af-401a-b54c-f33bec011fe2.mp4&#34;&gt;https://user-images.githubusercontent.com/79218792/178027554-a0b084d8-10af-401a-b54c-f33bec011fe2.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example output&lt;/h2&gt; &#xA;&lt;p&gt;In attack mode (without &lt;code&gt;--analyze&lt;/code&gt; option) you get the following output:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/p0dalirius/Coercer/master/.github/example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;After all the RPC calls, you get plenty of authentications in Responder:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/p0dalirius/Coercer/master/.github/hashes.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests are welcome. Feel free to open an issue if you want to add other features.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/tifkin_&#34;&gt;@tifkin_&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/elad_shamir&#34;&gt;@elad_shamir&lt;/a&gt; for finding and implementing &lt;strong&gt;PrinterBug&lt;/strong&gt; on &lt;a href=&#34;https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-rprn/d42db7d5-f141-4466-8f47-0a4be14e2fc1&#34;&gt;MS-RPRN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/topotam77&#34;&gt;@topotam77&lt;/a&gt; for finding and implementing &lt;strong&gt;PetitPotam&lt;/strong&gt; on &lt;a href=&#34;https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-efsr/08796ba8-01c8-4872-9221-1000ec2eff31&#34;&gt;MS-EFSR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/topotam77&#34;&gt;@topotam77&lt;/a&gt; for finding and &lt;a href=&#34;https://twitter.com/_nwodtuhs&#34;&gt;@_nwodtuhs&lt;/a&gt; for implementing &lt;strong&gt;ShadowCoerce&lt;/strong&gt; on &lt;a href=&#34;https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-fsrvp/dae107ec-8198-4778-a950-faa7edad125b&#34;&gt;MS-FSRVP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/filip_dragovic&#34;&gt;@filip_dragovic&lt;/a&gt; for finding and implementing &lt;strong&gt;DFSCoerce&lt;/strong&gt; on &lt;a href=&#34;https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-dfsnm/95a506a8-cae6-4c42-b19d-9c1ed1223979&#34;&gt;MS-DFSNM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>UberGuidoZ/Flipper</title>
    <updated>2022-07-12T01:34:07Z</updated>
    <id>tag:github.com,2022-07-12:/UberGuidoZ/Flipper</id>
    <link href="https://github.com/UberGuidoZ/Flipper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Playground (and dump) of stuff I make or modify for the Flipper Zero&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Playground (and dump) of stuff I made, modified, or found for the Flipper Zero.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/demucs</title>
    <updated>2022-07-12T01:34:07Z</updated>
    <id>tag:github.com,2022-07-12:/facebookresearch/demucs</id>
    <link href="https://github.com/facebookresearch/demucs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for the paper Hybrid Spectrogram and Waveform Source Separation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Demucs Music Source Separation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.fb.com/support-ukraine&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB&#34; alt=&#34;Support Ukraine&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/facebookresearch/demucs/workflows/tests/badge.svg?sanitize=true&#34; alt=&#34;tests badge&#34;&gt; &lt;img src=&#34;https://github.com/facebookresearch/demucs/workflows/linter/badge.svg?sanitize=true&#34; alt=&#34;linter badge&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the 3rd release of Demucs (v3), featuring hybrid source separation. &lt;strong&gt;For the waveform only Demucs (v2):&lt;/strong&gt; &lt;a href=&#34;https://github.com/facebookresearch/demucs/tree/v2&#34;&gt;Go this commit&lt;/a&gt;. If you are experiencing issues and want the old Demucs back, please fill an issue, and then you can get back to the v2 with &lt;code&gt;git checkout v2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide an implementation of Hybrid Demucs for music source separation, trained both on the &lt;a href=&#34;https://sigsep.github.io/datasets/musdb.html&#34;&gt;MusDB HQ&lt;/a&gt; dataset, and with internal extra training data. They can separate drums, bass and vocals from the rest and achieved the first rank at the 2021 &lt;a href=&#34;https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021&#34;&gt;Sony Music DemiXing Challenge (MDX)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Demucs is based on U-Net convolutional architecture inspired by &lt;a href=&#34;https://github.com/f90/Wave-U-Net&#34;&gt;Wave-U-Net&lt;/a&gt;. The most recent version features hybrid spectrogram/waveform separation, along with compressed residual branches, local attention and singular value regularization. Checkout our paper &lt;a href=&#34;https://arxiv.org/abs/2111.03600&#34;&gt;Hybrid Spectrogram and Waveform Source Separation&lt;/a&gt; for more details. As far as we know, Demucs is currently the only model supporting true end-to-end hybrid model training with shared information between the domains, as opposed to post-training model blending.&lt;/p&gt; &#xA;&lt;p&gt;When trained only on MusDB HQ, Hybrid Demucs achieved a SDR of 7.33 on the MDX test set, and 8.11 dB with 200 extra training tracks. It is particularly efficient for drums and bass extraction, although &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/kuielab&#34;&gt;KUIELAB-MDX-Net&lt;/a&gt; performs better for vocals and other accompaniments.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/demucs.png&#34; alt=&#34;Schema representing the structure of Demucs,&#xA;    with a dual U-Net structure with a shared core, one branch for the temporal domain,&#xA;    and one branch for the spectral domain.&#34; width=&#34;800px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Important news if you are already using Demucs&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/docs/release.md&#34;&gt;release notes&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;24/02/2022: Releasing v3.0.4: split into two stems (i.e. karaoke mode). Export as float32 or int24.&lt;/li&gt; &#xA; &lt;li&gt;17/12/2021: Releasing v3.0.3: bug fixes (thanks @keunwoochoi), memory drastically reduced on GPU (thanks @famzah) and new multi-core evaluation on CPU (&lt;code&gt;-j&lt;/code&gt; flag).&lt;/li&gt; &#xA; &lt;li&gt;12/11/2021: Releasing &lt;strong&gt;Demucs v3&lt;/strong&gt; with hybrid domain separation. Strong improvements on all sources. This is the model that won Sony MDX challenge.&lt;/li&gt; &#xA; &lt;li&gt;11/05/2021: Adding support for MusDB-HQ and arbitrary wav set, for the MDX challenge. For more information on joining the challenge with Demucs see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/docs/mdx.md&#34;&gt;the Demucs MDX instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;28/04/2021: &lt;strong&gt;Demucs v2&lt;/strong&gt;, with extra augmentation and DiffQ based quantization. &lt;strong&gt;EVERYTHING WILL BREAK&lt;/strong&gt;, please restart from scratch following the instructions hereafter. This version also adds overlap between prediction frames, with linear transition from one to the next, which should prevent sudden changes at frame boundaries. Also, Demucs is now on PyPI, so for separation only, installation is as easy as &lt;code&gt;pip install demucs&lt;/code&gt; :)&lt;/li&gt; &#xA; &lt;li&gt;13/04/2020: &lt;strong&gt;Demucs released under MIT&lt;/strong&gt;: We are happy to release Demucs under the MIT licence. We hope that this will broaden the impact of this research to new applications.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Comparison with other models&lt;/h2&gt; &#xA;&lt;p&gt;We provide hereafter a summary of the different metrics presented in the paper. You can also compare Hybrid Demucs (v3), &lt;a href=&#34;https://github.com/kuielab/mdx-net-submission&#34;&gt;KUIELAB-MDX-Net&lt;/a&gt;, &lt;a href=&#34;https://github.com/deezer/spleeter&#34;&gt;Spleeter&lt;/a&gt;, Open-Unmix, Demucs (v1), and Conv-Tasnet on one of my favorite songs on my &lt;a href=&#34;https://soundcloud.com/honualx/sets/source-separation-in-the-waveform-domain&#34;&gt;soundcloud playlist&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Comparison of accuracy&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;Overall SDR&lt;/code&gt; is the mean of the SDR for each of the 4 sources, &lt;code&gt;MOS Quality&lt;/code&gt; is a rating from 1 to 5 of the naturalness and absence of artifacts given by human listeners (5 = no artifacts), &lt;code&gt;MOS Contamination&lt;/code&gt; is a rating from 1 to 5 with 5 being zero contamination by other sources. We refer the reader to our &lt;a href=&#34;https://arxiv.org/abs/2111.03600&#34;&gt;paper&lt;/a&gt;, for more details.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Domain&lt;/th&gt; &#xA;   &lt;th&gt;Extra data?&lt;/th&gt; &#xA;   &lt;th&gt;Overall SDR&lt;/th&gt; &#xA;   &lt;th&gt;MOS Quality&lt;/th&gt; &#xA;   &lt;th&gt;MOS Contamination&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/f90/Wave-U-Net&#34;&gt;Wave-U-Net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;waveform&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sigsep/open-unmix-pytorch&#34;&gt;Open-Unmix&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;spectrogram&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;5.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.01733&#34;&gt;D3Net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;spectrogram&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;6.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/demucs/tree/v2&#34;&gt;Conv-Tasnet&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;waveform&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;5.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/demucs/tree/v2&#34;&gt;Demucs (v2)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;waveform&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;6.3&lt;/td&gt; &#xA;   &lt;td&gt;2.37&lt;/td&gt; &#xA;   &lt;td&gt;2.36&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.05418&#34;&gt;ResUNetDecouple+&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;spectrogram&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;6.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kuielab/mdx-net-submission&#34;&gt;KUIELAB-MDX-Net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;hybrid&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;7.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.86&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.55&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Hybrid Demucs (v3)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;hybrid&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.83&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.04&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.02410&#34;&gt;MMDenseLSTM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;spectrogram&lt;/td&gt; &#xA;   &lt;td&gt;804 songs&lt;/td&gt; &#xA;   &lt;td&gt;6.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.01733&#34;&gt;D3Net&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;spectrogram&lt;/td&gt; &#xA;   &lt;td&gt;1.5k songs&lt;/td&gt; &#xA;   &lt;td&gt;6.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deezer/spleeter&#34;&gt;Spleeter&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;spectrogram&lt;/td&gt; &#xA;   &lt;td&gt;25k songs&lt;/td&gt; &#xA;   &lt;td&gt;5.9&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You will need at least Python 3.7. See &lt;code&gt;requirements_minimal.txt&lt;/code&gt; for requirements for separation only, and &lt;code&gt;environment-[cpu|cuda].yml&lt;/code&gt; (or &lt;code&gt;requirements.txt&lt;/code&gt;) if you want to train a new model.&lt;/p&gt; &#xA;&lt;h3&gt;For Windows users&lt;/h3&gt; &#xA;&lt;p&gt;Everytime you see &lt;code&gt;python3&lt;/code&gt;, replace it with &lt;code&gt;python.exe&lt;/code&gt;. You should always run commands from the Anaconda console.&lt;/p&gt; &#xA;&lt;h3&gt;For musicians&lt;/h3&gt; &#xA;&lt;p&gt;If you just want to use Demucs to separate tracks, you can install it with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m pip install -U demucs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For bleeding edge versions, you can install directly from this repo using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m pip install -U git+https://github.com/facebookresearch/demucs#egg=demucs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Advanced OS support are provided on the following page, &lt;strong&gt;you must read the page for your OS before posting an issues&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;If you are using Windows:&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/docs/windows.md&#34;&gt;Windows support&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;If you are using MAC OS X:&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/docs/mac.md&#34;&gt;Mac OS X support&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;If you are using Linux:&lt;/strong&gt; &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/docs/linux.md&#34;&gt;Linux support&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;For machine learning scientists&lt;/h3&gt; &#xA;&lt;p&gt;If you have anaconda installed, you can run from the root of this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env update -f environment-cpu.yml  # if you don&#39;t have GPUs&#xA;conda env update -f environment-cuda.yml # if you have GPUs&#xA;conda activate demucs&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a &lt;code&gt;demucs&lt;/code&gt; environment with all the dependencies installed.&lt;/p&gt; &#xA;&lt;p&gt;You will also need to install &lt;a href=&#34;https://www.surina.net/soundtouch/soundstretch.html&#34;&gt;soundstretch/soundtouch&lt;/a&gt;: on Mac OSX you can do &lt;code&gt;brew install sound-touch&lt;/code&gt;, and on Ubuntu &lt;code&gt;sudo apt-get install soundstretch&lt;/code&gt;. This is used for the pitch/tempo augmentation.&lt;/p&gt; &#xA;&lt;h3&gt;Running in Docker&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to @xserrat, there is now a Docker image definition ready for using Demucs. This can ensure all libraries are correctly installed without interfering with the host OS. See his repo &lt;a href=&#34;https://github.com/xserrat/docker-facebook-demucs&#34;&gt;Docker Facebook Demucs&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h3&gt;Running from Colab&lt;/h3&gt; &#xA;&lt;p&gt;I made a Colab to easily separate track with Demucs. Note that transfer speeds with Colab are a bit slow for large media files, but it will allow you to use Demucs without installing anything.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1dC9nVxk3V_VPjUADsnFu8EiT-xnU1tGH?usp=sharing&#34;&gt;Demucs on Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Web Demo&lt;/h3&gt; &#xA;&lt;p&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/demucs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Graphical Interface&lt;/h3&gt; &#xA;&lt;p&gt;@CarloGao4 has released a GUI for Demucs: &lt;a href=&#34;https://github.com/CarlGao4/Demucs-Gui&#34;&gt;CarlGao4/Demucs-Gui&lt;/a&gt;. Downloads for Windows and macOS is available &lt;a href=&#34;https://github.com/CarlGao4/Demucs-Gui/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;@Anjok07 is providing a self contained GUI in &lt;a href=&#34;https://github.com/facebookresearch/demucs/issues/334&#34;&gt;UVR (Ultimate Vocal Remover)&lt;/a&gt; that supports Demucs.&lt;/p&gt; &#xA;&lt;h3&gt;Other providers&lt;/h3&gt; &#xA;&lt;p&gt;Audiostrip is providing free online separation with Demucs on their website &lt;a href=&#34;https://audiostrip.co.uk/&#34;&gt;https://audiostrip.co.uk/&lt;/a&gt;. &lt;a href=&#34;https://mvsep.com/&#34;&gt;MVSep&lt;/a&gt; also provides free online separation, select &lt;code&gt;Demucs3 model B&lt;/code&gt; for the best quality.&lt;/p&gt; &#xA;&lt;p&gt;Spleeter.io provides free online separation with Demucs on their website &lt;a href=&#34;https://www.spleeter.io/demucs&#34;&gt;https://www.spleeter.io/demucs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Separating tracks&lt;/h2&gt; &#xA;&lt;p&gt;In order to try Demucs, you can just run from any folder (as long as you properly installed it)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;demucs PATH_TO_AUDIO_FILE_1 [PATH_TO_AUDIO_FILE_2 ...]   # for Demucs&#xA;# If you used `pip install --user` you might need to replace demucs with python3 -m demucs&#xA;python3 -m demucs --mp3 --mp3-bitrate BITRATE PATH_TO_AUDIO_FILE_1  # output files saved as MP3&#xA;# If your filename contain spaces don&#39;t forget to quote it !!!&#xA;demucs &#34;my music/my favorite track.mp3&#34;&#xA;# You can select different models with `-n` mdx_q is the quantized model, smaller but maybe a bit less accurate.&#xA;demucs -n mdx_q myfile.mp3&#xA;# If you only want to separate vocals out of an audio, use `--two-stems=vocal` (You can also set to drums or bass)&#xA;demucs --two-stems=vocals myfile.mp3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have a GPU, but you run out of memory, please use &lt;code&gt;--segment SEGMENT&lt;/code&gt; to reduce length of each split. &lt;code&gt;SEGMENT&lt;/code&gt; should be changed to a integer. Personally recommend not less than 10 (the bigger the number is, the more memory is required, but quality may increase). Create an environment variable &lt;code&gt;PYTORCH_NO_CUDA_MEMORY_CACHING=1&lt;/code&gt; is also helpful. If this still cannot help, please add &lt;code&gt;-d cpu&lt;/code&gt; to the command line. See the section hereafter for more details on the memory requirements for GPU acceleration.&lt;/p&gt; &#xA;&lt;p&gt;Separated tracks are stored in the &lt;code&gt;separated/MODEL_NAME/TRACK_NAME&lt;/code&gt; folder. There you will find four stereo wav files sampled at 44.1 kHz: &lt;code&gt;drums.wav&lt;/code&gt;, &lt;code&gt;bass.wav&lt;/code&gt;, &lt;code&gt;other.wav&lt;/code&gt;, &lt;code&gt;vocals.wav&lt;/code&gt; (or &lt;code&gt;.mp3&lt;/code&gt; if you used the &lt;code&gt;--mp3&lt;/code&gt; option).&lt;/p&gt; &#xA;&lt;p&gt;All audio formats supported by &lt;code&gt;torchaudio&lt;/code&gt; can be processed (i.e. wav, mp3, flac, ogg/vorbis on Linux/Mac OS X etc.). On Windows, &lt;code&gt;torchaudio&lt;/code&gt; has limited support, so we rely on &lt;code&gt;ffmpeg&lt;/code&gt;, which should support pretty much anything. Audio is resampled on the fly if necessary. The output will be a wave file encoded as int16. You can save as float32 wav files with &lt;code&gt;--float32&lt;/code&gt;, or 24 bits integer wav with &lt;code&gt;--int24&lt;/code&gt;. You can pass &lt;code&gt;--mp3&lt;/code&gt; to save as mp3 instead, and set the bitrate with &lt;code&gt;--mp3-bitrate&lt;/code&gt; (default is 320kbps).&lt;/p&gt; &#xA;&lt;p&gt;It can happen that the output would need clipping, in particular due to some separation artifacts. Demucs will automatically rescale each output stem so as to avoid clipping. This can however break the relative volume between stems. If instead you prefer hard clipping, pass &lt;code&gt;--clip-mode clamp&lt;/code&gt;. You can also try to reduce the volume of the input mixture before feeding it to Demucs.&lt;/p&gt; &#xA;&lt;p&gt;Other pre-trained models can be selected with the &lt;code&gt;-n&lt;/code&gt; flag. The list of pre-trained models is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mdx&lt;/code&gt;: trained only on MusDB HQ, winning model on track A at the &lt;a href=&#34;https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021&#34;&gt;MDX&lt;/a&gt; challenge.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mdx_extra&lt;/code&gt;: trained with extra training data (including MusDB test set), ranked 2nd on the track B of the &lt;a href=&#34;https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021&#34;&gt;MDX&lt;/a&gt; challenge.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mdx_q&lt;/code&gt;, &lt;code&gt;mdx_extra_q&lt;/code&gt;: quantized version of the previous models. Smaller download and storage but quality can be slightly worse. &lt;code&gt;mdx_extra_q&lt;/code&gt; is the default model used.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SIG&lt;/code&gt;: where &lt;code&gt;SIG&lt;/code&gt; is a single model from the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/docs/training.md#model-zoo&#34;&gt;model zoo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Faster separation:&lt;/strong&gt; if you want faster separation, in particular if you do not have a GPU, you can use &lt;code&gt;-n 83fc094f&lt;/code&gt; for instance to use a single model, as opposed to the bag of 4 models used for the competition.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--two-stems=vocals&lt;/code&gt; option allows to separate vocals from the rest (e.g. karaoke mode). &lt;code&gt;vocals&lt;/code&gt; can be changed into any source in the selected model. This will mix the files after separating the mix fully, so this won&#39;t be faster or use less memory.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--shifts=SHIFTS&lt;/code&gt; performs multiple predictions with random shifts (a.k.a the &lt;em&gt;shift trick&lt;/em&gt;) of the input and average them. This makes prediction &lt;code&gt;SHIFTS&lt;/code&gt; times slower. Don&#39;t use it unless you have a GPU.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;--overlap&lt;/code&gt; option controls the amount of overlap between prediction windows (for Demucs one window is 10 seconds). Default is 0.25 (i.e. 25%) which is probably fine.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;-j&lt;/code&gt; flag allow to specify a number of parallel jobs (e.g. &lt;code&gt;demucs -j 2 myfile.mp3&lt;/code&gt;). This will multiply by the same amount the RAM used so be careful!&lt;/p&gt; &#xA;&lt;h3&gt;Memory requirements for GPU acceleration&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use GPU acceleration, you will need at least 3GB of RAM on your GPU for &lt;code&gt;demucs&lt;/code&gt;. However, about 7GB of RAM will be required if you use the default arguments. Add &lt;code&gt;--segment SEGMENT&lt;/code&gt; to change size of each split. If you only have 3GB memory, set SEGMENT to 8 (though quality may be worse if this argument is too small). Creating an environment variable &lt;code&gt;PYTORCH_NO_CUDA_MEMORY_CACHING=1&lt;/code&gt; can help users with even smaller RAM such as 2GB (I separated a track that is 4 minutes but only 1.5GB is used), but this would make the separation slower.&lt;/p&gt; &#xA;&lt;p&gt;If you do not have enough memory on your GPU, simply add &lt;code&gt;-d cpu&lt;/code&gt; to the command line to use the CPU. With Demucs, processing time should be roughly equal to 1.5 times the duration of the track.&lt;/p&gt; &#xA;&lt;h2&gt;Training Demucs&lt;/h2&gt; &#xA;&lt;p&gt;If you want to train (Hybrid) Demucs, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/docs/training.md&#34;&gt;training doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;MDX Challenge reproduction&lt;/h2&gt; &#xA;&lt;p&gt;In order to reproduce the results from the Track A and Track B submissions, checkout the &lt;a href=&#34;https://github.com/adefossez/mdx21_demucs&#34;&gt;MDX Hybrid Demucs submission repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to cite&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{defossez2021hybrid,&#xA;  title={Hybrid Spectrogram and Waveform Source Separation},&#xA;  author={D{\&#39;e}fossez, Alexandre},&#xA;  booktitle={Proceedings of the ISMIR 2021 Workshop on Music Source Separation},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Demucs is released under the MIT license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/demucs/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
</feed>