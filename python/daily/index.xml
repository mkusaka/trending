<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-24T01:36:54Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TarlogicSecurity/BlueSpy</title>
    <updated>2024-03-24T01:36:54Z</updated>
    <id>tag:github.com,2024-03-24:/TarlogicSecurity/BlueSpy</id>
    <link href="https://github.com/TarlogicSecurity/BlueSpy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BlueSpy - PoC to record audio from a Bluetooth device&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TarlogicSecurity/BlueSpy/main/resources/BlueSpy.png&#34; alt=&#34;BlueSpy script&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the implementation of a proof of concept to record and replay audio from a bluetooth device without the legitimate user&#39;s awareness.&lt;/p&gt; &#xA;&lt;p&gt;The PoC was demonstrated during the talk &lt;strong&gt;BSAM: Seguridad en Bluetooth&lt;/strong&gt; at &lt;strong&gt;RootedCON 2024&lt;/strong&gt; in Madrid.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s designed to raise awareness about the insecure use of Bluetooth devices, and the need of a consistent methodology for security evaluations. That&#39;s the purspose of &lt;strong&gt;BSAM, the Bluetooth Security Assessment Methodology&lt;/strong&gt;, published by Tarlogic and available &lt;a href=&#34;https://www.tarlogic.com/bsam/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This proof of concept exploits the failure to comply with the &lt;a href=&#34;https://www.tarlogic.com/bsam/controls/bluetooth-pairing-without-interaction/&#34;&gt;&lt;strong&gt;BSAM-PA-05 control&lt;/strong&gt;&lt;/a&gt; within the BSAM methodolgy. Consequently, the device enables the pairing procedure without requiring user interaction and exposes its functionality to any agent within the signal range.&lt;/p&gt; &#xA;&lt;p&gt;More information on our &lt;a href=&#34;https://www.tarlogic.com/blog/bluespy-spying-on-bluetooth-conversations/&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;The code is written in Python and has been tested with Python 3.11.8, but it mainly uses widely available tools in Linux systems.&lt;/p&gt; &#xA;&lt;p&gt;The PoC uses the following tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;bluetoothctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;btmgmt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pactl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;parecord&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;paplay&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In Arch Linux distributions, &lt;code&gt;bluetoothctl&lt;/code&gt; and &lt;code&gt;btmgmt&lt;/code&gt; can be installed with the package &lt;code&gt;bluez-utils&lt;/code&gt;, while &lt;code&gt;pactl&lt;/code&gt;, &lt;code&gt;parecord&lt;/code&gt; and &lt;code&gt;paplay&lt;/code&gt; are avaliable in the &lt;code&gt;libpulse&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;p&gt;For the PoC to work, it is necessary to have a working instalation of the BlueZ Bluetooth stack, available in the &lt;code&gt;bluez&lt;/code&gt;package for Arch Linux distributions. A working instalation of an audio server compatible with PulseAudio, such as PipeWire, is also required to record and play audio.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Ensure that your device is capable of functioning as an audio source, meaning it has a microphone, and that it is discoverable and connectable via Bluetooth.&lt;/p&gt; &#xA;&lt;p&gt;For instance, to be discoverable and connectable, the earbuds used during the talk must be outside of their charging case. By default, they only activate the microphone when placed in the user&#39;s ears, although this setting can be adjusted in the configuration app.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, ensure that the device is not already connected, or alteratively, that it supports multiple connections.&lt;/p&gt; &#xA;&lt;h2&gt;Execution&lt;/h2&gt; &#xA;&lt;p&gt;Firstly, the address of the device must be discovered using a tool such as &lt;code&gt;bluetoothctl&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bluetoothctl&#xA;[bluetooth]# scan on&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the address of the device is discovered, the script can handle the rest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python BlueSpy.py -a &amp;lt;address&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: The script might prompt for superuser permissions to modify the configuration of your &lt;strong&gt;BlueZ&lt;/strong&gt; instance and pair with the remote device.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;BlueSpy.py&lt;/code&gt; is the main script that executes every step of the process. However, if you encounter issues with nay of the phases, so it might be helpful to execute them individually:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;pair.py&lt;/code&gt; utilizes the command-line tool &lt;code&gt;btmgmt&lt;/code&gt; to modify the configuration of your &lt;strong&gt;BlueZ&lt;/strong&gt; and initiate a pairing process with the remote device. The exact commands used are in the &lt;code&gt;pair&lt;/code&gt; function inside &lt;code&gt;core.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;connect.py&lt;/code&gt; utilizes the command-line tool &lt;code&gt;bluetoothctl&lt;/code&gt; to initiate a quick scan (necesary for BlueZ) and establish a connection to the device. The exact commands used are in the &lt;code&gt;connect&lt;/code&gt; function inside &lt;code&gt;core.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;just_record.py&lt;/code&gt; utilizes the command-line tools &lt;code&gt;pactl&lt;/code&gt; and &lt;code&gt;parecord&lt;/code&gt; to search for the device in the system&#39;s audio sources (it must function as a microphone) and initiate a recording session. The exact commands used are in the &lt;code&gt;record&lt;/code&gt; function inside &lt;code&gt;core.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;playback&lt;/code&gt; function inside &lt;code&gt;core.py&lt;/code&gt; executes &lt;code&gt;paplay&lt;/code&gt; to play back the captured audio.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you encounter issues with any of the phases, examine the commands in &lt;code&gt;core.py&lt;/code&gt; and try to execute them in a shell. This will provide more information on what may be failing.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions regarding how the Bluetooth standard operates or how to assess the security of a Bluetooth device, please refer to our BSAM methodology webpage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tarlogic.com/bsam/&#34;&gt;BSAM: Bluetooth Security Assessment Methodology&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>OpenInterpreter/01</title>
    <updated>2024-03-24T01:36:54Z</updated>
    <id>tag:github.com,2024-03-24:/OpenInterpreter/01</id>
    <link href="https://github.com/OpenInterpreter/01" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The open-source language model computer&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;‚óã&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/Hvz9Axh84z&#34;&gt;&lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/discord/1146610656779440188?logo=discord&amp;amp;style=social&amp;amp;logoColor=black&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;strong&gt;The open-source language model computer.&lt;/strong&gt;&lt;br&gt; &#xA; &lt;!-- &lt;br&gt;&lt;a href=&#34;https://openinterpreter.com&#34;&gt;Preorder the Light&lt;/a&gt;‚Äé ‚Äé |‚Äé ‚Äé &lt;a href=&#34;https://openinterpreter.com&#34;&gt;Get Updates&lt;/a&gt;‚Äé ‚Äé |‚Äé ‚Äé &lt;a href=&#34;https://docs.openinterpreter.com/&#34;&gt;Documentation&lt;/a&gt;&lt;br&gt; --&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://www.openinterpreter.com/OI-O1-BannerDemo-3.jpg&#34; alt=&#34;OI-O1-BannerDemo-2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We want to help you build. &lt;a href=&#34;https://0ggfznkwh4j.typeform.com/to/kkStE8WF&#34;&gt;Apply for 1-on-1 support.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;WARNING:&lt;/strong&gt; This experimental project is under rapid development and lacks basic safeguards. Until a stable &lt;code&gt;1.0&lt;/code&gt; release, &lt;strong&gt;ONLY&lt;/strong&gt; run this repository on devices without sensitive information or access to paid services. ‚ö†Ô∏è&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;The 01 Project&lt;/strong&gt; is building an open-source ecosystem for AI devices.&lt;/p&gt; &#xA;&lt;p&gt;Our flagship operating system can power conversational devices like the Rabbit R1, Humane Pin, or &lt;a href=&#34;https://www.youtube.com/watch?v=1ZXugicgn6U&#34;&gt;Star Trek computer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We intend to become the GNU/Linux of this space by staying open, modular, and free.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Software&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/OpenInterpreter/01 # Clone the repository&#xA;cd 01/software # CD into the source directory&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- &gt; Not working? Read our [setup guide](https://docs.openinterpreter.com/getting-started/setup). --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install portaudio ffmpeg cmake # Install Mac OSX dependencies&#xA;poetry install # Install Python dependencies&#xA;export OPENAI_API_KEY=sk... # OR run `poetry run 01 --local` to run everything locally&#xA;poetry run 01 # Runs the 01 Light simulator (hold your spacebar, speak, release)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Hardware&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;strong&gt;01 Light&lt;/strong&gt; is an ESP32-based voice interface. &lt;a href=&#34;https://github.com/OpenInterpreter/01/tree/main/hardware/light&#34;&gt;Build instructions are here.&lt;/a&gt; It works in tandem with the &lt;strong&gt;01 Server&lt;/strong&gt; (&lt;a href=&#34;https://github.com/OpenInterpreter/01/raw/main/README.md#01-server&#34;&gt;setup guide below&lt;/a&gt;) running on your home computer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mac OSX&lt;/strong&gt; and &lt;strong&gt;Ubuntu&lt;/strong&gt; are supported by running &lt;code&gt;poetry run 01&lt;/code&gt;. This uses your spacebar to simulate the 01 Light.&lt;/li&gt; &#xA; &lt;li&gt;(coming soon) The &lt;strong&gt;01 Heavy&lt;/strong&gt; is a standalone device that runs everything locally.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;We need your help supporting &amp;amp; building more hardware.&lt;/strong&gt; The 01 should be able to run on any device with input (microphone, keyboard, etc.), output (speakers, screens, motors, etc.), and an internet connection (or sufficient compute to run everything locally). &lt;a href=&#34;https://github.com/OpenInterpreter/01/raw/main/CONTRIBUTING.md&#34;&gt;Contribution Guide ‚Üí&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;What does it do?&lt;/h1&gt; &#xA;&lt;p&gt;The 01 exposes a speech-to-speech websocket at &lt;code&gt;localhost:10001&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you stream raw audio bytes to &lt;code&gt;/&lt;/code&gt; in &lt;a href=&#34;https://docs.openinterpreter.com/protocols/lmc-messages&#34;&gt;LMC format&lt;/a&gt;, you will receive its response in the same format.&lt;/p&gt; &#xA;&lt;p&gt;Inspired in part by &lt;a href=&#34;https://twitter.com/karpathy/status/1723140519554105733&#34;&gt;Andrej Karpathy&#39;s LLM OS&lt;/a&gt;, we run a &lt;a href=&#34;https://github.com/OpenInterpreter/open-interpreter&#34;&gt;code-interpreting language model&lt;/a&gt;, and call it when certain events occur at your computer&#39;s &lt;a href=&#34;https://github.com/OpenInterpreter/01/raw/main/software/source/server/utils/kernel.py&#34;&gt;kernel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The 01 wraps this in a voice interface:&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img width=&#34;100%&#34; alt=&#34;LMC&#34; src=&#34;https://github.com/OpenInterpreter/01/assets/63927363/52417006-a2ca-4379-b309-ffee3509f5d4&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Protocols&lt;/h1&gt; &#xA;&lt;h2&gt;LMC Messages&lt;/h2&gt; &#xA;&lt;p&gt;To communicate with different components of this system, we introduce &lt;a href=&#34;https://docs.openinterpreter.com/protocols/lmc-messages&#34;&gt;LMC Messages&lt;/a&gt; format, which extends OpenAI‚Äôs messages format to include a &#34;computer&#34; role.&lt;/p&gt; &#xA;&lt;h2&gt;Dynamic System Messages&lt;/h2&gt; &#xA;&lt;p&gt;Dynamic System Messages enable you to execute code inside the LLM&#39;s system message, moments before it appears to the AI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Edit the following settings in i.py&#xA;interpreter.system_message = r&#34; The time is {{time.time()}}. &#34; # Anything in double brackets will be executed as Python&#xA;interpreter.chat(&#34;What time is it?&#34;) # It will know, without making a tool/API call&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Guides&lt;/h1&gt; &#xA;&lt;h2&gt;01 Server&lt;/h2&gt; &#xA;&lt;p&gt;To run the server on your Desktop and connect it to your 01 Light, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install ngrok/ngrok/ngrok&#xA;ngrok authtoken ... # Use your ngrok authtoken&#xA;poetry run 01 --server --expose&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The final command will print a server URL. You can enter this into your 01 Light&#39;s captive WiFi portal to connect to your 01 Server.&lt;/p&gt; &#xA;&lt;h2&gt;Local Mode&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;poetry run 01 --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to run local speech-to-text using Whisper, you must install Rust. Follow the instructions given &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Customizations&lt;/h2&gt; &#xA;&lt;p&gt;To customize the behavior of the system, edit the &lt;a href=&#34;https://docs.openinterpreter.com/settings/all-settings&#34;&gt;system message, model, skills library path,&lt;/a&gt; etc. in &lt;code&gt;i.py&lt;/code&gt;. This file sets up an interpreter, and is powered by Open Interpreter.&lt;/p&gt; &#xA;&lt;h2&gt;Ubuntu Dependencies&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install portaudio19-dev ffmpeg cmake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenInterpreter/01/graphs/contributors&#34;&gt;&lt;img src=&#34;https://contrib.rocks/image?repo=OpenInterpreter/01&amp;amp;max=2000&#34; alt=&#34;01 project contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://raw.githubusercontent.com/OpenInterpreter/01/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; for more details on how to get involved.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://raw.githubusercontent.com/OpenInterpreter/01/main/ROADMAP.md&#34;&gt;our roadmap&lt;/a&gt; to see the future of the 01.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Background&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/KillianLucas/01/raw/main/CONTEXT.md&#34;&gt;Context ‚Üó&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The story of devices that came before the 01.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/KillianLucas/01/tree/main/INSPIRATION.md&#34;&gt;Inspiration ‚Üó&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Things we want to steal great ideas from.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;‚óã&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>SakanaAI/evolutionary-model-merge</title>
    <updated>2024-03-24T01:36:54Z</updated>
    <id>tag:github.com,2024-03-24:/SakanaAI/evolutionary-model-merge</id>
    <link href="https://github.com/SakanaAI/evolutionary-model-merge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository of Evolutionary Optimization of Model Merging Recipes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üêü Evolutionary Optimization of Model Merging Recipes&lt;/h1&gt; &#xA;&lt;p&gt;ü§ó &lt;a href=&#34;https://huggingface.co/SakanaAI&#34;&gt;Models&lt;/a&gt; | üëÄ &lt;a href=&#34;https://huggingface.co/spaces/SakanaAI/EvoVLM-JP&#34;&gt;Demo&lt;/a&gt; | üìö &lt;a href=&#34;https://arxiv.org/abs/2403.13187&#34;&gt;Paper&lt;/a&gt; | üìù &lt;a href=&#34;https://sakana.ai/evolutionary-model-merge/&#34;&gt;Blog&lt;/a&gt; | üê¶ &lt;a href=&#34;https://twitter.com/SakanaAILabs&#34;&gt;Twitter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/SakanaAI/evolutionary-model-merge/main/assets/method.gif&#34; alt=&#34;Method&#34; title=&#34;method&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This repository serves as a central hub for SakanaAI&#39;s &lt;a href=&#34;https://arxiv.org/abs/2403.13187&#34;&gt;Evolutionary Model Merge&lt;/a&gt; series, showcasing its releases and resources. It includes models and code for reproducing the evaluation presented in our paper. Look forward to more updates and additions coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h3&gt;Our Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;License&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B&#34;&gt;EvoLLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Microsoft Research License&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/WizardLM/WizardMath-7B-V1.1&#34;&gt;WizardMath-7B-V1.1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/GAIR/Abel-7B-002&#34;&gt;GAIR/Abel-7B-002&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B&#34;&gt;EvoLLM-JP-v1-10B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;10B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Microsoft Research License&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;EvoLLM-JP-v1-7B, &lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B&#34;&gt;EvoLLM-JP-A-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apache 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/upaya07/Arithmo2-Mistral-7B&#34;&gt;Arithmo2-Mistral-7B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/GAIR/Abel-7B-002&#34;&gt;GAIR/Abel-7B-002&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B&#34;&gt;EvoVLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;7B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Apache 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b&#34;&gt;LLaVA-1.6-Mistral-7B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;shisa-gamma-7b-v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Comparing EvoLLM-JP w/ Source LLMs&lt;/h3&gt; &#xA;&lt;p&gt;For details on the evaluation, please refer to Section 4.1 of the paper.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;MGSM-JA (acc ‚Üë)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;&lt;a href=&#34;https://github.com/Stability-AI/lm-evaluation-harness/tree/jp-stable&#34;&gt;lm-eval-harness&lt;/a&gt; (avg ‚Üë)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/augmxnt/shisa-gamma-7b-v1&#34;&gt;Shisa Gamma 7B v1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;9.6&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;66.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/WizardLM/WizardMath-7B-V1.1&#34;&gt;WizardMath 7B V1.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;18.4&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;60.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/GAIR/Abel-7B-002&#34;&gt;Abel 7B 002&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;30.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/upaya07/Arithmo2-Mistral-7B&#34;&gt;Arithmo2 Mistral 7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;24.0&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;56.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B&#34;&gt;EvoLLM-JP-A-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;52.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;69.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B&#34;&gt;EvoLLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;52.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;70.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B&#34;&gt;EvoLLM-JP-v1-10B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;55.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;66.2&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Comparing EvoVLM-JP w/ Existing VLMs&lt;/h3&gt; &#xA;&lt;p&gt;For details on the evaluation, please see Section 4.2 of the paper.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;JA-VG-VQA-500 (ROUGE-L ‚Üë)&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;JA-VLM-Bench-In-the-Wild (ROUGE-L ‚Üë)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://llava-vl.github.io/blog/2024-01-30-llava-next/&#34;&gt;LLaVA-1.6-Mistral-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;14.32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;41.10&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/japanese-stable-vlm&#34;&gt;Japanese Stable VLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;-&lt;sup&gt;*1&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;40.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k&#34;&gt;Heron BLIP Japanese StableLM Base 7B llava-620k&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;8.73&lt;sup&gt;*2&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;27.37&lt;sup&gt;*2&lt;/sup&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B&#34;&gt;EvoVLM-JP-v1-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;19.70&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;&lt;strong&gt;51.25&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;*1: Japanese Stable VLM cannot be evaluated using the JA-VG-VQA-500 dataset because this model has used this dataset for training.&lt;/li&gt; &#xA; &lt;li&gt;*2: We are checking with the authors to see if this current results are valid.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reproducing the Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;1. Clone the Repo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/SakanaAI/evolutionary-model-merge.git&#xA;cd evolutionary-model-merge&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Download fastext Model&lt;/h3&gt; &#xA;&lt;p&gt;We use fastext to detect language for evaluation. Please download &lt;code&gt;lid.176.ftz&lt;/code&gt; from &lt;a href=&#34;https://fasttext.cc/docs/en/language-identification.html&#34;&gt;this link&lt;/a&gt; and place it in your current directory. If you place the file in a directory other than the current directory, specify the path to the file using the &lt;code&gt;LID176FTZ_PATH&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;h3&gt;3. Install Libraries&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We conducted our tests in the following environment: Python Version 3.10.12 and CUDA Version 12.3. We cannot guarantee that it will work in other environments.&lt;/p&gt; &#xA;&lt;h3&gt;4. Run&lt;/h3&gt; &#xA;&lt;p&gt;To launch evaluation, run the following script with a certain config. All configs used for the paper are in &lt;code&gt;configs&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python evaluate.py --config_path {path-to-config}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the developers of the source models for their contributions and for making their work available. Our math evaluation code builds on the WizardMath repository, and we are grateful for their work.&lt;/p&gt;</summary>
  </entry>
</feed>