<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-20T01:42:38Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pengxiao-song/LaWGPT</title>
    <updated>2023-05-20T01:42:38Z</updated>
    <id>tag:github.com,2023-05-20:/pengxiao-song/LaWGPT</id>
    <link href="https://github.com/pengxiao-song/LaWGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ‰ Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. åŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LaWGPTï¼šåŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/logo/lawgpt.jpeg&#34; width=&#34;80%&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-Wiki-brightgreen&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/version-beta1.0-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/pengxiao-song/lawgpt&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://www.lamda.nju.edu.cn/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/support-NJU--LAMDA-9cf.svg&#34;&gt;&lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p&gt;LaWGPT æ˜¯ä¸€ç³»åˆ—åŸºäºä¸­æ–‡æ³•å¾‹çŸ¥è¯†çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¯¥ç³»åˆ—æ¨¡å‹åœ¨é€šç”¨ä¸­æ–‡åŸºåº§æ¨¡å‹ï¼ˆå¦‚ Chinese-LLaMAã€ChatGLM ç­‰ï¼‰çš„åŸºç¡€ä¸Šæ‰©å……æ³•å¾‹é¢†åŸŸä¸“æœ‰è¯è¡¨ã€&lt;strong&gt;å¤§è§„æ¨¡ä¸­æ–‡æ³•å¾‹è¯­æ–™é¢„è®­ç»ƒ&lt;/strong&gt;ï¼Œå¢å¼ºäº†å¤§æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œ&lt;strong&gt;æ„é€ æ³•å¾‹é¢†åŸŸå¯¹è¯é—®ç­”æ•°æ®é›†ã€ä¸­å›½å¸æ³•è€ƒè¯•æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒ&lt;/strong&gt;ï¼Œæå‡äº†æ¨¡å‹å¯¹æ³•å¾‹å†…å®¹çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è¯¦ç»†å†…å®¹è¯·å‚è€ƒ&lt;a href=&#34;&#34;&gt;æŠ€æœ¯æŠ¥å‘Š&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æŒç»­å¼€å±•ï¼Œæ³•å¾‹é¢†åŸŸæ•°æ®é›†åŠç³»åˆ—æ¨¡å‹åç»­ç›¸ç»§å¼€æºï¼Œæ•¬è¯·å…³æ³¨ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æ›´æ–°&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸª´ 2023/05/15ï¼šå‘å¸ƒ &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;ä¸­æ–‡æ³•å¾‹æ•°æ®æºæ±‡æ€»ï¼ˆAwesome Chinese Legal Resourcesï¼‰&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/raw/main/resources/legal_vocab.txt&#34;&gt;æ³•å¾‹é¢†åŸŸè¯è¡¨&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸŒŸ 2023/05/13ï¼šå…¬å¼€å‘å¸ƒ &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Legal--Base--7B-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-LaWGPT--7B--beta1.0-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Legal-Base-7B&lt;/strong&gt;ï¼šæ³•å¾‹åŸºåº§æ¨¡å‹ï¼Œä½¿ç”¨ 50w ä¸­æ–‡è£åˆ¤æ–‡ä¹¦æ•°æ®äºŒæ¬¡é¢„è®­ç»ƒ&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;LaWGPT-7B-beta1.0&lt;/strong&gt;ï¼šæ³•å¾‹å¯¹è¯æ¨¡å‹ï¼Œæ„é€  30w é«˜è´¨é‡æ³•å¾‹é—®ç­”æ•°æ®é›†åŸºäº Legal-Base-7B æŒ‡ä»¤ç²¾è°ƒ&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ğŸŒŸ 2023/04/12ï¼šå†…éƒ¨æµ‹è¯• &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model-Lawgpt--7B--alpha-yellow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;LaWGPT-7B-alpha&lt;/strong&gt;ï¼šåœ¨ Chinese-LLaMA-7B çš„åŸºç¡€ä¸Šç›´æ¥æ„é€  30w æ³•å¾‹é—®ç­”æ•°æ®é›†æŒ‡ä»¤ç²¾è°ƒ&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å¿«é€Ÿå¼€å§‹&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;å‡†å¤‡ä»£ç ï¼Œåˆ›å»ºç¯å¢ƒ&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:pengxiao-song/LaWGPT.git&#xA;cd LaWGPT&#xA;conda activate lawgpt&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;åˆå¹¶æ¨¡å‹æƒé‡ï¼ˆå¯é€‰ï¼‰&lt;/p&gt; &lt;p&gt;&lt;strong&gt;å¦‚æœæ‚¨æƒ³ä½¿ç”¨ LaWGPT-7B-alpha æ¨¡å‹ï¼Œå¯è·³è¿‡æ”¹æ­¥ï¼Œç›´æ¥è¿›å…¥æ­¥éª¤3.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;å¦‚æœæ‚¨æƒ³ä½¿ç”¨ LaWGPT-7B-beta1.0 æ¨¡å‹ï¼š&lt;/p&gt; &lt;p&gt;ç”±äº &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-LLaMA&lt;/a&gt; å‡æœªå¼€æºæ¨¡å‹æƒé‡ã€‚æ ¹æ®ç›¸åº”å¼€æºè®¸å¯ï¼Œ&lt;strong&gt;æœ¬é¡¹ç›®åªèƒ½å‘å¸ƒ LoRA æƒé‡&lt;/strong&gt;ï¼Œæ— æ³•å‘å¸ƒå®Œæ•´çš„æ¨¡å‹æƒé‡ï¼Œè¯·å„ä½è°…è§£ã€‚&lt;/p&gt; &lt;p&gt;æœ¬é¡¹ç›®ç»™å‡º&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT/wiki/%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6&#34;&gt;åˆå¹¶æ–¹å¼&lt;/a&gt;ï¼Œè¯·å„ä½è·å–åŸç‰ˆæƒé‡åè‡ªè¡Œé‡æ„æ¨¡å‹ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å¯åŠ¨ç¤ºä¾‹&lt;/p&gt; &lt;p&gt;å¯åŠ¨æœ¬åœ°æœåŠ¡ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate lawgpt&#xA;cd LaWGPT&#xA;sh src/scripts/generate.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;æ¥å…¥æœåŠ¡ï¼š&lt;/p&gt; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/demo.png&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;é¡¹ç›®ç»“æ„&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;LaWGPT&#xA;â”œâ”€â”€ assets # é¡¹ç›®é™æ€èµ„æº&#xA;â”œâ”€â”€ data   # è¯­æ–™åŠç²¾è°ƒæ•°æ®&#xA;â”œâ”€â”€ tools  # æ•°æ®æ¸…æ´—ç­‰å·¥å…·&#xA;â”œâ”€â”€ README.md&#xA;â”œâ”€â”€ requirements.txt&#xA;â””â”€â”€ src    # æºç &#xA;    â”œâ”€â”€ finetune.py&#xA;    â”œâ”€â”€ generate.py&#xA;    â”œâ”€â”€ models  # åŸºåº§æ¨¡å‹åŠ Lora æƒé‡&#xA;    â”‚   â”œâ”€â”€ base_models&#xA;    â”‚   â””â”€â”€ lora_weights&#xA;    â”œâ”€â”€ outputs&#xA;    â”œâ”€â”€ scripts # è„šæœ¬æ–‡ä»¶&#xA;    â”‚   â”œâ”€â”€ finetune.sh # æŒ‡ä»¤å¾®è°ƒ&#xA;    â”‚   â””â”€â”€ generate.sh # æœåŠ¡åˆ›å»º&#xA;    â”œâ”€â”€ templates&#xA;    â””â”€â”€ utils&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;æ•°æ®æ„å»º&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®åŸºäºä¸­æ–‡è£åˆ¤æ–‡ä¹¦ç½‘å…¬å¼€æ³•å¾‹æ–‡ä¹¦æ•°æ®ã€å¸æ³•è€ƒè¯•æ•°æ®ç­‰æ•°æ®é›†å±•å¼€ï¼Œè¯¦æƒ…å‚è€ƒ&lt;a href=&#34;&#34;&gt;ä¸­æ–‡æ³•å¾‹æ•°æ®æ±‡æ€»&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;åˆçº§æ•°æ®ç”Ÿæˆï¼šæ ¹æ® &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca#data-generation-process&#34;&gt;Stanford_alpaca&lt;/a&gt; å’Œ &lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;self-instruct&lt;/a&gt; æ–¹å¼ç”Ÿæˆå¯¹è¯é—®ç­”æ•°æ®&lt;/li&gt; &#xA; &lt;li&gt;çŸ¥è¯†å¼•å¯¼çš„æ•°æ®ç”Ÿæˆï¼šé€šè¿‡ Knowledge-based Self-Instruct æ–¹å¼åŸºäºä¸­æ–‡æ³•å¾‹ç»“æ„åŒ–çŸ¥è¯†ç”Ÿæˆæ•°æ®ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¼•å…¥ ChatGPT æ¸…æ´—æ•°æ®ï¼Œè¾…åŠ©æ„é€ é«˜è´¨é‡æ•°æ®é›†ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;æ¨¡å‹è®­ç»ƒ&lt;/h2&gt; &#xA;&lt;p&gt;LawGPT ç³»åˆ—æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ç¬¬ä¸€é˜¶æ®µï¼šæ‰©å……æ³•å¾‹é¢†åŸŸè¯è¡¨ï¼Œåœ¨å¤§è§„æ¨¡æ³•å¾‹æ–‡ä¹¦åŠæ³•å…¸æ•°æ®ä¸Šé¢„è®­ç»ƒ Chinese-LLaMA&lt;/li&gt; &#xA; &lt;li&gt;ç¬¬äºŒé˜¶æ®µï¼šæ„é€ æ³•å¾‹é¢†åŸŸå¯¹è¯é—®ç­”æ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠæŒ‡ä»¤ç²¾è°ƒ&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;äºŒæ¬¡è®­ç»ƒæµç¨‹&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å‚è€ƒ &lt;code&gt;src/data/example_instruction_train.json&lt;/code&gt; æ„é€ äºŒæ¬¡è®­ç»ƒæ•°æ®é›†&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;code&gt;src/scripts/train_lora.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;æŒ‡ä»¤ç²¾è°ƒæ­¥éª¤&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å‚è€ƒ &lt;code&gt;src/data/example_instruction_tune.json&lt;/code&gt; æ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†&lt;/li&gt; &#xA; &lt;li&gt;è¿è¡Œ &lt;code&gt;src/scripts/finetune.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;è®¡ç®—èµ„æº&lt;/h3&gt; &#xA;&lt;p&gt;8 å¼  Tesla V100-SXM2-32GB&lt;/p&gt; &#xA;&lt;h2&gt;æ¨¡å‹è¯„ä¼°&lt;/h2&gt; &#xA;&lt;h3&gt;è¾“å‡ºç¤ºä¾‹&lt;/h3&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šè¯·ç»™å‡ºåˆ¤å†³æ„è§ã€‚&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-05.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šè¯·ä»‹ç»èµŒåšç½ªçš„å®šä¹‰ã€‚&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-06.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šè¯·é—®åŠ ç­å·¥èµ„æ€ä¹ˆç®—ï¼Ÿ&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-04.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šæ°‘é—´å€Ÿè´·å—å›½å®¶ä¿æŠ¤çš„åˆæ³•åˆ©æ¯æ˜¯å¤šå°‘?&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-02.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šæ¬ äº†ä¿¡ç”¨å¡çš„é’±è¿˜ä¸ä¸Šè¦åç‰¢å—ï¼Ÿ&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-01.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;é—®é¢˜ï¼šä½ èƒ½å¦å†™ä¸€æ®µæŠ¢åŠ«ç½ªç½ªåçš„æ¡ˆæƒ…æè¿°ï¼Ÿ&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pengxiao-song/LaWGPT/main/assets/demo/example-03.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;å±€é™æ€§&lt;/h3&gt; &#xA;&lt;p&gt;ç”±äºè®¡ç®—èµ„æºã€æ•°æ®è§„æ¨¡ç­‰å› ç´ é™åˆ¶ï¼Œå½“å‰é˜¶æ®µ LawGPT å­˜åœ¨è¯¸å¤šå±€é™æ€§ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æ•°æ®èµ„æºæœ‰é™ã€æ¨¡å‹å®¹é‡è¾ƒå°ï¼Œå¯¼è‡´å…¶ç›¸å¯¹è¾ƒå¼±çš„æ¨¡å‹è®°å¿†å’Œè¯­è¨€èƒ½åŠ›ã€‚å› æ­¤ï¼Œåœ¨é¢å¯¹äº‹å®æ€§çŸ¥è¯†ä»»åŠ¡æ—¶ï¼Œå¯èƒ½ä¼šç”Ÿæˆä¸æ­£ç¡®çš„ç»“æœã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯¥ç³»åˆ—æ¨¡å‹åªè¿›è¡Œäº†åˆæ­¥çš„äººç±»æ„å›¾å¯¹é½ã€‚å› æ­¤ï¼Œå¯èƒ½äº§ç”Ÿä¸å¯é¢„æµ‹çš„æœ‰å®³å†…å®¹ä»¥åŠä¸ç¬¦åˆäººç±»åå¥½å’Œä»·å€¼è§‚çš„å†…å®¹ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è‡ªæˆ‘è®¤çŸ¥èƒ½åŠ›å­˜åœ¨é—®é¢˜ï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›æœ‰å¾…å¢å¼ºã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;è¯·è¯¸å›åœ¨ä½¿ç”¨å‰äº†è§£ä¸Šè¿°é—®é¢˜ï¼Œä»¥å…é€ æˆè¯¯è§£å’Œä¸å¿…è¦çš„éº»çƒ¦ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;åä½œè€…&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚ä¸‹å„ä½åˆä½œå¼€å±•ï¼ˆæŒ‰å­—æ¯åºæ’åˆ—ï¼‰ï¼š&lt;a href=&#34;https://github.com/herobrine19&#34;&gt;@cainiao&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/njuyxw&#34;&gt;@njuyxw&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/pengxiao-song&#34;&gt;@pengxiao-song&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å…è´£å£°æ˜&lt;/h2&gt; &#xA;&lt;p&gt;è¯·å„ä½ä¸¥æ ¼éµå®ˆå¦‚ä¸‹çº¦å®šï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®ä»»ä½•èµ„æº&lt;strong&gt;ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨ï¼Œä¸¥ç¦ä»»ä½•å•†ä¸šç”¨é€”&lt;/strong&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æ¨¡å‹è¾“å‡ºå—å¤šç§ä¸ç¡®å®šæ€§å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®å½“å‰æ— æ³•ä¿è¯å…¶å‡†ç¡®æ€§ï¼Œ&lt;strong&gt;ä¸¥ç¦ç”¨äºçœŸå®æ³•å¾‹åœºæ™¯&lt;/strong&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;æœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;é—®é¢˜åé¦ˆ&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨ GitHub Issue ä¸­æäº¤ã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æäº¤é—®é¢˜ä¹‹å‰ï¼Œå»ºè®®æŸ¥é˜… FAQ åŠä»¥å¾€çš„ issue çœ‹æ˜¯å¦èƒ½è§£å†³æ‚¨çš„é—®é¢˜ã€‚&lt;/li&gt; &#xA; &lt;li&gt;è¯·ç¤¼è²Œè®¨è®ºï¼Œæ„å»ºå’Œè°ç¤¾åŒºã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;åä½œè€…ç§‘ç ”ä¹‹ä½™æ¨è¿›é¡¹ç›®è¿›å±•ï¼Œç”±äºäººåŠ›æœ‰é™éš¾ä»¥å®æ—¶åé¦ˆï¼Œç»™è¯¸å›å¸¦æ¥ä¸ä¾¿ï¼Œæ•¬è¯·è°…è§£ï¼&lt;/p&gt; &#xA;&lt;h2&gt;è‡´è°¢&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®åŸºäºå¦‚ä¸‹å¼€æºé¡¹ç›®å±•å¼€ï¼Œåœ¨æ­¤å¯¹ç›¸å…³é¡¹ç›®å’Œå¼€å‘äººå‘˜è¡¨ç¤ºè¯šæŒšçš„æ„Ÿè°¢ï¼š&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chinese-LLaMA-Alpaca: &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;https://github.com/ymcui/Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLaMA: &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;https://github.com/facebookresearch/llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Alpaca: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;alpaca-lora: &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ChatGLM-6B: &lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;https://github.com/THUDM/ChatGLM-6B&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;æ­¤å¤–ï¼Œæœ¬é¡¹ç›®åŸºäºå¼€æ”¾æ•°æ®èµ„æºï¼Œè¯¦è§ &lt;a href=&#34;https://github.com/pengxiao-song/awesome-chinese-legal-resources&#34;&gt;Awesome Chinese Legal Resources&lt;/a&gt;ï¼Œä¸€å¹¶è¡¨ç¤ºæ„Ÿè°¢ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;å¼•ç”¨&lt;/h2&gt; &#xA;&lt;p&gt;å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨è¯¥é¡¹ç›®&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>haotian-liu/LLaVA</title>
    <updated>2023-05-20T01:42:38Z</updated>
    <id>tag:github.com,2023-05-20:/haotian-liu/LLaVA</id>
    <link href="https://github.com/haotian-liu/LLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸŒ‹ LLaVA: Large Language and Vision Assistant&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0&#34;&gt;Model&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://hliu.cc&#34;&gt;Haotian Liu*&lt;/a&gt;, &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&amp;amp;hl=en/&#34;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&#34;https://pages.cs.wisc.edu/~yongjaelee/&#34;&gt;Yong Jae Lee&lt;/a&gt; (*Equal Contribution)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png&#34; width=&#34;50%&#34;&gt; &lt;br&gt; Generated by &lt;a href=&#34;https://gligen.github.io/&#34;&gt;GLIGEN&lt;/a&gt; via &#34;a cute lava llama with glasses&#34; and box prompt &lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[5/6] ğŸ”¥ We are releasing &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#LLaVA-MPT-7b&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[5/2] ğŸ”¥ We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train-llava-lightning&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;[5/2] We upgrade LLaVA package to v0.1 to support Vicuna v0 and v1 checkpoints, please upgrade following instructions &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[4/30] Our checkpoint with Vicuna-7b-v0 has been released &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-7b&#34;&gt;here&lt;/a&gt;! This checkpoint is more accessible and device friendly. Stay tuned for a major upgrade next week!&lt;/li&gt; &#xA; &lt;li&gt;[4/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[4/17] ğŸ”¥ We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#data-download&#34;&gt;Data Download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights&#34;&gt;LLaVA Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#serving&#34;&gt;Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Download&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data file name&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/llava_instruct_150k.json&#34;&gt;llava_instruct_150k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;229 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/llava_instruct_80k.json&#34;&gt;llava_instruct_80k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;229 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/conversation_58k.json&#34;&gt;conversation_58k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;126 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/detail_23k.json&#34;&gt;detail_23k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;20.5 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/complex_reasoning_77k.json&#34;&gt;complex_reasoning_77k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.6 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To download our langauge-image multimodal instruction-folllowing dataset &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;&lt;code&gt;LLaVA-Instruct-150K&lt;/code&gt;&lt;/a&gt;, please run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh download_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretraining Dataset&lt;/h3&gt; &#xA;&lt;p&gt;The pretraining dataset used in this release is a subset of CC-3M dataset, filtered with a more balanced concept coverage distribution. Please see &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K&#34;&gt;here&lt;/a&gt; for a detailed description on the dataset structure and how to download the images.&lt;/p&gt; &#xA;&lt;p&gt;If you already have CC-3M dataset on your disk, the image names follow this format: &lt;code&gt;GCC_train_000000000.jpg&lt;/code&gt;. You may edit the &lt;code&gt;image&lt;/code&gt; field correspondingly if necessary.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;Chat File&lt;/th&gt; &#xA;   &lt;th&gt;Meta Data&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CC-3M Concept-balanced 595K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/raw/main/chat.json&#34;&gt;chat.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/raw/main/metadata.json&#34;&gt;metadata.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;211 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LAION/CC/SBU BLIP-Caption Concept-balanced 558K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/raw/main/blip_laion_cc_sbu_558k.json&#34;&gt;blip_laion_cc_sbu_558k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#&#34;&gt;metadata.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;181 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important notice&lt;/strong&gt;: Upon the request from the community, as ~15% images of the original CC-3M dataset are no longer accessible, we upload &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/blob/main/images.zip&#34;&gt;&lt;code&gt;images.zip&lt;/code&gt;&lt;/a&gt; for better reproducing our work in research community. It must not be used for any other purposes. The use of these images must comply with the CC-3M license. This may be taken down at any time when requested by the original CC-3M dataset owner or owners of the referenced images.&lt;/p&gt; &#xA;&lt;h3&gt;GPT-4 Prompts&lt;/h3&gt; &#xA;&lt;p&gt;We provide our prompts and few-shot samples for GPT-4 queries, to better facilitate research in this domain. Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/playground/data/prompts&#34;&gt;&lt;code&gt;prompts&lt;/code&gt;&lt;/a&gt; folder for three kinds of questions: conversation, detail description, and complex reasoning.&lt;/p&gt; &#xA;&lt;p&gt;They are organized in a format of &lt;code&gt;system_message.txt&lt;/code&gt; for system message, pairs of &lt;code&gt;abc_caps.txt&lt;/code&gt; for few-shot sample user input, and &lt;code&gt;abc_conv.txt&lt;/code&gt; for few-shot sample reference output.&lt;/p&gt; &#xA;&lt;p&gt;Note that you may find them in different format. For example, &lt;code&gt;conversation&lt;/code&gt; is in &lt;code&gt;jsonl&lt;/code&gt;, and detail description is answer-only. The selected format in our preliminary experiments work slightly better than a limited set of alternatives that we tried: &lt;code&gt;jsonl&lt;/code&gt;, more natural format, answer-only. If interested, you may try other variants or conduct more careful study in this. Contributions are welcomed!&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to LLaVA folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/haotian-liu/LLaVA.git&#xA;cd LLaVA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: [Update 4/30/23] We have successfully moved LLaVA framework to this repo, without the need of a special &lt;code&gt;transformers&lt;/code&gt; modified by us. If you install our repo before &lt;code&gt;4/30/23&lt;/code&gt;, please reinstall &lt;code&gt;transformers&lt;/code&gt; following the instructions &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#upgrade-to-v01&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install ninja&#xA;pip install flash-attn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upgrade to v0.1&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you install our package before 4/30/23, please make sure to execute the command below to correctly upgrade to v0.1. You may try a &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;clean install&lt;/a&gt; as well.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;git pull&#xA;pip uninstall transformers&#xA;pip install git+https://github.com/huggingface/transformers@cae78c46&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LLaVA Weights&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt; weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the LLaVA weights.&lt;/p&gt; &#xA;&lt;p&gt;Instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the original LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get LLaVA weights by applying our delta (&lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0&#34;&gt;13b-v0&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0&#34;&gt;7b-v0&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1&#34;&gt;lightning-7B-v1-1&lt;/a&gt;). It will automatically download delta weights from our Hugging Face account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;LLaVA-13B&lt;/h3&gt; &#xA;&lt;p&gt;This conversion command needs around 60 GB of CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m llava.model.apply_delta \&#xA;    --base /path/to/llama-13b \&#xA;    --target /output/path/to/LLaVA-13B-v0 \&#xA;    --delta liuhaotian/LLaVA-13b-delta-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaVA-7B&lt;/h3&gt; &#xA;&lt;p&gt;This conversion command needs around 30 GB of CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m llava.model.apply_delta \&#xA;    --base /path/to/llama-7b \&#xA;    --target /output/path/to/LLaVA-7B-v0 \&#xA;    --delta liuhaotian/LLaVA-7b-delta-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaVA pretrained projector weights&lt;/h3&gt; &#xA;&lt;p&gt;The initial release is pretrained on &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K&#34;&gt;LLaVA-filtered CC3M 595K&lt;/a&gt; with 1 epoch. The pretrained weights are released &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may perform instruction tuning on our pretrained checkpoints, by using our &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;visual instruction tuning&lt;/a&gt; data following the instructions &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#fine-tuning-with-local-gpus&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Serving&lt;/h2&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If your the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal --num-gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.gradio_web_server --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;You can open your browser and chat with a model now.&lt;/h4&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;p&gt;A starting script for inference with LLaVA without the need of Gradio interface. The current implementation only supports for a single-turn Q-A session, and the interactive CLI is WIP. This also serves as an example for users to build customized inference scripts.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.eval.run_llava \&#xA;    --model-name /path/to/LLaVA-13B-v0 \&#xA;    --image-file &#34;https://llava-vl.github.io/static/images/view.jpg&#34; \&#xA;    --query &#34;What are the things I should be cautious about when I visit here?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example output (varies in different runs):&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;When visiting this picturesque location with a serene lake and a wooden pier extending over the water, one should be cautious about various safety aspects. Some important considerations include:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Ensuring that the pier is structurally sound andstable, as old or weakened pier structures might not support the weight of visitors.&lt;/li&gt; &#xA;  &lt;li&gt;Being aware of the water depth around the pier and lake, as sudden drop-offs or strong currents may pose a risk to swimmers, boaters, or those who venture too close to the edge.&lt;/li&gt; &#xA;  &lt;li&gt;Staying vigilant about the presence of wildlife in the area, such as slippery, stealthy fish or other animals that might cause harm or inconvenience.&lt;/li&gt; &#xA;  &lt;li&gt;Maintaining a safe distance from the water&#39;s edge, particularly for children, elderly individuals, or those who are not strong swimmers.&lt;/li&gt; &#xA;  &lt;li&gt;Following any posted signs or guidelines related to safety and the use of the pier and surrounding areas.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;By considering these safety precautions, visitors can enjoy the natural beauty of the location while minimizing risks and ensuring a safe and pleasant experience.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;GPT-assisted Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models. Please see our paper for more details.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate LLaVA responses&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python model_vqa.py \&#xA;    --model-name ./checkpoints/LLaVA-13B-v0 \&#xA;    --question-file \&#xA;    playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --image-folder \&#xA;    /path/to/coco2014_val \&#xA;    --answers-file \&#xA;    /path/to/answer-file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Evaluate the generated responses. In our case, &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/playground/data/coco2014_val_qa_eval/qa90_gpt4_answer.jsonl&#34;&gt;&lt;code&gt;answer-file-1.jsonl&lt;/code&gt;&lt;/a&gt; is the response generated by text-only GPT-4 (0314), with the context captions/boxes provided.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;OPENAI_API_KEY=&#34;sk-***********************************&#34; python eval_gpt_review_visual.py \&#xA;    --question playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --context table/caps_boxes_coco2014_val_80.jsonl \&#xA;    --answer-list \&#xA;    /path/to/answer-file-1.jsonl \&#xA;    /path/to/answer-file-2.jsonl \&#xA;    --rule table/rule.json \&#xA;    --output /path/to/review.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Summarize the evaluation results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python summarize_gpt_review.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ScienceQA&lt;/h3&gt; &#xA;&lt;h4&gt;Prepare Data&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Please see ScienceQA &lt;a href=&#34;https://github.com/lupantech/ScienceQA&#34;&gt;repo&lt;/a&gt; for setting up the dataset.&lt;/li&gt; &#xA; &lt;li&gt;Generate ScienceQA dataset for LLaVA conversation-style format.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python scripts/convert_sqa_to_llava \&#xA;    convert_to_llava \&#xA;    --base-dir /path/to/ScienceQA/data/scienceqa \&#xA;    --split {train,val,minival,test,minitest}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Evaluation&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download our pretrained LLaVA-13B (delta) weights for ScienceQA dataset &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0-science_qa&#34;&gt;here&lt;/a&gt;. Convert the delta weights to actual weights.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.model.apply_delta \&#xA;    --base /path/to/llama-13b \&#xA;    --target /path/to/LLaVA-13b-v0-science_qa \&#xA;    --delta liuhaotian/LLaVA-13b-delta-v0-science_qa&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;[Option 1] Multiple-GPU inference You may evaluate this with multiple GPUs, and concatenate the generated jsonl files. Please refer to our script for &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/scripts/sqa_eval_batch.sh&#34;&gt;batch evaluation&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/scripts/sqa_eval_gather.sh&#34;&gt;results gathering&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[Option 2] Single-GPU inference&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;(a) Generate LLaVA responses on ScienceQA dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.eval.model_vqa_science \&#xA;    --model-name /path/to/LLaVA-13b-v0-science_qa \&#xA;    --question-file /path/to/ScienceQA/data/scienceqa/llava_test.json \&#xA;    --image-folder /path/to/ScienceQA/data/scienceqa/images/test \&#xA;    --answers-file vqa/results/ScienceQA/test_llava-13b.jsonl \&#xA;    --answer-prompter&#xA;    --conv-mode simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(b) Evaluate the generated responses&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python eval_science_qa.py \&#xA;    --base-dir /path/to/ScienceQA/data/scienceqa \&#xA;    --result-file vqa/results/ScienceQA/test_llava-13b.jsonl \&#xA;    --output-file vqa/results/ScienceQA/test_llava-13b_output.json \&#xA;    --output-result vqa/results/ScienceQA/test_llava-13b_result.json \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For reference, we attach our prediction file &lt;code&gt;test_llava-13b_result.json&lt;/code&gt; &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/llava/eval/table/results/test_sqa_llava_13b_v0.json&#34;&gt;here&lt;/a&gt; for comparison when reproducing our results, as well as for further analysis in detail.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;The current version of LLaVA is fine-tuned from a Vicuna-13B model. We use approximately 600K filtered CC3M in feature alignment pretraining and 150K GPT-generated multimodal instruction-following data in finetuning. For detailed description of the data generation pipeline, please refer see our &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are working on a more capable model that is pretrained with the data at a larger scale. Stay tuned!&lt;/p&gt; &#xA;&lt;p&gt;We release all three types of multimodal instruction-following data. The use of these data is subject to OpenAI &lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;TOS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Code and Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tune the model using the code from &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. We use a similar set of hyperparameters as Vicuna in finetuning. Both hyperparameters used in pretraining and finetuning are provided below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fine-tuning with Local GPUs&lt;/h3&gt; &#xA;&lt;p&gt;LLaVA is trained on 8 A100 GPUs with 80GB memory with the following code. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly to keep the global batch size the same.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-13B, 8x A100 (80G). Time: ~4 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You may run this with a single A100 GPU with the following code. Please note that the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; * &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; should be equal to 128 to keep the global batch size the same.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-13B, 1x A100 (80G). Time: ~33 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-7B, 1x A100 (80G/40G). Time: ~19 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-7b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-7b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Experimental: use FSDP to save memory in pretraining&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Learn more&lt;/summary&gt; &#xA; &lt;p&gt;Currently, PyTorch and Huggingface does not yet have stable/native support for FSDP on parameter efficient tuning (part of the parameters are frozen). However, the feature is being developed in PyTorch nightly and shall be shipped in the next release. We provide an experimental script to enable FSDP in pretraining. To use it, please &lt;strong&gt;create a new enviroment&lt;/strong&gt; (to be safe), install PyTorch nightly (&lt;strong&gt;MUST&lt;/strong&gt;), and &lt;code&gt;LLaVA&lt;/code&gt; package following the instructions below.&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Prepare environment&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava_beta python=3.10 -y&#xA;conda activate llava_beta&#xA;pip install --upgrade pip&#xA;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu117&#xA;pip install -e .&#xA;pip install einops ninja&#xA;pip install flash-attn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Run pretraining with FSDP (experimental)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain_fsdp \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Extract projector features&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python scripts/extract_mm_projector.py \&#xA;  --model_name_or_path ./checkpoints/llava-13b-pretrain \&#xA;  --output ./checkpoints/mm_projector/llava-13b-pretrain.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path /path/to/llama-vicuna-13b \&#xA;    --data_path /path/to/llava_instruct_150k.json \&#xA;    --image_folder /Data/haotian/coco/train2014 \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain.bin \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end True \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 5000 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Train LLaVA Lightning&lt;/h3&gt; &#xA;&lt;p&gt;LLaVA-Lightning can be trained on 8x A100 GPUs in just 3 hours, including both pretraining and finetuning. When using spot instances, it costs just ~$40. &lt;em&gt;We are working on &lt;a href=&#34;https://github.com/skypilot-org/skypilot.git&#34;&gt;SkyPilot&lt;/a&gt; tutorial to make spot instance training even easier, stay tuned!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please make sure to: (1) &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;install&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#upgrade-to-v01&#34;&gt;upgrade&lt;/a&gt; to the latest code base, and (2) pass the correct model version identifier &lt;code&gt;v0&lt;/code&gt;/&lt;code&gt;v1&lt;/code&gt; to ensure the correct conversation template is loaded.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;bash ./scripts/train_lightning.sh {v0,v1}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Hyperparameters&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Lightning-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Lightning-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;LLaVA-MPT-7b&lt;/h4&gt; &#xA;&lt;p&gt;Thanks to LLaVA-Lightning, we are able to train a checkpoint based on MPT-7b-Chat on 8x A100 GPUs in just 3 hours, including both pretraining and finetuning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This is a research preview of the LLaVA-Lightning based on MPT-7B-chat checkpoint. The usage of the model should comply with MPT-7B-chat license and agreements.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Unlike other LLaVA models, this model should be used directly without delta weights conversion!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: You need to upgrade to our latest code base to use LLaVA-MPT-7b!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Usage&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You do not need to download our checkpoint, it will directly load from our Hugging Face model: &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview&#34;&gt;&lt;code&gt;liuhaotian/LLaVA-Lightning-MPT-7B-preview&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/LLaVA-Lightning-MPT-7B-preview&#xA;python -m llava.serve.gradio_web_server --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Training&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We use the same set of training dataset, and the hyperparameters as other Lightning checkpoints.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;bash ./scripts/train_lightning_mpt.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning on ScienceQA&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Due to that ScienceQA experiments were done earlier, the current checkpoints are trained &lt;em&gt;without&lt;/em&gt; &lt;code&gt;&amp;lt;im_start&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;im_end&amp;gt;&lt;/code&gt; tokens. Checkpoints with these tokens will be updated later. Here we provide our training scripts for the current checkpoints.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;1. Pretraining&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain-no_im_start_end_token \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;2. Extract projector features&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python scripts/extract_mm_projector.py \&#xA;  --model_name_or_path ./checkpoints/llava-13b-pretrain-no_im_start_end_token \&#xA;  --output ./checkpoints/mm_projector/llava-13b-pretrain-no_im_start_end_token.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;3. Finetuning&lt;/summary&gt; &#xA; &lt;p&gt;You may download our pretrained &lt;code&gt;llava-13b-pretrain-no_im_start_end_token.bin&lt;/code&gt; &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0/blob/main/LLaVA-13b-pretrain-projector-v0-CC3M-595K-original_caption-no_im_token.bin&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path /path/to/llama-vicuna-13b \&#xA;    --data_path /path/to/scienceqa/llava_train_QCM-LEPA.json \&#xA;    --image_folder /path/to/scienceqa/images/train \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain-no_im_start_end_token.bin \&#xA;    --mm_vision_select_layer -2 \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain-no_im_start_end_token-finetune_scienceqa \&#xA;    --num_train_epochs 12 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 5000 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you find LLaVA useful for your your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={arXiv:2304.08485},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For future project ideas, pleae check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>JaidedAI/EasyOCR</title>
    <updated>2023-05-20T01:42:38Z</updated>
    <id>tag:github.com,2023-05-20:/JaidedAI/EasyOCR</id>
    <link href="https://github.com/JaidedAI/EasyOCR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EasyOCR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/easyocr&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/easyocr.svg?sanitize=true&#34; alt=&#34;PyPI Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.to/easyocr&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/JaidedAI&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ready-to-use OCR with 80+ &lt;a href=&#34;https://www.jaided.ai/easyocr&#34;&gt;supported languages&lt;/a&gt; and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jaided.ai/easyocr&#34;&gt;Try Demo on our website&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Integrated into &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces ğŸ¤—&lt;/a&gt; using &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. Try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/tomofi/EasyOCR&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s new&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;15 September 2022 - Version 1.6.2&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add CPU support for DBnet&lt;/li&gt; &#xA;   &lt;li&gt;DBnet will only be compiled when users initialize DBnet detector.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;1 September 2022 - Version 1.6.1&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fix DBnet path bug for Windows&lt;/li&gt; &#xA;   &lt;li&gt;Add new built-in model &lt;code&gt;cyrillic_g2&lt;/code&gt;. This model is a new default for Cyrillic script.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;24 August 2022 - Version 1.6.0&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Restructure code to support alternative text detectors.&lt;/li&gt; &#xA;   &lt;li&gt;Add detector &lt;code&gt;DBnet&lt;/code&gt;, see &lt;a href=&#34;https://arxiv.org/abs/2202.10304v1&#34;&gt;paper&lt;/a&gt;. It can be used by initializing like this &lt;code&gt;reader = easyocr.Reader([&#39;en&#39;], detect_network = &#39;dbnet18&#39;)&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;2 June 2022 - Version 1.5.0&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add trainer for CRAFT detection model (thanks&lt;a href=&#34;https://github.com/gmuffiness&#34;&gt;@gmuffiness&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/739&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;9 April 2022 - Version 1.4.2&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Update dependencies (opencv and pillow issues)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;11 September 2021 - Version 1.4.1&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add trainer folder&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;readtextlang&lt;/code&gt; method (thanks&lt;a href=&#34;https://github.com/arkya-art&#34;&gt;@arkya-art&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/525&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Extend &lt;code&gt;rotation_info&lt;/code&gt; argument to support all possible angles (thanks&lt;a href=&#34;https://github.com/abde0103&#34;&gt;abde0103&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/515&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;29 June 2021 - Version 1.4&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/custom_model.md&#34;&gt;Instructions&lt;/a&gt; on training/using custom recognition models&lt;/li&gt; &#xA;   &lt;li&gt;Example &lt;a href=&#34;https://www.jaided.ai/easyocr/modelhub&#34;&gt;dataset&lt;/a&gt; for model training&lt;/li&gt; &#xA;   &lt;li&gt;Batched image inference for GPUs (thanks &lt;a href=&#34;https://github.com/SamSamhuns&#34;&gt;@SamSamhuns&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/458&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Vertical text support (thanks &lt;a href=&#34;https://github.com/interactivetech&#34;&gt;@interactivetech&lt;/a&gt;). This is for rotated text, not to be confused with vertical Chinese or Japanese text. (see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/450&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Output in dictionary format (thanks &lt;a href=&#34;https://github.com/A2va&#34;&gt;@A2va&lt;/a&gt;, see &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/pull/441&#34;&gt;PR&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;30 May 2021 - Version 1.3.2&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Faster greedy decoder (thanks &lt;a href=&#34;https://github.com/samayala22&#34;&gt;@samayala22&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Fix bug when a text box&#39;s aspect ratio is disproportional (thanks &lt;a href=&#34;https://iquartic.com/&#34;&gt;iQuartic&lt;/a&gt; for bug report)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;20 April 2021 - Version 1.3.1&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add support for PIL image (thanks &lt;a href=&#34;https://github.com/prays&#34;&gt;@prays&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Add Tajik language (tjk)&lt;/li&gt; &#xA;   &lt;li&gt;Update argument setting for command line&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;x_ths&lt;/code&gt; and &lt;code&gt;y_ths&lt;/code&gt; to control merging behavior when &lt;code&gt;paragraph=True&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;21 March 2021 - Version 1.3&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Second-generation models: multiple times smaller size, multiple times faster inference, additional characters and comparable accuracy to the first generation models. EasyOCR will choose the latest model by default but you can also specify which model to use by passing &lt;code&gt;recog_network&lt;/code&gt; argument when creating a &lt;code&gt;Reader&lt;/code&gt; instance. For example, &lt;code&gt;reader = easyocr.Reader([&#39;en&#39;,&#39;fr&#39;], recog_network=&#39;latin_g1&#39;)&lt;/code&gt; will use the 1st generation Latin model&lt;/li&gt; &#xA;   &lt;li&gt;List of all models: &lt;a href=&#34;https://www.jaided.ai/easyocr/modelhub&#34;&gt;Model hub&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/releasenotes.md&#34;&gt;Read all release notes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What&#39;s coming next&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Handwritten text support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example.png&#34; alt=&#34;example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example2.png&#34; alt=&#34;example2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/example3.png&#34; alt=&#34;example3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For the latest stable release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install easyocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the latest development release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/JaidedAI/EasyOCR.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note 1: For Windows, please install torch and torchvision first by following the official instructions here &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt;. On the pytorch website, be sure to select the right CUDA version you have. If you intend to run on CPU mode only, select &lt;code&gt;CUDA = None&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note 2: We also provide a Dockerfile &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/Dockerfile&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import easyocr&#xA;reader = easyocr.Reader([&#39;ch_sim&#39;,&#39;en&#39;]) # this needs to run only once to load the model into memory&#xA;result = reader.readtext(&#39;chinese.jpg&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[([[189, 75], [469, 75], [469, 165], [189, 165]], &#39;æ„šå›­è·¯&#39;, 0.3754989504814148),&#xA; ([[86, 80], [134, 80], [134, 128], [86, 128]], &#39;è¥¿&#39;, 0.40452659130096436),&#xA; ([[517, 81], [565, 81], [565, 123], [517, 123]], &#39;ä¸œ&#39;, 0.9989598989486694),&#xA; ([[78, 126], [136, 126], [136, 156], [78, 156]], &#39;315&#39;, 0.8125889301300049),&#xA; ([[514, 126], [574, 126], [574, 156], [514, 156]], &#39;309&#39;, 0.4971577227115631),&#xA; ([[226, 170], [414, 170], [414, 220], [226, 220]], &#39;Yuyuan Rd.&#39;, 0.8261902332305908),&#xA; ([[79, 173], [125, 173], [125, 213], [79, 213]], &#39;W&#39;, 0.9848111271858215),&#xA; ([[529, 173], [569, 173], [569, 213], [529, 213]], &#39;E&#39;, 0.8405593633651733)]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note 1: &lt;code&gt;[&#39;ch_sim&#39;,&#39;en&#39;]&lt;/code&gt; is the list of languages you want to read. You can pass several languages at once but not all languages can be used together. English is compatible with every language and languages that share common characters are usually compatible with each other.&lt;/p&gt; &#xA;&lt;p&gt;Note 2: Instead of the filepath &lt;code&gt;chinese.jpg&lt;/code&gt;, you can also pass an OpenCV image object (numpy array) or an image file as bytes. A URL to a raw image is also acceptable.&lt;/p&gt; &#xA;&lt;p&gt;Note 3: The line &lt;code&gt;reader = easyocr.Reader([&#39;ch_sim&#39;,&#39;en&#39;])&lt;/code&gt; is for loading a model into memory. It takes some time but it needs to be run only once.&lt;/p&gt; &#xA;&lt;p&gt;You can also set &lt;code&gt;detail=0&lt;/code&gt; for simpler output.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reader.readtext(&#39;chinese.jpg&#39;, detail = 0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Result:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[&#39;æ„šå›­è·¯&#39;, &#39;è¥¿&#39;, &#39;ä¸œ&#39;, &#39;315&#39;, &#39;309&#39;, &#39;Yuyuan Rd.&#39;, &#39;W&#39;, &#39;E&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weights for the chosen language will be automatically downloaded or you can download them manually from the &lt;a href=&#34;https://www.jaided.ai/easyocr/modelhub&#34;&gt;model hub&lt;/a&gt; and put them in the &#39;~/.EasyOCR/model&#39; folder&lt;/p&gt; &#xA;&lt;p&gt;In case you do not have a GPU, or your GPU has low memory, you can run the model in CPU-only mode by adding &lt;code&gt;gpu=False&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reader = easyocr.Reader([&#39;ch_sim&#39;,&#39;en&#39;], gpu=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, read the &lt;a href=&#34;https://www.jaided.ai/easyocr/tutorial&#34;&gt;tutorial&lt;/a&gt; and &lt;a href=&#34;https://www.jaided.ai/easyocr/documentation&#34;&gt;API Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Run on command line&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ easyocr -l ch_sim en -f chinese.jpg --detail=1 --gpu=True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train/use your own model&lt;/h2&gt; &#xA;&lt;p&gt;For recognition model, &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/custom_model.md&#34;&gt;Read here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detection model (CRAFT), &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/raw/master/trainer/craft/README.md&#34;&gt;Read here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Implementation Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Handwritten support&lt;/li&gt; &#xA; &lt;li&gt;Restructure code to support swappable detection and recognition algorithms The api should be as easy as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reader = easyocr.Reader([&#39;en&#39;], detection=&#39;DB&#39;, recognition = &#39;Transformer&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The idea is to be able to plug in any state-of-the-art model into EasyOCR. There are a lot of geniuses trying to make better detection/recognition models, but we are not trying to be geniuses here. We just want to make their works quickly accessible to the public ... for free. (well, we believe most geniuses want their work to create a positive impact as fast/big as possible) The pipeline should be something like the below diagram. Grey slots are placeholders for changeable light blue modules.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JaidedAI/EasyOCR/master/examples/easyocr_framework.jpeg&#34; alt=&#34;plan&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement and References&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on research and code from several papers and open-source repositories.&lt;/p&gt; &#xA;&lt;p&gt;All deep learning execution is based on &lt;a href=&#34;https://pytorch.org&#34;&gt;Pytorch&lt;/a&gt;. &lt;span&gt;â¤ï¸&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;Detection execution uses the CRAFT algorithm from this &lt;a href=&#34;https://github.com/clovaai/CRAFT-pytorch&#34;&gt;official repository&lt;/a&gt; and their &lt;a href=&#34;https://arxiv.org/abs/1904.01941&#34;&gt;paper&lt;/a&gt; (Thanks @YoungminBaek from &lt;a href=&#34;https://github.com/clovaai&#34;&gt;@clovaai&lt;/a&gt;). We also use their pretrained model. Training script is provided by &lt;a href=&#34;https://github.com/gmuffiness&#34;&gt;@gmuffiness&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The recognition model is a CRNN (&lt;a href=&#34;https://arxiv.org/abs/1507.05717&#34;&gt;paper&lt;/a&gt;). It is composed of 3 main components: feature extraction (we are currently using &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;Resnet&lt;/a&gt;) and VGG, sequence labeling (&lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;LSTM&lt;/a&gt;) and decoding (&lt;a href=&#34;https://www.cs.toronto.edu/~graves/icml_2006.pdf&#34;&gt;CTC&lt;/a&gt;). The training pipeline for recognition execution is a modified version of the &lt;a href=&#34;https://github.com/clovaai/deep-text-recognition-benchmark&#34;&gt;deep-text-recognition-benchmark&lt;/a&gt; framework. (Thanks &lt;a href=&#34;https://github.com/ku21fan&#34;&gt;@ku21fan&lt;/a&gt; from &lt;a href=&#34;https://github.com/clovaai&#34;&gt;@clovaai&lt;/a&gt;) This repository is a gem that deserves more recognition.&lt;/p&gt; &#xA;&lt;p&gt;Beam search code is based on this &lt;a href=&#34;https://github.com/githubharald/CTCDecoder&#34;&gt;repository&lt;/a&gt; and his &lt;a href=&#34;https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7&#34;&gt;blog&lt;/a&gt;. (Thanks &lt;a href=&#34;https://github.com/githubharald&#34;&gt;@githubharald&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Data synthesis is based on &lt;a href=&#34;https://github.com/Belval/TextRecognitionDataGenerator&#34;&gt;TextRecognitionDataGenerator&lt;/a&gt;. (Thanks &lt;a href=&#34;https://github.com/Belval&#34;&gt;@Belval&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;And a good read about CTC from distill.pub &lt;a href=&#34;https://distill.pub/2017/ctc/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Want To Contribute?&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s advance humanity together by making AI available to everyone!&lt;/p&gt; &#xA;&lt;p&gt;3 ways to contribute:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coder:&lt;/strong&gt; Please send a PR for small bugs/improvements. For bigger ones, discuss with us by opening an issue first. There is a list of possible bug/improvement issues tagged with &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/issues?q=is%3Aissue+is%3Aopen+label%3A%22PR+WELCOME%22&#34;&gt;&#39;PR WELCOME&#39;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; Tell us how EasyOCR benefits you/your organization to encourage further development. Also post failure cases in &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/issues&#34;&gt;Issue Section&lt;/a&gt; to help improve future models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tech leader/Guru:&lt;/strong&gt; If you found this library useful, please spread the word! (See &lt;a href=&#34;https://www.facebook.com/yann.lecun/posts/10157018122787143&#34;&gt;Yann Lecun&#39;s post&lt;/a&gt; about EasyOCR)&lt;/p&gt; &#xA;&lt;h2&gt;Guideline for new language request&lt;/h2&gt; &#xA;&lt;p&gt;To request a new language, we need you to send a PR with the 2 following files:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In folder &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/character&#34;&gt;easyocr/character&lt;/a&gt;, we need &#39;yourlanguagecode_char.txt&#39; that contains list of all characters. Please see format examples from other files in that folder.&lt;/li&gt; &#xA; &lt;li&gt;In folder &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/dict&#34;&gt;easyocr/dict&lt;/a&gt;, we need &#39;yourlanguagecode.txt&#39; that contains list of words in your language. On average, we have ~30000 words per language with more than 50000 words for more popular ones. More is better in this file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If your language has unique elements (such as 1. Arabic: characters change form when attached to each other + write from right to left 2. Thai: Some characters need to be above the line and some below), please educate us to the best of your ability and/or give useful links. It is important to take care of the detail to achieve a system that really works.&lt;/p&gt; &#xA;&lt;p&gt;Lastly, please understand that our priority will have to go to popular languages or sets of languages that share large portions of their characters with each other (also tell us if this is the case for your language). It takes us at least a week to develop a new model, so you may have to wait a while for the new model to be released.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/JaidedAI/EasyOCR/issues/91&#34;&gt;List of languages in development&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Github Issues&lt;/h2&gt; &#xA;&lt;p&gt;Due to limited resources, an issue older than 6 months will be automatically closed. Please open an issue again if it is critical.&lt;/p&gt; &#xA;&lt;h2&gt;Business Inquiries&lt;/h2&gt; &#xA;&lt;p&gt;For Enterprise Support, &lt;a href=&#34;https://www.jaided.ai/&#34;&gt;Jaided AI&lt;/a&gt; offers full service for custom OCR/AI systems from implementation, training/finetuning and deployment. Click &lt;a href=&#34;https://www.jaided.ai/contactus?ref=github&#34;&gt;here&lt;/a&gt; to contact us.&lt;/p&gt;</summary>
  </entry>
</feed>