<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-09T01:44:47Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>01-ai/Yi</title>
    <updated>2023-11-09T01:44:47Z</updated>
    <id>tag:github.com,2023-11-09:/01-ai/Yi</id>
    <link href="https://github.com/01-ai/Yi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of large language models trained from scratch by developers @01-ai&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/01-ai/Yi/raw/main/assets/img/Yi.svg?sanitize=true&#34; width=&#34;200px&#34;&gt; &lt;/p&gt; &#xA; &lt;a href=&#34;https://github.com/01-ai/Yi/actions/workflows/ci.yml&#34;&gt; &lt;img src=&#34;https://github.com/01-ai/Yi/actions/workflows/ci.yml/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/01-ai&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-01--ai-blue&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://www.modelscope.cn/organization/01ai/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/ModelScope-01--ai-blue&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/01-ai/Yi/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Code_License-Apache_2.0-lightblue&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/01-ai/Yi/raw/main/MODEL_LICENSE_AGREEMENT.txt&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-lightblue&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;mailto:oss@01.ai&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/‚úâÔ∏è-yi@01.ai-FFE01B&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Yi&lt;/strong&gt; series models are large language models trained from scratch by developers at &lt;a href=&#34;https://01.ai/&#34;&gt;01.AI&lt;/a&gt;. The first public release contains two bilingual (English/Chinese) base models with the parameter sizes of 6B and 34B. Both of them are trained with 4K sequence length and can be extended to 32K during inference time.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üéØ &lt;strong&gt;2023/11/05&lt;/strong&gt;: The base model of &lt;code&gt;Yi-6B-200K&lt;/code&gt; and &lt;code&gt;Yi-34B-200K&lt;/code&gt; with 200K context length.&lt;/li&gt; &#xA; &lt;li&gt;üéØ &lt;strong&gt;2023/11/02&lt;/strong&gt;: The base model of &lt;code&gt;Yi-6B&lt;/code&gt; and &lt;code&gt;Yi-34B&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CMMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GAOKAO&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BBH&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Common-sense Reasoning&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reading Comprehension&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Math &amp;amp; Code&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3-shot@1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-34B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;LLaMA2-70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baichuan2-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Qwen-14B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;39.8&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Skywork-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;InternLM-20B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Aquila-34B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Falcon-180B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yi-6B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yi-6B-200K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;72.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Yi-34B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;83.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;81.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;54.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;80.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Yi-34B-200K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;81.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;83.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;While benchmarking open-source models, we have observed a disparity between the results generated by our pipeline and those reported in public sources (e.g. OpenCompass). Upon conducting a more in-depth investigation of this difference, we have discovered that various models may employ different prompts, post-processing strategies, and sampling techniques, potentially resulting in significant variations in the outcomes. Our prompt and post-processing strategy remains consistent with the original benchmark, and greedy decoding is employed during evaluation without any post-processing for the generated content. For scores that were not reported by the original authors (including scores reported with different settings), we try to get results with our pipeline.&lt;/p&gt; &#xA;&lt;p&gt;To evaluate the model&#39;s capability extensively, we adopted the methodology outlined in Llama2. Specifically, we included PIQA, SIQA, HellaSwag, WinoGrande, ARC, OBQA, and CSQA to assess common sense reasoning. SquAD, QuAC, and BoolQ were incorporated to evaluate reading comprehension. CSQA was exclusively tested using a 7-shot setup, while all other tests were conducted with a 0-shot configuration. Additionally, we introduced GSM8K (8-shot@1), MATH (4-shot@1), HumanEval (0-shot@1), and MBPP (3-shot@1) under the category &#34;Math &amp;amp; Code&#34;. Due to technical constraints, we did not test Falcon-180 on QuAC and OBQA; the score is derived by averaging the scores on the remaining tasks. Since the scores for these two tasks are generally lower than the average, we believe that Falcon-180B&#39;s performance was not underestimated.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Feel free to &lt;a href=&#34;https://github.com/01-ai/Yi/issues/new&#34;&gt;create an issue&lt;/a&gt; if you encounter any problem when using the &lt;strong&gt;Yi&lt;/strong&gt; series models.&lt;/p&gt; &#xA;&lt;h3&gt;1. Prepare development environment&lt;/h3&gt; &#xA;&lt;p&gt;The best approach to try the &lt;strong&gt;Yi&lt;/strong&gt; series models is through Docker with GPUs. We provide the following docker images to help you get started.&lt;/p&gt; &#xA;&lt;!-- - `ghcr.io/01-ai/yi:latest` --&gt; &#xA;&lt;!-- - `registry.lingyiwanwu.com/ci/01-ai/yi:latest` --&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;latest&lt;/code&gt; tag always points to the latest code in the &lt;code&gt;main&lt;/code&gt; branch. To test a stable version, please replace it with a specific &lt;a href=&#34;https://github.com/01-ai/Yi/tags&#34;&gt;tag&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer trying out with your local development environment. First, create a virtual environment and clone this repo. Then install the dependencies with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;. For the best performance, we recommend you also install the latest version (&lt;code&gt;&amp;gt;=2.3.3&lt;/code&gt;) of &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention#installation-and-features&#34;&gt;flash-attention&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;2. Download the model (optional)&lt;/h3&gt; &#xA;&lt;p&gt;By default the model weights and tokenizer will be downloaded from &lt;a href=&#34;https://huggingface.co/01-ai&#34;&gt;HuggingFace&lt;/a&gt; automatically in the next step. You can also download them manually from the following places:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.modelscope.cn/organization/01ai/&#34;&gt;ModelScope&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wisemodel.cn/models&#34;&gt;WiseModel&lt;/a&gt; (Search for &lt;code&gt;Yi&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Mirror site (remember to extract the content with &lt;code&gt;tar&lt;/code&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://storage.lingyiwanwu.com/yi/models/Yi-6B.tar&#34;&gt;Yi-6B.tar&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://storage.lingyiwanwu.com/yi/models/Yi-6B-200K.tar&#34;&gt;Yi-6B-200K.tar&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://storage.lingyiwanwu.com/yi/models/Yi-34B.tar&#34;&gt;Yi-34B.tar&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://storage.lingyiwanwu.com/yi/models/Yi-34B-200K.tar&#34;&gt;Yi-34B-200K.tar&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Examples&lt;/h3&gt; &#xA;&lt;h4&gt;3.1 Use the base model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo/text_generation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To reuse the downloaded models in the previous step, you can provide the extra &lt;code&gt;--model&lt;/code&gt; argument:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo/text_generation.py  --model /path/to/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you&#39;d like to get your hands dirty:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;01-ai/Yi-34B&#34;, device_map=&#34;auto&#34;, torch_dtype=&#34;auto&#34;, trust_remote_code=True)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;01-ai/Yi-34B&#34;, trust_remote_code=True)&#xA;inputs = tokenizer(&#34;There&#39;s a place where time stands still. A place of breath taking wonder, but also&#34;, return_tensors=&#34;pt&#34;)&#xA;max_length = 256&#xA;&#xA;outputs = model.generate(&#xA;    inputs.input_ids.cuda(),&#xA;    max_length=max_length,&#xA;    eos_token_id=tokenizer.eos_token_id&#xA;    do_sample=True,&#xA;    repetition_penalty=1.3,&#xA;    no_repeat_ngram_size=5,&#xA;    temperature=0.7,&#xA;    top_k=40,&#xA;    top_p=0.8,&#xA;)&#xA;print(tokenizer.decode(outputs[0], skip_special_tokens=True))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Output&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: There&#39;s a place where time stands still. A place of breath taking wonder, but also&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Generation&lt;/strong&gt;: There&#39;s a place where time stands still. A place of breath taking wonder, but also of great danger. A place where the very air you breathe could kill you. A place where the only way to survive is to be prepared. The place is called the Arctic. The Arctic is a vast, frozen wilderness. It is a place of extremes. The temperatures can drop to -40 degrees Celsius. The winds can reach speeds of 100 kilometers per hour. The sun can shine for 24 hours a day, or not at all for weeks on end. The Arctic is also a place of great beauty. The ice and snow are a pristine white. The sky is a deep blue. The sunsets are spectacular. But the Arctic is also a place of great danger. The ice can be treacherous. The winds can be deadly. The sun can be blinding. The Arctic is a place where the only way to survive is to be prepared. The Arctic is a place of extremes. The temperatures can drop to -40 degrees Celsius. The winds can reach speeds of 100 kilometers per hour. The sun can shine for 24 hours a day, or not at all for weeks on end. The Arctic is a place of great beauty. The ice and snow are a&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;For more advanced usage, please refer the &lt;a href=&#34;https://github.com/01-ai/Yi/tree/main/demo&#34;&gt;doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;3.2 Finetuning from the base model:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash finetune/scripts/run_sft_Yi_6b.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once finished, you can compare the finetuned model and the base model with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash finetune/scripts/run_eval.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced usage like fine-tuning based on your custom data, please refer the &lt;a href=&#34;https://github.com/01-ai/Yi/tree/main/finetune&#34;&gt;doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- #### 3.3 Quantization&#xA;&#xA;```bash&#xA;python quantization/gptq/quant_autogptq.py \&#xA;  --model /base_model                      \&#xA;  --output_dir /quantized_model            \&#xA;  --trust_remote_code&#xA;```&#xA;&#xA;Once finished, you can then evaluate the resulted model as follows:&#xA;&#xA;```bash&#xA;python quantization/gptq/eval_quantized_model.py \&#xA;  --model /quantized_model                       \&#xA;  --trust_remote_code&#xA;```&#xA;&#xA;For more detailed explanation, please read the [doc](https://github.com/01-ai/Yi/tree/main/quantization/gptq) --&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;We use data compliance checking algorithms during the training process, to ensure the compliance of the trained model to the best of our ability. Due to complex data and the diversity of language model usage scenarios, we cannot guarantee that the model will generate correct, and reasonable output in all scenarios. Please be aware that there is still a risk of the model producing problematic outputs. We will not be responsible for any risks and issues resulting from misuse, misguidance, illegal usage, and related misinformation, as well as any associated data security concerns.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The source code in this repo is licensed under the &lt;a href=&#34;https://github.com/01-ai/Yi/raw/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;. The Yi series models are fully open for academic research and free commercial usage with permission via applications. All usage must adhere to the &lt;a href=&#34;https://github.com/01-ai/Yi/raw/main/MODEL_LICENSE_AGREEMENT.txt&#34;&gt;Model License Agreement 2.0&lt;/a&gt;. To apply for the official commercial license, please contact us (&lt;a href=&#34;mailto:yi@01.ai&#34;&gt;yi@01.ai&lt;/a&gt;).&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>imoneoi/openchat</title>
    <updated>2023-11-09T01:44:47Z</updated>
    <id>tag:github.com,2023-11-09:/imoneoi/openchat</id>
    <link href="https://github.com/imoneoi/openchat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenChat: Advancing Open-source Language Models with Imperfect Data&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenChat: Advancing Open-source Language Models with Mixed-Quality Data&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/assets/logo_new.png&#34; style=&#34;width: 65%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://openchat.team&#34;&gt;üíªOnline Demo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/openchat&#34;&gt;ü§óHuggingface&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/pdf/2309.11235.pdf&#34;&gt;üìÉPaper&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/pQjnXvNKHY&#34;&gt;üí≠Discord&lt;/a&gt; &lt;br&gt;&lt;br&gt; &lt;strong&gt;üî• First 7B model that Achieves Comparable Results with ChatGPT (March)! üî•&lt;/strong&gt; &lt;br&gt; &lt;strong&gt;ü§ñ #1 Open-source model on MT-bench scoring 7.81, outperforming 70B models ü§ñ&lt;/strong&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/assets/openchat.png&#34; style=&#34;width: 45%;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/assets/openchat_grok.png&#34; style=&#34;width: 47%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenChat is an innovative library of &lt;strong&gt;open-source language models&lt;/strong&gt;, fine-tuned with &lt;a href=&#34;https://arxiv.org/pdf/2309.11235.pdf&#34;&gt;&lt;strong&gt;C-RLFT&lt;/strong&gt;&lt;/a&gt; - a strategy inspired by offline reinforcement learning.&lt;/li&gt; &#xA; &lt;li&gt;Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with &lt;code&gt;ChatGPT&lt;/code&gt;, even with a &lt;code&gt;7B&lt;/code&gt; model which can be run on a &lt;strong&gt;consumer GPU (e.g. RTX 3090)&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/badge/latestdoi/645397533&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/645397533.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;‚ú®News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/11/01] We released the &lt;a href=&#34;https://huggingface.co/openchat/openchat_3.5&#34;&gt;OpenChat-3.5-7B&lt;/a&gt; model, surpassing ChatGPT on various benchmarks üî•.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/09/21] We released our paper &lt;a href=&#34;https://arxiv.org/pdf/2309.11235.pdf&#34;&gt;OpenChat: Advancing Open-source Language Models with Mixed-Quality Data&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/09/03] We released the &lt;a href=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/%5B#models%5D(https://huggingface.co/openchat/openchat_v3.2_super)&#34;&gt;OpenChat V3.2 SUPER&lt;/a&gt; model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/08/04] We have launched an &lt;a href=&#34;https://openchat.team&#34;&gt;Online Demo&lt;/a&gt; featuring the latest version, OpenChat 3.2.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/07/30] We are thrilled to introduce the &lt;a href=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/#models&#34;&gt;OpenChat V3 model series&lt;/a&gt;, based on Llama 2, and now available for free for commercial use!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/07/07] We released the &lt;a href=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/#legacy-models&#34;&gt;OpenChat V2 model series&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2023/07/01] We released the &lt;a href=&#34;https://raw.githubusercontent.com/imoneoi/openchat/master/#legacy-models&#34;&gt;OpenChat V1 model series&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üè∑Ô∏èBenchmarks&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;# Params&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;   &lt;th&gt;MT-Bench&lt;/th&gt; &#xA;   &lt;th&gt;AGIEval&lt;/th&gt; &#xA;   &lt;th&gt;BBH MC&lt;/th&gt; &#xA;   &lt;th&gt;TruthfulQA&lt;/th&gt; &#xA;   &lt;th&gt;MMLU&lt;/th&gt; &#xA;   &lt;th&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th&gt;BBH CoT&lt;/th&gt; &#xA;   &lt;th&gt;GSM8K&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenChat-3.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7.81&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;47.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;47.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;59.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;55.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;63.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;77.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ChatGPT (March)*&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;7.94&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;47.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;47.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;57.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;48.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;70.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenHermes 2.5&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;7.54&lt;/td&gt; &#xA;   &lt;td&gt;46.5&lt;/td&gt; &#xA;   &lt;td&gt;49.4&lt;/td&gt; &#xA;   &lt;td&gt;57.5&lt;/td&gt; &#xA;   &lt;td&gt;63.8&lt;/td&gt; &#xA;   &lt;td&gt;48.2&lt;/td&gt; &#xA;   &lt;td&gt;59.9&lt;/td&gt; &#xA;   &lt;td&gt;73.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenOrca Mistral&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;52.7&lt;/td&gt; &#xA;   &lt;td&gt;6.86&lt;/td&gt; &#xA;   &lt;td&gt;42.9&lt;/td&gt; &#xA;   &lt;td&gt;49.4&lt;/td&gt; &#xA;   &lt;td&gt;45.9&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;38.4&lt;/td&gt; &#xA;   &lt;td&gt;58.1&lt;/td&gt; &#xA;   &lt;td&gt;59.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Zephyr-Œ≤^&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;34.6&lt;/td&gt; &#xA;   &lt;td&gt;7.34&lt;/td&gt; &#xA;   &lt;td&gt;39.0&lt;/td&gt; &#xA;   &lt;td&gt;40.6&lt;/td&gt; &#xA;   &lt;td&gt;40.8&lt;/td&gt; &#xA;   &lt;td&gt;39.8&lt;/td&gt; &#xA;   &lt;td&gt;22.0&lt;/td&gt; &#xA;   &lt;td&gt;16.0&lt;/td&gt; &#xA;   &lt;td&gt;5.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral**&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;6.84&lt;/td&gt; &#xA;   &lt;td&gt;38.0&lt;/td&gt; &#xA;   &lt;td&gt;39.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;   &lt;td&gt;30.5&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open-source SOTA**&lt;/td&gt; &#xA;   &lt;td&gt;13B-70B&lt;/td&gt; &#xA;   &lt;td&gt;61.4&lt;/td&gt; &#xA;   &lt;td&gt;7.71&lt;/td&gt; &#xA;   &lt;td&gt;41.7&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;63.7&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;41.4&lt;/td&gt; &#xA;   &lt;td&gt;82.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WizardLM 70B&lt;/td&gt; &#xA;   &lt;td&gt;Orca 13B&lt;/td&gt; &#xA;   &lt;td&gt;Orca 13B&lt;/td&gt; &#xA;   &lt;td&gt;Platypus2 70B&lt;/td&gt; &#xA;   &lt;td&gt;WizardLM 70B&lt;/td&gt; &#xA;   &lt;td&gt;WizardCoder 34B&lt;/td&gt; &#xA;   &lt;td&gt;Flan-T5 11B&lt;/td&gt; &#xA;   &lt;td&gt;MetaMath 70B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Evaluation details&lt;/summary&gt; *: ChatGPT (March) results are from GPT-4 Technical Report, Chain-of-Thought Hub, and our evaluation. &#xA; &lt;p&gt;^: Zephyr-Œ≤ often fails to follow few-shot CoT instructions, likely because it was aligned with only chat data but not trained on few-shot data.&lt;/p&gt; &#xA; &lt;p&gt;**: Mistral and Open-source SOTA results are taken from reported results in instruction-tuned model papers and official repositories.&lt;/p&gt; &#xA; &lt;p&gt;All models are evaluated in chat mode (e.g. with the respective conversation template applied). All zero-shot benchmarks follow the same setting as in the AGIEval paper and Orca paper. CoT tasks use the same configuration as Chain-of-Thought Hub, HumanEval is evaluated with EvalPlus, and MT-bench is run using FastChat. To reproduce our results, follow the instructions below.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Reproducing benchmarks&lt;/summary&gt; &#xA; &lt;p&gt;Reasoning:&lt;/p&gt; &#xA; &lt;p&gt;Note: Please run the following commands at the base directory of this repository.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m ochat.evaluation.run_eval --condition &#34;GPT4 Correct&#34; --model openchat/openchat_3.5&#xA;python ochat/evaluation/view_results.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;HumanEval:&lt;/p&gt; &#xA; &lt;p&gt;Note: Please run the following commands at the base directory of this repository.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m ochat.evaluation.run_eval --condition &#34;Code&#34; --eval_sets coding --model openchat/openchat_3.5&#xA;python ochat/evaluation/convert_to_evalplus.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Then all humaneval code samples are placed in &lt;code&gt;ochat/evaluation/evalplus_codegen&lt;/code&gt;. Use the following command to evaluate an individual code sample named &lt;code&gt;samples.jsonl&lt;/code&gt; using Docker as a sandbox.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -v $(pwd):/app ganler/evalplus:latest --dataset humaneval --samples samples.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;MT-Bench:&lt;/p&gt; &#xA; &lt;p&gt;Please first launch a local API server, then download FastChat and run the following commands.&lt;/p&gt; &#xA; &lt;p&gt;Note: Due to non-zero temperature and GPT-4 API changes over time, there might be variations in the results.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd fastchat/llm_judge&#xA;python gen_api_answer.py --model openchat_3.5 --max-tokens 4096 --parallel 128 --openai-api-base http://localhost:18888/v1&#xA;python gen_judgment.py --model-list openchat_3.5 --parallel 8 --mode single&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Comparison with &lt;a href=&#34;https://x.ai/&#34;&gt;X.AI Grok&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;   &lt;th&gt;# Param&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;   &lt;th&gt;MMLU&lt;/th&gt; &#xA;   &lt;th&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th&gt;MATH&lt;/th&gt; &#xA;   &lt;th&gt;GSM8k&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenChat 3.5&lt;/td&gt; &#xA;   &lt;td&gt;Apache-2.0&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;56.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;55.5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;28.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;77.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Grok-0&lt;/td&gt; &#xA;   &lt;td&gt;Proprietary&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;44.5&lt;/td&gt; &#xA;   &lt;td&gt;65.7&lt;/td&gt; &#xA;   &lt;td&gt;39.7&lt;/td&gt; &#xA;   &lt;td&gt;15.7&lt;/td&gt; &#xA;   &lt;td&gt;56.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Grok-1&lt;/td&gt; &#xA;   &lt;td&gt;Proprietary&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;55.8&lt;/td&gt; &#xA;   &lt;td&gt;73&lt;/td&gt; &#xA;   &lt;td&gt;63.2&lt;/td&gt; &#xA;   &lt;td&gt;23.9&lt;/td&gt; &#xA;   &lt;td&gt;62.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;‚¨áÔ∏èInstallation&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Need &lt;a href=&#34;https://pytorch.org/get-started/locally/#start-locally&#34;&gt;&lt;code&gt;pytorch&lt;/code&gt;&lt;/a&gt; to run OpenChat&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;pip&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install ochat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] If you facing issue using pip, try Anaconda way below or check &lt;a href=&#34;https://github.com/imoneoi/openchat/issues/41&#34;&gt;issue&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Anaconda&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -y --name openchat python=3.11&#xA;conda activate openchat&#xA;&#xA;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#xA;&#xA;pip3 install ochat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;From source&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Installing ochat from source&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/imoneoi/openchat&#xA;cd openchat&#xA;&#xA;pip3 install --upgrade pip  # enable PEP 660 support&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h1&gt;üöÄ Deploy API server&lt;/h1&gt; &#xA;&lt;h3&gt;For a single GPU (e.g. RTX 3090, 4090)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m ochat.serving.openai_api_server --model openchat/openchat_3.5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;For multiple GPUs (tensor parallel)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# N is the number of tensor parallel GPUs&#xA;python -m ochat.serving.openai_api_server --model openchat/openchat_3.5 --engine-use-ray --worker-use-ray --tensor-parallel-size N&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;use &lt;code&gt;-h&lt;/code&gt; to see more settings&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m ochat.serving.openai_api_server --model openchat/openchat_3.5 -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deploy as online service&lt;/summary&gt; &#xA; &lt;p&gt;If you want to deploy the server as an online service, you can use &lt;code&gt;--api-keys sk-KEY1 sk-KEY2 ...&lt;/code&gt; to specify allowed API keys and &lt;code&gt;--disable-log-requests --disable-log-stats --log-file openchat.log&lt;/code&gt; for logging only to a file. For security purposes, we recommend using an &lt;a href=&#34;https://fastapi.tiangolo.com/es/deployment/concepts/#security-https&#34;&gt;HTTPS gateway&lt;/a&gt; in front of the server.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Request example&lt;/h2&gt; &#xA;&lt;p&gt;Once started, the server listens at &lt;code&gt;localhost:18888&lt;/code&gt; for requests and is compatible with the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI ChatCompletion API specifications&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:18888/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;    &#34;model&#34;: &#34;openchat_3.5&#34;,&#xA;    &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;You are a large language model named OpenChat. Write a poem to describe yourself&#34;}]&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Coding Mode&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl http://localhost:18888/v1/chat/completions \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;    &#34;model&#34;: &#34;openchat_3.5&#34;,&#xA;    &#34;condition&#34;: &#34;Code&#34;,&#xA;    &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Write an aesthetic TODO app using HTML5 and JS, in a single file. You should use round corners and gradients to make it more aesthetic.&#34;}]&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt;  &#xA;&lt;h1&gt;&lt;a id=&#34;web-ui&#34;&gt;&lt;/a&gt; üåêWeb UI - &lt;a href=&#34;https://github.com/imoneoi/openchat-ui&#34;&gt;OpenChat-UI&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;After launching the API server, OpenChat provide user interface that easy to interact with. &lt;a href=&#34;https://github.com/imoneoi/openchat-ui&#34;&gt;Click here to check Web UI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;ü§óInference with Huggingface&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Slow and not recommended&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;tokenizer = transformers.AutoTokenizer.from_pretrained(&#34;openchat/openchat_3.5&#34;)&#xA;&#xA;# Single-turn&#xA;tokens = tokenizer(&#34;GPT4 Correct User: Hello&amp;lt;|end_of_turn|&amp;gt;GPT4 Correct Assistant:&#34;).input_ids&#xA;assert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]&#xA;&#xA;# Multi-turn&#xA;tokens = tokenizer(&#34;GPT4 Correct User: Hello&amp;lt;|end_of_turn|&amp;gt;GPT4 Correct Assistant: Hi&amp;lt;|end_of_turn|&amp;gt;GPT4 Correct User: How are you today?&amp;lt;|end_of_turn|&amp;gt;GPT4 Correct Assistant:&#34;).input_ids&#xA;assert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]&#xA;&#xA;# Coding Mode&#xA;tokens = tokenizer(&#34;Code User: Implement quicksort using C++&amp;lt;|end_of_turn|&amp;gt;Code Assistant:&#34;).input_ids&#xA;assert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;&lt;a id=&#34;training&#34;&gt;&lt;/a&gt; üõ†Ô∏èTraining&lt;/h1&gt; &#xA;&lt;p&gt;The OpenChat training system utilizes padding-free training and the &lt;a href=&#34;https://github.com/imoneoi/multipack_sampler&#34;&gt;Multipack Sampler&lt;/a&gt;, achieving a &lt;strong&gt;3~10x&lt;/strong&gt; speedup compared to the conventional padded training.&lt;/p&gt; &#xA;&lt;h2&gt;Choose a base model&lt;/h2&gt; &#xA;&lt;p&gt;OpenChat supports Llama 2 and Mistral models. Please first choose a base model to fit your needs. Each base model has a corresponding weight repo, model type, and recommended batch size as listed below, they should be filled into &lt;code&gt;BASE_REPO&lt;/code&gt;, &lt;code&gt;MODEL_TYPE&lt;/code&gt;, and &lt;code&gt;BATCH_SIZE&lt;/code&gt; in the following instructions.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Weights (with EOT token)&lt;/th&gt; &#xA;   &lt;th&gt;Model Type&lt;/th&gt; &#xA;   &lt;th&gt;Recommended Batch Size per GPU (8xA100 80GB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;imone/Mistral_7B_with_EOT_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openchat_v3.2_mistral&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83968&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;imone/LLaMA2_7B_with_EOT_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openchat_v3.2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83968&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;imone/Llama2_13B_with_EOT_token&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;openchat_v3.2&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;36864&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note: The OpenChat conversation template requires an &lt;code&gt;&amp;lt;|end_of_turn|&amp;gt;&lt;/code&gt; special token. The base model specified must include this token. Our provided weights are the original base weights with this token added. If you want to add them manually, use the &lt;code&gt;convert_llama_weights_to_hf_add_tokens.py&lt;/code&gt; or &lt;code&gt;mistral_add_tokens.py&lt;/code&gt; in the &lt;code&gt;scripts&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Installing DeepSpeed&lt;/h2&gt; &#xA;&lt;p&gt;First, ensure that the CUDA &lt;code&gt;nvcc&lt;/code&gt; compiler is available in your environment. If it is not, install the CUDA toolkit that matches the version used by PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Next, install DeepSpeed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install deepspeed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Preparing Your Data&lt;/h3&gt; &#xA;&lt;p&gt;To utilize the OpenChat trainer, prepare your SFT data into a JSON Lines format where each line corresponds to a &lt;code&gt;Conversation&lt;/code&gt; object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Message(BaseModel):&#xA;    role: str     # Must be &#34;user&#34; or &#34;assistant&#34;&#xA;    content: str  # Message content&#xA;    weight: Optional[float] = None  # Loss weight for this message. Typically 0 for user and 1 for assistant to supervise assistant&#39;s responses only&#xA;&#xA;&#xA;class Conversation(BaseModel):&#xA;    items: List[Message]  # All messages within the conversation&#xA;    condition: str = &#34;&#34;  # C-RLFT condition, can be any string or empty.&#xA;    system: str = &#34;&#34;  # System message for this conversation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For basic SFT, assign &lt;code&gt;weight&lt;/code&gt; as &lt;code&gt;0&lt;/code&gt; for human messages and &lt;code&gt;1&lt;/code&gt; for assistant responses.&lt;/p&gt; &#xA;&lt;p&gt;SFT example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;items&#34;:[{&#34;from&#34;:&#34;user&#34;,&#34;content&#34;:&#34;Hello&#34;,&#34;weight&#34;:0.0},{&#34;from&#34;:&#34;assistant&#34;,&#34;content&#34;:&#34;Hi&#34;,&#34;weight&#34;:1.0},{&#34;from&#34;:&#34;user&#34;,&#34;content&#34;:&#34;How are you today?&#34;,&#34;weight&#34;:0.0},{&#34;from&#34;:&#34;assistant&#34;,&#34;content&#34;:&#34;I&#39;m fine.&#34;,&#34;weight&#34;:1.0}],&#34;system&#34;:&#34;&#34;}&#xA;{&#34;items&#34;:[{&#34;from&#34;:&#34;user&#34;,&#34;content&#34;:&#34;Who are you?&#34;,&#34;weight&#34;:0.0},{&#34;from&#34;:&#34;assistant&#34;,&#34;content&#34;:&#34;I&#39;m OpenChat.&#34;,&#34;weight&#34;:1.0}],&#34;system&#34;:&#34;You are a helpful assistant named OpenChat.&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For C-RLFT, &lt;code&gt;condition&lt;/code&gt; should be set as the class the conversation belongs to (e.g. &lt;code&gt;GPT3&lt;/code&gt; or &lt;code&gt;GPT4&lt;/code&gt;). The &lt;code&gt;weight&lt;/code&gt; is assigned as &lt;code&gt;0&lt;/code&gt; for human messages and &lt;code&gt;w&lt;/code&gt; for assistant responses, where &lt;code&gt;w&lt;/code&gt; is the weight of the class (e.g. &lt;code&gt;0.1&lt;/code&gt; for &lt;code&gt;GPT3&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; for &lt;code&gt;GPT4&lt;/code&gt;, as found in our C-RLFT paper).&lt;/p&gt; &#xA;&lt;p&gt;C-RLFT example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#34;items&#34;:[{&#34;from&#34;:&#34;user&#34;,&#34;content&#34;:&#34;What is C-RLFT?&#34;,&#34;weight&#34;:0.0},{&#34;from&#34;:&#34;assistant&#34;,&#34;content&#34;:&#34;C-RLFT is a method for improving open-source LLMs with mixed-quality data.&#34;,&#34;weight&#34;:1.0}],&#34;condition&#34;:&#34;GPT4&#34;,&#34;system&#34;:&#34;&#34;}&#xA;{&#34;items&#34;:[{&#34;from&#34;:&#34;user&#34;,&#34;content&#34;:&#34;What is C-RLFT?&#34;,&#34;weight&#34;:0.0},{&#34;from&#34;:&#34;assistant&#34;,&#34;content&#34;:&#34;I don&#39;t know.&#34;,&#34;weight&#34;:0.1}],&#34;condition&#34;:&#34;GPT3&#34;,&#34;system&#34;:&#34;&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-tokenizing the Dataset&lt;/h3&gt; &#xA;&lt;p&gt;You&#39;ll then need to pre-tokenize the dataset using the command (please specify a filename as &lt;code&gt;PRETOKENIZED_DATA_OUTPUT_PATH&lt;/code&gt; to store the pretokenized dataset):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m ochat.data.generate_dataset --model-type MODEL_TYPE --model-path BASE_REPO --in-files data.jsonl --out-prefix PRETOKENIZED_DATA_OUTPUT_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Launching the OpenChat Trainer&lt;/h3&gt; &#xA;&lt;p&gt;You can now launch the OpenChat trainer using the command below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;13B model requires eight &lt;code&gt;A/H100s&lt;/code&gt; with 80GB VRAM&lt;/li&gt; &#xA; &lt;li&gt;7B model can be trained with four &lt;code&gt;A/H100s&lt;/code&gt; with 80GB VRAM or eight &lt;code&gt;A/H100s&lt;/code&gt; with 40GB VRAM.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For hyperparameters, we recommend first setting the batch size to the recommended batch size. If OOM occurs, try setting it to the exact maximum that VRAM can hold and as a multiple of &lt;code&gt;2048&lt;/code&gt;. Other hyperparameters have been carefully selected as the default. Furthermore, the learning rate is automatically determined based on the &lt;a href=&#34;https://arxiv.org/abs/2006.09092&#34;&gt;inverse square-root rule&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Training Commands (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NUM_GPUS=8&#xA;&#xA;deepspeed --num_gpus=$NUM_GPUS --module ochat.training_deepspeed.train \&#xA;          --model_path BASE_REPO \&#xA;          --data_prefix PRETOKENIZED_DATA_OUTPUT_PATH \&#xA;          --save_path PATH_TO_SAVE_MODEL \&#xA;          --batch_max_len BATCH_SIZE \&#xA;          --epochs 5 \&#xA;          --save_every 1 \&#xA;          --deepspeed \&#xA;          --deepspeed_config ochat/training_deepspeed/deepspeed_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You can find checkpoints of all epochs in &lt;code&gt;PATH_TO_SAVE_MODEL&lt;/code&gt;. Then you may evaluate each epoch and choose the best one.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;h2&gt;Foundation Model Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Despite its advanced capabilities, OpenChat is still bound by the limitations inherent in its foundation models. These limitations may impact the model&#39;s performance in areas such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Complex reasoning&lt;/li&gt; &#xA; &lt;li&gt;Mathematical and arithmetic tasks&lt;/li&gt; &#xA; &lt;li&gt;Programming and coding challenges&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hallucination of Non-existent Information&lt;/h2&gt; &#xA;&lt;p&gt;OpenChat may sometimes generate information that does not exist or is not accurate, also known as &#34;hallucination&#34;. Users should be aware of this possibility and verify any critical information obtained from the model.&lt;/p&gt; &#xA;&lt;h2&gt;Safety&lt;/h2&gt; &#xA;&lt;p&gt;OpenChat may sometimes generate harmful, hate speech, biased responses, or answer unsafe questions. It&#39;s crucial to apply additional AI safety measures in use cases that require safe and moderated responses.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Our OpenChat 3.5 &lt;code&gt;code&lt;/code&gt; and &lt;code&gt;models&lt;/code&gt; are distributed under the &lt;strong&gt;Apache License 2.0&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;a id=&#34;models&#34;&gt;&lt;/a&gt; Models&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Context&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;   &lt;th&gt;Serving&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenChat 3.5&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;8192&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openchat/openchat_3.5&#34;&gt;Huggingface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m ochat.serving.openai_api_server --model openchat/openchat_3.5 --engine-use-ray --worker-use-ray&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;a id=&#34;legacy-models&#34;&gt;&lt;/a&gt; Legacy Models&lt;/h2&gt; &#xA;&lt;p&gt;The following models are older versions of OpenChat and have inferior performance compared to the latest version. They will be deprecated in the next release. Please note that OpenChat V1 and V2 series are now deprecated, &lt;a href=&#34;https://github.com/imoneoi/openchat/tree/83a683c775c77867cc45937fafdf48e8dcb68daa&#34;&gt;please install 3.1.x for using V1 and V2 models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run the models on multiple GPUs with smaller VRAM, you can enable tensor parallelization, for example, using the &lt;code&gt;--tensor-parallel-size 2&lt;/code&gt; flag.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Context&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;   &lt;th&gt;Serving&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenChat 3.2 SUPER&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openchat/openchat_v3.2_super&#34;&gt;Huggingface&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;python -m ochat.serving.openai_api_server --model openchat/openchat_v3.2_super --engine-use-ray --worker-use-ray&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;üíåContact&lt;/h1&gt; &#xA;&lt;p&gt;We are a student team from Tsinghua University, working on OpenChat, a project that requires additional computing power or LLMs API keys for further development. If you are interested in our project and would like to offer support, please feel free to reach out to us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Wang Guan [imonenext at gmail dot com]&lt;/li&gt; &#xA; &lt;li&gt;Cheng Sijie [csj23 at mails dot tsinghua dot edu dot cn]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We look forward to hearing from you and collaborating on this exciting project!&lt;/p&gt; &#xA;&lt;h1&gt;Citation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wang2023openchat,&#xA;  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},&#xA;  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},&#xA;  journal={arXiv preprint arXiv:2309.11235},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;We extend our heartfelt gratitude to Alignment Lab AI, Nous Research, and Pygmalion AI for their substantial contributions to data collection and model training.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks go to Changling Liu from GPT Desk Pte. Ltd., Qiying Yu at Tsinghua University, Baochang Ma, and Hao Wan from 01.AI company for their generous provision of resources. We are also deeply grateful to Jianxiong Li and Peng Li at Tsinghua University for their insightful discussions.&lt;/p&gt; &#xA;&lt;p&gt;Furthermore, we appreciate the developers behind the following projects for their significant contributions to our research: &lt;a href=&#34;https://mistral.ai/&#34;&gt;Mistral&lt;/a&gt;, &lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;Chain-of-Thought Hub&lt;/a&gt;, &lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;Llama 2&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Self-Instruct&lt;/a&gt;, &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat (Vicuna)&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca.git&#34;&gt;Alpaca&lt;/a&gt;, and &lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34;&gt;StarCoder&lt;/a&gt;. Their work has been instrumental in driving our research forward.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/playwright-python</title>
    <updated>2023-11-09T01:44:47Z</updated>
    <id>tag:github.com,2023-11-09:/microsoft/playwright-python</id>
    <link href="https://github.com/microsoft/playwright-python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python version of the Playwright testing and automation library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üé≠ &lt;a href=&#34;https://playwright.dev&#34;&gt;Playwright&lt;/a&gt; for Python &lt;a href=&#34;https://pypi.python.org/pypi/playwright/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/playwright.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/Microsoft/playwright&#34;&gt;&lt;img src=&#34;https://img.shields.io/conda/v/microsoft/playwright&#34; alt=&#34;Anaconda version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://aka.ms/playwright/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/join-discord-infomational&#34; alt=&#34;Join Discord&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Playwright is a Python library to automate &lt;a href=&#34;https://www.chromium.org/Home&#34;&gt;Chromium&lt;/a&gt;, &lt;a href=&#34;https://www.mozilla.org/en-US/firefox/new/&#34;&gt;Firefox&lt;/a&gt; and &lt;a href=&#34;https://webkit.org/&#34;&gt;WebKit&lt;/a&gt; browsers with a single API. Playwright delivers automation that is &lt;strong&gt;ever-green&lt;/strong&gt;, &lt;strong&gt;capable&lt;/strong&gt;, &lt;strong&gt;reliable&lt;/strong&gt; and &lt;strong&gt;fast&lt;/strong&gt;. &lt;a href=&#34;https://playwright.dev/python/docs/why-playwright&#34;&gt;See how Playwright is better&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linux&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;macOS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Windows&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Chromium &#xA;    &lt;!-- GEN:chromium-version --&gt;119.0.6045.9&#xA;    &lt;!-- GEN:stop --&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;WebKit &#xA;    &lt;!-- GEN:webkit-version --&gt;17.4&#xA;    &lt;!-- GEN:stop --&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Firefox &#xA;    &lt;!-- GEN:firefox-version --&gt;118.0.1&#xA;    &lt;!-- GEN:stop --&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://playwright.dev/python/docs/intro&#34;&gt;https://playwright.dev/python/docs/intro&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;API Reference&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://playwright.dev/python/docs/api/class-playwright&#34;&gt;https://playwright.dev/python/docs/api/class-playwright&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from playwright.sync_api import sync_playwright&#xA;&#xA;with sync_playwright() as p:&#xA;    for browser_type in [p.chromium, p.firefox, p.webkit]:&#xA;        browser = browser_type.launch()&#xA;        page = browser.new_page()&#xA;        page.goto(&#39;http://playwright.dev&#39;)&#xA;        page.screenshot(path=f&#39;example-{browser_type.name}.png&#39;)&#xA;        browser.close()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import asyncio&#xA;from playwright.async_api import async_playwright&#xA;&#xA;async def main():&#xA;    async with async_playwright() as p:&#xA;        for browser_type in [p.chromium, p.firefox, p.webkit]:&#xA;            browser = await browser_type.launch()&#xA;            page = await browser.new_page()&#xA;            await page.goto(&#39;http://playwright.dev&#39;)&#xA;            await page.screenshot(path=f&#39;example-{browser_type.name}.png&#39;)&#xA;            await browser.close()&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Other languages&lt;/h2&gt; &#xA;&lt;p&gt;More comfortable in another programming language? &lt;a href=&#34;https://playwright.dev&#34;&gt;Playwright&lt;/a&gt; is also available in&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://playwright.dev/docs/intro&#34;&gt;Node.js (JavaScript / TypeScript)&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://playwright.dev/dotnet/docs/intro&#34;&gt;.NET&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://playwright.dev/java/docs/intro&#34;&gt;Java&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>