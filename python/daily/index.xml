<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-12T01:35:59Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dvlab-research/LISA</title>
    <updated>2023-08-12T01:35:59Z</updated>
    <id>tag:github.com,2023-08-12:/dvlab-research/LISA</id>
    <link href="https://github.com/dvlab-research/LISA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Project Page for &#34;LISA: Reasoning Segmentation via Large Language Model&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LISA: Reasoning Segmentation via Large Language Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;font size=&#34;7&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;font size=&#34;7&#34;&gt;&lt;b&gt;LISA&lt;/b&gt;: Large &lt;b&gt;L&lt;/b&gt;anguage &lt;b&gt;I&lt;/b&gt;nstructed &lt;b&gt;S&lt;/b&gt;egmentation &lt;b&gt;A&lt;/b&gt;ssistant&lt;/font&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;font size=&#34;7&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;div align=&#34;center&#34;&gt;&#xA; &lt;font size=&#34;7&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/2308.00692.pdf&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/xinlai&#34;&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/#training&#34;&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/#inference&#34;&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/#dataset&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;&lt;strong&gt;Online Demo&lt;/strong&gt;&lt;/a&gt;&lt;/font&gt;&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;imgs/teaser.jpg&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; --&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34;&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34;&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34;&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34;&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/obama.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/obama.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/trump.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/trump.jpg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;Who was &lt;ins&gt;the president of the US&lt;/ins&gt; in this image? Please output segmentation mask and explain the reason.&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, the segmentation result is [SEG]. The President of the United States in the image is &lt;ins&gt;President Obama.&lt;/ins&gt;‚Äù&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray&#34;&gt;&#34;Who was &lt;ins&gt;the president of the US&lt;/ins&gt; in this image? Please output segmentation mask and explain why.&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, [SEG]. In the image, the President of the United States is &lt;ins&gt;President Trump.&lt;/ins&gt;&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/stand_higher.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/stand_higher.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/camera_lens.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/camera_lens.jpg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;What can &lt;ins&gt;make the woman stand higher&lt;/ins&gt;? Please output segmentation mask and explain why.&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, [SEG]. The woman is &lt;ins&gt;standing higher by using a ladder&lt;/ins&gt;...&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray&#34;&gt;&#34;Can you segment &lt;ins&gt;the camera lens that is more suitable for photographing nearby objects&lt;/ins&gt; in this image?&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, [SEG].&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/dog_with_horn.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/dog_with_horn.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/wash_hands.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/wash_hands.jpg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;Can you segment &lt;ins&gt;the unusual part&lt;/ins&gt; in this image and explain why.&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, it is [SEG]. In the image, the unusual part is &lt;ins&gt;the dog wearing a reindeer antler headband&lt;/ins&gt;...&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray&#34;&gt;&#34;Where to &lt;ins&gt;wash hands&lt;/ins&gt; in this image? Please output segmentation mask.&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, [SEG].&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/jackma.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/jackma.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/blackpink.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/blackpink.jpg&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;Can you segment &lt;ins&gt;the founder of Alibaba&lt;/ins&gt; in this image and explain why?&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, it is [SEG]. In the image, the man wearing a red tie and sitting on one of the chairs is &lt;ins&gt;Jack Ma&lt;/ins&gt;, the co-founder of Alibaba Group...&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray&#34;&gt;&#34;Please segment &lt;ins&gt;Lisa&lt;/ins&gt; in this figure.&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Sure, [SEG].&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/fig_overview.jpg&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; [2023.8.9] Training code is released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; [2023.8.4] &lt;a href=&#34;http://103.170.5.190:7860/&#34;&gt;Online Demo&lt;/a&gt; is released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; [2023.8.4] &lt;a href=&#34;https://drive.google.com/drive/folders/125mewyg5Ao6tZ3ZdJ-1-E3n04LGVELqy?usp=sharing&#34;&gt;&lt;em&gt;ReasonSeg&lt;/em&gt; Dataset&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/xinlai/LISA-13B-llama2-v0-explanatory&#34;&gt;LISA-13B-llama2-v0-explanatory&lt;/a&gt; model are released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; [2023.8.3] Inference code and the &lt;a href=&#34;https://huggingface.co/xinlai/LISA-13B-llama2-v0&#34;&gt;LISA-13B-llama2-v0&lt;/a&gt; model are released. Welcome to check out!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; [2023.8.2] &lt;a href=&#34;https://arxiv.org/pdf/2308.00692.pdf&#34;&gt;Paper&lt;/a&gt; is released and GitHub repo is created.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;LISA: Reasoning Segmentation Via Large Language Model [&lt;a href=&#34;https://arxiv.org/abs/2308.00692&#34;&gt;Paper&lt;/a&gt;]&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://scholar.google.com/citations?user=tqNDPA4AAAAJ&amp;amp;hl=zh-CN&#34;&gt;Xin Lai&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=mEjhz-IAAAAJ&amp;amp;hl=en&#34;&gt;Zhuotao Tian&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=6p0ygKUAAAAJ&amp;amp;hl=en&#34;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=I-UCPPcAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Yanwei Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=PzyvzksAAAAJ&amp;amp;hl=en&#34;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=BUEDUFkAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=XPAkzTEAAAAJ&amp;amp;hl=en&#34;&gt;Jiaya Jia&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;In this work, we propose a new segmentation task --- &lt;em&gt;&lt;strong&gt;reasoning segmentation&lt;/strong&gt;&lt;/em&gt;. The task is designed to output a segmentation mask given a complex and implicit query text. We establish a benchmark comprising over one thousand image-instruction pairs, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: Large-language Instructed Segmentation Assistant, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks. For more details, please refer to the &lt;a href=&#34;https://arxiv.org/abs/2308.00692&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;LISA&lt;/strong&gt; unlocks the new segmentation capabilities of multi-modal LLMs, and can handle cases involving:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;complex reasoning;&lt;/li&gt; &#xA; &lt;li&gt;world knowledge;&lt;/li&gt; &#xA; &lt;li&gt;explanatory answers;&lt;/li&gt; &#xA; &lt;li&gt;multi-turn conversation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;LISA&lt;/strong&gt; also demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation image-instruction pairs results in further performance enhancement.&lt;/p&gt; &#xA;&lt;h2&gt;Experimental results&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/table1.jpg&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Training Data Preparation&lt;/h3&gt; &#xA;&lt;p&gt;The training data consists of 4 types of data:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Semantic segmentation datasets: &lt;a href=&#34;http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip&#34;&gt;ADE20K&lt;/a&gt;, &lt;a href=&#34;http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip&#34;&gt;COCO-Stuff&lt;/a&gt;, &lt;a href=&#34;https://www.mapillary.com/dataset/vistas&#34;&gt;Mapillary&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/paco/tree/main#dataset-setup&#34;&gt;PACO-LVIS&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/VLPart/tree/main/datasets#pascal-part&#34;&gt;PASCAL-Part&lt;/a&gt;, &lt;a href=&#34;http://images.cocodataset.org/zips/train2017.zip&#34;&gt;COCO Images&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: For COCO-Stuff, we use the annotation file stuffthingmaps_trainval2017.zip. We only use the PACO-LVIS part in PACO. COCO Images should be put into the &lt;code&gt;dataset/coco/&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Referring segmentation datasets: &lt;a href=&#34;https://web.archive.org/web/20220413011718/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcoco.zip&#34;&gt;refCOCO&lt;/a&gt;, &lt;a href=&#34;https://web.archive.org/web/20220413011656/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcoco+.zip&#34;&gt;refCOCO+&lt;/a&gt;, &lt;a href=&#34;https://web.archive.org/web/20220413012904/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcocog.zip&#34;&gt;refCOCOg&lt;/a&gt;, &lt;a href=&#34;https://web.archive.org/web/20220413011817/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refclef.zip&#34;&gt;refCLEF&lt;/a&gt; (&lt;a href=&#34;https://web.archive.org/web/20220515000000/http://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip&#34;&gt;saiapr_tc-12&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Note: the origianl links of refCOCO series data are down, and we update them with new ones&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Visual Question Answering dataset: &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json&#34;&gt;LLaVA-Instruct-150k&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Reasoning segmentation dataset: &lt;a href=&#34;https://github.com/dvlab-research/LISA#dataset&#34;&gt;ReasonSeg&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Download them from the above links, and organize them as follows.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ dataset&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ ade20k&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ annotations&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ images&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ coco&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ train2017&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ 000000000009.jpg&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ ...&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ cocostuff&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ train2017&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ 000000000009.png&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ ...&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ llava_dataset&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ llava_instruct_150k.json&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ mapillary&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ config_v2.0.json&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ testing&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ training&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ validation&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ reason_seg&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ ReasonSeg&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ train&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ val&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ explanatory&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ refer_seg&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ images&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; |   ‚îú‚îÄ‚îÄ saiapr_tc-12 &#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; |   ‚îî‚îÄ‚îÄ mscoco&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; |       ‚îî‚îÄ‚îÄ images&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; |           ‚îî‚îÄ‚îÄ train2014&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ refclef&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ refcoco&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ refcoco+&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ refcocog&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ vlpart&#xA;‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ paco&#xA;‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ annotations&#xA;‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ pascal_part&#xA;‚îÇ&amp;nbsp;&amp;nbsp;         ‚îú‚îÄ‚îÄ train.json&#xA;‚îÇ           ‚îî‚îÄ‚îÄ VOCdevkit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pre-trained weights&lt;/h3&gt; &#xA;&lt;h4&gt;LLaVA&lt;/h4&gt; &#xA;&lt;p&gt;To train LISA-7B or 13B, you need to follow the &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md&#34;&gt;instruction&lt;/a&gt; to merge the LLaVA delta weights. Typically, we use the final weights &lt;code&gt;LLaVA-Lightning-7B-v1-1&lt;/code&gt; and &lt;code&gt;LLaVA-13B-v1-1&lt;/code&gt; merged from &lt;code&gt;liuhaotian/LLaVA-Lightning-7B-delta-v1-1&lt;/code&gt; and &lt;code&gt;liuhaotian/LLaVA-13b-delta-v1-1&lt;/code&gt;, respectively. For Llama2, we can directly use the LLaVA full weights &lt;code&gt;liuhaotian/llava-llama-2-13b-chat-lightning-preview&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;SAM ViT-H weights&lt;/h4&gt; &#xA;&lt;p&gt;Download SAM ViT-H pre-trained weights from the &lt;a href=&#34;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepspeed --master_port=24999 train_ds.py \&#xA;  --version=&#34;PATH_TO_LLaVA&#34; \&#xA;  --dataset_dir=&#39;./dataset&#39; \&#xA;  --vision_pretrained=&#34;PATH_TO_SAM&#34; \&#xA;  --dataset=&#34;sem_seg||refer_seg||vqa||reason_seg&#34; \&#xA;  --sample_rates=&#34;9,3,3,1&#34; \&#xA;  --exp_name=&#34;lisa-7b&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When training is finished, to get the full model weight:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ./runs/lisa-7b/ckpt_model &amp;amp;&amp;amp; python zero_to_fp32.py . ../pytorch_model.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Validation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepspeed --master_port=24999 train_ds.py \&#xA;  --version=&#34;PATH_TO_LLaVA&#34; \&#xA;  --dataset_dir=&#39;./dataset&#39; \&#xA;  --vision_pretrained=&#34;PATH_TO_SAM&#34; \&#xA;  --exp_name=&#34;lisa-7b&#34; \&#xA;  --weight=&#39;PATH_TO_pytorch_model.bin&#39; \&#xA;  --eval_only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;To chat with &lt;a href=&#34;https://huggingface.co/xinlai/LISA-13B-llama2-v0&#34;&gt;LISA-13B-llama2-v0&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/xinlai/LISA-13B-llama2-v0-explanatory&#34;&gt;LISA-13B-llama2-v0-explanatory&lt;/a&gt;: (Note that LISA-13B-llama2-v0 currently does not support explanatory answers.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python3 chat.py --version=&#39;xinlai/LISA-13B-llama2-v0&#39;&#xA;CUDA_VISIBLE_DEVICES=0 python3 chat.py --version=&#39;xinlai/LISA-13B-llama2-v0-explanatory&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use &lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt; data type for inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python3 chat.py --version=&#39;xinlai/LISA-13B-llama2-v0&#39; --precision=&#39;bf16&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use &lt;code&gt;8bit&lt;/code&gt; or &lt;code&gt;4bit&lt;/code&gt; data type for inference (this enables running 13B model on a single 24G or 12G GPU at some cost of generation quality):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python3 chat.py --version=&#39;xinlai/LISA-13B-llama2-v0&#39; --precision=&#39;fp16&#39; --load_in_8bit&#xA;CUDA_VISIBLE_DEVICES=0 python3 chat.py --version=&#39;xinlai/LISA-13B-llama2-v0&#39; --precision=&#39;fp16&#39; --load_in_4bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, input the text prompt and then the image path. For exampleÔºå&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- Please input your prompt: Where can the driver see the car speed in this image? Please output segmentation mask.&#xA;- Please input the image path: imgs/example1.jpg&#xA;&#xA;- Please input your prompt: Can you segment the food that tastes spicy and hot?&#xA;- Please input the image path: imgs/example2.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results should be like:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/example1.jpg&#34; width=&#34;22%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/example1_masked_img_0.jpg&#34; width=&#34;22%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/imgs/example2.jpg&#34; width=&#34;25%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/dvlab-research/LISA/main/vis_output/example2_masked_img_0.jpg&#34; width=&#34;25%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;In ReasonSeg, we have collected 1218 images (239 train, 200 val, and 779 test). The training and validation sets can be download from &lt;a href=&#34;https://drive.google.com/drive/folders/125mewyg5Ao6tZ3ZdJ-1-E3n04LGVELqy?usp=sharing&#34;&gt;&lt;strong&gt;this link&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Each image is provided with an annotation JSON file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;image_1.jpg, image_1.json&#xA;image_2.jpg, image_2.json&#xA;...&#xA;image_n.jpg, image_n.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Important keys contained in JSON files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- &#34;text&#34;: text instructions.&#xA;- &#34;is_sentence&#34;: whether the text instructions are long sentences.&#xA;- &#34;shapes&#34;: target polygons.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The elements of the &#34;shapes&#34; exhibit two categories, namely &lt;strong&gt;&#34;target&#34;&lt;/strong&gt; and &lt;strong&gt;&#34;ignore&#34;&lt;/strong&gt;. The former category is indispensable for evaluation, while the latter category denotes the ambiguous region and hence disregarded during the evaluation process.&lt;/p&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://github.com/dvlab-research/LISA/raw/main/utils/data_processing.py&#34;&gt;&lt;strong&gt;script&lt;/strong&gt;&lt;/a&gt; that demonstrates how to process the annotations:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 utils/data_processing.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Besides, we leveraged GPT-3.5 for rephrasing instructions, so images in the training set may have &lt;strong&gt;more than one instructions (but fewer than six)&lt;/strong&gt; in the &#34;text&#34; field. During training, users may randomly select one as the text query to obtain a better model.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{reason_seg,&#xA;  title={LISA: Reasoning Segmentation via Large Language Model},&#xA;  author={Xin Lai and Zhuotao Tian and Yukang Chen and Yanwei Li and Yuhui Yuan and Shu Liu and Jiaya Jia},&#xA;  journal={arXiv:2308.00692},&#xA;  year={2023}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This work is built upon the &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>morph-labs/rift</title>
    <updated>2023-08-12T01:35:59Z</updated>
    <id>tag:github.com,2023-08-12:/morph-labs/rift</id>
    <link href="https://github.com/morph-labs/rift" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Rift: an AI-native language server for your personal AI software engineer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rift&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=Morph.rift-vscode&#34;&gt;Download for VSCode&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Rift is open-source infrastructure for AI-native development environments. Rift makes your IDE &lt;em&gt;agentic&lt;/em&gt;. Software will soon be written mostly by AI software engineers that work alongside you. Codebases will soon be living, spatial artifacts that &lt;em&gt;maintain context&lt;/em&gt;, &lt;em&gt;listen to&lt;/em&gt;, &lt;em&gt;anticipate&lt;/em&gt;, &lt;em&gt;react to&lt;/em&gt;, and &lt;em&gt;execute&lt;/em&gt; your every intent. The &lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/rift-engine/&#34;&gt;Rift Code Engine&lt;/a&gt; implements an AI-native extension of the &lt;a href=&#34;https://microsoft.github.io/language-server-protocol/&#34;&gt;language server protocol&lt;/a&gt;. The &lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/editors/rift-vscode&#34;&gt;Rift VSCode extension&lt;/a&gt; implements an client and end-user interface which is the first step into that future.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/morph-labs/rift/assets/13114790/726f35ed-4959-4f69-9a80-fd903b26f909&#34;&gt;https://github.com/morph-labs/rift/assets/13114790/726f35ed-4959-4f69-9a80-fd903b26f909&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/wa5sgWMfqv&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/#getting-started&#34;&gt;Getting started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/#manual-installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/#tips&#34;&gt;Tips&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/#the-road-ahead&#34;&gt;The road ahead&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conversational code editing&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/morph-labs/rift/raw/pranav/dev/assets/code-edit.gif&#34; alt=&#34;code edit screencast&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Codebase-wide edits&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/morph-labs/rift/raw/pranav/dev/assets/aider.gif&#34; alt=&#34;aider screencast&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contextual codebase generation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/morph-labs/rift/raw/pranav/dev/assets/smol.gif&#34; alt=&#34;smol screencast&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tips&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Press Command+K to focus the Rift Omnibar. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Once focused, you can either engage with the current chat or use a slash-command (e.g. &lt;code&gt;/aider&lt;/code&gt;) to spawn a new agent.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Each instance of a Rift Chat or Code Edit agent will remain attached to the open file / selection you used to spawn it. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;To switch to a new file or request a code edit on a new selection, spawn a new agent by pressing Command+K and running a slash-command (e.g. &lt;code&gt;/edit&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Both Rift Chat and Code Edit see a window around your cursor or selection in the currently active editor window. To tell them about other resources in your codebase, mention them with &lt;code&gt;@&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Code Edit&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You can &lt;code&gt;@&lt;/code&gt;-mention files and directories to tell your agents about other parts of the codebase.&lt;/li&gt; &#xA; &lt;li&gt;Currently, Rift works best when the active workspace directory is the same as the root directory of the &lt;code&gt;git&lt;/code&gt; project.&lt;/li&gt; &#xA; &lt;li&gt;Command+Shift+P -&amp;gt; &#34;Rift: Start Server&#34; restarts the server if it has been auto-installed.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Install the VSCode extension from the VSCode Marketplace. By default, the extension will attempt to automatically start the Rift Code Engine every time the extension is activated. During this process, if a &lt;code&gt;rift&lt;/code&gt; executable is not found in a virtual environment under &lt;code&gt;~/.morph&lt;/code&gt;, the extension will ask you to attempt an automatic installation of a Python environment and the Rift Code Engine. To disable this behavior, such as for development, go to the VSCode settings, search for &#34;rift&#34;, and set &lt;code&gt;rift.autostart&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If the automatic installation of the Rift Code Engine fails, follow the below instructions for manual installation.&lt;/p&gt; &#xA;&lt;h3&gt;Manual installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rift Code Engine&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set up a Python virtual environment for Python 3.10 or higher. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;On Mac OSX: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Install &lt;a href=&#34;https://brew.sh&#34;&gt;homebrew&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;brew install python@3.10&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;mkdir -p ~/.morph/ &amp;amp;&amp;amp; cd ~/.morph/ &amp;amp;&amp;amp; python3.10 -m venv env&lt;/code&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;code&gt;source ./env/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;On Linux: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;On Ubuntu: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;code&gt;sudo apt install software-properties-common -y&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;sudo add-apt-repository ppa:deadsnakes/ppa&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;sudo apt install python3.10 &amp;amp;&amp;amp; sudo apt install python3.10-venv&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;mkdir -p ~/.morph/ &amp;amp;&amp;amp; cd ~/.morph/ &amp;amp;&amp;amp; python3.10 -m venv env&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;source ./env/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;On Arch: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;code&gt;yay -S python310&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;mkdir -p ~/.morph/ &amp;amp;&amp;amp; cd ~/.morph/ &amp;amp;&amp;amp; python3.10 -m venv env&lt;/code&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;code&gt;source ./env/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install Rift. We recommend that you &lt;code&gt;pip install&lt;/code&gt; Rift in a dedicated Python &amp;gt;=3.10 virtual environment from this repository. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure that &lt;code&gt;which pip&lt;/code&gt; returns a path whose prefix matches the location of a virtual environment, such as the one installed above.&lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;!-- - Using `pip` and PyPI: --&gt; &#xA;  &lt;!--   - `pip install --upgrade &#39;pyrift[all]&#39;` --&gt; &#xA;  &lt;!--     - `[all]` is required to pull in direct dependencies needed for third-party agents like Aider, Smol Dev, and GPT Engineer. --&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;using &lt;code&gt;pip&lt;/code&gt; from GitHub: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;pip install &#34;git+https://github.com/morph-labs/rift.git@dc27f3b299b79e37b1bcd169efa2216aa07f65b0#egg=pyrift&amp;amp;subdirectory=rift-engine&#34;&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;From source: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;code&gt;cd ~/.morph/ &amp;amp;&amp;amp; git clone git@github.com:morph-labs/rift &amp;amp;&amp;amp; cd ./rift/rift-engine/ &amp;amp;&amp;amp; pip install -e .&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rift VSCode Extension&lt;/strong&gt; (via &lt;code&gt;code --install-extension&lt;/code&gt;, change the executable as needed):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;From the repository root: &lt;code&gt;cd ./editors/rift-vscode &amp;amp;&amp;amp; npm i &amp;amp;&amp;amp; bash reinstall.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The road ahead&lt;/h2&gt; &#xA;&lt;!-- TODO(jesse): rephrase / polish in light of Rift 2.0 --&gt; &#xA;&lt;p&gt;Existing code generation tooling is presently mostly code-agnostic, operating at the level of tokens in / tokens out of code LMs. The &lt;a href=&#34;https://microsoft.github.io/language-server-protocol/&#34;&gt;language server protocol&lt;/a&gt; (LSP) defines a standard for &lt;em&gt;language servers&lt;/em&gt;, objects which index a codebase and provide structure- and runtime-aware interfaces to external development tools like IDEs.&lt;/p&gt; &#xA;&lt;p&gt;The Rift Code Engine is an AI-native language server which will expose interfaces for code transformations and code understanding in a uniform, model- and language-agnostic way --- e.g. &lt;code&gt;rift.summarize_callsites&lt;/code&gt; or &lt;code&gt;rift.launch_ai_swe_async&lt;/code&gt; should work on a Python codebase with &lt;a href=&#34;https://huggingface.co/blog/starcoder&#34;&gt;StarCoder&lt;/a&gt; as well as it works on a Rust codebase using &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;CodeGen&lt;/a&gt;. Within the language server, models will have full programatic access to language-specific tooling like compilers, unit and integration test frameworks, and static analyzers to produce correct code with minimal user intervention. We will develop UX idioms as needed to support this functionality in the Rift IDE extensions.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to Rift at all levels of the stack, for example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;adding support for new open-source models in the Rift Code Engine&lt;/li&gt; &#xA; &lt;li&gt;implementing the Rift API for your favorite programming language&lt;/li&gt; &#xA; &lt;li&gt;UX polish in the VSCode extension&lt;/li&gt; &#xA; &lt;li&gt;adding support for your favorite editor.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See our &lt;a href=&#34;https://raw.githubusercontent.com/morph-labs/rift/main/CONTRIBUTORS.md&#34;&gt;contribution guide&lt;/a&gt; for details and guidelines.&lt;/p&gt; &#xA;&lt;p&gt;Programming is evolving. Join the &lt;a href=&#34;https://discord.gg/wa5sgWMfqv&#34;&gt;community&lt;/a&gt;, contribute to our roadmap, and help shape the future of software.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Chainlit/chainlit</title>
    <updated>2023-08-12T01:35:59Z</updated>
    <id>tag:github.com,2023-08-12:/Chainlit/chainlit</id>
    <link href="https://github.com/Chainlit/chainlit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build Python LLM apps in minutes ‚ö°Ô∏è&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Chainlit üëã&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Build Python LLM apps in minutes ‚ö°Ô∏è&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chainlit lets you create ChatGPT-like UIs on top of any Python code in minutes! Some of the key features include intermediary steps visualisation, element management &amp;amp; display (images, text, carousel, etc.) as well as cloud deployment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/k73SQ3FyUh&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/ZThrUxbAYw?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/chainlit_io&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/chainlit_io.svg?style=social&amp;amp;label=Follow%20%40chainlit_io&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml&#34;&gt;&lt;img src=&#34;https://github.com/Chainlit/chainlit/actions/workflows/ci.yaml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Chainlit/chainlit/assets/13104895/e347e52c-35b2-4c35-8a88-f8ac02dd198e&#34;&gt;https://github.com/Chainlit/chainlit/assets/13104895/e347e52c-35b2-4c35-8a88-f8ac02dd198e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Open a terminal and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install chainlit&#xA;$ chainlit hello&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If this opens the &lt;code&gt;hello app&lt;/code&gt; in your browser, you&#39;re all set!&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://docs.chainlit.io&#34;&gt;here&lt;/a&gt; for full documentation on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Getting started (installation, simple examples)&lt;/li&gt; &#xA; &lt;li&gt;Examples&lt;/li&gt; &#xA; &lt;li&gt;Reference (full API docs)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;üêç Pure Python&lt;/h3&gt; &#xA;&lt;p&gt;Create a new file &lt;code&gt;demo.py&lt;/code&gt; with the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import chainlit as cl&#xA;&#xA;&#xA;@cl.on_message  # this function will be called every time a user inputs a message in the UI&#xA;async def main(message: str):&#xA;    # this is an intermediate step&#xA;    await cl.Message(author=&#34;Tool 1&#34;, content=f&#34;Response from tool1&#34;, indent=1).send()&#xA;&#xA;    # send back the final answer&#xA;    await cl.Message(content=f&#34;This is the final answer&#34;).send()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now run it!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ chainlit run demo.py -w&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Chainlit/chainlit/main/images/quick-start.png&#34; alt=&#34;Quick Start&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üîó With LangChain&lt;/h3&gt; &#xA;&lt;p&gt;Check out our plug-and-play &lt;a href=&#34;https://docs.chainlit.io/integrations/langchain&#34;&gt;integration&lt;/a&gt; with LangChain!&lt;/p&gt; &#xA;&lt;h3&gt;üìö More Examples - Cookbook&lt;/h3&gt; &#xA;&lt;p&gt;You can find various examples of Chainlit apps &lt;a href=&#34;https://github.com/Chainlit/cookbook&#34;&gt;here&lt;/a&gt; that leverage tools and services such as OpenAI, Anthropi—Å, LangChain, LlamaIndex, ChromaDB, Pinecone and more.&lt;/p&gt; &#xA;&lt;h2&gt;üõ£ Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; New UI elements (spreadsheet, video, carousel...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create your own UI elements via component framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; DAG-based chain-of-thought interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support more LLMs in the prompt playground&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; App deployment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Tell us what you would like to see added in Chainlit using the Github issues or on &lt;a href=&#34;https://discord.gg/ZThrUxbAYw&#34;&gt;Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üíÅ Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open-source initiative in a rapidly evolving domain, we welcome contributions, be it through the addition of new features or the improvement of documentation.&lt;/p&gt; &#xA;&lt;p&gt;For detailed information on how to contribute, see &lt;a href=&#34;https://raw.githubusercontent.com/Chainlit/chainlit/main/.github/CONTRIBUTING.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Chainlit is open-source and licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/Chainlit/chainlit/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
</feed>