<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Python Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-05T01:34:14Z</updated>
  <subtitle>Daily Trending of Python in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>KdaiP/StableTTS</title>
    <updated>2024-04-05T01:34:14Z</updated>
    <id>tag:github.com,2024-04-05:/KdaiP/StableTTS</id>
    <link href="https://github.com/KdaiP/StableTTS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Next-generation TTS model using flow-matching and DiT, inspired by Stable Diffusion 3&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;StableTTS&lt;/h1&gt; &#xA; &lt;p&gt;Next-generation TTS model using flow-matching and DiT, inspired by &lt;a href=&#34;https://stability.ai/news/stable-diffusion-3&#34;&gt;Stable Diffusion 3&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;As the first open-source TTS model that tried to combine flow-matching and DiT, StableTTS is a fast and lightweight TTS model for chinese and english speech generation. It has only 10M parameters.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Work is in progress now. Pretrained models and detailed instructions will be released soon!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;For detailed inference instructions, please refer to &lt;code&gt;inference.ipynb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Setting up and training your model with StableTTS is straightforward. Follow these steps to get started:&lt;/p&gt; &#xA;&lt;h3&gt;Preparing Your Data&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Generate Text and Audio pairs&lt;/strong&gt;: Generate the text and audio pair filelist as &lt;code&gt;./filelists/example.txt&lt;/code&gt;. Some recipes of open-source datasets could be found in &lt;code&gt;./recipes&lt;/code&gt;. (Since we use reference encoder to capture speaker identity, there is no need for a speaker ID in multispeaker synthesis and training.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Preprocessing&lt;/strong&gt;: Adjust the &lt;code&gt;DataConfig&lt;/code&gt; in &lt;code&gt;preprocess.py&lt;/code&gt; to set your input and output paths, then run the script. This will process the audio and text according to your list, outputting a JSON file with paths to resampled audios, mel features, and phonemes. &lt;strong&gt;Note: Ensure to switch &lt;code&gt;chinese=False&lt;/code&gt; in &lt;code&gt;DataConfig&lt;/code&gt; for English text processing.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Start training&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Adjust Training Configuration&lt;/strong&gt;: In &lt;code&gt;config.py&lt;/code&gt;, modify &lt;code&gt;TrainConfig&lt;/code&gt; to set your file list path and adjust training parameters as needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the Training Process&lt;/strong&gt;: Launch &lt;code&gt;train.py&lt;/code&gt; to start training your model.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Experiment with Configurations&lt;/h3&gt; &#xA;&lt;p&gt;Feel free to explore and modify settings in &lt;code&gt;config.py&lt;/code&gt; to modify the hyperparameters!&lt;/p&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Task Details&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;StableTTS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;text to mel&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Model is currently in training...&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Vocos&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;mel to wav&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/KdaiP/StableTTS/blob/main/vocos.pt&#34;&gt;🤗&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model structure&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KdaiP/StableTTS/main/figures/structure.jpg&#34; height=&#34;512&#34;&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We use the Diffusion Convolution Transformer block from &lt;a href=&#34;https://github.com/sh-lee-prml/HierSpeechpp&#34;&gt;Hierspeech++&lt;/a&gt;, which is a combination of original &lt;a href=&#34;https://github.com/sh-lee-prml/HierSpeechpp&#34;&gt;DiT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1905.09263.pdf&#34;&gt;FFT&lt;/a&gt;(Feed forward Transformer from fastspeech) for better prosody.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In flow-matching decoder, we add a &lt;a href=&#34;https://arxiv.org/abs/1709.07871&#34;&gt;FiLM layer&lt;/a&gt; before DiT block to condition timestep embedding into model.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;p&gt;The development of our models heavily relies on insights and code from various projects. We express our heartfelt thanks to the creators of the following:&lt;/p&gt; &#xA;&lt;h3&gt;Direct Inspirations&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/shivammehta25/Matcha-TTS&#34;&gt;Matcha TTS&lt;/a&gt;: Essential flow-matching code.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS&#34;&gt;Grad TTS&lt;/a&gt;: Diffusion model structure.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://stability.ai/news/stable-diffusion-3&#34;&gt;Stable Diffusion 3&lt;/a&gt;: Idea of combining flow-matching and DiT.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;Vits&lt;/a&gt;: Code style and MAS insights, DistributedBucketSampler.&lt;/p&gt; &#xA;&lt;h3&gt;Additional References:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/p0p4k/pflowtts_pytorch&#34;&gt;plowtts-pytorch&lt;/a&gt;: codes of MAS in training&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VITS-fast-fine-tuning&#34;&gt;Bert-VITS2&lt;/a&gt; : numba version of MAS and modern pytorch codes of Vits&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fishaudio/fish-speech&#34;&gt;fish-speech&lt;/a&gt;: dataclass usage and mel-spectrogram transforms using torchaudio&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/RVC-Boss/GPT-SoVITS&#34;&gt;gpt-sovits&lt;/a&gt;: melstyle encoder for voice clone&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/openvpi/DiffSinger&#34;&gt;diffsinger&lt;/a&gt;: chinese three section phoneme scheme for chinese g2p&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release pretrained models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Provide finetuning instructions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support Japanese language.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; User friendly preprocess and inference script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance documentation and citations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add chinese version of readme.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Any organization or individual is prohibited from using any technology in this repo to generate or edit someone&#39;s speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>X-PLUG/mPLUG-DocOwl</title>
    <updated>2024-04-05T01:34:14Z</updated>
    <id>tag:github.com,2024-04-05:/X-PLUG/mPLUG-DocOwl</id>
    <link href="https://github.com/X-PLUG/mPLUG-DocOwl" rel="alternate"></link>
    <summary type="html">&lt;p&gt;mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/mPLUG_new1.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;The Powerful Multi-modal LLM Family &lt;p&gt;for OCR-free Document Understanding&lt;/p&gt;&lt;/h2&gt;&#xA; &lt;h2&gt; &lt;strong&gt;Alibaba Group&lt;/strong&gt;&lt;p&gt;&lt;/p&gt; &lt;/h2&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥🔥 [2024.4.3] We build demos of DocOwl1.5 on both &lt;a href=&#34;https://modelscope.cn/studios/iic/mPLUG-DocOwl/&#34;&gt;ModelScope&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/modelscope.png&#34; width=&#34;20&#34;&gt; and &lt;a href=&#34;https://huggingface.co/spaces/mPLUG/DocOwl&#34;&gt;HuggingFace&lt;/a&gt; 🤗, supported by the DocOwl1.5-Omni.&lt;/li&gt; &#xA; &lt;li&gt;🔥[2024.3.28] We release the training data (DocStruct4M, DocDownstream-1.0, DocReason25K), codes and models (DocOwl1.5-stage1, DocOwl1.5, DocOwl1.5-Chat, DocOwl1.5-Omni) of &lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl1.5/&#34;&gt;mPLUG-DocOwl 1.5&lt;/a&gt; on both &lt;strong&gt;HuggingFace&lt;/strong&gt; 🤗 and &lt;strong&gt;ModelScope&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/modelscope.png&#34; width=&#34;20&#34;&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024.3.20] We release the arxiv paper of &lt;a href=&#34;http://arxiv.org/abs/2403.12895&#34;&gt;mPLUG-DocOwl 1.5&lt;/a&gt;, a SOTA 8B Multimodal LLM on OCR-free Document Understanding (DocVQA 82.2, InfoVQA 50.7, ChartQA 70.2, TextVQA 68.6).&lt;/li&gt; &#xA; &lt;li&gt;[2024.01.13] Our Scientific Diagram Analysis dataset &lt;a href=&#34;https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl&#34;&gt;M-Paper&lt;/a&gt; has been available on both &lt;strong&gt;HuggingFace&lt;/strong&gt; 🤗 and &lt;strong&gt;ModelScope&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/modelscope.png&#34; width=&#34;20&#34;&gt;, containing 447k high-resolution diagram images and corresponding paragraph analysis.&lt;/li&gt; &#xA; &lt;li&gt;[2023.10.13] Training data, models of &lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl/&#34;&gt;mPLUG-DocOwl&lt;/a&gt;/&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/UReader/&#34;&gt;UReader&lt;/a&gt; has been open-soruced.&lt;/li&gt; &#xA; &lt;li&gt;[2023.10.10] Our paper &lt;a href=&#34;https://arxiv.org/abs/2310.05126&#34;&gt;UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model&lt;/a&gt; is accepted by EMNLP 2023.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- * 🔥 [10.10] The source code and instruction data will be released in [UReader](https://github.com/LukeForeverYoung/UReader). --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023.07.10] The demo of mPLUG-DocOwl on &lt;a href=&#34;https://modelscope.cn/studios/damo/mPLUG-DocOwl/summary&#34;&gt;ModelScope&lt;/a&gt; is avaliable.&lt;/li&gt; &#xA; &lt;li&gt;[2023.07.07] We release the technical report and evaluation set of mPLUG-DocOwl.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl1.5/&#34;&gt;&lt;strong&gt;mPLUG-DocOwl1.5&lt;/strong&gt;&lt;/a&gt; (Arxiv 2024) - mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/PaperOwl/&#34;&gt;&lt;strong&gt;mPLUG-PaperOwl&lt;/strong&gt;&lt;/a&gt; (Arxiv 2023) - mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/UReader/&#34;&gt;&lt;strong&gt;UReader&lt;/strong&gt;&lt;/a&gt; (EMNLP 2023) - UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/DocOwl/&#34;&gt;&lt;strong&gt;mPLUG-DocOwl&lt;/strong&gt;&lt;/a&gt; (Arxiv 2023) - mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Demo&lt;/h2&gt; &#xA;&lt;p&gt;Note: The demo of HuggingFace is not as stable as ModelScope because the GPU in ZeroGPU Spaces of HuggingFace is dynamically assigned.&lt;/p&gt; &#xA;&lt;h3&gt;ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://modelscope.cn/studios/iic/mPLUG-DocOwl/summary&#34;&gt;&lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;HuggingFace&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/mPLUG/DocOwl&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/huggingface.png&#34; width=&#34;250&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/mPLUG-DocOwl/main/assets/docowl1.5_chat_case.png&#34; alt=&#34;images&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alibaba/AliceMind/tree/main/mPLUG&#34;&gt;mPLUG&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alibaba/AliceMind&#34;&gt;mPLUG-2&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/X-PLUG/mPLUG-Owl&#34;&gt;mPLUG-Owl&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>youtube-jocoding/gpt-bitcoin</title>
    <updated>2024-04-05T01:34:14Z</updated>
    <id>tag:github.com,2024-04-05:/youtube-jocoding/gpt-bitcoin</id>
    <link href="https://github.com/youtube-jocoding/gpt-bitcoin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;비트코인 GPT 인공지능 AI 자동매매 시스템&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;비트코인 GPT 인공지능 AI 업비트 자동매매 시스템 만들기&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT API를 활용하여 투자를 자동화 합니다. by 유튜버 조코딩&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;관련 링크&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jocoding.net/bitcoin&#34;&gt;수업 자료&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/live/-7IVgjUw79s?feature=share&#34;&gt;1편 - 라이브 풀버전 링크(멤버십 전용)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/live/GhZenus5rww?feature=share&#34;&gt;2편 - 라이브 풀버전 링크(멤버십 전용)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtube.com/live/ORo8QAn-g74?feature=share&#34;&gt;3편 - 라이브 풀버전 링크(멤버십 전용)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;편집본 (업데이트 예정)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;전략 소개&lt;/h2&gt; &#xA;&lt;h3&gt;1.autotrade.py, instruction.md&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;데이터: 일(30일), 시간(24시간) OHLCV, Moving Averages, RSI, Stochastic Oscillator, MACD, Bollinger Bands, Orderbook Data&lt;/li&gt; &#xA; &lt;li&gt;전략: 1시간에 한번 판단하여 전량 매수/매도 or 홀드&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2.autotrade_v2.py, instruction_v2.md&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;데이터: 일(30일), 시간(24시간) OHLCV, Moving Averages, RSI, Stochastic Oscillator, MACD, Bollinger Bands, Orderbook Data, 최신 뉴스 데이터(SerpApi), 공포/탐욕 지수&lt;/li&gt; &#xA; &lt;li&gt;전략: 8시간에 한번 판단하여 부분 매수/매도 or 홀드, 투자 데이터 기록하고 AI 재귀 개선&lt;/li&gt; &#xA; &lt;li&gt;뉴스 데이터 조회를 위한 &lt;a href=&#34;https://serpapi.com/&#34;&gt;SerpApi&lt;/a&gt; 가입 및 API KEY 등록 필요&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;.env 파일 생성 및 설정&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#34;YourKey&#34;&#xA;UPBIT_ACCESS_KEY=&#34;YourKey&#34;&#xA;UPBIT_SECRET_KEY=&#34;YourKey&#34;&#xA;SERPAPI_API_KEY=&#34;YourKey&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;로컬 환경 설정&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;AWS EC2 Ubuntu 서버 설정 방법&lt;/h2&gt; &#xA;&lt;h3&gt;업비트 API 허용 IP 설정&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://upbit.com/mypage/open_api_management&#34;&gt;업비트 API 홈페이지&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;기본 세팅&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;한국 기준으로 서버 시간 설정&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;패키지 목록 업데이트&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt update&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;패키지 목록 업그레이드&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;pip3 설치&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install python3-pip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;레포지토리 가져오기&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/youtube-jocoding/gpt-bitcoin.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;서버에서 라이브러리 설치&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;.env 파일 만들고 API KEY 넣기&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;vim .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;명령어&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;현재 경로 상세 출력&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ls -al&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;경로 이동&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd 경로&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vim 에디터로 파일 열기&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;vim autotrade.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;vim 에디터 입력: i&lt;/li&gt; &#xA; &lt;li&gt;vim 에디터 저장: ESC + wq!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;실행하기&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;그냥 실행&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 autotrade.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;백그라운드 실행&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;nohup python3 -u autotrade.py &amp;gt; output.log 2&amp;gt;&amp;amp;1 &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;로그 보기&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cat output.log&#xA;tail -f output.log&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;실행 확인&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ps ax | grep .py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;종료하기&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kill -9 PID&#xA;ex. kill -9 13586&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;추후 계획&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;빗썸, 바이낸스, 코인베이스, OKX, 바이비트도 가능하면 다루겠음&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>